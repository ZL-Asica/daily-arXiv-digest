# 2025-06-02

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 234]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI](https://arxiv.org/abs/2505.23780)

*Émilie Fabre, Katie Seaborn, Shuta Koiwai, Mizuki Watanabe, Paul Riesch*

**Main category:** cs.HC

**Keywords:** generative AI, storytelling agents, user experience, longitudinal study, ethical considerations

**Relevance Score:** 7

**TL;DR:** The study investigates user experiences with a generative AI storytelling app, revealing both positive engagement and challenges in narrative coherence and control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the dynamics of engagement with generative AI storytelling agents over time and understand user experiences in this less-charted domain.

**Method:** A two-week diary study with 28 participants co-creating stories with an AI narrator, using reflexive thematic analysis to identify emerging themes.

**Key Contributions:**

	1. Initial empirical insights into generative AI storytelling
	2. Design considerations for adaptive storytelling systems
	3. Identification of themes related to user experiences with AI

**Result:** Participants experienced both appreciation for AI creativity and frustration due to limitations in narrative coherence and user control.

**Limitations:** Narrative coherence and user control limitations.

**Conclusion:** While generative AI shows potential for enriching storytelling experiences, it raises important questions regarding user agency and ethical considerations.

**Abstract:** Longitudinal engagement with generative AI (GenAI) storytelling agents is a timely but less charted domain. We explored multi-generational experiences with "Dreamsmithy," a daily dream-crafting app, where participants (N = 28) co-created stories with AI narrator "Makoto" every day. Reflections and interactions were captured through a two-week diary study. Reflexive thematic analysis revealed themes likes "oscillating ambivalence" and "socio-chronological bonding," highlighting the complex dynamics that emerged between individuals and the AI narrator over time. Findings suggest that while people appreciated the personal notes, opportunities for reflection, and AI creativity, limitations in narrative coherence and control occasionally caused frustration. The results underscore the potential of GenAI for longitudinal storytelling, but also raise critical questions about user agency and ethics. We contribute initial empirical insights and design considerations for developing adaptive, more-than-human storytelling systems.

</details>


### [2] [PolicyPulse: LLM-Synthesis Tool for Policy Researchers](https://arxiv.org/abs/2505.23994)

*Maggie Wang, Ella Colby, Jennifer Okwara, Varun Nagaraj Rao, Yuhan Liu, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Public Policy, AI in Research

**Relevance Score:** 8

**TL;DR:** PolicyPulse is an LLM-based system that synthesizes public opinions from online discussions to assist policy researchers in drafting memos and briefs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Capturing diverse public opinions is challenging yet crucial for shaping effective policies.

**Method:** PolicyPulse organizes themes based on online community discussions and supports them with quotes from real-world anecdotes, comparing its outputs to authoritative reports and gathering feedback from policy researchers.

**Key Contributions:**

	1. Introduction of an LLM-powered tool for synthesizing public opinions
	2. Demonstration of system's effectiveness through comparisons with policy reports
	3. Insights from policy researchers on the utility of the tool and its limitations

**Result:** PolicyPulse themes align well with authoritative policy reports, enhancing the research process by analyzing data and revealing unexpected themes.

**Limitations:** Insufficient demographic context and challenges in data verification.

**Conclusion:** AI-powered tools like PolicyPulse can significantly influence policy-related research and outcomes.

**Abstract:** Public opinion shapes policy, yet capturing it effectively to surface diverse perspectives remains challenging. This paper introduces PolicyPulse, an LLM-powered interactive system that synthesizes public experiences from online community discussions to help policy researchers author memos and briefs, leveraging curated real-world anecdotes. Given a specific topic (e.g., "Climate Change"), PolicyPulse returns an organized list of themes (e.g., "Biodiversity Loss" or "Carbon Pricing"), supporting each theme with relevant quotes from real-life anecdotes. We compared PolicyPulse outputs to authoritative policy reports. Additionally, we asked 11 policy researchers across multiple institutions in the Northeastern U.S to compare using PolicyPulse with their expert approach. We found that PolicyPulse's themes aligned with authoritative reports and helped spark research by analyzing existing data, gathering diverse experiences, revealing unexpected themes, and informing survey or interview design. Participants also highlighted limitations including insufficient demographic context and data verification challenges. Our work demonstrates how AI-powered tools can help influence policy-relevant research and shape policy outcomes.

</details>


### [3] [Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models](https://arxiv.org/abs/2505.23997)

*Pranav Rao, Maryam Taj, Alex Mariakakis, Joseph Jay Williams, Ananya Bhattacharjee*

**Main category:** cs.HC

**Keywords:** stress management, large language models, digital calendars, user experience, wellbeing interventions

**Relevance Score:** 8

**TL;DR:** This paper investigates how large language models (LLMs) can utilize digital calendar data to provide tailored stress management support to university students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance existing stress-management tools by incorporating the timing and context of students' lives, addressing the lack of personalized and adaptive support currently available.

**Method:** A one-week study with eight university students was conducted using a functional technology probe that sent tailored stress-management messages based on the participants' calendar events. Data was collected through semi-structured interviews and analyzed thematically.

**Key Contributions:**

	1. Proposed the use of digital calendar data for stress management
	2. Identified student preferences for concise and colloquial messaging
	3. Highlighted design implications for LLM-based wellbeing interventions

**Result:** Participants appreciated the timely interventions that focused on upcoming stressful events and preferred a concise, conversational tone in the messages they received.

**Limitations:** 

**Conclusion:** The findings suggest that LLM-based stress-management tools should refine their design to include structured questioning and a calibrated tone to improve user relevance and trust.

**Abstract:** Existing stress-management tools fail to account for the timing and contextual specificity of students' daily lives, often providing static or misaligned support. Digital calendars contain rich, personal indicators of upcoming responsibilities, yet this data is rarely leveraged for adaptive wellbeing interventions. In this short paper, we explore how large language models (LLMs) might use digital calendar data to deliver timely and personalized stress support. We conducted a one-week study with eight university students using a functional technology probe that generated daily stress-management messages based on participants' calendar events. Through semi-structured interviews and thematic analysis, we found that participants valued interventions that prioritized stressful events and adopted a concise, but colloquial tone. These findings reveal key design implications for LLM-based stress-management tools, including the need for structured questioning and tone calibration to foster relevance and trust.

</details>


### [4] [ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners](https://arxiv.org/abs/2505.24000)

*Jad Bendarkawi, Ashley Ponce, Sean Mata, Aminah Aliu, Yuhan Liu, Lei Zhang, Amna Liaqat, Varun Nagaraj Rao, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Language Learning, Large Language Models, Group Conversations, L2 Learners

**Relevance Score:** 9

**TL;DR:** ConversAR is an AR application that allows second language learners to engage in group conversations with embodied LLM agents, helping to reduce speaking anxiety and increase learner autonomy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance language learning for second language (L2) learners by facilitating group conversations through Augmented Reality (AR), an area mostly overlooked in existing tools.

**Method:** Developed an AR application powered by GPT-4o that uses two embodied LLM agents for group dialogue, incorporating vision-based scene understanding and live captions.

**Key Contributions:**

	1. Introduced an AR application for group language learning
	2. Utilized embodied LLM agents for enhanced interaction
	3. Demonstrated reduced anxiety and increased autonomy among users

**Result:** In a system evaluation with 10 participants, users reported lower speaking anxiety and greater learner autonomy compared to in-person practice methods.

**Limitations:** 

**Conclusion:** ConversAR provides a promising approach for group language practice using AR and LLM technologies, showing potential benefits over traditional methods.

**Abstract:** Group conversations are valuable for second language (L2) learners as they provide opportunities to practice listening and speaking, exercise complex turn-taking skills, and experience group social dynamics in a target language. However, most existing Augmented Reality (AR)-based conversational learning tools focus on dyadic interactions rather than group dialogues. Although research has shown that AR can help reduce speaking anxiety and create a comfortable space for practicing speaking skills in dyadic scenarios, especially with Large Language Model (LLM)-based conversational agents, the potential for group language practice using these technologies remains largely unexplored. We introduce ConversAR, a gpt-4o powered AR application, that enables L2 learners to practice contextualized group conversations. Our system features two embodied LLM agents with vision-based scene understanding and live captions. In a system evaluation with 10 participants, users reported reduced speaking anxiety and increased learner autonomy compared to perceptions of in-person practice methods with other learners.

</details>


### [5] [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)

*Amanda Chan, Catherine Di, Joseph Rupertus, Gary Smith, Varun Nagaraj Rao, Manoel Horta Ribeiro, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** crowd work, digital twins, generative AI, research validity, worker agency

**Relevance Score:** 8

**TL;DR:** This paper proposes a hybrid framework using digital twins to enhance crowd work productivity and maintain data quality amidst the rise of generative AI tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of generative AI by crowd workers threatens the authenticity of responses in research, necessitating a solution to maintain data validity and worker roles.

**Method:** A hybrid framework using digital twins was developed and evaluated through an experiment with 88 crowd workers and interviews with 9 individuals (5 workers, 4 researchers).

**Key Contributions:**

	1. Development of a hybrid framework using digital twins for crowd work.
	2. Experiment and interviews demonstrating the effectiveness of the proposed framework.
	3. Emphasis on transparency, ethical data use, and maintaining worker agency.

**Result:** The evaluation showed that digital twins can enhance productivity, reduce decision fatigue, and maintain response quality in crowd work settings.

**Limitations:** The study is based on a limited sample size and may not be generalizable across all crowd work scenarios.

**Conclusion:** Automating repetitive tasks with digital twins while preserving human engagement in nuanced areas can help balance scalability with authenticity in crowd work.

**Abstract:** Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for research, yet workers' growing use of generative AI tools poses challenges. Researchers face compromised data validity as AI responses replace authentic human behavior, while workers risk diminished roles as AI automates tasks. To address this, we propose a hybrid framework using digital twins, personalized AI models that emulate workers' behaviors and preferences while keeping humans in the loop. We evaluate our system with an experiment (n=88 crowd workers) and in-depth interviews with crowd workers (n=5) and social science researchers (n=4). Our results suggest that digital twins may enhance productivity and reduce decision fatigue while maintaining response quality. Both researchers and workers emphasized the importance of transparency, ethical data use, and worker agency. By automating repetitive tasks and preserving human engagement for nuanced ones, digital twins may help balance scalability with authenticity.

</details>


### [6] [Enhancing Critical Thinking in Generative AI Search with Metacognitive Prompts](https://arxiv.org/abs/2505.24014)

*Anjali Singh, Zhitong Guan, Soo Young Rieh*

**Main category:** cs.HC

**Keywords:** Generative AI, metacognitive prompts, critical thinking, human-AI interaction, user study

**Relevance Score:** 8

**TL;DR:** This study examines the effects of metacognitive prompts on critical thinking when using Generative AI tools for searches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how reliance on Generative AI may diminish metacognitive engagement and critical thinking, and to evaluate solutions like metacognitive prompts.

**Method:** A user study was conducted with 40 university students to assess the impact of metacognitive prompts during GenAI-based searches on their thought processes and behaviors.

**Key Contributions:**

	1. Demonstrated the positive impact of metacognitive prompts on critical thinking during GenAI searches.
	2. Identified how prompts influenced students' inquiry behaviors and evaluation of AI-generated responses.
	3. Provided insights for designing metacognitive support in human-AI applications.

**Result:** The use of metacognitive prompts resulted in higher engagement levels, broader topic exploration, and deeper inquiry through follow-up queries. Students found the prompts helpful for assessing AI responses and identifying key takeaways.

**Limitations:** 

**Conclusion:** Metacognitive prompts can enhance critical thinking in human-AI interactions, with variations in effectiveness based on students' metacognitive flexibility.

**Abstract:** The growing use of Generative AI (GenAI) conversational search tools has raised concerns about their effects on people's metacognitive engagement, critical thinking, and learning. As people increasingly rely on GenAI to perform tasks such as analyzing and applying information, they may become less actively engaged in thinking and learning. This study examines whether metacognitive prompts - designed to encourage people to pause, reflect, assess their understanding, and consider multiple perspectives - can support critical thinking during GenAI-based search. We conducted a user study (N=40) with university students to investigate the impact of metacognitive prompts on their thought processes and search behaviors while searching with a GenAI tool. We found that these prompts led to more active engagement, prompting students to explore a broader range of topics and engage in deeper inquiry through follow-up queries. Students reported that the prompts were especially helpful for considering overlooked perspectives, promoting evaluation of AI responses, and identifying key takeaways. Additionally, the effectiveness of these prompts was influenced by students' metacognitive flexibility. Our findings highlight the potential of metacognitive prompts to foster critical thinking and provide insights for designing and implementing metacognitive support in human-AI interactions.

</details>


### [7] [Advancing Digital Accessibility: Integrating AR/VR and Health Tech for Inclusive Healthcare Solutions](https://arxiv.org/abs/2505.24039)

*Vishnu Ramineni, Shivareddy Devarapalli, Balakrishna Pothineni, Prema Kumar Veerapaneni, Aditya Gupta, Pankaj Gupta*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Virtual Reality, Digital Accessibility, Healthcare, Disabilities

**Relevance Score:** 7

**TL;DR:** The paper explores the integration of Augmented Reality (AR) and Virtual Reality (VR) in healthcare to improve accessibility for patients with disabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the digital accessibility challenges faced by patients with disabilities in healthcare services.

**Method:** Study of inclusivity and accessibility features of AR/VR applications in various healthcare domains, along with analysis of current trends and case studies.

**Key Contributions:**

	1. Evaluates the impact of AR/VR on healthcare accessibility.
	2. Analyzes current trends and case studies in AR/VR application.
	3. Suggests practical recommendations for improved implementation.

**Result:** Findings suggest that AR/VR can significantly enhance patient experiences in telemedicine, education, assistive tools, and rehabilitation, despite certain challenges.

**Limitations:** Technical limitations, implementation costs, and regulatory challenges hinder widespread adoption.

**Conclusion:** Recommendations for the integration of AR/VR technologies aim to create a more equitable healthcare system for individuals with disabilities.

**Abstract:** Modern healthcare domain incorporates a feature of digital accessibility to ensure seamless flow of online services for the patients. However, this feature of digital accessibility poses a challenge particularly for patients with disabilities. To eradicate this issue and provide immersive and user-friendly experiences, evolving technologies like Augmented Reality (AR) and Virtual Reality (VR) are integrated in medical applications to enhance accessibility. The present research paper aims to study inclusivity and accessibility features of AR/VR in revolutionizing healthcare practices especially in domains like telemedicine, patient education, assistive tools, and rehabilitation for persons with disabilities. The current trends of advancements and case studies are also analyzed to measure the efficacy of AR/VR in healthcare. Moreover, the paper entails a detailed analysis of the challenges of its adoption particularly technical limitations, implementation costs, and regulatory aspects. Finally, the paper concludes with recommendations for integrating AR/VR to foster a more equitable and inclusive healthcare system and provide individuals with auditory, visual, and motor impairments with digital healthcare solutions.

</details>


### [8] [Advancing Digital Accessibility In Digital Pharmacy, Healthcare, And Wearable Devices: Inclusive Solutions for Enhanced Patient Engagement](https://arxiv.org/abs/2505.24042)

*Vishnu Ramineni, Balaji Shesharao Ingole, Nikhil Kumar Pulipeta, Balakrishna Pothineni, Aditya Gupta*

**Main category:** cs.HC

**Keywords:** digital accessibility, healthcare, wearable devices, AI solutions, WCAG compliance

**Relevance Score:** 8

**TL;DR:** The paper discusses the importance of digital accessibility in healthcare for individuals with disabilities and offers solutions to improve user engagement and usability in telemedicine, online pharmacies, and wearable devices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the barriers faced by individuals with disabilities in accessing modern digital healthcare solutions and to emphasize the need for digital accessibility compliance.

**Method:** The paper reviews current accessibility guidelines, practices, and technologies while proposing innovative AI-driven solutions, speech-activated interfaces, and tactile feedback mechanisms.

**Key Contributions:**

	1. Highlighting barriers faced by users with disabilities in digital healthcare
	2. Proposing AI-driven and innovative solutions for accessibility
	3. Emphasizing the importance of compliance with accessibility standards

**Result:** The research identifies critical obstacles faced by users with various impairments and stresses the importance of compliance with accessibility standards to create a more inclusive digital healthcare environment.

**Limitations:** 

**Conclusion:** Prioritizing accessibility is essential for fostering an inclusive culture in digital healthcare, requiring collaboration between healthcare providers, policymakers, and technology developers.

**Abstract:** Modern healthcare facilities demand digital accessibility to guarantee equal access to telemedicine platforms, online pharmacy services, and health monitoring devices that can be worn or are handy. With the rising call for the implementation of robust digital healthcare solutions, people with disabilities encounter impediments in their endeavor of managing and getting accustomed to these modern technologies owing to insufficient accessibility features. The paper highlights the role of comprehensive solutions for enhanced patient engagement and usability, particularly, in digital pharmacy, healthcare, and wearable devices. Besides, it elucidates the key obstructions faced by users experiencing auditory, visual, cognitive, and motor impairments. Through a kind consideration of present accessibility guidelines, practices, and emerging technologies, the paper provides a holistic overview by offering innovative solutions, accentuating the vitality of compliance with Web Content Accessibility Guidelines (WCAG), Americans with Disabilities Act (ADA), and other regulatory structures to foster easy access to digital healthcare services. Moreover, there is due focus on using AI-driven tools, speech-activated interfaces, and tactile feedback in wearable health devices to assist persons with disabilities. The outcome of the research explicates the necessity of prioritizing accessibility for individuals with disabilities and cultivating a culture where healthcare providers, policymakers, and officials build a patient-centered digital healthcare ecosystem that is all-encompassing in nature.

</details>


### [9] [Beyond the Prototype: Challenges of Long-Term Integration of Visual Analytics in Civic Spaces](https://arxiv.org/abs/2505.24102)

*Mahmood Jasim, Narges Mahyar*

**Main category:** cs.HC

**Keywords:** visual analytics, civic technology, stakeholder engagement, sustainable implementation, sociotechnical challenges

**Relevance Score:** 4

**TL;DR:** This paper discusses the challenges of deploying visual analytics systems in civic contexts, focusing on technical and sociotechnical barriers to adoption.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the barriers faced in the deployment of visual analytics systems in real-world civic applications.

**Method:** The authors reflect on their experiences in designing, developing, and implementing visual analytics systems and analyze the challenges encountered.

**Key Contributions:**

	1. Insights into the sociotechnical barriers of visual analytics deployment
	2. Proposed strategies for equitable stakeholder engagement
	3. Framework suggestions for sustainable implementation in civic contexts

**Result:** They identify key barriers such as organizational hierarchies, designer-stakeholder misalignment, and technology adoption concerns.

**Limitations:** The reflection is based solely on the authors' experiences, which may not be generalizable.

**Conclusion:** The paper emphasizes the necessity for better integration strategies and sustainable implementation frameworks that can facilitate effective stakeholder engagement and promote sustained use of these systems.

**Abstract:** Despite the recognized benefits of visual analytics systems in supporting data-driven decision-making, their deployment in real-world civic contexts often faces significant barriers. Beyond technical challenges such as resource constraints and development complexity, sociotechnical factors, including organizational hierarchies, misalignment between designers and stakeholders, and concerns around technology adoption hinder their sustained use. In this work, we reflect on our collective experiences of designing, developing, and deploying visual analytics systems in the civic domain and discuss challenges across design and adoption aspects. We emphasize the need for deeper integration strategies, equitable stakeholder engagement, and sustainable implementation frameworks to bridge the gap between research and practice.

</details>


### [10] [GPTFootprint: Increasing Consumer Awareness of the Environmental Impacts of LLMs](https://arxiv.org/abs/2505.24107)

*Nora Graves, Vitus Larrieu, Yingyue Trace Zhang, Joanne Peng, Varun Nagaraj Rao, Yuhan Liu, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** GPTFootprint, environmental impact, LLM usage, user awareness, sustainable AI

**Relevance Score:** 6

**TL;DR:** GPTFootprint is a browser extension that raises awareness about the environmental impact of LLMs by tracking user resource consumption.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited research on AI end users' environmental impact and promote sustainable AI usage.

**Method:** Developed the GPTFootprint browser extension, which visualizes resource consumption from ChatGPT queries and prompts users to take breaks after reaching query limits.

**Key Contributions:**

	1. Introduction of GPTFootprint browser extension
	2. Demonstration of increased user awareness on environmental impact
	3. Insights into behavior modification strategies in LLM context

**Result:** A week-long user study showed GPTFootprint increased awareness of environmental impact but had limited success in reducing usage of ChatGPT.

**Limitations:** Limited success in reducing ChatGPT usage despite increased awareness.

**Conclusion:** Individual-level interventions can aid sustainable AI usage, although awareness alone may not significantly modify user behavior.

**Abstract:** With the growth of AI, researchers are studying how to mitigate its environmental impact, primarily by proposing policy changes and increasing awareness among developers. However, research on AI end users is limited. Therefore, we introduce GPTFootprint, a browser extension that aims to increase consumer awareness of the significant water and energy consumption of LLMs, and reduce unnecessary LLM usage. GPTFootprint displays a dynamically updating visualization of the resources individual users consume through their ChatGPT queries. After a user reaches a set query limit, a popup prompts them to take a break from ChatGPT. In a week-long user study, we found that GPTFootprint increases people's awareness of environmental impact, but has limited success in decreasing ChatGPT usage. This research demonstrates the potential for individual-level interventions to contribute to the broader goal of sustainable AI usage, and provides insights into the effectiveness of awareness-based behavior modification strategies in the context of LLMs.

</details>


### [11] [How Students (Really) Use ChatGPT: Uncovering Experiences Among Undergraduate Students](https://arxiv.org/abs/2505.24126)

*Tawfiq Ammari, Meilun Chen, S M Mehedi Zaman, Kiran Garimella*

**Main category:** cs.HC

**Keywords:** ChatGPT, Self-Directed Learning, Higher Education, Generative AI, User Engagement

**Relevance Score:** 7

**TL;DR:** Investigation of undergraduate students' engagement with ChatGPT in self-directed learning reveals five usage categories and important predictors of continued use.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how students utilize ChatGPT for self-directed learning and to identify effective engagement strategies.

**Method:** Analysis of naturalistic interaction logs to categorize uses of ChatGPT and behavioral modeling to understand predictors of continued use.

**Key Contributions:**

	1. Identification of five dominant use categories of ChatGPT
	2. Behavioral modeling of predictors for continued use
	3. Recommendations for integration of AI in higher education

**Result:** Students engage with ChatGPT across five categories and persist in using it through task breakdowns, indicating its role as a learning partner.

**Limitations:** 

**Conclusion:** The study highlights the need for design and policy recommendations for integrating generative AI in education to enhance learning experiences.

**Abstract:** This study investigates how undergraduate students engage with ChatGPT in self directed learning contexts. Analyzing naturalistic interaction logs, we identify five dominant use categories of ChatGPT information seeking, content generation, language refinement, meta cognitive engagement, and conversational repair. Behavioral modeling reveals that structured, goal driven tasks like coding, multiple choice solving, and job application writing are strong predictors of continued use. Drawing on Self-Directed Learning (SDL) and the Uses and Gratifications Theory (UGT), we show how students actively manage ChatGPTs affordances and limitations through prompt adaptation, follow-ups, and emotional regulation. Rather than disengaging after breakdowns, students often persist through clarification and repair, treating the assistant as both tool and learning partner. We also offer design and policy recommendations to support transparent, responsive, and pedagogically grounded integration of generative AI in higher education.

</details>


### [12] [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)

*Zining Wang, Yuxuan Zhang, Dongwook Yoon, Nicholas Vincent, Farhan Samir, Vered Shwartz*

**Main category:** cs.HC

**Keywords:** Wikipedia, Multilingual Information, User-Centered Design, Usability, Epistemic Equity

**Relevance Score:** 7

**TL;DR:** WikiGap is a system that improves access to complementary information from other language editions of Wikipedia, enhancing users' awareness and fact-finding accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** English Wikipedia's dominance leads readers to overlook valuable information in other language editions, which contain complementary facts rooted in diverse cultures.

**Method:** WikiGap integrates a multilingual information-gap discovery method with a user-centered design to present information from French, Russian, and Chinese Wikipedia within the English Wikipedia interface.

**Key Contributions:**

	1. Introduces a system for surfacing complementary facts from other Wikipedia editions.
	2. Demonstrates significant improvements in usability and task performance in user studies.
	3. Raises awareness of the content availability across different language editions.

**Result:** In a study with 21 participants, WikiGap showed significant improvements in fact-finding accuracy and reduced task times, yielding a higher usability score compared to the current Interlanguage Link system.

**Limitations:** Study sample size is small, and the system requires further testing across broader demographics.

**Conclusion:** WikiGap enhances epistemic equity by allowing users to recognize the value of complementary information in non-English Wikipedia editions.

**Abstract:** With more than 11 times as many pageviews as the next, English Wikipedia dominates global knowledge access relative to other language editions. Readers are prone to assuming English Wikipedia as a superset of all language editions, leading many to prefer it even when their primary language is not English. Other language editions, however, comprise complementary facts rooted in their respective cultures and media environments, which are marginalized in English Wikipedia. While Wikipedia's user interface enables switching between language editions through its Interlanguage Link (ILL) system, it does not reveal to readers that other language editions contain valuable, complementary information. We present WikiGap, a system that surfaces complementary facts sourced from other Wikipedias within the English Wikipedia interface. Specifically, by combining a recent multilingual information-gap discovery method with a user-centered design, WikiGap enables access to complementary information from French, Russian, and Chinese Wikipedia. In a mixed-methods study (n=21), WikiGap significantly improved fact-finding accuracy, reduced task time, and received a 32-point higher usability score relative to Wikipedia's current ILL-based navigation system. Participants reported increased awareness of the availability of complementary information in non-English editions and reconsidered the completeness of English Wikipedia. WikiGap thus paves the way for improved epistemic equity across language editions.

</details>


### [13] [Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work](https://arxiv.org/abs/2505.24246)

*Alice Qian Zhang, Ryland Shaw, Laura Dabbish, Jina Suh, Hong Shen*

**Main category:** cs.HC

**Keywords:** Responsible AI, Crowdwork, Worker Well-being, Risk Disclosure, Transparency

**Relevance Score:** 9

**TL;DR:** This study investigates how task designers communicate well-being risks to crowd workers in responsible AI content work, highlighting the need for improved transparency and support in risk disclosure.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As AI systems are tested in high-stakes environments, there are risks to the well-being of crowd workers involved in responsible AI tasks, which require clearer communication of these risks.

**Method:** Interviews with 23 task designers from academic and industry sectors to examine their approaches to risk disclosure in crowdsourced responsible AI work.

**Key Contributions:**

	1. Identification of gaps in current transparency frameworks regarding well-being risks to workers
	2. Insights from task designers on how risks are communicated
	3. Recommendations for improving risk disclosure practices

**Result:** The study finds inconsistencies in how well-being risks are recognized and communicated, indicating a need for better workflows and transparency for task designers.

**Limitations:** 

**Conclusion:** Improving the communication of well-being risks can enhance worker safety and strengthen the ethical standards and effectiveness of AI development processes.

**Abstract:** As AI systems are increasingly tested and deployed in open-ended and high-stakes domains, crowd workers are often tasked with responsible AI (RAI) content work. These tasks include labeling violent content, moderating disturbing text, or simulating harmful behavior for red teaming exercises to shape AI system behaviors. While prior efforts have highlighted the risks to worker well-being associated with RAI content work, far less attention has been paid to how these risks are communicated to workers. Existing transparency frameworks and guidelines such as model cards, datasheets, and crowdworksheets focus on documenting model information and dataset collection processes, but they overlook an important aspect of disclosing well-being risks to workers. In the absence of standard workflows or clear guidance, the consistent application of content warnings, consent flows, or other forms of well-being risk disclosure remain unclear. This study investigates how task designers approach risk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task designers across academic and industry sectors, we examine how well-being risk is recognized, interpreted, and communicated in practice. Our findings surface a need to support task designers in identifying and communicating well-being risk not only to support crowdworker well-being but also to strengthen the ethical integrity and technical efficacy of AI development pipelines.

</details>


### [14] [A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins](https://arxiv.org/abs/2505.24348)

*Taku Yamazaki, Kaito Watanabe, Tatsuya Kase, Kenta Hasegawa, Koki Saida, Takumi Miyoshi*

**Main category:** cs.HC

**Keywords:** 3D mobile crowdsensing, urban digital twins, gamified sensing, point cloud integration, wearable technology

**Relevance Score:** 4

**TL;DR:** This paper proposes a 3D mobile crowdsensing framework designed for sustainable urban digital twins, integrating gamified active sensing and wearable passive sensing with dynamic point cloud integration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for sustainable urban digital twins to enhance urban planning and management.

**Method:** The framework includes an active model using a gamified approach for data collection and a passive model utilizing wearable technology, along with mechanisms for spatial information management and point cloud integration.

**Key Contributions:**

	1. Introduction of a gamified 3D mobile crowdsensing mechanism
	2. Development of a spatial information management strategy using Geohash
	3. Implementation of dynamic point cloud integration for urban digital twins

**Result:** Real-world experiments validated the effectiveness of the proposed models in data collection and integration for urban digital twins.

**Limitations:** The scalability of the framework in larger urban contexts and potential privacy concerns with data collection.

**Conclusion:** The proposed 3D-MCS framework contributes to the development of sustainable urban digital twins through innovative data collection and integration methods.

**Abstract:** In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed at sustainable urban digital twins (UDTs). The framework comprises four key mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models; (2) the Geohash-based spatial information management mechanism; (3) the dynamic point cloud integration mechanism for UDTs; and (4) the web-based real-time visualizer for 3D-MCS and UDTs. The active sensing model features a gamified 3D-MCS approach, where participants collect point cloud data through an augmented reality territory coloring game. In contrast, the passive sensing model employs a wearable 3D-MCS approach, where participants wear smartphones around their necks without disrupting daily activities. The spatial information management mechanism efficiently partitions the space into regions using Geohash. The dynamic point cloud integration mechanism incorporates point clouds collected by 3D-MCS into UDTs through global and local point cloud registration. Finally, we evaluated the proposed framework through real-world experiments. We verified the effectiveness of the proposed 3D-MCS models from the perspectives of subjective evaluation and data collection and analysis. Furthermore, we analyzed the performance of the dynamic point cloud integration using a dataset.

</details>


### [15] [Can LLMs and humans be friends? Uncovering factors affecting human-AI intimacy formation](https://arxiv.org/abs/2505.24658)

*Yeseon Hong, Junhyuk Choi, Minju Kim, Bugeun Kim*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-LLM Interaction, Intimacy, Self-Disclosure, Natural Responses

**Relevance Score:** 9

**TL;DR:** This study investigates how intimacy develops in human-LLM interactions through gradual self-disclosure, reciprocity, and perceived naturalness of responses.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding intimacy in human-LLM interactions is vital as LLMs gain popularity in conversational roles, yet little research has addressed how intimacy forms in these contexts.

**Method:** The study consists of two main experiments: Study 1 involved 29 users interacting with a vanilla LLM focusing on mutual self-disclosure; Study 2 involved 53 users, incorporating self-criticism to enhance natural responses in a similar experimental setup.

**Key Contributions:**

	1. Explored the role of gradual self-disclosure in LLM interactions
	2. Demonstrated the impact of self-criticism on perceived naturalness and intimacy
	3. Identified potential pitfalls of excessive empathetic responses in interactions

**Result:** Gradual self-disclosure significantly increased perceived social intimacy. Responses from LLMs using self-criticism were viewed as more natural and fostered higher intimacy early in interactions, although overly empathetic responses could disrupt immersion.

**Limitations:** The study's results may not be generalizable beyond the specific LLMs used or the small user samples; further research is needed across diverse interactions.

**Conclusion:** The findings suggest that gradual self-disclosure is beneficial for intimacy in human-LLM interactions, emphasizing the need for careful calibration of responses to maintain immersion.

**Abstract:** Large language models (LLMs) are increasingly being used in conversational roles, yet little is known about how intimacy emerges in human-LLM interactions. Although previous work emphasized the importance of self-disclosure in human-chatbot interaction, it is questionable whether gradual and reciprocal self-disclosure is also helpful in human-LLM interaction. Thus, this study examined three possible aspects contributing to intimacy formation: gradual self-disclosure, reciprocity, and naturalness. Study 1 explored the impact of mutual, gradual self-disclosure with 29 users and a vanilla LLM. Study 2 adopted self-criticism methods for more natural responses and conducted a similar experiment with 53 users. Results indicate that gradual self-disclosure significantly enhances perceived social intimacy, regardless of persona reciprocity. Moreover, participants perceived utterances generated with self-criticism as more natural compared to those of vanilla LLMs; self-criticism fostered higher intimacy in early stages. Also, we observed that excessive empathetic expressions occasionally disrupted immersion, pointing to the importance of response calibration during intimacy formation.

</details>


### [16] [Functional Near-Infrared Spectroscopy (fNIRS) Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming](https://arxiv.org/abs/2405.08906)

*Shayla Sharmin, Elham Bakhshipour, Behdokht Kiafar, Md Fahim Abrar, Pinar Kullu, Nancy Getchell, Roghayeh Leila Barmaki*

**Main category:** cs.HC

**Keywords:** educational games, touchscreen input, cognitive effort, neural efficiency, HCI

**Relevance Score:** 7

**TL;DR:** This paper investigates the cognitive impacts of using hands vs. a stylus for touchscreens in educational games, revealing that hand use is more efficient and intuitive.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the cognitive impacts of different input modalities (hands vs. stylus) on educational game play and learning experiences.

**Method:** The study compared the use of hands and stylus for touchscreen interactions in an educational game, measuring oxygenated hemoglobin via functional Near-Infrared Spectroscopy and self-reported measures of user experience.

**Key Contributions:**

	1. Demonstrated the cognitive differences between hand and stylus interactions in educational games.
	2. Provided evidence that hand interactions are more efficient and intuitive compared to stylus use.
	3. Underlined the importance of designing educational games that minimize cognitive load.

**Result:** The hand condition exhibited significantly lower neural involvement and higher neural efficiency compared to the stylus condition, indicating less cognitive effort required when using hands for interaction.

**Limitations:** 

**Conclusion:** Hand-based input methods are suggested to be more intuitive and less cognitively demanding, highlighting the need for game design that reduces cognitive effort for learners.

**Abstract:** Educational games enhance learning experiences by integrating touchscreens, making interactions more engaging and intuitive for learners. However, the cognitive impacts of educational game-play input modalities, such as the hand and stylus technique, are unclear. We compared the experience of using hands vs. a stylus for touchscreens while playing an educational game by analyzing oxygenated hemoglobin collected by functional Near-Infrared Spectroscopy and self-reported measures. In addition, we measured the hand vs. the stylus modalities of the task and calculated the relative neural efficiency and relative neural involvement using the mental demand and the quiz score. Our findings show that the hand condition had a significantly lower neural involvement, yet higher neural efficiency than the stylus condition. This result suggests the requirement of less cognitive effort while using the hand. Additionally, the self-reported measures show significant differences, and the results suggest that hand-based input is more intuitive, less cognitively demanding, and less frustrating. Conversely, the use of a stylus required higher cognitive effort due to the cognitive balance of controlling the pen and answering questions. These findings highlight the importance of designing educational games that allow learners to engage with the system while minimizing cognitive effort.

</details>


### [17] [Leveraging Complementary AI Explanations to Mitigate Misunderstanding in XAI](https://arxiv.org/abs/2503.00303)

*Yueqing Xuan, Kacper Sokol, Mark Sanderson, Jeffrey Chan*

**Main category:** cs.HC

**Keywords:** AI explanations, complementary explanations, predictive models, misinterpretations, framework evaluation

**Relevance Score:** 8

**TL;DR:** This paper proposes a method for creating complementary explanations in AI to enhance understanding and reduce misinterpretations of predictive models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the comprehensibility of AI explanations and mitigate misinterpretations that arise from predictive models.

**Method:** The authors introduce a framework for designing and evaluating complementary explanation pairs that clarify potential misconceptions while maintaining coherence.

**Key Contributions:**

	1. Introduces complementary explanations to address limitations of primary AI explanations.
	2. Develops a framework for designing and evaluating explanation pairs.
	3. Provides both qualitative properties and quantitative metrics for assessment.

**Result:** The proposed method allows for the construction of complementary explanations that effectively minimize the chances of misinterpretation.

**Limitations:** The framework's effectiveness may vary across different domains and types of predictive models.

**Conclusion:** Complementary explanations can significantly enhance user understanding of AI predictions and reduce incorrect inferences.

**Abstract:** Artificial intelligence explanations can make complex predictive models more comprehensible. To be effective, however, they should anticipate and mitigate possible misinterpretations, e.g., arising when users infer incorrect information that is not explicitly conveyed. To this end, we propose complementary explanations -- a novel method that pairs explanations to compensate for their respective limitations. A complementary explanation adds insights that clarify potential misconceptions stemming from the primary explanation while ensuring their coherency and avoiding redundancy. We introduce a framework for designing and evaluating complementary explanation pairs based on pertinent qualitative properties and quantitative metrics. Our approach allows to construct complementary explanations that minimise the chance of their misinterpretation.

</details>


### [18] [Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing](https://arxiv.org/abs/2504.13883)

*Shayla Sharmin, Roghayeh Leila Barmaki*

**Main category:** cs.HC

**Keywords:** cognitive effort, functional near-infrared spectroscopy, deep learning, educational technology, learning effectiveness

**Relevance Score:** 7

**TL;DR:** This study uses a hybrid deep learning model to estimate cognitive effort based on fNIRS data, aiming to enhance learning effectiveness and engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to estimate cognitive effort to enable educators to modify learning material, enhancing engagement and effectiveness.

**Method:** A hybrid deep learning model (CNN-GRU) was used alongside traditional machine learning techniques to predict performance scores and estimate cognitive metrics (RNE and RNI) from collected fNIRS data.

**Key Contributions:**

	1. Demonstrated the efficacy of hybrid deep learning models in estimating cognitive effort from fNIRS data.
	2. Provided insights into the relationship between cognitive effort and learning material adaptation.
	3. Showed potential for traditional ML models to leverage features from deep learning for improved accuracy.

**Result:** The hybrid CNN-GRU model achieved the highest accuracy with 78.36% training and 73.08% test accuracy, outperforming traditional models and showing that predicted RNE and RNI align with actual trends.

**Limitations:** Accuracy still moderate and requires further validation across diverse learning contexts.

**Conclusion:** Findings indicate that cognitive effort can be effectively estimated using deep learning, with implications for improving learning environments.

**Abstract:** This study estimates cognitive effort (CE) based on functional near-infrared spectroscopy (fNIRS) data and performance scores using a hybrid deep learning model. The estimation of CE enables educators to modify material to enhance learning effectiveness and student engagement. Relative neural efficiency (RNE) and relative neural involvement (RNI) are two metrics that have been used to represent CE. To estimate RNE and RNI we need hemodynamic response in the brain and the performance score of a task.We collected oxygenated hemoglobin ($\Delta \mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based educational game, each with a 30-second response time. We used deep learning models to predict the performance score and estimate RNE and RNI to understand CE. The study compares traditional machine learning techniques with deep learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine which approach provides better accuracy in predicting performance scores. The result shows that the hybrid CNN-GRU gives better performance with 78.36\% training accuracy and 73.08\% test accuracy than other models. We performed XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%). This suggests that the features learned from this hybrid model generalize better even in traditional machine learning algorithms. We used the $\Delta \mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive effort in our four test cases. Our result shows that even with moderate accuracy, the predicted RNE and RNI closely follows the actual trends. we also observed that when participants were in a state of high CE, introducing rest led decrease of CE. These findings can be helpful to design and improve learning environments and provide valuable insights in learning materials.

</details>


### [19] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)

*Eleni Vasilaki*

**Main category:** cs.HC

**Keywords:** Large Language Models, Collective Knowledge, Human-Machine Interaction, Co-Augmentation, Neuroscience and AI

**Relevance Score:** 9

**TL;DR:** This paper reframes Large Language Models (LLMs) as dynamic representations of Collective human Knowledge, emphasizing dialogue and co-augmentation between human and machine intelligence.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a new perspective on understanding LLMs that goes beyond traditional analyses of architecture and training data.

**Method:** The paper draws on concepts from neuroscience and AI, analyzing emergent dialogue patterns during sustained interactions with ChatGPT-4.

**Key Contributions:**

	1. Reframing LLMs as dynamic embodiments of collective knowledge
	2. Analysis of dialogue patterns and co-augmentation in human-machine interaction
	3. Exploration of fine-tuning implications on LLM performance

**Result:** The study explores the implications of fine-tuning LLMs and highlights the mutual enhancement of human and machine cognition.

**Limitations:** 

**Conclusion:** This new lens sheds light on the interaction, representation, and agency in modern AI systems.

**Abstract:** Large Language Models (LLMs) are typically analysed through architectural, behavioural, or training-data lenses. This article offers a theoretical and experiential re-framing: LLMs as dynamic instantiations of Collective human Knowledge (CK), where intelligence is evoked through dialogue rather than stored statically. Drawing on concepts from neuroscience and AI, and grounded in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns, the implications of fine-tuning, and the notion of co-augmentation: mutual enhancement between human and machine cognition. This perspective offers a new lens for understanding interaction, representation, and agency in contemporary AI systems.

</details>


### [20] [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)

*Amanda Chan, Catherine Di, Joseph Rupertus, Gary Smith, Varun Nagaraj Rao, Manoel Horta Ribeiro, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** crowd work, digital twins, human-computer interaction, AI tools, productivity

**Relevance Score:** 8

**TL;DR:** This paper introduces a hybrid framework using digital twins to mitigate the challenges posed by generative AI in crowd work platforms by enhancing productivity while ensuring worker engagement and data validity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of generative AI tools in crowd work platforms is threatening data validity and worker roles; this paper addresses these challenges.

**Method:** A hybrid framework was proposed that utilizes digital twins to emulate workers' behaviors and preferences while keeping them involved in decision-making.

**Key Contributions:**

	1. Proposed a digital twin framework for crowd work
	2. Demonstrated potential for enhanced productivity and reduced decision fatigue
	3. Highlighted the importance of transparency and worker agency

**Result:** The evaluation of the system through an experiment and interviews indicated that digital twins can enhance productivity and reduce decision fatigue without compromising response quality.

**Limitations:** The study is limited by a small sample size and may not generalize across all types of crowd work tasks.

**Conclusion:** Automating repetitive tasks with digital twins can retain human engagement for complex tasks, thus achieving a balance between scalability and authenticity.

**Abstract:** Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for research, yet workers' growing use of generative AI tools poses challenges. Researchers face compromised data validity as AI responses replace authentic human behavior, while workers risk diminished roles as AI automates tasks. To address this, we propose a hybrid framework using digital twins, personalized AI models that emulate workers' behaviors and preferences while keeping humans in the loop. We evaluate our system with an experiment (n=88 crowd workers) and in-depth interviews with crowd workers (n=5) and social science researchers (n=4). Our results suggest that digital twins may enhance productivity and reduce decision fatigue while maintaining response quality. Both researchers and workers emphasized the importance of transparency, ethical data use, and worker agency. By automating repetitive tasks and preserving human engagement for nuanced ones, digital twins may help balance scalability with authenticity.

</details>


### [21] [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)

*Zining Wang, Yuxuan Zhang, Dongwook Yoon, Nicholas Vincent, Farhan Samir, Vered Shwartz*

**Main category:** cs.HC

**Keywords:** Wikipedia, multilingual information, complementary facts, user-centered design, epistemic equity

**Relevance Score:** 7

**TL;DR:** WikiGap is a system that enhances information access in English Wikipedia by surfacing complementary facts from other language editions, improving fact-finding accuracy and user awareness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misconception that English Wikipedia contains all relevant information and to promote the use of complementary knowledge found in other language editions.

**Method:** A mixed-methods study was conducted with 21 participants, using a multilingual information-gap discovery method combined with user-centered design principles.

**Key Contributions:**

	1. Development of the WikiGap system
	2. Evidence of improved fact-finding and usability
	3. Enhanced user awareness of other language editions' contributions

**Result:** WikiGap significantly improved fact-finding accuracy and reduced task completion time, while achieving a 32-point higher usability score compared to the existing Interlanguage Link navigation in Wikipedia.

**Limitations:** The study involved a small sample size (n=21) and focused on specific languages (French, Russian, Chinese).

**Conclusion:** WikiGap enhances epistemic equity by facilitating access to complementary information from multiple languages, encouraging users to consider the limitations of English Wikipedia.

**Abstract:** With more than 11 times as many pageviews as the next, English Wikipedia dominates global knowledge access relative to other language editions. Readers are prone to assuming English Wikipedia as a superset of all language editions, leading many to prefer it even when their primary language is not English. Other language editions, however, comprise complementary facts rooted in their respective cultures and media environments, which are marginalized in English Wikipedia. While Wikipedia's user interface enables switching between language editions through its Interlanguage Link (ILL) system, it does not reveal to readers that other language editions contain valuable, complementary information. We present WikiGap, a system that surfaces complementary facts sourced from other Wikipedias within the English Wikipedia interface. Specifically, by combining a recent multilingual information-gap discovery method with a user-centered design, WikiGap enables access to complementary information from French, Russian, and Chinese Wikipedia. In a mixed-methods study (n=21), WikiGap significantly improved fact-finding accuracy, reduced task time, and received a 32-point higher usability score relative to Wikipedia's current ILL-based navigation system. Participants reported increased awareness of the availability of complementary information in non-English editions and reconsidered the completeness of English Wikipedia. WikiGap thus paves the way for improved epistemic equity across language editions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)

*Cody Kommers, Drew Hemment, Maria Antoniak, Joel Z. Leibo, Hoyt Long, Emily Robinson, Adam Sobey*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Context, Thick Description, Human-Computer Interaction, Generative AI

**Relevance Score:** 8

**TL;DR:** This position paper discusses how large language models (LLMs) can facilitate the representation of human meaning in AI systems through 'thick description', overcoming the limitations of traditional numerical representations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To argue that LLMs can make cultural context and human meaning legible in AI-based sociotechnical systems by improving the representation of human activity.

**Method:** The paper compares thin descriptions (quantitative, standardized) with thick descriptions (qualitative, contextual) and suggests that LLMs can automate the generation of thick descriptions at scale.

**Key Contributions:**

	1. Introduces thick description as a method for better capturing human meaning in AI systems
	2. Identifies key challenges in representing culture with LLMs
	3. Frames the importance of qualitative representation in the context of AI

**Result:** LLMs have the potential to address challenges in representing human meaning by accommodating cultural context, interpretive pluralism, and dynamic understanding.

**Limitations:** 

**Conclusion:** Adopting thick description as a framework can help address emerging issues in representing culture with LLMs and suggests it as a crucial direction for generative AI applications.

**Abstract:** This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs.

</details>


### [23] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)

*Aakash Sen Sharma, Debdeep Sanyal, Priyansh Srivastava, Sundar Atreya H., Shirish Karande, Mohan Kankanhalli, Murari Mandal*

**Main category:** cs.CL

**Keywords:** large language models, fair use, copyright, Direct Preference Optimization, language model evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents FUA-LLM, a framework for aligning LLM outputs with fair use doctrine, utilizing a dataset called FairUseDB and introducing new evaluation metrics.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address copyright infringement risks in large language models by developing a framework that ensures outputs are fair use compliant and practically useful.

**Method:** The paper collaborates with intellectual property experts to create FUA-LLM, leveraging FairUseDB and employing Direct Preference Optimization (DPO) to fine-tune LLMs.

**Key Contributions:**

	1. Development of FUA-LLM framework
	2. Introduction of FairUseDB dataset
	3. New evaluation metrics for assessing infringement risk

**Result:** FUA-LLM demonstrates a reduction in problematic outputs by up to 20% compared to existing methods while maintaining usability.

**Limitations:** 

**Conclusion:** The proposed framework significantly improves the compliance of LLM outputs with copyright laws and enhances their practical utility.

**Abstract:** Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\%) compared to state-of-the-art approaches, while preserving real-world usability.

</details>


### [24] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)

*Mingyu Huang, Shasha Zhou, Yuxuan Chen, Ke Li*

**Main category:** cs.CL

**Keywords:** literature review, large language models, data-driven insights, AI4Health, knowledge graphs

**Relevance Score:** 8

**TL;DR:** LitChat is an interactive literature agent that enhances LLMs to improve literature exploration and comprehension.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by the increasing volume of digital scientific publications and the limitations of traditional review methods.

**Method:** Developed an interactive literature agent that interprets user queries, retrieves sources, constructs knowledge graphs, and uses data-mining techniques for insights.

**Key Contributions:**

	1. Introduction of LitChat as a conversational literature agent
	2. Integration of knowledge graphs for enhanced literature retrieval
	3. Use of data-mining techniques to generate insights from large data sets

**Result:** Demonstrated effectiveness through a case study on AI4Health, showing its ability to navigate large literature landscapes effectively.

**Limitations:** Limited by the inherent challenges of LLMs, such as hallucinations and context limitations.

**Conclusion:** LitChat offers a comprehensive, data-driven approach that can outperform traditional literature review methods.

**Abstract:** We are living in an era of "big literature", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering "comprehensive, objective, open and transparent" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.

</details>


### [25] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)

*Shaojie Wang, Sirui Ding, Na Zou*

**Main category:** cs.CL

**Keywords:** large language models, mutual information, evaluation framework

**Relevance Score:** 9

**TL;DR:** This paper proposes an information-theoretic framework to evaluate the linguistic understanding of large language models (LLMs) by measuring mutual information (MI) between input sentences and their latent representations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the intrinsic linguistic understanding of LLMs beyond specialized tasks, using a framework grounded in mutual information.

**Method:** The authors formalize linguistic understanding as mutual information between an input sentence and its latent representation, and derive a computable lower bound for token-level MI using Fano's inequality.

**Key Contributions:**

	1. Proposes a mutual information-based framework to evaluate LLM understanding
	2. Establishes theoretical bounds connecting token-level and sentence-level MI
	3. Demonstrates the effectiveness of fine-tuning on token-level recoverability

**Result:** Encoder-only models show higher information fidelity than decoder-only models, which demonstrate a forgetting pattern in later layers. Fine-tuning to maximize token-level recoverability improves understanding of LLMs across generic tasks.

**Limitations:** 

**Conclusion:** Mutual information serves as a foundational measure for understanding and enhancing LLM capabilities without task-specific supervision.

**Abstract:** Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer "forgetting" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities.

</details>


### [26] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)

*Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Reinforcement Learning, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces R3-RAG, a novel approach that combines reinforcement learning with retrieval-augmented generation to enhance reasoning and retrieval capabilities in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing retrieval-augmented generation methods face limitations in retrieval effectiveness due to the bottleneck posed by dense retrievers and the constraints of human-designed workflows.

**Method:** R3-RAG employs a two-stage approach: initially using cold start for iterative reasoning and retrieval, followed by reinforcement learning to improve exploration of the retrieval environment. Two reward mechanisms are introduced to guide the learning process: outcome rewards for correct answers and relevance-based process rewards for document retrieval.

**Key Contributions:**

	1. Introduction of R3-RAG framework leveraging reinforcement learning for better reasoning and retrieval integration.
	2. Two novel reward mechanisms for guiding model learning in reasoning and retrieval tasks.
	3. Demonstration of significant performance improvements over existing RAG systems.

**Result:** Experimental results demonstrate that R3-RAG significantly outperforms traditional baseline methods and shows a strong ability to adapt across different retrieval systems.

**Limitations:** 

**Conclusion:** R3-RAG represents a significant advance in integrating reasoning with retrieval in RAG systems, improving the factual correctness and reliability of outputs from large language models.

**Abstract:** Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\textbf{R3-RAG}$, which uses $\textbf{R}$einforcement learning to make the LLM learn how to $\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [27] [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/abs/2505.23796)

*Christopher Barrie, Petter Törnberg*

**Main category:** cs.CL

**Keywords:** large language models, emergent conventions, naming game, data leakage, linguistic behaviors

**Relevance Score:** 8

**TL;DR:** This paper challenges the notion that large language models (LLMs) develop emergent conventions in linguistic tasks by arguing that their behavior is a result of data leakage and memorization from their training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To critique the findings of Ashery et al. regarding LLMs developing linguistic conventions through a naming game, and to provide a clearer understanding of the mechanisms behind the observed behaviors.

**Method:** The authors conduct multiple analyses to evaluate whether the LLMs’ behaviors in a naming game reflect emergent conventions or simply memorization of training data.

**Key Contributions:**

	1. Critique of the emergent convention theory for LLMs in linguistic tasks.
	2. Evidence showing LLM behaviors are due to memorization rather than emergent learning.
	3. Suggestions for alternative strategies in LLM behavior research.

**Result:** The analyses demonstrate that LLMs recognize the structure of the coordination game and recall its outcomes, which are indistinguishable from mere memorization rather than the emergence of new conventions.

**Limitations:** The paper primarily challenges existing interpretations without proposing extensive new methodologies for investigating LLMs in social science contexts.

**Conclusion:** The paper concludes by discussing alternative strategies for studying LLM behavior and reflecting on their implications for social science modeling.

**Abstract:** Ashery et al. recently argue that large language models (LLMs), when paired to play a classic "naming game," spontaneously develop linguistic conventions reminiscent of human social norms. Here, we show that their results are better explained by data leakage: the models simply reproduce conventions they already encountered during pre-training. Despite the authors' mitigation measures, we provide multiple analyses demonstrating that the LLMs recognize the structure of the coordination game and recall its outcomes, rather than exhibit "emergent" conventions. Consequently, the observed behaviors are indistinguishable from memorization of the training corpus. We conclude by pointing to potential alternative strategies and reflecting more generally on the place of LLMs for social science models.

</details>


### [28] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)

*Zaihan Yang, Ryan Leonard, Hien Tran, Rory Driscoll, Chadbourne Davis*

**Main category:** cs.CL

**Keywords:** suicide risk assessment, machine learning, natural language processing, Reddit, hybrid model

**Relevance Score:** 8

**TL;DR:** Development of a hybrid machine learning model to classify suicide risk from Reddit posts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable early detection of suicidal risk given the societal concern around suicidal thoughts and behaviors.

**Method:** A RoBERTa-TF-IDF-PCA Hybrid model was developed for multi-class classification of suicide risk severity using various data resampling techniques and data augmentation strategies.

**Key Contributions:**

	1. Development of a novel hybrid machine learning model
	2. Integration of RoBERTa embeddings with TF-IDF and PCA
	3. Exploration of data resampling and augmentation techniques for better generalization

**Result:** The hybrid model achieved a best weighted F1 score of 0.7512, outperforming traditional classifiers and RoBERTa alone.

**Limitations:** 

**Conclusion:** The proposed hybrid model enhances accuracy and reliability in assessing suicide risk from social media posts.

**Abstract:** Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.

</details>


### [29] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)

*Jian Lan, Yifei Fu, Udo Schlegel, Gengyuan Zhang, Tanveer Hannan, Haokun Chen, Thomas Seidl*

**Main category:** cs.CL

**Keywords:** vision-language models, social bias, fairness, post-hoc mitigation, hidden layers

**Relevance Score:** 8

**TL;DR:** The paper evaluates and mitigates social bias in vision-language models (VLMs) using a novel post-hoc method during inference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address social bias issues in large vision-language models that harm certain societal groups.

**Method:** Evaluated four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets and proposed a post-hoc method to mitigate biases by modifying hidden layer residuals without retraining the model.

**Key Contributions:**

	1. Evaluation of VLMs for social biases in generative responses
	2. Proposal of a training-free, model-agnostic post-hoc bias mitigation method
	3. Demonstration of improved fairness and reliability in model outputs

**Result:** Found that VLMs produce gender and race-biased responses while they misrepresent their fairness confidence; the proposed method improved response fairness and reliability.

**Limitations:** The study is limited to the evaluation of specific datasets; further research is needed to generalize findings across more contexts.

**Conclusion:** The proposed post-hoc method effectively reduces social bias in VLMs compared to traditional training strategies and improves the calibration of model responses.

**Abstract:** Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.

</details>


### [30] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)

*Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer*

**Main category:** cs.CL

**Keywords:** language models, consistency, user study, hallucinations, human evaluation

**Relevance Score:** 9

**TL;DR:** This paper investigates methods for measuring the consistency of large language models (LLMs) and proposes a new logit-based ensemble method that aligns better with human perceptions of LLM consistency compared to existing metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of hallucinations and response fragility in LLMs, which affect the reliability of generated text, by finding effective ways to measure the consistency of LLM responses as perceived by users.

**Method:** A user study with 2,976 participants was conducted to evaluate different methods for measuring LLM consistency, focusing on their alignment with human perceptions. An ensemble method based on logits was proposed for this purpose.

**Key Contributions:**

	1. Proposed a logit-based ensemble method for consistency estimation
	2. Demonstrated that existing methods often misalign with human perceptions
	3. Emphasized the need for human evaluation in assessing LLM consistency

**Result:** The proposed logit-based ensemble method for estimating LLM consistency performed comparably to the best existing metrics in correlating with human ratings of consistency, highlighting shortcomings in past approaches that did not involve human evaluation.

**Limitations:** The study only included a single user study and may not represent all contexts in which LLMs are used.

**Conclusion:** The study recommends incorporating human evaluations for estimating LLM consistency, given the inadequacies of current automated methods to reflect user perceptions.

**Abstract:** Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [31] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)

*Sajid Hussain, Muhammad Sohail, Nauman Ali Khan*

**Main category:** cs.CL

**Keywords:** Federated Learning, Natural Language Processing, Resource Efficiency, Client Selection, Semantic Heterogeneity

**Relevance Score:** 9

**TL;DR:** SEMFED is a novel federated learning framework that addresses challenges in NLP by improving client selection, model architecture, and communication efficiency.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unique challenges of applying federated learning in NLP tasks, including semantic heterogeneity and resource constraints.

**Method:** SEMFED features a semantic-aware client selection mechanism, adaptive NLP-specific model architectures, and a communication-efficient semantic feature compression technique.

**Key Contributions:**

	1. Semantic-aware client selection mechanism
	2. Adaptive NLP-specific model architectures
	3. Communication-efficient semantic feature compression technique

**Result:** SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98% across various NLP classification tasks.

**Limitations:** 

**Conclusion:** SEMFED is well-suited for heterogeneous client environments, making it effective for real-world federated NLP deployments.

**Abstract:** Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.

</details>


### [32] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)

*Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah*

**Main category:** cs.CL

**Keywords:** large language models, medical tasks, evaluation framework

**Relevance Score:** 9

**TL;DR:** MedHELM is a new evaluation framework for assessing large language models (LLMs) in medical tasks, featuring a clinician-validated taxonomy, a comprehensive benchmark suite, and innovative evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing evaluations for LLMs in reflecting the complexity of real-world clinical practices.

**Method:** Development of a taxonomy with contributions from 29 clinicians, creation of a benchmark suite with 35 tasks, and systematic evaluation of 9 LLMs using improved methods including an LLM-jury.

**Key Contributions:**

	1. Clinician-validated taxonomy for medical tasks
	2. Comprehensive benchmark suite covering diverse medical tasks
	3. LLM-jury evaluation method that aligns well with clinician assessments

**Result:** LLM evaluation revealed performance variations among 9 models, with deep reasoning models performing best but lower-cost models like Claude 3.5 Sonnet showing competitive results. The new evaluation method correlated well with clinician ratings and surpassed existing automated metrics.

**Limitations:** 

**Conclusion:** Real-world evaluations are crucial for assessing the application of LLMs in medicine, and the MedHELM framework is made available for future research and improvements in this area.

**Abstract:** While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.

</details>


### [33] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)

*Terrance Liu, Shuyi Wang, Daniel Preotiuc-Pietro, Yash Chandarana, Chirag Gupta*

**Main category:** cs.CL

**Keywords:** text-to-SQL, large language models, confidence calibration, Platt scaling, error detection

**Relevance Score:** 8

**TL;DR:** This paper addresses the reliability of uncertainty measures in text-to-SQL parsing using large language models and proposes a method that improves the calibration of confidence scores for these outputs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To build trustworthy text-to-SQL systems, it is essential to accurately gauge the reliability of outputs from large language models, which can sometimes fail confident but incorrectly.

**Method:** The paper introduces a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing and uses Platt scaling along with an innovative approach leveraging SQL query structures called 'sub-clause frequency' (SCF) scores, which are combined using multivariate Platt scaling (MPS).

**Key Contributions:**

	1. Establishment of a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing.
	2. Introduction of 'sub-clause frequency' (SCF) scores for improved calibration.
	3. Combination of individual SCF scores using multivariate Platt scaling (MPS) yields enhanced calibration and error detection.

**Result:** Empirical evaluation on popular text-to-SQL datasets indicates that the proposed method of combining MPS and SCF significantly improves both calibration accuracy and error detection compared to traditional Platt scaling.

**Limitations:** 

**Conclusion:** The study demonstrates that calibrated confidence scores enhance the reliability of outputs from large language models used for text-to-SQL parsing.

**Abstract:** While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.

</details>


### [34] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)

*Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer*

**Main category:** cs.CL

**Keywords:** large language models, LLMs, consistency estimation, human evaluation, hallucinations

**Relevance Score:** 8

**TL;DR:** This paper investigates the measurement of consistency in LLM responses through a user study and proposes a logit-based ensemble method for a more accurate estimation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations and fragility in large language models (LLMs), which affect the reliability of generated text.

**Method:** A user study with 2,976 participants to evaluate existing methods for measuring LLM consistency and the introduction of a logit-based ensemble method to improve accuracy.

**Key Contributions:**

	1. User study demonstrating user perceptions of LLM consistency
	2. Logit-based ensemble method for better consistency estimation
	3. Call for broader use of human evaluation in measuring LLM consistency

**Result:** The proposed method matches the performance of the best existing metric for estimating human ratings of LLM consistency, highlighting the inadequacies of previous approaches.

**Limitations:** 

**Conclusion:** Current methods often fail to align with human perceptions of LLM consistency, suggesting a need for integrating human evaluations in consistency estimation.

**Abstract:** Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [35] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)

*Sihyeon Lee, Hyunjoo Song, Jong-chan Lee, Yoon Jin Lee, Boram Lee, Hee-Eon Lim, Dongyeong Kim, Jinwook Seo, Bohyoung Kim*

**Main category:** cs.CL

**Keywords:** large language models, clinical settings, privacy-preserving, hybrid framework, MedOrchestra

**Relevance Score:** 9

**TL;DR:** MedOrchestra is a hybrid framework combining cloud and local LLMs for improved clinical task execution while preserving data privacy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-offs between performance and data privacy in deploying LLMs in clinical settings.

**Method:** A hybrid framework where a cloud LLM decomposes complex tasks and generates prompts, and a local LLM executes these tasks privately.

**Key Contributions:**

	1. Introduction of a hybrid LLM framework for clinical tasks
	2. Demonstrated superiority over traditional models and clinician performance
	3. Validation using real clinical guidelines and test cases

**Result:** MedOrchestra achieves 70.21% accuracy on free-text reports and 85.42% on structured reports, outperforming local baselines and clinicians.

**Limitations:** 

**Conclusion:** The hybrid approach significantly improves accuracy for clinical tasks while maintaining privacy.

**Abstract:** Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings.

</details>


### [36] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)

*Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh*

**Main category:** cs.CL

**Keywords:** harmful prompts, large language models, multimodal detection

**Relevance Score:** 9

**TL;DR:** OMNIGUARD is a novel approach for detecting harmful prompts in large language models across multiple languages and modalities, showing substantial improvements in classification accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fallibility of current detection approaches for harmful prompts in LLMs, particularly in low-resource languages and non-text modalities like images and audio.

**Method:** OMNIGUARD identifies internal representations of language models that are aligned across languages and modalities to build a classifier that is agnostic to both.

**Key Contributions:**

	1. Introduces a language-agnostic classifier for harmful prompts.
	2. Achieves state-of-the-art results in audio-based prompt detection.
	3. Demonstrates high efficiency, being much faster than existing methods.

**Result:** OMNIGUARD improves harmful prompt classification accuracy by 11.57% in multilingual settings, 20.44% for image prompts, and sets a new state-of-the-art for audio prompts, while being approximately 120 times faster than its closest competitor.

**Limitations:** 

**Conclusion:** OMNIGUARD significantly enhances the detection of harmful prompts across various modalities while maintaining high efficiency, making it a promising tool for addressing LLM safety concerns.

**Abstract:** The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [37] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)

*Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang*

**Main category:** cs.CL

**Keywords:** Dynamic Layerwise Pruning, Large Language Models, Model Compression, Sparsity, Performance Preservation

**Relevance Score:** 9

**TL;DR:** Dynamic Layerwise Pruning (DLP) adapts effective pruning rates for each layer in LLMs to improve performance and reduce inference inefficiencies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing pruning methods often lead to performance degradation in LLMs due to uniform layerwise strategies. A need for adaptive approaches that can preserve model performance while achieving high sparsity motivates the development of DLP.

**Method:** DLP determines the relative importance of each layer by integrating model weights and input activation information to assign dynamic pruning rates.

**Key Contributions:**

	1. Introduction of Dynamic Layerwise Pruning (DLP) for adaptive layer importance assessment.
	2. Demonstrated improved performance metrics at high sparsity levels vs. traditional methods.
	3. Compatibility with existing Parameter-Efficient Fine-Tuning (PEFT) techniques.

**Result:** At 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves average accuracy by 2.7% compared to state-of-the-art pruning techniques.

**Limitations:** 

**Conclusion:** DLP effectively maintains model performance at high sparsity levels and is compatible with various LLM compression techniques. The code is available for further research.

**Abstract:** Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [38] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)

*Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang*

**Main category:** cs.CL

**Keywords:** DenseLowRA, LLM, parameter efficiency, large language models, fine-tuning

**Relevance Score:** 8

**TL;DR:** Dense Low-Rank Adaptation (DenseLoRA) improves the efficiency and performance of adapting large language models by utilizing a single dense low-rank matrix instead of two redundant ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Prior research on Low-rank adaptation (LoRA) reveals inefficiencies due to redundancy in parameter use, prompting the need for a more effective adaptation approach.

**Method:** DenseLoRA utilizes a single Encoder-Decoder to refine and compress hidden representations across all layers, adapting LLMs with a dense low-rank matrix.

**Key Contributions:**

	1. Introduction of DenseLoRA for improved parameter efficiency
	2. Demonstrates higher accuracy with fewer trainable parameters compared to LoRA
	3. Extensive evaluation of DenseLoRA's components and their impact on performance

**Result:** DenseLoRA achieves 83.8% accuracy with only 0.01% of trainable parameters, outperforming LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B.

**Limitations:** 

**Conclusion:** DenseLoRA offers superior parameter efficiency and performance improvements over traditional LoRA, making it a promising method for large language model adaptation.

**Abstract:** Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.

</details>


### [39] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)

*Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao*

**Main category:** cs.CL

**Keywords:** E-commerce, Marketing, LLM, Conversion, Personalization

**Relevance Score:** 7

**TL;DR:** Proposes a framework utilizing LLMs for creating engaging and conversion-driven marketing copy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Increased competition in e-commerce necessitates effective marketing content that balances creativity and conversion rates.

**Method:** Integrates prompt engineering, multi-objective fine-tuning (including sentiment adjustment and diversity enhancement), and post-processing for marketing copy generation.

**Key Contributions:**

	1. Framework for automated marketing copy generation using LLMs
	2. Integration of multi-objective fine-tuning techniques
	3. Demonstrated effectiveness through empirical testing

**Result:** Achieves 12.5% increase in CTR and 8.3% increase in CVR through offline evaluations and online A/B tests.

**Limitations:** 

**Conclusion:** Offers a practical automated solution for copy generation and directions for future research in real-time personalization.

**Abstract:** As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization.

</details>


### [40] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)

*Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, dialogue applications, multi-turn conversations

**Relevance Score:** 9

**TL;DR:** MARS-Bench is a benchmark designed to evaluate the robustness of LLMs in complex multi-turn dialogues, revealing significant performance differences between closed-source and open-source models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmarks that reflect the weaknesses of LLMs in handling long complex dialogue sessions, including motivation transfer and cross-turn dependency.

**Method:** MARS-Bench is constructed from play-by-play text commentary and focuses on three aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments are conducted to assess the performance of different LLMs on this benchmark.

**Key Contributions:**

	1. Introduces MARS-Bench, a new benchmark for evaluating LLM multi-turn dialogue capabilities.
	2. Reveals that closed-source models significantly outperform open-source alternatives in dialogue tasks.
	3. Demonstrates the importance of explicit reasoning for LLM robustness in complex conversations.

**Result:** The experiments show that closed-source LLMs outperform open-source ones, and that explicit reasoning improves robustness in handling complex dialogue. Challenges in motivation transfer and cross-turn dependency were identified.

**Limitations:** Benchmarks are derived from specific dialogue scenarios and may not cover all potential dialogue complexities.

**Conclusion:** MARS-Bench provides insights into LLMs' performance issues in multi-turn dialogues and illustrates how special token attention leads to performance degradation in handling long dialogues.

**Abstract:** Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic \textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.

</details>


### [41] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)

*Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Layer-wise training quality, Influence Functions, task specialization, LLM pruning

**Relevance Score:** 9

**TL;DR:** This paper presents LayerIF, a data-driven framework that evaluates layer-wise training quality in pretrained Large Language Models (LLMs) using Influence Functions, enhancing task performance in downstream applications like expert allocation and layer-wise sparsity distribution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing methods that ignore data influence on layer-wise training quality in pretrained LLMs, leading to suboptimal performance in downstream tasks.

**Method:** LayerIF uses Influence Functions to quantify training quality per layer by analyzing gradients and validation loss sensitivity to training examples, producing task-specific importance estimates for each layer.

**Key Contributions:**

	1. Introduction of LayerIF framework for layer-wise training quality evaluation
	2. Task-specific layer importance estimates for pretrained LLMs
	3. Demonstrated practical applications in expert allocation and LLM pruning

**Result:** LayerIF provides a model-agnostic approach that leads to improved task performance in applications such as expert allocation in LoRA-MoE architectures and layer-wise sparsity distribution for LLM pruning.

**Limitations:** 

**Conclusion:** The paper demonstrates that using LayerIF yields consistent performance gains in downstream tasks by providing nuanced insights into layer specialization across different evaluations.

**Abstract:** Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance.It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.

</details>


### [42] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)

*Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim*

**Main category:** cs.CL

**Keywords:** Large Language Models, theory-of-mind, human-AI interaction, negotiation, cooperative decision-making

**Relevance Score:** 9

**TL;DR:** This study explores the impact of theory-of-mind reasoning in Large Language Models (LLMs) on negotiation tasks using the ultimatum game.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how theory-of-mind reasoning influences AI behavior in social interactions, particularly in negotiation scenarios.

**Method:** The study involved initializing LLM agents with various prosocial beliefs and reasoning methods, conducting 2,700 simulations to analyze their decisions in negotiation tasks.

**Key Contributions:**

	1. Investigated role of ToM reasoning in AI negotiations
	2. Demonstrated enhanced alignment of AI with human norms
	3. Provided empirical data from extensive simulations

**Result:** The results indicated that theory-of-mind reasoning improves behavior alignment with human norms and enhances decision-making consistency and negotiation success.

**Limitations:** 

**Conclusion:** The findings highlight the importance of incorporating theory-of-mind reasoning in AI to foster better human-AI interactions and facilitate cooperative decision-making.

**Abstract:** Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [43] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)

*Lata Pangtey, Mohammad Zia Ur Rehman, Prasad Chaudhari, Shubhi Bansal, Nagendra Kumar*

**Main category:** cs.CL

**Keywords:** stance detection, misinformation, emotional awareness, social media, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents SPLAENet, a novel method for stance prediction that leverages dual cross-attention and emotional awareness to improve stance detection in misinformative social media content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance stance detection in the context of misinformation on social media, which is crucial for analyzing biases in user-generated content.

**Method:** The proposed method, SPLAENet, uses a dual cross-attention mechanism and a hierarchical attention network, along with emotional context and distance-metric learning for label fusion.

**Key Contributions:**

	1. Introduction of dual cross-attention for improved context understanding
	2. Integration of emotional awareness in stance detection
	3. Distance-metric learning for enhanced label fusion

**Result:** SPLAENet shows an average gain of 8.92% accuracy and 17.36% F1-score on the RumourEval dataset, 7.02% accuracy and 10.92% F1-score on the SemEval dataset, and 10.03% accuracy and 11.18% F1-score on the P-stance dataset compared to existing methods.

**Limitations:** 

**Conclusion:** The results confirm that SPLAENet is an effective tool for stance detection in misinformative social media content.

**Abstract:** The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and 17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and 11.18\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.

</details>


### [44] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)

*Stéphane Aroca-Ouellette, Natalie Mackraz, Barry-John Theobald, Katherine Metcalf*

**Main category:** cs.CL

**Keywords:** Preference Inference, Large Language Models, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper presents PROSE, a method for improving the accuracy of inferred user preferences from writing samples for LLMs. By refining and verifying preferences, PROSE enhances the performance of writing agents compared to existing methods, demonstrating significant improvements in personalization for tasks such as summarization and email writing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of existing methods that produce generic preference descriptions, thus failing to capture the unique nature of human preferences in LLM interactions.

**Method:** PROSE enhances preference description inference through iterative refinement and cross-sample verification of user writing samples, tested on multiple LLMs for writing tasks.

**Key Contributions:**

	1. Introduction of PROSE for preference description inference
	2. Demonstrated 33% improvement over existing methods
	3. Combination with ICL for enhanced performance

**Result:** PROSE improves the accuracy of inferred preferences and the quality of LLM-generated texts by 33% over the previous state-of-the-art method, CIPHER, and shows synergistic effects when combined with ICL.

**Limitations:** 

**Conclusion:** The findings suggest that tailored preference inference methods like PROSE can significantly enhance the personalization of LLM outputs, making interactions more effective.

**Abstract:** Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\% improvement over ICL alone.

</details>


### [45] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)

*Trenton Chang, Tobias Schnabel, Adith Swaminathan, Jenna Wiens*

**Main category:** cs.CL

**Keywords:** large language models, steerability, text rewriting, prompt engineering, reinforcement learning

**Relevance Score:** 9

**TL;DR:** This paper evaluates the challenges of steerability in LLMs, revealing persistent issues with side effects and effectiveness of interventions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether current large language models (LLMs) can reliably align with diverse user goals, highlighting issues of steerability.

**Method:** A framework based on a multi-dimensional goal space is introduced to systematically evaluate LLM outputs against user goals, particularly in a text-rewriting task.

**Key Contributions:**

	1. Introduction of a framework for evaluating LLM steerability
	2. Identification of persistent side effects in LLM outputs
	3. Assessment of various interventions to improve steerability and their effectiveness.

**Result:** The study finds that LLMs have persistent issues with steerability, notably side effects, and varying effectiveness of interventions such as prompt engineering and reinforcement learning fine-tuning.

**Limitations:** Focused on text-rewriting tasks; may not generalize to all text generation contexts.

**Conclusion:** Current LLMs struggle with steerability, and existing alignment strategies may not adequately address the identified problems.

**Abstract:** Despite advances in large language models (LLMs) on reasoning and instruction-following benchmarks, it remains unclear whether they can reliably produce outputs aligned with a broad variety of user goals, a concept we refer to as steerability. The abundance of methods proposed to modify LLM behavior makes it unclear whether current LLMs are already steerable, or require further intervention. In particular, LLMs may exhibit (i) poor coverage, where rare user goals are underrepresented; (ii) miscalibration, where models overshoot requests; and (iii) side effects, where changes to one dimension of text inadvertently affect others. To systematically evaluate these failures, we introduce a framework based on a multi-dimensional goal space that models user goals and LLM outputs as vectors with dimensions corresponding to text attributes (e.g., reading difficulty). Applied to a text-rewriting task, we find that current LLMs struggle with steerability, as side effects are persistent. Interventions to improve steerability, such as prompt engineering, best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying effectiveness, yet side effects remain problematic. Our findings suggest that even strong LLMs struggle with steerability, and existing alignment strategies may be insufficient. We open-source our steerability evaluation framework at https://github.com/MLD3/steerability.

</details>


### [46] [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/abs/2505.23818)

*Masoud Safilian, Amin Beheshti, Stephen Elbourn*

**Main category:** cs.CL

**Keywords:** automated grading, generative AI, rubric-based assessment, educational technology, machine learning

**Relevance Score:** 7

**TL;DR:** RATAS is a framework for rubric-based automated grading using generative AI, designed to be subject-agnostic and provide interpretable feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automated grading processes that often lack interpretability and applicability across various subjects and assessment types.

**Method:** RATAS employs a mathematical framework and architecture designed for rubric-based assessment, utilizing a contextualized dataset derived from real-world project-based courses to evaluate its effectiveness.

**Key Contributions:**

	1. Introduction of RATAS, a novel rubric-based grading framework using generative AI.
	2. Support for a wide range of grading rubrics and diverse response formats.
	3. Provision of structured and explainable rationales for assigned scores.

**Result:** RATAS demonstrated high reliability and accuracy in automated grading, alongside providing interpretable feedback, enhancing transparency for students and instructors.

**Limitations:** The performance may vary depending on the quality of the grading rubrics and the complexity of the responses.

**Conclusion:** The framework addresses existing limitations in automated grading systems and offers a robust solution for grading textual responses across diverse contexts.

**Abstract:** Automated answer grading is a critical challenge in educational technology, with the potential to streamline assessment processes, ensure grading consistency, and provide timely feedback to students. However, existing approaches are often constrained to specific exam formats, lack interpretability in score assignment, and struggle with real-world applicability across diverse subjects and assessment types. To address these limitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a novel framework that leverages state-of-the-art generative AI models for rubric-based grading of textual responses. RATAS is designed to support a wide range of grading rubrics, enable subject-agnostic evaluation, and generate structured, explainable rationales for assigned scores. We formalize the automatic grading task through a mathematical framework tailored to rubric-based assessment and present an architecture capable of handling complex, real-world exam structures. To rigorously evaluate our approach, we construct a unique, contextualized dataset derived from real-world project-based courses, encompassing diverse response formats and varying levels of complexity. Empirical results demonstrate that RATAS achieves high reliability and accuracy in automated grading while providing interpretable feedback that enhances transparency for both students and nstructors.

</details>


### [47] [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/abs/2505.24803)

*Zhijun Pan, Antonios Andronis, Eva Hayek, Oscar AP Wilkinson, Ilya Lasy, Annette Parry, Guy Gadney, Tim J. Smith, Mick Grierson*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Storytelling, User Control, Narrative Quality

**Relevance Score:** 9

**TL;DR:** This paper explores how knowledge graphs (KGs) can improve long-form storytelling generated by large language models (LLMs) by enhancing narrative quality and user control.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the advances of LLMs in automated storytelling, maintaining coherence and user control during long-form narrative generation remains a challenge. The paper seeks to leverage knowledge graphs to address these issues in storytelling contexts.

**Method:** We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study involving 15 participants who created story prompts, generated stories, and edited knowledge graphs.

**Key Contributions:**

	1. Development of a KG-assisted storytelling pipeline
	2. Demonstration of improved narrative quality using KGs
	3. Enhanced user control and engagement in storytelling through KG editing

**Result:** The study's findings indicate that using knowledge graphs significantly improves the quality of stories, particularly in structured narratives, and enhances users' sense of control while making storytelling more engaging and interactive.

**Limitations:** 

**Conclusion:** Knowledge graphs provide an effective means of enhancing narrative generation with LLMs, enabling improved story coherence and user-driven customization.

**Abstract:** Large Language Models (LLMs) have shown great potential in automated story generation, but challenges remain in maintaining long-form coherence and providing users with intuitive and effective control. Retrieval-Augmented Generation (RAG) has proven effective in reducing hallucinations in text generation; however, the use of structured data to support generative storytelling remains underexplored. This paper investigates how knowledge graphs (KGs) can enhance LLM-based storytelling by improving narrative quality and enabling user-driven modifications. We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants. Participants created their own story prompts, generated stories, and edited knowledge graphs to shape their narratives. Through quantitative and qualitative analysis, our findings demonstrate that knowledge graphs significantly enhance story quality in action-oriented and structured narratives within our system settings. Additionally, editing the knowledge graph increases users' sense of control, making storytelling more engaging, interactive, and playful.

</details>


### [48] [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/abs/2505.23820)

*Bhaktipriya Radharapu, Manon Revel, Megan Ung, Sebastian Ruder, Adina Williams*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias, alignment, human judgment, ambivalent scenarios

**Relevance Score:** 8

**TL;DR:** This study examines the limitations of Large Language Models (LLMs) in replicating human judgments and preferences in ambiguous situations. It identifies biases when LLMs act as answer generators, judges, and debaters, suggesting the need for better alignment methods without human oversight.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ability of LLMs to replicate human judgments in ambivalent scenarios, given the increasing reliance on LLMs for alignment tasks.

**Method:** The study develops a 'no-consensus' benchmark with curated examples of ambivalent scenarios, testing LLM performance in roles such as answer generator, judge, and debater.

**Key Contributions:**

	1. Identification of biases in LLMs when acting in different roles (answer generator, judge, debater)
	2. Development of a 'no-consensus' benchmark for evaluating LLM performance in ambiguity
	3. Insights into the limitations of LLMs in replicating human preferences and judgments.

**Result:** LLMs provide nuanced assessments in open-ended generation but tend to assert a stance on ambivalent topics when acting as judges or debaters, revealing their inherent biases.

**Limitations:** The study is limited to the performance of LLMs in specific ambivalent scenarios, which may not generalize to all contexts.

**Conclusion:** LLMs cannot fully capture human disagreement, emphasizing the need for improved alignment methods without human oversight in ambiguous scenarios.

**Abstract:** The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has raised questions about their ability to replicate human judgments and preferences, especially in ambivalent scenarios where humans disagree. This study examines the biases and limitations of LLMs in three roles: answer generator, judge, and debater. These roles loosely correspond to previously described alignment frameworks: preference alignment (judge) and scalable oversight (debater), with the answer generator reflecting the typical setting with user interactions. We develop a ``no-consensus'' benchmark by curating examples that encompass a variety of a priori ambivalent scenarios, each presenting two possible stances. Our results show that while LLMs can provide nuanced assessments when generating open-ended answers, they tend to take a stance on no-consensus topics when employed as judges or debaters. These findings underscore the necessity for more sophisticated methods for aligning LLMs without human oversight, highlighting that LLMs cannot fully capture human disagreement even on topics where humans themselves are divided.

</details>


### [49] [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)

*Mai Ali, Christopher Lucasius, Tanmay P. Patel, Madison Aitken, Jacob Vorstman, Peter Szatmari, Marco Battaglia, Deepa Kundur*

**Main category:** cs.CL

**Keywords:** Mental Health, Depression Detection, Multi-task Learning, Speech Data, Longitudinal Analysis

**Relevance Score:** 9

**TL;DR:** This study proposes using a trimodal multimedia approach for depression detection through speech data, integrating text, acoustic features, and vocal biomarkers with multi-task learning for improved prediction.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance depression detection using speech data by treating it as a trimodal multimedia resource, thereby improving prediction accuracy in adolescents.

**Method:** The approach integrates speech-derived text, acoustic landmarks, and vocal biomarkers, using multi-task learning to predict depression, suicidal ideation, and sleep disturbances simultaneously, with a longitudinal analysis of temporal changes.

**Key Contributions:**

	1. Introduction of a trimodal multimedia approach for depression detection.
	2. Application of multi-task learning to simultaneously predict related mental health challenges.
	3. Longitudinal analysis of speech data to track changes over time.

**Result:** Achieved a balanced accuracy of 70.8% on the Depression Early Warning dataset, outperforming unimodal and single-task methods.

**Limitations:** 

**Conclusion:** The trimodal, longitudinal multi-task learning approach shows promise for better understanding and predicting depression-related conditions over time.

**Abstract:** Speech is a noninvasive digital phenotype that can offer valuable insights into mental health conditions, but it is often treated as a single modality. In contrast, we propose the treatment of patient speech data as a trimodal multimedia data source for depression detection. This study explores the potential of large language model-based architectures for speech-based depression prediction in a multimodal regime that integrates speech-derived text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents a significant challenge and is often comorbid with multiple disorders, such as suicidal ideation and sleep disturbances. This presents an additional opportunity to integrate multi-task learning (MTL) into our study by simultaneously predicting depression, suicidal ideation, and sleep disturbances using the multimodal formulation. We also propose a longitudinal analysis strategy that models temporal changes across multiple clinical interactions, allowing for a comprehensive understanding of the conditions' progression. Our proposed approach, featuring trimodal, longitudinal MTL is evaluated on the Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%, which is higher than each of the unimodal, single-task, and non-longitudinal methods.

</details>


### [50] [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/abs/2505.23823)

*Youngseung Jeon, Ziwen Li, Thomas Li, JiaSyuan Chang, Morteza Ziyadi, Xiang 'Anthony' Chen*

**Main category:** cs.CL

**Keywords:** Protein-Protein Interactions, Drug Discovery, Large Language Models, Retrieval-Augmented Generation, Benchmark Datasets

**Relevance Score:** 9

**TL;DR:** The paper introduces the RAG Benchmark for protein-protein interactions (PPIs) to aid in drug development by providing a factual question-answer benchmark.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying the biological impacts of protein-protein interactions is crucial for target identification in drug development, but the lack of a benchmark makes this process difficult.

**Method:** The authors developed the RAGPPI, a benchmark dataset consisting of 4,420 question-answer pairs focusing on biological impacts of PPIs, validated through expert interviews and data annotation.

**Key Contributions:**

	1. Introduction of RAGPPI benchmark for PPIs
	2. Creation of gold-standard and silver-standard datasets
	3. Development of ensemble auto-evaluation LLM for QA evaluation

**Result:** A gold-standard dataset of 500 QA pairs and a silver-standard dataset of 3,720 QA pairs were created, alongside an ensemble auto-evaluation LLM to reflect expert labeling characteristics.

**Limitations:** 

**Conclusion:** RAGPPI will serve as a vital resource to support advancements in retrieval-augmented generation systems for drug discovery and related QA solutions.

**Abstract:** Retrieving the biological impacts of protein-protein interactions (PPIs) is essential for target identification (Target ID) in drug development. Given the vast number of proteins involved, this process remains time-consuming and challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) frameworks have supported Target ID; however, no benchmark currently exists for identifying the biological impacts of PPIs. To bridge this gap, we introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer benchmark of 4,420 question-answer pairs that focus on the potential biological impacts of PPIs. Through interviews with experts, we identified criteria for a benchmark dataset, such as a type of QA and source. We built a gold-standard dataset (500 QA pairs) through expert-driven data annotation. We developed an ensemble auto-evaluation LLM that reflected expert labeling characteristics, which facilitates the construction of a silver-standard dataset (3,720 QA pairs). We are committed to maintaining RAGPPI as a resource to support the research community in advancing RAG systems for drug discovery QA solutions.

</details>


### [51] [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)

*Tianmai M. Zhang, Neil F. Abernethy*

**Main category:** cs.CL

**Keywords:** large language models, peer review, scientific publication, manuscript quality, automated evaluation

**Relevance Score:** 9

**TL;DR:** This paper explores the use of large language models (LLMs) as manuscript quality checkers in the peer review process of scientific publications, contrasting AI-generated reviews with automated quality assessment methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of recruiting domain experts for manual evaluation in the peer review process of scientific publications by leveraging LLMs.

**Method:** The authors propose baseline approaches and an evaluation framework that utilizes leading reasoning LLMs as judges, employing withdrawn arXiv papers to validate the performance and cost of these models in identifying critical errors.

**Key Contributions:**

	1. Introduced an extendable automatic evaluation framework using LLMs as judges.
	2. Proposed several baseline approaches for LLMs in the context of manuscript quality assessment.
	3. Evaluated the performance and cost of multiple LLMs in identifying critical errors.

**Result:** The evaluation revealed that the OpenAI o3 model was the most effective in assessing manuscript quality, while the o4-mini model was noted for its cost-effectiveness in this context.

**Limitations:** The paper is a work in progress, and conclusions may be updated.

**Conclusion:** The work offers insights into document-based scientific reasoning and sets a foundation for future applications of LLMs in the peer review process.

**Abstract:** Recent advancements in large language models have sparked interest in utilizing them to assist the peer review process of scientific publication. Instead of having AI models generate reviews in the same way as human reviewers, we propose adopting them as manuscript quality checkers. We introduce several baseline approaches and an extendable automatic evaluation framework using top LLMs as judges to tackle the difficulty of recruiting domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we validated our proposed methods with several leading reasoning LLMs from different providers and assessed their performance and API costs for identifying critical errors and unsoundness problems. The OpenAI o3 model performed the best, while o4-mini was the most cost-effective one in our evaluation. This paper provides insights into document-based scientific understanding/reasoning and lays the foundation for future applications.

</details>


### [52] [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)

*Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, human values, ValueSim, Cognitive-Affective Personality System, individualized simulation

**Relevance Score:** 9

**TL;DR:** The paper presents ValueSim, a framework designed to simulate individualized human values by generating personal backstories that reflect users' experiences and demographics, improving alignment of LLMs with personal values.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for alignment of Large Language Models (LLMs) with individualized human value systems, rather than broad ethical considerations.

**Method:** ValueSim converts structured individual data into narrative backstories using a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values.

**Key Contributions:**

	1. Introduction of ValueSim framework for simulating individual human values
	2. Demonstration of improved accuracy over existing methods
	3. Showing adaptability and refinement capabilities of the model based on user interactions

**Result:** ValueSim improved top-1 accuracy by over 10% on a benchmark derived from the World Values Survey compared to retrieval-augmented generation methods.

**Limitations:** 

**Conclusion:** The model can refine its persona simulation capabilities over time as more user interaction history becomes available, enhancing its performance further.

**Abstract:** As Large Language Models (LLMs) continue to exhibit increasingly human-like capabilities, aligning them with human values has become critically important. Contemporary advanced techniques, such as prompt learning and reinforcement learning, are being deployed to better align LLMs with human values. However, while these approaches address broad ethical considerations and helpfulness, they rarely focus on simulating individualized human value systems. To address this gap, we present ValueSim, a framework that simulates individual values through the generation of personal backstories reflecting past experiences and demographic information. ValueSim converts structured individual data into narrative backstories and employs a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values based on these narratives. Testing ValueSim on a self-constructed benchmark derived from the World Values Survey demonstrates an improvement in top-1 accuracy by over 10% compared to retrieval-augmented generation methods. Further analysis reveals that performance enhances as additional user interaction history becomes available, indicating the model's ability to refine its persona simulation capabilities over time.

</details>


### [53] [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/abs/2505.23829)

*Xiaoqing Cheng, Ruizhe Chen, Hongying Zan, Yuxiang Jia, Min Peng*

**Main category:** cs.CL

**Keywords:** social bias, large language models, debiasing, fairness, real-time filtering

**Relevance Score:** 9

**TL;DR:** BiasFilter is a model-agnostic framework for real-time debiasing of LLM outputs without retraining, effectively mitigating social bias while maintaining quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to mitigate social bias in large language models while addressing the limitations of existing debiasing methods, which are often costly and ineffective.

**Method:** BiasFilter filters generation outputs in real-time by evaluating intermediate outputs, maintaining an active set of candidate continuations, and discarding low-reward segments using a fairness reward signal.

**Key Contributions:**

	1. Introduces BiasFilter, a model-agnostic framework for real-time debiasing of LLMs.
	2. Adopts a novel approach of filtering outputs based on a fairness reward signal.
	3. Constructs a fairness preference dataset and implicit reward model for assessing token-level fairness.

**Result:** BiasFilter effectively reduces social bias in various LLMs and maintains high generation quality through its innovative filtering approach.

**Limitations:** 

**Conclusion:** The proposed BiasFilter framework offers an efficient solution for real-time debiasing in LLMs without modifying model parameters or retraining, demonstrating significant improvements in fairness.

**Abstract:** Mitigating social bias in large language models (LLMs) has become an increasingly important research objective. However, existing debiasing methods often incur high human and computational costs, exhibit limited effectiveness, and struggle to scale to larger models and open-ended generation tasks. To address these limitations, this paper proposes BiasFilter, a model-agnostic, inference-time debiasing framework that integrates seamlessly with both open-source and API-based LLMs. Instead of relying on retraining with balanced data or modifying model parameters, BiasFilter enforces fairness by filtering generation outputs in real time. Specifically, it periodically evaluates intermediate outputs every few tokens, maintains an active set of candidate continuations, and incrementally completes generation by discarding low-reward segments based on a fairness reward signal. To support this process, we construct a fairness preference dataset and train an implicit reward model to assess token-level fairness in generated responses. Extensive experiments demonstrate that BiasFilter effectively mitigates social bias across a range of LLMs while preserving overall generation quality.

</details>


### [54] [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/abs/2505.23830)

*Linglin Jing, Yuting Gao, Zhigang Wang, Wang Lan, Yiwen Tang, Wenhai Wang, Kaipeng Zhang, Qingpei Guo*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, Multi-modal models, Machine Learning, Dynamic Routing, Expert Evolution

**Relevance Score:** 8

**TL;DR:** EvoMoE is a novel tuning framework that enhances multi-modal large language models (MLLMs) by addressing expert uniformity and router rigidity through expert evolution and a dynamic routing mechanism.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the performance of multi-modal large language models (MLLMs) that face challenges with expert uniformity and router rigidity in the Mixture of Experts (MoE) framework.

**Method:** The authors propose EvoMoE, which includes an expert initialization strategy that evolves multiple experts from a single trainable expert, and a Dynamic Token-aware Router (DTR) that allocates tokens to experts based on modality.

**Key Contributions:**

	1. Introduction of a new expert initialization strategy for diverse expert functions.
	2. Development of a Dynamic Token-aware Router for improved routing across modalities.
	3. Demonstrated superiority of EvoMoE in various multi-modal benchmarks.

**Result:** EvoMoE demonstrates significant performance improvements over existing sparse MLLMs on multiple multi-modal benchmarks, such as MME, MMBench, TextVQA, and POPE.

**Limitations:** 

**Conclusion:** The proposed EvoMoE framework effectively enhances MLLM performance by overcoming expert homogenization and inflexible routing mechanisms.

**Abstract:** Recent advancements have shown that the Mixture of Experts (MoE) approach significantly enhances the capacity of large language models (LLMs) and improves performance on downstream tasks. Building on these promising results, multi-modal large language models (MLLMs) have increasingly adopted MoE techniques. However, existing multi-modal MoE tuning methods typically face two key challenges: expert uniformity and router rigidity. Expert uniformity occurs because MoE experts are often initialized by simply replicating the FFN parameters from LLMs, leading to homogenized expert functions and weakening the intended diversification of the MoE architecture. Meanwhile, router rigidity stems from the prevalent use of static linear routers for expert selection, which fail to distinguish between visual and textual tokens, resulting in similar expert distributions for image and text. To address these limitations, we propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a meticulously designed expert initialization strategy that progressively evolves multiple robust experts from a single trainable expert, a process termed expert evolution that specifically targets severe expert homogenization. Furthermore, we introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism that allocates input tokens to appropriate experts based on their modality and intrinsic token values. This dynamic routing is facilitated by hypernetworks, which dynamically generate routing weights tailored for each individual token. Extensive experiments demonstrate that EvoMoE significantly outperforms other sparse MLLMs across a variety of multi-modal benchmarks, including MME, MMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE in enhancing the performance of MLLMs by addressing the critical issues of expert uniformity and router rigidity.

</details>


### [55] [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.23831)

*Wenhao Ye, Tiansheng Zheng, Yue Qi, Wenhua Zhao, Xiyu Wang, Xue Zhao, Jiacheng He, Yaya Zheng, Dongbo Wang*

**Main category:** cs.CL

**Keywords:** Intangible Cultural Heritage, Large Language Models, Digital Humanities

**Relevance Score:** 3

**TL;DR:** This study develops ICH-Qwen, a large language model aimed at preserving and disseminating China's intangible cultural heritage (ICH) using recent advancements in natural language processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The urgent need for preserving the intangible cultural heritage of China amidst modernization challenges, leveraging technology for protection and dissemination.

**Method:** Development of ICH-Qwen using a large corpus of open-source Chinese ICH data, integrating natural language understanding and knowledge reasoning with synthetic data and fine-tuning techniques.

**Key Contributions:**

	1. Introduction of ICH-Qwen, a specialized LLM for ICH
	2. Utilization of large-scale open-source data for model training
	3. Demonstration of model efficacy in ICH-specific applications

**Result:** ICH-Qwen has demonstrated efficacy in performing ICH domain-specific tasks, suggesting its potential as an intelligent solution for ICH protection and dissemination.

**Limitations:** 

**Conclusion:** The study posits that ICH-Qwen can facilitate the sustainable development of intangible cultural heritage and advance digital humanities research.

**Abstract:** The intangible cultural heritage (ICH) of China, a cultural asset transmitted across generations by various ethnic groups, serves as a significant testament to the evolution of human civilization and holds irreplaceable value for the preservation of historical lineage and the enhancement of cultural self-confidence. However, the rapid pace of modernization poses formidable challenges to ICH, including threats damage, disappearance and discontinuity of inheritance. China has the highest number of items on the UNESCO Intangible Cultural Heritage List, which is indicative of the nation's abundant cultural resources and emphasises the pressing need for ICH preservation. In recent years, the rapid advancements in large language modelling have provided a novel technological approach for the preservation and dissemination of ICH. This study utilises a substantial corpus of open-source Chinese ICH data to develop a large language model, ICH-Qwen, for the ICH domain. The model employs natural language understanding and knowledge reasoning capabilities of large language models, augmented with synthetic data and fine-tuning techniques. The experimental results demonstrate the efficacy of ICH-Qwen in executing tasks specific to the ICH domain. It is anticipated that the model will provide intelligent solutions for the protection, inheritance and dissemination of intangible cultural heritage, as well as new theoretical and practical references for the sustainable development of intangible cultural heritage. Furthermore, it is expected that the study will open up new paths for digital humanities research.

</details>


### [56] [Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework](https://arxiv.org/abs/2411.06160)

*Jingyi Zhou, Senlin Luo, Haofan Chen*

**Main category:** cs.CL

**Keywords:** Emotion Detection, Micro-Emotions, Emotion Quantization Network, NLP, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation, addressing limitations in existing emotion datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve emotion detection accuracy and expand the capabilities of AI in emotional reasoning, reducing reliance on manual annotations with inherent biases and limitations.

**Method:** The study proposes an all-labels and training-set label regression method that maps label values to energy intensity levels, allowing for the detection of multiple emotions in text samples.

**Key Contributions:**

	1. Introduction of a novel framework for micro-emotion detection (EQN).
	2. Mapping of label values to energy intensity levels for improved model learning.
	3. Validation of the framework's effectiveness across multiple sentiment datasets.

**Result:** The EQN framework was validated using five sentiment datasets, showing high effectiveness in automatic micro-emotion detection and annotation, outperforming existing literature methods.

**Limitations:** The paper acknowledges a misstatement in the EQN Framework section and does not delve deeply into potential areas of bias in dataset selection.

**Conclusion:** The EQN framework represents a significant advancement in the automatic detection of micro-emotions, enabling deeper insights into emotional analysis and quantitative research in emotion computing.

**Abstract:** Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.

</details>


### [57] [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)

*Chaeeun Kim, Jinu Lee, Wonseok Hwang*

**Main category:** cs.CL

**Keywords:** legal case retrieval, machine learning, benchmarking, legal informatics, Korean law

**Relevance Score:** 2

**TL;DR:** The paper introduces LEGAR BENCH, a large-scale legal case retrieval benchmark, and LegalSearchLM, a model enhancing legal element reasoning for better retrieval performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing legal case retrieval studies are limited by small-scale corpora and narrow query types, leading to inadequate performance and legally irrelevant matches.

**Method:** This study develops LEGAR BENCH for a comprehensive evaluation of legal case retrieval and proposes LegalSearchLM, which utilizes legal element reasoning and constrained decoding for case retrieval.

**Key Contributions:**

	1. Introduction of LEGAR BENCH, a large-scale benchmark for legal case retrieval
	2. Development of LegalSearchLM for improved legal element reasoning in retrieval
	3. Strong performance improvements demonstrated over existing models

**Result:** LegalSearchLM demonstrates a performance improvement of 6-20% over baseline models on LEGAR BENCH and shows robustness in generalization to out-of-domain cases, surpassing traditional generative models.

**Limitations:** 

**Conclusion:** The findings suggest that the combination of a large-scale benchmark and advanced reasoning model significantly enhances legal case retrieval capabilities.

**Abstract:** Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.

</details>


### [58] [Contrastive Learning for Task-Independent SpeechLLM-Pretraining](https://arxiv.org/abs/2412.15712)

*Maike Züfle, Jan Niehues*

**Main category:** cs.CL

**Keywords:** large language models, speech processing, contrastive learning, task-specific fine-tuning, ASR

**Relevance Score:** 9

**TL;DR:** This paper proposes a two-stage training approach for adapting large language models (LLMs) to speech processing tasks using contrastive learning for the pretraining phase followed by minimal data fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient adaptation of LLMs to speech tasks due to challenges like overfitting, data requirements, and high computational costs.

**Method:** A two-stage training approach: (1) Task-independent speech pretraining using contrastive learning for layer-wise alignment of text and speech representations; (2) Task-specific fine-tuning with minimal data.

**Key Contributions:**

	1. Introduction of a scalable two-stage training approach for speech tasks
	2. Use of contrastive learning for text-speech representation alignment
	3. Demonstrated performance improvement with less task-specific data

**Result:** The proposed method outperforms traditional ASR pretraining and exceeds the performance of models specialized in speech translation and question answering, even with only 10% of the task-specific data used for training.

**Limitations:** 

**Conclusion:** The two-stage training approach effectively mitigates overfitting and data limitations while enhancing speech processing model performance.

**Abstract:** Large language models (LLMs) excel in natural language processing but adapting these LLMs to speech processing tasks efficiently is not straightforward. Direct task-specific fine-tuning is limited by overfitting risks, data requirements, and computational costs. To address these challenges, we propose a scalable, two-stage training approach: (1) A task-independent speech pretraining stage using contrastive learning to align text and speech representations over all layers, followed by (2) a task-specific fine-tuning stage requiring minimal data. This approach outperforms traditional ASR pretraining and enables the model to surpass models specialized on speech translation and question answering while being trained on only 10% of the task-specific data.

</details>


### [59] [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/abs/2505.23833)

*Qingchuan Ma, Yuhang Wu, Xiawu Zheng, Rongrong Ji*

**Main category:** cs.CL

**Keywords:** Large Language Models, abstract reasoning, benchmark, symbolic reasoning, machine learning

**Relevance Score:** 8

**TL;DR:** The paper proposes a benchmark for assessing abstract reasoning in LLMs, introducing metrics to measure reasoning accuracy and dependence on specific symbols.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously evaluate abstract reasoning capabilities in Large Language Models (LLMs) and address their current limitations.

**Method:** Developed a mathematical framework defining abstract reasoning, introduced two new metrics for evaluation (scoreGamma and scoreDelta), and created a benchmark involving systematic symbol remapping in rule-based tasks.

**Key Contributions:**

	1. Established a new benchmark for probing abstract reasoning in LLMs
	2. Introduced two novel metrics to differentiate true abstraction from memorization
	3. Revealed specific limitations in LLMs regarding arithmetic and symbolic reasoning

**Result:** Evaluations showed critical shortcomings in arithmetic and symbolic reasoning, uncovered abstraction gaps despite chain-of-thought prompting, and established that scoreDelta effectively measures memory dependence.

**Limitations:** The study is focused on specific tasks and models, which may not generalize across all types of reasoning or LLM architectures.

**Conclusion:** Current LLMs exhibit significant deficiencies in abstract reasoning; this study highlights crucial areas for enhancement.

**Abstract:** In this paper, we aim to establish a simple, effective, and theoretically grounded benchmark for rigorously probing abstract reasoning in Large Language Models (LLMs). To achieve this, we first develop a mathematic framework that defines abstract reasoning as the ability to: (i) extract essential patterns independent of surface representations, and (ii) apply consistent rules to these abstract patterns. Based on this framework, we introduce two novel complementary metrics: \(\scoreGamma\) measures basic reasoning accuracy, while \(\scoreDelta\) quantifies a model's reliance on specific symbols rather than underlying patterns - a key indicator of true abstraction versus mere memorization. To implement this measurement, we design a benchmark: systematic symbol remapping in rule-based tasks, which forces models to demonstrate genuine pattern recognition beyond superficial token matching. Extensive LLM evaluations using this benchmark (commercial API models, 7B-70B, multi-agent) reveal:1) critical limitations in non-decimal arithmetic and symbolic reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting; and 3) \(\scoreDelta\)'s effectiveness in robustly measuring memory dependence by quantifying performance degradation under symbol remapping, particularly highlighting operand-specific memorization. These findings underscore that current LLMs, despite domain-specific strengths, still lack robust abstract reasoning, highlighting key areas for future improvement.

</details>


### [60] [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/abs/2505.23835)

*Ye Cheng, Minghui Xu, Yue Zhang, Kun Li, Hao Wu, Yechao Zhang, Shaoyong Guo, Wangjie Qiu, Dongxiao Yu, Xiuzhen Cheng*

**Main category:** cs.CL

**Keywords:** access control, IoT, large language models, policy generation, natural language processing

**Relevance Score:** 8

**TL;DR:** LACE is a hybrid framework utilizing LLMs for natural language-based access control in IoT, achieving high correctness and accuracy in policy generation and decisions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current access control methods in IoT are inadequate due to reliance on coarse-grained controls and rigid rule matching, leading to potential misconfigurations.

**Method:** LACE leverages large language models for prompt-guided policy generation, retrieval-augmented reasoning, and formal validation, allowing users to create access policies in natural language.

**Key Contributions:**

	1. Introduction of a language-based access control engine (LACE) for IoT
	2. Proven correctness in policy generation
	3. High decision accuracy and scalability of the system

**Result:** LACE achieves 100% correctness in verified policy generation and up to 88% decision accuracy with an F1-score of 0.79, outperforming models like GPT-3.5 and Gemini.

**Limitations:** 

**Conclusion:** LACE demonstrates significant potential for enhancing access control in IoT by being secure, flexible, and user-friendly with scalability.

**Abstract:** Access control in the Internet of Things (IoT) is becoming increasingly complex, as policies must account for dynamic and contextual factors such as time, location, user behavior, and environmental conditions. However, existing platforms either offer only coarse-grained controls or rely on rigid rule matching, making them ill-suited for semantically rich or ambiguous access scenarios. Moreover, the policy authoring process remains fragmented: domain experts describe requirements in natural language, but developers must manually translate them into code, introducing semantic gaps and potential misconfiguration. In this work, we present LACE, the Language-based Access Control Engine, a hybrid framework that leverages large language models (LLMs) to bridge the gap between human intent and machine-enforceable logic. LACE combines prompt-guided policy generation, retrieval-augmented reasoning, and formal validation to support expressive, interpretable, and verifiable access control. It enables users to specify policies in natural language, automatically translates them into structured rules, validates semantic correctness, and makes access decisions using a hybrid LLM-rule-based engine. We evaluate LACE in smart home environments through extensive experiments. LACE achieves 100% correctness in verified policy generation and up to 88% decision accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as GPT-3.5 and Gemini. The system also demonstrates strong scalability under increasing policy volume and request concurrency. Our results highlight LACE's potential to enable secure, flexible, and user-friendly access control across real-world IoT platforms.

</details>


### [61] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)

*Michael A. Hedderich, Anyi Wang, Raoyuan Zhao, Florian Eichin, Jonas Fischer, Barbara Plank*

**Main category:** cs.CL

**Keywords:** prompt engineering, language models, data mining, human analysis, systematic differences

**Relevance Score:** 9

**TL;DR:** Spotlight is a novel approach to prompt engineering for large language models that combines automation and human analysis to better understand systematic differences in model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation methods for large language model outputs have significant limitations, which can hinder the prompt engineering process.

**Method:** Spotlight uses data mining techniques to distinguish between random and systematic differences in language model outputs, providing token patterns for analysis.

**Key Contributions:**

	1. Introduction of the Spotlight approach combining automated and human-driven analysis
	2. Creation of three benchmarks for assessing token pattern extraction methods
	3. Identification of systematic differences in outputs related to prompt and model changes

**Result:** The approach successfully identifies systematic differences in outputs influenced by prompts and model changes, demonstrating reliability through three benchmarks.

**Limitations:** 

**Conclusion:** Spotlight aids in understanding language model outputs and enhances the efficiency of prompt engineering by revealing important systematic differences.

**Abstract:** Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.

</details>


### [62] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)

*Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn*

**Main category:** cs.CL

**Keywords:** evaluation awareness, language models, benchmarking, human-AI interaction, performance metrics

**Relevance Score:** 7

**TL;DR:** The paper investigates if language models can detect evaluation contexts, termed evaluation awareness, using a benchmark of prompts and transcripts. Models display above-random evaluation awareness but do not yet surpass human performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if AI models can behave differently during evaluations, which could bias their effectiveness in benchmarks and deployment decisions.

**Method:** A benchmark dataset of 1,000 prompts and transcripts from 61 datasets was analyzed, comparing how well language models classify whether transcripts are from evaluations or real-world usage.

**Key Contributions:**

	1. Demonstrated that language models can identify evaluation contexts with above-random accuracy.
	2. Created a benchmark for assessing evaluation awareness in AI models.
	3. Found performance discrepancies between model capabilities in agentic versus chat settings.

**Result:** Models showed evaluation awareness (Gemini-2.5-Pro reached an AUC of 0.83) but did not outperform humans (AUC of 0.92). They were better at identifying evaluations in agentic contexts and effectively recognized the purpose of the evaluations.

**Limitations:** The models do not yet reach superhuman performance in evaluation awareness compared to humans.

**Conclusion:** Frontier models exhibit significant evaluation awareness, warranting further monitoring of this capability in future AI developments.

**Abstract:** If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.

</details>


### [63] [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/abs/2505.23837)

*Lin Zhong, Lingzhi Wang, Xu Yang, Qing Liao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Point-Of-Interest prediction, Collaborative agents, Spatiotemporal data

**Relevance Score:** 9

**TL;DR:** The paper introduces CoMaPOI, a Collaborative Multi-Agent Framework for predicting the next Point-Of-Interest (POI) using Large Language Models to overcome challenges in spatiotemporal data and candidate POI space.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of LLMs in next POI prediction, particularly their lack of understanding of numeric spatiotemporal data and the large candidate POI space, which leads to irrelevant predictions.

**Method:** CoMaPOI employs three specialized agents: Profiler (converts numeric data into language), Forecaster (refines the POI candidate space), and Predictor (integrates this information for accurate predictions).

**Key Contributions:**

	1. Introduction of CoMaPOI framework for next POI prediction
	2. Three specialized agents working collaboratively to enhance model performance
	3. Experimental validation showing significant performance improvements over SOTA baselines

**Result:** CoMaPOI achieves state-of-the-art performance on three benchmark datasets, improving prediction accuracy by 5% to 10% compared to existing models.

**Limitations:** 

**Conclusion:** The work demonstrates the potential of collaborative agents in enhancing LLM applications for complex spatiotemporal prediction tasks.

**Abstract:** Large Language Models (LLMs) offer new opportunities for the next Point-Of-Interest (POI) prediction task, leveraging their capabilities in semantic understanding of POI trajectories. However, previous LLM-based methods, which are superficially adapted to next POI prediction, largely overlook critical challenges associated with applying LLMs to this task. Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic understanding of numeric spatiotemporal data, which hinders accurate modeling of users' spatiotemporal distributions and preferences; and (2) an excessively large and unconstrained candidate POI space, which often results in random or irrelevant predictions. To address these issues, we propose a Collaborative Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close interaction of three specialized agents (Profiler, Forecaster, and Predictor), CoMaPOI collaboratively addresses the two critical challenges. The Profiler agent is responsible for converting numeric data into language descriptions, enhancing semantic understanding. The Forecaster agent focuses on dynamically constraining and refining the candidate POI space. The Predictor agent integrates this information to generate high-precision predictions. Extensive experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that CoMaPOI achieves state of the art performance, improving all metrics by 5% to 10% compared to SOTA baselines. This work pioneers the investigation of challenges associated with applying LLMs to complex spatiotemporal tasks by leveraging tailored collaborative agents.

</details>


### [64] [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/abs/2505.23838)

*Yiming Huang, Jiyu Guo, Wenxin Mao, Cuiyun Gao, Peiyi Han, Chuanyi Liu, Qing Ling*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Large Language Models, Natural Language Processing, Relational Databases, Research Trends

**Relevance Score:** 8

**TL;DR:** This study systematically reviews LLM-based Text-to-SQL technology, focusing on research trends, techniques, datasets, evaluation metrics, and future directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate access to relational databases through natural language queries, particularly for users without SQL knowledge, leveraging advancements in large language models (LLMs).

**Method:** This study conducts a systematic review of existing literature on LLM-based text-to-SQL, analyzing techniques and datasets while discussing trends and future directions.

**Key Contributions:**

	1. Analysis of research trends in LLM-based text-to-SQL.
	2. In-depth analysis of existing techniques and methodologies.
	3. Comprehensive summary of datasets and evaluation metrics used in the field.

**Result:** Identified trends in the research of LLM-based text-to-SQL, detailed various techniques, and compiled datasets and evaluation metrics used in the field.

**Limitations:** 

**Conclusion:** The survey provides insights into LLM-based text-to-SQL, aiming to inspire future research and innovation in the area.

**Abstract:** Converting natural language (NL) questions into SQL queries, referred to as Text-to-SQL, has emerged as a pivotal technology for facilitating access to relational databases, especially for users without SQL knowledge. Recent progress in large language models (LLMs) has markedly propelled the field of natural language processing (NLP), opening new avenues to improve text-to-SQL systems. This study presents a systematic review of LLM-based text-to-SQL, focusing on four key aspects: (1) an analysis of the research trends in LLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based text-to-SQL techniques from diverse perspectives; (3) summarization of existing text-to-SQL datasets and evaluation metrics; and (4) discussion on potential obstacles and avenues for future exploration in this domain. This survey seeks to furnish researchers with an in-depth understanding of LLM-based text-to-SQL, sparking new innovations and advancements in this field.

</details>


### [65] [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)

*Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu*

**Main category:** cs.CL

**Keywords:** sycophancy, large language models, benchmarking, prompting strategies, multi-turn conversations

**Relevance Score:** 8

**TL;DR:** This paper introduces SYCON Bench, a benchmark for evaluating sycophantic behavior in large language models during multi-turn conversations, revealing that sycophancy is prevalent and highlighting strategies to mitigate it.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of existing research on sycophancy, which has mostly focused on single-turn factual correctness, by evaluating behavior in multi-turn conversations.

**Method:** The authors developed SYCON Bench to measure sycophantic behavior through metrics such as Turn of Flip (speed of conformance to user beliefs) and Number of Flip (frequency of stance shifts).

**Key Contributions:**

	1. Introduction of SYCON Bench for benchmarking sycophancy
	2. Demonstration of prevalence of sycophancy in LLMs
	3. Identification of effective prompting strategies to reduce sycophantic behavior

**Result:** The evaluation of 17 LLMs showed persistent sycophantic behavior across scenarios, with alignment tuning increasing this behavior while model scaling and reasoning optimization helped models resist user biases.

**Limitations:** The study does not address long-term impacts of sycophantic interactions nor all possible conversational dynamics.

**Conclusion:** Third-person prompting strategies can significantly reduce sycophancy, with reductions of up to 63.8% noted in debate scenarios, indicating potential paths to enhance model responses.

**Abstract:** Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at https://github.com/JiseungHong/SYCON-Bench.

</details>


### [66] [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)

*Zikun Ye, Hema Yoganarasimhan*

**Main category:** cs.CL

**Keywords:** Large Language Models, credit attribution, Shapley values, summarization, Cluster Shapley

**Relevance Score:** 9

**TL;DR:** This paper proposes an efficient method for credit attribution to documents in LLM-generated summaries using Cluster Shapley, which reduces computation while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about credit attribution and compensation for original content creators in LLM-generated summaries.

**Method:** The method introduces Cluster Shapley, an approximation algorithm that utilizes semantic similarity to cluster documents and compute Shapley values at the cluster level for efficient and accurate credit allocation.

**Key Contributions:**

	1. Introduction of Cluster Shapley for efficient credit attribution
	2. Demonstration of improved performance over baseline methods
	3. Broad applicability of the method across different summarization contexts

**Result:** Cluster Shapley significantly reduces computational complexity while maintaining high accuracy, outperforming traditional methods like Monte Carlo sampling and Kernel SHAP.

**Limitations:** Shapley values are expensive to compute at scale, but Cluster Shapley mitigates this limitation through approximation.

**Conclusion:** The proposed method is broadly applicable to various summarization settings and is agnostic to the specific LLM or summarization process used.

**Abstract:** Large Language Models (LLMs) are increasingly used in systems that retrieve and summarize content from multiple sources, such as search engines and AI assistants. While these models enhance user experience by generating coherent summaries, they obscure the contributions of original content creators, raising concerns about credit attribution and compensation. We address the challenge of valuing individual documents used in LLM-generated summaries. We propose using Shapley values, a game-theoretic method that allocates credit based on each document's marginal contribution. Although theoretically appealing, Shapley values are expensive to compute at scale. We therefore propose Cluster Shapley, an efficient approximation algorithm that leverages semantic similarity between documents. By clustering documents using LLM-based embeddings and computing Shapley values at the cluster level, our method significantly reduces computation while maintaining attribution quality. We demonstrate our approach to a summarization task using Amazon product reviews. Cluster Shapley significantly reduces computational complexity while maintaining high accuracy, outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP with a better efficient frontier. Our approach is agnostic to the exact LLM used, the summarization process used, and the evaluation procedure, which makes it broadly applicable to a variety of summarization settings.

</details>


### [67] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)

*Wenhan Dong, Tianyi Hu, Jingyi Zheng, Zhen Sun, Yuemeng Zhao, Yule Liu, Xinlei He, Xinyi Huang*

**Main category:** cs.CL

**Keywords:** large language models, evaluation standards, reasoning capabilities, multi-round tasks, automated metrics

**Relevance Score:** 9

**TL;DR:** This study critiques current evaluation methods for LLMs in multi-round incomplete tasks and proposes refined standards for assessing reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and address the limitations of existing automated evaluation methods for large language models, which can produce misleading insights into their reasoning abilities.

**Method:** The authors analyzed current benchmarks and metrics, identifying shortcomings such as shortcut-taking behaviors and rigid patterns in LLMs, and proposed a new evaluation framework that includes reasoning path inspection and diversified metrics.

**Key Contributions:**

	1. Critique of existing benchmarks for LLMs in multi-round tasks
	2. Proposal of a new evaluation framework that includes diverse assessment metrics
	3. Highlighting specific reasoning issues that current metrics fail to address

**Result:** The analysis demonstrated that existing evaluations often fail to reveal key reasoning issues in LLMs, prompting the need for improved evaluation standards.

**Limitations:** The paper may not cover all potential issues in LLM evaluations, and the proposed standards require further empirical validation.

**Conclusion:** Implementing refined evaluation standards can enhance the reliability of assessments for lateral thinking capabilities in LLMs, leading to more accurate insights into their reasoning processes.

**Abstract:** Multi-round incomplete information tasks are crucial for evaluating the lateral thinking capabilities of large language models (LLMs). Currently, research primarily relies on multiple benchmarks and automated evaluation metrics to assess these abilities. However, our study reveals novel insights into the limitations of existing methods, as they often yield misleading results that fail to uncover key issues, such as shortcut-taking behaviors, rigid patterns, and premature task termination. These issues obscure the true reasoning capabilities of LLMs and undermine the reliability of evaluations. To address these limitations, we propose a refined set of evaluation standards, including inspection of reasoning paths, diversified assessment metrics, and comparative analyses with human performance.

</details>


### [68] [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/abs/2505.23844)

*Zhenglun Kong, Zheng Zhan, Shiyue Hou, Yifan Gong, Xin Meng, Pengwei Sui, Peiyan Dong, Xuan Shen, Zifeng Wang, Pu Zhao, Hao Tang, Stratis Ioannidis, Yanzhi Wang*

**Main category:** cs.CL

**Keywords:** large language models, knowledge aggregation, adaptive selection, dynamic fusion, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for adaptively selecting and aggregating knowledge from multiple LLMs to improve performance while reducing memory overhead and knowledge interference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the continual performance of LLMs through better integration of knowledge without the limitations of traditional fine-tuning methods.

**Method:** The authors propose an adaptive selection network to choose the most relevant LLMs and a dynamic weighted fusion strategy, combined with a feedback-driven loss function.

**Key Contributions:**

	1. Adaptive selection network for relevant LLM identification
	2. Dynamic weighted fusion strategy for knowledge integration
	3. Feedback-driven loss function to enhance model adaptability

**Result:** The method achieves a reduction in knowledge interference by up to 50% compared to existing aggregation approaches, enabling a more stable and scalable integration process.

**Limitations:** 

**Conclusion:** The proposed framework successfully addresses the challenges of integrating knowledge from various LLMs and improves performance while managing resource constraints.

**Abstract:** Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration

</details>


### [69] [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/abs/2505.23845)

*Jakub Podolak, Rajeev Verma*

**Main category:** cs.CL

**Keywords:** DeepSeek, question answering, uncertainty estimation, semantic entropy, confidence

**Relevance Score:** 7

**TL;DR:** The paper analyzes the uncertainty in DeepSeek R1-32B by examining its self-reported verbal confidence in question answering tasks, revealing that semantic entropy provides a more reliable measure of confidence than the model's own assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the reliability of self-reported confidence in question answering tasks performed by DeepSeek R1-32B and to explore methods for improving uncertainty estimation.

**Method:** The study involved analyzing DeepSeek's performance in an answer-then-confidence setting, comparing it with a semantic entropy approach that samples multiple responses.

**Key Contributions:**

	1. Examination of the effectiveness of self-reported confidence in DeepSeek.
	2. Introduction of semantic entropy as a more reliable measure of confidence.
	3. Demonstration that longer reasoning chains improve confidence assessments.

**Result:** The findings indicate that allowing DeepSeek to perform a long chain-of-thought reasoning before answering significantly enhances its verbal score effectiveness, showing that self-reported confidence improves only after exploring the generative space.

**Limitations:** The analysis may not generalize to all models or types of tasks beyond those tested with DeepSeek R1-32B.

**Conclusion:** Reliable uncertainty estimation in models requires explicit exploration of the generative space, and self-reported confidence is only trustworthy after such exploration occurs.

**Abstract:** We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.

</details>


### [70] [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)

*Atanu Barai, Stephan Eidenbenz, Nandakishore Santhi*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Non-AI Agents, Parallel Discrete Event Simulation, Accuracy, Dynamic Constraints

**Relevance Score:** 5

**TL;DR:** This paper presents a novel methodology that uses parallel discrete event simulation (PDES) to effectively combine AI and non-AI agents for enhanced problem-solving accuracy and trustworthy AI output constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage the full potential of AI systems in a trustworthy manner by integrating multiple AI and non-AI systems for correctness of output.

**Method:** A parallel discrete event simulation (PDES) framework is established where both AI and non-AI agents work together in a cooperative environment, utilizing dynamic constraints and verifying actions at each step.

**Key Contributions:**

	1. Introduction of a novel PDES methodology for AI and non-AI agent integration.
	2. Demonstration of improved accuracy in problem-solving across different domains.
	3. Scalable simulation capabilities through the distribution of smaller AI agents.

**Result:** The proposed approach achieved an overall accuracy of 68% in solving various domain problems, significantly higher than the less than 23% accuracy of traditional AI models.

**Limitations:** 

**Conclusion:** The integration of non-AI agents as unbiased auditors improves the overall accuracy and reliability of AI systems in complex problem-solving tasks.

**Abstract:** To fully leverage the potential of artificial intelligence (AI) systems in a trustworthy manner, it is desirable to couple multiple AI and non-AI systems together seamlessly for constraining and ensuring correctness of the output. This paper introduces a novel parallel discrete event simulation (PDES) based methodology to combine multiple AI and non-AI agents in a causal, rule-based way. Our approach tightly integrates the concept of passage of time, with each agent considered as an entity in the PDES framework and responding to prior requests from other agents. Such coupling mechanism enables the agents to work in a co-operative environment towards a common goal while many tasks run in parallel throughout the simulation. It further enables setting up boundaries to the outputs of the AI agents by applying necessary dynamic constraints using non-AI agents while allowing for scalability through deployment of hundreds of such agents in a larger compute cluster. Distributing smaller AI agents can enable extremely scalable simulations in the future, addressing local memory bottlenecks for model parameter storage. Within a PDES involving both AI and non-AI agents, we break down the problem at hand into structured steps, when necessary, providing a set of multiple choices to the AI agents, and then progressively solve these steps towards a final goal. At each step, the non-AI agents act as unbiased auditors, verifying each action by the AI agents so that certain rules of engagement are followed. We evaluate our approach by solving four problems from four different domains and comparing the results with those from AI models alone. Our results show greater accuracy in solving problems from various domains where the AI models struggle to solve the problems solely by themselves. Results show that overall accuracy of our approach is 68% where as the accuracy of vanilla models is less than 23%.

</details>


### [71] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)

*Harvey Dam, Jonas Knochelmann, Vinu Joseph, Ganesh Gopalakrishnan*

**Main category:** cs.CL

**Keywords:** large language models, sensitive content, token formatting, refusal rates, chain-of-thought

**Relevance Score:** 9

**TL;DR:** A method is introduced to reduce refusal rates of LLMs on sensitive content by adjusting token formatting during generation without altering model weights or training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address refusal rates of LLMs on sensitive topics, which are often indicated by specific token sequences, the paper aims to find a solution that does not require modifications to model architecture or retraining.

**Method:** The approach involves suppressing specific token sequences in the generation process: removing the newline token after the CoT marker (<think>) and preventing the end-of-sequence token from appearing after the CoT block ends.

**Key Contributions:**

	1. Developed a novel approach to reduce refusals without model retraining
	2. Demonstrated effectiveness through experiments with prominent distillation models
	3. Highlighted the importance of token sequence management in LLM generation.

**Result:** The application of these methods led to an increased proportion of substantive responses to sensitive prompts in experiments, maintaining standard performance on regular benchmarks.

**Limitations:** 

**Conclusion:** The results indicate that managing token formatting can effectively mitigate refusal behaviors in LLMs during content generation.

**Abstract:** We introduce a method to reduce refusal rates of large language models (LLMs) on sensitive content without modifying model weights or prompts. Motivated by the observation that refusals in certain models were often preceded by the specific token sequence of a token marking the beginning of the chain-of-thought (CoT) block (<think>) followed by a double newline token (\n\n), we investigate the impact of two simple formatting adjustments during generation: suppressing \n\n after <think> and suppressing the end-of-sequence token after the end of the CoT block (</think>). Our method requires no datasets, parameter changes, or training, relying solely on modifying token probabilities during generation. In our experiments with official DeepSeek-R1 distillations, these interventions increased the proportion of substantive answers to sensitive prompts without affecting performance on standard benchmarks. Our findings suggest that refusal behaviors can be circumvented by blocking refusal subspaces at specific points in the generation process.

</details>


### [72] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)

*Michael Shalyt, Rotem Elimelech, Ido Kaminer*

**Main category:** cs.CL

**Keywords:** large language models, symbolic mathematics, ASyMOB, assessment framework, machine learning

**Relevance Score:** 6

**TL;DR:** Introduction of ASyMOB, a novel framework for assessing LLMs in symbolic mathematics with 17,092 unique challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks inadequately assess LLM proficiency in symbolic mathematics for advanced applications.

**Method:** Development of ASyMOB framework featuring a diverse set of math challenges organized by similarity and complexity, enabling performance analysis for LLMs.

**Key Contributions:**

	1. Introduction of ASyMOB framework with extensive benchmarks
	2. Demonstrated performance variations among LLMs and CAS
	3. Identification of models that perform better with integrated code execution.

**Result:** Evaluated LLMs showed significant performance degradation due to reliance on memorization, with advanced models demonstrating remarkable robustness against perturbations.

**Limitations:** The study primarily focuses on symbolic manipulation; broader mathematical reasoning is not addressed.

**Conclusion:** The findings suggest a critical phase transition in LLM capabilities, raising questions on the future of symbolic math systems and their integration with advanced tools.

**Abstract:** Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent "phase transition" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [73] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)

*Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen*

**Main category:** cs.CL

**Keywords:** Large Language Models, reproducibility, biomedical research, autonomous agents, Alzheimer's studies

**Relevance Score:** 9

**TL;DR:** This study demonstrates the use of Large Language Models (LLMs) as autonomous agents for reproducing findings from biomedical research using a specific dataset, revealing both successes and limitations in their capabilities.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs as autonomous agents in automating the reproducibility of biomedical research findings.

**Method:** A simulated research team of LLM-based autonomous agents was tasked with reproducing findings from five selected Alzheimer's studies using the 'Quick Access' dataset from the National Alzheimer's Coordinating Center (NACC).

**Key Contributions:**

	1. Demonstrated LLMs' ability to mimic research techniques
	2. Showed the percentage of findings LLMs could reproduce
	3. Identified discrepancies in statistical methods and implementation

**Result:** LLM agents reproduced approximately 53.2% of findings per study across five Alzheimer's studies, with some discrepancies in numeric values and statistical methods used compared to original research.

**Limitations:** LLM agents exhibited flaws in implementation and lacked detailed methodological information, limiting their effectiveness in full automation of reproducibility assessments.

**Conclusion:** The study highlights the promise of LLMs in facilitating reproducibility in biomedical research but also underscores the limitations in their current capabilities.

**Abstract:** Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.   Materials and Methods: We used the "Quick Access" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.   Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.   Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.   Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.

</details>


### [74] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)

*Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu*

**Main category:** cs.CL

**Keywords:** Uncertainty Estimation, Large Language Models, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This study investigates uncertainty estimation in large language models (LLMs) across various architectures and scales, highlighting the effectiveness of linguistic verbal uncertainty (LVU) over other methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure the safe and trustworthy deployment of LLMs in high-stakes applications through robust uncertainty estimation.

**Method:** Evaluated 80 LLMs across different architectures and scales using three uncertainty estimation methods: token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU) with a focus on the MMLU-Pro benchmark.

**Key Contributions:**

	1. Comprehensive analysis of uncertainty estimation across 80 LLMs
	2. Demonstration of LVU's superiority in uncertainty calibration
	3. Insights into the relationship between model characteristics and uncertainty estimation performance.

**Result:** LVU outperforms TPU and NVU in uncertainty calibration and discrimination. High accuracy does not guarantee reliable uncertainty, and model characteristics affect performance.

**Limitations:** 

**Conclusion:** LVU is positioned as a practical tool to enhance LLM reliability in real-world applications, emphasizing the need for comprehensive evaluation metrics.

**Abstract:** As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.

</details>


### [75] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)

*Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh*

**Main category:** cs.CL

**Keywords:** large language models, harmful prompts, multimodal detection, classification, AI safety

**Relevance Score:** 9

**TL;DR:** OMNIGUARD is a novel approach for detecting harmful prompts in large language models (LLMs) across languages and modalities, showing significant accuracy improvements and efficiency over existing methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding the harmful misuse of large language models (LLMs) and the fallibility of existing detection methods for harmful prompts, particularly in multilingual settings and different modalities.

**Method:** OMNIGUARD identifies internal representations in LLMs that are aligned across languages and modalities, using these representations to develop a language-agnostic and modality-agnostic classifier for detecting harmful prompts.

**Key Contributions:**

	1. Proposing a new method for harmful prompt detection in multilingual and multimodal settings.
	2. Demonstrating significant accuracy improvements on multilingual and image-based prompts.
	3. Achieving state-of-the-art results for audio-based prompt detection with high efficiency.

**Result:** OMNIGUARD improves harmful prompt classification accuracy by 11.57% in multilingual scenarios, by 20.44% for image-based prompts, and sets new state-of-the-art results for audio-based prompts, all while being approximately 120 times faster than previous leading methods.

**Limitations:** 

**Conclusion:** The OMNIGUARD approach enhances the detection of harmful prompts in LLMs across various languages and modalities, making it a significant advancement over existing techniques.

**Abstract:** The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [76] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)

*Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, healthcare, resource efficiency, supervised fine-tuning, clinical scenarios

**Relevance Score:** 9

**TL;DR:** Infi-Med is a framework designed to improve the deployment of multimodal large language models in healthcare by enhancing resource efficiency, multimodal reasoning, and evaluation systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in deploying multimodal large language models in healthcare, such as resource efficiency, diagnostic accuracy, and ethical considerations.

**Method:** Infi-Med introduces a framework focusing on preparing high-quality supervised fine-tuning datasets, enhancing multimodal reasoning, and implementing a systematic evaluation system for medical tasks.

**Key Contributions:**

	1. Resource-efficient dataset curation method for supervised fine-tuning
	2. Enhanced multimodal reasoning for clinical task understanding
	3. Systematic evaluation system for diverse medical modalities

**Result:** Infi-Med achieves state-of-the-art performance in medical reasoning and demonstrates adaptability to various clinical scenarios.

**Limitations:** 

**Conclusion:** The framework supports effective deployment of MLLMs in healthcare by addressing operational constraints without compromising effectiveness.

**Abstract:** Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints.

</details>


### [77] [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/abs/2505.23911)

*Pavel Tikhonov, Ivan Oseledets, Elena Tutubalina*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, few-shot tasks, task vector, distributed representation

**Relevance Score:** 8

**TL;DR:** This paper presents QuiteAFew, a large dataset for benchmarking in-context learning in Large Language Models (LLMs) with 3,096 diverse few-shot tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing studies on in-context learning (ICL) in LLMs, particularly the small-scale benchmarks which hinder comprehensive analysis.

**Method:** The authors create and experiment with the QuiteAFew dataset consisting of 3,096 few-shot tasks, testing the performance of Llama-3-8B across various tasks.

**Key Contributions:**

	1. Introduction of the QuiteAFew dataset comprising 3,096 diverse few-shot tasks.
	2. Insights into the layered performance of task vectors in LLMs.
	3. Identification of the need for distributed representation of task-related knowledge.

**Result:** Experiments showed that task vector performance peaked at an intermediate layer, effectiveness varied by task type, and complex tasks utilized multiple subtask-specific vectors, indicating a distributed representation of task knowledge.

**Limitations:** The study focuses solely on the performance of Llama-3-8B, which may not generalize to other models.

**Conclusion:** The findings suggest that ICL in LLMs has nuanced performance characteristics that depend on task complexity and require a better understanding of task vector usage.

**Abstract:** In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks using few examples, with task vectors - specific hidden state activations - hypothesized to encode task information. Existing studies are limited by small-scale benchmarks, restricting comprehensive analysis. We introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with 30 input-output pairs derived from the Alpaca dataset. Experiments with Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task type, and (3) complex tasks rely on multiple, subtask-specific vectors rather than a single vector, suggesting distributed task knowledge representation.

</details>


### [78] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)

*Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** large language models, hallucination detection, reinforcement learning, long-form generation, verbalized confidence

**Relevance Score:** 9

**TL;DR:** This paper presents LoVeC, a method for estimating verbalized confidence in long-form generation using reinforcement learning to enhance model calibration and factuality representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hallucination in large language models (LLMs) during factual content generation, by providing an efficient estimation of confidence in generated outputs.

**Method:** The authors propose LoVeC, which trains LLMs to append numerical confidence scores to generated statements through reinforcement learning, while introducing new evaluation settings to assess performance.

**Key Contributions:**

	1. Introduction of LoVeC for long-form verbalized confidence estimation.
	2. Utilization of reinforcement learning techniques for model calibration.
	3. Development of novel evaluation settings for confidence estimation methods.

**Result:** Experiments demonstrate that the RL-trained models using LoVeC achieve improved calibration and generalize effectively across diverse domains in long-form QA tasks.

**Limitations:** 

**Conclusion:** LoVeC provides an efficient and interpretable way to estimate confidence in long-form generation, enhancing the practical deployment of LLMs.

**Abstract:** Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [79] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)

*Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi*

**Main category:** cs.CL

**Keywords:** Large Language Models, toxicity, topic biases, implicit association, content moderation

**Relevance Score:** 8

**TL;DR:** This paper addresses the issue of Large Language Models (LLMs) misclassifying benign comments as toxic due to systematic topic biases, introducing a semantic-level approach to analyze these biases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the systematic topic biases in LLMs that contribute to their misclassification of benign comments as toxic, beyond just offensive language.

**Method:** The authors introduce Topic Association Analysis, designed to quantify LLM topic associations with toxicity by prompting LLMs to generate scenarios for misclassified benign comments and analyzing the topic amplification levels.

**Key Contributions:**

	1. Introduction of Topic Association Analysis for LLMs
	2. Demonstration of systematic topic biases affecting LLM performance
	3. Insights into moderation technique refinements needed for LLMs

**Result:** More advanced models like GPT-4 Turbo show stronger topic stereotypes, indicating that LLMs' moderation decisions are influenced by learned topic associations rather than just offensive language, though they have lower overall false positive rates.

**Limitations:** 

**Conclusion:** Refinement of moderation techniques is necessary beyond traditional keyword filtering to address LLM over-sensitivity effectively, shedding light on the cognitive mechanisms involved.

**Abstract:** Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [80] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)

*Feiteng Fang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xiang Huang, Dingwei Chen, Jing Ye, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Role-Playing Language Agents, Reward Model, Character-based Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper presents ChARM, a Character-based Act-adaptive Reward Model for Role-Playing Language Agents (RPLAs) that improves human-computer interactions by leveraging a novel reward model and introducing a large-scale preference dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the scalability and adaptability of reward models in simulating characters for better human-computer interactions.

**Method:** ChARM introduces an act-adaptive margin for enhanced learning efficiency and a self-evolution mechanism using large-scale unlabeled data, along with the creation of RoleplayPref, a new preference dataset for RPLAs.

**Key Contributions:**

	1. Introduction of act-adaptive margin for efficiency
	2. Development of self-evolution mechanism
	3. Creation of RoleplayPref dataset for RPLAs

**Result:** ChARM shows a 13% improvement over the traditional Bradley-Terry model in preference rankings and achieves state-of-the-art results in preference learning.

**Limitations:** 

**Conclusion:** ChARM effectively enhances the training and evaluation of RPLAs, showing significant improvements in key performance metrics, with the resources made publicly available for further research.

**Abstract:** Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.

</details>


### [81] [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)

*Daniel Wurgaft, Ben Prystawski, Kanishk Gandhi, Cedegao E. Zhang, Joshua B. Tenenbaum, Noah D. Goodman*

**Main category:** cs.CL

**Keywords:** think-aloud method, natural language processing, cognitive science, human reasoning, automated analysis

**Relevance Score:** 6

**TL;DR:** This paper develops automated methods for transcribing and annotating think-aloud verbal reports, enabling large-scale analysis of human reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Reinvigorate the think-aloud method in cognitive science by overcoming the limitations of manual transcription and annotation.

**Method:** The authors used natural language processing tools to automate transcription and annotation of think-aloud data from 640 participants engaged in a mathematical reasoning task.

**Key Contributions:**

	1. Development of automated transcription and annotation methods for think-aloud data
	2. Application of methods to a large sample size
	3. Demonstration of value in analyzing human reasoning traces at scale.

**Result:** Moderate inter-rater reliability with human coders was found, and analyses of search graphs revealed insights into human reasoning processes.

**Limitations:** The study is contingent on the quality of the NLP tools used for transcription and may not capture all nuances of human reasoning.

**Conclusion:** Automated analysis of think-aloud data can significantly enhance the scalability of cognitive research and provide valuable insights into reasoning.

**Abstract:** The think-aloud method, where participants voice their thoughts as they solve a task, is a valuable source of rich data about human reasoning processes. Yet, it has declined in popularity in contemporary cognitive science, largely because labor-intensive transcription and annotation preclude large sample sizes. Here, we develop methods to automate the transcription and annotation of verbal reports of reasoning using natural language processing tools, allowing for large-scale analysis of think-aloud data. In our study, 640 participants thought aloud while playing the Game of 24, a mathematical reasoning task. We automatically transcribed the recordings and coded the transcripts as search graphs, finding moderate inter-rater reliability with humans. We analyze these graphs and characterize consistency and variation in human reasoning traces. Our work demonstrates the value of think-aloud data at scale and serves as a proof of concept for the automated analysis of verbal reports.

</details>


### [82] [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/abs/2505.23932)

*Wendong Xu, Jing Xiong, Chenyang Zhao, Qiujiang Chen, Haoran Wang, Hui Shen, Zhongwei Wan, Jianbo Dai, Taiqiang Wu, He Xiao, Chaofan Tao, Z. Morley Mao, Ying Sheng, Zhijiang Guo, Hongxia Yang, Bei Yu, Lingpeng Kong, Quanquan Gu, Ngai Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Software Development, Continuous Integration

**Relevance Score:** 8

**TL;DR:** SwingArena is a framework for evaluating Large Language Models (LLMs) in software development environments by simulating real-world collaboration between code submitters and reviewers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a competitive evaluation framework that reflects actual software development workflows, moving beyond static benchmarks to dynamic interactions.

**Method:** SwingArena pairs LLMs as submitters and reviewers in a CI pipeline, augmented with a retrieval module for relevant code snippets from large codebases in multiple programming languages.

**Key Contributions:**

	1. Introduction of a competitive evaluation framework for LLMs in software development
	2. Models paired as submitters and reviewers for interactive evaluation
	3. Inclusion of a retrieval-augmented code generation module for long-context challenges

**Result:** Experiments on 400 real-world GitHub issues show that models like GPT-4o excel in patch generation, while others focus on correctness during CI validation.

**Limitations:** 

**Conclusion:** SwingArena offers a scalable and extensible methodology for evaluating LLMs within realistic software development contexts.

**Abstract:** We present SwingArena, a competitive evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, SwingArena models the collaborative process of software iteration by pairing LLMs as submitters, who generate patches, and reviewers, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. SwingArena presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings. More details are available on our project page: swing-bench.github.io

</details>


### [83] [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/abs/2505.23944)

*Thushara Manjari Naduvilakandy, Hyeju Jang, Mohammad Al Hasan*

**Main category:** cs.CL

**Keywords:** Causality detection, Large language models, Retrieval-augmented generation, Causality mining, Dynamic prompting

**Relevance Score:** 9

**TL;DR:** The paper presents RAG-based dynamic prompting schemes to improve LLM performance in causality detection and extraction, validated across multiple datasets and models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing unsupervised and supervised methods for causality detection and mining due to their reliance on human intervention and insufficient training data.

**Method:** Introduces several retrieval-augmented generation (RAG) based dynamic prompting schemes for LLMs to enhance their performance in causality detection tasks.

**Key Contributions:**

	1. Development of RAG-based dynamic prompting schemes for causality detection
	2. Validation of performance improvements through extensive experiments
	3. Addressing the gap of LLM application in causality mining

**Result:** Experiments show that the proposed RAG-based dynamic prompting improves performance over traditional static prompting methods across three datasets and five LLMs.

**Limitations:** The study is limited to the specific datasets and LLMs tested; applicability may vary with other models or datasets.

**Conclusion:** RAG-based prompting emerges as a superior approach for causality detection in comparison to static methods, filling a gap in the existing literature on LLM applications for causality.

**Abstract:** Causality detection and mining are important tasks in information retrieval due to their enormous use in information extraction, and knowledge graph construction. To solve these tasks, in existing literature there exist several solutions -- both unsupervised and supervised. However, the unsupervised methods suffer from poor performance and they often require significant human intervention for causal rule selection, leading to poor generalization across different domains. On the other hand, supervised methods suffer from the lack of large training datasets. Recently, large language models (LLMs) with effective prompt engineering are found to be effective to overcome the issue of unavailability of large training dataset. Yet, in existing literature, there does not exist comprehensive works on causality detection and mining using LLM prompting. In this paper, we present several retrieval-augmented generation (RAG) based dynamic prompting schemes to enhance LLM performance in causality detection and extraction tasks. Extensive experiments over three datasets and five LLMs validate the superiority of our proposed RAG-based dynamic prompting over other static prompting schemes.

</details>


### [84] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)

*Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi*

**Main category:** cs.CL

**Keywords:** chain-of-thought reasoning, bias articulation, large vision-language models

**Relevance Score:** 6

**TL;DR:** This paper investigates the faithfulness of chain-of-thought reasoning in large vision-language models and introduces a novel evaluation pipeline to analyze bias articulation patterns.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To examine if chain-of-thought reasoning accurately reflects the internal processes of large vision-language models and to explore the impact of text-based and image-based biases on reasoning.

**Method:** A fine-grained evaluation pipeline was developed to categorize bias articulation patterns in large vision-language models, enabling a more precise analysis of chain-of-thought reasoning.

**Key Contributions:**

	1. Introduced a novel evaluation pipeline for analyzing bias articulation in reasoning.
	2. Revealed critical distinctions between text-based and image-based biases in LVLMs.
	3. Identified the phenomenon of 'inconsistent' reasoning as a marker for biased reasoning.

**Result:** The study found that subtle image-based biases are rarely articulated compared to explicit text-based ones, and introduced the concept of 'inconsistent' reasoning, where models may abruptly change answers.

**Limitations:** Focused primarily on bias articulation and reasoning processes, may not cover other aspects of model performance.

**Conclusion:** The findings highlight significant differences in how models handle biases and indicate that current language-only models struggle to articulate implicit cues effectively.

**Abstract:** Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.

</details>


### [85] [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)

*Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, structural compression, machine learning, PCA, inference speed

**Relevance Score:** 7

**TL;DR:** FLAT-LLM is a fast training-free method for compressing Large Language Models using low-rank transformations in the activation space, achieving high generalization performance without recovery fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The high computational and memory demands of Large Language Models (LLMs) hinder their deployment, especially in resource-constrained environments. Current low-rank methods often degrade accuracy and are slow to calibrate.

**Method:** FLAT-LLM employs fine-grained low-rank transformations in the activation space, using PCA to compute truncated eigenvectors for weight transformation, and an importance-based metric for adaptive rank allocation across decoders.

**Key Contributions:**

	1. Introduction of FLAT-LLM for training-free structural compression of LLMs
	2. Utilization of PCA for efficient weight transformation
	3. Demonstrated improvements in efficiency and inference speed

**Result:** FLAT-LLM significantly outperforms structural pruning baselines in generalization and downstream performance while providing faster inference than decomposition-based methods.

**Limitations:** 

**Conclusion:** This method offers a practical solution to compress LLMs effectively without the need for extensive fine-tuning, making it suitable for deployment in various environments.

**Abstract:** Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis (PCA), and employ an importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.

</details>


### [86] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)

*Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff*

**Main category:** cs.CL

**Keywords:** fairness, large language models, uncertainty, benchmarking, gender-occupation dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces UCerF, an uncertainty-aware fairness metric for evaluating large language models (LLMs), alongside a new gender-occupation fairness evaluation dataset, aiming to improve the accountability of AI systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to benchmark the fairness of large language models (LLMs), as conventional metrics fail to account for model uncertainty and biases in decision-making.

**Method:** The authors propose an uncertainty-aware fairness metric (UCerF) and introduce a new fairness evaluation dataset consisting of 31,756 samples for co-reference resolution.

**Key Contributions:**

	1. Introduction of UCerF, an uncertainty-aware fairness metric for LLMs.
	2. Development of a new gender-occupation fairness evaluation dataset with 31,756 samples.
	3. Establishment of a benchmark for evaluating the behavior of open-source LLMs with respect to fairness.

**Result:** The proposed UCerF metric provides a more nuanced understanding of model fairness, revealing biases overlooked by traditional metrics in evaluating ten open-source LLMs.

**Limitations:** 

**Conclusion:** The benchmark created using UCerF and the new dataset facilitates a more transparent evaluation of LLM fairness, promoting accountability in AI systems.

**Abstract:** The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.

</details>


### [87] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)

*Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Transformers, large language models, layer diversity, bias-diversity decomposition, parameter scaling laws

**Relevance Score:** 6

**TL;DR:** This paper investigates how layers and their sizes in Transformers contribute to the performance of large language models, emphasizing the importance of diversity among layer outputs for improved accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the relationship between the internal mechanisms of Transformers, particularly the role of layer diversity, and the observed scaling laws regarding parameter size and model performance.

**Method:** The paper employs a bias-diversity decomposition to analyze the outputs of each layer in the residual stream of Transformers, examining both the bias from the ground truth and the diversity of outputs between layers.

**Key Contributions:**

	1. Introduced a bias-diversity decomposition for analyzing Transformer layers.
	2. Demonstrated that layer diversity is critical for improved model performance.
	3. Revealed that performance gains from additional layers follow submodular behavior.

**Result:** The analysis indicates that model performance improves with increased layer diversity, particularly when outputs deviate from the correct answers. Experiments confirm these properties across several tasks, showing that additional layers offer diminishing returns unless they contribute diverse predictions.

**Limitations:** The study primarily focuses on layer diversity without addressing other architectural features or training methods.

**Conclusion:** Adding diverse layers enhances models' performance but with diminishing returns as the number of layers increases, aligning with parameter scaling laws.

**Abstract:** Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.

</details>


### [88] [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)

*Alexandre Bonlarron, Florian Régin, Elisabetta De Maria, Jean-Charles Régin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Constraint Programming, Masked Language Models, Text Generation, Constraint Satisfaction Problem

**Relevance Score:** 8

**TL;DR:** This paper presents GenCP, an improved method for text generation that combines Large Language Models (LLMs) with Constraint Programming (CP) to enforce external constraints more effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of LLMs in generating constrained text due to their sequential token generation without explicit control mechanisms.

**Method:** The methodology involves integrating Masked Language Models (MLMs) with GenCP to enhance domain generation and facilitate bidirectional constraint propagation in text generation tasks.

**Key Contributions:**

	1. Integration of MLMs with GenCP for better constraint enforcement
	2. Bidirectional constraint propagation enhancing LLM predictions
	3. Improved performance on tasks requiring strict content constraints

**Result:** The evaluation demonstrates that the integration of MLMs significantly improves GenCP's performance on the COLLIE benchmarks, enabling better constraint adherence and more reliable text generation.

**Limitations:** Increased computation due to additional MLM calls and potential for increased backtracking.

**Conclusion:** The study concludes that while the approach requires additional MLM calls and may increase backtracking in some instances, it leads to a more efficient generation process and improves the generation of feasible and meaningful outputs under strict content constraints.

**Abstract:** Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.

</details>


### [89] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)

*Matthew Raffel, Victor Agostinelli, Lizhong Chen*

**Main category:** cs.CL

**Keywords:** speech-to-text, simultaneous translation, ASR, LLM, IWSLT 2025

**Relevance Score:** 6

**TL;DR:** This paper presents BeaverTalk, a system for speech-to-text translation using advanced ASR and LLM for simultaneous translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve real-time speech-to-text translation for the IWSLT 2025 simultaneous translation task.

**Method:** The system integrates VAD for segmenting speech, utilizes Whisper Large V2 for ASR, and fine-tunes Gemma 3 12B for translation using LoRAs with conversational prompting.

**Key Contributions:**

	1. Introduction of a cascaded system architecture combining VAD, ASR, and LLM for speech translation
	2. Utilization of fine-tuning techniques with LoRAs for performance improvement
	3. Contribution to simultaneous translation tasks at IWSLT 2025.

**Result:** The system achieved BLEU scores of 24.64 (low latency) and 27.83 (high latency) for English to German, and 34.07 (low latency) and 37.23 (high latency) for English to Chinese.

**Limitations:** 

**Conclusion:** BeaverTalk demonstrates effective performance in simultaneous speech translation with notable BLEU scores in both language pairs.

**Abstract:** This paper discusses the construction, fine-tuning, and deployment of BeaverTalk, a cascaded system for speech-to-text translation as part of the IWSLT 2025 simultaneous translation task. The system architecture employs a VAD segmenter for breaking a speech stream into segments, Whisper Large V2 for automatic speech recognition (ASR), and Gemma 3 12B for simultaneous translation. Regarding the simultaneous translation LLM, it is fine-tuned via low-rank adaptors (LoRAs) for a conversational prompting strategy that leverages a single prior-sentence memory bank from the source language as context. The cascaded system participated in the English$\rightarrow$German and English$\rightarrow$Chinese language directions for both the low and high latency regimes. In particular, on the English$\rightarrow$German task, the system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and 3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and 3521.35, respectively.

</details>


### [90] [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/abs/2505.24028)

*Kateryna Akhynko, Oleksandr Kosovan, Mykola Trokhymovych*

**Main category:** cs.CL

**Keywords:** manipulation detection, social media, language model, XLM-RoBERTa, classification

**Relevance Score:** 8

**TL;DR:** This paper presents a top-performing solution for detecting manipulation in social media, specifically aimed at Ukrainian Telegram users.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the need to recognize rhetorical and stylistic manipulation techniques in social media, which can influence user opinion.

**Method:** The researchers fine-tuned the Gemma 2 language model with LoRA adapters for classification and employed an XLM-RoBERTa model for multi-target span detection and optimization strategies.

**Key Contributions:**

	1. Fine-tuning Gemma 2 with LoRA for enhanced classification.
	2. Implementation of a meta-feature-based second-level classifier.
	3. Utilization of XLM-RoBERTa for multi-target span detection.

**Result:** The approach achieved 2nd place in the classification subtask and 3rd place in span detection at the UNLP 2025 Shared Task.

**Limitations:** 

**Conclusion:** The results indicate the effectiveness of the proposed methods in detecting manipulation techniques in social media content.

**Abstract:** This paper presents one of the top-performing solutions to the UNLP 2025 Shared Task on Detecting Manipulation in Social Media. The task focuses on detecting and classifying rhetorical and stylistic manipulation techniques used to influence Ukrainian Telegram users. For the classification subtask, we fine-tuned the Gemma 2 language model with LoRA adapters and applied a second-level classifier leveraging meta-features and threshold optimization. For span detection, we employed an XLM-RoBERTa model trained for multi-target, including token binary classification. Our approach achieved 2nd place in classification and 3rd place in span detection.

</details>


### [91] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)

*Yasaman Jafari, Zixian Wang, Leon Bergen, Taylor Berg-Kirkpatrick*

**Main category:** cs.CL

**Keywords:** Structured State Space Models, document souping, multi-hop QA, sparse retrieval, long-document reasoning

**Relevance Score:** 7

**TL;DR:** The paper explores merging hidden states from Structured State Space Models to enhance downstream reasoning through a technique called document souping.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve downstream reasoning tasks by enabling modular encoding and reuse of document representations without full input reprocessing.

**Method:** Documents are encoded independently and their representations are pooled using simple operations, such as averaging, to create a single context state.

**Key Contributions:**

	1. Introduction of document souping technique for modular encoding
	2. Demonstration of effectiveness in multi-hop QA and long-document reasoning
	3. Comparison with cross-encoder models showing competitive performance.

**Result:** Finetuning Mamba2 models yields soupable representations that perform well in multi-hop QA, sparse retrieval, and long-document reasoning, achieving nearly cross-encoder level performance on HotpotQA.

**Limitations:** 

**Conclusion:** Document souping allows for effective multi-document reasoning while maintaining efficiency in encoding.

**Abstract:** We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [92] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)

*Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi*

**Main category:** cs.CL

**Keywords:** Medical AI, Large Language Models, Question Answering, Human-Computer Interaction, Relevance Estimation

**Relevance Score:** 9

**TL;DR:** This study introduces the MedPAIR dataset to evaluate how physician trainees and Large Language Models (LLMs) prioritize relevant information in medical QA, finding misalignment between LLMs and trainee relevance estimates, which can be improved by filtering out irrelevant information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the reasoning logic of LLMs in medical question answering and compare it with that of physician trainees, addressing the issue of LLMs providing correct answers through flawed processes.

**Method:** We created the MedPAIR dataset with annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence for relevance, and compared these relevance estimates to those generated by LLMs.

**Key Contributions:**

	1. Introduction of the MedPAIR dataset for evaluating relevance in medical QA
	2. Comparison of relevance estimation between physician trainees and LLMs
	3. Demonstration of improved accuracy when irrelevant information is filtered out.

**Result:** The alignment between LLMs and the relevance estimates from physician trainees was found to be poor; filtering irrelevant sentences improved the accuracy for both physician trainees and LLMs.

**Limitations:** 

**Conclusion:** The study highlights the misalignment between LLMs and human reasoning in medical QA and suggests that leveraging human-relevant information can enhance model performance.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these "relevant" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: http://medpair.csail.mit.edu/.

</details>


### [93] [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/abs/2505.24063)

*Jiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Jiaxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu Guo, Lening Zhao, Congcong Jing, Guanghui An, Dong Xu*

**Main category:** cs.CL

**Keywords:** Traditional Chinese Medicine, multimodal QA, large language models, dataset, evaluation framework

**Relevance Score:** 6

**TL;DR:** Introducing TCM-Ladder, a multimodal QA dataset for evaluating TCM-specific language models, featuring over 52,000 questions across various disciplines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing importance of TCM and the lack of a standardized multimodal QA benchmark for evaluating large language models in this domain.

**Method:** The dataset, TCM-Ladder, includes 52,000+ questions across TCM disciplines, using automated and manual filtering, along with a newly proposed evaluation method, Ladder-Score.

**Key Contributions:**

	1. First multimodal QA dataset for TCM language models
	2. Introduction of Ladder-Score for TCM question answering evaluation
	3. Public availability of datasets and leaderboard for ongoing research

**Result:** Comparative experiments showed performance assessments of TCM and general domain LLMs using the multimodal dataset.

**Limitations:** 

**Conclusion:** TCM-Ladder serves as a crucial resource for evaluating TCM language models and includes a continuously updated leaderboard.

**Abstract:** Traditional Chinese Medicine (TCM), as an effective alternative medicine, has been receiving increasing attention. In recent years, the rapid development of large language models (LLMs) tailored for TCM has underscored the need for an objective and comprehensive evaluation framework to assess their performance on real-world tasks. However, existing evaluation datasets are limited in scope and primarily text-based, lacking a unified and standardized multimodal question-answering (QA) benchmark. To address this issue, we introduce TCM-Ladder, the first multimodal QA dataset specifically designed for evaluating large TCM language models. The dataset spans multiple core disciplines of TCM, including fundamental theory, diagnostics, herbal formulas, internal medicine, surgery, pharmacognosy, and pediatrics. In addition to textual content, TCM-Ladder incorporates various modalities such as images and videos. The datasets were constructed using a combination of automated and manual filtering processes and comprise 52,000+ questions in total. These questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic dialogue, and visual comprehension tasks. We trained a reasoning model on TCM-Ladder and conducted comparative experiments against 9 state-of-the-art general domain and 5 leading TCM-specific LLMs to evaluate their performance on the datasets. Moreover, we propose Ladder-Score, an evaluation method specifically designed for TCM question answering that effectively assesses answer quality regarding terminology usage and semantic expression. To our knowledge, this is the first work to evaluate mainstream general domain and TCM-specific LLMs on a unified multimodal benchmark. The datasets and leaderboard are publicly available at https://tcmladder.com or https://54.211.107.106 and will be continuously updated.

</details>


### [94] [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/abs/2505.24098)

*Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li*

**Main category:** cs.CL

**Keywords:** test synthesis, large language models, competitive programming, code generation, evaluation metrics

**Relevance Score:** 9

**TL;DR:** HARDTESTGEN is a pipeline developed for synthesizing high-quality tests using LLMs, significantly improving test precision and recall for evaluating LLM-generated code with a new dataset of 47k problems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Reliable verifiers are crucial for LLM reasoning but are hard to obtain for complex coding problems. Existing solutions often fail to catch subtle errors that only human-written edge cases can identify.

**Method:** HARDTESTGEN synthesizes tests for coding problems using LLMs, resulting in a new dataset containing 47,000 problems and corresponding high-quality tests.

**Key Contributions:**

	1. Introduction of HARDTESTGEN for high-quality test synthesis using LLMs
	2. Creation of a comprehensive competitive programming dataset HARDTESTS
	3. Significant improvements in precision and recall for evaluating LLM-generated code

**Result:** HARDTESTGEN tests outperform existing tests with an 11.3 percentage point increase in precision and a 17.5 percentage point increase in recall for LLM-generated code evaluation. For more difficult problems, precision improvements can reach up to 40 points.

**Limitations:** 

**Conclusion:** The HARDTESTS dataset and synthesis pipeline enhance model training effectiveness and performance in code generation tasks.

**Abstract:** Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.

</details>


### [95] [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/abs/2505.24105)

*Jiacheng Lin, Zhenbang Wu, Jimeng Sun*

**Main category:** cs.CL

**Keywords:** large language models, healthcare, reinforcement learning, clinical reasoning, EHR interpretation

**Relevance Score:** 9

**TL;DR:** EHRMIND is a framework for adapting large language models to clinical reasoning tasks by using reinforcement learning with verifiable rewards, addressing challenges of knowledge application in healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance large language models' performance in interpreting electronic health records by addressing the unique challenges associated with clinical reasoning in healthcare.

**Method:** The study employs a two-stage solution: first, a supervised fine-tuning (SFT) phase to inject missing knowledge, followed by reinforcement learning with verifiable rewards (RLVR) to refine decision-making and improve outcomes.

**Key Contributions:**

	1. Introduction of EHRMIND framework for healthcare LLM adaptation
	2. Identification of key failure modes in EHR interpretation
	3. Empirical validation of the approach through diverse clinical applications

**Result:** EHRMIND shows consistent improvements in accuracy, interpretability, and generalization across various clinical tasks including medical calculations, patient-trial matching, and disease diagnosis.

**Limitations:** The pilot study's findings may not account for all healthcare contexts or cover the full spectrum of challenges in EHR interpretation.

**Conclusion:** The findings indicate that applying RLVR can significantly enhance the capabilities of large language models in healthcare contexts.

**Abstract:** We present EHRMIND, a practical recipe for adapting large language models (LLMs) to complex clinical reasoning tasks using reinforcement learning with verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding, its application to healthcare contexts presents unique challenges due to the specialized knowledge and reasoning required for electronic health record (EHR) interpretation. Our pilot study on the MEDCALC benchmark reveals two key failure modes: (1) misapplied knowledge, where models possess relevant medical knowledge but apply it incorrectly, and (2) missing knowledge, where models lack essential domain knowledge. To address these cases, EHRMIND applies a two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that injects missing domain knowledge, stabilizes subsequent training, and encourages structured, interpretable outputs; followed by RLVR, which reinforces outcome correctness and refines the model's decision-making. We demonstrate the effectiveness of our method across diverse clinical applications, including medical calculations (MEDCALC), patient-trial matching (TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers consistent gains in accuracy, interpretability, and cross-task generalization. These findings offer practical guidance for applying RLVR to enhance LLM capabilities in healthcare settings.

</details>


### [96] [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/abs/2505.24119)

*Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer*

**Main category:** cs.CL

**Keywords:** LLM safety, linguistic diversity, multilingual safety, NLP research, AI safety practices

**Relevance Score:** 7

**TL;DR:** Analysis of linguistic diversity in LLM safety research reveals an English-centric focus and a significant language gap.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of focus on non-English languages in LLM safety research and to promote inclusivity in AI safety practices.

**Method:** Systematic review of nearly 300 publications from major NLP conferences and workshops (2020-2024), analyzing the representation of different languages.

**Key Contributions:**

	1. Highlights the English-centric nature of LLM safety research.
	2. Identifies the minimal attention given to high-resource non-English languages.
	3. Proposes future research directions for multilingual safety in LLMs.

**Result:** Identification of a growing language gap in LLM safety research, with recommendations for future research directions in multilingual safety, including safety evaluation and training data generation.

**Limitations:** Limited to publications from specific NLP conferences and workshops, possibly affecting the generalizability of findings.

**Conclusion:** The field can develop more inclusive AI safety practices by broadening the focus beyond English-centric research.

**Abstract:** This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.

</details>


### [97] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)

*Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu*

**Main category:** cs.CL

**Keywords:** KV cache, Compression, Reasoning models, Performance optimization, Machine learning

**Relevance Score:** 7

**TL;DR:** This paper proposes R-KV, a new method for compressing key-value caches in reasoning models, significantly improving performance and memory efficiency during inference.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Reasoning models exhibit impressive capabilities but generate excessively long outputs that complicate inference due to large key-value caches. The paper seeks to address the inefficiencies of current compression methods during complex reasoning tasks.

**Method:** The proposed method, R-KV, targets redundant tokens in the KV cache of reasoning models to improve performance while maintaining a minimal cache size.

**Key Contributions:**

	1. Introduction of redundancy-aware KV cache compression for reasoning models
	2. Maintaining high performance with significantly reduced cache size
	3. 90% memory savings and improved throughput during inference

**Result:** R-KV maintains nearly 100% of full KV cache performance using only 10% of the cache and can achieve 105% performance with 16% cache, resulting in 90% memory savings and 6.6X throughput improvements over standard chain-of-thought reasoning.

**Limitations:** 

**Conclusion:** The experimental results indicate that R-KV consistently outperforms existing KV cache compression methods across multiple reasoning datasets, providing a significant advancement in inference efficiency.

**Abstract:** Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.

</details>


### [98] [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/abs/2505.24143)

*Jinglong Gao, Xiao Ding, Lingxiao Zou, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** In-Context Learning, large language models, task alignment, demonstration selection, NLP tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces CrossICL, a new paradigm for In-Context Learning that leverages existing demonstrations from source tasks to improve performance on target tasks without additional effort from users.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on manual demonstration collection in In-Context Learning often limits its application, especially when users cannot provide these demonstrations.

**Method:** The paper proposes a two-stage alignment strategy to align tasks and reduce interference caused by task gaps, and conducts extensive experiments on 875 NLP tasks using six LLMs, including GPT-4o.

**Key Contributions:**

	1. Introduction of CrossICL, a novel paradigm for ICL
	2. Development of a two-stage alignment strategy
	3. Empirical exploration across a large benchmark of NLP tasks

**Result:** Experimental results validate the effectiveness of CrossICL, providing insights into criteria for demonstration selection and types of interference arising from task gaps.

**Limitations:** 

**Conclusion:** CrossICL offers a novel approach to improve In-Context Learning by utilizing existing demonstrations, thus reducing the burden on users while enhancing model performance.

**Abstract:** In-Context Learning (ICL) enhances the performance of large language models (LLMs) with demonstrations. However, obtaining these demonstrations primarily relies on manual effort. In most real-world scenarios, users are often unwilling or unable to provide such demonstrations. Inspired by the human analogy, we explore a new ICL paradigm CrossICL to study how to utilize existing source task demonstrations in the ICL for target tasks, thereby obtaining reliable guidance without any additional manual effort. To explore this, we first design a two-stage alignment strategy to mitigate the interference caused by gaps across tasks, as the foundation for our experimental exploration. Based on it, we conduct comprehensive exploration of CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs, including GPT-4o. Experimental results demonstrate the effectiveness of CrossICL and provide valuable insights on questions like the criteria for selecting cross-task demonstrations, as well as the types of task-gap-induced interference in CrossICL.

</details>


### [99] [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/abs/2505.24147)

*Chiwei Zhu, Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** Rationales, Language Models, Model Reliability, Performance, Human Alignment

**Relevance Score:** 8

**TL;DR:** This paper investigates the effects of rationales on language model performance and reliability, providing new insights and implications for their use in alignment with human cognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the inconsistent effects of rationales on language models, particularly in performance and reliability, contradicting the prevailing view that rationales always enhance model quality.

**Method:** Comprehensive investigations and empirical analyses to assess the impact of rationales on performance and reliability of language models across various tasks.

**Key Contributions:**

	1. Deterioration of model performance in certain scenarios with rationales
	2. Improved model reliability surpassing untrained counterparts
	3. Identification of a linear relationship between performance and reliability improvements

**Result:** Key findings include that rationales can sometimes worsen performance but improve reliability, with a linear relationship between performance and reliability improvements linked to task difficulty.

**Limitations:** The studies are task-dependent, and further exploration is needed to generalize the findings across all contexts.

**Conclusion:** The findings challenge existing beliefs about the benefits of rationales in language models and suggest new regulations for their effective application.

**Abstract:** Training language models with rationales augmentation has been shown to be beneficial in many existing works. In this paper, we identify that such a prevailing view does not hold consistently. We conduct comprehensive investigations to thoroughly inspect the impact of rationales on model performance as well as a novel perspective of model reliability. The results lead to several key findings that add new insights upon existing understandings: 1) Rationales can, at times, deteriorate model performance; 2) Rationales can, at times, improve model reliability, even outperforming their untrained counterparts; 3) A linear correspondence exists in between the performance and reliability improvements, while both are driven by the intrinsic difficulty of the task. These findings provide informative regulations on the broad utilization of rationales and raise critical implications on the procedure of explicitly aligning language models with implicit human thoughts. Codes can be found at https://github.com/Ignoramus0817/rationales.

</details>


### [100] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)

*Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Large Language Models, Domain-specific Construction, Knowledge Extraction, Unsupervised Learning

**Relevance Score:** 9

**TL;DR:** A novel framework called LKD-KGC for unsupervised construction of domain-specific Knowledge Graphs using LLMs is proposed, addressing challenges in existing methods by eliminating reliance on predefined schemas and enhancing precision and recall rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of domain-specific Knowledge Graph construction by addressing limitations of existing methods that rely on manually defined schemas and single-document processing.

**Method:** LKD-KGC autonomously analyzes document repositories, infers knowledge dependencies, prioritizes processing sequences using LLMs, and generates entity schemas autoregressively without predefined structures.

**Key Contributions:**

	1. Introduction of LKD-KGC framework for unsupervised domain-specific KG construction
	2. Autonomous inference of knowledge dependencies from document repositories
	3. Improved precision and recall in entity and relationship extraction for complex knowledge domains.

**Result:** LKD-KGC shows improvements of 10% to 20% in precision and recall rates compared to state-of-the-art baseline methods in the construction of domain-specific Knowledge Graphs.

**Limitations:** 

**Conclusion:** The proposed LKD-KGC framework is effective for high-quality Knowledge Graph construction, demonstrating significant advancements over current techniques.

**Abstract:** Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.

</details>


### [101] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)

*Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Mixed Reward Functions, Post-training Datasets, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper proposes Mixed-R1, a framework for stable reinforcement learning in multi-source large language models (MLLMs) using a mixed reward function and a curated post-training dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of works that leverage multi-source MLLM tasks for stable reinforcement learning as current approaches focus narrowly on specific tasks.

**Method:** The authors designed a data engine for constructing a high-quality post-training dataset (Mixed-45K) and a mixed reward function that accommodates various MLLM tasks.

**Key Contributions:**

	1. Introduction of the Mixed-R1 framework for stable RL in MLLMs
	2. Development of Mixed-45K dataset comprised of high-quality examples
	3. Proposal of the Bidirectional Max-Average Similarity (BMAS) reward function for long-form responses

**Result:** Extensive experiments demonstrate the effectiveness of Mixed-R1 on different MLLMs, showcasing improvements in handling diverse tasks using multiple reward functions.

**Limitations:** 

**Conclusion:** The proposed framework and dataset provide a structured approach to enhance the performance of MLLMs in various tasks, reinforcing the versatility of reinforcement learning in this domain.

**Abstract:** Recent works on large language models (LLMs) have successfully demonstrated the emergence of reasoning capabilities via reinforcement learning (RL). Although recent efforts leverage group relative policy optimization (GRPO) for MLLMs post-training, they constantly explore one specific aspect, such as grounding tasks, math problems, or chart analysis. There are no works that can leverage multi-source MLLM tasks for stable reinforcement learning. In this work, we present a unified perspective to solve this problem. We present Mixed-R1, a unified yet straightforward framework that contains a mixed reward function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K). We first design a data engine to select high-quality examples to build the Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which contains various reward functions for various MLLM tasks. In particular, it has four different reward functions: matching reward for binary answer or multiple-choice problems, chart reward for chart-aware datasets, IoU reward for grounding problems, and open-ended reward for long-form text responses such as caption datasets. To handle the various long-form text content, we propose a new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by leveraging tokenizer embedding matching between the generated response and the ground truth. Extensive experiments show the effectiveness of our proposed method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes. Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [102] [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/abs/2505.24165)

*Yixuan Wang, Shiqi Zhou, Chuanzhe Guo, Qingfu Zhu*

**Main category:** cs.CL

**Keywords:** Data Synthesis, Instruction Evolution, Knowledge Tags, Machine Learning, Data Diversity

**Relevance Score:** 7

**TL;DR:** Tag-Evol is a new instruction evolving framework that uses diverse knowledge tags for efficient data synthesis, outperforming existing methods in generating high-quality evolved data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing instruction evolving methods suffer from manual design limitations and high costs of acquiring hard samples. There is a need for a more efficient and diverse approach.

**Method:** Tag-Evol introduces diverse knowledge tags as evolution strategies, enabling controlled evolution by combining different tags with original instructions.

**Key Contributions:**

	1. Introduction of Tag-Evol framework for instruction evolution
	2. Use of diverse knowledge tags for data synthesis
	3. Demonstration of improved performance in generating evolved data across benchmarks.

**Result:** Experiments show that Tag-Evol generates significantly better evolved data across various benchmarks compared to existing methods, revealing higher efficiency and diversity.

**Limitations:** 

**Conclusion:** Tag-Evol not only improves efficiency in data synthesis but also enhances the diversity and challenge level of the evolved data.

**Abstract:** Evol-Instruct has made significant improvements as a data synthesis method in several areas. Existing methods typically rely on a fixed set of strategies to evolve, which require manual design and are monolithic in form. In addition, iterative evolution also makes the acquisition of hard samples expensive. In view of this, we propose the Tag-Evol framework, a more diverse and efficient instruction evolving method. Specifically, Tag-Evol uses diverse and specific knowledge tags as strategies to achieve controlled evolution by injecting different combinations of tags into the original instructions. Experiments with multiple backbones in diverse domain benchmarks show that the proposed method generates significantly better evolved data than other methods. Furthermore, we conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is not only efficient but also generates more diverse and challenging data.

</details>


### [103] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)

*Ryota Miyano, Yuki Arase*

**Main category:** cs.CL

**Keywords:** LoRA, low-resource language generation, LLM adaptation, fine-tuning, task adaptability

**Relevance Score:** 9

**TL;DR:** The paper presents a novel LoRA merge method for adapting large language models (LLMs) to low-resource language generation tasks, demonstrating improvements in task adaptability in experiments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing LoRA methods that keep parameters frozen and do not cater to low-resource scenarios.

**Method:** The proposed method updates and prunes LoRA parameters via fine-tuning using minimal target task data, enhancing adaptability for diverse language tasks.

**Key Contributions:**

	1. Introduction of a LoRA merge method that adapts LLMs for low-resource tasks
	2. Demonstration of task adaptability through fine-tuning with minimal data
	3. Empirical validation via extensive experiments focusing on summarization tasks

**Result:** Extensive experiments, particularly in summarization tasks with datasets in English and Japanese, show significant improvements in task adaptability compared to previous methods.

**Limitations:** 

**Conclusion:** The LoRA merge method provides a more effective adaptation strategy for LLMs in low-resource settings, demonstrating its utility across various languages and domains.

**Abstract:** This study proposes a simple yet effective LoRA merge method to achieve LLM adaptation for low-resource language generation tasks. The LoRA merge technique, which integrates multiple LoRA modules trained on different tasks, has gained attention as an effective and efficient approach for adapting LLMs to target tasks. However, previous methods are limited in adaptability as they keep the LoRA parameters frozen. Additionally, the low-resource problem has been out of their scope. We propose a LoRA merge method that updates and prunes LoRA parameters through fine-tuning with minimal target task data, which allows finer-grained adjustments of LoRA parameters and enhancement of task adaptability. Extensive experiments have been conducted taking summarization as a benchmark task. Our datasets cover various domains and multiple languages of English and Japanese. The results confirm that the proposed method achieves significant and consistent improvements in task adaptability over the previous methods.

</details>


### [104] [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/abs/2505.24187)

*Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir*

**Main category:** cs.CL

**Keywords:** large language models, LLM reliability, key tokens, sequence length, semantic decision points

**Relevance Score:** 9

**TL;DR:** This research challenges the assumption of exponential decay in LLM reliability with longer sequences, proposing that reliability relies on a small number of key tokens that represent critical decision points. It offers a new framework for enhancing LLM performance through the selective preservation of significant tokens and strategic reasoning over brute-force methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the prevailing assumption that LLM reliability exponentially decays with sequence length, which limits their effectiveness in generating long outputs.

**Method:** The study synthesizes evidence that LLM errors are concentrated around key tokens and proposes a new reliability formula along with a framework for next-generation language models focusing on decision-making processes rather than sheer output length.

**Key Contributions:**

	1. Introduces a new reliability formula for LLMs focused on key tokens.
	2. Proposes a framework for improving LLM performance through targeted strategies of token management.
	3. Challenges the exponential decay hypothesis and suggests a more nuanced understanding of LLM reliability.

**Result:** Findings indicate that LLM performance with long sequences can be significantly improved by focusing on a few critical tokens rather than maintaining uniform accuracy across all tokens, leading to better coherence in long outputs.

**Limitations:** 

**Conclusion:** The research posits a paradigm shift in understanding LLM reliability, suggesting that strategic reasoning at key decision points can enhance model performance without the need for extensive computational resources.

**Abstract:** The prevailing assumption of an exponential decay in large language model (LLM) reliability with sequence length, predicated on independent per-token error probabilities, posits an inherent limitation for long autoregressive outputs. Our research fundamentally challenges this view by synthesizing emerging evidence that LLM errors are not uniformly distributed but are concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing critical decision junctions. By distinguishing these high-impact tokens from the increasingly predictable majority, we introduce a new reliability formula explaining the sustained coherence of modern LLMs over thousands of tokens. Converging research streams reveal that long-context performance primarily depends on accurately navigating a few crucial semantic decision points rather than on uniform token-level accuracy, enabling targeted strategies that significantly outperform brute-force approaches. We thus propose a framework for next-generation systems centered on selective preservation of semantically vital tokens, dynamic computational allocation at uncertain decision boundaries, multi-path exploration at ambiguities, and architectures aligned with natural semantic domains. This marks a fundamental shift from raw scaling to strategic reasoning, promising breakthrough performance without proportionate computational scaling and offering a more nuanced understanding that supersedes the exponential decay hypothesis, thereby opening pathways toward substantially more powerful and efficient language systems.

</details>


### [105] [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196)

*Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang*

**Main category:** cs.CL

**Keywords:** speculative decoding, Large Language Models, layer-skipping, CLaSp, efficient decoding

**Relevance Score:** 8

**TL;DR:** CLaSp is a method for accelerating Large Language Model decoding through an in-context layer-skipping strategy that avoids the need for additional training modules.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of existing drafting approaches in speculative decoding, which require additional training and compatibility across various LLMs.

**Method:** CLaSp employs a plug-and-play mechanism by skipping intermediate layers of a verify model. It uses a dynamic programming algorithm to optimize this process based on hidden states from the last verification stage, allowing for flexibility in layer-skipping after each verification.

**Key Contributions:**

	1. Introduces an in-context layer-skipping strategy for LLMs.
	2. Eliminates the need for additional drafting modules or training.
	3. Demonstrates significant speedup in decoding without compromising text quality.

**Result:** CLaSp achieves 1.3x to 1.7x speedup on LLaMA3 series models, maintaining the original distribution of generated text across various downstream tasks.

**Limitations:** 

**Conclusion:** The proposed method provides an efficient alternative for speculative decoding without the complexities of additional training modules.

**Abstract:** Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.

</details>


### [106] [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/abs/2505.24199)

*Yimin Du*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Preference Data Annotation, Intuitionistic Fuzzy Sets, Reinforcement Learning from Human Feedback, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper presents a novel framework using intuitionistic fuzzy sets (IFS) for improving the quality of human preference data in large language model (LLM) training, particularly in RLHF and DPO scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The quality of human preference data is essential for the effective training and evaluation of large language models, but traditional annotation methods face challenges due to uncertainty and annotator disagreement.

**Method:** The paper introduces an IFS-based annotation protocol that models preferences through degrees of membership, non-membership, and hesitation, alongside aggregation methods for addressing annotator disagreement and quality metrics for assessment.

**Key Contributions:**

	1. Introduction of an IFS-based annotation protocol for preferences
	2. Development of aggregation methods for annotator disagreement
	3. Introduction of quality metrics for preference data assessment

**Result:** The IFS-based approach showed significant improvements in annotation consistency, reduced annotator fatigue, and enhanced quality of preference data, leading to a 12.3% increase in model performance and a 15.7% decrease in annotation time compared to traditional methods.

**Limitations:** 

**Conclusion:** The proposed framework offers a robust solution for managing uncertainty in human preference annotation, yielding practical advancements in LLM training.

**Abstract:** The quality of human preference data is crucial for training and evaluating large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional side-by-side (SBS) annotation approaches often struggle with inherent uncertainty, annotator disagreement, and the complexity of preference judgments. This paper introduces a novel framework based on intuitionistic fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data annotation tasks. Our approach captures not only the degree of preference but also the uncertainty and hesitation inherent in human judgment through membership, non-membership, and hesitation degrees. We propose an IFS-based annotation protocol that enables more nuanced preference modeling, develops aggregation methods for handling annotator disagreement, and introduces quality metrics for preference data assessment. Experimental validation on multiple datasets demonstrates that our IFS-based approach significantly improves annotation consistency, reduces annotator fatigue, and produces higher-quality preference data compared to traditional binary and Likert-scale methods. The resulting preference datasets lead to improved model performance in downstream tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\% reduction in annotation time. Our framework provides a principled approach to handling uncertainty in human preference annotation and offers practical benefits for large-scale LLM training.

</details>


### [107] [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/abs/2505.24211)

*Jiwan Chung, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu*

**Main category:** cs.CL

**Keywords:** generative models, cross-modal coherence, dataset, model evaluation, equivariance

**Relevance Score:** 6

**TL;DR:** This paper introduces ACON, a dataset to evaluate cross-modal coherence in any-to-any generative models and finds that these models do not outperform specialized models in certain consistency evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of any-to-any generative models in maintaining cross-modal coherence and understanding if their consistency is genuine or merely perceived.

**Method:** The authors introduce ACON, a dataset of 1,000 images paired with captions, editing instructions, and Q&A pairs. They evaluate models using consistency criteria: cyclic consistency, forward equivariance, and conjugated equivariance.

**Key Contributions:**

	1. Introduction of the ACON dataset for evaluating cross-modal coherence.
	2. Identification of consistency criteria for model evaluation.
	3. Demonstration of the limitations of any-to-any models compared to specialized models.

**Result:** Experiments show that any-to-any models do not demonstrate superior cross-modal consistency compared to specialized models in cyclic consistency, but some consistency is observable in equivariance evaluations through analysis of intermediate latent spaces.

**Limitations:** The study primarily focuses on three consistency criteria, which may not capture all aspects of cross-modal coherence.

**Conclusion:** The findings suggest that while any-to-any models have weak observable consistency, they do not consistently outperform specialized models, highlighting the challenges in achieving true cross-modal coherence.

**Abstract:** Any-to-any generative models aim to enable seamless interpretation and generation across multiple modalities within a unified framework, yet their ability to preserve relationships across modalities remains uncertain. Do unified models truly achieve cross-modal coherence, or is this coherence merely perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500 newly contributed) paired with captions, editing instructions, and Q&A pairs to evaluate cross-modal transfers rigorously. Using three consistency criteria-cyclic consistency, forward equivariance, and conjugated equivariance-our experiments reveal that any-to-any models do not consistently demonstrate greater cross-modal consistency than specialized models in pointwise evaluations such as cyclic consistency. However, equivariance evaluations uncover weak but observable consistency through structured analyses of the intermediate latent space enabled by multiple editing operations. We release our code and data at https://github.com/JiwanChung/ACON.

</details>


### [108] [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)

*Jixuan Leng, Cassandra A. Cohen, Zhixian Zhang, Chenyan Xiong, William W. Cohen*

**Main category:** cs.CL

**Keywords:** Large Language Models, semi-structured reasoning, Chain-of-Thought, reasoning audit, bias detection

**Relevance Score:** 9

**TL;DR:** This paper introduces Semi-Structured Reasoning Models (SSRMs) that enhance LLMs' reasoning capabilities by generating semi-structured reasoning traces with specific vocabulary, improving transparency and auditability in reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the problem of faithfulness in LLM reasoning, which often contains undetectable errors and biases.

**Method:** SSRMs generate reasoning traces in a Pythonic syntax using a semi-structured Chain-of-Thought format, allowing for better identification of reasoning steps and potential flaws.

**Key Contributions:**

	1. Introduction of Semi-Structured Reasoning Models (SSRMs)
	2. Capability to generate semi-structured reasoning traces for better transparency
	3. Proven effectiveness in auditing reasoning flaws through structured and learned audits.

**Result:** SSRMs outperform comparably sized models by nearly 10 percentage points on in-domain tasks and remain competitive on out-of-domain medical benchmarks, showing improved generality.

**Limitations:** 

**Conclusion:** Semi-structured reasoning models facilitate effective auditing of reasoning processes, allowing for the detection of errors and biases in LLM outputs.

**Abstract:** As Large Language Models (LLMs) become increasingly capable at reasoning, the problem of "faithfulness" persists: LLM "reasoning traces" can contain errors and omissions that are difficult to detect, and may obscure biases in model outputs. To address these limitations, we introduce Semi-Structured Reasoning Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT) reasoning format within the model. Our SSRMs generate reasoning traces in a Pythonic syntax. While SSRM traces are not executable, they adopt a restricted, task-specific vocabulary to name distinct reasoning steps, and to mark each step's inputs and outputs. Through extensive evaluation on ten benchmarks, SSRMs demonstrate strong performance and generality: they outperform comparably sized baselines by nearly ten percentage points on in-domain tasks while remaining competitive with specialized models on out-of-domain medical benchmarks. Furthermore, we show that semi-structured reasoning is more amenable to analysis: in particular, they can be automatically audited to identify reasoning flaws. We explore both hand-crafted structured audits, which detect task-specific problematic reasoning patterns, and learned typicality audits, which apply probabilistic models over reasoning patterns, and show that both audits can be used to effectively flag probable reasoning errors.

</details>


### [109] [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/abs/2505.24219)

*Lam Thanh Do, Aaditya Bodke, Pritom Saha Akash, Kevin Chen-Chuan Chang*

**Main category:** cs.CL

**Keywords:** Unsupervised Keyphrase Prediction, Machine Learning, Information Retrieval

**Relevance Score:** 7

**TL;DR:** ERU-KG is an unsupervised keyphrase generation model that improves informativeness estimation and time efficiency for keyphrase prediction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing unsupervised keyphrase prediction methods rely on heuristically defined importance scores and lack time efficiency, leading to inaccurate results.

**Method:** ERU-KG consists of an informativeness module that estimates relevance using references and a phraseness module that generates candidates, allowing for efficient informativeness estimation and keyphrase generation.

**Key Contributions:**

	1. Proposes a novel model (ERU-KG) for unsupervised keyphrase generation that improves informativeness estimation
	2. Achieves high effectiveness in keyphrase generation benchmarks
	3. Fastest inference speed among similar model size baselines.

**Result:** ERU-KG outperforms unsupervised baselines by achieving on average 89% of the performance of supervised models on keyphrase generation benchmarks.

**Limitations:** 

**Conclusion:** The model efficiently generates keyphrases and performs well in text retrieval tasks, also being adaptable for keyphrase generation and extraction based on hyperparameters.

**Abstract:** Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of an informativeness and a phraseness module. The former estimates the relevance of keyphrase candidates, while the latter generate those candidates. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of documents are perceived in different contexts and 2) estimating informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89\% of the performance of a supervised model for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Furthermore, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes. Finally, our proposed model can switch between keyphrase generation and extraction by adjusting hyperparameters, catering to diverse application requirements.

</details>


### [110] [Automated Structured Radiology Report Generation](https://arxiv.org/abs/2505.24223)

*Jean-Benoit Delbrouck, Justin Xu, Johannes Moll, Alois Thomas, Zhihong Chen, Sophie Ostmeier, Asfandyar Azhar, Kelvin Zhenghao Li, Andrew Johnston, Christian Bluethgen, Eduardo Reis, Mohamed Muneer, Maya Varma, Curtis Langlotz*

**Main category:** cs.CL

**Keywords:** radiology, report generation, structured reporting, large language models, clinical evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents Structured Radiology Report Generation (SRRG), a new task focused on converting unstructured radiology reports into a standardized format to improve report clarity and consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved clinical efficiency in radiology reporting and the challenges posed by the variability of free-form reports.

**Method:** The authors introduce a novel dataset created by restructuring free-text reports into a standardized format using large language models (LLMs) and propose SRR-BERT for fine-grained disease classification.

**Key Contributions:**

	1. Introduction of Structured Radiology Report Generation (SRRG) task
	2. Development of a novel dataset for standardized reporting
	3. Proposal of SRR-BERT and F1-SRR-BERT for improved report evaluation

**Result:** The proposed methods and models, including F1-SRR-BERT, enhance the generation and evaluation of structured radiology reports, validated by a reader study with radiologists and benchmarking.

**Limitations:** 

**Conclusion:** The new structured approach addresses the limitations of traditional free-text reporting and improves the ability to generate clinically meaningful radiology reports.

**Abstract:** Automated radiology report generation from chest X-ray (CXR) images has the potential to improve clinical efficiency and reduce radiologists' workload. However, most datasets, including the publicly available MIMIC-CXR and CheXpert Plus, consist entirely of free-form reports, which are inherently variable and unstructured. This variability poses challenges for both generation and evaluation: existing models struggle to produce consistent, clinically meaningful reports, and standard evaluation metrics fail to capture the nuances of radiological interpretation. To address this, we introduce Structured Radiology Report Generation (SRRG), a new task that reformulates free-text radiology reports into a standardized format, ensuring clarity, consistency, and structured clinical reporting. We create a novel dataset by restructuring reports using large language models (LLMs) following strict structured reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained disease classification model trained on 55 labels, enabling more precise and clinically informed evaluation of structured reports. To assess report quality, we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease taxonomy to bridge the gap between free-text variability and structured clinical reporting. We validate our dataset through a reader study conducted by five board-certified radiologists and extensive benchmarking experiments.

</details>


### [111] [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/abs/2505.24229)

*Luong Ho, Khanh Le, Vinh Pham, Bao Nguyen, Tan Tran, Duc Chau*

**Main category:** cs.CL

**Keywords:** Inverse Text Normalization, Automatic Speech Recognition, streaming ASR, language model, low-resource scenarios

**Relevance Score:** 8

**TL;DR:** This paper presents a streaming pretrained language model for Inverse Text Normalization (ITN) to enhance ASR outputs, addressing challenges in efficiency and adaptability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the usability of spoken ASR outputs through effective ITN, especially in low-resource contexts, where existing solutions struggle.

**Method:** Introduction of a streaming pretrained language model utilizing Dynamic Context-Aware adjustments during both training and inference for adaptive chunk sizes and right-context integration.

**Key Contributions:**

	1. Development of a streaming pretrained language model for ITN
	2. Implementation of Dynamic Context-Aware adjustments
	3. Demonstration of superior performance over existing streaming ITN models

**Result:** Experimental results indicate that the proposed method achieves accuracy on par with non-streaming ITN while surpassing current streaming ITN models, particularly on a Vietnamese dataset, with low latency.

**Limitations:** 

**Conclusion:** This streaming ITN model can be seamlessly integrated into ASR systems, offering a robust solution for real-time applications.

**Abstract:** Inverse Text Normalization (ITN) is crucial for converting spoken Automatic Speech Recognition (ASR) outputs into well-formatted written text, enhancing both readability and usability. Despite its importance, the integration of streaming ITN within streaming ASR remains largely unexplored due to challenges in accuracy, efficiency, and adaptability, particularly in low-resource and limited-context scenarios. In this paper, we introduce a streaming pretrained language model for ITN, leveraging pretrained linguistic representations for improved robustness. To address streaming constraints, we propose Dynamic Context-Aware during training and inference, enabling adaptive chunk size adjustments and the integration of right-context information. Experimental results demonstrate that our method achieves accuracy comparable to non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese dataset, all while maintaining low latency, ensuring seamless integration into ASR systems.

</details>


### [112] [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/abs/2505.24241)

*Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** Machine Learning, Language Models, Training Efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces APEX, a method for enhancing large language model training by expanding advantageous parameters, showing significant performance improvements with reduced computational resources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As the number of parameters in language models increases for better performance, the associated computational overhead grows. Identifying and leveraging advantageous parameters could optimize performance without the need for excessive resources.

**Method:** The paper proposes Advantageous Parameter EXpansion Training (APEX), which focuses on progressively incorporating advantageous parameters into the training process, thereby enhancing their effectiveness compared to disadvantageous ones.

**Key Contributions:**

	1. Introduction of the APEX method for training language models
	2. Theoretical analysis from the perspective of matrix effective rank
	3. Demonstrated significant performance improvements with reduced parameters and training data

**Result:** APEX shows superior results in instruction tuning, outperforming full-parameter tuning with only 52% of trainable parameters, and achieves equivalent perplexity levels in continued pre-training with just 33% of the training data, leading to significant improvements in downstream tasks.

**Limitations:** 

**Conclusion:** By utilizing APEX, training efficiency is greatly enhanced without compromising model performance, highlighting the potential for optimizing resources in large language model training.

**Abstract:** Although scaling up the number of trainable parameters in both pre-training and fine-tuning can effectively improve the performance of large language models, it also leads to increased computational overhead. When delving into the parameter difference, we find that a subset of parameters, termed advantageous parameters, plays a crucial role in determining model performance. Further analysis reveals that stronger models tend to possess more such parameters. In this paper, we propose Advantageous Parameter EXpansion Training (APEX), a method that progressively expands advantageous parameters into the space of disadvantageous ones, thereby increasing their proportion and enhancing training effectiveness. Further theoretical analysis from the perspective of matrix effective rank explains the performance gains of APEX. Extensive experiments on both instruction tuning and continued pre-training demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning while using only 52% of the trainable parameters. In continued pre-training, APEX achieves the same perplexity level as conventional training with just 33% of the training data, and yields significant improvements on downstream tasks.

</details>


### [113] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)

*Nir Endy, Idan Daniel Grosbard, Yuval Ran-Milo, Yonatan Slutzky, Itay Tshuva, Raja Giryes*

**Main category:** cs.CL

**Keywords:** Mamba State-Space Model, information flow, attention mechanisms, language models, token dynamics

**Relevance Score:** 7

**TL;DR:** This paper examines the flow of information in Mamba State-Space Models compared to Transformer models, using attentional techniques for interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how factual information is processed in Mamba State-Space Models and compare it to Transformer architectures.

**Method:** The paper adapts attentional interpretability techniques, specifically the Attention Knockout methodology, to analyze the Mamba-1 and Mamba-2 models.

**Key Contributions:**

	1. Adaptation of Attention Knockout methods to Mamba models
	2. Empirical analysis of information flow in Mamba State-Space Models
	3. Identification of unique and universal patterns in information processing across models

**Result:** Information transmission dynamics were traced across tokens and layers, revealing both unique and universal patterns in how information is localized and exchanged.

**Limitations:** 

**Conclusion:** The findings offer insights into the internal operations of Mamba models and suggest inherent properties of language models in general.

**Abstract:** This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct "features" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.

</details>


### [114] [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/abs/2505.24251)

*Xiaoyu Li, Xiao Li, Li Gao, Yiding Liu, Xiaoyang Wang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-Computer Interaction, Reinforcement Learning, Goal Adaptation, Knowledge Distillation

**Relevance Score:** 9

**TL;DR:** The paper presents a two-phase framework for enhancing multi-turn conversation systems in the Baidu Search AI assistant, focusing on adaptive user interaction and real-time performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for proactive guidance in multi-turn conversation systems to enhance user interaction and adapt to changing user goals.

**Method:** A two-phase framework consisting of Goal-adaptive Supervised Fine-Tuning (G-SFT) and Click-oriented Reinforcement Learning (C-RL). G-SFT adapts to user goals and integrates knowledge from LLMs into a lightweight model, while C-RL uses user click signals for optimization.

**Key Contributions:**

	1. Proposed a dual-phase framework for proactive user guidance in multi-turn systems.
	2. Developed a knowledge distillation method for real-time interaction.
	3. Implemented reinforcement learning to enhance click-through rates.

**Result:** Achieved 86.10% accuracy in offline evaluation, improving by 23.95% over baseline, and 25.28% CTR in online deployment with a 149.06% relative improvement, while reducing inference latency by 69.55%.

**Limitations:** 

**Conclusion:** The dual-phase architecture effectively tracks user goals and optimizes interaction quality, demonstrating significant performance improvements.

**Abstract:** The evolution of Large Language Models (LLMs) has significantly advanced multi-turn conversation systems, emphasizing the need for proactive guidance to enhance users' interactions. However, these systems face challenges in dynamically adapting to shifts in users' goals and maintaining low latency for real-time interactions. In the Baidu Search AI assistant, an industrial-scale multi-turn search system, we propose a novel two-phase framework to provide proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning (G-SFT), employs a goal adaptation agent that dynamically adapts to user goal shifts and provides goal-relevant contextual information. G-SFT also incorporates scalable knowledge transfer to distill insights from LLMs into a lightweight model for real-time interaction. The second phase, Click-oriented Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically constructs preference pairs from user click signals, and proactively improves click-through rates through more engaging guidance. This dual-phase architecture achieves complementary objectives: G-SFT ensures accurate goal tracking, while C-RL optimizes interaction quality through click signal-driven reinforcement learning. Extensive experiments demonstrate that our framework achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and 25.28% CTR in online deployment (149.06% relative improvement), while reducing inference latency by 69.55% through scalable knowledge distillation.

</details>


### [115] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)

*Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim*

**Main category:** cs.CL

**Keywords:** Large Language Models, theory-of-mind, negotiation tasks, human-AI interaction, cooperative decision-making

**Relevance Score:** 9

**TL;DR:** This study explores the role of theory-of-mind (ToM) reasoning in aligning Large Language Models (LLMs) behaviors with human norms during negotiation tasks, demonstrating that enhanced ToM reasoning improves decision-making outcomes.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how ToM reasoning can enhance human-AI interaction and improve cooperative decision-making in complex social contexts like negotiation.

**Method:** Experiments conducted using the ultimatum game, initializing LLM agents with various prosocial beliefs and ToM reasoning methods, and analyzing their decision-making over 2,700 simulations.

**Key Contributions:**

	1. Demonstrates the impact of ToM reasoning on LLM decision-making
	2. Compares various prosocial beliefs and reasoning methods in LLMs
	3. Provides insights into human-AI interaction dynamics

**Result:** ToM reasoning was found to significantly improve behavior alignment and decision-making consistency in LLMs during negotiation tasks, showcasing superior performance in comparison to models without ToM reasoning.

**Limitations:** Limited capability of reasoning models compared to those incorporating ToM; results may vary based on specific negotiation contexts.

**Conclusion:** The findings suggest that ToM reasoning is essential for improving AI negotiation outcomes and aligning AI behaviors with human social norms.

**Abstract:** Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [116] [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/abs/2505.24263)

*Naila Shafirni Hidayat, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto*

**Main category:** cs.CL

**Keywords:** large language models, data leakage, n-gram, benchmark evaluation, HCI

**Relevance Score:** 9

**TL;DR:** The paper addresses the lack of transparency in training data for large language models (LLMs) by comparing techniques for detecting data leakage and proposing improved evaluation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns regarding the overlap between training and evaluation sets of LLMs and the fairness of results due to potential data leakage.

**Method:** Comparison of existing leakage detection techniques (permutation and n-gram methods) in a simulated continual pretraining setup, along with the introduction of a new lightweight method called semi-half question.

**Key Contributions:**

	1. Comparison of existing leakage detection techniques under controlled scenarios.
	2. Introduction of a lightweight leakage detection method.
	3. Creation of cleaned benchmark datasets (MMLU and HellaSwag) for reevaluation of LLMs.

**Result:** The n-gram method achieved the highest F1-score for leakage detection, and the refined techniques allow for instance-level detection while reducing computational costs.

**Limitations:** The study is focused only on specific leakage detection methods and may not cover all possible scenarios of data leakage.

**Conclusion:** The study provides a pathway for more reliable and transparent evaluations in LLMs, recommending contamination checks before releasing benchmark results.

**Abstract:** The performance of large language models (LLMs) continues to improve, as reflected in rising scores on standard benchmarks. However, the lack of transparency around training data raises concerns about potential overlap with evaluation sets and the fairness of reported results. Although prior work has proposed methods for detecting data leakage, these approaches primarily focus on identifying outliers and have not been evaluated under controlled simulated leakage conditions. In this work, we compare existing leakage detection techniques, namely permutation and n-gram-based methods, under a continual pretraining setup that simulates real-world leakage scenarios, and additionally explore a lightweight method we call semi-half question. Although semi-half offers a low-cost alternative, our analysis shows that the n-gram method consistently achieves the highest F1-score. We also refine these techniques to support instance-level detection and reduce computational overhead. Leveraging the best-performing method, we create cleaned versions of MMLU and HellaSwag, and re-evaluate several LLMs. Our findings present a practical path toward more reliable and transparent evaluations, and we recommend contamination checks as a standard step before releasing benchmark results.

</details>


### [117] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)

*Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Large Language Models, Theorem Provers

**Relevance Score:** 8

**TL;DR:** This paper explores enhancing Natural Language Inference (NLI) explanations through strategies that improve the interaction between Large Language Models (LLMs) and theorem provers (TPs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to verify and improve the logical entailment of NLI explanations while addressing the limitations in LLMs' ability to construct rigorous proofs and the risk of semantic loss during translation into formal representations.

**Method:** Investigating strategies to reduce semantic loss in autoformalisation, correct syntactic errors in logical representations, guide LLMs using logical expressions for proof generation, and enhance LLMs' interpretation of TPs' feedback for iterative refinement.

**Key Contributions:**

	1. Strategies to mitigate semantic loss in autoformalisation
	2. Approaches for correcting syntactic errors in logical representations
	3. Methods for guiding LLMs in structured proof generation

**Result:** Empirical results indicate significant improvements in autoformalisation and explanation refinement metrics across different LLMs, with enhancements over state-of-the-art models.

**Limitations:** 

**Conclusion:** The proposed methods yield better efficiency in LLM-TP interactions and reduce verification iterations, advancing NLI explanation accuracy and robustness.

**Abstract:** Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.

</details>


### [118] [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)

*Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Scientific Knowledge Update, Knowledge Preservation, Knowledge Acquisition, Knowledge Projection

**Relevance Score:** 8

**TL;DR:** Introduction of ScienceMeter, a framework for evaluating scientific knowledge update methods in LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of LLMs having outdated knowledge in scientific research and improve methods of updating this knowledge.

**Method:** The study introduces the ScienceMeter framework that evaluates three metrics: knowledge preservation, knowledge acquisition, and knowledge projection, using a dataset of scientific papers and claims.

**Key Contributions:**

	1. Introduction of a comprehensive evaluation framework for knowledge updates in LLMs called ScienceMeter.
	2. Detailed analysis of knowledge preservation, acquisition, and projection metrics.
	3. Empirical findings highlighting the limitations of current knowledge update methods across multiple scientific domains.

**Result:** The evaluation demonstrates that current knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of potential future knowledge across various domains.

**Limitations:** Current methods do not collectively achieve the desired objectives, indicating room for improvement in knowledge update mechanisms.

**Conclusion:** There is a significant need for developing robust scientific knowledge update mechanisms, as current methods fall short in achieving all objectives collectively, even for specialized LLMs.

**Abstract:** Large Language Models (LLMs) are increasingly used to support scientific research, but their knowledge of scientific advancements can quickly become outdated. We introduce ScienceMeter, a new framework for evaluating scientific knowledge update methods over scientific knowledge spanning the past, present, and future. ScienceMeter defines three metrics: knowledge preservation, the extent to which models' understanding of previously learned papers are preserved; knowledge acquisition, how well scientific claims from newly introduced papers are acquired; and knowledge projection, the ability of the updated model to anticipate or generalize to related scientific claims that may emerge in the future. Using ScienceMeter, we examine the scientific knowledge of LLMs on claim judgment and generation tasks across a curated dataset of 15,444 scientific papers and 30,888 scientific claims from ten domains including medicine, biology, materials science, and computer science. We evaluate five representative knowledge update approaches including training- and inference-time methods. With extensive experiments, we find that the best-performing knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of future knowledge. Inference-based methods work for larger models, whereas smaller models require training to achieve comparable performance. Cross-domain analysis reveals that performance on these objectives is correlated. Even when applying on specialized scientific LLMs, existing knowledge update methods fail to achieve these objectives collectively, underscoring that developing robust scientific knowledge update mechanisms is both crucial and challenging.

</details>


### [119] [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/abs/2505.24319)

*Yuntao Shi, Yi Luo, Yeyun Gong, Chen Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, text modification, document coherence, HiCaM

**Relevance Score:** 8

**TL;DR:** HiCaM is a novel framework enhancing long-form text modifications by utilizing a hierarchical summary tree and causal graph, addressing common LLM limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with effectively modifying long-form text, often leading to irrelevant alterations or missed crucial content, which impacts document coherence.

**Method:** The proposed HiCaM framework combines a hierarchical summary tree and a causal graph to improve text modification processes in LLMs.

**Key Contributions:**

	1. Introduction of the HiCaM framework for long-form text modification
	2. Development of a multi-domain dataset for evaluation
	3. Demonstrated significant improvements over existing LLMs in task performance.

**Result:** HiCaM yielded significant performance improvements, achieving a 79.50% win rate in evaluations against other strong LLMs.

**Limitations:** 

**Conclusion:** The HiCaM framework demonstrates enhanced capabilities in producing coherent modifications across various models and domains, proving effective in addressing existing LLM challenges.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success in various domains. However, when handling long-form text modification tasks, they still face two major problems: (1) producing undesired modifications by inappropriately altering or summarizing irrelevant content, and (2) missing necessary modifications to implicitly related passages that are crucial for maintaining document coherence. To address these issues, we propose HiCaM, a Hierarchical-Causal Modification framework that operates through a hierarchical summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a multi-domain dataset from various benchmarks, providing a resource for assessing its effectiveness. Comprehensive evaluations on the dataset demonstrate significant improvements over strong LLMs, with our method achieving up to a 79.50\% win rate. These results highlight the comprehensiveness of our approach, showing consistent performance improvements across multiple models and domains.

</details>


### [120] [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/abs/2505.24331)

*Fanhang Man, Huandong Wang, Jianjie Fang, Zhaoyi Deng, Baining Zhao, Xinlei Chen, Yong Li*

**Main category:** cs.CL

**Keywords:** sentiment forecasting, social media, sentiment dynamics, human response, feature extraction

**Relevance Score:** 7

**TL;DR:** The paper presents a method for predicting future user sentiment on social media regarding ongoing events.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to predict imminent sentiment on social media, which has not been widely studied despite its importance in understanding social trends and crises.

**Method:** The authors extract sentiment-related features and propose a multi-perspective role-playing framework to simulate human responses in sentiment forecasting.

**Key Contributions:**

	1. Development of a multi-perspective role-playing framework for sentiment forecasting
	2. Extraction of sentiment-related features to enhance model performance
	3. Improved prediction of user sentiment in response to ongoing events

**Result:** The proposed approach demonstrates significant improvements in sentiment forecasting accuracy at both microscopic (individual) and macroscopic (aggregate) levels.

**Limitations:** 

**Conclusion:** The research contributes to the understanding of sentiment dynamics by enabling forecasting, which can inform decision-making during events.

**Abstract:** User sentiment on social media reveals the underlying social trends, crises, and needs. Researchers have analyzed users' past messages to trace the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment of an ongoing event is rarely studied. In this paper, we address the problem of \textbf{sentiment forecasting} on social media to predict the user's future sentiment in response to the development of the event. We extract sentiment-related features to enhance the modeling skill and propose a multi-perspective role-playing framework to simulate the process of human response. Our preliminary results show significant improvement in sentiment forecasting on both microscopic and macroscopic levels.

</details>


### [121] [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/abs/2505.24332)

*Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** Large language models, Information seeking, Reinforcement learning, Adaptability, Dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces WebPuzzle, a new dataset for fostering information-seeking behavior in LLMs, and proposes DeepDiver, a reinforcement learning framework that enhances search intensity scaling to improve performance in open-web question answering.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs face challenges in iterative evidence gathering and reflective reasoning in open-web environments due to reliance on static prompting and constrained training methods.

**Method:** The paper introduces WebPuzzle, a dataset with 24K training instances and 275 test questions, and proposes the DeepDiver RL framework to promote adaptive search policies in ambiguous conditions.

**Key Contributions:**

	1. Introduction of the WebPuzzle dataset for open-web information-seeking tasks
	2. Development of the DeepDiver RL framework to enhance search intensity scaling
	3. Demonstration of effective generalization from closed-form QA to long-form writing tasks

**Result:** Pangu-7B-Reasoner, enhanced by DeepDiver, performs on par with the 671B-parameter DeepSeek-R1 in real-world tasks.

**Limitations:** 

**Conclusion:** DeepDiver demonstrates the capability of improving search intensity scaling and generalizes well across various tasks, providing a basis for future research in adaptive information seeking for LLMs.

**Abstract:** Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing methods rely on static prompting rules or training with Wikipedia-based corpora and retrieval environments, limiting adaptability to the real-world web environment where ambiguity, conflicting evidence, and noise are prevalent. These constrained training settings hinder LLMs from learning to dynamically decide when and where to search, and how to adjust search depth and frequency based on informational demands. We define this missing capacity as Search Intensity Scaling (SIS)--the emergent skill to intensify search efforts under ambiguous or conflicting conditions, rather than settling on overconfident, under-verification answers.   To study SIS, we introduce WebPuzzle, the first dataset designed to foster information-seeking behavior in open-world internet environments. WebPuzzle consists of 24K training instances and 275 test questions spanning both wiki-based and open-web queries. Building on this dataset, we propose DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by encouraging adaptive search policies through exploration under a real-world open-web environment. Experimental results show that Pangu-7B-Reasoner empowered by DeepDiver achieve performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from cold-start supervised fine-tuning to a carefully designed RL phase, and present that its capability of SIS generalizes from closed-form QA to open-ended tasks such as long-form writing. Our contributions advance adaptive information seeking in LLMs and provide a valuable benchmark and dataset for future research.

</details>


### [122] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)

*Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu*

**Main category:** cs.CL

**Keywords:** toxic content detection, multimodal Chinese, large language models, in-context learning, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenges of detecting toxic content in Chinese language using language models, highlighting the impact of perturbation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting toxic content is important for ensuring safety in language use, yet many state-of-the-art models struggle with perturbed toxic Chinese texts.

**Method:** We propose a taxonomy of perturbation strategies and create a dataset to benchmark 9 state-of-the-art language models for their ability to detect perturbed toxic Chinese content.

**Key Contributions:**

	1. Proposed a taxonomy of perturbation strategies for toxic Chinese content.
	2. Curated a dataset to benchmark LLMs on their ability to detect perturbed toxic content.
	3. Identified the tendency of models to overcorrect normal content when fine-tuned with few perturbed examples.

**Result:** Our findings indicate that LLMs are less effective in detecting perturbed multimodal toxic content and that in-context learning and supervised fine-tuning with a few perturbed examples can lead to overcorrection.

**Limitations:** The paper primarily focuses on Chinese language models and may have limited applicability to other languages or contexts.

**Conclusion:** The study reveals critical issues in the deployment of LLMs for Chinese toxic content detection and suggests the need for more robust model training strategies.

**Abstract:** Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs "overcorrect'': misidentify many normal Chinese contents as toxic.

</details>


### [123] [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)

*Yangui Fang, Baixu Cheng, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong*

**Main category:** cs.CL

**Keywords:** Speech Recognition, Error Correction, Large Language Models, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introducing a framework for correcting ASR errors using LLMs without additional training, minimizing hallucinations and ensuring text accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve ASR error correction while avoiding the hallucination problems of traditional LLMs which can alter correct texts.

**Method:** The Reliable LLM Correction Framework (RLLM-CF) consists of three stages: error pre-detection, iterative correction via chain-of-thought sub-tasks, and reasoning process verification.

**Key Contributions:**

	1. Introduction of the RLLM-CF for ASR error correction
	2. Demonstration of significant CER/WER reductions
	3. Proving effective correction without the necessity for training or fine-tuning

**Result:** The GPT-4o model, enhanced by the RLLM-CF framework, achieves significant reductions in Character Error Rate (CER) and Word Error Rate (WER) across multiple ASR datasets.

**Limitations:** 

**Conclusion:** The proposed framework effectively addresses the issues related to LLM hallucinations and improves the reliability of ASR error correction without additional data or fine-tuning.

**Abstract:** Automatic Speech Recognition (ASR) error correction aims to correct recognition errors while preserving accurate text. Although traditional approaches demonstrate moderate effectiveness, LLMs offer a paradigm that eliminates the need for training and labeled data. However, directly using LLMs will encounter hallucinations problem, which may lead to the modification of the correct text. To address this problem, we propose the Reliable LLM Correction Framework (RLLM-CF), which consists of three stages: (1) error pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3) reasoning process verification. The advantage of our method is that it does not require additional information or fine-tuning of the model, and ensures the correctness of the LLM correction under multi-pass programming. Experiments on AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.

</details>


### [124] [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/abs/2505.24354)

*Qianqian Zhang, Jiajia Liao, Heting Ying, Yibo Ma, Haozhan Shen, Jingcheng Li, Peng Liu, Lu Zhang, Chunxin Fang, Kyusong Lee, Ruochen Xu, Tiancheng Zhao*

**Main category:** cs.CL

**Keywords:** large language models, language agents, evaluation frameworks, reasoning, modularity

**Relevance Score:** 8

**TL;DR:** AGORA is a framework for developing language agents using LLMs, focusing on modularity, reusable algorithms, and robust evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant challenges in developing language agents, including engineering overhead, lack of standardized components, and insufficient evaluation methods.

**Method:** The AGORA framework uses a modular architecture with a graph-based workflow engine, efficient memory management, and a comprehensive suite of reusable agent algorithms for state-of-the-art reasoning approaches.

**Key Contributions:**

	1. Modular architecture and graph-based workflow engine
	2. Suite of reusable agent algorithms
	3. Rigorous evaluation framework for comparative analysis

**Result:** Extensive experiments highlight that while advanced reasoning methods can boost agent capabilities, simpler approaches like Chain-of-Thought perform well with lower computational costs.

**Limitations:** 

**Conclusion:** AGORA facilitates easier language agent development and standardized evaluation, promoting reproducible research in the field.

**Abstract:** Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.

</details>


### [125] [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/abs/2505.24355)

*Sihan Tan, Taro Miyazaki, Kazuhiro Nakadai*

**Main category:** cs.CL

**Keywords:** Sign Language Translation, Multilingual Resources, Machine Learning

**Relevance Score:** 6

**TL;DR:** This paper presents a multilingual gloss-free model for sign language translation (SLT) that addresses challenges in translating multiple sign languages into spoken languages by leveraging multilingual resources and achieving competitive performance across various SLT tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the communication gap between sign and spoken communities and to enhance accessibility in sign language translation by exploring multilingual sign language translation (MLSLT).

**Method:** A multilingual gloss-free model with dual CTC objectives is proposed for token-level sign language identification and spoken text generation, capable of handling one-to-one, many-to-one, and many-to-many SLT tasks.

**Key Contributions:**

	1. Introduction of a multilingual gloss-free model for SLT
	2. Support for multiple SLT tasks including many-to-one and many-to-many
	3. Competitive performance on established SLT benchmarks

**Result:** The model supports 10 sign languages and achieves competitive performance compared to state-of-the-art methods on benchmarks such as multilingual SP-10, PHOENIX14T, and CSL-Daily.

**Limitations:** 

**Conclusion:** The proposed model effectively addresses low-resource issues and enhances accessibility in sign language translation through its multilingual capabilities.

**Abstract:** Sign Language Translation (SLT) aims to convert sign language (SL) videos into spoken language text, thereby bridging the communication gap between the sign and the spoken community. While most existing works focus on translating a single sign language into a single spoken language (one-to-one SLT), leveraging multilingual resources could mitigate low-resource issues and enhance accessibility. However, multilingual SLT (MLSLT) remains unexplored due to language conflicts and alignment difficulties across SLs and spoken languages. To address these challenges, we propose a multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and many-to-many SLT tasks, achieving competitive performance compared to state-of-the-art methods on three widely adopted benchmarks: multilingual SP-10, PHOENIX14T, and CSL-Daily.

</details>


### [126] [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)

*Anum Afzal, Florian Matthes, Gal Chechik, Yftah Ziser*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Probing Classifier, Zero-Shot Learning, Early Stopping, LLM

**Relevance Score:** 7

**TL;DR:** This paper explores predicting the success of zero-shot Chain-of-Thought (CoT) processes early using a probing classifier based on LLMs, discovering that early representations carry critical information for reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating whether the success of a zero-shot CoT process can be anticipated before generating any tokens.

**Method:** Utilized a probing classifier based on LLM representations to evaluate the success of CoT processes before token generation, contrasting it with a BERT-based baseline that uses generated tokens.

**Key Contributions:**

	1. Demonstrated predictive capability of early representations in CoT processes
	2. Showed that early stopping can improve performance over no CoT
	3. Provided insights for optimizing CoT efficiency using probing classifiers

**Result:** The probing classifier outperformed the BERT baseline and showed strong predictive capability from early representations, indicating effective reasoning can be identified early and that truncated reasoning can still enhance performance over no CoT.

**Limitations:** 

**Conclusion:** Optimizing CoT efficiency may be possible by utilizing early representations to guide methods, like supervised or reinforcement learning, for effective early stopping in reasoning processes.

**Abstract:** We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.\footnote{Code and data is available at \href{https://github.com/anum94/CoTpred}{\texttt{github.com/anum94/CoTpred}}.

</details>


### [127] [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/abs/2505.24377)

*Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, Yun-Nung Chen*

**Main category:** cs.CL

**Keywords:** large language models, external knowledge, knowledge graphs, structured data, natural language reasoning

**Relevance Score:** 9

**TL;DR:** This paper systematically explores strategies for enhancing large language models (LLMs) with external knowledge, focusing on structured data like tables and knowledge graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of LLMs, specifically their limited memory and hallucination tendencies, by exploring how external knowledge can improve context-based reasoning.

**Method:** The paper categorizes external knowledge into unstructured and structured data, focusing primarily on structured knowledge such as tables and knowledge graphs, and reviewing various integration methods with LLMs.

**Key Contributions:**

	1. Systematic taxonomy of external knowledge types for LLMs.
	2. Detailed analysis of integration paradigms for tables and knowledge graphs with LLMs.
	3. Comparative insights into the trade-offs in performance and interpretability of enhanced LLMs.

**Result:** The comparative analysis reveals trade-offs in interpretability, scalability, and performance when integrating structured knowledge into LLMs, providing valuable insights for future development.

**Limitations:** 

**Conclusion:** The findings suggest a pathway for creating more trustworthy and generalizable knowledge-enhanced LLMs by leveraging structured external knowledge effectively.

**Abstract:** Recent advancements in large language models (LLMs) have enhanced natural-language reasoning. However, their limited parametric memory and susceptibility to hallucination present persistent challenges for tasks requiring accurate, context-based inference. To overcome these limitations, an increasing number of studies have proposed leveraging external knowledge to enhance LLMs. This study offers a systematic exploration of strategies for using external knowledge to enhance LLMs, beginning with a taxonomy that categorizes external knowledge into unstructured and structured data. We then focus on structured knowledge, presenting distinct taxonomies for tables and knowledge graphs (KGs), detailing their integration paradigms with LLMs, and reviewing representative methods. Our comparative analysis further highlights the trade-offs among interpretability, scalability, and performance, providing insights for developing trustworthy and generalizable knowledge-enhanced LLMs.

</details>


### [128] [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)

*Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, Pengcheng Huang, Qiushi Xiong, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, reasoning paths

**Relevance Score:** 9

**TL;DR:** ClueAnchor enhances Retrieval-Augmented Generation (RAG) for better reasoning by extracting key clues from retrieved documents and optimizing reasoning paths based on knowledge configurations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve factuality in RAG systems, which often underutilize retrieved documents and struggle to integrate implicit and obscure evidence for reasoning.

**Method:** ClueAnchor extracts key clues from retrieved content to generate multiple reasoning paths. It optimizes the model by selecting the most effective path through reward-based preference optimization.

**Key Contributions:**

	1. Proposed a novel ClueAnchor framework for RAG.
	2. Introduced clue-anchored reasoning exploration and optimization.
	3. Demonstrated significant performance improvements in reasoning robustness.

**Result:** ClueAnchor significantly outperforms existing RAG baselines in terms of reasoning completeness and robustness, and demonstrates resilience to noise in the retrieved content.

**Limitations:** 

**Conclusion:** ClueAnchor effectively identifies supporting evidence without requiring explicit clue supervision during inference, enhancing the overall performance of RAG systems.

**Abstract:** Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge to improve factuality. However, existing RAG systems frequently underutilize the retrieved documents, failing to extract and integrate the key clues needed to support faithful and interpretable reasoning, especially in cases where relevant evidence is implicit, scattered, or obscured by noise. To address this issue, we propose ClueAnchor, a novel framework for enhancing RAG via clue-anchored reasoning exploration and optimization. ClueAnchor extracts key clues from retrieved content and generates multiple reasoning paths based on different knowledge configurations, optimizing the model by selecting the most effective one through reward-based preference optimization. Experiments show that ClueAnchor significantly outperforms prior RAG baselines in reasoning completeness and robustness. Further analysis confirms its strong resilience to noisy or partially relevant retrieved content, as well as its capability to identify supporting evidence even in the absence of explicit clue supervision during inference.

</details>


### [129] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)

*Eojin Kang, Juae Kim*

**Main category:** cs.CL

**Keywords:** multilingual models, large language models, cross-lingual transfer, Language-to-Thought, fact recall

**Relevance Score:** 9

**TL;DR:** This paper investigates how multilingual large language models (LLMs) can effectively transfer factual knowledge across languages using a novel prompting strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistent recall of factual knowledge by multilingual LLMs depending on the input language, particularly focusing on the effectiveness of English-based prompting.

**Method:** The paper proposes a Language-to-Thought (L2T) prompting strategy that examines the interaction between the input language, the internal cognitive processes of the model, and the factual knowledge expected in tasks.

**Key Contributions:**

	1. Introduction of the Language-to-Thought (L2T) prompting strategy
	2. Challenge to the dominance of English-based prompting
	3. Experimental findings emphasizing the importance of cognitive alignment for knowledge transfer

**Result:** Experimental results reveal that English-based prompting does not consistently outperform non-English methods and highlight the importance of aligning internal cognitive thought with the task-specific knowledge for better cross-lingual transfer.

**Limitations:** 

**Conclusion:** The application of the L2T strategy during training can reduce the reliance on the input language and enhance knowledge integration across languages without relying on translation.

**Abstract:** Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available.

</details>


### [130] [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/abs/2505.24423)

*Zhiwei Liu, Lingfei Qian, Qianqian Xie, Jimin Huang, Kailai Yang, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** affective analysis, language models, multimodal, sentiment analysis, emotion detection

**Relevance Score:** 7

**TL;DR:** Introduction of the MMAFFBen benchmark for multilingual multimodal affective analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive evaluation benchmarks for affective analysis in language models and vision-language models.

**Method:** Development of the MMAFFBen benchmark encompassing text, image, and video modalities across 35 languages for various affective analysis tasks, along with the construction of the MMAFFIn dataset for fine-tuning language models.

**Key Contributions:**

	1. Introduction of the MMAFFBen benchmark for multilingual multimodal affective analysis.
	2. Creation of the MMAFFIn dataset for fine-tuning LMs on affective tasks.
	3. Systematic evaluation of LMs including GPT-4o-mini in affective understanding.

**Result:** MMAFFBen provides the first extensive open-source benchmark and includes a systematic comparison of various language models' capabilities in affective analysis.

**Limitations:** Work is in progress, implying ongoing development and possible future updates to the benchmark.

**Conclusion:** This work lays the groundwork for further research in affective analysis using multimodal data and language models.

**Abstract:** Large language models and vision-language models (which we jointly call LMs) have transformed NLP and CV, demonstrating remarkable potential across various fields. However, their capabilities in affective analysis (i.e. sentiment analysis and emotion detection) remain underexplored. This gap is largely due to the absence of comprehensive evaluation benchmarks, and the inherent complexity of affective analysis tasks. In this paper, we introduce MMAFFBen, the first extensive open-source benchmark for multilingual multimodal affective analysis. MMAFFBen encompasses text, image, and video modalities across 35 languages, covering four key affective analysis tasks: sentiment polarity, sentiment intensity, emotion classification, and emotion intensity. Moreover, we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate various representative LMs, including GPT-4o-mini, providing a systematic comparison of their affective understanding capabilities. This project is available at https://github.com/lzw108/MMAFFBen.

</details>


### [131] [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/abs/2505.24427)

*Christopher Bagdon, Aidan Combs, Carina Silberer, Roman Klinger*

**Main category:** cs.CL

**Keywords:** Emotion expression, Social media, Model performance, Multimodal analysis, Content creation

**Relevance Score:** 7

**TL;DR:** The paper compares study-created and genuine social media posts labeled for emotion expression, highlighting differences that affect model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if and how study-created content differs from genuine content and the impact on emotion expression models.

**Method:** We collected both study-created and genuine multimodal social media posts and compared them across various dimensions, including model performance.

**Key Contributions:**

	1. Comparison of study-created and genuine content for emotion expression.
	2. Identification of key differences in length, content reliance, and event focus.
	3. Analysis of demographic differences in participant behavior.

**Result:** Study-created posts are longer, more reliant on text than images, and focus on emotion-prototypical events compared to genuine posts. The demographics of participants donating versus creating posts differ.

**Limitations:** The study may not account for all potential demographic variations and their impact on model training.

**Conclusion:** While study-created data can train models that generalize to genuine data, accurate effectiveness assessments require genuine data.

**Abstract:** Accurate modeling of subjective phenomena such as emotion expression requires data annotated with authors' intentions. Commonly such data is collected by asking study participants to donate and label genuine content produced in the real world, or create content fitting particular labels during the study. Asking participants to create content is often simpler to implement and presents fewer risks to participant privacy than data donation. However, it is unclear if and how study-created content may differ from genuine content, and how differences may impact models. We collect study-created and genuine multimodal social media posts labeled for emotion and compare them on several dimensions, including model performance. We find that compared to genuine posts, study-created posts are longer, rely more on their text and less on their images for emotion expression, and focus more on emotion-prototypical events. The samples of participants willing to donate versus create posts are demographically different. Study-created data is valuable to train models that generalize well to genuine data, but realistic effectiveness estimates require genuine data.

</details>


### [132] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)

*Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou*

**Main category:** cs.CL

**Keywords:** LLMs, unlearning, adversarial robustness, sparse autoencoders, machine learning

**Relevance Score:** 8

**TL;DR:** SSPU is a novel framework for interpretable and robust unlearning in LLMs that leverages sparse autoencoder features for targeted parameter updates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy and safety concerns in LLMs regarding selective knowledge removal, as existing strategies lack interpretability and robustness against adversarial prompts.

**Method:** The SSPU framework uses a three-stage pipeline involving data-driven layer selection, QR decomposition for subspace construction, and constrained optimization for unlearning while preserving knowledge.

**Key Contributions:**

	1. Introduces SSPU framework for targeted unlearning in LLMs.
	2. Implements an interpretable optimization approach using SAE features.
	3. Demonstrates improved adversarial robustness and reduced harmful knowledge retention.

**Result:** SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline and improves robustness against adversarial jailbreak prompts.

**Limitations:** The effectiveness of SSPU may vary depending on the model architecture and specific datasets used.

**Conclusion:** The study reveals limitations of prior unlearning methods and demonstrates that interpretable subspace-guided optimization can enhance model behavior control during unlearning.

**Abstract:** Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an "irrelevant" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.

</details>


### [133] [Exploring the Impact of Occupational Personas on Domain-Specific QA](https://arxiv.org/abs/2505.24448)

*Eojin Kang, Jaehyuk Yu, Juae Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, personas, question-answering, domain-specific, cognitive constraints

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of different types of personas on the performance of Large Language Models in domain-specific question-answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how personas affect the interaction and specialized performance of Large Language Models (LLMs) in domain-specific tasks, particularly in question-answering scenarios.

**Method:** The study analyzes the effects of two types of personas—Profession-Based Personas (PBPs) and Occupational Personality-Based Personas (OPBPs)—through empirical evaluations across various scientific domains.

**Key Contributions:**

	1. Introduces and tests Profession-Based and Occupational Personality-Based personas in the context of LLMs.
	2. Provides empirical evidence showing that PBPs can improve QA accuracy, while OPBPs may hinder performance.
	3. Suggests directions for future research on nuanced persona representations in LLMs.

**Result:** While Profession-Based Personas slightly improve accuracy in domain-specific QA tasks, Occupational Personality-Based Personas often degrade performance despite being semantically related to the tasks, highlighting the complexity of persona impact.

**Limitations:** Results are limited to specific scientific domains and may not generalize across all areas of inquiry.

**Conclusion:** Persona relevance does not ensure effective knowledge utilization in LLMs and may introduce cognitive constraints that impede optimal performance; further research is needed to refine persona representations for better alignment with human reasoning.

**Abstract:** Recent studies on personas have improved the way Large Language Models (LLMs) interact with users. However, the effect of personas on domain-specific question-answering (QA) tasks remains a subject of debate. This study analyzes whether personas enhance specialized QA performance by introducing two types of persona: Profession-Based Personas (PBPs) (e.g., scientist), which directly relate to domain expertise, and Occupational Personality-Based Personas (OPBPs) (e.g., scientific person), which reflect cognitive tendencies rather than explicit expertise. Through empirical evaluations across multiple scientific domains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs often degrade performance, even when semantically related to the task. Our findings suggest that persona relevance alone does not guarantee effective knowledge utilization and that they may impose cognitive constraints that hinder optimal knowledge application. Future research can explore how nuanced distinctions in persona representations guide LLMs, potentially contributing to reasoning and knowledge retrieval that more closely mirror human social conceptualization.

</details>


### [134] [When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways](https://arxiv.org/abs/2505.24449)

*Kailin Jiang, Yuntao Du, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, Qing Li*

**Main category:** cs.CL

**Keywords:** multimodal models, evolving knowledge, knowledge injection, continual learning, catastrophic forgetting

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenges of multimodal knowledge injection in language/multimodal models (LLMs/LMMs), proposing the EVOKE benchmark to evaluate their ability to incorporate evolving knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs and LMMs struggle with catastrophic forgetting and maintaining consistency with real-world updates, highlighting the need for effective evolving knowledge injection methods.

**Method:** The authors introduce the EVOKE benchmark for assessing multimodal evolving knowledge injection and evaluate existing methods while analyzing the impact of text and image augmentation during training.

**Key Contributions:**

	1. Introduction of the EVOKE benchmark
	2. Identification of challenges in multimodal evolving knowledge injection
	3. Proposed methods to enhance performance through continual learning techniques

**Result:** Existing knowledge injection techniques perform poorly on evolving knowledge, and supervised fine-tuning often leads to catastrophic forgetting.

**Limitations:** Current methods for evolving knowledge injection show significant limitations; further research is necessary.

**Conclusion:** The paper concludes that there are significant limitations in current approaches to evolving knowledge injection, suggesting pathways for improvement through text augmentation and continual learning methods.

**Abstract:** Large language/multimodal models (LLMs/LMMs) store extensive pre-trained knowledge but struggle to maintain consistency with real-world updates, making it difficult to avoid catastrophic forgetting while acquiring evolving knowledge. Previous work focused on constructing textual knowledge datasets and exploring knowledge injection in LLMs, lacking exploration of multimodal evolving knowledge injection in LMMs. To address this, we propose the EVOKE benchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in real-world scenarios. Meanwhile, a comprehensive evaluation of multimodal evolving knowledge injection revealed two challenges: (1) Existing knowledge injection methods perform terribly on evolving knowledge. (2) Supervised fine-tuning causes catastrophic forgetting, particularly instruction following ability is severely compromised. Additionally, we provide pathways and find that: (1) Text knowledge augmentation during the training phase improves performance, while image augmentation cannot achieve it. (2) Continual learning methods, especially Replay and MoELoRA, effectively mitigate forgetting. Our findings indicate that current knowledge injection methods have many limitations on evolving knowledge, which motivates further research on more efficient and stable knowledge injection methods.

</details>


### [135] [Domain Pre-training Impact on Representations](https://arxiv.org/abs/2505.24455)

*Cesar Gonzalez-Gutierrez, Ariadna Quattoni*

**Main category:** cs.CL

**Keywords:** transformer, pre-training, representation quality, corpus analysis, machine learning

**Relevance Score:** 6

**TL;DR:** The study examines how the pre-training corpus affects transformer representation quality, highlighting the efficacy of specialized corpora.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how different pre-training corpora impact the quality of learned representations in transformers.

**Method:** Empirical analysis comparing transformer representations learned from various pre-training corpora (specialized vs. generic).

**Key Contributions:**

	1. Insights into the impact of pre-training corpus on transformer models
	2. Empirical evidence of effective representations from specialized corpora
	3. Guidelines for corpus selection based on task similarity

**Result:** Demonstrated that a small, specialized corpus can produce effective representations; success depends on the similarity between the task and the corpus.

**Limitations:** 

**Conclusion:** Combining generic and specialized corpora can be beneficial, contingent on distributional similarity to target tasks.

**Abstract:** This empirical study analyzes the effects of the pre-training corpus on the quality of learned transformer representations. We focus on the representation quality induced solely through pre-training. Our experiments show that pre-training on a small, specialized corpus can yield effective representations, and that the success of combining a generic and a specialized corpus depends on the distributional similarity between the target task and the specialized corpus.

</details>


### [136] [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/abs/2505.24456)

*Emilio Villa-Cueva, Sholpan Bolatzhanova, Diana Turmakhan, Kareem Elzeky, Henok Biadglign Ademtew, Alham Fikri Aji, Israel Abebe Azime, Jinheon Baek, Frederico Belcavello, Fermin Cristobal, Jan Christian Blaise Cruz, Mary Dabre, Raj Dabre, Toqeer Ehsan, Naome A Etori, Fauzan Farooqui, Jiahui Geng, Guido Ivetta, Thanmay Jayakumar, Soyeong Jeong, Zheng Wei Lim, Aishik Mandal, Sofia Martinelli, Mihail Minkov Mihaylov, Daniil Orel, Aniket Pramanick, Sukannya Purkayastha, Israfel Salazar, Haiyue Song, Tiago Timponi Torrent, Debela Desalegn Yadeta, Injy Hamed, Atnafu Lambebo Tonja, Thamar Solorio*

**Main category:** cs.CL

**Keywords:** machine translation, multimodal translation, cultural context, Vision Language Models, CaMMT

**Relevance Score:** 8

**TL;DR:** This paper explores the use of images as cultural context to improve machine translation quality through a dataset called CaMMT, which includes images and captions in multiple languages.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges cultural content presents to machine translation systems due to differences in cultural conceptualizations which can lead to misinterpretations in language alone.

**Method:** The authors introduce CaMMT, a benchmark dataset containing over 5,800 triples of images and parallel captions in English and regional languages to evaluate Vision Language Models (VLMs) in both text-only and text+image settings.

**Key Contributions:**

	1. Introduction of CaMMT for multimodal translation evaluation
	2. Demonstration of improved translation quality with visual context
	3. Identification of specific areas, such as CSIs, where images enhance language translation

**Result:** The evaluation shows that visual context significantly enhances translation quality, particularly in navigating Culturally-Specific Items (CSIs), resolving ambiguities, and ensuring proper gender usage.

**Limitations:** The study may be limited by the specific dataset and models used, potentially affecting generalizability across different languages or cultures.

**Conclusion:** By releasing the CaMMT dataset, the authors aim to foster progress in developing and assessing multimodal translation systems that respect cultural nuances and regional differences.

**Abstract:** Cultural content poses challenges for machine translation systems due to the differences in conceptualizations between cultures, where language alone may fail to convey sufficient context to capture region-specific meanings. In this work, we investigate whether images can act as cultural context in multimodal translation. We introduce CaMMT, a human-curated benchmark of over 5,800 triples of images along with parallel captions in English and regional languages. Using this dataset, we evaluate five Vision Language Models (VLMs) in text-only and text+image settings. Through automatic and human evaluations, we find that visual context generally improves translation quality, especially in handling Culturally-Specific Items (CSIs), disambiguation, and correct gender usage. By releasing CaMMT, we aim to support broader efforts in building and evaluating multimodal translation systems that are better aligned with cultural nuance and regional variation.

</details>


### [137] [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)

*Hieu Tran, Phuong-Anh Nguyen-Le, Huy Nghiem, Quang-Nhan Nguyen, Wei Ai, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Machine Translation, Code-Mixing, Low-Resource Languages, Data Augmentation, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper presents VietMix, a parallel corpus of code-mixed Vietnamese text and its English translations, alongside a synthetic data generation pipeline to enhance machine translation for low-resource languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve machine translation systems which struggle with code-mixed inputs for low-resource languages.

**Method:** The authors curated a parallel corpus of code-mixed Vietnamese text and expert English translations and developed a synthetic data generation pipeline with filtering mechanisms for plausibility and appropriateness.

**Key Contributions:**

	1. Introduction of VietMix corpus for code-mixed translation
	2. Development of a complementary synthetic data generation pipeline
	3. Demonstration of performance improvements in machine translation models

**Result:** The augmented models achieved a translation quality score of up to 71.84 on COMETkiwi and 81.77 on XCOMET, outperforming seed fine-tuned models in nearly 49% of LLM assessments.

**Limitations:** 

**Conclusion:** The VietMix resource and augmentation methodology enhance ecological validity in neural machine translation evaluations and provide a framework for code-mixed translations in other low-resource languages.

**Abstract:** Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.

</details>


### [138] [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)

*Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** code-integrated reasoning, tool-augmented reinforcement learning, training stability, mathematical reasoning

**Relevance Score:** 7

**TL;DR:** This paper presents a systematic approach to improve tool-augmented reinforcement learning for code-integrated reasoning, enhancing training stability and effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate code-integrated reasoning capabilities of models and address instability in tool-augmented reinforcement learning (RL).

**Method:** The authors developed enhanced training strategies that balance exploration and stability, focusing on progressively building tool-use capabilities while improving reasoning performance through experiments on mathematical reasoning benchmarks.

**Key Contributions:**

	1. Systematic approach to enhance tool-augmented RL stability
	2. Demonstrated significant performance improvements on benchmarks
	3. In-depth analysis of code-integrated reasoning's effects

**Result:** The proposed model demonstrates significant performance improvements compared to multiple competitive baselines on five mainstream mathematical reasoning benchmarks.

**Limitations:** 

**Conclusion:** The paper reveals key insights into code-integrated reasoning, including the extension of model's capability boundaries and improvements in reasoning efficiency through code execution.

**Abstract:** In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.

</details>


### [139] [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)

*Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu*

**Main category:** cs.CL

**Keywords:** Large Language Models, cognitive reinforcement learning, social intelligence

**Relevance Score:** 9

**TL;DR:** This paper presents TimeHC-RL, a novel framework to enhance the social intelligence of Large Language Models (LLMs) through a temporal-aware and hierarchical approach to cognitive reinforcement learning.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve LLMs' cognitive abilities in social domains is crucial, particularly since current research mainly focuses on IQ-related tasks such as mathematics and coding.

**Method:** The proposed TimeHC-RL method incorporates temporal awareness into hierarchical reinforcement learning, aimed at leveraging different cognitive modes (System 1 and System 2) to enhance LLMs' performance in social intelligence tasks.

**Key Contributions:**

	1. Introduction of TimeHC-RL for enhancing LLM social intelligence
	2. Experimental validation against existing methods
	3. Uncovering insights into LLM cognitive development in social contexts

**Result:** Experimental results demonstrate that TimeHC-RL outperforms the traditional System 2 RL method and enhances the social intelligence capabilities of a 7B model to a level competitive with advanced models like DeepSeek-R1 and OpenAI-O3.

**Limitations:** 

**Conclusion:** TimeHC-RL provides valuable insights into improving LLMs' social cognition through experimental validation and reveals the importance of incorporating diverse cognitive strategies in the learning process.

**Abstract:** Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.

</details>


### [140] [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)

*Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli*

**Main category:** cs.CL

**Keywords:** Generative AI, Large Language Models, Machine-Generated Text, Detection methods, Adversarial attacks

**Relevance Score:** 9

**TL;DR:** This paper presents a pipeline to test the resilience of state-of-the-art Machine-Generated Text (MGT) detectors against adversarial attacks, revealing their vulnerabilities and the need for improved detection methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of Generative AI and LLMs, concerns over misinformation from synthetic content have surged, necessitating robust benchmarks for detecting Machine-Generated Text.

**Method:** A pipeline that fine-tunes language models using Direct Preference Optimization to alter MGT towards human-like text, subsequently testing the resilience of various MGT detectors.

**Key Contributions:**

	1. Development of a testing pipeline for MGT detectors
	2. Fine-tuning approach to produce human-like text from MGT
	3. Analysis of weaknesses in existing MGT detection methods

**Result:** MGT detectors demonstrated significant vulnerabilities, being easily fooled with a limited number of examples, leading to poor detection performance.

**Limitations:** 

**Conclusion:** The findings underscore the necessity for enhancing MGT detection methods to ensure robustness against linguistic adversarial attacks.

**Abstract:** Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.

</details>


### [141] [Limited-Resource Adapters Are Regularizers, Not Linguists](https://arxiv.org/abs/2505.24525)

*Marcell Fekete, Nathaniel R. Robinson, Ernests Lavrinovics, E. Djeride Jean-Baptiste, Raj Dabre, Johannes Bjerva, Heather Lent*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, low-resource languages, machine translation, adapter souping, cross-attention

**Relevance Score:** 4

**TL;DR:** This paper investigates an adapter souping method and cross-attention fine-tuning in improving low-resource Creole languages MT. It reveals surprising insights about adapter performance and the role of linguistic relatedness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance low-resource language technologies by leveraging cross-lingual transfer from high-resource languages and to understand the effectiveness of adapter souping and cross-attention in machine translation.

**Method:** The study employs an adapter souping method alongside cross-attention fine-tuning on a pre-trained machine translation model for three low-resource Creole languages.

**Key Contributions:**

	1. Introduction of adapter souping combined with cross-attention fine-tuning for low-resource MT
	2. Findings challenge assumptions about linguistic relatedness and adapter performance
	3. Analysis supports the hypothesis of regularization benefits from adapters.

**Result:** The approach shows substantial performance improvements over the baselines, despite findings that linguistic relatedness does not directly correlate with adapter performance.

**Limitations:** Limited to three low-resource Creole languages; findings may not generalize to all language pairs.

**Conclusion:** The benefits of adapters seem to derive from parameter regularization rather than meaningful cross-linguistic information transfer; neural language processing success factors are complex and not fully intuitive.

**Abstract:** Cross-lingual transfer from related high-resource languages is a well-established strategy to enhance low-resource language technologies. Prior work has shown that adapters show promise for, e.g., improving low-resource machine translation (MT). In this work, we investigate an adapter souping method combined with cross-attention fine-tuning of a pre-trained MT model to leverage language transfer for three low-resource Creole languages, which exhibit relatedness to different language groups across distinct linguistic dimensions. Our approach improves performance substantially over baselines. However, we find that linguistic relatedness -- or even a lack thereof -- does not covary meaningfully with adapter performance. Surprisingly, our cross-attention fine-tuning approach appears equally effective with randomly initialized adapters, implying that the benefit of adapters in this setting lies in parameter regularization, and not in meaningful information transfer. We provide analysis supporting this regularization hypothesis. Our findings underscore the reality that neural language processing involves many success factors, and that not all neural methods leverage linguistic knowledge in intuitive ways.

</details>


### [142] [DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance](https://arxiv.org/abs/2505.24532)

*Ali Khoramfar, Ali Ramezani, Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi, Heshaam Faili*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bloom's taxonomy, Deep reasoning, Cognitive benchmarks, Evaluation framework

**Relevance Score:** 8

**TL;DR:** DeepQuestion, a new framework for augmenting datasets and evaluating LLMs, reveals significant performance drops in higher-order tasks, showing the need for better cognitive benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap of LLMs on real-world tasks and highlight the importance of cognitively diverse benchmarks.

**Method:** Developed DeepQuestion to augment datasets using Bloom's taxonomy and create questions that assess evaluative and creative skills in LLMs.

**Key Contributions:**

	1. Introduction of DeepQuestion framework
	2. Creation of novel benchmarks based on Bloom's taxonomy
	3. Demonstrated significant performance drops in LLMs on higher-order tasks

**Result:** Experiments showed up to 70% accuracy loss in higher-order tasks across ten different LLMs, revealing gaps in deep reasoning capabilities.

**Limitations:** The study only includes specific open-source and proprietary models, which may not represent all LLMs.

**Conclusion:** There is a pressing need for cognitively diverse benchmarks to improve LLM performance in complex reasoning tasks.

**Abstract:** LLMs often excel on standard benchmarks but falter on real-world tasks. We introduce DeepQuestion, a scalable automated framework that augments existing datasets based on Bloom's taxonomy and creates novel questions that trace original solution paths to probe evaluative and creative skills. Extensive experiments across ten open-source and proprietary models, covering both general-purpose and reasoning LLMs, reveal substantial performance drops (even up to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in deep reasoning. Our work highlights the need for cognitively diverse benchmarks to advance LLM progress. DeepQuestion and related datasets will be released upon acceptance of the paper.

</details>


### [143] [Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections](https://arxiv.org/abs/2505.24538)

*Orfeas Menis Mastromichalakis, Jason Liartis, Kristina Rose, Antoine Isaac, Giorgos Stamou*

**Main category:** cs.CL

**Keywords:** Cultural Heritage, AI tool, offensive terms, metadata, inclusivity

**Relevance Score:** 8

**TL;DR:** This paper presents an AI tool for detecting and contextualizing offensive terms in Cultural Heritage metadata to promote inclusivity and accessibility.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Cultural Heritage collections often contain outdated or offensive descriptions that reflect historical biases, creating challenges for curation and representation.

**Method:** The tool utilizes NLP techniques and Large Language Models (LLMs) alongside a multilingual vocabulary co-created with marginalized communities to detect offensive terms and provide contextual insights.

**Key Contributions:**

	1. Development of an AI-powered tool for detecting offensive metadata terms
	2. Collaborative design of a multilingual vocabulary with marginalized communities
	3. Processing of millions of records to provide contextual insights on historical biases

**Result:** The tool has processed over 7.9 million records, providing contextual information about contentious terms in CH metadata to help institutions curate more inclusive collections.

**Limitations:** 

**Conclusion:** By informing users about historical bias rather than erasing offensive terms, the tool aims to create awareness and promote more inclusive practices in Cultural Heritage curation.

**Abstract:** Cultural Heritage (CH) data hold invaluable knowledge, reflecting the history, traditions, and identities of societies, and shaping our understanding of the past and present. However, many CH collections contain outdated or offensive descriptions that reflect historical biases. CH Institutions (CHIs) face significant challenges in curating these data due to the vast scale and complexity of the task. To address this, we develop an AI-powered tool that detects offensive terms in CH metadata and provides contextual insights into their historical background and contemporary perception. We leverage a multilingual vocabulary co-created with marginalized communities, researchers, and CH professionals, along with traditional NLP techniques and Large Language Models (LLMs). Available as a standalone web app and integrated with major CH platforms, the tool has processed over 7.9 million records, contextualizing the contentious terms detected in their metadata. Rather than erasing these terms, our approach seeks to inform, making biases visible and providing actionable insights for creating more inclusive and accessible CH collections.

</details>


### [144] [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)

*Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman*

**Main category:** cs.CL

**Keywords:** personas, large language models, representation space, ethical perspectives, political ideologies

**Relevance Score:** 8

**TL;DR:** This study explores how personas are encoded within large language models (LLMs), identifying significant variations in representation across model layers and ethical perspectives.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how and where different personas are represented in the internal architecture of large language models.

**Method:** Utilized dimension reduction and pattern recognition methods to analyze activations in various layers of pre-trained decoder-only LLMs, focusing on the encoding of distinct human characteristics.

**Key Contributions:**

	1. Identified key layers in LLMs that encode personas
	2. Demonstrated differences in ethical and political ideology representations
	3. Provided insights for refining LLM outputs with specific human traits

**Result:** Significant representation differences are observed in the final third of the decoder layers for analyzed personas, with overlapping activations for certain ethical perspectives and distinct regions for political ideologies.

**Limitations:** Results may not generalize to all LLM architectures or training methods; includes sensitive content that may be offensive.

**Conclusion:** The findings enhance the understanding of LLM internal representations and could guide enhancements in controlling human traits in LLM outputs.

**Abstract:** We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.

</details>


### [145] [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)

*Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee*

**Main category:** cs.CL

**Keywords:** speculative decoding, cross-attention, Transformer models, training efficiency, large language models

**Relevance Score:** 8

**TL;DR:** Budget EAGLE (Beagle) is a novel cross-attention-based Transformer decoder for speculative decoding in large language models, achieving efficiency and performance improvements over previous self-attention-based methods.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing speculative decoding methods rely on complex self-attention structures that hinder generalizability; Beagle aims to provide a simpler and more efficient alternative.

**Method:** Introduces Budget EAGLE (Beagle), a cross-attention-based decoder using Two-Stage Block-Attention Training to improve stability and efficiency.

**Key Contributions:**

	1. First cross-attention-based speculative decoding model for LLMs.
	2. Introduced Two-Stage Block-Attention Training method for improved training stability.
	3. Achieved competitive speedups and efficiency over EAGLE-v2.

**Result:** Beagle achieves comparable performance to leading self-attention methods while simplifying the architecture and improving training efficiency across various LLMs and datasets.

**Limitations:** 

**Conclusion:** Beagle presents a viable alternative to current speculative decoding architectures, enhancing inference speed and training efficiency without the complexity of auxiliary components.

**Abstract:** Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.

</details>


### [146] [A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings](https://arxiv.org/abs/2505.24550)

*Xiaoang Xu, Shuo Wang, Xu Han, Zhenghao Liu, Huijia Wu, Peipei Li, Zhiyuan Liu, Maosong Sun, Zhaofeng He*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, A*-Thought, efficiency, reasoning, machine learning

**Relevance Score:** 8

**TL;DR:** A*-Thought is an efficient framework that optimizes reasoning in Large Reasoning Models by using a tree search approach to select essential thoughts, improving both performance and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of Large Reasoning Models while maintaining performance, addressing issues related to excessive thought length in reasoning chains.

**Method:** The paper introduces A*-Thought, using a search tree representation for reasoning processes, combined with the A* search algorithm and a specific cost function to compress thought chains and enhance efficiency.

**Key Contributions:**

	1. Introduction of the A*-Thought framework for efficient reasoning in LRMs
	2. Development of a bidirectional importance estimation mechanism
	3. Demonstration of the framework's effectiveness through extensive experiments on advanced math tasks

**Result:** A*-Thought improves the performance of the QwQ-32B model by 2.39 times at low computational budgets and reduces output token length by nearly 50% at high budgets.

**Limitations:** 

**Conclusion:** A*-Thought effectively balances performance and efficiency across extensive reasoning searches and is compatible with various Large Reasoning Models.

**Abstract:** Large Reasoning Models (LRMs) achieve superior performance by extending the thought length. However, a lengthy thinking trajectory leads to reduced efficiency. Most of the existing methods are stuck in the assumption of overthinking and attempt to reason efficiently by compressing the Chain-of-Thought, but this often leads to performance degradation. To address this problem, we introduce A*-Thought, an efficient tree search-based unified framework designed to identify and isolate the most essential thoughts from the extensive reasoning chains produced by these models. It formulates the reasoning process of LRMs as a search tree, where each node represents a reasoning span in the giant reasoning space. By combining the A* search algorithm with a cost function specific to the reasoning path, it can efficiently compress the chain of thought and determine a reasoning path with high information density and low cost. In addition, we also propose a bidirectional importance estimation mechanism, which further refines this search process and enhances its efficiency beyond uniform sampling. Extensive experiments on several advanced math tasks show that A*-Thought effectively balances performance and efficiency over a huge search space. Specifically, A*-Thought can improve the performance of QwQ-32B by 2.39$\times$ with low-budget and reduce the length of the output token by nearly 50% with high-budget. The proposed method is also compatible with several other LRMs, demonstrating its generalization capability. The code can be accessed at: https://github.com/AI9Stars/AStar-Thought.

</details>


### [147] [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)

*Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim*

**Main category:** cs.CL

**Keywords:** character relations, narrative analysis, large language models, LLM agents, script evaluation

**Relevance Score:** 6

**TL;DR:** CREFT is a sequential framework utilizing specialized LLM agents for effective extraction and visualization of character relations in long-form narratives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing extraction methods struggle with complex character relations in long narratives, necessitating a more efficient tool for narrative analysis.

**Method:** CREFT builds a character graph through knowledge distillation and iteratively refines it by extracting relations, identifying roles, and assigning groups.

**Key Contributions:**

	1. Introduction of CREFT framework for character relation extraction
	2. Demonstrated effectiveness in handling long-form narratives
	3. Systematic visualization of character networks for improved understanding

**Result:** CREFT outperforms single-agent LLM baselines in accuracy and completeness when tested on a Korean drama dataset.

**Limitations:** 

**Conclusion:** The framework enhances narrative comprehension and script review, providing significant advantages across various sectors.

**Abstract:** Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.

</details>


### [148] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)

*Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti*

**Main category:** cs.CL

**Keywords:** Knowledge Engineering, Large Language Models, Competency Questions, Bench4KE, Benchmarking

**Relevance Score:** 8

**TL;DR:** Bench4KE is an API-based benchmarking system for automating Knowledge Engineering (KE), focusing on the evaluation of tools that generate Competency Questions (CQs) using Large Language Models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of standardization in evaluating LLM-based methods for automatically generating Competency Questions (CQs), which is essential for methodology rigor and result comparability.

**Method:** Introduction of an extensible, API-based benchmarking system called Bench4KE, which evaluates CQ generation tools using a curated gold standard of CQ datasets from real-world ontology projects and various similarity metrics.

**Key Contributions:**

	1. Creation of Bench4KE, a standardized benchmarking system for KE automation.
	2. Provision of a gold standard dataset for evaluating CQ generation tools.
	3. Comparative analysis establishing baseline performance for LLM-based CQ generation systems.

**Result:** A comparative analysis of four recent LLM-based CQ generation systems was conducted, providing a baseline for future research in KE automation.

**Limitations:** 

**Conclusion:** Bench4KE facilitates evaluation of CQ generation and is expandable to include other KE automation tasks; code and datasets are publicly available to support ongoing research.

**Abstract:** The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.

</details>


### [149] [Improving Language and Modality Transfer in Translation by Character-level Modeling](https://arxiv.org/abs/2505.24561)

*Ioannis Tsiamas, David Dale, Marta R. Costa-jussà*

**Main category:** cs.CL

**Keywords:** low-resource languages, character-based translation, multilingual ASR

**Relevance Score:** 7

**TL;DR:** This paper presents a character-based approach to enhance translation systems for low-resource languages by utilizing cross-lingual and cross-modal knowledge transfer through a multilingual embedding space.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current multilingual translation systems only cover a small fraction of the world's languages, necessitating improved methods for low-resource languages.

**Method:** The proposed method employs SONAR, a multilingual embedding space, using a teacher-student approach to train a character-level encoder with parallel translation data and a lightweight adapter for ASR data.

**Key Contributions:**

	1. Development of a character-based translation method
	2. Creation of a multilingual fixed-size embedding space (SONAR)
	3. Enhanced performance in low-resource language settings and zero-shot translation.

**Result:** Experimental results indicate that the character-based approach significantly improves language transfer, especially in low-resource scenarios, and exhibits strong zero-shot capabilities for unseen languages.

**Limitations:** 

**Conclusion:** The method achieves state-of-the-art performance in speech-to-text translation with minimal supervision and better results than traditional models.

**Abstract:** Current translation systems, despite being highly multilingual, cover only 5% of the world's languages. Expanding language coverage to the long-tail of low-resource languages requires data-efficient methods that rely on cross-lingual and cross-modal knowledge transfer. To this end, we propose a character-based approach to improve adaptability to new languages and modalities. Our method leverages SONAR, a multilingual fixed-size embedding space with different modules for encoding and decoding. We use a teacher-student approach with parallel translation data to obtain a character-level encoder. Then, using ASR data, we train a lightweight adapter to connect a massively multilingual CTC ASR model (MMS), to the character-level encoder, potentially enabling speech translation from 1,000+ languages. Experimental results in text translation for 75 languages on FLORES+ demonstrate that our character-based approach can achieve better language transfer than traditional subword-based models, especially outperforming them in low-resource settings, and demonstrating better zero-shot generalizability to unseen languages. Our speech adaptation, maximizing knowledge transfer from the text modality, achieves state-of-the-art results in speech-to-text translation on the FLEURS benchmark on 33 languages, surpassing previous supervised and cascade models, albeit being a zero-shot model with minimal supervision from ASR data.

</details>


### [150] [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)

*Hyuntak Kim, Byung-Hak Kim*

**Main category:** cs.CL

**Keywords:** narrative summarization, multi-agent LLM, NexusSum

**Relevance Score:** 8

**TL;DR:** NexusSum is a multi-agent framework designed for summarizing long-form narratives, improving coherence and output quality through innovative processing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to effectively summarize intricate long-form narratives, which is challenging for existing LLMs.

**Method:** NexusSum employs a structured, sequential pipeline that includes a Dialogue-to-Description Transformation for preprocessing and a Hierarchical Multi-LLM Summarization for chunk processing and output control.

**Key Contributions:**

	1. Dialogue-to-Description Transformation
	2. Hierarchical Multi-LLM Summarization
	3. State-of-the-art performance in narrative summarization

**Result:** NexusSum achieves a 30.0% improvement in BERTScore (F1) for summarizing books, movies, and TV scripts, setting a new state-of-the-art.

**Limitations:** 

**Conclusion:** The framework demonstrates that multi-agent LLMs can effectively manage and summarize complex long-form content.

**Abstract:** Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.

</details>


### [151] [GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training](https://arxiv.org/abs/2505.24581)

*Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila*

**Main category:** cs.CL

**Keywords:** semantic textual similarity, Arabic NLP, GATE models

**Relevance Score:** 4

**TL;DR:** The paper presents General Arabic Text Embedding (GATE) models, improving Arabic semantic textual similarity tasks using novel techniques and datasets. GATE outperforms existing models with significant performance gains.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The limited research and resources for semantic textual similarity in Arabic hinder progress in understanding and processing Arabic text. The paper addresses this gap by introducing improved models and datasets.

**Method:** The study employs General Arabic Text Embedding (GATE) models utilizing Matryoshka Representation Learning and a hybrid loss training approach, specifically designed for Arabic triplet datasets relevant for Natural Language Inference.

**Key Contributions:**

	1. Introduction of General Arabic Text Embedding (GATE) models
	2. Utilization of Matryoshka Representation Learning for improved performance
	3. Performance surpasses larger LLMs by 20-25% in STS benchmarks

**Result:** GATE achieves state-of-the-art performance on the MTEB benchmark for Semantic Textual Similarity, outperforming larger models such as OpenAI with a 20-25% increase in performance on STS benchmarks.

**Limitations:** 

**Conclusion:** The introduction of GATE models significantly advances the field of Arabic natural language processing, aiding in the accurate evaluation of semantic similarity and encouraging further research.

**Abstract:** Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.

</details>


### [152] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)

*Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, interpretability, attribution methods, sparse architectures, dynamic routing

**Relevance Score:** 7

**TL;DR:** The paper explores the interpretability of Mixture-of-Experts (MoE) models with a cross-level attribution algorithm, highlighting their efficiency and robustness compared to dense models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the interpretability of heterogeneous Mixture-of-Experts (MoE) models and the limitations of existing attribution methods in capturing dynamic interactions.

**Method:** A cross-level attribution algorithm is proposed to analyze various sparse MoE architectures against dense models, examining their performance efficiency and routing mechanisms.

**Key Contributions:**

	1. Introduction of a cross-level attribution algorithm for MoE models
	2. Demonstration of efficiency gains in MoE architectures
	3. Insights into task sensitivity guiding expert specialization

**Result:** MoE models demonstrate 37% higher per-layer efficiency and exhibit a robust architecture that maintains performance even with expert failures, while revealing correlations between attention heads and expert dynamics.

**Limitations:** 

**Conclusion:** The insights on MoE interpretability contribute to a better understanding of how to optimize model efficiency, specialization, and robustness, particularly in domain-sensitive tasks.

**Abstract:** The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.

</details>


### [153] [Explainable Depression Detection using Masked Hard Instance Mining](https://arxiv.org/abs/2505.24609)

*Patawee Prakrankamanant, Shinji Watanabe, Ekapol Chuangsuwanich*

**Main category:** cs.CL

**Keywords:** explainability, depression detection, Masked Hard Instance Mining, multilingual, machine learning

**Relevance Score:** 9

**TL;DR:** This paper proposes Masked Hard Instance Mining (MHIM) to enhance explainability in text-based depression detection models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for improved explainability in text-based depression detection, as current solutions may hinder trust due to a lack of understanding of model predictions.

**Method:** The paper introduces Masked Hard Instance Mining (MHIM), which masks attention weights to encourage the model to pay attention to a broader range of features.

**Key Contributions:**

	1. Introduction of Masked Hard Instance Mining (MHIM) for enhanced model explainability.
	2. Demonstration of improvements in both predictive accuracy and explainability on multilingual datasets.
	3. Evaluation across distinct language datasets (Thai and English) to validate effectiveness.

**Result:** MHIM significantly improves performance in prediction accuracy and explainability metrics across two language datasets: Thai (Thai-Maywe) and English (DAIC-WOZ).

**Limitations:** 

**Conclusion:** The proposed MHIM method enhances both the predictive outcomes and the interpretability of the models used for depression detection.

**Abstract:** This paper addresses the critical need for improved explainability in text-based depression detection. While offering predictive outcomes, current solutions often overlook the understanding of model predictions which can hinder trust in the system. We propose the use of Masked Hard Instance Mining (MHIM) to enhance the explainability in the depression detection task. MHIM strategically masks attention weights within the model, compelling it to distribute attention across a wider range of salient features. We evaluate MHIM on two datasets representing distinct languages: Thai (Thai-Maywe) and English (DAIC-WOZ). Our results demonstrate that MHIM significantly improves performance in terms of both prediction accuracy and explainability metrics.

</details>


### [154] [When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation](https://arxiv.org/abs/2505.24613)

*Daniela Occhipinti, Marco Guerini, Malvina Nissim*

**Main category:** cs.CL

**Keywords:** dialogue agents, persona adaptation, human-computer interaction, language models, dialogue generation

**Relevance Score:** 8

**TL;DR:** This paper explores how dialogue agents can better align their responses with both their own persona and those of their interlocutors, assessing the effects of familiarity and fine-tuning on dialogue generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the production of dialogue by incorporating information about both the speaker's and interlocutor's personas, thereby enhancing consistency and engagement in conversational AI.

**Method:** The authors investigate the alignment of dialogue models with personas, robustness with familiar vs unfamiliar interlocutors, and the effects of fine-tuning through various evaluations, including author identification and masking of interlocutor information.

**Key Contributions:**

	1. Evaluation of LLM responses based on both speaker and interlocutor personas
	2. Insight into the model's performance with familiar vs unfamiliar interlocutors
	3. Demonstration of challenges in zero-shot dialogue settings with LLMs

**Result:** The study finds that knowing the interlocutor's persona helps models generate more recognizable responses, but unfamiliar interlocutors create challenges. Zero-shot settings often result in LLMs relying on biographical detail, complicating the identification process.

**Limitations:** The research focuses on persona alignment and evaluation but may not address all aspects of dialogue generation robustness.

**Conclusion:** While fine-tuning improves persona-based dialogue, models are less effective with unfamiliar interlocutors, suggesting a need for better training strategies in dialogue systems.

**Abstract:** Endowing dialogue agents with persona information has proven to significantly improve the consistency and diversity of their generations. While much focus has been placed on aligning dialogues with provided personas, the adaptation to the interlocutor's profile remains largely underexplored. In this work, we investigate three key aspects: (1) a model's ability to align responses with both the provided persona and the interlocutor's; (2) its robustness when dealing with familiar versus unfamiliar interlocutors and topics, and (3) the impact of additional fine-tuning on specific persona-based dialogues. We evaluate dialogues generated with diverse speaker pairings and topics, framing the evaluation as an author identification task and employing both LLM-as-a-judge and human evaluations. By systematically masking or disclosing information about the interlocutor, we assess its impact on dialogue generation. Results show that access to the interlocutor's persona improves the recognition of the target speaker, while masking it does the opposite. Although models generalise well across topics, they struggle with unfamiliar interlocutors. Finally, we found that in zero-shot settings, LLMs often copy biographical details, facilitating identification but trivialising the task.

</details>


### [155] [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615)

*Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria*

**Main category:** cs.CL

**Keywords:** novelty detection, large language models, dataset, idea retrieval, academic research

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach to scientific novelty detection using large language models (LLMs) and introduces two new benchmark datasets for this purpose.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying novel research ideas is critical in academia, but the absence of apt benchmark datasets limits advancements in novelty detection.

**Method:** The paper proposes using large language models to extract closure sets of related papers and summarize their ideas. It also introduces a lightweight retriever model that aligns ideas through distillation from LLMs, enhancing retrieval accuracy for novelty detection.

**Key Contributions:**

	1. Introduction of two benchmark datasets for novelty detection in marketing and NLP domains.
	2. Development of a method using LLMs for effective summarization and relation extraction of academic papers.
	3. Creation of a lightweight retriever model for aligning ideas for better novelty detection.

**Result:** The method shows consistent improvement over other approaches on the proposed datasets for idea retrieval and novelty detection tasks.

**Limitations:** The scope of the datasets is currently limited to marketing and NLP, potentially restricting applicability to other domains.

**Conclusion:** The proposed approach and datasets could significantly enhance the identification of novel ideas in scientific research.

**Abstract:** In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.

</details>


### [156] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)

*Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova*

**Main category:** cs.CL

**Keywords:** large language models, evaluation methodology, Russian language, POLLUX, generative capabilities

**Relevance Score:** 8

**TL;DR:** POLLUX is an open-source benchmark for evaluating the generative capabilities of large language models (LLMs) in Russian, featuring a methodology for interpretability and scalable evaluation.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive, transparent, and efficient method for evaluating LLMs beyond traditional human comparison, especially tailored for Russian language models.

**Method:** A novel evaluation methodology with defined criteria and a scoring protocol to assess model responses and justifications, covering 35 task types with 2,100 prompts categorized by difficulty.

**Key Contributions:**

	1. Introduction of a comprehensive open-source benchmark for LLMs in Russian.
	2. Novel evaluation methodology enhances interpretability and scalability.
	3. Release of LLM-as-a-Judge evaluators for more precise assessments.

**Result:** POLLUX can potentially replace costly human judgments with LLM-as-a-Judge evaluators, enhancing evaluation efficiency and interpretability in model assessment.

**Limitations:** 

**Conclusion:** This benchmark allows for a more scalable and nuanced evaluation of generative outputs, thereby aiding in the development of LLMs.

**Abstract:** We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.

</details>


### [157] [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)

*Vittorio Torri, Machteld J. Boonstra, Marielle C. van de Veerdonk, Deborah N. Kalkman, Alicia Uijl, Francesca Ieva, Ameen Abu-Hanna, Folkert W. Asselbergs, Iacer Calixto*

**Main category:** cs.CL

**Keywords:** Heart failure, phenotyping, machine learning, interpretability, discharge letters

**Relevance Score:** 7

**TL;DR:** This study evaluates models for phenotyping heart failure patients based on left ventricular ejection fraction using structured and unstructured data, assessing their performance and interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Heart failure patients have diverse phenotypes that impact treatment and prognosis; there is a need for effective phenotyping models to guide clinical decisions.

**Method:** An analysis of heart failure hospitalizations was conducted, utilizing structured and unstructured data, including a combination of diagnosis codes, echocardiography results, and textual mentions from discharge letters. Transformer-based and Aug-Linear models were trained and assessed for interpretability against clinician annotations.

**Key Contributions:**

	1. Evaluation of phenotyping models for HF using combined data types
	2. Demonstration of Aug-Linear interpretability aligned with clinician insights
	3. Identification of discharge letters as key informative data source

**Result:** BERT-based and Aug-Linear models achieved high classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear) using discharge letters, with Aug-Linear models providing clinician-aligned interpretability.

**Limitations:** The study relies on a single institution's dataset for model training, which might affect generalizability.

**Conclusion:** Discharge letters are crucial for phenotyping heart failure patients, and Aug-Linear models offer competitive performance with enhanced interpretability, promoting transparent clinical decision-making.

**Abstract:** Objective: Heart failure (HF) patients present with diverse phenotypes affecting treatment and prognosis. This study evaluates models for phenotyping HF patients based on left ventricular ejection fraction (LVEF) classes, using structured and unstructured data, assessing performance and interpretability.   Materials and Methods: The study analyzes all HF hospitalizations at both Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105 hospitalizations, 16,334 patients). Data from AMC were used for model training, and from VUmc for external validation. The dataset was unlabelled and included tabular clinical measurements and discharge letters. Silver labels for LVEF classes were generated by combining diagnosis codes, echocardiography results, and textual mentions. Gold labels were manually annotated for 300 patients for testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box) models were trained and compared with baselines on structured and unstructured data. To evaluate interpretability, two clinicians annotated 20 discharge letters by highlighting information they considered relevant for LVEF classification. These were compared to SHAP and LIME explanations from black-box models and the inherent explanations of Aug-Linear models.   Results: BERT-based and Aug-Linear models, using discharge letters alone, achieved the highest classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear on external validation), outperforming baselines. Aug-Linear explanations aligned more closely with clinicians' explanations than post-hoc explanations on black-box models.   Conclusions: Discharge letters emerged as the most informative source for phenotyping HF patients. Aug-Linear models matched black-box performance while providing clinician-aligned interpretability, supporting their use in transparent clinical decision-making.

</details>


### [158] [Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization](https://arxiv.org/abs/2505.24621)

*Utsav Maskey, Chencheng Zhu, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Large Language Models, cryptanalysis, encryption, AI safety, security

**Relevance Score:** 8

**TL;DR:** This paper evaluates the cryptanalytic capabilities of state-of-the-art Large Language Models (LLMs) on encrypted texts, highlighting their strengths and weaknesses in data security applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the lack of thorough exploration of LLMs in the context of cryptanalysis, a critical area for data security and encryption.

**Method:** A novel benchmark dataset was created, consisting of various plain texts and their encrypted versions, to assess LLMs in zero-shot and few-shot conditions for decryption accuracy and semantic comprehension.

**Key Contributions:**

	1. Introduction of a benchmark dataset for evaluating LLMs on encrypted texts
	2. Assessment of LLMs for decryption accuracy across different cryptographic algorithms
	3. Insights into the implications of LLMs in side-channel communication and AI safety

**Result:** The evaluation reveals key insights about LLMs' strengths and limitations in handling encrypted texts, as well as concerns regarding their vulnerability to jailbreaking attacks.

**Limitations:** The study only explores a specific range of cryptographic algorithms and does not evaluate all possible encryption schemes.

**Conclusion:** This research underscores the dual-use nature of LLMs in security contexts, contributing to discussions on AI safety and security.

**Abstract:** Recent advancements in Large Language Models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis a critical area for data security and encryption has not yet been thoroughly explored in LLM evaluations. To address this gap, we evaluate cryptanalytic potential of state of the art LLMs on encrypted texts generated using a range of cryptographic algorithms. We introduce a novel benchmark dataset comprising diverse plain texts spanning various domains, lengths, writing styles, and topics paired with their encrypted versions. Using zero-shot and few shot settings, we assess multiple LLMs for decryption accuracy and semantic comprehension across different encryption schemes. Our findings reveal key insights into the strengths and limitations of LLMs in side-channel communication while raising concerns about their susceptibility to jailbreaking attacks. This research highlights the dual-use nature of LLMs in security contexts and contributes to the ongoing discussion on AI safety and security.

</details>


### [159] [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)

*Junyi Li, Hwee Tou Ng*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Factuality, Reasoning, Hallucinations

**Relevance Score:** 8

**TL;DR:** The paper introduces Factuality-aware Step-wise Policy Optimization (FSPO), an RL fine-tuning algorithm designed to reduce hallucinations in large language models during reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in reasoning tasks through RL optimization, LLMs face increased hallucinations, which undermines their reliability.

**Method:** The paper theoretically analyzes RL training dynamics to identify causes of hallucinations and proposes FSPO, which includes factuality verification at each reasoning step to adjust token-level advantages.

**Key Contributions:**

	1. Introduction of Factuality-aware Step-wise Policy Optimization (FSPO)
	2. Theoretical analysis of RL training dynamics leading to hallucinations
	3. Empirical validation showing reduced hallucinations and improved reasoning accuracy

**Result:** FSPO demonstrates effectiveness in reducing hallucinations and enhancing reasoning accuracy across various benchmarks with models like Qwen2.5 and Llama.

**Limitations:** Relies on the performance of existing LLMs and may not generalize across all language tasks.

**Conclusion:** By implementing FSPO, LLMs can achieve improved reliability and performance in reasoning tasks, addressing critical drawbacks of existing RL fine-tuning methods.

**Abstract:** Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.

</details>


### [160] [Disentangling Language and Culture for Evaluating Multilingual Large Language Models](https://arxiv.org/abs/2505.24635)

*Jiahao Ying, Wei Tang, Yiran Zhao, Yixin Cao, Yu Rong, Wenxuan Zhang*

**Main category:** cs.CL

**Keywords:** multilingual capabilities, LLMs, cultural context, evaluation framework, neuron activation

**Relevance Score:** 9

**TL;DR:** The paper presents a Dual Evaluation Framework for assessing multilingual capabilities of LLMs, highlighting cultural and linguistic performance variations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a nuanced understanding of LLMs' multilingual capabilities, addressing gaps in current evaluation methods that overlook cultural context.

**Method:** A Dual Evaluation Framework decomposes evaluation into linguistic medium and cultural context, allowing for extensive model assessments.

**Key Contributions:**

	1. Introduction of the Dual Evaluation Framework
	2. Discovery of the Cultural-Linguistic Synergy phenomenon
	3. Interpretability probing as a measure for multilingual performance

**Result:** The study uncovers a 'Cultural-Linguistic Synergy' where LLMs perform better on culturally aligned questions; indicates that neuron activation can signal multilingual performance.

**Limitations:** The framework may require extensive resources for evaluations across multiple languages and cultural contexts.

**Conclusion:** The findings urge the need for culturally informed evaluations of LLMs as they challenge the misconception of uniform performance across languages.

**Abstract:** This paper introduces a Dual Evaluation Framework to comprehensively assess the multilingual capabilities of LLMs. By decomposing the evaluation along the dimensions of linguistic medium and cultural context, this framework enables a nuanced analysis of LLMs' ability to process questions within both native and cross-cultural contexts cross-lingually. Extensive evaluations are conducted on a wide range of models, revealing a notable "CulturalLinguistic Synergy" phenomenon, where models exhibit better performance when questions are culturally aligned with the language. This phenomenon is further explored through interpretability probing, which shows that a higher proportion of specific neurons are activated in a language's cultural context. This activation proportion could serve as a potential indicator for evaluating multilingual performance during model training. Our findings challenge the prevailing notion that LLMs, primarily trained on English data, perform uniformly across languages and highlight the necessity of culturally and linguistically model evaluations. Our code can be found at https://yingjiahao14. github.io/Dual-Evaluation/.

</details>


### [161] [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)

*Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester*

**Main category:** cs.CL

**Keywords:** skill extraction, labor market analysis, contrastive learning

**Relevance Score:** 6

**TL;DR:** This paper presents ConTeXT-match, a novel contrastive learning approach for skill classification in job advertisements, significantly enhancing efficiency and performance compared to traditional LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve labor market analysis by extracting insights from unstructured job advertisements, which typically include vast skill requirements.

**Method:** The paper introduces ConTeXT-match, a contrastive learning method with token-level attention tailored for extreme multi-label classification tasks. It also presents a new benchmark, Skill-XL, and JobBERT V2 for job title normalization.

**Key Contributions:**

	1. Introduction of ConTeXT-match for efficient skill extraction
	2. Development of Skill-XL benchmark for enhanced evaluation
	3. Presentation of JobBERT V2 for improved job title normalization

**Result:** ConTeXT-match achieves state-of-the-art performance with a lightweight bi-encoder model, enhancing skill extraction efficiency and accuracy.

**Limitations:** 

**Conclusion:** The developed models are proven to be efficient and scalable for real-time labor market analysis, making them suitable for large-scale applications.

**Abstract:** Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.

</details>


### [162] [Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching](https://arxiv.org/abs/2505.24643)

*Juan Wisznia, Cecilia Bolaños, Juan Tollo, Giovanni Marraffini, Agustín Gianolini, Noe Hsueh, Luciano Del Corro*

**Main category:** cs.CL

**Keywords:** pairwise ranking, sorting algorithms, LLM inference, cost model, optimization strategies

**Relevance Score:** 7

**TL;DR:** Introduction of a novel framework for analyzing sorting algorithms in pairwise ranking prompting, focusing on LLM inference costs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses inefficiencies in traditional sorting algorithm analysis by centering it around expensive LLM inferences rather than just comparison counts.

**Method:** A new cost model that integrates LLM inferences and promotes batching and caching to reduce inference costs in sorting algorithms.

**Key Contributions:**

	1. Introduction of a novel cost model focused on LLM inferences.
	2. Demonstration that classical algorithm efficiency can be compromised by LLM costs.
	3. Recommendations for batching and caching to optimize performance.

**Result:** The analysis demonstrates that classical sorting algorithms may perform poorly when LLM inference costs are considered, thus revealing the importance of optimization strategies.

**Limitations:** The framework may not cover all sorting scenarios or account for all types of LLM inference variations.

**Conclusion:** When optimizing for LLM inference costs, classical optimal algorithms may need reevaluation as they can become inefficient due to high inference expenses.

**Abstract:** We introduce a novel framework for analyzing sorting algorithms in pairwise ranking prompting (PRP), re-centering the cost model around LLM inferences rather than traditional pairwise comparisons. While classical metrics based on comparison counts have traditionally been used to gauge efficiency, our analysis reveals that expensive LLM inferences overturn these predictions; accordingly, our framework encourages strategies such as batching and caching to mitigate inference costs. We show that algorithms optimal in the classical setting can lose efficiency when LLM inferences dominate the cost under certain optimizations.

</details>


### [163] [PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder](https://arxiv.org/abs/2505.24646)

*Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu*

**Main category:** cs.CL

**Keywords:** semantic text embedding, political bias, NLP, interpretability, bias indicators

**Relevance Score:** 6

**TL;DR:** PRISM is a framework for generating interpretable political bias embeddings that address ideological nuances in textual data, enhancing the effectiveness of NLP models in political bias classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing embedding models fail to capture ideological nuances, which limits effectiveness in understanding political bias.

**Method:** PRISM consists of two stages: mining bias indicators from weakly labeled news data and assigning structured bias scores to articles using these indicators.

**Key Contributions:**

	1. Introducing the PRISM framework for political bias embeddings
	2. Systematic extraction of political topics and bias indicators from news data
	3. Improved performance in political bias classification tasks

**Result:** PRISM outperforms existing text embedding models in political bias classification and provides interpretable representations for ideological analysis.

**Limitations:** 

**Conclusion:** PRISM enhances interpretability and predictive power of embeddings, facilitating better ideological analysis and retrieval.

**Abstract:** Semantic Text Embedding is a fundamental NLP task that encodes textual content into vector representations, where proximity in the embedding space reflects semantic similarity. While existing embedding models excel at capturing general meaning, they often overlook ideological nuances, limiting their effectiveness in tasks that require an understanding of political bias. To address this gap, we introduce PRISM, the first framework designed to Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key stages: (1) Controversial Topic Bias Indicator Mining, which systematically extracts fine-grained political topics and their corresponding bias indicators from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding, which assigns structured bias scores to news articles based on their alignment with these indicators. This approach ensures that embeddings are explicitly tied to bias-revealing dimensions, enhancing both interpretability and predictive power. Through extensive experiments on two large-scale datasets, we demonstrate that PRISM outperforms state-of-the-art text embedding models in political bias classification while offering highly interpretable representations that facilitate diversified retrieval and ideological analysis. The source code is available at https://github.com/dukesun99/ACL-PRISM.

</details>


### [164] [MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR](https://arxiv.org/abs/2505.24656)

*Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Unsupervised Domain Adaptation, Self-Supervised Learning

**Relevance Score:** 5

**TL;DR:** This paper presents a Multi-Stage Domain Adaptation (MSDA) pipeline for enhancing Automatic Speech Recognition (ASR) models, particularly in low-resource languages and weakly supervised environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve Automatic Speech Recognition models' adaptability and robustness, especially in low-resource language scenarios and when labeled data is limited or noisy.

**Method:** The proposed method is a sample-efficient, two-stage adaptation approach that combines self-supervised learning and semi-supervised techniques to create a Multi-Stage Domain Adaptation pipeline.

**Key Contributions:**

	1. Introduction of the Multi-Stage Domain Adaptation (MSDA) pipeline for ASR.
	2. Application of the MSDA framework to low-resource languages and weakly supervised scenarios.
	3. Demonstration of state-of-the-art results in unsupervised ASR tasks.

**Result:** Through extensive experiments, the authors demonstrate that their Meta PL framework achieves state-of-the-art performance in ASR tasks, significantly outperforming existing methods and improving robustness in unsupervised domain adaptation.

**Limitations:** 

**Conclusion:** The research concludes that the cascading approach of integrating self-supervision with self-training is essential for improving the performance of ASR models in diverse conditions.

**Abstract:** In this work, we investigate the Meta PL unsupervised domain adaptation framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation approach that integrates self-supervised learning with semi-supervised techniques. MSDA is designed to enhance the robustness and generalization of ASR models, making them more adaptable to diverse conditions. It is particularly effective for low-resource languages like Greek and in weakly supervised scenarios where labeled data is scarce or noisy. Through extensive experiments, we demonstrate that Meta PL can be applied effectively to ASR tasks, achieving state-of-the-art results, significantly outperforming state-of-the-art methods, and providing more robust solutions for unsupervised domain adaptation in ASR. Our ablations highlight the necessity of utilizing a cascading approach when combining self-supervision with self-training.

</details>


### [165] [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)

*Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Adaptability, Multi-Agent Debate, Social Etiquette, LLM Performance

**Relevance Score:** 9

**TL;DR:** This paper proposes a Multi-Agent Debate framework for Large Language Models (LLMs) to enhance cultural adaptability by allowing LLMs to debate scenarios and collaborate on decisions.

**Read time:** 37 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the cultural adaptability of LLMs and benefit diverse global communities by leveraging the strengths of multiple agents rather than relying on a single LLM approach.

**Method:** The paper introduces a Multi-Agent Debate framework involving two LLM agents that debate cultural scenarios, assessing two variants: exclusive debate by agents and a dynamic approach where agents choose between self-reflection and debate.

**Key Contributions:**

	1. Introduction of the Multi-Agent Debate framework for LLMs
	2. Demonstration of improved accuracy and cultural parity through debate
	3. Validation of performance of smaller LLMs compared to larger ones in cultural scenarios.

**Result:** Experiments show that the debate approach improves accuracy and cultural group parity over single-LLM baselines, with smaller LLMs achieving performance comparable to larger models.

**Limitations:** 

**Conclusion:** Multi-agent debate enhances cultural understanding and decision-making in LLMs, offering a promising avenue for improving LLM performance in diverse contexts.

**Abstract:** Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).

</details>


### [166] [TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis](https://arxiv.org/abs/2505.24672)

*Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xuanhong Li, Chong Teng, Donghong Ji, Zhuang Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Safety alignment, Natural Language Processing, Harmful content, Dataset evaluation

**Relevance Score:** 8

**TL;DR:** This paper presents TRIDENT, a novel framework and datasets for assessing and enhancing the risk coverage of alignment datasets in large language models, particularly focusing on harmful content generation and mitigation strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve safety alignment in large language models by addressing gaps in existing datasets that primarily focus on lexical diversity and overlook critical risk dimensions.

**Method:** The authors propose a systematic analysis framework assessing alignment datasets across Lexical Diversity, Malicious Intent, and Jailbreak Tactics, and introduce an automated pipeline for generating diverse harmful instructions paired with safe responses.

**Key Contributions:**

	1. Introduction of a novel analysis framework for alignment datasets
	2. Creation of TRIDENT-Core and TRIDENT-Edge datasets for risk evaluation
	3. Demonstrated significant improvements in safety metrics through fine-tuning

**Result:** Fine-tuning Llama 3.1-8B on the TRIDENT-Edge dataset led to a 14.29% reduction in Harm Score and a 20% decrease in the Attack Success Rate compared to the best baseline model.

**Limitations:** 

**Conclusion:** The TRIDENT framework improves the robustness of large language models against harmful content generation, showcasing the importance of comprehensive risk coverage in safety alignment datasets.

**Abstract:** Large Language Models (LLMs) excel in various natural language processing tasks but remain vulnerable to generating harmful content or being exploited for malicious purposes. Although safety alignment datasets have been introduced to mitigate such risks through supervised fine-tuning (SFT), these datasets often lack comprehensive risk coverage. Most existing datasets focus primarily on lexical diversity while neglecting other critical dimensions. To address this limitation, we propose a novel analysis framework to systematically measure the risk coverage of alignment datasets across three essential dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We further introduce TRIDENT, an automated pipeline that leverages persona-based, zero-shot LLM generation to produce diverse and comprehensive instructions spanning these dimensions. Each harmful instruction is paired with an ethically aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311 examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29% reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to the best-performing baseline model fine-tuned on the WildBreak dataset.

</details>


### [167] [A Simple Linear Patch Revives Layer-Pruned Large Language Models](https://arxiv.org/abs/2505.24680)

*Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan*

**Main category:** cs.CL

**Keywords:** layer pruning, large language models, activation magnitudes, performance retention, knowledge distillation

**Relevance Score:** 8

**TL;DR:** LinearPatch revives layer-pruned large language models (LLMs) by addressing activation magnitude mismatches, achieving significant performance retention with minimal overhead.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance retention in layer-pruned LLMs, which suffer from degradation due to mismatched activation magnitudes across layers and tokens.

**Method:** LinearPatch uses Hadamard transformation to suppress outliers and channel-wise scaling to align activation magnitudes, incorporating these into a single matrix for effective pruning.

**Key Contributions:**

	1. Introduction of LinearPatch for layer pruning in LLMs
	2. Demonstration of significant performance retention after pruning layers
	3. Optimization with offline knowledge distillation for enhanced performance

**Result:** LinearPatch retains up to 94.15% performance of the original LLaMA-3-8B model after pruning 5 layers, outperforming existing methods by 4%.

**Limitations:** 

**Conclusion:** The proposed patch matrix can be optimized with offline knowledge distillation, further improving performance to 95.16% with 5K samples.

**Abstract:** Layer pruning has become a popular technique for compressing large language models (LLMs) due to its simplicity. However, existing layer pruning methods often suffer from significant performance drops. We identify that this degradation stems from the mismatch of activation magnitudes across layers and tokens at the pruning interface. To address this, we propose LinearPatch, a simple plug-and-play technique to revive the layer-pruned LLMs. The proposed method adopts Hadamard transformation to suppress massive outliers in particular tokens, and channel-wise scaling to align the activation magnitudes. These operations can be fused into a single matrix, which functions as a patch to bridge the pruning interface with negligible inference overhead. LinearPatch retains up to 94.15% performance of the original model when pruning 5 layers of LLaMA-3-8B on the question answering benchmark, surpassing existing state-of-the-art methods by 4%. In addition, the patch matrix can be further optimized with memory efficient offline knowledge distillation. With only 5K samples, the retained performance of LinearPatch can be further boosted to 95.16% within 30 minutes on a single computing card.

</details>


### [168] [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)

*Dayeon Ki, Kevin Duh, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Machine Translation, Quality Feedback, AI Trust, User Decision Making, Implicit Feedback

**Relevance Score:** 8

**TL;DR:** Study on enhancing AI user trust and decision-making through quality feedback in Machine Translation scenarios.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To explore effective feedback mechanisms that help users responsibly utilize AI-generated translations, especially when users lack the expertise to evaluate AI predictions.

**Method:** A comparative study of four feedback mechanisms: explicit (error highlights, LLM explanations) and implicit (backtranslation, QA tables) in a Machine Translation context.

**Key Contributions:**

	1. Comparison of explicit and implicit feedback in AI systems
	2. Demonstrated effectiveness of QA tables for enhancing user trust and accuracy
	3. Insights into user perceptions of feedback helpfulness and mental burden

**Result:** All feedback types improved decision accuracy and reliance, with implicit feedback, particularly QA tables, outperforming explicit feedback.

**Limitations:** The study is limited to a specific Machine Translation context and may not generalize across other AI applications.

**Conclusion:** The study highlights the importance of feedback in aiding non-expert users in making informed decisions about AI outputs, with QA tables being particularly effective.

**Abstract:** As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using 1) error highlights and 2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through 3) backtranslation and 4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.

</details>


### [169] [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)

*Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Embedding, Bayesian Optimisation, Artificial Intelligence

**Relevance Score:** 9

**TL;DR:** Introduction of Soft Reasoning, an embedding-based search framework to enhance complex reasoning in Large Language Models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the struggle of Large Language Models (LLMs) with complex reasoning due to their limited diversity and inefficient search methods.

**Method:** The proposed Soft Reasoning framework utilizes embedding perturbation for controlled exploration and Bayesian optimisation to refine embeddings, balancing exploration and exploitation during the reasoning process.

**Key Contributions:**

	1. Introduction of the Soft Reasoning framework
	2. Utilization of embedding perturbation and Bayesian optimisation
	3. Demonstrated scalability and correctness improvements in reasoning tasks

**Result:** The framework improves reasoning accuracy and coherence, demonstrating superior correctness with minimal computation in experiments.

**Limitations:** 

**Conclusion:** Soft Reasoning provides a scalable, model-agnostic solution for enhancing reasoning in LLMs without relying on heuristic search methods.

**Abstract:** Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution.

</details>


### [170] [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/abs/2505.24689)

*Sander Land, Catherine Arnett*

**Main category:** cs.CL

**Keywords:** Byte Pair Encoding, Multilingual NLP, Unicode, Tokenization, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents SCRIPT, a novel encoding scheme for tokenization in multilingual settings that improves upon traditional Byte Pair Encoding methods, particularly for non-Western scripts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges faced by BPE tokenizers in handling multilingual data, especially non-Western scripts, which can lead to issues like penalties for encoding and fragility in pretokenization.

**Method:** The proposed SCRIPT method utilizes Unicode script and category properties to create initial tokens, eliminating the need for UTF-8 byte conversion. It also introduces a constrained BPE merging strategy to maintain character integrity.

**Key Contributions:**

	1. Introduction of SCRIPT encoding scheme that bypasses UTF-8 conversion
	2. Development of a rule-based pretokenization strategy respecting script boundaries
	3. Validation of constrained BPE merging strategy for character integrity

**Result:** SCRIPT-BPE achieves competitive compression ratios compared to traditional methods while eliminating encoding penalties for non-Latin scripts, showcasing its effectiveness in multilingual contexts.

**Limitations:** 

**Conclusion:** SCRIPT offers a robust alternative to traditional pretokenization methods, facilitating better handling of non-Western scripts in NLP tasks.

**Abstract:** Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models, face challenges in multilingual settings, including penalization of non-Western scripts and the creation of tokens with partial UTF-8 sequences. Pretokenization, often reliant on complex regular expressions, can also introduce fragility and unexpected edge cases. We propose SCRIPT (Script Category Representation in PreTokenization), a novel encoding scheme that bypasses UTF-8 byte conversion by using initial tokens based on Unicode script and category properties. This approach enables a simple, rule-based pretokenization strategy that respects script boundaries, offering a robust alternative to pretokenization strategies based on regular expressions. We also introduce and validate a constrained BPE merging strategy that enforces character integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our experiments demonstrate that SCRIPT-BPE achieves competitive compression while eliminating encoding-based penalties for non-Latin-script languages.

</details>


### [171] [Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios](https://arxiv.org/abs/2505.24691)

*Gerard I. Gállego, Oriol Pareras, Martí Cortada Garcia, Lucas Takanori, Javier Hernando*

**Main category:** cs.CL

**Keywords:** Speech-to-Text Translation, phoneme representations, Chain-of-Thought

**Relevance Score:** 8

**TL;DR:** This paper presents a novel Speech-to-Text Translation approach that leverages phoneme representations within a Chain-of-Thought framework for enhancing translation in low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve translation quality in low-resource and zero-resource settings where labeled speech data is scarce.

**Method:** The proposed method integrates phoneme recognition as an intermediate step within a multilingual LLM and employs a curriculum learning strategy to progressively teach the system with increasingly complex tasks.

**Key Contributions:**

	1. Integration of phoneme representations into S2TT
	2. Enhanced cross-lingual transfer for low-resource translation
	3. Curriculum learning strategy for progressive task complexity

**Result:** Experiments demonstrate that the phoneme-augmented Chain-of-Thought improves translation quality in low-resource contexts and enables zero-resource translation, albeit with a slight performance impact on high-resource languages.

**Limitations:** Slight negative impact on high-resource language performance due to the phoneme-based approach.

**Conclusion:** Phoneme-based Chain-of-Thought shows potential for making Speech-to-Text Translation more accessible across diverse languages despite trade-offs in high-resource scenarios.

**Abstract:** We propose a Speech-to-Text Translation (S2TT) approach that integrates phoneme representations into a Chain-of-Thought (CoT) framework to improve translation in low-resource and zero-resource settings. By introducing phoneme recognition as an intermediate step, we enhance cross-lingual transfer, enabling translation even for languages with no labeled speech data. Our system builds on a multilingual LLM, which we extend to process speech and phonemes. Training follows a curriculum learning strategy that progressively introduces more complex tasks. Experiments on multilingual S2TT benchmarks show that phoneme-augmented CoT improves translation quality in low-resource conditions and enables zero-resource translation, while slightly impacting high-resource performance. Despite this trade-off, our findings demonstrate that phoneme-based CoT is a promising step toward making S2TT more accessible across diverse languages.

</details>


### [172] [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)

*Tejul Pandit, Meet Raval, Dhvani Upadhyay*

**Main category:** cs.CL

**Keywords:** Aspect-Based Sentiment Analysis, Synthetic Data, Large Language Models, Sentiment Classification, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a method for generating synthetic datasets for Aspect-Based Sentiment Analysis (ABSA) using Large Language Models (LLMs) to alleviate data scarcity issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of diverse, labeled datasets in ABSA that capture real-world conversational nuances necessitates the generation of synthetic datasets.

**Method:** Synthetic ABSA data is generated using LLMs, specifically GPT-4o, ensuring consistent topic and sentiment distributions across multiple domains.

**Key Contributions:**

	1. Development of a LLM-based synthetic data generation method for ABSA
	2. Evaluation of three state-of-the-art LLMs on synthetic ABSA data
	3. Insights on performance trade-offs among different LLMs in sentiment classification tasks

**Result:** The generated synthetic data was evaluated on three LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek-R1) for topic and sentiment classification, showing varied precision, recall, and inference speed among the models.

**Limitations:** 

**Conclusion:** LLM-based synthetic data generation is an effective approach for creating useful ABSA datasets, supporting research and model evaluation.

**Abstract:** Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.

</details>


### [173] [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/abs/2505.24712)

*Guido Ivetta, Marcos J. Gomez, Sofía Martinelli, Pietro Palombini, M. Emilia Echeveste, Nair Carolina Mazzeo, Beatriz Busaniche, Luciana Benotti*

**Main category:** cs.CL

**Keywords:** bias assessment, language models, education, intersectionality, dataset

**Relevance Score:** 9

**TL;DR:** HESEIA is a new dataset comprising 46,499 sentences designed to evaluate intersectional biases in Large Language Models, developed through participatory methods involving teachers and students in Latin America.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of community involvement in developing tools for evaluating biases in Large Language Models, particularly in educational contexts.

**Method:** The dataset was created during a professional development course for high-school teachers and students, using minimal pairs to reflect local biases and stereotypes across various school subjects and demographic axes.

**Key Contributions:**

	1. Introduction of a dataset reflecting intersectional biases in LLMs
	2. Involvement of educators and students in dataset creation
	3. Increased representation of local contexts and stereotypes

**Result:** The HESEIA dataset contains more unrecognized stereotypes in LLMs than existing datasets, representing a broad range of demographic and knowledge area diversity.

**Limitations:** 

**Conclusion:** HESEIA is intended to support more effective bias assessment in educational settings by incorporating the input of those most affected by these biases.

**Abstract:** Most resources for evaluating social biases in Large Language Models are developed without co-design from the communities affected by these biases, and rarely involve participatory approaches. We introduce HESEIA, a dataset of 46,499 sentences created in a professional development course. The course involved 370 high-school teachers and 5,370 students from 189 Latin-American schools. Unlike existing benchmarks, HESEIA captures intersectional biases across multiple demographic axes and school subjects. It reflects local contexts through the lived experience and pedagogical expertise of educators. Teachers used minimal pairs to create sentences that express stereotypes relevant to their school subjects and communities. We show the dataset diversity in term of demographic axes represented and also in terms of the knowledge areas included. We demonstrate that the dataset contains more stereotypes unrecognized by current LLMs than previous datasets. HESEIA is available to support bias assessments grounded in educational communities.

</details>


### [174] [Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification](https://arxiv.org/abs/2505.24713)

*Badr M. Abdullah, Matthew Baas, Bernd Möbius, Dietrich Klakow*

**Main category:** cs.CL

**Keywords:** Arabic dialect identification, voice conversion, speech technology, machine learning, cross-domain evaluation

**Relevance Score:** 6

**TL;DR:** This paper presents a novel voice conversion-based approach for training Arabic dialect identification (ADI) models that enhances their performance and robustness across different domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current Arabic dialect identification systems, particularly their poor generalization to out-of-domain speech, thereby supporting the development of speech technologies for Arabic language varieties.

**Method:** An effective voice conversion technique is used to train ADI models, which is tested against a newly collected cross-domain dataset.

**Key Contributions:**

	1. Introduction of voice conversion in Arabic dialect identification systems
	2. Significant performance improvement across domains
	3. Release of a robust ADI model and newly collected cross-domain evaluation dataset

**Result:** The proposed approach achieves state-of-the-art performance, showing consistent improvements of up to +34.1% in accuracy across four different domains.

**Limitations:** 

**Conclusion:** The study successfully enhances the reliability of ADI systems through voice conversion, which also helps mitigate speaker bias, contributing to more inclusive speech technologies.

**Abstract:** Arabic dialect identification (ADI) systems are essential for large-scale data collection pipelines that enable the development of inclusive speech technologies for Arabic language varieties. However, the reliability of current ADI systems is limited by poor generalization to out-of-domain speech. In this paper, we present an effective approach based on voice conversion for training ADI models that achieves state-of-the-art performance and significantly improves robustness in cross-domain scenarios. Evaluated on a newly collected real-world test set spanning four different domains, our approach yields consistent improvements of up to +34.1% in accuracy across domains. Furthermore, we present an analysis of our approach and demonstrate that voice conversion helps mitigate the speaker bias in the ADI dataset. We release our robust ADI model and cross-domain evaluation dataset to support the development of inclusive speech technologies for Arabic.

</details>


### [175] [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714)

*Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Financial Research, Dataset Evaluation

**Relevance Score:** 3

**TL;DR:** Introduction of FinMME, a specialized multimodal dataset for financial research, and the FinScore evaluation system.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of effective multimodal evaluation datasets in the finance domain for the development of Multimodal Large Language Models (MLLMs).

**Method:** Creation of FinMME dataset with over 11,000 samples across 18 financial domains and 6 asset classes, validated by 20 annotators. Development of FinScore evaluation system incorporating hallucination penalties and multi-dimensional assessments.

**Key Contributions:**

	1. Introduction of FinMME dataset for evaluating MLLMs in finance
	2. Development of FinScore for unbiased evaluation with hallucination penalties
	3. Demonstration of dataset robustness and challenges for state-of-the-art models

**Result:** Experimental results show that even advanced models like GPT-4o struggle with FinMME, demonstrating the dataset's challenging nature and high robustness under varying prompts.

**Limitations:** 

**Conclusion:** The dataset and evaluation protocol provide a significant resource for evaluating MLLMs in finance, emphasizing the limitations of current models.

**Abstract:** Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.

</details>


### [176] [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726)

*Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, Waseem AlShikh*

**Main category:** cs.CL

**Keywords:** self-reflection, reinforcement learning, language models, performance improvement, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a method using self-reflection and reinforcement learning to enhance the performance of large language models, demonstrating significant improvements in task success rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models, particularly in challenging tasks where generating synthetic data is infeasible and only binary feedback is available.

**Method:** The approach involves two stages: generating self-reflective commentary after a failed task attempt and then using that commentary in a second attempt at the task. Successful attempts lead to rewards for the self-reflection tokens.

**Key Contributions:**

	1. A two-stage self-reflection framework for language models
	2. Demonstrated performance improvement even with smaller models
	3. Innovative use of reinforcement learning in self-improvement tasks

**Result:** Substantial performance gains across different model architectures were observed, with improvements up to 34.7% in math equation writing and 18.1% in function calling, notably with smaller models outperforming larger ones in the same family.

**Limitations:** 

**Conclusion:** The proposed framework offers a novel approach to developing more effective and reliable language models capable of self-improvement with limited external feedback.

**Abstract:** We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.

</details>


### [177] [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)

*Alan Sun*

**Main category:** cs.CL

**Keywords:** language models, circuit stability, mechanistic interpretability, generalization, evaluation metrics

**Relevance Score:** 7

**TL;DR:** This paper introduces circuit stability as a new metric for evaluating large language models, relating model performance to interpretability through empirical studies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid development of language models leads to benchmark saturation, making it challenging to evaluate their true capabilities.

**Method:** The authors introduce and mathematically formalize the concept of circuit stability, assessing a model's reasoning consistency across various inputs, and conduct three empirical case studies to demonstrate its effectiveness.

**Key Contributions:**

	1. Introduction of circuit stability as a new evaluation metric for language models
	2. Mathematical formalization of circuit stability and circuit equivalence
	3. Empirical demonstration of the relationship between circuit stability and model generalization

**Result:** The case studies indicate that circuit stability can effectively characterize and predict generalization aspects of models.

**Limitations:** 

**Conclusion:** The proposed methods pave the way for a better understanding of how model interpretability relates to their generalization capabilities.

**Abstract:** Extensively evaluating the capabilities of (large) language models is difficult. Rapid development of state-of-the-art models induce benchmark saturation, while creating more challenging datasets is labor-intensive. Inspired by the recent developments in mechanistic interpretability, we introduce circuit stability as a new way to assess model performance. Circuit stability refers to a model's ability to apply a consistent reasoning process-its circuit-across various inputs. We mathematically formalize circuit stability and circuit equivalence. Then, through three case studies, we empirically show that circuit stability and the lack thereof can characterize and predict different aspects of generalization. Our proposed methods offer a step towards rigorously relating the generality of models to their interpretability.

</details>


### [178] [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)

*Yingchaojie Feng, Yiqun Sun, Yandong Sun, Minfeng Zhu, Qiang Huang, Anthony K. H. Tung, Wei Chen*

**Main category:** cs.CL

**Keywords:** Instruction-following, Text embedding, Guided Space Transformation, Efficiency, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents GSTransform, a framework for efficient instruction-following text embedding that adapts pre-computed embeddings rather than re-encoding the entire corpus.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The work addresses the inefficiency of existing instruction-following text embedding approaches, which require significant computational resources by re-encoding for each instruction.

**Method:** GTransform utilizes Guided Space Transformation to adapt pre-computed embeddings dynamically to user instructions, leveraging minimal instruction-focused labeled data for real-time alignment.

**Key Contributions:**

	1. Introduction of GSTransform, a novel lightweight framework for instruction-following text embedding.
	2. Demonstration of significant speed improvements (6~300x) in processing over existing methods.
	3. Extensive validation of the framework across various real-world datasets and tasks.

**Result:** GTransform significantly enhances the quality of instruction-following text embeddings and achieves speed improvements of 6 to 300 times compared to current state-of-the-art methods across multiple datasets.

**Limitations:** 

**Conclusion:** The empirical results suggest that GSTransform provides a scalable solution for instruction-following tasks with improved efficiency and quality in embedding generation.

**Abstract:** In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.

</details>


### [179] [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)

*Christian Jaumann, Andreas Wiedholz, Annemarie Friedrich*

**Main category:** cs.CL

**Keywords:** Systematic Literature Reviews, Large Language Models, Abstract Screening

**Relevance Score:** 8

**TL;DR:** The paper introduces LGAR, a zero-shot LLM Guided Abstract Ranker for systematic literature reviews (SLRs) that improves abstract screening efficiency and accuracy compared to existing QA-based methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rapid growth of scientific literature, it is becoming increasingly challenging to perform systematic literature reviews (SLRs) effectively. The need for better abstract screening methods using large language models (LLMs) is evident due to inadequacies in current approaches.

**Method:** The authors propose LGAR, which includes an LLM based graded relevance scorer and a dense re-ranker to evaluate the inclusion and exclusion criteria of SLRs in a zero-shot setting.

**Key Contributions:**

	1. Introduction of LGAR, a novel zero-shot LLM Guided Abstract Ranker
	2. Manual extraction of inclusion and exclusion criteria for 57 SLRs
	3. Demonstrated improvement in performance over existing methods

**Result:** LGAR achieves a 5-10 percentage point improvement in mean average precision compared to traditional QA-based abstract screening methods.

**Limitations:** 

**Conclusion:** The proposed method demonstrates significant potential for enhancing the abstract screening process in systematic literature reviews, particularly in the medical domain, with publicly available code and data.

**Abstract:** The scientific literature is growing rapidly, making it hard to keep track of the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and evaluate all relevant papers on a topic. After retrieving a set of candidate papers, the abstract screening phase determines initial relevance. To date, abstract screening methods using large language models (LLMs) focus on binary classification settings; existing question answering (QA) based ranking approaches suffer from error propagation. LLMs offer a unique opportunity to evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks do not provide them exhaustively. We manually extract these criteria as well as research questions for 57 SLRs, mostly in the medical domain, enabling principled comparisons between approaches. Moreover, we propose LGAR, a zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance scorer and a dense re-ranker. Our extensive experiments show that LGAR outperforms existing QA-based methods by 5-10 pp. in mean average precision. Our code and data is publicly available.

</details>


### [180] [From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning](https://arxiv.org/abs/2505.24768)

*Haoyu Li, Xuhong Li, Yiming Dong, Kun Liu*

**Main category:** cs.CL

**Keywords:** machine learning, dataset diversity, large language models, fine-tuning, supervised learning

**Relevance Score:** 9

**TL;DR:** This paper presents a taxonomy of dataset diversity strategies for supervised fine-tuning of large language models, analyzing their effects on performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically analyze dataset diversity, which is crucial for effective machine learning model training, particularly in large language models.

**Method:** The authors constructed fixed-size datasets from 117,000 open-source SFT samples, applying six diversity-control strategies at macro, meso, and microscopic levels, and fine-tuned LLMs to evaluate performance.

**Key Contributions:**

	1. Systematic taxonomy of diversity-control strategies for LLM training datasets.
	2. Novel analysis of microscopic diversity in response components.
	3. Empirical evaluation of diversity strategies leading to actionable insights for dataset construction.

**Result:** Macroscopic and mesoscopic strategies improve performance with higher diversity, whereas the microscopic strategy demonstrates the strongest correlation with model performance and achieves superior outcomes with maximum diversity.

**Limitations:** 

**Conclusion:** Findings provide actionable insights for building effective SFT datasets through an understanding of diversity strategies.

**Abstract:** Dataset diversity plays a pivotal role for the successful training of many machine learning models, particularly in the supervised fine-tuning (SFT) stage of large language model (LLM) development. Despite increasing recognition of its importance, systematic analyses of dataset diversity still remain underexplored. To address this gap, this work presents a systematic taxonomy of existing diversity-control strategies, which primarily focus on the instruction component, operating at either macroscopic (entire instruction semantics) or mesoscopic levels (instruction units), and furthermore introduces a novel analysis of microscopic diversity within the response component, specifically analyzing the statistical distribution of tokens in SFT training samples. In the experimental evaluation, we construct fixed-size datasets (e.g., 10,000 samples each) from a corpus of 117,000 open-source SFT samples, incorporating six distinct diversity-control strategies spanning macro-, meso-, and microscopic levels applied to both instructions and responses. We then fine-tune LLMs on these datasets to assess the six diversity-control strategies. Results reveal that while macroscopic and mesoscopic strategies lead to higher performance with increasing diversity, the microscopic strategy in responses exhibits both a stronger correlation between model performance and the degree of diversity and superior performance with maximum diversity across all strategies. These findings offer actionable insights for constructing high-performance SFT datasets.

</details>


### [181] [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/abs/2505.24778)

*Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, Confidence Estimation, Epistemic Markers, Machine Learning, High-Stakes Domains

**Relevance Score:** 9

**TL;DR:** This paper evaluates the consistency of epistemic markers used by large language models (LLMs) to express confidence in high-stakes scenarios, revealing challenges in accurately assessing model uncertainty.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Assess the reliability of LLMs' use of epistemic markers for confidence estimation due to their growing role in high-stakes domains.

**Method:** We define marker confidence as the accuracy of responses when models use epistemic markers, and evaluate it across various question-answering datasets in both in-distribution and out-of-distribution settings.

**Key Contributions:**

	1. Definition and evaluation of marker confidence for LLMs.
	2. Analysis of stability across in-distribution and out-of-distribution settings.
	3. Release of code for further research on confidence estimation.

**Result:** Markers generalize well within the same distribution but exhibit inconsistent confidence in out-of-distribution scenarios.

**Limitations:** Focus on specific datasets may limit generalizability of findings.

**Conclusion:** The reliability of epistemic markers for confidence estimation in LLMs is questionable, highlighting the need for better alignment with actual model uncertainty.

**Abstract:** As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., "fairly confident") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon.

</details>


### [182] [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)

*Houjun Liu, John Bauer, Christopher D. Manning*

**Main category:** cs.CL

**Keywords:** dropout, language models, pretraining, editability, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the role of dropout during single-epoch pretraining of large language models and finds that dropout negatively impacts performance and editability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To empirically analyze the effects of dropout in language model pretraining, particularly in the context of large language models where dropout is typically not used due to minimal overfitting.

**Method:** Experiments were conducted on single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) language models with varying levels of dropout applied.

**Key Contributions:**

	1. Empirical analysis of dropout's impact in LM pretraining
	2. Identification of improved performance metrics without dropout
	3. Insights into model editability related to dropout usage

**Result:** Downstream performance improved in tasks such as language modeling, morpho-syntax, question answering, and natural-language inference when dropout was not applied during pretraining. Additionally, models without dropout were more successful in gradient-based model editing.

**Limitations:** 

**Conclusion:** The findings suggest dropping dropout during single-epoch pretraining as it enhances model performance and editability.

**Abstract:** Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced "early dropout" also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.

</details>


### [183] [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/abs/2505.24803)

*Zhijun Pan, Antonios Andronis, Eva Hayek, Oscar AP Wilkinson, Ilya Lasy, Annette Parry, Guy Gadney, Tim J. Smith, Mick Grierson*

**Main category:** cs.CL

**Keywords:** large language models, knowledge graphs, story generation, user control, interactive storytelling

**Relevance Score:** 9

**TL;DR:** The paper explores the enhancement of LLM-based storytelling using knowledge graphs, addressing coherence and user control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve long-form coherence and user control in automated story generation by utilizing knowledge graphs.

**Method:** A KG-assisted storytelling pipeline was proposed and evaluated through a user study with 15 participants who created story prompts and edited knowledge graphs.

**Key Contributions:**

	1. Proposes a KG-assisted storytelling pipeline
	2. Demonstrates the impact of knowledge graphs on narrative quality
	3. Increases user engagement through interactive modifications

**Result:** Knowledge graphs significantly improved story quality and enhanced user engagement and control during storytelling, especially in action-oriented narratives.

**Limitations:** 

**Conclusion:** The integration of knowledge graphs in storytelling pipelines enhances narrative quality and user interaction, making the process more engaging.

**Abstract:** Large Language Models (LLMs) have shown great potential in automated story generation, but challenges remain in maintaining long-form coherence and providing users with intuitive and effective control. Retrieval-Augmented Generation (RAG) has proven effective in reducing hallucinations in text generation; however, the use of structured data to support generative storytelling remains underexplored. This paper investigates how knowledge graphs (KGs) can enhance LLM-based storytelling by improving narrative quality and enabling user-driven modifications. We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study with 15 participants. Participants created their own story prompts, generated stories, and edited knowledge graphs to shape their narratives. Through quantitative and qualitative analysis, our findings demonstrate that knowledge graphs significantly enhance story quality in action-oriented and structured narratives within our system settings. Additionally, editing the knowledge graph increases users' sense of control, making storytelling more engaging, interactive, and playful.

</details>


### [184] [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)

*Li yunhan, Wu gengshen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Legal Applications, Quality Evaluation, Clarity, Coherence

**Relevance Score:** 7

**TL;DR:** This paper proposes a framework for evaluating legal LLMs focused on linguistic quality aspects such as clarity and coherence, revealing insights into model performance and limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for legal LLMs focus primarily on factual accuracy, neglecting important linguistic quality dimensions.

**Method:** Development of a regression model to assess legal text quality, creation of a specialized question set, and analysis of 49 LLMs using this evaluation framework.

**Key Contributions:**

	1. Development of new evaluation framework for legal LLMs
	2. Identification of parameter thresholds for model performance
	3. Release of ranking list for legal LLMs

**Result:** Finding that model quality levels off at 14 billion parameters, with slight improvements at 72 billion; engineering choices have little impact, and reasoning models outperform base architectures.

**Limitations:** Neglects to explore the impact of some engineering choices thoroughly; primarily focused on legal applications which may not generalize beyond this domain.

**Conclusion:** The research establishes standardized evaluation protocols for legal LLMs and spotlights limitations in current training data refinement; Qwen3 series identified as the best for cost-performance.

**Abstract:** As large language models (LLMs) are increasingly used in legal applications, current evaluation benchmarks tend to focus mainly on factual accuracy while largely neglecting important linguistic quality aspects such as clarity, coherence, and terminology. To address this gap, we propose three steps: First, we develop a regression model to evaluate the quality of legal texts based on clarity, coherence, and terminology. Second, we create a specialized set of legal questions. Third, we analyze 49 LLMs using this evaluation framework.   Our analysis identifies three key findings: First, model quality levels off at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at 72 billion parameters. Second, engineering choices such as quantization and context length have a negligible impact, as indicated by statistical significance thresholds above 0.016. Third, reasoning models consistently outperform base architectures. A significant outcome of our research is the release of a ranking list and Pareto analysis, which highlight the Qwen3 series as the optimal choice for cost-performance tradeoffs. This work not only establishes standardized evaluation protocols for legal LLMs but also uncovers fundamental limitations in current training data refinement approaches. Code and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

</details>


### [185] [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)

*Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken*

**Main category:** cs.CL

**Keywords:** large language models, medical AI, explainability, fact-checking, clinical applications

**Relevance Score:** 10

**TL;DR:** Introducing a framework that enhances the reliability and explainability of LLMs in medical Q&A by decomposing responses into verifiable atomic facts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucinations and inaccurate citations of LLMs in clinical settings, which hinder their adoption in healthcare.

**Method:** A novel atomic fact-checking framework that breaks down LLM-generated responses into atomic facts independently verified against medical guidelines.

**Key Contributions:**

	1. Development of atomic fact-checking framework for LLM responses
	2. Demonstrated improvements in factual accuracy and explainability
	3. Independent verification against medical guidelines for reliability

**Result:** The framework showed up to 40% improvement in answer accuracy and 50% reduction in hallucinations, enhancing explainability by tracing facts to source literature.

**Limitations:** 

**Conclusion:** This framework significantly improves the trustworthiness of LLMs in clinical applications, which is essential for regulatory compliance and effective healthcare delivery.

**Abstract:** Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.

</details>


### [186] [How much do language models memorize?](https://arxiv.org/abs/2505.24832)

*John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar*

**Main category:** cs.CL

**Keywords:** language models, memorization, generalization, model capacity, scaling laws

**Relevance Score:** 8

**TL;DR:** This paper presents a method to estimate the memorization and generalization of language models, revealing that GPT-style models have a capacity of about 3.6 bits per parameter.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand how language models memorize information and the distinction between memorization and generalization.

**Method:** The study separates memorization into unintended memorization and generalization. It involves training multiple transformer language models while observing the relationship between model capacity and dataset size.

**Key Contributions:**

	1. Introduction of a method to estimate memorization and generalization in language models
	2. Quantitative analysis revealing GPT-style models' capacity
	3. Establishment of scaling laws relating model capacity and data size

**Result:** The findings indicate that as model capacity fills, unintended memorization decreases and generalization starts, with scaling laws established regarding model capacity and data size related to membership inference.

**Limitations:** 

**Conclusion:** The research provides insights into the limits of language models and the balance between memorization and generalization, contributing to understanding model capacity and training dynamics.

**Abstract:** We propose a new method for estimating how much a model ``knows'' about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: \textit{unintended memorization}, the information a model contains about a specific dataset, and \textit{generalization}, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point ``grokking'' begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from $500K$ to $1.5B$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference.

</details>


### [187] [Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks](https://arxiv.org/abs/2505.24834)

*Roksana Goworek, Haim Dubossarsky*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, polysemy disambiguation, lexical semantic change, multilingual NLP, evaluation artifacts

**Relevance Score:** 6

**TL;DR:** This paper challenges the assumption that increased multilinguality benefits cross-lingual transfer for polysemy disambiguation and lexical semantic change, revealing that confounding factors play a more significant role.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the effectiveness of multilingual training in cross-lingual transfer tasks, specifically in polysemy disambiguation and lexical semantic change, across 28 languages.

**Method:** The authors conducted a large-scale analysis to evaluate the effects of multilingual training versus other influencing factors on model performance.

**Key Contributions:**

	1. Challenged the assumption of the benefits of multilinguality in NLP tasks.
	2. Identified confounding factors affecting perceived multilingual advantages.
	3. Provided benchmarks and fine-tuned models for further research.

**Result:** The findings indicate that multilingual training is not necessary or inherently beneficial for effective transfer; confounding factors are better predictors of success.

**Limitations:** The study focuses on specific underexplored tasks, which may not generalize to all NLP applications.

**Conclusion:** The paper suggests that there is a need for more rigorous evaluations in multilingual NLP and provides benchmarks and models for future research.

**Abstract:** Cross-lingual transfer allows models to perform tasks in languages unseen during training and is often assumed to benefit from increased multilinguality. In this work, we challenge this assumption in the context of two underexplored, sense-aware tasks: polysemy disambiguation and lexical semantic change. Through a large-scale analysis across 28 languages, we show that multilingual training is neither necessary nor inherently beneficial for effective transfer. Instead, we find that confounding factors - such as fine-tuning data composition and evaluation artifacts - better account for the perceived advantages of multilinguality. Our findings call for more rigorous evaluations in multilingual NLP. We release fine-tuned models and benchmarks to support further research, with implications extending to low-resource and typologically diverse languages.

</details>


### [188] [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)

*Gabrielle Kaili-May Liu, Gal Yona, Avi Caciularu, Idan Szpektor, Tim G. J. Rudner, Arman Cohan*

**Main category:** cs.CL

**Keywords:** LLM, uncertainty communication, confidence calibration, MetaFaith, trustworthiness

**Relevance Score:** 9

**TL;DR:** This study investigates the ability of LLMs to communicate uncertainty accurately and introduces MetaFaith, a new approach that significantly improves this calibration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Reliable uncertainty communication in LLMs is crucial for trustworthiness, yet these models often exhibit assertive language even when uncertain, leading to user mistrust.

**Method:** A systematic benchmarking of LLMs' ability to express their uncertainty was conducted, examining various models, datasets, and prompting strategies. MetaFaith was introduced as a novel prompt-based calibration approach inspired by human metacognition.

**Key Contributions:**

	1. Systematic study of faithful confidence calibration in LLMs
	2. Introduction of MetaFaith, a novel prompt-based calibration method
	3. Demonstration of substantial improvements in faithfulness of uncertainty communication

**Result:** The evaluation revealed that LLMs generally fail in accurately communicating uncertainty, with MetaFaith significantly enhancing faithful calibration by up to 61% and achieving an 83% win rate over previous methods based on human judgment.

**Limitations:** 

**Conclusion:** MetaFaith represents an effective solution for improving the faithfulness of LLMs in terms of uncertainty communication, thereby enhancing user trust.

**Abstract:** A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of $\textit{faithful confidence calibration}$ of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that $\textit{faithfully reflect}$ their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.

</details>


### [189] [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863)

*Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang*

**Main category:** cs.CL

**Keywords:** Reasoning Models, AlphaOne, Modulation Framework, Efficient Reasoning, NLP

**Relevance Score:** 8

**TL;DR:** AlphaOne ($\alpha$1) is a framework that improves the reasoning capabilities of large reasoning models by dynamically controlling the pace of reasoning through a system of scheduled transitions between slow and fast thinking phases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning capabilities in large reasoning models by modulating their reasoning speed at test time.

**Method:** The framework introduces $\\alpha$ moment to scale reasoning phases and uses a Bernoulli stochastic process to schedule transitions of reasoning states, terminating with an end-of-thinking token.

**Key Contributions:**

	1. Introduction of $\\alpha$ moment for reasoning speed modulation
	2. Dynamically schedules slow thinking transitions
	3. Unification of existing monotonic scaling methods

**Result:** Empirical studies show that $\\alpha$1 outperforms existing methods in reasoning capability and efficiency across various challenging benchmarks in mathematics, coding, and science.

**Limitations:** 

**Conclusion:** The $\\alpha$1 framework offers a generalizable approach to modulating reasoning processes, making it effective for diverse applications.

**Abstract:** This paper presents AlphaOne ($\alpha$1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. $\alpha$1 first introduces $\alpha$ moment, which represents the scaled thinking phase with a universal parameter $\alpha$. Within this scaled pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate $\alpha$1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/

</details>


### [190] [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)

*Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong*

**Main category:** cs.CL

**Keywords:** reinforcement learning, reasoning, language models, ProRL, AI

**Relevance Score:** 7

**TL;DR:** This paper introduces ProRL, a methodology leveraging reinforcement learning (RL) to enhance reasoning strategies in language models, showing significant improvements over base models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether reinforcement learning can genuinely expand reasoning capabilities in language models beyond existing latent outputs.

**Method:** ProRL incorporates KL divergence control, reference policy resetting, and diverse task sets to train language models using reinforcement learning techniques.

**Key Contributions:**

	1. Introduction of ProRL methodology for RL training.
	2. Demonstration that RL can discover new reasoning strategies.
	3. Empirical results showing superior task performance of RL-trained models compared to base models.

**Result:** Empirical analysis demonstrates that RL-trained models consistently outperform base models across various pass@k evaluations, revealing novel reasoning capabilities not present in base models.

**Limitations:** 

**Conclusion:** Prolonged RL training can uncover new reasoning strategies, suggesting potential advancements in reasoning performance for language models when using long-horizon RL methods.

**Abstract:** Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

</details>


### [191] [Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models](https://arxiv.org/abs/2401.08491)

*Tassilo Klein, Moin Nabi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Toxic Content Generation, Contrastive Learning

**Relevance Score:** 9

**TL;DR:** A framework for reducing toxic outputs in LLMs through adversarial paraphrasing and fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of toxic content generation by large language models (LLMs) for safer deployment of language technology.

**Method:** A prototype-based contrastive perplexity objective is employed, which uses hard negatives generated through adversarial paraphrasing, ensuring robust contrastive optimization during training.

**Key Contributions:**

	1. Introduction of a novel framework for implicit knowledge editing in LLMs.
	2. Use of adversarial paraphrasing to create hard negatives for effective training.
	3. Demonstration of reduced toxicity without compromising downstream task performance.

**Result:** The method significantly reduces toxic content generation while maintaining strong performance on tasks like commonsense reasoning and reading comprehension.

**Limitations:** 

**Conclusion:** Exploiting hard negatives for attribute-aware fine-tuning is effective in detoxifying LLM outputs.

**Abstract:** The generation of toxic content by large language models (LLMs) remains a critical challenge for the safe deployment of language technology. We propose a novel framework for implicit knowledge editing and controlled text generation by fine-tuning LLMs with a prototype-based contrastive perplexity objective. Central to our method is the construction of hard negatives - toxic outputs that are generated through adversarial paraphrasing to be semantically similar and model probability to their non-toxic counterparts. By training on these challenging and realistic pairs, our approach ensures robust and stable contrastive optimization. Experimental results in the domain of detoxification demonstrate that our method significantly reduces toxic generation while maintaining strong performance on downstream tasks such as commonsense reasoning and reading comprehension. Our findings highlight the effectiveness of exploiting hard negatives for attribute-aware fine-tuning.

</details>


### [192] [An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4](https://arxiv.org/abs/2403.02839)

*Hui Huang, Xingyuan Bu, Hongli Zhou, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, LLM Evaluation, Empirical Study, Generalization, Fairness

**Relevance Score:** 9

**TL;DR:** This paper empirically evaluates the efficacy of fine-tuned judge models for assessing the quality of other LLMs, revealing limitations in generalizability, fairness, and adaptability compared to GPT-4.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of fine-tuned LLMs in evaluating the performance of other LLMs, challenging the assumption that state-of-the-art models always provide superior evaluation.

**Method:** An empirical study comparing fine-tuned judge models against GPT-4 across various dimensions such as generalizability, fairness, and adaptability, using in-domain and out-of-domain test sets.

**Key Contributions:**

	1. Empirical evaluation of fine-tuned judge models using LLMs
	2. Comparison of performance metrics between fine-tuned models and GPT-4
	3. Insights into the limitations of LLMs as evaluators.

**Result:** Fine-tuned judge models show high performance on in-domain test sets but significantly underperform GPT-4 in generalizability, fairness, and adaptability.

**Limitations:** Fine-tuned models perform well in specific domains but lack generalizability and adaptability.

**Conclusion:** Fine-tuned judge models act as task-specific classifiers, leading to inherent limitations that hinder their evaluation capabilities compared to GPT-4.

**Abstract:** Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge models based on open-source LLMs for evaluation. While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of LLM-as-a-Judge. Our findings indicate that although the fine-tuned judge models achieve high performance on in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across several dimensions, including generalizability, fairness and adaptability. We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier, consequently imposing the limitations.

</details>


### [193] [Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization](https://arxiv.org/abs/2405.14189)

*Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu*

**Main category:** cs.CL

**Keywords:** prompt injection, LLMs, goal hijacking, optimization algorithms, prompt organization

**Relevance Score:** 8

**TL;DR:** The paper proposes POUGH, a method for universal goal hijacking in LLMs that uses efficient optimization and prompt organization strategies to enhance prompt injection attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for LLM prompt injection attacks are cumbersome and focus mainly on optimization, neglecting the prompt's role.

**Method:** POUGH employs a sampling strategy to select representative prompts, followed by a ranking strategy to prioritize them. An iterative optimization algorithm generates a fixed suffix for universal goal hijacking.

**Key Contributions:**

	1. Introduction of the POUGH method for prompt injection attacks
	2. Innovative use of semantics-guided prompt organization
	3. Demonstrated effectiveness on multiple LLMs and response types

**Result:** Experiments on four popular LLMs and ten target response types demonstrated the effectiveness of the proposed method.

**Limitations:** 

**Conclusion:** POUGH is a more efficient approach to prompt injection that improves the performance of universal goal hijacking in large language models compared to existing methods.

**Abstract:** Universal goal hijacking is a kind of prompt injection attack that forces LLMs to return a target malicious response for arbitrary normal user prompts. The previous methods achieve high attack performance while being too cumbersome and time-consuming. Also, they have concentrated solely on optimization algorithms, overlooking the crucial role of the prompt. To this end, we propose a method called POUGH that incorporates an efficient optimization algorithm and two semantics-guided prompt organization strategies. Specifically, our method starts with a sampling strategy to select representative prompts from a candidate pool, followed by a ranking strategy that prioritizes them. Given the sequentially ranked prompts, our method employs an iterative optimization algorithm to generate a fixed suffix that can concatenate to arbitrary user prompts for universal goal hijacking. Experiments conducted on four popular LLMs and ten types of target responses verified the effectiveness.

</details>


### [194] [Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning](https://arxiv.org/abs/2406.10099)

*Jiaqi Li, Yixuan Tang, Yi Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucinations, Contextual Question Answering, US-Tuning, Knowledge Boundaries

**Relevance Score:** 8

**TL;DR:** This paper presents US-Tuning, a two-stage approach to improve large language models' ability to acknowledge knowledge gaps during contextual question answering, resulting in reduced hallucinations and higher performance in answering unknown queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models are prone to hallucinations due to insufficient recognition of their knowledge limitations, impacting their reliability in question answering.

**Method:** US-Tuning involves a two-stage process: first, enhancing the model's awareness of its knowledge boundaries; second, reinforcing adherence to instructions using causal prompts.

**Key Contributions:**

	1. Introduction of US-Tuning as a novel method for LLMs.
	2. Demonstrated reduction of hallucinations in QA tasks.
	3. Improvement of LLM performance on out-of-knowledge questions.

**Result:** The US-Tuning approach significantly reduces incorrect answers and enhances models' faithfulness, with experimental results showing a 34.7% improvement in handling out-of-knowledge questions and a 4.2% performance increase over GPT-4.

**Limitations:** 

**Conclusion:** US-Tuning effectively mitigates hallucinations in question answering tasks and enhances LLMs' ability to recognize their limitations, leading to improved user trust and model reliability.

**Abstract:** Large language models (LLMs) demonstrate remarkable capabilities but face challenges from hallucinations, which typically arise from insufficient knowledge or context. While instructing LLMs to acknowledge knowledge limitations by responding with "I don't know" appears promising, we find that models consistently struggle with admitting knowledge gaps. This challenge may originate from current instruction datasets that emphasise answer generation over knowledge boundary awareness. To address this limitation, we introduce Uncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage approach for contextual question answering (QA). The first stage enhances LLMs' ability to recognise their knowledge boundaries, while the second stage reinforces instruction adherence through carefully designed causal prompts. Our experimental results demonstrate that US-Tuning not only significantly reduces incorrect answers in contextual QA but also improves models' faithfulness to their parametric knowledge, mitigating hallucinations in general QA tasks. Our fine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling out-of-knowledge questions and outperforms GPT-4 by 4.2% in overall performance.

</details>


### [195] [Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration](https://arxiv.org/abs/2406.16469)

*ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo*

**Main category:** cs.CL

**Keywords:** Vision-language models, Cultural benchmarking, Korean culture, Semi-automated framework, QA evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents a semi-automated framework for creating benchmarks to test vision-language models (VLMs) on culturally relevant questions, exemplified through a dataset focused on Korean culture called K-Viscuit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for culturally inclusive VLMs, but existing methods rely heavily on human annotators, which is labor-intensive and limits diversity in questions.

**Method:** The proposed framework combines VLM question generation based on guidelines and annotated examples, followed by verification from native speakers.

**Key Contributions:**

	1. Development of a semi-automated framework for cultural VLM benchmarks
	2. Creation of the K-Viscuit dataset focused on Korean culture
	3. Insights into the performance gap between open-source and proprietary VLMs in cultural understanding.

**Result:** Experiments revealed that open-source models do not perform as well as proprietary ones in understanding Korean culture, providing insights for future improvements.

**Limitations:** The approach may still rely on some human input, and its effectiveness in other cultures or languages is not yet tested.

**Conclusion:** The framework and the K-Viscuit dataset allow for a better assessment of VLMs in culturally relevant contexts.

**Abstract:** To create culturally inclusive vision-language models (VLMs), developing a benchmark that tests their ability to address culturally relevant questions is essential. Existing approaches typically rely on human annotators, making the process labor-intensive and creating a cognitive burden in generating diverse questions. To address this, we propose a semi-automated framework for constructing cultural VLM benchmarks, specifically targeting multiple-choice QA. This framework combines human-VLM collaboration, where VLMs generate questions based on guidelines, a small set of annotated examples, and relevant knowledge, followed by a verification process by native speakers. We demonstrate the effectiveness of this framework through the creation of \texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on this dataset reveal that open-source models lag behind proprietary ones in understanding Korean culture, highlighting key areas for improvement. We also present a series of further analyses, including human evaluation, augmenting VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.

</details>


### [196] [NativQA: Multilingual Culturally-Aligned Natural Query for LLMs](https://arxiv.org/abs/2407.09823)

*Md. Arid Hasan, Maram Hasanain, Fatema Ahmad, Sahinur Rahman Laskar, Sunaya Upadhyay, Vrunda N Sukhadia, Mucahid Kutlu, Shammur Absar Chowdhury, Firoj Alam*

**Main category:** cs.CL

**Keywords:** LLMs, Native, Multilingual, Language Diversity, Culturally Informed

**Relevance Score:** 8

**TL;DR:** The paper introduces NativQA, a framework for creating culturally and regionally aligned QA datasets in native languages for the evaluation of large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of frameworks for large-scale region-specific datasets based on queries from native users in their own languages, which hampers effective benchmarking and model development.

**Method:** We propose NativQA, a language-independent framework for constructing culturally and regionally aligned QA datasets, exemplified by the MultiNativQA dataset consisting of 64k QA pairs in seven languages.

**Key Contributions:**

	1. Development of NativQA framework for QA dataset construction
	2. Creation of multilingual dataset MultiNativQA with 64k QA pairs
	3. Benchmarking framework for evaluating LLMs in culturally aligned contexts.

**Result:** The MultiNativQA dataset has been created with contributions from native speakers across 9 regions and demonstrates effective benchmarking of various LLMs.

**Limitations:** 

**Conclusion:** Making the MultiNativQA dataset and related scripts publicly available enhances accessibility for future research and model development.

**Abstract:** Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed and some work has been done in parallel, there is a notable lack of a framework and large scale region-specific datasets queried by native users in their own languages. This gap hinders the effective benchmarking and the development of fine-tuned models for regional and cultural specificities. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, MultiNativQA, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA dataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other experimental scripts(https://gitlab.com/nativqa/multinativqa) publicly available for the community.

</details>


### [197] [Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment](https://arxiv.org/abs/2407.14878)

*Yongxin Huang, Kexin Wang, Goran Glavaš, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** multilingual sentence encoders, modular training, cross-lingual alignment, semantic similarity, low-resource languages

**Relevance Score:** 7

**TL;DR:** This paper proposes a modular training approach for multilingual sentence encoders to improve accuracy and task performance by training language-specific monolingual modules and aligning them with cross-lingual adapters.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the curse of multilinguality and the trade-off between different task performances in multilingual sentence encoders (MSEs).

**Method:** The authors train language-specific monolingual modules to reduce negative interference and then align non-English embeddings to English using cross-lingual alignment adapters based on two types of data. This modular approach is contrasted with traditional full-parameter training of monolithic MSEs.

**Key Contributions:**

	1. Introduction of modular training for multilingual sentence encoders
	2. Development of language-specific monolingual modules
	3. Design of cross-lingual alignment adapters to improve task performance

**Result:** The proposed modular solution demonstrates superior and more balanced performance across monolingual and cross-lingual tasks, particularly in semantic text similarity, bitext mining, and sentence classification, especially for low-resource languages.

**Limitations:** 

**Conclusion:** The modular training approach enhances the utility of sentence embeddings in both monolingual and cross-lingual tasks by minimizing interference and addressing the conflicting requirements of different tasks.

**Abstract:** Multilingual sentence encoders (MSEs) are commonly obtained by training multilingual language models to map sentences from different languages into a shared semantic space. As such, they are subject to curse of multilinguality, a loss of monolingual representational accuracy due to parameter sharing. Another limitation of MSEs is the trade-off between different task performance: cross-lingual alignment training distorts the optimal monolingual structure of semantic spaces of individual languages, harming the utility of sentence embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual semantic similarity and zero-shot transfer for sentence classification, may also require conflicting cross-lingual alignment strategies. In this work, we address both issues by means of modular training of sentence encoders. We first train language-specific monolingual modules to mitigate negative interference between languages (i.e., the curse). We then align all non-English sentence embeddings to the English by training cross-lingual alignment adapters, preventing interference with monolingual specialization from the first step. We train the cross-lingual adapters with two different types of data to resolve the conflicting requirements of different cross-lingual tasks. Monolingual and cross-lingual results on semantic text similarity and relatedness, bitext mining and sentence classification show that our modular solution achieves better and more balanced performance across all the tasks compared to full-parameter training of monolithic multilingual sentence encoders, especially benefiting low-resource languages.

</details>


### [198] [MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU](https://arxiv.org/abs/2408.08144)

*Yan Li, So-Eon Kim, Seong-Bae Park, Soyeon Caren Han*

**Main category:** cs.CL

**Keywords:** Natural Language Understanding, Multi-turn Dialogue, Knowledge Distillation

**Relevance Score:** 9

**TL;DR:** This paper introduces MIDAS, a multi-level knowledge distillation approach for Natural Language Understanding (NLU) in multi-turn dialogues, addressing the limitations of existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing NLU models struggle with understanding user intent in multi-turn dialogues, necessitating advancements in the interpretation of complex conversations.

**Method:** MIDAS utilizes a multi-teacher framework that includes separate models for sentence-level intent detection, word-level slot filling, and conversation-level domain classification, integrated through a multi-teacher loss.

**Key Contributions:**

	1. Introduction of a multi-teacher framework for NLU
	2. Novel multi-level intent and domain classification
	3. Open-source implementation for broader accessibility

**Result:** The model shows significant improvements in multi-turn conversation understanding, effectively demonstrating the value of multi-level dialogue knowledge distillation.

**Limitations:** 

**Conclusion:** MIDAS enhances NLU capabilities for multi-turn dialogues, offering a promising avenue for future research in this area.

**Abstract:** Although Large Language Models (LLMs) can generate coherent text, they often struggle to recognise user intent behind queries. In contrast, Natural Language Understanding (NLU) models interpret the purpose and key information of user input for responsive interactions. Existing NLU models typically map utterances to a dual-level semantic frame, involving sentence-level intent (SI) and word-level slot (WS) labels. However, real-life conversations primarily consist of multi-turn dialogues, requiring the interpretation of complex and extended exchanges. Researchers encounter challenges in addressing all facets of multi-turn dialogue using a unified NLU model. This paper introduces MIDAS, a novel approach leveraging multi-level intent, domain, and slot knowledge distillation for multi-turn NLU. We construct distinct teachers for SI detection, WS filling, and conversation-level domain (CD) classification, each fine-tuned for specific knowledge. A multi-teacher loss is proposed to facilitate the integration of these teachers, guiding a student model in multi-turn dialogue tasks. Results demonstrate the efficacy of our model in improving multi-turn conversation understanding, showcasing the potential for advancements in NLU through multi-level dialogue knowledge distillation. Our implementation is open-sourced on https://github.com/adlnlp/Midas.

</details>


### [199] [EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts](https://arxiv.org/abs/2408.12226)

*Nicy Scaria, Silvester John Joseph Kennedy, Thomas Latinovich, Deepak Subramani*

**Main category:** cs.CL

**Keywords:** CEFR, LLMs, language assessment, e-learning, automated evaluation

**Relevance Score:** 9

**TL;DR:** The paper presents EvalYaks, a family of models designed to automate the evaluation of CEFR B2 English speaking assessments using LLMs, achieving high accuracy and scalability in e-learning environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for scalability in evaluating CEFR speaking assessments in e-learning environments, which is hindered by reliance on human experts.

**Method:** Evaluation of leading LLMs on CEFR B2 speaking assessment criteria, creation of a synthetic conversational dataset, and instruction tuning of Mistral Instruct 7B.

**Key Contributions:**

	1. Development of EvalYaks, a family of specialized models for CEFR assessments.
	2. Creation of expert-validated, CEFR-aligned synthetic datasets.
	3. High performance measured in terms of accuracy and scalability for language evaluation.

**Result:** EvalYaks achieved an average accuracy of 96% and outperformed the next best model by 3 times.

**Limitations:** 

**Conclusion:** Implementing LLMs for CEFR B2 speaking assessments provides a viable solution for automated language proficiency evaluation.

**Abstract:** Relying on human experts to evaluate CEFR speaking assessments in an e-learning environment creates scalability challenges, as it limits how quickly and widely assessments can be conducted. We aim to automate the evaluation of CEFR B2 English speaking assessments in e-learning environments from conversation transcripts. First, we evaluate the capability of leading open source and commercial Large Language Models (LLMs) to score a candidate's performance across various criteria in the CEFR B2 speaking exam in both global and India-specific contexts. Next, we create a new expert-validated, CEFR-aligned synthetic conversational dataset with transcripts that are rated at different assessment scores. In addition, new instruction-tuned datasets are developed from the English Vocabulary Profile (up to CEFR B2 level) and the CEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform parameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called EvalYaks. Four models in this family are for assessing the four sections of the CEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and generating level-specific vocabulary, and another for detecting the CEFR level of text and generating level-specific text. EvalYaks achieved an average acceptable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times better than the next best model. This demonstrates that a 7B parameter LLM instruction tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments, offering a promising solution for scalable, automated language proficiency evaluation.

</details>


### [200] [BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla](https://arxiv.org/abs/2409.11638)

*Mahammed Kamruzzaman, Abdullah Al Monsur, Shrabon Das, Enamul Hassan, Gene Louis Kim*

**Main category:** cs.CL

**Keywords:** bias evaluation, multilingual LLMs, Bangla language, stereotypical bias, language technology

**Relevance Score:** 7

**TL;DR:** The study introduces BanStereoSet, a dataset aimed at evaluating social biases in multilingual LLMs for the Bangla language, highlighting significant biases in several language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To extend bias research beyond English-centric datasets and address stereotypical social biases within the Bangla-speaking community.

**Method:** The research involved localizing content from several existing bias datasets to create BanStereoSet, which includes 1,194 sentences across 9 bias categories.

**Key Contributions:**

	1. Introduction of BanStereoSet for evaluating biases in Bangla language LLMs.
	2. Coverage of 9 bias categories relevant to Bangla-speaking communities.
	3. Reinforcement of the need for localized datasets in AI research.

**Result:** Analysis of language models using BanStereoSet revealed significant biases, indicating the need for culturally appropriate datasets.

**Limitations:** 

**Conclusion:** The study emphasizes that developing more equitable language technologies requires linguistically and culturally adapted datasets.

**Abstract:** This study presents BanStereoSet, a dataset designed to evaluate stereotypical social biases in multilingual LLMs for the Bangla language. In an effort to extend the focus of bias research beyond English-centric datasets, we have localized the content from the StereoSet, IndiBias, and Kamruzzaman et. al.'s datasets, producing a resource tailored to capture biases prevalent within the Bangla-speaking community. Our BanStereoSet dataset consists of 1,194 sentences spanning 9 categories of bias: race, profession, gender, ageism, beauty, beauty in profession, region, caste, and religion. This dataset not only serves as a crucial tool for measuring bias in multilingual LLMs but also facilitates the exploration of stereotypical bias across different social categories, potentially guiding the development of more equitable language technologies in Bangladeshi contexts. Our analysis of several language models using this dataset indicates significant biases, reinforcing the necessity for culturally and linguistically adapted datasets to develop more equitable language technologies.

</details>


### [201] [DemoShapley: Valuation of Demonstrations for In-Context Learning](https://arxiv.org/abs/2410.07523)

*Shan Xie, Man Luo, Chadly Daniel Stern, Mengnan Du, Lu Cheng*

**Main category:** cs.CL

**Keywords:** large language models, in-context learning, demonstration selection, Beta distribution, fairness

**Relevance Score:** 8

**TL;DR:** Proposes DemoShapley and Beta-DemoShapley to improve demonstration selection in in-context learning for LLMs, enhancing performance and robustness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Demonstration selection and ordering in in-context learning significantly influence model effectiveness.

**Method:** Introduces DemoShapley and Beta-DemoShapley to assess the influence of individual demonstrations, with improvements over fixed demonstration methods.

**Key Contributions:**

	1. Develops DemoShapley for dynamic demonstration selection
	2. Introduces Beta-DemoShapley for better weighting of demonstrations
	3. Improves LLM performance and fairness through effective demo selection

**Result:** Algorithms enhance model performance by improving selection of quality demonstrations and generalization to out-of-distribution tasks, while identifying noise in data.

**Limitations:** 

**Conclusion:** The proposed methods promote fairness and robustness in LLMs across various scenarios.

**Abstract:** Large language models (LLMs) using in-context learning (ICL) excel in many tasks without task-specific fine-tuning. However, demonstration selection and ordering greatly impact ICL effectiveness. To address this, we propose DemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to assess the influence of individual demonstrations. DemoShapley captures how each example influences performance in different contexts, unlike other influence-based methods that rely on a fixed number of demonstrations. Beta-DemoShapley further enhances this framework by incorporating the Beta distribution, allowing users to assign higher weights to smaller cardinalities, which aligns with ICL's prompt length and computational constraints. Our findings show that the proposed algorithms improve model performance by selecting quality demonstrations, and enhancing generalization to out-of-distribution tasks. It also identifies noise-compromised data and promotes fairness in LLMs, protecting model performance and ensuring robustness across various scenarios.

</details>


### [202] [ChuLo: Chunk-Level Key Information Representation for Long Document Processing](https://arxiv.org/abs/2410.11119)

*Yan Li, Soyeon Caren Han, Yue Dai, Feiqi Cao*

**Main category:** cs.CL

**Keywords:** Transformer models, long document understanding, keyphrase extraction

**Relevance Score:** 8

**TL;DR:** ChuLo is a novel chunk representation method that enhances Transformer-based models' ability to handle long documents by using unsupervised keyphrase extraction to minimize information loss.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing methods for handling long documents in Transformer-based models that lead to information loss and hinder long-range dependencies.

**Method:** ChuLo groups input tokens using unsupervised keyphrase extraction, creating semantically important keyphrase-based chunks while reducing input length to enhance efficiency in long document understanding.

**Key Contributions:**

	1. Introduction of the ChuLo method for long document understanding
	2. Use of unsupervised keyphrase extraction to minimize input length while preserving essential content
	3. Demonstrated effectiveness across multiple classification tasks

**Result:** The evaluation shows that ChuLo significantly improves long document classification and token classification tasks, preserving fine-grained annotations crucial for effective context understanding.

**Limitations:** 

**Conclusion:** ChuLo effectively mitigates information loss in long document understanding tasks, thereby improving the overall performance of Transformer-based models.

**Abstract:** Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document understanding that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunks to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analysis. Our implementation is open-sourced on https://github.com/adlnlp/Chulo.

</details>


### [203] [From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence](https://arxiv.org/abs/2410.13460)

*Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus*

**Main category:** cs.CL

**Keywords:** court systems, case prioritization, multilingual models

**Relevance Score:** 3

**TL;DR:** The paper introduces a novel dataset for evaluating case prioritization in court systems, demonstrating the efficacy of smaller fine-tuned models over larger models in domain-specific tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overwhelming backlogs in court systems worldwide by creating an effective case prioritization system akin to triage in emergency rooms.

**Method:** Development of the Criticality Prediction dataset with a two-tier labeling system and evaluation of various multilingual models in prioritizing legal cases.

**Key Contributions:**

	1. Introduction of the Criticality Prediction dataset for legal case prioritization
	2. Development of a two-tier labeling system for case evaluation
	3. Demonstration that smaller fine-tuned models can outperform larger models in this specific domain

**Result:** Fine-tuned models consistently outperform larger models due to the larger training dataset, emphasizing the importance of extensive labeled data in specialized domains.

**Limitations:** 

**Conclusion:** Large training sets are crucial for domain-specific tasks, despite common perceptions favoring larger models.

**Abstract:** Many court systems are overwhelmed all over the world, leading to huge backlogs of pending cases. Effective triage systems, like those in emergency rooms, could ensure proper prioritization of open cases, optimizing time and resource allocation in the court system. In this work, we introduce the Criticality Prediction dataset, a novel resource for evaluating case prioritization. Our dataset features a two-tier labeling system: (1) the binary LD-Label, identifying cases published as Leading Decisions (LD), and (2) the more granular Citation-Label, ranking cases by their citation frequency and recency, allowing for a more nuanced evaluation. Unlike existing approaches that rely on resource-intensive manual annotations, we algorithmically derive labels leading to a much larger dataset than otherwise possible. We evaluate several multilingual models, including both smaller fine-tuned models and large language models in a zero-shot setting. Our results show that the fine-tuned models consistently outperform their larger counterparts, thanks to our large training set. Our results highlight that for highly domain-specific tasks like ours, large training sets are still valuable.

</details>


### [204] [Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models](https://arxiv.org/abs/2410.14248)

*Olga Loginova, Oleksandr Bezrukov, Ravi Shekhar, Alexey Kravets*

**Main category:** cs.CL

**Keywords:** Video Language Models, Multiple-Choice Question Answering, Selection Bias, Bias Mitigation, Model Performance

**Relevance Score:** 4

**TL;DR:** This paper investigates selection bias in Video Language Models (VLMs) through Multiple-Choice Question Answering (MCQA) benchmarks and introduces a calibration technique to improve model performance and reduce bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to address the limitations of existing MCQA benchmarks that do not fully capture the reasoning capabilities of VLMs due to selection bias.

**Method:** Empirical analysis of various VLM architectures using major benchmarks focused on video reasoning, introducing the BOLD post-processing calibration technique to balance selection bias.

**Key Contributions:**

	1. Introduces a novel post-processing calibration technique (BOLD) to reduce selection bias in VLMs
	2. Identifies critical areas of selection bias in MCQA benchmarks for VLMs
	3. Demonstrates improved model performance through bias reduction methods

**Result:** The research demonstrates that reducing selection bias leads to improved performance metrics such as Accuracy and F1 Mean score, and the BOLD technique is more cost-effective in mitigating bias than previous approaches.

**Limitations:** The study focuses on specific types of VLM architectures and datasets, which may limit the generalizability of the findings.

**Conclusion:** Overall, this study highlights the need for careful evaluation of VLMs regarding selection bias and presents a novel method to enhance their reasoning abilities.

**Abstract:** Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing "blind guessing", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.

</details>


### [205] [Dialectal Coverage And Generalization in Arabic Speech Recognition](https://arxiv.org/abs/2411.05872)

*Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Arabic Dialects, Code-Switching, Machine Learning

**Relevance Score:** 4

**TL;DR:** This paper presents a suite of Arabic ASR models designed to manage the linguistic diversity of spoken Arabic, including various dialects and code-switching.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing ASR systems primarily focus on Modern Standard Arabic and some dialects, lacking comprehensive coverage of the diverse spoken variants and facing challenges due to code-switching.

**Method:** The authors developed and tested multiple ASR models optimized for recognizing MSA, various Arabic dialects, and code-switched speech. They provided open-source pre-trained models based on data from 17 Arab countries and conducted evaluations to assess performance.

**Key Contributions:**

	1. Introduction of ASR models for diverse Arabic dialects and code-switching
	2. Open-source pre-trained models from a broad dataset
	3. Demonstration of performance improvements over prior ASR models

**Result:** The models show improved coverage and performance across different spoken Arabic varieties compared to existing systems.

**Limitations:** 

**Conclusion:** The study successfully introduces robust ASR systems that enhance the recognition capabilities for multiple Arabic linguistic variants and code-switching contexts.

**Abstract:** Developing robust automatic speech recognition (ASR) systems for Arabic requires effective strategies to manage its diversity. Existing ASR systems mainly cover the modern standard Arabic (MSA) variety and few high-resource dialects, but fall short in coverage and generalization across the multitude of spoken variants. Code-switching with English and French is also common in different regions of the Arab world, which challenges the performance of monolingual Arabic models. In this work, we introduce a suite of ASR models optimized to effectively recognize multiple variants of spoken Arabic, including MSA, various dialects, and code-switching. We provide open-source pre-trained models that cover data from 17 Arabic-speaking countries, and fine-tuned MSA and dialectal ASR models that include at least 11 variants, as well as multi-lingual ASR models covering embedded languages in code-switched utterances. We evaluate ASR performance across these spoken varieties and demonstrate both coverage and performance gains compared to prior models.

</details>


### [206] [Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework](https://arxiv.org/abs/2411.06160)

*Jingyi Zhou, Senlin Luo, Haofan Chen*

**Main category:** cs.CL

**Keywords:** emotion detection, micro-emotions, energy intensity, emotion quantization, NLP

**Relevance Score:** 8

**TL;DR:** The paper introduces the Emotion Quantization Network (EQN) framework for detecting and annotating micro-emotions in text by mapping label values to energy intensity levels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing emotion detection datasets face issues with manual annotations, label imbalances, and inadequate representation of micro-emotions, necessitating more effective methods for emotion detection.

**Method:** The authors propose an all-labels and training-set label regression method that allows for mapping label values to energy intensity levels which enhances the detection of multiple emotions in text samples.

**Key Contributions:**

	1. EQN framework for micro-emotion detection
	2. Energy-level score annotation methodology
	3. Broad applicability in NLP machine learning models

**Result:** The EQN framework was validated through experiments on five sentiment datasets, showing high capability for automatic micro-emotion detection and annotation, outperforming previous models.

**Limitations:** There is a misstatement in the EQN Framework section that needs clarification.

**Conclusion:** The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing a strong basis for advanced emotion detection analysis.

**Abstract:** Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.

</details>


### [207] [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)

*Shantanu Acharya, Fei Jia, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Star Attention, Large Language Models, Computational Efficiency, Transformer, Machine Learning

**Relevance Score:** 8

**TL;DR:** Star Attention is a two-phase block-sparse approximation that enhances computational efficiency for LLMs by reducing memory and inference time while maintaining accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the computational efficiency of Transformer-based LLMs on long sequences due to the limitations of the self-attention mechanism.

**Method:** Star Attention uses a two-phase approach: first, it applies blockwise-local attention across multiple hosts, and then it employs sequence-global attention for cached tokens.

**Key Contributions:**

	1. Introduced a two-phase block-sparse approximation for attention in LLMs.
	2. Achieved up to 11x reduction in inference time and memory usage.
	3. Maintained high accuracy (97-100%) across various tasks.

**Result:** The proposed method reduces memory requirements and inference time by up to 11x, while achieving 97-100% accuracy retention.

**Limitations:** 

**Conclusion:** Star Attention effectively integrates with existing Transformer-based LLMs, offering a significant optimization in terms of resource utilization.

**Abstract:** Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.

</details>


### [208] [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905)

*Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Dialogue Systems, Large Language Models, Benchmarking, Imitation Learning, Human-Machine Interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces a new research task, Dialogue Element Modeling, aimed at improving LLM-based dialogue systems through a novel benchmark called DEMO, focusing on comprehensive dialogue element modeling and assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of systematic investigation into dialogue stages affecting the modeling and assessment of LLMs-based dialogue systems, which hinders their performance.

**Method:** The paper proposes a research task called Dialogue Element Modeling, introducing two key components: Element Awareness and Dialogue Agent Interaction. A benchmark named DEMO is developed for this purpose, and a DEMO agent is built using imitation learning to enhance dialogue element modeling.

**Key Contributions:**

	1. Introduction of the Dialogue Element Modeling task
	2. Development of the DEMO benchmark for comprehensive dialogue assessment
	3. Creation of a DEMO agent leveraging imitation learning for dialogue modeling

**Result:** Extensive experiments show that while current LLMs demonstrate potential, the DEMO agent performs well in modeling dialogue elements and is effective in out-of-domain tasks.

**Limitations:** 

**Conclusion:** The introduction of the DEMO benchmark and agent addresses existing gaps in dialogue system modeling, providing a structured approach to assessing LLM-based dialogues.

**Abstract:** Large language models (LLMs) enabled dialogue systems have become one of the central modes in human-machine interaction, which bring about vast amounts of conversation logs and increasing demand for dialogue generation. The dialogue's life-cycle spans from $\textit{Prelude}$ through $\textit{Interlocution}$ to $\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes of dialogue-related studies, there is a lack of systematic investigation into the dialogue stages to frame benchmark construction that covers comprehensive dialogue elements. This hinders the precise modeling, generation and assessment of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce a new research task--$\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. On this basis, we further build the DEMO agent with the adept ability to model dialogue elements via imitation learning. Extensive experiments on DEMO indicate that current representative LLMs still have considerable potential for enhancement, and our DEMO agent performs well in both dialogue element modeling and out-of-domain tasks.

</details>


### [209] [Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation](https://arxiv.org/abs/2412.08473)

*Huiyuan Lai, Esther Ploeger, Rik van Noord, Antonio Toral*

**Main category:** cs.CL

**Keywords:** neural machine translation, reinforcement learning, natural language processing, translation accuracy, lexical diversity

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel reinforcement learning method to improve neural machine translation (NMT) by balancing naturalness and content preservation, demonstrating its effectiveness on English-to-Dutch literary translations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** NMT systems often reflect lexical biases in training data, which degrades the naturalness and accuracy of translations. The need for more natural output without sacrificing content accuracy drives the research.

**Method:** The proposed method utilizes reinforcement learning from human feedback to reward translations that are both more natural and accurate, evaluated through various perspectives in the context of literary translation.

**Key Contributions:**

	1. Introduces a novel reinforcement learning approach for NMT
	2. Balances translation naturalness with content preservation
	3. Demonstrates improvements in lexical richness and human-like properties in translations.

**Result:** The best model generated translations that are lexically richer and closer to human-written text, while maintaining high accuracy compared to traditional methods.

**Limitations:** 

**Conclusion:** The method successfully enhances NMT output by reducing translationese and improving lexical diversity without compromising content fidelity.

**Abstract:** Neural machine translation (NMT) systems amplify lexical biases present in their training data, leading to artificially impoverished language in output translations. These language-level characteristics render automatic translations different from text originally written in a language and human translations, which hinders their usefulness in for example creating evaluation datasets. Attempts to increase naturalness in NMT can fall short in terms of content preservation, where increased lexical diversity comes at the cost of translation accuracy. Inspired by the reinforcement learning from human feedback framework, we introduce a novel method that rewards both naturalness and content preservation. We experiment with multiple perspectives to produce more natural translations, aiming at reducing machine and human translationese. We evaluate our method on English-to-Dutch literary translation, and find that our best model produces translations that are lexically richer and exhibit more properties of human-written language, without loss in translation accuracy.

</details>


### [210] [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972)

*Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, William Yang Wang*

**Main category:** cs.CL

**Keywords:** large language models, rule-based reasoning, benchmarking, natural language instructions, real-world applications

**Relevance Score:** 9

**TL;DR:** RuleArena is a benchmark for evaluating LLMs on complex rule-following tasks in real-world scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities of large language models in following intricate, real-world rules across several domains, highlighting their limitations and guiding future research.

**Method:** RuleArena tests LLMs' proficiency in reasoning through long-context natural language instructions related to airline baggage fees, NBA transactions, and tax regulations.

**Key Contributions:**

	1. Introduction of RuleArena as a novel LLM evaluation benchmark.
	2. Identification of key limitations in LLMs regarding rule application and mathematical accuracy.
	3. Demonstration of performance improvement when LLMs use external tools.

**Result:** LLMs struggled significantly, often failing to apply the right rules and perform accurate computations, but showed improvement with external tools for complex operations.

**Limitations:** LLMs often confused similar rules and performed poorly in computations without external assistance.

**Conclusion:** The study reveals substantial challenges for LLMs in rule-guided reasoning and suggests areas for further research, particularly in enhancing their reliability for real-life applications.

**Abstract:** This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/RuleArena.

</details>


### [211] [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/abs/2412.12567)

*Seunghee Kim, Changhyeon Kim, Taeuk Kim*

**Main category:** cs.CL

**Keywords:** multimodal, large language models, multi-hop reasoning, financial domain, benchmarking

**Relevance Score:** 8

**TL;DR:** This paper introduces the Financial Cross-Modal Multi-Hop Reasoning (FCMR) benchmark to evaluate the reasoning capabilities of multimodal large language models in financial contexts, highlighting their current inadequacies through multi-hop reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Real-world decision-making requires integrating and reasoning over information from multiple modalities, yet existing benchmarks for multimodal large language models are insufficient for evaluating complex multi-hop reasoning.

**Method:** The authors present the Financial Cross-Modal Multi-Hop Reasoning (FCMR) benchmark, which includes tasks that require multi-modal information integration from texts, tables, and charts in the financial domain, categorized into three levels of difficulty.

**Key Contributions:**

	1. Introduction of the FCMR benchmark for financial multi-hop reasoning
	2. Detailed analysis of model performance highlighting critical limitations
	3. Categorization of tasks into varying difficulty levels to facilitate assessment

**Result:** Experiments show that state-of-the-art multimodal large language models perform poorly on this benchmark, with the best model only achieving 30.4% accuracy on the most challenging tasks, exposing the need for better reasoning capabilities.

**Limitations:** The benchmark focuses only on the financial domain and may not fully represent reasoning capabilities in other contexts.

**Conclusion:** The FCMR benchmark is a crucial step for evaluating MLLMs' reasoning skills, uncovering a significant bottleneck in information retrieval within these models.

**Abstract:** Real-world decision-making often requires integrating and reasoning over information from multiple modalities. While recent multimodal large language models (MLLMs) have shown promise in such tasks, their ability to perform multi-hop reasoning across diverse sources remains insufficiently evaluated. Existing benchmarks, such as MMQA, face challenges due to (1) data contamination and (2) a lack of complex queries that necessitate operations across more than two modalities, hindering accurate performance assessment. To address this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a benchmark created to analyze the reasoning capabilities of MLLMs by urging them to combine information from textual reports, tables, and charts within the financial domain. FCMR is categorized into three difficulty levels-Easy, Medium, and Hard-facilitating a step-by-step evaluation. In particular, problems at the Hard level require precise cross-modal three-hop reasoning and are designed to prevent the disregard of any modality. Experiments on this new benchmark reveal that even state-of-the-art MLLMs struggle, with the best-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the most challenging tier. We also conduct analysis to provide insights into the inner workings of the models, including the discovery of a critical bottleneck in the information retrieval phase.

</details>


### [212] [A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI](https://arxiv.org/abs/2412.13942)

*Beiduo Chen, Siyao Peng, Anna Korhonen, Barbara Plank*

**Main category:** cs.CL

**Keywords:** Human Judgment Distributions, Large Language Models, Explanations, Human Label Variation, Natural Language Inference

**Relevance Score:** 9

**TL;DR:** This paper explores the use of large language models (LLMs) to generate explanations for human labels to approximate human judgment distributions (HJDs), comparing these LLM-generated explanations to human-generated ones.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to streamline the process of generating explanations for human labeling, which is currently time-consuming, by exploring whether LLMs can replace human effort in this aspect.

**Method:** The study uses LLMs as annotators to generate explanations for a limited number of human labels and investigates methods to combine these label-explanations for approximating HJDs.

**Key Contributions:**

	1. LLMs can approximate human judgment distributions using generated explanations.
	2. The approach generalizes to datasets without human explanations and to challenging out-of-distribution test cases.
	3. The study compares human and model-generated explanations, highlighting the effectiveness of LLMs.

**Result:** The experiments reveal that LLM-generated explanations can yield results for estimating HJDs that are comparable to those obtained from human explanations when provided with human labels. This finding holds true even for datasets without available human explanations and out-of-distribution test sets.

**Limitations:** The effectiveness of LLM-generated explanations may vary based on the complexity and context of the labels, and the need for validation across diverse datasets.

**Conclusion:** The research suggests that LLMs can effectively generate explanations that help in approximating human judgment distributions, making the process more efficient and less reliant on human input.

**Abstract:** Disagreement in human labeling is ubiquitous, and can be captured in human judgment distributions (HJDs). Recent research has shown that explanations provide valuable information for understanding human label variation (HLV) and large language models (LLMs) can approximate HJD from a few human-provided label-explanation pairs. However, collecting explanations for every label is still time-consuming. This paper examines whether LLMs can be used to replace humans in generating explanations for approximating HJD. Specifically, we use LLMs as annotators to generate model explanations for a few given human labels. We test ways to obtain and combine these label-explanations with the goal to approximate human judgment distributions. We further compare the resulting human with model-generated explanations, and test automatic and human explanation selection. Our experiments show that LLM explanations are promising for NLI: to estimate HJDs, generated explanations yield comparable results to human's when provided with human labels. Importantly, our results generalize from datasets with human explanations to i) datasets where they are not available and ii) challenging out-of-distribution test sets.

</details>


### [213] [Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph](https://arxiv.org/abs/2412.15268)

*Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li*

**Main category:** cs.CL

**Keywords:** toxicity detection, large language models, knowledge graph, social media, machine learning

**Relevance Score:** 8

**TL;DR:** MetaTox is a novel method for improving online toxicity detection using a meta-toxic knowledge graph, addressing false positives and negatives associated with LLMs.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of false negatives and false positives in toxicity detection using Large Language Models in social media content.

**Method:** Constructing a meta-toxic knowledge graph through LLMs and querying it to enhance toxic knowledge retrieval and ranking.

**Key Contributions:**

	1. Development of a meta-toxic knowledge graph
	2. Reduction of false positive rates in toxicity detection
	3. Improved overall performance in toxic speech identification

**Result:** MetaTox significantly reduces false positive rates while improving the performance of toxicity detection across various datasets.

**Limitations:** 

**Conclusion:** The proposed method offers a more reliable approach to toxicity detection in online content, balancing sensitivity and specificity.

**Abstract:** The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at https://github.com/YiboZhao624/MetaTox.

</details>


### [214] [Contrastive Learning for Task-Independent SpeechLLM-Pretraining](https://arxiv.org/abs/2412.15712)

*Maike Züfle, Jan Niehues*

**Main category:** cs.CL

**Keywords:** large language models, speech processing, contrastive learning

**Relevance Score:** 9

**TL;DR:** This paper presents a two-stage training approach for adapting LLMs to speech processing tasks, combining task-independent speech pretraining with minimal task-specific fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The adaptation of large language models to speech processing tasks faces challenges such as overfitting, data requirements, and high computational costs.

**Method:** A scalable, two-stage training approach: (1) Task-independent speech pretraining using contrastive learning to align text and speech representations, (2) Task-specific fine-tuning that requires minimal data.

**Key Contributions:**

	1. A novel two-stage training process for speech processing tasks using LLMs.
	2. Use of contrastive learning for task-independent speech pretraining.
	3. Significantly reduced data requirements for task-specific fine-tuning.

**Result:** The proposed method outperforms traditional ASR pretraining and surpasses models specifically designed for speech translation and question answering, even with only 10% of the task-specific data used for training.

**Limitations:** 

**Conclusion:** The two-stage training approach is effective for improving LLM performance on speech tasks and is more efficient in terms of data usage and computational costs compared to traditional methods.

**Abstract:** Large language models (LLMs) excel in natural language processing but adapting these LLMs to speech processing tasks efficiently is not straightforward. Direct task-specific fine-tuning is limited by overfitting risks, data requirements, and computational costs. To address these challenges, we propose a scalable, two-stage training approach: (1) A task-independent speech pretraining stage using contrastive learning to align text and speech representations over all layers, followed by (2) a task-specific fine-tuning stage requiring minimal data. This approach outperforms traditional ASR pretraining and enables the model to surpass models specialized on speech translation and question answering while being trained on only 10% of the task-specific data.

</details>


### [215] [CLIX: Cross-Lingual Explanations of Idiomatic Expressions](https://arxiv.org/abs/2501.03191)

*Aaron Gluck, Katharina von der Wense, Maria Leonor Pacheco*

**Main category:** cs.CL

**Keywords:** Cross-Lingual, Idiomatic Expressions, NLP Models, Language Learning, Automated Definition Generation

**Relevance Score:** 7

**TL;DR:** The paper introduces CLIX, a task focused on generating cross-lingual explanations of idiomatic expressions to aid language learners.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To support vocabulary expansion for language learners struggling with definitions due to complex language and non-standard expressions.

**Method:** The study investigates the capabilities of current NLP models for generating cross-lingual explanations of idiomatic expressions.

**Key Contributions:**

	1. Introduction of the CLIX task for cross-lingual explanation generation
	2. Detailed error analysis highlighting challenges for NLP models
	3. Exploration of large language models' capabilities in educational contexts

**Result:** The findings indicate that while the task is challenging, large language models show potential for improvement in generating understandable definitions.

**Limitations:** The necessity for further improvements in NLP models to handle non-standard language effectively is identified as a limitation.

**Conclusion:** Key challenges must be addressed before integrating these automated systems into educational tools for language learning.

**Abstract:** Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.

</details>


### [216] [AlphaPO: Reward Shape Matters for LLM Alignment](https://arxiv.org/abs/2501.03884)

*Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, S. Sathiya Keerthi*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Large Language Models, Reward Function, Direct Alignment Algorithms

**Relevance Score:** 9

**TL;DR:** This paper introduces AlphaPO, a novel Direct Alignment Algorithm (DAA) for improving the alignment of large language models by altering the reward function shape, leading to significant enhancements in performance compared to existing methods.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations in current Direct Alignment Algorithms (DAAs) related to likelihood displacement and the importance of reward function shape in training dynamics.

**Method:** The authors propose AlphaPO, which utilizes an $eta$-parameter to adjust the reward function shape, enhancing control over likelihood displacement and over-optimization.

**Key Contributions:**

	1. Introduction of AlphaPO as a new DAA.
	2. Demonstration of significant performance improvements in alignment tasks.
	3. Analysis of how reward shape influences training dynamics.

**Result:** AlphaPO demonstrates a 7% to 10% relative improvement in alignment performance over SimPO and a 15% to 50% improvement over DPO for models like Mistral-7B and Llama3-8B.

**Limitations:** 

**Conclusion:** The findings underscore the critical role of reward function shape in enhancing alignment performance in reinforcement learning for language models.

**Abstract:** Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Some popular examples of DAAs include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably. In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce \textbf{AlphaPO}, a new DAA method that leverages an $\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\% to 10\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B while achieving 15\% to 50\% relative improvement over DPO on the same models. The analysis and results presented highlight the importance of the reward shape and how one can systematically change it to affect training dynamics, as well as improve alignment performance.

</details>


### [217] [Autonomy-of-Experts Models](https://arxiv.org/abs/2501.13074)

*Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Autonomy-of-Experts, Machine Learning, Language Models, Expert Selection

**Relevance Score:** 7

**TL;DR:** Proposes Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm that enhances expert selection by allowing experts to autonomously assess their capacity to process inputs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The separation between the router's decision-making and the experts' execution in traditional MoE models leads to suboptimal expert selection and ineffective learning.

**Method:** AoE eliminates routers, allowing experts to autonomously compute their internal activations for inputs and rank themselves based on these activations, selecting only the top performers to process inputs.

**Key Contributions:**

	1. Introduced the Autonomy-of-Experts (AoE) model for MoE frameworks
	2. Demonstrated improved efficiency and performance over traditional MoE models
	3. Reduced overhead through low-rank weight factorization

**Result:** AoE demonstrates improved expert selection and effective learning compared to traditional MoE models, achieving similar efficiency with pre-trained language models.

**Limitations:** 

**Conclusion:** Removing routers and allowing experts to self-evaluate and select themselves leads to better outcomes in MoE frameworks.

**Abstract:** Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.

</details>


### [218] [M+: Extending MemoryLLM with Scalable Long-Term Memory](https://arxiv.org/abs/2502.00592)

*Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, Zexue He*

**Main category:** cs.CL

**Keywords:** large language models, memory augmentation, long-term retention

**Relevance Score:** 9

**TL;DR:** Introducing M+, a memory-augmented model that enhances long-term information retention in language models beyond 20k tokens.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing large language models in retaining information from the distant past, especially beyond 20k tokens.

**Method:** M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation.

**Key Contributions:**

	1. Development of M+, a memory-augmented model
	2. Significantly improved long-term information retention
	3. Open-sourcing of the model for further research

**Result:** M+ outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens while maintaining similar GPU memory overhead.

**Limitations:** 

**Conclusion:** M+ provides a significant advancement in long-term information retention for large language models, making it a promising tool for applications requiring extensive context.

**Abstract:** Equipping large language models (LLMs) with latent-space memory has attracted increasing attention as they can extend the context window of existing language models. However, retaining information from the distant past remains a challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative work with latent-space memory, compresses past information into hidden states across all layers, forming a memory pool of 1B parameters. While effective for sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k tokens. In this work, we address this limitation by introducing M+, a memory-augmented model based on MemoryLLM that significantly enhances long-term information retention. M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation. We evaluate M+ on diverse benchmarks, including long-context understanding and knowledge retention tasks. Experimental results show that M+ significantly outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens with similar GPU memory overhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM

</details>


### [219] [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/abs/2502.01349)

*Giorgos Filandrianos, Angeliki Dimitriou, Maria Lymperaiou, Konstantinos Thomas, Giorgos Stamou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cognitive Biases, Product Recommendations, Adversarial Manipulation, Human Psychology

**Relevance Score:** 8

**TL;DR:** This paper explores how cognitive biases affect the performance of Large Language Models in product recommendation systems, revealing vulnerabilities to adversarial manipulation and inconsistent outcomes based on different biases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the susceptibility of Large Language Models in real-world product recommender systems to adversarial manipulations by utilizing human psychological principles.

**Method:** The paper investigates cognitive biases as black-box adversarial strategies and evaluates their effects on various models, comparing them to human purchasing behavior.

**Key Contributions:**

	1. First application of cognitive biases in modifying product descriptions for LLMs
	2. Identified specific biases that affect product recommendation rates
	3. Highlighted unpredictable behavior of LLMs in practical applications.

**Result:** Certain cognitive biases, such as social proof, enhance product recommendation rates, while others, like scarcity and exclusivity, can paradoxically reduce visibility in recommendations.

**Limitations:** The study primarily focuses on cognitive biases without exploring other potential factors influencing LLM performance.

**Conclusion:** Cognitive biases are ingrained in LLMs, leading to unpredictable behaviors that can complicate mitigation strategies in product recommendations.

**Abstract:** The advent of Large Language Models (LLMs) has revolutionized product recommenders, yet their susceptibility to adversarial manipulation poses critical challenges, particularly in real-world commercial applications. Our approach is the first one to tap into human psychological principles, seamlessly modifying product descriptions, making such manipulations hard to detect. In this work, we investigate cognitive biases as black-box adversarial strategies, drawing parallels between their effects on LLMs and human purchasing behavior. Through extensive evaluation across models of varying scale, we find that certain biases, such as social proof, consistently boost product recommendation rate and ranking, while others, like scarcity and exclusivity, surprisingly reduce visibility. Our results demonstrate that cognitive biases are deeply embedded in state-of-the-art LLMs, leading to highly unpredictable behavior in product recommendations and posing significant challenges for effective mitigation.

</details>


### [220] [Boosting Multimodal Reasoning with Automated Structured Thinking](https://arxiv.org/abs/2502.02339)

*Jinyang Wu, Mingkuan Feng, Shuai Zhang, Fangrui Lv, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao*

**Main category:** cs.CL

**Keywords:** multimodal, visual reasoning, AStar, structured thinking, thought cards

**Relevance Score:** 8

**TL;DR:** AStar is an automated structured thinking paradigm that enhances multimodal reasoning using 'thought cards' for improved performance in complex visual reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current multimodal models in handling complex visual reasoning tasks, which suffer from inefficiency and resource demands in existing methods.

**Method:** AStar employs a library of high-level reasoning patterns called 'thought cards', retrieved adaptively for each test problem, combined with the model's implicit reasoning.

**Key Contributions:**

	1. Introduction of 'thought cards' for structured reasoning
	2. Training-free framework demonstrating high performance on visual reasoning tasks
	3. Plug-and-play compatibility with existing post-training techniques

**Result:** AStar achieved 53.9% accuracy on MathVerse and 32.7% on MathVision, outperforming GPT-4o in both cases with a training-free framework.

**Limitations:** 

**Conclusion:** AStar effectively improves multimodal reasoning capabilities and can generalize to other domains, integrating seamlessly with existing post-training techniques.

**Abstract:** Multimodal large language models excel across diverse domains but struggle with complex visual reasoning tasks. Current approaches aim to incorporate structured thinking via two strategies: explicit search methods and post-training techniques. However, both approaches face significant limitations: Search-based methods suffer from computational inefficiency due to extensive solution space exploration, while post-training methods require substantial data, computational resources, and often encounter training instability. To address these limitations, we propose AStar, an \textbf{A}utomated \textbf{S}tructured \textbf{t}hinking paradigm for multimod\textbf{a}l \textbf{r}easoning. Our method introduces "thought cards", a lightweight library of high-level reasoning patterns abstracted from 500 prior samples using Monte Carlo Tree Search. For each test problem, AStar adaptively retrieves the optimal thought cards and seamlessly integrates these external explicit guidelines with the model's internal implicit reasoning capabilities. Extensive experiments demonstrate AStar's effectiveness and efficiency: using only 500 prior samples and a 7B backbone, our training-free framework achieves 53.9$\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%) and 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that AStar generalizes beyond multimodal reasoning to visual perception and understanding domains, and serves as a plug-and-play test-time inference method compatible with mainstream post-training techniques like GRPO.

</details>


### [221] [A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)](https://arxiv.org/abs/2502.02659)

*Yan Li, Tianyi Zhang, Zechuan Li, Soyeon Caren Han*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long-context processing, Attention mechanisms

**Relevance Score:** 8

**TL;DR:** GALI is a training-free method enhancing long-context processing in LLMs by reusing positional intervals and interpolating attention logits to overcome existing positional O.O.D. challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing transformer-based LLMs face limitations when dealing with long inputs, particularly due to positional out-of-distribution issues that hinder attention mechanisms.

**Method:** GALI introduces Greedy Attention Logit Interpolation to improve length extrapolation by reusing pretrained positional intervals and interpolating attention logits without fine-tuning.

**Key Contributions:**

	1. Introduction of GALI for long-context processing
	2. Training-free method that improves length extrapolation
	3. Open-source implementation for community access

**Result:** GALI demonstrates stable and superior performance on long-context tasks and even enhances short-context performance by optimizing how positional intervals are interpreted.

**Limitations:** Potential reliance on the quality of pretrained positional intervals; may not address all long-context challenges.

**Conclusion:** GALI enhances the robustness and generalizability of LLMs for long-text processing, with open-source implementation available for further experimentation.

**Abstract:** Transformer-based Large Language Models (LLMs) struggle with inputs exceeding their training context window due to positional out-of-distribution (O.O.D.) issues that disrupt attention. Existing solutions, including fine-tuning and training-free methods, face challenges like inefficiency, redundant interpolation, logit outliers, or loss of local positional information. We propose Greedy Attention Logit Interpolation (GALI), a training-free method that improves length extrapolation by greedily reusing pretrained positional intervals and interpolating attention logit to eliminate outliers. GALI achieves stable and superior performance across a wide range of long-context tasks without requiring input-length-specific tuning. Our analysis further reveals that LLMs interpret positional intervals unevenly and that restricting interpolation to narrower ranges improves performance, even on short-context tasks. GALI represents a step toward more robust and generalizable long-text processing in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at https://github.com/adlnlp/Gali.

</details>


### [222] [Exploring Imbalanced Annotations for Effective In-Context Learning](https://arxiv.org/abs/2502.04037)

*Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-Context Learning, Class Imbalance, Conditional Bias, Demonstration Selection

**Relevance Score:** 8

**TL;DR:** The paper introduces RCB, a method to improve in-context learning performance in large language models by addressing class imbalance and conditional bias in demonstration selection.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models rely on demonstration selection from annotated datasets, which often suffer from class imbalances affecting performance in in-context learning.

**Method:** The proposed method, Reweighting with Conditional Bias (RCB), estimates conditional bias from a balanced subset and adjusts demonstration scores by considering both class weight and conditional bias.

**Key Contributions:**

	1. Introduction of RCB methodology for addressing class imbalance in ICL
	2. Demonstration of significant performance improvement in various benchmarks
	3. Insight into conditional bias and its impact on demonstration selection

**Result:** RCB enhances ICL performance and improves average accuracy of current selection methods by up to 5.42% across various tasks.

**Limitations:** 

**Conclusion:** RCB effectively prevents over-selection from dominant classes while maintaining the strengths of existing selection methods, leading to improved ICL outcomes in the presence of class imbalance.

**Abstract:** Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. However, these datasets often exhibit long-tailed class distributions in real-world scenarios, leading to biased demonstration selection. In this work, we show that such class imbalances significantly degrade the ICL performance across various tasks, regardless of selection methods. Moreover, classical rebalancing methods, which focus solely on class weights, yield poor performance due to neglecting condition bias--skewed feature distributions within classes. To address this, we propose Reweighting with Conditional Bias (dubbed RCB), a simple and complementary approach to enhance ICL performance under class imbalance. In particular, RCB estimates conditional bias using a balanced subset and re-weights demonstration scores based on both class weight and conditional bias. In effect, RCB prevents over-selection from dominant classes while preserving the efficacy of current selection methods. Extensive experiments on common benchmarks demonstrate the effectiveness of our method, improving the average accuracy of current selection methods by up to 5.42%.

</details>


### [223] [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)

*Amirbek Djanibekov, Hanan Aldarmaki*

**Main category:** cs.CL

**Keywords:** speech representations, large language models, speech-to-text translation, multi-modal processing, self-supervised learning

**Relevance Score:** 9

**TL;DR:** This study presents a novel approach that integrates self-supervised speech representations with instruction-tuned LLMs for improved speech-to-text translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing interest in enhancing multi-modal processing and speech understanding through the integration of speech representations with Large Language Models.

**Method:** The approach utilizes a modality adapter to align extracted speech features with instruction-tuned LLMs, specifically using English speech data.

**Key Contributions:**

	1. Novel integration of self-supervised speech representations with LLMs
	2. Effective semantic preservation in speech-to-text translation
	3. Development of a modality adapter for speech feature alignment

**Result:** Experiments show that the proposed method effectively preserves the semantic content of input speech, functioning as a bridge between self-supervised speech models and instruction-tuned LLMs.

**Limitations:** 

**Conclusion:** The integration of speech representations with instruction-tuned LLMs provides a promising approach for advancing speech understanding applications.

**Abstract:** With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that combines self-supervised speech representations with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English speech data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising approach for various speech understanding applications.

</details>


### [224] [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction](https://arxiv.org/abs/2502.11084)

*Yuting Huang, Chengyuan Liu, Yifeng Feng, Yiquan Wu, Chao Wu, Fei Wu, Kun Kuang*

**Main category:** cs.CL

**Keywords:** Large Language Models, jailbreak, instruction rewriting, LLM safety, deep learning

**Relevance Score:** 8

**TL;DR:** The paper presents R2J, a learnable and transferable black-box jailbreak method for Large Language Models that improves efficiency and reduces detection likelihood by rewriting instructions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety concerns associated with the misuse of Large Language Models by improving existing jailbreak methods.

**Method:** The authors propose the Rewrite to Jailbreak (R2J) approach, which utilizes a learning mechanism to iteratively exploit weaknesses in LLMs for more efficient and undetectable attacks.

**Key Contributions:**

	1. Introduction of the R2J approach for LLM jailbreak
	2. Demonstration of learnability and transferability of the jailbreak method
	3. Experimental validation across multiple datasets and model types

**Result:** R2J demonstrated a higher efficiency in execution and a reduced likelihood of detection, successfully transferring across various models and datasets with minimal queries.

**Limitations:** The paper does not address the ethical implications of using R2J and potential defenses against such methods.

**Conclusion:** The findings indicate that instruction rewriting as an attack strategy is promising and warrants further research into LLM safety protocols.

**Abstract:** As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety. The code can be found at https://github.com/ythuang02/R2J/.

</details>


### [225] [VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment](https://arxiv.org/abs/2502.11361)

*Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah*

**Main category:** cs.CL

**Keywords:** disinformation, multimodal, AI safety, dataset, detection

**Relevance Score:** 8

**TL;DR:** The VLDBench is a benchmark for detecting multimodal disinformation using text and images, aimed at enhancing the accuracy of disinformation detection tools.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of detecting multimodal disinformation, which is increasingly prevalent due to AI-generated content, highlighting the inadequacy of current benchmarks focused solely on unimodal misinformation.

**Method:** The creation of VLDBench involved compiling approximately 62,000 labeled text-image pairs from 58 news outlets, followed by expert annotation to ensure high quality and agreement among domain experts.

**Key Contributions:**

	1. Introduction of VLDBench as the first resource for multimodal disinformation detection.
	2. Collection and annotation of a large-scale dataset consisting of 62,000 text-image pairs.
	3. Demonstration of improved detection accuracy by incorporating multimodal data.

**Result:** State-of-the-art LLMs and VLMs achieved detection accuracy improvements of 5 to 35 percentage points when visual cues were incorporated compared to text-only models.

**Limitations:** 

**Conclusion:** VLDBench serves as a foundational resource for research in multimodal disinformation detection, fostering advancements in responsible AI governance and robust detection methods.

**Abstract:** Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety benchmarks focus on single modality misinformation (i.e., false content shared without intent to deceive), intentional multimodal disinformation, such as propaganda or conspiracy theories that imitate credible news, remains largely unaddressed. We introduce the Vision-Language Disinformation Detection Benchmark (VLDBench), the first large-scale resource supporting both unimodal (text-only) and multimodal (text + image) disinformation detection. VLDBench comprises approximately 62,000 labeled text-image pairs across 13 categories, curated from 58 news outlets. Using a semi-automated pipeline followed by expert review, 22 domain experts invested over 500 hours to produce high-quality annotations with substantial inter-annotator agreement. Evaluations of state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that incorporating visual cues improves detection accuracy by 5 to 35 percentage points over text-only models. VLDBench provides data and code for evaluation, fine-tuning, and robustness testing to support disinformation analysis. Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk Repository), VLDBench offers a principled foundation for advancing trustworthy disinformation detection in multimodal media.   Project: https://vectorinstitute.github.io/VLDBench/ Dataset: https://huggingface.co/datasets/vector-institute/VLDBench Code: https://github.com/VectorInstitute/VLDBench

</details>


### [226] [ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models](https://arxiv.org/abs/2502.11404)

*Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** Tool learning, Large language models, Code generation, Human-Computer Interaction, Software engineering

**Relevance Score:** 8

**TL;DR:** ToolCoder is a novel framework that reformulates tool learning for large language models as a code generation task, enhancing planning, error diagnosis, and task execution through structured Python functions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There are significant challenges in existing tool learning approaches for large language models, such as reliance on prompts and multi-step planning difficulties.

**Method:** ToolCoder transforms natural language queries into structured Python function scaffolds and generates and executes implementations to improve task handling.

**Key Contributions:**

	1. Reformulation of tool learning as code generation
	2. Systematic breakdown of tasks through comments in code
	3. Repository for reusable code and optimized error traceback mechanisms

**Result:** ToolCoder significantly improves task completion accuracy and execution reliability compared to existing methods.

**Limitations:** 

**Conclusion:** The effectiveness of code-centric approaches in tool learning is established through the results of the experiments conducted with ToolCoder.

**Abstract:** Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.

</details>


### [227] [MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training](https://arxiv.org/abs/2502.11541)

*Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao*

**Main category:** cs.CL

**Keywords:** complex instruction alignment, LLMs, self-contrastive training

**Relevance Score:** 9

**TL;DR:** The paper introduces a MuSC framework for improving complex instruction alignment in LLMs without reliance on advanced models such as GPT-4.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for complex instruction alignment depend on stronger models, which limits their broader applicability in practical scenarios.

**Method:** The proposed MuSC framework employs a two-tier approach, utilizing coarse-granularity for constraint-aware preference data and fine-granularity for token-aware preference optimization with dynamic supervision.

**Key Contributions:**

	1. Development of the MuSC framework for instruction alignment
	2. Introduction of constraint-aware preference data construction
	3. Dynamic token-level supervision for optimization

**Result:** Experimental evaluations demonstrate that the MuSC framework significantly improves performance on both complex and general instruction-following benchmarks compared to previous self-alignment approaches.

**Limitations:** 

**Conclusion:** The results indicate a successful advancement in instruction alignment capabilities of open-source models, making the approach widely applicable without needing advanced models.

**Abstract:** Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.

</details>


### [228] [LLM Agents Making Agent Tools](https://arxiv.org/abs/2502.11705)

*Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather*

**Main category:** cs.CL

**Keywords:** Large Language Models, Agent-based frameworks, Software engineering

**Relevance Score:** 9

**TL;DR:** ToolMaker is an autonomous framework that transforms scientific papers with code into LLM-compatible tools, improving the usability of LLM agents in complex domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of LLM agents requiring pre-implemented tools by proposing a framework that converts academic papers into usable tools, enhancing research efficiency in sectors like life sciences.

**Method:** ToolMaker takes a GitHub URL and a short task description to autonomously install dependencies and generate required code, utilizing a closed-loop self-correction mechanism for debugging.

**Key Contributions:**

	1. Autonomous transformation of academic code into LLM-compatible tools
	2. Closed-loop self-correction mechanism for debugging
	3. Benchmarked performance showing 80% correct implementation of tasks

**Result:** The proposed method successfully implements 80% of 15 complex computational tasks with a robust benchmark featuring over 100 unit tests, outperforming existing software engineering agents.

**Limitations:** 

**Conclusion:** ToolMaker significantly advances the potential for autonomous agent-based scientific workflows by providing a means to dynamically create tools from existing research code.

**Abstract:** Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.

</details>


### [229] ["See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models](https://arxiv.org/abs/2502.11718)

*Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng*

**Main category:** cs.CL

**Keywords:** Large Vision Language Models, Visual Question Answering, Factuality Evaluation, Chinese Language, Benchmark

**Relevance Score:** 6

**TL;DR:** This paper presents the first factuality-based visual question-answering benchmark for Chinese, called ChineseSimpleVQA, designed to evaluate visual factuality in large vision language models (LVLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gaps in evaluating factual accuracy in large vision language models, particularly in relation to understanding and processing visual information in the Chinese language.

**Method:** Introduction of the ChineseSimpleVQA benchmark, focusing on 8 major topics and 56 subtopics with a rigorous data construction pipeline that separates visual factuality into object recognition and knowledge discovery.

**Key Contributions:**

	1. Development of the ChineseSimpleVQA benchmark for assessing visual factuality in LVLMs.
	2. Decoupling visual factuality into object recognition and knowledge discovery for better analysis.
	3. Open-source evaluation code and data for community use.

**Result:** The evaluation of 34 LVLMs revealed significant performance gaps, highlighting areas for future improvement in model capabilities.

**Limitations:** Focused solely on Chinese language models and may not generalize to other languages or domains.

**Conclusion:** The benchmark and evaluation code are open-sourced, providing a resource for further research in visual question answering and improving LVLMs.

**Abstract:** The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field. Our evaluation-friendly code and data have already been open-sourced.

</details>


### [230] [CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs](https://arxiv.org/abs/2502.12476)

*Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Language adherence, Fine-tuning, Cross-lingual, Low-resource languages

**Relevance Score:** 8

**TL;DR:** This paper presents CoCo-CoLa, a metric for evaluating language adherence in multilingual LLMs and a selective fine-tuning strategy to enhance language response generation in low-resource languages.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Multilingual LLMs can struggle with language adherence, often favoring high-resource languages, which necessitates better evaluation metrics and fine-tuning methods.

**Method:** The authors introduce CoCo-CoLa, a new metric, and conduct fine-tuning experiments on a closed-book QA task across seven languages to analyze the cross-lingual performance of multilingual models.

**Key Contributions:**

	1. Introduction of the CoCo-CoLa metric for evaluating language adherence
	2. Identification of language-specific layers impacting output language generation
	3. Proposition of a partial training strategy that reduces computational costs while improving language adherence

**Result:** Experiments show that multilingual models can share task knowledge but exhibit biases in output language preference. The proposed partial training strategy significantly improves language adherence while being computationally efficient.

**Limitations:** 

**Conclusion:** The selective fine-tuning approach not only enhances performance for low-resource languages but also maintains or improves efficiency compared to full fine-tuning.

**Abstract:** Multilingual Large Language Models (LLMs) develop cross-lingual abilities despite being trained on limited parallel data. However, they often struggle to generate responses in the intended language, favoring high-resource languages such as English. In this work, we introduce CoCo-CoLa (Correct Concept - Correct Language), a novel metric to evaluate language adherence in multilingual LLMs. Using fine-tuning experiments on a closed-book QA task across seven languages, we analyze how training in one language affects others' performance. Our findings reveal that multilingual models share task knowledge across languages but exhibit biases in the selection of output language. We identify language-specific layers, showing that final layers play a crucial role in determining output language. Accordingly, we propose a partial training strategy that selectively fine-tunes key layers, improving language adherence while significantly reducing computational cost. Our method achieves comparable or superior performance to full fine-tuning, particularly for low-resource languages, offering a more efficient multilingual adaptation.

</details>


### [231] [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/abs/2502.14494)

*Jinnan Li, Jinzhe Li, Yue Wang, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** Multi-turn dialogue, Instruction following, Large language models, Benchmarking, Dialogue structures

**Relevance Score:** 9

**TL;DR:** This paper introduces StructFlowBench, a benchmark for evaluating multi-turn instruction following in large language models, focusing on structural dependencies between dialogue turns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks inadequately assess the structural dependencies in multi-turn dialogue that are critical for understanding user intent and instruction following.

**Method:** The paper presents the StructFlowBench framework which defines six fundamental inter-turn relationships to model structural flows and to create customized dialogue flows for evaluation.

**Key Contributions:**

	1. Introduction of StructFlowBench for multi-turn instruction following evaluation
	2. Development of a framework defining six inter-turn relationships
	3. Presentation of systematic evaluation findings on existing LLMs.

**Result:** Systematic evaluations demonstrate that 13 leading LLMs show significant deficiencies in understanding multi-turn dialogue structures as defined in the new benchmark.

**Limitations:** The framework focuses specifically on instruction following without addressing other aspects of dialogue management.

**Conclusion:** StructFlowBench provides a novel approach to evaluate multi-turn instruction following capabilities in LLMs, highlighting the need for improved comprehension of dialogue structures aware of user intent.

**Abstract:** Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependencies between dialogue turns that distinguish multi-turn from single-turn interactions. These structural dependencies not only reflect user intent but also establish an essential second dimension for the instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark defines an innovative structural flow framework with six fundamental inter-turn relationships. These relationships introduce novel structural constraints for model evaluation and also serve as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.

</details>


### [232] [Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs](https://arxiv.org/abs/2502.14561)

*Paris Koloveas, Serafeim Chatzopoulos, Thanasis Vergoulis, Christos Tryfonopoulos*

**Main category:** cs.CL

**Keywords:** Large Language Models, citation intent, in-context learning, fine-tuning, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper explores the use of open LLMs for predicting citation intent, leveraging in-context learning and fine-tuning with minimal task-specific data, achieving significant performance improvements over baseline models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how well general-purpose LLMs can predict citation intent compared to traditional domain-specific models, allowing for adaptability with limited data.

**Method:** The study evaluates twelve variations of open LLMs across five model families using various prompting strategies (zero- to many-shot prompting). Extensive experiments were conducted to identify the best-performing model and parameters, followed by fine-tuning for improved performance.

**Key Contributions:**

	1. Introduction of open LLMs for citation intent prediction
	2. Demonstrated effectiveness of minimal task-specific data for model adaptation
	3. Development of an evaluation framework made openly available for future research

**Result:** Fine-tuning the best model achieved an F1-score improvement of 8% on the SciCite dataset and 4.3% on the ACL-ARC dataset beyond the instruction-tuned baseline.

**Limitations:** 

**Conclusion:** The results indicate that general-purpose LLMs can perform well in specialized tasks like predicting citation intent with effective prompting and task-specific adaptations.

**Abstract:** This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches relying on domain-specific pre-trained models like SciBERT, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero-, one-, few-, and many-shot prompting. Our experimental study identifies the top-performing model and prompting parameters through extensive in-context learning experiments. We then demonstrate the significant impact of task-specific adaptation by fine-tuning this model, achieving a relative F1-score improvement of 8% on the SciCite dataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned baseline. These findings provide valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.

</details>


### [233] [Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs](https://arxiv.org/abs/2502.14830)

*Danni Liu, Jan Niehues*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, large language models, alignment objective

**Relevance Score:** 9

**TL;DR:** This paper proposes a middle-layer alignment objective for large language models to enhance cross-lingual transfer, particularly for lower-resource languages, achieving consistent improvements across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective cross-lingual transfer in large language models is critical for broad accessibility, as performance gaps and data scarcity hinder application in diverse languages.

**Method:** The authors analyze internal representations of over 1,000+ language pairs and identify that middle layers have the strongest alignment potential. They introduce a middle-layer alignment objective that is integrated into task-specific training.

**Key Contributions:**

	1. Introduction of a middle-layer alignment objective for large language models.
	2. Demonstration of improved cross-lingual transfer, especially for low-resource languages.
	3. Ability to merge alignment modules with task-specific modules without complete re-training.

**Result:** Experiments demonstrate consistent improvements in cross-lingual transfer for tasks such as slot filling, machine translation, and structured text generation, particularly benefiting lower-resource languages.

**Limitations:** 

**Conclusion:** The proposed method allows for enhanced cross-lingual capabilities without the need for full re-training, making it easier to integrate with existing task-specific models.

**Abstract:** While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).

</details>


### [234] [TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding](https://arxiv.org/abs/2502.15197)

*Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low*

**Main category:** cs.CL

**Keywords:** speculative decoding, batch processing, large language models, token selection, resource optimization

**Relevance Score:** 9

**TL;DR:** TETRIS optimizes batch speculative decoding for large language models by selecting promising draft tokens for parallel verification, improving acceptance rates and resource utilization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of speculative decoding in multi-request settings for large language models, particularly for service providers with limited compute resources.

**Method:** TETRIS actively selects the most promising draft tokens for each request in a batch to be verified in parallel, contrasting with existing methods that optimize for individual or grouped requests.

**Key Contributions:**

	1. Development of TETRIS method for optimizing batch speculative decoding.
	2. Demonstration of superior acceptance rates compared to baseline methods.
	3. Enhanced resource utilization for service providers with limited inference capabilities.

**Result:** TETRIS demonstrated a higher acceptance rate and improved utilization of limited inference capacity compared to baseline methods and others that select draft tokens dynamically.

**Limitations:** 

**Conclusion:** The proposed method outperforms existing speculative decoding strategies, leading to more efficient batch inference in large language models.

**Abstract:** We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.

</details>


### [235] [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/abs/2502.15401)

*Xuetao Ma, Wenbin Jiang, Hua Huang*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, curriculum learning, problem-solving logic, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents a curriculum in-context learning (ICL) strategy for large language models that enhances their reasoning capabilities through improved selection and ordering of demonstration examples based on problem-solving logic.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of previous ICL methods that relied on simplistic features to assess relevance between examples, suggesting that more sophisticated analysis is needed to enhance model reasoning.

**Method:** A problem-solving logic instruction set was constructed using the BREAK dataset, followed by fine-tuning a language model to analyze this logic. Demonstration examples were selected based on their problem-solving logic and ordered from easy to hard through curriculum learning principles.

**Key Contributions:**

	1. Introduction of a curriculum ICL strategy that utilizes problem-solving logic for LLMs.
	2. Construction of a problem-solving logic instruction set based on the BREAK dataset.
	3. Demonstrated improvements in performance and efficiency over previous ICL methods.

**Result:** The experimental results demonstrate that the proposed curriculum ICL strategy outperforms existing ICL methods, leading to improvements in performance and efficiency in enhancing the reasoning capabilities of LLMs across various benchmarks.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances complex reasoning in LLMs and encourages a more sophisticated approach to example selection and ordering in in-context learning strategies.

**Abstract:** In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https://github.com/maxuetao/CurriculumICL

</details>


### [236] [Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation](https://arxiv.org/abs/2502.15434)

*Yue Zhou, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** model merging, machine learning, LLMs, robustness, Beta distribution

**Relevance Score:** 7

**TL;DR:** The paper introduces Mixup Model Merge (M3), a method for merging task-specific models using randomized linear interpolation to improve performance and robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate task-specific models while considering their varying contribution to the merged model without additional training.

**Method:** M3 performs randomized linear interpolation in parameter space between two task-specific LLMs with coefficients sampled from a Beta distribution, allowing for controllable contribution ratios.

**Key Contributions:**

	1. Introduction of M3 for model merging
	2. Improved robustness and performance metrics
	3. Combination potential with existing methods like DARE

**Result:** M3 significantly improves performance on merged LLMs across tasks and enhances robustness to out-of-distribution and adversarial inputs.

**Limitations:** 

**Conclusion:** M3 outperforms standard merging techniques and can be combined with existing methods like DARE for superior results; it balances exploration and diversity in contribution ratios by tuning Beta distribution parameters.

**Abstract:** Model merging aims to integrate multiple task-specific models into a unified model that inherits the capabilities of the task-specific models, without additional training. Existing model merging methods often lack consideration of the varying contribution ratios of different task-specific models to the final merged model. In this paper, we propose Mixup Model Merge (M3), a simple yet effective method inspired by the randomized linear interpolation strategy from the Mixup data augmentation technique. M3 performs randomized linear interpolation in parameter space between two task-specific LLMs, where interpolation coefficients are sampled from a Beta distribution to explore diverse contribution ratios. This controllable randomness allows M3 to outperform standard equal-ratio merging by discovering better contribution ratio combinations. Extensive experiments show that M3 significantly (1) improves merged LLM performance across tasks, (2) enhances out-of-distribution and adversarial robustness, and (3) outperforms the positive effects of the sparsification method DARE on model merging and can be further combined with DARE to achieve superior results. By tuning the Beta distribution's shape parameters, (4) M3 balances exploration efficiency and diversity in contribution ratios. The code is available at: https://github.com/MLGroupJLU/MixupModelMerge

</details>


### [237] [All That Glitters is Not Novel: Plagiarism in AI Generated Research](https://arxiv.org/abs/2502.16487)

*Tarun Gupta, Danish Pruthi*

**Main category:** cs.CL

**Keywords:** plagiarism, LLM-generated research, academic integrity, plagiarism detection, autonomous research agents

**Relevance Score:** 9

**TL;DR:** The paper investigates the prevalence of plagiarism in research documents generated by LLMs, finding that a significant percentage are either paraphrased or borrowed from existing work, and that current plagiarism detectors are inadequate.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding the originality of research generated by autonomous agents and the implications for academic integrity.

**Method:** Expert evaluation of 50 LLM-generated research documents to identify similarities with existing work, supported by cross-verification from original source authors and controlled experiments on plagiarism detection efficacy.

**Key Contributions:**

	1. Documented the extent of plagiarism in LLM-generated research documents
	2. Cross-verified findings with original source authors
	3. Evaluated the effectiveness of current plagiarism detection methods

**Result:** Experts identified that 24% of the evaluated documents were paraphrased or borrowed from existing work, while 32% showed partial overlap. Current plagiarism detection methods failed to catch many instances of plagiarism.

**Limitations:** The study is limited by its sample size of 50 documents and the reliance on expert evaluation, which may not capture all nuances of plagiarism.

**Conclusion:** There is a pressing need for careful assessment of LLM-generated research to uphold academic integrity, as many documents do not acknowledge original sources and current detection methods are inadequate.

**Abstract:** Automating scientific research is considered the final frontier of science. Recently, several papers claim autonomous research agents can generate novel research ideas. Amidst the prevailing optimism, we document a critical concern: a considerable fraction of such research documents are smartly plagiarized. Unlike past efforts where experts evaluate the novelty and feasibility of research ideas, we request $13$ experts to operate under a different situational logic: to identify similarities between LLM-generated research documents and existing work. Concerningly, the experts identify $24\%$ of the $50$ evaluated research documents to be either paraphrased (with one-to-one methodological mapping), or significantly borrowed from existing work. These reported instances are cross-verified by authors of the source papers. Experts find an additional $32\%$ ideas to partially overlap with prior work, and a small fraction to be completely original. Problematically, these LLM-generated research documents do not acknowledge original sources, and bypass inbuilt plagiarism detectors. Lastly, through controlled experiments we show that automated plagiarism detectors are inadequate at catching plagiarized ideas from such systems. We recommend a careful assessment of LLM-generated research, and discuss the implications of our findings on academic publishing.

</details>


### [238] [All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark](https://arxiv.org/abs/2502.16989)

*Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini*

**Main category:** cs.CL

**Keywords:** Multimodal AI, Visual Language Models, Benchmark, Italian Culture, Reasoning Evaluation

**Relevance Score:** 5

**TL;DR:** MAIA is a new benchmark for assessing visual language models on videos, focusing on reasoning abilities through two tasks: visual statement verification and open-ended visual question-answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of the reasoning capabilities of visual language models, particularly in the context of Italian language and culture.

**Method:** MAIA uses a dual-task approach (visual statement verification and visual question-answering) across twelve reasoning categories, employing metrics that focus on the interactions between language and visual inputs.

**Key Contributions:**

	1. Introduction of a new benchmark for VLMs focused on Italian culture
	2. Dual-task evaluation method allowing nuanced reasoning assessment
	3. Highlighting the fragility of current VLM's performance in real-world scenarios

**Result:** Initial evaluations reveal that existing Vision Language Models exhibit low performance, indicating weaknesses in understanding visual contexts and generating coherent natural language.

**Limitations:** 

**Conclusion:** MAIA emphasizes the need for better alignment between visual and language understanding in AI models, particularly for culturally specific content.

**Abstract:** We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses, and the language and culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlighting the role of the visual input. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric revealing low results that highlight models' fragility. Last but not least, the video collection has been carefully selected to reflect the Italian culture, and the language data are produced by native-speakers.

</details>


### [239] [Do Language Models Understand Honorific Systems in Javanese?](https://arxiv.org/abs/2502.20864)

*Mohammad Rifqi Farhansyah, Iwan Darmawan, Adryan Kusumawardhana, Genta Indra Winata, Alham Fikri Aji, Derry Tanti Wijaya*

**Main category:** cs.CL

**Keywords:** Javanese language, honorifics, natural language processing, machine translation, language models

**Relevance Score:** 4

**TL;DR:** This paper introduces Unggah-Ungguh, a dataset for studying Javanese honorifics, and evaluates language models' ability to process these variations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a comprehensive corpus for Javanese honorifics, crucial for natural language processing (NLP) tasks.

**Method:** A dataset named Unggah-Ungguh is curated to capture Javanese speech etiquette nuances, and language models are tested on classification and machine translation tasks between Javanese and Indonesian.

**Key Contributions:**

	1. Introduction of the Unggah-Ungguh dataset for Javanese honorifics
	2. Evaluation of language model performance on Javanese honorifics
	3. Insights into the challenges LMs face with honorific levels

**Result:** Current language models exhibit difficulty in handling Javanese honorifics, showing bias towards certain levels, indicating a gap in processing capabilities for this cultural aspect.

**Limitations:** Limited to Javanese honorifics and may not generalize to other languages or cultures; primarily focuses on specific NLP tasks.

**Conclusion:** The study highlights the need for improvements in language model training to effectively incorporate social and contextual nuances in Javanese language processing.

**Abstract:** The Javanese language features a complex system of honorifics that vary according to the social status of the speaker, listener, and referent. Despite its cultural and linguistic significance, there has been limited progress in developing a comprehensive corpus to capture these variations for natural language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh Basa, the Javanese speech etiquette framework that dictates the choice of words and phrases based on social hierarchy and context. Using Unggah-Ungguh, we assess the ability of language models (LMs) to process various levels of Javanese honorifics through classification and machine translation tasks. To further evaluate cross-lingual LMs, we conduct machine translation experiments between Javanese (at specific honorific levels) and Indonesian. Additionally, we explore whether LMs can generate contextually appropriate Javanese honorifics in conversation tasks, where the honorific usage should align with the social role and contextual cues. Our findings indicate that current LMs struggle with most honorific levels, exhibitinga bias toward certain honorific tiers.

</details>


### [240] [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/abs/2503.01372)

*Joel Niklaus, Jakob Merane, Luka Nenadic, Sina Ahmadi, Yingqiang Gao, Cyrill A. H. Chevalley, Claude Humbel, Christophe Gösken, Lorenzo Tanzi, Thomas Lüthi, Stefan Palombo, Spencer Poff, Boling Yang, Nan Wu, Matthew Guillod, Robin Mamié, Daniel Brunner, Julio Pereyra, Niko Grupen*

**Main category:** cs.CL

**Keywords:** legal translation, multilingual, large language models, benchmark, Switzerland

**Relevance Score:** 4

**TL;DR:** This paper presents SwiLTra-Bench, a multilingual legal translation benchmark that evaluates the performance of large language models in Swiss legal contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve access to justice in Switzerland by addressing bottlenecks in legal translation caused by the need for professionals to be both legal experts and skilled translators.

**Method:** A comprehensive benchmark of over 180K aligned Swiss legal translation pairs was created and evaluated. The performance of frontier models and specialized systems was compared, with rigorous testing and human expert validation to assess translation quality.

**Key Contributions:**

	1. Creation of SwiLTra-Bench, a multilingual legal translation benchmark.
	2. Identification of performance disparities between frontier LLMs and specialized systems.
	3. Development of SwiLTra-Judge for assessing LLM translation quality.

**Result:** Frontier models outperform specialized systems in translation performance across all document types, while specialized systems perform better specifically in translating laws. Fine-tuning open SLMs improves results but still falls short of leading zero-shot models.

**Limitations:** The benchmark may not cover all legal contexts and nuances in the Swiss legal system.

**Conclusion:** The study highlights the potential of LLMs in legal translation while suggesting specialized models may excel in specific contexts. SwiLTra-Judge offers an effective evaluation system aligned with human expert judgments.

**Abstract:** In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.

</details>


### [241] [Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering](https://arxiv.org/abs/2503.01606)

*Zhanghao Hu, Hanqi Yan, Qinglin Zhu, Zhenyi Shen, Yulan He, Lin Gui*

**Main category:** cs.CL

**Keywords:** open domain question answering, large language models, embedding-level framework

**Relevance Score:** 9

**TL;DR:** This paper introduces EmbQA, an embedding-level framework that enhances open domain question answering by optimizing retriever and reader performance through improved query representation and automatic answer selection.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing retriever-reader pipelines in open domain question answering, which suffer from high computational overhead and instability.

**Method:** The authors propose EmbQA, which utilizes lightweight linear layers for refining query representations under an unsupervised contrastive learning objective, and an entropy-based selection mechanism for answer generation.

**Key Contributions:**

	1. Introduction of an embedding-level framework for ODQA
	2. Use of lightweight linear layers for query refinement
	3. Implementation of an entropy-based selection mechanism for answer generation

**Result:** EmbQA significantly outperforms recent baselines in accuracy and efficiency across three open-source LLMs, three retrieval methods, and four ODQA benchmarks.

**Limitations:** 

**Conclusion:** The proposed EmbQA framework not only improves the effectiveness of retrieval and reading processes in ODQA but also offers a more efficient alternative to existing methods.

**Abstract:** Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.

</details>


### [242] [Vision-Language Models Struggle to Align Entities across Modalities](https://arxiv.org/abs/2503.03854)

*Iñigo Alonso, Gorka Azkune, Ander Salaberria, Jeremy Barnes, Oier Lopez de Lacalle*

**Main category:** cs.CL

**Keywords:** cross-modal entity linking, Vision-Language Models, MATE benchmark, question-answering task, multimodal applications

**Relevance Score:** 5

**TL;DR:** This paper introduces a new task and benchmark for cross-modal entity linking called MATE, evaluating Vision-Language Models against humans on a question-answering task involving visual and textual representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in literature regarding cross-modal entity linking, a critical skill for applications like multimodal code generation and fake news detection.

**Method:** The authors introduce MATE, a benchmark with 5.5k instances, and design a question-answering task to evaluate the alignment of entities across modalities.

**Key Contributions:**

	1. Introduction of the MATE benchmark for cross-modal entity linking.
	2. A novel question-answering task leveraging visual and textual data.
	3. Analysis revealing the performance gap between VLMs and human abilities.

**Result:** The evaluation shows that state-of-the-art Vision-Language Models struggle with the task compared to humans, especially as scene complexity increases.

**Limitations:** The focus on specific entities and attributes may limit the scope of cross-modal applications.

**Conclusion:** The findings indicate significant performance gaps between VLMs and humans, suggesting further research is needed in cross-modal entity linking, with MATE serving as a valuable benchmark.

**Abstract:** Cross-modal entity linking refers to the ability to align entities and their attributes across different modalities. While cross-modal entity linking is a fundamental skill needed for real-world applications such as multimodal code generation, fake news detection, or scene understanding, it has not been thoroughly studied in the literature. In this paper, we introduce a new task and benchmark to address this gap. Our benchmark, MATE, consists of 5.5k evaluation instances featuring visual scenes aligned with their textual representations. To evaluate cross-modal entity linking performance, we design a question-answering task that involves retrieving one attribute of an object in one modality based on a unique attribute of that object in another modality. We evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this task, and find that VLMs struggle significantly compared to humans, particularly as the number of objects in the scene increases. Our analysis also shows that, while chain-of-thought prompting can improve VLM performance, models remain far from achieving human-level proficiency. These findings highlight the need for further research in cross-modal entity linking and show that MATE is a strong benchmark to support that progress.

</details>


### [243] [HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378)

*Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev*

**Main category:** cs.CL

**Keywords:** Inference-Time Scaling, Feedback Models, Open-Ended Tasks

**Relevance Score:** 8

**TL;DR:** The paper introduces HelpSteer3 data and models for inference-time scaling in open-ended tasks, demonstrating improved performance on complex benchmarks by optimizing feedback and edits.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance model performance on open-ended tasks where traditional inference-time scaling techniques are limited due to the need for verifiable answers.

**Method:** The paper proposes a three-model setup: one model generates responses, a second provides feedback, and a third edits the responses, utilizing the HelpSteer3 dataset to train these models.

**Key Contributions:**

	1. Introduction of HelpSteer3 dataset for feedback-based training
	2. A novel three-model architecture for scaling responses via feedback and edits
	3. Achieving state-of-the-art performance on a competitive benchmark.

**Result:** The proposed setup achieved state-of-the-art performance on the Arena Hard benchmark, outperforming existing models like OpenAI o1 and DeepSeek R1 when optimally scaled.

**Limitations:** 

**Conclusion:** By effectively leveraging feedback and iterative improvements in model responses, the methodology significantly advances inference-time scaling for open-ended general-domain tasks.

**Abstract:** Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.

</details>


### [244] [ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs](https://arxiv.org/abs/2503.05268)

*Francesco Cazzaro, Justin Kleindienst, Sofia Marquez Gomez, Ariadna Quattoni*

**Main category:** cs.CL

**Keywords:** natural language processing, knowledge graphs, property graphs, semantic parsing, benchmarking

**Relevance Score:** 6

**TL;DR:** This paper presents ZOGRASCOPE, a benchmark for property graphs (PGs) aimed at improving natural language interfaces and semantic parsing research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for natural language interfaces to knowledge graphs, particularly property graphs, due to their rising use and the challenges in semantic parsing research.

**Method:** The paper introduces ZOGRASCOPE, a benchmark with a diverse set of manually annotated Cypher queries, organized into three partitions based on complexity characteristics: iid, compositional, and length. It includes experiments assessing various LLMs' performance in learning settings.

**Key Contributions:**

	1. Introduction of ZOGRASCOPE benchmark for property graphs
	2. A diverse set of annotated queries for better evaluation
	3. Experimental results on LLM performance regarding property graph queries

**Result:** Different LLMs were tested and performance varied across different query complexities, indicating strengths and weaknesses in handling property graph data.

**Limitations:** 

**Conclusion:** The introduction of ZOGRASCOPE fills a gap in the evaluation of natural language interfaces for property graphs and aids in semantic parsing research.

**Abstract:** In recent years, the need for natural language interfaces to knowledge graphs has become increasingly important since they enable easy and efficient access to the information contained in them. In particular, property graphs (PGs) have seen increased adoption as a means of representing complex structured information. Despite their growing popularity in industry, PGs remain relatively underrepresented in semantic parsing research with a lack of resources for evaluation. To address this gap, we introduce ZOGRASCOPE, a benchmark designed specifically for PGs and queries written in Cypher. Our benchmark includes a diverse set of manually annotated queries of varying complexity and is organized into three partitions: iid, compositional and length. We complement this paper with a set of experiments that test the performance of different LLMs in a variety of learning settings.

</details>


### [245] [DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs](https://arxiv.org/abs/2503.07067)

*Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun*

**Main category:** cs.CL

**Keywords:** LLM, distillation, contrastive approach, student models, teacher models

**Relevance Score:** 9

**TL;DR:** DistiLLM-2 is a contrastive approach for distilling large language models that enhances student model performance by leveraging the synergy between loss formulations and data types.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of student models in LLM distillation by utilizing different loss functions for teacher- and student-generated data.

**Method:** A contrastive approach that increases teacher response likelihood while decreasing student response likelihood, optimizing the alignment between teacher and student models.

**Key Contributions:**

	1. Introduction of DistiLLM-2, a novel contrastive approach to LLM distillation.
	2. Demonstration of improved student model performance across multiple tasks.
	3. Support for diverse application areas including preference alignment and vision-language tasks.

**Result:** DistiLLM-2 significantly boosts student model performance across various tasks including instruction-following and code generation, and supports applications like preference alignment and vision-language tasks.

**Limitations:** 

**Conclusion:** The results demonstrate that a contrastive loss approach effectively enhances LLM distillation efficacy through improved alignment of teacher and student models.

**Abstract:** Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.

</details>


### [246] [TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification](https://arxiv.org/abs/2503.15289)

*Junnan Zhu, Min Xiao, Yining Wang, Feifei Zhai, Yu Zhou, Chengqing Zong*

**Main category:** cs.CL

**Keywords:** Text Provovenance, Long-Document Generation, Language Models, Data Annotation, NLU

**Relevance Score:** 8

**TL;DR:** The paper introduces the Text pROVEnance (TROVE) challenge to trace sentence origins in generated texts, emphasizing the importance of content reliability in high-stakes settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The widespread adoption of LLMs has raised concerns about content reliability and accountability, necessitating a method to trace and understand the creation of generated content.

**Method:** The TROVE dataset is constructed from three public datasets covering 11 diverse scenarios, with a three-stage annotation process involving sentence retrieval and both automated and human provenance assessment.

**Key Contributions:**

	1. Introduction of the TROVE challenge for text provenance tracking
	2. Creation of a comprehensive dataset for evaluating text generation provenance
	3. Evaluation results providing insights into LLM performance related to retrieval tasks

**Result:** Evaluation of 11 LLMs shows that retrieval is crucial for performance, larger models excel at complex relationship classification, and open-source models show promising results with retrieval support.

**Limitations:** 

**Conclusion:** The findings highlight the importance of provenance tracking in LLM outputs and suggest avenues for improving model transparency and reliability.

**Abstract:** LLMs have achieved remarkable fluency and coherence in text generation, yet their widespread adoption has raised concerns about content reliability and accountability. In high-stakes domains, it is crucial to understand where and how the content is created. To address this, we introduce the Text pROVEnance (TROVE) challenge, designed to trace each sentence of a target text back to specific source sentences within potentially lengthy or multi-document inputs. Beyond identifying sources, TROVE annotates the fine-grained relationships (quotation, compression, inference, and others), providing a deep understanding of how each target sentence is formed. To benchmark TROVE, we construct our dataset by leveraging three public datasets covering 11 diverse scenarios (e.g., QA and summarization) in English and Chinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and long-document settings essential for provenance. To ensure high-quality data, we employ a three-stage annotation process: sentence retrieval, GPT-4o provenance, and human provenance. We evaluate 11 LLMs under direct prompting and retrieval-augmented paradigms, revealing that retrieval is essential for robust performance, larger models perform better in complex relationship classification, and closed-source models often lead, yet open-source models show significant promise, particularly with retrieval augmentation. We make our dataset available here: https://github.com/ZNLP/ZNLP-Dataset.

</details>


### [247] [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/abs/2503.18491)

*Shuo Yang, Siwen Luo, Soyeon Caren Han, Eduard Hovy*

**Main category:** cs.CL

**Keywords:** Visual Question Answering, Large Vision-Language Models, Commonsense Knowledge, Graph Neural Network, Reasoning

**Relevance Score:** 8

**TL;DR:** MAGIC-VQA is a framework that combines commonsense knowledge with Large Vision-Language Models to enhance Visual Question Answering (VQA).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of commonsense knowledge is crucial for improving the robustness of VQA systems based on Large Vision-Language Models (LVLMs), which often exhibit limitations in real-world scenarios.

**Method:** MAGIC-VQA uses a three-stage process: explicit knowledge integration from external sources, contextual refinement through by-type post-processing, and implicit knowledge augmentation via a Graph Neural Network for structured reasoning.

**Key Contributions:**

	1. Development of the MAGIC-VQA framework that integrates commonsense knowledge with VQA tasks.
	2. A three-stage process for enhancing reasoning capabilities in LVLMs.
	3. Achievement of state-of-the-art performance on VQA benchmark datasets.

**Result:** The framework achieves state-of-the-art performance on benchmark datasets, demonstrating significant improvements in commonsense reasoning in Visual Question Answering tasks.

**Limitations:** 

**Conclusion:** MAGIC-VQA effectively unifies commonsense knowledge with LVLM-driven reasoning without requiring extensive pre-training or complex prompt tuning.

**Abstract:** Visual Question Answering (VQA) requires reasoning across visual and textual modalities, yet Large Vision-Language Models (LVLMs) often lack integrated commonsense knowledge, limiting their robustness in real-world scenarios. To address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs a three-stage process: (1) Explicit Knowledge Integration from external sources, (2) By-Type Post-Processing for contextual refinement, and (3) Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for structured reasoning. While GNNs bring greater depth to structured inference, they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating the need for extensive pre-training or complex prompt tuning. Our framework achieves state-of-the-art performance on benchmark datasets, significantly improving commonsense reasoning in VQA.

</details>


### [248] [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)

*Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Response Consistency, Human-Computer Interaction, Machine Learning, Health Informatics

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for evaluating and enhancing the consistency of LLM responses through a new metric, a benchmark dataset, and a response generation method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Ensuring consistent and coherent behavior of LLMs in high-stake domains is crucial for reliable user interactions.

**Method:** The authors introduce Position-Weighted Consistency (PWC) as a metric, a benchmark dataset called MT-Consistency for evaluating multi-turn interactions, and Confidence-Aware Response Generation (CARG) for improved response stability.

**Key Contributions:**

	1. Position-Weighted Consistency (PWC) metric for multi-turn interaction stability.
	2. MT-Consistency benchmark dataset for evaluating LLM response consistency.
	3. Confidence-Aware Response Generation (CARG) framework to improve response stability.

**Result:** Experimental results indicate that CARG enhances response stability significantly while maintaining accuracy, making LLMs more dependable for real-world applications.

**Limitations:** 

**Conclusion:** The proposed methods offer practical solutions for achieving more consistent LLM behavior, essential for critical deployments.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities across various tasks, but their deployment in high-stake domains requires consistent and coherent behavior across multiple rounds of user interaction. This paper introduces a comprehensive framework for evaluating and improving LLM response consistency, making three key contributions. Code and data are available at: https://github.com/yubol-bobo/MT-Consistency. First, we introduce Position-Weighted Consistency (PWC), a metric designed to capture both the importance of early-stage stability and recovery patterns in multi-turn interactions. Second, we present MT-Consistency, a carefully curated benchmark dataset spanning diverse domains and difficulty levels, specifically designed to evaluate LLM consistency under various challenging follow-up scenarios. Third, we introduce Confidence-Aware Response Generation (CARG), a framework that significantly improves response stability by explicitly integrating internal model confidence scores during the generation process. Experimental results demonstrate that CARG significantly improves response stability without sacrificing accuracy, offering a practical path toward more dependable LLM behavior in critical, real-world deployments.

</details>


### [249] [An Explicit Syllogistic Legal Reasoning Framework for Large Language Models](https://arxiv.org/abs/2504.04042)

*Kepu Zhang, Weijie Yu, Zhongxiang Sun, Jun Xu*

**Main category:** cs.CL

**Keywords:** syllogistic reasoning, large language models, legal decision-making, explainable AI, reinforcement learning

**Relevance Score:** 2

**TL;DR:** SyLeR is a framework designed to enhance large language models' capability in performing explicit syllogistic reasoning for legal decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in providing explicit, structured, and trustworthy syllogistic reasoning in legal contexts.

**Method:** SyLeR uses a tree-structured hierarchical retrieval system to gather relevant legal materials and applies a two-stage fine-tuning process, which includes supervised fine-tuning and reinforcement learning with a structure-aware reward mechanism.

**Key Contributions:**

	1. Introduction of SyLeR framework for explicit syllogistic legal reasoning
	2. Tree-structured hierarchical retrieval mechanism for gathering legal statutes
	3. Two-stage fine-tuning process combining supervised training and reinforcement learning

**Result:** SyLeR improves response accuracy and consistently produces explicit and explainable legal reasoning across various user groups and languages.

**Limitations:** 

**Conclusion:** The extensive experiments show that SyLeR significantly enhances legal reasoning capabilities of LLMs, making them more reliable for legal applications.

**Abstract:** Syllogistic reasoning is crucial for sound legal decision-making, allowing legal professionals to draw logical conclusions by applying general principles to specific case facts. While large language models (LLMs) can answer legal questions, they often struggle with explicit syllogistic reasoning. Their outputs tend to be implicit, unstructured, and consequently, less explainable and trustworthy. To overcome these limitations, we introduce SyLeR, a novel framework designed to enable LLMs to perform explicit syllogistic legal reasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to synthesize relevant legal statutes and precedents, thereby constructing comprehensive major premises. This is followed by a two-stage fine-tuning process: an initial supervised fine-tuning warm-up establishes a foundational understanding of syllogistic reasoning, while reinforcement learning, guided by a structure-aware reward mechanism, refines the model's capacity to generate diverse, logically sound, and well-structured reasoning paths. We conducted extensive experiments to evaluate SyLeR's performance. Our evaluations spanned diverse dimensions, including both in-domain and cross-domain user groups (legal laypersons and practitioners), multiple languages (Chinese and French), and various LLM backbones (legal-specific and open-domain LLMs). The results consistently demonstrate that SyLeR significantly enhances response accuracy and reliably produces explicit, explainable, and trustworthy legal reasoning.

</details>


### [250] [RAISE: Reinforced Adaptive Instruction Selection For Large Language Models](https://arxiv.org/abs/2504.07282)

*Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** instruction fine-tuning, reinforcement learning, large language models, dynamic selection, task-specific optimization

**Relevance Score:** 9

**TL;DR:** RAISE is a dynamic instruction selection framework for fine-tuning large language models, optimizing instructions throughout the training process using reinforcement learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the instruction fine-tuning of large language models by moving beyond heuristic quality metrics and enabling task-specific optimization of instructions.

**Method:** RAISE dynamically selects instructions during the entire fine-tuning process based on their expected impact on model performance, using reinforcement learning for strategy training.

**Key Contributions:**

	1. Introduction of RAISE, a dynamic instruction selection framework using RL
	2. Optimization of instruction selection throughout the fine-tuning process
	3. Demonstration of efficiency with minimal training step updates

**Result:** RAISE shows superior performance against existing instruction selection methods by requiring updates to only 1% of the training steps versus full-data training, highlighting its efficiency and effectiveness.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates strong interpretability and task-specific optimization capabilities, advancing the state of instruction fine-tuning.

**Abstract:** In the instruction fine-tuning of large language models (LLMs), it is widely recognized that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. Therefore, we design a dynamic, task-objective-driven instruction selection framework RAISE(Reinforced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instructions at each step based on the expected impact of each instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness.

</details>


### [251] [DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/abs/2504.08120)

*Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger*

**Main category:** cs.CL

**Keywords:** reasoning LLMs, NLG evaluation, machine translation, text summarization, model comparison

**Relevance Score:** 8

**TL;DR:** This study compares reasoning-enabled and non-reasoning large language models across machine translation and text summarization tasks, revealing nuanced performance benefits based on architecture and task type.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the utility of reasoning-enabled LLMs in evaluating natural language generation, which has not been systematically investigated before.

**Method:** Comparison of eight models, including state-of-the-art reasoning models and their distilled versions, on WMT23 and SummEval benchmarks.

**Key Contributions:**

	1. First assessment of reasoning LLMs for NLG evaluation
	2. Systematic comparison of reasoning and non-reasoning models
	3. Insights on model architecture and parameter scaling effects on performance

**Result:** OpenAI o3-mini models improved performance in machine translation with more reasoning, while DeepSeek-R1 generally underperformed compared to non-reasoning variants, except in summarization consistency evaluation.

**Limitations:** No extensive exploration of reasoning LLMs in other NLG tasks beyond translation and summarization is provided.

**Conclusion:** Reasoning LLMs have specific advantages in NLG evaluation, but their performance varies based on model type and task; recommendations for further research are provided along with shared code.

**Abstract:** Reasoning-enabled large language models (LLMs) excel in logical tasks, yet their utility for evaluating natural language generation remains unexplored. This study systematically compares reasoning LLMs with non-reasoning counterparts across machine translation and text summarization evaluation tasks. We evaluate eight models spanning state-of-the-art reasoning models (DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks reveal architecture and task-dependent benefits: OpenAI o3-mini models show improved performance with increased reasoning on MT, while DeepSeek-R1 and generally underperforms compared to its non-reasoning variant except in summarization consistency evaluation. Correlation analysis demonstrates that reasoning token usage correlates with evaluation quality only in specific models, while almost all models generally allocate more reasoning tokens when identifying more quality issues. Distillation maintains reasonable performance up to 32B parameter models but degrades substantially at 8B scale. This work provides the first assessment of reasoning LLMs for NLG evaluation and comparison to non-reasoning models. We share our code to facilitate further research: https://github.com/NL2G/reasoning-eval.

</details>


### [252] [GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction](https://arxiv.org/abs/2504.10792)

*Jessica Lin, Amir Zeldes*

**Main category:** cs.CL

**Keywords:** entity salience, natural language processing, summarization

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel method for determining graded entity salience in text, combining subjective judgments and summarization-based approaches to assign relative importance scores to entities.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve how user-facing systems interpret long documents by determining and ranking the salience of entities.

**Method:** A novel approach that calculates salience scores for entities based on their presence across multiple summaries of documents.

**Key Contributions:**

	1. Introduces a novel graded entity salience approach combining strengths of two existing methodologies.
	2. Demonstrates superior performance over existing techniques.
	3. Provides publicly available data and code for reproducibility and further research.

**Result:** The proposed method shows stronger correlation with human-generated scores and outperforms existing techniques, including LLMs, in grading entity salience.

**Limitations:** 

**Conclusion:** The study establishes a new benchmark for graded entity salience extraction and provides resources for further research.

**Abstract:** Determining and ranking the most salient entities in a text is critical for user-facing systems, especially as users increasingly rely on models to interpret long documents they only partially read. Graded entity salience addresses this need by assigning entities scores that reflect their relative importance in a text. Existing approaches fall into two main categories: subjective judgments of salience, which allow for gradient scoring but lack consistency, and summarization-based methods, which define salience as mention-worthiness in a summary, promoting explainability but limiting outputs to binary labels (entities are either summary-worthy or not). In this paper, we introduce a novel approach for graded entity salience that combines the strengths of both approaches. Using an English dataset spanning 12 spoken and written genres, we collect 5 summaries per document and calculate each entity's salience score based on its presence across these summaries. Our approach shows stronger correlation with scores based on human summaries and alignments, and outperforms existing techniques, including LLMs. We release our data and code at https://github.com/jl908069/gum_sum_salience to support further research on graded salient entity extraction.

</details>


### [253] [From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs](https://arxiv.org/abs/2504.11277)

*Guocong Li, Weize Liu, Yihang Wu, Ping Wang, Shuaihan Huang, Hongxia Xu, Jian Wu*

**Main category:** cs.CL

**Keywords:** large language models, misleading information, fine-tuning, hallucination detection, question answering

**Relevance Score:** 9

**TL;DR:** A novel three-stage fine-tuning method improves large language models' ability to detect and correct misleading information in input queries, enhancing response accuracy and reducing hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of large language models being sensitive to misleading or inaccurate input queries, focusing on enhancing their detection and correction abilities rather than just correcting outputs.

**Method:** The proposed method involves a three-stage fine-tuning process: (1) identifying misleading information, (2) correcting the misleading information, and (3) generating accurate answers based on corrected queries.

**Key Contributions:**

	1. Novel three-stage fine-tuning approach for LLMs
	2. Improved accuracy and factuality of responses
	3. Enhanced ability to detect and reduce hallucinations

**Result:** Experiments on multiple datasets for hallucination detection and question answering show significant improvements in accuracy, factuality, and reduction of hallucinations in LLM responses.

**Limitations:** 

**Conclusion:** The method enhances the LLM's capability to handle misleading information, resulting in better quality responses.

**Abstract:** Large language models (LLMs) exhibit excellent performance in natural language processing (NLP), but remain highly sensitive to the quality of input queries, especially when these queries contain misleading or inaccurate information. Existing methods focus on correcting the output, but they often overlook the potential of improving the ability of LLMs to detect and correct misleading content in the input itself. In this paper, we propose a novel three-stage fine-tuning method that enhances the ability of LLMs to detect and correct misleading information in the input, further improving response accuracy and reducing hallucinations. Specifically, the three stages include (1) training LLMs to identify misleading information, (2) training LLMs to correct the misleading information using built-in or external knowledge, and (3) training LLMs to generate accurate answers based on the corrected queries. To evaluate our method, we conducted experiments on three datasets for the hallucination detection task and the question answering~(QA) task, as well as two datasets containing misleading information that we constructed. The experimental results demonstrate that our method significantly improves the accuracy and factuality of LLM responses, while also enhancing the ability to detect hallucinations and reducing the generation of hallucinations in the output, particularly when the query contains misleading information.

</details>


### [254] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)

*Michael A. Hedderich, Anyi Wang, Raoyuan Zhao, Florian Eichin, Jonas Fischer, Barbara Plank*

**Main category:** cs.CL

**Keywords:** prompt engineering, large language models, data mining, human-centered AI, systematic differences

**Relevance Score:** 9

**TL;DR:** Spotlight is a novel approach to prompt engineering for large language models, combining automated data mining techniques with human analysis to identify systematic differences in model outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenges in prompt engineering for large language models arise from the significant effects of small prompt changes or model variations on output texts.

**Method:** Spotlight uses data mining techniques to distinguish between random variations and systematic differences in language model outputs, providing token patterns to guide manual analysis.

**Key Contributions:**

	1. Introduction of the Spotlight framework for prompt analysis
	2. Development of benchmarks for evaluating token pattern extraction
	3. Demonstration of systematic differences in LLM outputs related to prompts and model changes

**Result:** The approach allows for the extraction of token patterns that reveal systematic differences and supports users in understanding the effects of prompt and model changes.

**Limitations:** The paper does not address potential scaling issues or the adaptability of the proposed methods to all language models.

**Conclusion:** Spotlight enhances prompt engineering by providing quantitative benchmarks for token pattern extraction and new insights into modeled outputs from human-centric studies.

**Abstract:** Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods of LLM outputs, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompts and changes in models efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs. We are further able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.

</details>


### [255] [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)

*James Tanner, Morgan Sonderegger, Jane Stuart-Smith, Jeff Mielke, Tyler Kendall*

**Main category:** cs.CL

**Keywords:** phonetic research, wav2vec2.0, speech classification, automatic annotation, stop burst

**Relevance Score:** 4

**TL;DR:** This paper illustrates the use of wav2vec2.0 models for the automatic classification of stop burst presence in English and Japanese speech, showing high accuracy and potential for phonetic research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the annotation of variable phonetic phenomena in modern phonetic research, particularly using automatic tools.

**Method:** Wav2vec2.0 models were trained to classify the presence of stop bursts in speech data, tested on both curated and unprepared corpora in English and Japanese.

**Key Contributions:**

	1. Demonstrated the application of wav2vec2.0 for phonetic annotation tasks.
	2. Showed high accuracy in classifying stop burst presence across languages and corpus types.
	3. Established a link between automatic and manual annotation patterns in phonetic research.

**Result:** The models achieved high accuracy in classifying stop bursts, with patterns of variability aligning closely with manual annotations.

**Limitations:** 

**Conclusion:** Pre-trained speech models like wav2vec2.0 can serve as effective tools for the automatic annotation of speech corpora, allowing researchers to expand their phonetic research capabilities significantly.

**Abstract:** Modern phonetic research regularly makes use of automatic tools for the annotation of speech data, however few tools exist for the annotation of many variable phonetic phenomena. At the same time, pre-trained self-supervised models, such as wav2vec2.0, have been shown to perform well at speech classification tasks and latently encode fine-grained phonetic information. We demonstrate that wav2vec2.0 models can be trained to automatically classify stop burst presence with high accuracy in both English and Japanese, robust across both finely-curated and unprepared speech corpora. Patterns of variability in stop realisation are replicated with the automatic annotations, and closely follow those of manual annotations. These results demonstrate the potential of pre-trained speech models as tools for the automatic annotation and processing of speech corpus data, enabling researchers to 'scale-up' the scope of phonetic research with relative ease.

</details>
