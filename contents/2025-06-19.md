# 2025-06-19

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 55]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [See What I Mean? CUE: A Cognitive Model of Understanding Explanations](https://arxiv.org/abs/2506.14775)

*Tobias Labarta, Nhi Hoang, Katharina Weitz, Wojciech Samek, Sebastian Lapuschkin, Leander Weber*

**Main category:** cs.HC

**Keywords:** Explainable AI, Cognitive Understanding, Accessibility, Human-Computer Interaction, Color Maps

**Relevance Score:** 8

**TL;DR:** The paper presents CUE, a cognitive model for understanding explanations in Explainable AI (XAI) focused on improving accessibility for visually impaired users, revealing shortcomings of current color maps in enhancing explanation legibility and understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for human-understandable explanations in machine learning systems, particularly for users with visual impairments, and to challenge the focus on technical fidelity over cognitive accessibility in XAI evaluations.

**Method:** A study involving 455 participants tested the effectiveness of different heatmap colormaps (BWR, Cividis, Coolwarm) on task performance, confidence, and effort in understanding explanations.

**Key Contributions:**

	1. A formalized cognitive model for explanation understanding (CUE)
	2. An integrated definition of human-centered explanation properties
	3. Empirical evidence for the importance of accessible, user-tailored XAI interfaces

**Result:** The study revealed that while task performance was comparable across groups, visually impaired users exhibited lower confidence and effort, with accessibility-focused color maps like Cividis not improving gaps as expected.

**Limitations:** Study limited to heatmap color maps; further exploration needed across other explanation modalities and user demographics.

**Conclusion:** The findings suggest a need for adaptive XAI interfaces that take cognitive processes into account, emphasizing that alterations in legibility significantly affect users' understanding of explanations.

**Abstract:** As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.

</details>


### [2] [WebXAII: an open-source web framework to study human-XAI interaction](https://arxiv.org/abs/2506.14777)

*Jules Leguy, Pierre-Antoine Jean, Felipe Torres Figueroa, SÃ©bastien Harispe*

**Main category:** cs.HC

**Keywords:** explainable AI, human-computer interaction, web framework

**Relevance Score:** 8

**TL;DR:** WebXAII is an open-source web framework for research on human interaction with explainable AI, providing a flexible platform for conducting reproducible experiments with minimal programming.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations in sharing and reusability of ad hoc interfaces used in studying interactions with explainable AI.

**Method:** WebXAII is designed as a web-based platform that can embody full experimental protocols, translating them into a composite architecture of generic views and modules.

**Key Contributions:**

	1. Open-source web framework for XAI research
	2. Supports full experimental protocol implementation
	3. Flexible architecture for minimal programming

**Result:** WebXAII effectively implements and reproduces experimental protocols, demonstrated through a state-of-the-art study from the literature.

**Limitations:** 

**Conclusion:** The framework promotes reproducibility in XAI research by allowing detailed implementation of experimental protocols with little programming required.

**Abstract:** This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature. The framework is available at https://github.com/PAJEAN/WebXAII.

</details>


### [3] [Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust](https://arxiv.org/abs/2506.14799)

*Evdoxia Taka, Debadyuti Bhattacharya, Joanne Garde-Hansen, Sanjay Sharma, Tanaya Guha*

**Main category:** cs.HC

**Keywords:** AI, character representation, user study, CLIP, visualization

**Relevance Score:** 8

**TL;DR:** This paper presents a new AI-based tool for analyzing character representation in media, focusing on gender and age, and evaluates its usefulness and trustworthiness through a user study.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the usefulness and trustworthiness of AI-generated character representation analytics in media for the general public, as previous works lacked audience involvement.

**Method:** Utilized a tool based on the Contrastive Language Image Pretraining (CLIP) model to analyze character representation across age and gender dimensions, supported by effective visualizations designed for a lay audience. Conducted a user study to evaluate understanding and trust in the analytics provided by the tool.

**Key Contributions:**

	1. Development of an AI-based character representation tool using the CLIP model
	2. User study demonstrating the tool's empirical usefulness
	3. Insights into public trust in AI-generated demographic analytics

**Result:** Participants found the tool's visualizations helpful and expressed overall utility, although their trust in AI-generated analytics on gender and age was moderate to low. More detailed visualizations were requested to cover additional demographic categories and contextual information.

**Limitations:** Participants' trust in AI models was moderate to low; more detailed visualizations for various demographics are needed.

**Conclusion:** The AI-generated character representation tool provides useful insights but requires improvement in trust and detail for greater acceptance by users.

**Abstract:** Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: https://anonymous.4open.science/r/Character-Representation-Media-FF7B

</details>


### [4] [Impact of a Deployed LLM Survey Creation Tool through the IS Success Model](https://arxiv.org/abs/2506.14809)

*Peng Jiang, Vinicius Cezar Monteiro de Lira, Antonio Maiorino*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Survey Generation, Information Systems, Generative AI

**Relevance Score:** 8

**TL;DR:** This paper discusses the deployment of an LLM-powered system for automated survey generation in Information Systems (IS) research, focusing on quality and real-world application challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the labor-intensive nature of creating high-quality surveys in IS research by leveraging advancements in large language models (LLMs) for automation.

**Method:** Deployment of an LLM-powered system, evaluated using the DeLone and McLean IS Success Model.

**Key Contributions:**

	1. First use of the IS Success Model for a generative AI system in survey creation.
	2. Proposed a hybrid evaluation framework integrating automated and human assessments.
	3. Implemented safeguards for responsible integration into IS workflows.

**Result:** The study reveals how generative AI can enhance survey quality and efficiency while outlining complexities of real-world deployment.

**Limitations:** Complexities in user needs and quality control during real-world deployment.

**Conclusion:** Generative AI offers a transformative approach to survey creation in IS, but requires careful integration and evaluation frameworks to ensure quality and mitigate risks.

**Abstract:** Surveys are a cornerstone of Information Systems (IS) research, yet creating high-quality surveys remains labor-intensive, requiring both domain expertise and methodological rigor. With the evolution of large language models (LLMs), new opportunities emerge to automate survey generation. This paper presents the real-world deployment of an LLM-powered system designed to accelerate data collection while maintaining survey quality. Deploying such systems in production introduces real-world complexity, including diverse user needs and quality control. We evaluate the system using the DeLone and McLean IS Success Model to understand how generative AI can reshape a core IS method. This study makes three key contributions. To our knowledge, this is the first application of the IS Success Model to a generative AI system for survey creation. In addition, we propose a hybrid evaluation framework combining automated and human assessments. Finally, we implement safeguards that mitigate post-deployment risks and support responsible integration into IS workflows.

</details>


### [5] [Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction](https://arxiv.org/abs/2506.14820)

*Hyeon Jeon, Hyunwook Lee, Yun-Hsin Kuo, Taehyun Yang, Daniel Archambault, Sungahn Ko, Takanori Fujiwara, Kwan-Liu Ma, Jinwook Seo*

**Main category:** cs.HC

**Keywords:** visual analytics, dimensionality reduction, literature guide, data visualization, expert interviews

**Relevance Score:** 4

**TL;DR:** This paper presents a guide for reading literature on reliable visual analytics using dimensionality reduction (DR), aimed at aiding beginners in the field.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Novice analysts face challenges in navigating the extensive literature on dimensionality reduction and reliable visual analytics.

**Method:** The paper proposes a guide based on a classification of the literature, aiming to help practitioners assess their DR expertise and identify relevant papers.

**Key Contributions:**

	1. Development of a comprehensive guide for reliable visual analytics
	2. Classification of literature on dimensionality reduction
	3. Empirical validation through interviews with experts

**Result:** Interview studies with three experts validate the guide's significance and utility for enhancing understanding of DR.

**Limitations:** 

**Conclusion:** The proposed guide can significantly assist novice analysts and practitioners in navigating the literature on reliable visual analytics with DR.

**Abstract:** Visual analytics using dimensionality reduction (DR) can easily be unreliable for various reasons, e.g., inherent distortions in representing the original data. The literature has thus proposed a wide range of methodologies to make DR-based visual analytics reliable. However, the diversity and extensiveness of the literature can leave novice analysts and researchers uncertain about where to begin and proceed. To address this problem, we propose a guide for reading papers for reliable visual analytics with DR. Relying on the previous classification of the relevant literature, our guide helps both practitioners to (1) assess their current DR expertise and (2) identify papers that will further enhance their understanding. Interview studies with three experts in DR and data visualizations validate the significance, comprehensiveness, and usefulness of our guide.

</details>


### [6] [The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities](https://arxiv.org/abs/2506.14829)

*Aditya Majumdar, Wenbo Zhang, Kashvi Prawal, Amulya Yadav*

**Main category:** cs.HC

**Keywords:** AI for Social Impact, AI4SI, collaboration, socially impactful AI, deployment challenges

**Relevance Score:** 7

**TL;DR:** This paper examines the challenges faced in AI for Social Impact (AI4SI) projects, focusing on obstacles to collaboration and real-world implementation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to illuminate the challenges faced by AI4SI projects in achieving societal impact and to provide actionable strategies to overcome these barriers.

**Method:** The authors conducted semi-structured interviews with six leading AI4SI researchers and analyzed their findings through thematic analysis.

**Key Contributions:**

	1. Identification of key challenges facing AI4SI projects
	2. Thematic analysis of interviews with AI4SI researchers
	3. Recommendations for best practices and actionable strategies for effective collaboration

**Result:** Key challenges identified include structural, organizational, communication, collaboration, and operational barriers that hinder the deployment of AI solutions in social contexts.

**Limitations:** 

**Conclusion:** The paper synthesizes best practices and strategies to help AI4SI researchers and partner organizations enhance their collaboration and effectiveness in socially impactful projects.

**Abstract:** In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects "fail" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations.

</details>


### [7] [Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework](https://arxiv.org/abs/2506.14948)

*Mohna Chakraborty, Lu Wang, David Jurgens*

**Main category:** cs.HC

**Keywords:** language models, moral reasoning, value systems, ethical theories, cognitive strategies

**Relevance Score:** 9

**TL;DR:** This paper presents a value-grounded framework to enhance moral reasoning in LLMs, demonstrating that structured prompting improves decision-making and that moral competence can be distilled from large to small models effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often exhibit shallow reasoning in moral contexts, leading to biased decisions. This paper seeks to provide a better understanding of moral reasoning in LLMs by integrating explicit value systems and ethical theories into their processing.

**Method:** The authors benchmarked 12 open-source LLMs across four moral datasets using a range of structured prompts based on value systems and cognitive reasoning strategies. They evaluated whether structured prompting improves decision-making and the effectiveness of different moral frameworks.

**Key Contributions:**

	1. Introduction of a value-grounded framework for moral reasoning in LLMs
	2. Demonstration of performance improvements through structured moral prompting
	3. Successful distillation of moral competence from larger to smaller LLMs

**Result:** The study finds that prompting with explicit moral structures boosts accuracy and coherence. First-principles reasoning and Schwartz's care-ethics approaches yield significant improvements, and moral competence can be effectively distilled into smaller models without extra inference costs.

**Limitations:** The study may have limitations in the generalizability of the results across all moral dilemmas and its dependence on specific datasets used for evaluation.

**Conclusion:** The proposed framework demonstrates a scalable approach for developing interpretable and morally-grounded AI models, paving the way for more ethical AI applications.

**Abstract:** Large language models (LLMs) are increasingly deployed in domains requiring moral understanding, yet their reasoning often remains shallow, and misaligned with human reasoning. Unlike humans, whose moral reasoning integrates contextual trade-offs, value systems, and ethical theories, LLMs often rely on surface patterns, leading to biased decisions in morally and ethically complex scenarios. To address this gap, we present a value-grounded framework for evaluating and distilling structured moral reasoning in LLMs. We benchmark 12 open-source models across four moral datasets using a taxonomy of prompts grounded in value systems, ethical theories, and cognitive reasoning strategies. Our evaluation is guided by four questions: (1) Does reasoning improve LLM decision-making over direct prompting? (2) Which types of value/ethical frameworks most effectively guide LLM reasoning? (3) Which cognitive reasoning strategies lead to better moral performance? (4) Can small-sized LLMs acquire moral competence through distillation? We find that prompting with explicit moral structure consistently improves accuracy and coherence, with first-principles reasoning and Schwartz's + care-ethics scaffolds yielding the strongest gains. Furthermore, our supervised distillation approach transfers moral competence from large to small models without additional inference cost. Together, our results offer a scalable path toward interpretable and value-grounded models.

</details>


### [8] [Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output](https://arxiv.org/abs/2506.15008)

*Richa Gupta, Alexander Htet Kyaw*

**Main category:** cs.HC

**Keywords:** Generative AI, Sustainability, Interior Design, DALL-E 3, Materials Dataset

**Relevance Score:** 6

**TL;DR:** This paper presents a pipeline that integrates generative AI with sustainability metrics for interior architectural design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the actionable data provided by generative AI in architectural design, particularly focusing on sustainability metrics.

**Method:** The proposed pipeline combines DALL-E 3 with a materials dataset, analyzing generated designs for material usage and corresponding CO2e values. It includes user tests to evaluate how sustainability information affects design decisions.

**Key Contributions:**

	1. Introduction of a novel pipeline that enriches AI-generated designs with sustainability data
	2. Evaluation of user responses to sustainability metrics
	3. Insights on balancing design creativity with environmental considerations

**Result:** User tests indicated that providing sustainability metrics alongside AI-generated designs leads to more informed decisions, but may also cause decision fatigue and reduce satisfaction.

**Limitations:** The study indicates potential decision fatigue and decreased satisfaction when integrating extensive sustainability data.

**Conclusion:** The integration of sustainability metrics can enhance ecologically responsible design practices while highlighting the need to balance freedom and constraints in the creative process.

**Abstract:** Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts. While generative AI can produce visually appealing images they often lack actionable data for designers In this work, we propose a novel pipeline that integrates DALL-E 3 with a materials dataset to enrich AI-generated designs with sustainability metrics and material usage insights. After the model generates an interior design image, a post-processing module identifies the top ten materials present and pairs them with carbon dioxide equivalent (CO2e) values from a general materials dictionary. This approach allows designers to immediately evaluate environmental impacts and refine prompts accordingly. We evaluate the system through three user tests: (1) no mention of sustainability to the user prior to the prompting process with generative AI, (2) sustainability goals communicated to the user before prompting, and (3) sustainability goals communicated along with quantitative CO2e data included in the generative AI outputs. Our qualitative and quantitative analyses reveal that the introduction of sustainability metrics in the third test leads to more informed design decisions, however, it can also trigger decision fatigue and lower overall satisfaction. Nevertheless, the majority of participants reported incorporating sustainability principles into their workflows in the third test, underscoring the potential of integrated metrics to guide more ecologically responsible practices. Our findings showcase the importance of balancing design freedom with practical constraints, offering a clear path toward holistic, data-driven solutions in AI-assisted architectural design.

</details>


### [9] [Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.15047)

*Jiayue Melissa Shi, Dong Whi Yoo, Keran Wang, Violeta J. Rodriguez, Ravi Karkar, Koustuv Saha*

**Main category:** cs.HC

**Keywords:** Alzheimer's Disease, AI chatbot, caregiver support, mental health, large language models

**Relevance Score:** 9

**TL;DR:** The study develops 'Carey', a GPT-4o-based chatbot aimed at supporting family caregivers of individuals with Alzheimer's Disease and Related Dementia by addressing their emotional and informational needs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Family caregivers of individuals with Alzheimer's Disease face emotional and logistical challenges, leading to increased stress, anxiety, and depression, highlighting a need for supportive technologies.

**Method:** The authors developed a chatbot named Carey, conducting semi-structured interviews with 16 family caregivers after scenario-driven interactions to gather insights on their experiences and needs.

**Key Contributions:**

	1. Development of Carey, a supportive AI chatbot for caregivers
	2. Identification of six themes in caregiver needs and expectations
	3. Design recommendations for caregiver-centered AI systems

**Result:** The analysis revealed six key themes related to caregiver needs: on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy, along with tensions in their concerns and desires.

**Limitations:** 

**Conclusion:** The study provides insights on caregiver needs and chatbot capabilities, offering design recommendations for creating more effective AI systems that better support caregivers' mental health.

**Abstract:** Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.

</details>


### [10] [Data Verbalisation: What is Text Doing in a Data Visualisation?](https://arxiv.org/abs/2506.15129)

*Paul Murrell*

**Main category:** cs.HC

**Keywords:** data visualization, text elements, framework, user assessment, visual design

**Relevance Score:** 5

**TL;DR:** This paper presents a framework for understanding the role of text elements in data visualizations, aimed at enhancing their effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of coherent understanding and categorization of text elements in data visualizations, similar to which exists for non-text elements.

**Method:** The authors explore examples of text usage in visualizations and employ existing knowledge and assessment techniques to evaluate text effectiveness.

**Key Contributions:**

	1. Development of a framework for assessing text elements in visualizations
	2. Analysis of text effectiveness compared to non-text elements
	3. Guidelines for better integration of text in data visualizations

**Result:** The study results in a framework that is simple and easy to apply, facilitating the assessment of text elements in data visualizations.

**Limitations:** 

**Conclusion:** A clear framework for text elements in data visualization can improve their purpose and effectiveness, benefiting designers and users alike.

**Abstract:** This article discusses the role that text elements play in a data visualisation. We argue that there is a need for a simple, coherent explanation of text elements similar to the understanding that already exists for non-text elements like bars, points, and lines. We explore examples of how text is used within a data visualisation and use existing knowledge and assessment techniques to evaluate when text is effective and when it is not. The result is a framework that aims to be easy to understand and easy to apply in order to understand the purpose and effectiveness of the text elements in any data visualisation.

</details>


### [11] [Accessible Gesture-Driven Augmented Reality Interaction System](https://arxiv.org/abs/2506.15189)

*Yikan Wang*

**Main category:** cs.HC

**Keywords:** Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper presents a gesture-based interaction system for augmented reality (AR) that enhances accessibility for users with motor impairments by leveraging deep learning techniques for gesture recognition.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accessibility of augmented reality environments for users with motor impairments, who struggle with traditional input methods.

**Method:** The authors use deep learning models, including vision transformers, temporal convolutional networks, and graph attention networks, to process gestures from wearable sensors and cameras. Federated learning is employed for privacy-preserving model training, while reinforcement learning optimizes interface elements.

**Key Contributions:**

	1. Gesture-based interaction system for AR
	2. Application of advanced deep learning methods for gesture recognition
	3. Federated learning for privacy-preserving training

**Result:** The proposed system shows a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems.

**Limitations:** 

**Conclusion:** The study concludes that the gesture-based interaction system significantly enhances AR accessibility and scalability for users with motor impairments.

**Abstract:** Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction

</details>


### [12] [Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces](https://arxiv.org/abs/2506.15293)

*Francesco Chiossi, Julian Rasch, Robin Welsch, Albrecht Schmidt, Florian Michahelles*

**Main category:** cs.HC

**Keywords:** human-robot interaction, intent communication, collaborative workspaces, multimodal communication, task abstraction

**Relevance Score:** 8

**TL;DR:** This position paper discusses the need for effective intent communication between humans and robots in collaborative workspaces, proposing a multidimensional design space for multimodal communication strategies.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Ensuring mutual understanding between human workers and robots is essential for trust, safety, and efficiency in collaborative environments.

**Method:** The paper introduces a structured approach based on the SAT framework and task abstraction levels to create a design space mapping intent content, planning horizon, and communication modality.

**Key Contributions:**

	1. Structured approach to intent communication using SAT framework.
	2. Multidimensional design space for task and communication strategies.
	3. Foundation for a design toolkit for human-robot interactions.

**Result:** A conceptual design space is proposed to guide the development of multimodal communication strategies that enhance human-robot interaction in dynamic work contexts.

**Limitations:** 

**Conclusion:** The study lays the groundwork for a future design toolkit to support transparent and trustworthy collaboration between humans and robots, addressing design challenges and open questions.

**Abstract:** As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments.

</details>


### [13] [UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments](https://arxiv.org/abs/2506.15294)

*Jonas Lau, Annie Tran*

**Main category:** cs.HC

**Keywords:** UX research, feature prioritization, MaxDiff, users with disabilities, survey techniques

**Relevance Score:** 7

**TL;DR:** This paper presents the adapted MaxDiff method for feature prioritization in UX research, particularly for users with disabilities, improving survey efficiency and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of traditional surveying techniques in UX research and improve feature prioritization for consumer product development, especially for users with disabilities.

**Method:** The paper employs multinomial logistic regression, marketed as MaxDiff, to prioritize product features, with an adaptation to reduce the survey response requirement by half.

**Key Contributions:**

	1. Application of MaxDiff method in UX research
	2. Adaptation of the MaxDiff technique to reduce survey responses
	3. Case study focusing on users with disabilities for tablet feature prioritization

**Result:** The adapted MaxDiff method generates a reliable preference list for product features while reducing the burden on survey takers, demonstrated through a case study focused on tablets for users with disabilities.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of the adapted MaxDiff method in improving the user experience of surveys and ensures better feature prioritization for accessible design.

**Abstract:** This paper discusses a popular UX research activity, feature prioritization, using the User Experience Research Point of View (UXR PoV) Playbook framework. We describe an application of multinomial logistic regression, frequently marketed as MaxDiff, for prioritizing product features in consumer product development. It addresses challenges of traditional surveying techniques. We propose a solution using MaxDiff to generate a reliable preference list with a reasonable sample size. We also adapt the MaxDiff method to reduce the number of survey responses in half, making it less tedious from the survey takers' perspective. We present a case study using the adapted MaxDiff method for tablet feature prioritization research involving users with disabilities.

</details>


### [14] [Case Study for Developing a UXR Point of View for FinOps Product Innovation](https://arxiv.org/abs/2506.15314)

*Jason Dong, Anna Wu*

**Main category:** cs.HC

**Keywords:** User Experience Research, Cloud Financial Management, Product Innovation, Mixed-Methods Research, FinOps

**Relevance Score:** 4

**TL;DR:** This case study discusses developing a User Experience Research (UXR) Point of View (PoV) for driving FinOps product innovation through mixed-methods research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in understanding customer needs and aligning teams in Cloud financial management.

**Method:** A multi-phased research approach combining qualitative and quantitative methods to identify customer needs and pain points.

**Key Contributions:**

	1. Development of a UXR PoV for FinOps product innovation
	2. Implementation of a mixed-methods research approach
	3. Creation of a differentiated product strategy and dashboard

**Result:** The research led to the creation of a differentiated product strategy and a comprehensive dashboard for FinOps practitioners.

**Limitations:** 

**Conclusion:** Mixed-methods research can significantly drive product innovation by uncovering actionable insights.

**Abstract:** In the dynamic landscape of Cloud financial management, we are sharing a case study exploring the development of a User Experience Research (UXR) Point of View (PoV) to drive FinOps product innovation. We demonstrate how qualitative and quantitative research methods working together to navigate the challenges of understanding customer needs, aligning cross-functional teams, and prioritizing limited resources. Through a multi-phased research approach, the research team identifies opportunities, quantifies pain points, and segments diverse customer cohorts. This culminated in a UXR PoV that informed the creation of a differentiated product strategy, a 'one-stop shop' dashboard empowering FinOps practitioners with actionable insights and tools. This case study highlights the power of mixed-methods research in uncovering actionable insights that drive impactful product innovation.

</details>


### [15] [Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook](https://arxiv.org/abs/2506.15325)

*Festus Adedoyin, Huseyin Dogan*

**Main category:** cs.HC

**Keywords:** Human-Centred AI, financial services, user experience, machine learning, fraud detection

**Relevance Score:** 4

**TL;DR:** This paper explores the transformative impact of Human-Centred AI in the financial industry, focusing on personalized financial services, enhanced user experience, and AI-powered solutions for fraud detection and portfolio management.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how Human-Centred AI can enhance the financial industry by creating tailored financial products and improving user experience.

**Method:** The research draws upon contemporary studies and industry progress in using HCAI, machine learning, and natural language processing in finance.

**Key Contributions:**

	1. Identification of HCAI applications in finance
	2. Framework for integrating UX research into AI solutions
	3. Insights on AI's role in fraud detection and risk management

**Result:** The study finds that HCAI empowers financial institutions to understand customer needs better, leading to personalized financial solutions and improved user satisfaction.

**Limitations:** 

**Conclusion:** HCAI significantly transforms the financial landscape by tailoring services to user needs and enhancing security through AI-driven solutions.

**Abstract:** Advancements in Artificial Intelligence (AI) have significantly transformed the financial industry, enabling the development of more personalised and adaptable financial products and services. This research paper explores various instances where Human-Centred AI (HCAI) has facilitated these advancements, drawing from contemporary studies and industry progress. The paper examines how the application of HCAI-powered data analytics, machine learning, and natural language processing enables financial institutions to gain a deeper understanding of their customers' unique needs, preferences, and behavioural patterns. This, in turn, allows for the creation of tailored financial solutions that address individual consumer requirements, ultimately enhancing overall user experience and satisfaction. Additionally, the study highlights the integration of AI-powered robo-advisory services, which offer customised investment recommendations and portfolio management tailored to diverse risk profiles and investment goals. Moreover, the paper underscores the role of AI in strengthening fraud detection, risk assessment, and regulatory compliance, leading to a more secure and adaptable financial landscape. The findings of this research demonstrate the substantial impact of Human-Centred AI on the financial industry, offering a strategic framework for financial institutions to leverage these technologies. By incorporating a User Experience Research (UXR) Point of View (PoV), financial institutions can ensure that AI-driven solutions align with user needs and business objectives.

</details>


### [16] [Building Blocks of a User Experience Research Point of View](https://arxiv.org/abs/2506.15332)

*Patricia Diaz*

**Main category:** cs.HC

**Keywords:** User Experience Research, Artificial Intelligence, Enterprise Strategy

**Relevance Score:** 8

**TL;DR:** The paper discusses three User Experience Research perspectives utilizing AI to create impactful strategies in enterprise settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To showcase how User Experience Research can leverage AI and data to create strategies that significantly impact user needs and business priorities in enterprises.

**Method:** The paper outlines three points of view developed through UX research and AI applications in enterprise environments, including the use of smart visuals, AI feedback mechanisms, and identifying opportunities at the intersection of technology and user needs.

**Key Contributions:**

	1. Introduction of AI-driven user experience strategies
	2. Demonstration of effective integration of AI in learning tools
	3. Identification of key opportunity landscapes through UX research

**Result:** The implementation of the proposed UXR strategies had long-lasting positive effects on user engagement and business effectiveness.

**Limitations:** The paper does not address the scalability of these strategies in different organizational contexts.

**Conclusion:** Through these innovative UXR perspectives, the paper highlights the potential of integrating AI in understanding user needs and achieving business goals.

**Abstract:** This paper presents three User Experience Research (UXR) perspectives based on data, evidence and insights - known as Point of View (POV) - showcasing how the strategies and methods of building a POV work in an enterprise setting. The POV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in videos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the learner as it is the loop that requires the least effort for the highest impact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at the intersection of emergent technical capabilities that unlock novel approaches to critical user needs while addressing business strategic priorities (2019). They all seemed far-fetched and went against common practice. All were adopted and had long-lasting impact.

</details>


### [17] [Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI](https://arxiv.org/abs/2506.15468)

*Ryota Okumura, Tadahiro Taniguchi, Akira Taniguchi, Yoshinobu Hagiwara*

**Main category:** cs.HC

**Keywords:** co-creative learning, human-AI interaction, Metropolis-Hastings naming game, shared representations, symbiotic AI

**Relevance Score:** 8

**TL;DR:** Proposes co-creative learning where humans and AI collaboratively construct shared representations, tested via a human-AI interaction model.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses challenges of integrating information from different modalities in human-AI interactions.

**Method:** An online experiment involving participants playing a joint attention naming game with various AI agents based on the Metropolis-Hastings naming game.

**Key Contributions:**

	1. Introduction of co-creative learning paradigm
	2. Empirical evidence in human-AI dyads via MHNG
	3. Demonstration of improved categorization through interaction

**Result:** Human-AI pairs with an MH-based agent improved categorization accuracy and converged towards a shared sign system.

**Limitations:** 

**Conclusion:** Empirical evidence for co-creative learning in human-AI pairs suggests potential for symbiotic AI systems that learn with humans.

**Abstract:** We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment.

</details>


### [18] [Foundation of Affective Computing and Interaction](https://arxiv.org/abs/2506.15497)

*Changzeng Fu*

**Main category:** cs.HC

**Keywords:** affective computing, human-computer interaction, multimodal emotion recognition, brain-computer interfaces, ethical considerations

**Relevance Score:** 9

**TL;DR:** A comprehensive exploration of affective computing and human-computer interaction technologies, addressing emotional computing, interaction modalities, and ethical considerations.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the intersection of emotion and technology in human-computer interaction (HCI) and to identify future trends and considerations.

**Method:** The book covers historical developments, technical frameworks, and practical applications of various interaction technologies, including emotional computing, multimodal recognition, and brain-computer interfaces.

**Key Contributions:**

	1. Comprehensive overview of affective computing and HCI technologies
	2. Detailed exploration of emotional recognition systems
	3. Discussion of ethical implications in human-machine interactions

**Result:** The book provides detailed explanations of key technologies and their applications across fields such as education, healthcare, and entertainment, while addressing challenges in multimodal data fusion and ethical implications.

**Limitations:** 

**Conclusion:** The future of affective computing is anticipated to involve deeper AI integration, advancements in multimodal interaction technologies, and a focus on ethical considerations in development.

**Abstract:** This book provides a comprehensive exploration of affective computing and human-computer interaction technologies. It begins with the historical development and basic concepts of human-computer interaction, delving into the technical frameworks and practical applications of emotional computing, visual interaction, voice interaction, brain-computer interfaces, physiological electrical signal analysis, and social robotics. The book covers a wide range of topics, including the psychological and neuroscience foundations of emotion, multimodal emotion recognition, emotional expression mechanisms, and the principles of brain-computer interfaces.   Key technologies such as affective computing based on discrete emotion theory and dimensional models, visual perception principles, speech recognition and synthesis, EEG signal acquisition and processing, and multimodal emotion recognition are explained in detail. This book also addresses the technical challenges in the field, including multimodal data fusion, privacy and security, and ethical considerations in human-machine relationships. It discusses the applications of these technologies across various domains such as education, healthcare, entertainment, and intelligent assistance.   Looking to the future, the book anticipates trends such as the deep integration of artificial intelligence with emotion recognition, the advancement of multimodal interaction technologies, and the development of more personalized and adaptive emotion recognition systems. It emphasizes the importance of balancing technological innovation with ethical considerations to ensure the responsible development and application of affective computing technologies.

</details>


### [19] [Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach](https://arxiv.org/abs/2506.15512)

*Wenqi Guan, Yang Fang*

**Main category:** cs.HC

**Keywords:** Large Language Models, remote learning, LangChain, CoT reasoning, prompt engineering

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel approach to enhancing remote learning retrieval using GPT-based models within the LangChain framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current retrieval of remote learning resources lacks sufficient contextual depth for complex student queries, impacting user satisfaction and learning outcomes.

**Method:** The approach integrates GPT-based models within the LangChain framework, utilizing Chain of Thought (CoT) reasoning and prompt engineering to improve retrieval precision and relevance.

**Key Contributions:**

	1. Novel integration of GPT-based models in the LangChain framework for remote learning
	2. Use of CoT reasoning and prompt engineering to improve context and precision
	3. Demonstrated improvements in user satisfaction and learning outcomes.

**Result:** The proposed system shows improvements in user satisfaction and learning outcomes when compared to standard LLMs.

**Limitations:** 

**Conclusion:** Integrating GPT-based models within the LangChain framework significantly enhances the retrieval of remote learning resources, providing contextually enriched explanations that meet students' needs.

**Abstract:** Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes.

</details>


### [20] ["How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students](https://arxiv.org/abs/2506.15525)

*Isabella Pu, Prerna Ravi, Linh Dieu Dinh, Chelsea Joe, Caitlin Ogoe, Zixuan Li, Cynthia Breazeal, Anastasia K. Ostrowski*

**Main category:** cs.HC

**Keywords:** generative AI, high school, participatory design, educational technology, AI policy

**Relevance Score:** 4

**TL;DR:** This paper investigates high school students' perspectives on generative AI integration in education through a participatory design workshop, resulting in guidelines for developing AI tools that address their concerns.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate generative AI meaningfully in high school environments by understanding students' perspectives, a group often excluded from prior research.

**Method:** Conducted a participatory design workshop with 17 high school students to gather insights on their perspectives and design preferences regarding generative AI tools.

**Key Contributions:**

	1. Insights from high school students on GenAI tool design and policies.
	2. Proposed guidelines for educational technology designers.
	3. Emphasis on student involvement in AI policy creation.

**Result:** Students identified challenges related to bias, misinformation, crime, plagiarism, and over-reliance on AI, and proposed solutions outlining their ideal features for GenAI tools and appropriate regulations.

**Limitations:** 

**Conclusion:** New guidelines are proposed for educational technology designers to develop generative AI technologies in high schools, emphasizing the need for student voices in AI policy development.

**Abstract:** As generative AI (GenAI) emerges as a transformative force, clear understanding of high school students' perspectives is essential for GenAI's meaningful integration in high school environments. In this work, we draw insights from a participatory design workshop where we engaged 17 high school students -- a group rarely involved in prior research in this area -- through the design of novel GenAI tools and school policies addressing their key concerns. Students identified challenges and developed solutions outlining their ideal features in GenAI tools, appropriate school use, and regulations. These centered around the problem spaces of combating bias & misinformation, tackling crime & plagiarism, preventing over-reliance on AI, and handling false accusations of academic dishonesty. Building on our participants' underrepresented perspectives, we propose new guidelines targeted at educational technology designers for development of GenAI technologies in high schools. We also argue for further incorporation of student voices in development of AI policies in their schools.

</details>


### [21] [Implementation Considerations for Automated AI Grading of Student Work](https://arxiv.org/abs/2506.07955)

*Zewei, Tian, Alex Liu, Lief Esbenshade, Shawon Sarkar, Zachary Zhang, Kevin He, Min Sun*

**Main category:** cs.HC

**Keywords:** AI grading, K-12 education, teacher-centered design, assessment, pedagogical agency

**Relevance Score:** 7

**TL;DR:** This study investigates the use of an AI grading platform in K-12 classrooms, revealing mixed responses from teachers and students regarding AI-generated assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how teachers and students interact with AI-powered grading platforms and the implications for trust and pedagogical authority in assessments.

**Method:** Conducted a co-design pilot with 19 teachers, utilizing platform usage logs, surveys, and qualitative interviews to analyze the impact of AI-generated rubrics and feedback on teaching and learning experiences.

**Key Contributions:**

	1. Insights into teacher-student interactions with AI grading platforms
	2. Identification of trust issues surrounding automated scoring
	3. Guidelines for designing trustworthy AI assessment tools

**Result:** Teachers appreciated rapid feedback but distrusted automated scoring; students favored quick feedback yet expressed skepticism toward AI-only grading.

**Limitations:** Limited to K-12 settings and may not generalize to other educational contexts.

**Conclusion:** Trust and oversight are critical when integrating AI assessment tools, and any design must foster teacher agency while providing effective feedback.

**Abstract:** This study explores the classroom implementation of an AI-powered grading platform in K-12 settings through a co-design pilot with 19 teachers. We combine platform usage logs, surveys, and qualitative interviews to examine how teachers use AI-generated rubrics and grading feedback. Findings reveal that while teachers valued the AI's rapid narrative feedback for formative purposes, they distrusted automated scoring and emphasized the need for human oversight. Students welcomed fast, revision-oriented feedback but remained skeptical of AI-only grading. We discuss implications for the design of trustworthy, teacher-centered AI assessment tools that enhance feedback while preserving pedagogical agency.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)

*Imane Guellil, SalomÃ© Andres, Atul Anand, Bruce Guthrie, Huayu Zhang, Abul Hasan, Honghan Wu, Beatrice Alex*

**Main category:** cs.CL

**Keywords:** Adverse Event extraction, clinical NLP, dataset, elderly patients, transformer models

**Relevance Score:** 8

**TL;DR:** A dataset for extracting adverse events from discharge summaries of elderly patients is presented, addressing underrepresentation in clinical NLP.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in adverse event extraction from clinical text, particularly for elderly patients who are underrepresented in existing datasets.

**Method:** Development of a manually annotated corpus containing 14 clinically significant adverse events along with contextual attributes; evaluation using various models in FlairNLP.

**Key Contributions:**

	1. A novel dataset targeting adverse event extraction in discharge summaries of the elderly.
	2. Support for both discontinuous and overlapping entities in the annotation schema.
	3. Evaluation of multiple models, highlighting gaps in fine-grained entity extraction performance.

**Result:** Transformer models achieved high performance in coarse-grained extraction but struggled significantly with fine-grained tasks, revealing ongoing challenges in the extraction of nuanced clinical information.

**Limitations:** Focus on a specific population may limit generalizability; performance drop for fine-grained entity extraction shows existing challenges.

**Conclusion:** Despite high-level performance metrics, difficulties remain in extracting rare events and complex attributes, indicating a need for continued research in this area.

**Abstract:** In this work, we present a manually annotated corpus for Adverse Event (AE) extraction from discharge summaries of elderly patients, a population often underrepresented in clinical NLP resources. The dataset includes 14 clinically significant AEs-such as falls, delirium, and intracranial haemorrhage, along with contextual attributes like negation, diagnosis type, and in-hospital occurrence. Uniquely, the annotation schema supports both discontinuous and overlapping entities, addressing challenges rarely tackled in prior work. We evaluate multiple models using FlairNLP across three annotation granularities: fine-grained, coarse-grained, and coarse-grained with negation. While transformer-based models (e.g., BERT-cased) achieve strong performance on document-level coarse-grained extraction (F1 = 0.943), performance drops notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly for rare events and complex attributes. These results demonstrate that despite high-level scores, significant challenges remain in detecting underrepresented AEs and capturing nuanced clinical language. Developed within a Trusted Research Environment (TRE), the dataset is available upon request via DataLoch and serves as a robust benchmark for evaluating AE extraction methods and supporting future cross-dataset generalisation.

</details>


### [23] [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)

*Marija Å akota, Robert West*

**Main category:** cs.CL

**Keywords:** Boosted Constrained Decoding, Structured NLP, Information Extraction

**Relevance Score:** 8

**TL;DR:** Boosted Constrained Decoding (BoostCD) improves structured NLP task performance by combining predictions from constrained and unconstrained decoding phases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address low-quality outputs during constrained decoding at test time for structured NLP tasks using autoregressive models.

**Method:** BoostCD operates in two phases: Phase 1 involves decoding from the base model in both constrained and unconstrained modes to generate two weak predictions, and Phase 2 combines these predictions using a learned autoregressive model for a final output.

**Key Contributions:**

	1. Introduction of Boosted Constrained Decoding (BoostCD) methodology.
	2. Improvement of structured NLP task results using BoostIE.
	3. Demonstrated effectiveness in closed information extraction beyond prior approaches.

**Result:** BoostCD, demonstrated through the BoostIE model, outperforms prior methods in closed information extraction tasks both in and out of distribution.

**Limitations:** 

**Conclusion:** The complementary mistakes made by the base model in different modes are effectively leveraged by the boosted model, leading to improved structured outputs.

**Abstract:** Many recent approaches to structured NLP tasks use an autoregressive language model $M$ to map unstructured input text $x$ to output text $y$ representing structured objects (such as tuples, lists, trees, code, etc.), where the desired output structure is enforced via constrained decoding. During training, these approaches do not require the model to be aware of the constraints, which are merely implicit in the training outputs $y$. This is advantageous as it allows for dynamic constraints without requiring retraining, but can lead to low-quality output during constrained decoding at test time. We overcome this problem with Boosted Constrained Decoding (BoostCD), which combines constrained and unconstrained decoding in two phases: Phase 1 decodes from the base model $M$ twice, in constrained and unconstrained mode, obtaining two weak predictions. In phase 2, a learned autoregressive boosted model combines the two weak predictions into one final prediction. The mistakes made by the base model with vs. without constraints tend to be complementary, which the boosted model learns to exploit for improved performance. We demonstrate the power of BoostCD by applying it to closed information extraction. Our model, BoostIE, outperforms prior approaches both in and out of distribution, addressing several common errors identified in those approaches.

</details>


### [24] [CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision](https://arxiv.org/abs/2506.14912)

*Dyah Adila, Shuai Zhang, Boran Han, Bonan Min, Yuyang Wang*

**Main category:** cs.CL

**Keywords:** large language models, credibility assessment, machine learning, natural language processing, contextual information

**Relevance Score:** 9

**TL;DR:** CrEst is a weakly supervised framework for assessing the credibility of context documents in LLM inference, improving accuracy and robustness in knowledge-intensive tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLM performance by addressing the reliability of context documents that can influence the output.

**Method:** CrEst utilizes semantic coherence among credible documents to provide automated credibility estimation through inter-document agreement, using both black-box and white-box integration strategies.

**Key Contributions:**

	1. Introduction of a weakly supervised framework for document credibility assessment
	2. Development of two integration strategies for LLMs
	3. Significant improvements over baseline models in accuracy and F1 score

**Result:** CrEst improves accuracy by 26.86% and the F1 score by 3.49% across various models and datasets, outperforming strong baselines.

**Limitations:** 

**Conclusion:** The framework effectively integrates credibility assessment into LLM inference, demonstrating consistent performance even in high-noise environments.

**Abstract:** The integration of contextual information has significantly enhanced the performance of large language models (LLMs) on knowledge-intensive tasks. However, existing methods often overlook a critical challenge: the credibility of context documents can vary widely, potentially leading to the propagation of unreliable information. In this paper, we introduce CrEst, a novel weakly supervised framework for assessing the credibility of context documents during LLM inference--without requiring manual annotations. Our approach is grounded in the insight that credible documents tend to exhibit higher semantic coherence with other credible documents, enabling automated credibility estimation through inter-document agreement. To incorporate credibility into LLM inference, we propose two integration strategies: a black-box approach for models without access to internal weights or activations, and a white-box method that directly modifies attention mechanisms. Extensive experiments across three model architectures and five datasets demonstrate that CrEst consistently outperforms strong baselines, achieving up to a 26.86% improvement in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst maintains robust performance even under high-noise conditions.

</details>


### [25] [MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://arxiv.org/abs/2506.14927)

*Joseph J. Peper, Wenzhao Qiu, Ali Payani, Lu Wang*

**Main category:** cs.CL

**Keywords:** large language models, multi-document reasoning, evaluation benchmarks, synthetic data generation, natural language processing

**Relevance Score:** 9

**TL;DR:** MDBench is a new dataset created for evaluating large language models on multi-document reasoning tasks, utilizing a novel synthetic generation process to create challenging document sets and corresponding question-answer examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind MDBench is to provide an evaluation benchmark for the reasoning capabilities of large language models in the context of multi-document inputs, which are critical due to the limitations of existing benchmarks and the high cost of annotating such data.

**Method:** MDBench is generated through a synthetic process that modifies condensed structured seed knowledge using LLM-assisted edits, resulting in controllable creation of document sets and QA examples tailored for multi-document reasoning.

**Key Contributions:**

	1. Introduction of MDBench as a dataset for multi-document reasoning
	2. Development of a novel synthetic generation process allowing efficient creation of challenging document sets
	3. Analysis of LLM performance on MDBench revealing significant reasoning challenges

**Result:** Analysis shows that MDBench presents significant challenges for popular LLMs and prompting techniques, highlighting the need for robust handling of multi-document contexts, even for shorter sets.

**Limitations:** 

**Conclusion:** MDBench not only enables targeted analysis of multi-document reasoning but also adapts quickly to new challenges and future advancements in model performance.

**Abstract:** Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language mod-els (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, while multi-document (MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us to controllably and efficiently generate challenging document sets and the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example. We analyze the behavior of popular LLMs and prompting techniques, finding that MDBENCH poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.

</details>


### [26] [From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?](https://arxiv.org/abs/2506.14949)

*Shadman Sakib, Oishy Fatema Akhand, Ajwad Abrar*

**Main category:** cs.CL

**Keywords:** diabetes prediction, large language models, machine learning, healthcare, Pima Indian Diabetes Database

**Relevance Score:** 9

**TL;DR:** This study explores the effectiveness of Large Language Models (LLMs) in predicting diabetes using various prompting methods and compares their performance with traditional machine learning models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the application of Large Language Models in predicting medical conditions, specifically diabetes, given their under-explored potential for structured numerical data.

**Method:** An empirical analysis was conducted using the Pima Indian Diabetes Database (PIDD) with six LLMs and three traditional ML models, evaluating performance using accuracy, precision, recall, and F1-score.

**Key Contributions:**

	1. Demonstrated the effectiveness of LLMs in medical predictions
	2. Showed LLM performance surpassing traditional ML models in certain metrics
	3. Highlighted the need for further research in prompt engineering for healthcare applications

**Result:** Proprietary LLMs, particularly GPT-4o and Gemma-2-27B, outperformed open-source models and traditional ML models in diabetes prediction, especially in few-shot settings.

**Limitations:** Performance variance across different prompting strategies and necessity for domain-specific fine-tuning.

**Conclusion:** LLMs can effectively aid medical prediction tasks, but improvements in prompting strategies and domain-specific fine-tuning are needed for optimal performance.

**Abstract:** While Machine Learning (ML) and Deep Learning (DL) models have been widely used for diabetes prediction, the use of Large Language Models (LLMs) for structured numerical data is still not well explored. In this study, we test the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and three-shot prompting methods. We conduct an empirical analysis using the Pima Indian Diabetes Database (PIDD). We evaluate six LLMs, including four open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we compare their performance with three traditional machine learning models: Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use accuracy, precision, recall, and F1-score as evaluation metrics. Our results show that proprietary LLMs perform better than open-source ones, with GPT-4o and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably, Gemma-2-27B also outperforms the traditional ML models in terms of F1-score. However, there are still issues such as performance variation across prompting strategies and the need for domain-specific fine-tuning. This study shows that LLMs can be useful for medical prediction tasks and encourages future work on prompt engineering and hybrid approaches to improve healthcare predictions.

</details>


### [27] [Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings](https://arxiv.org/abs/2506.15001)

*Ignacio Sastre, Aiala RosÃ¡*

**Main category:** cs.CL

**Keywords:** reversible sentence embeddings, LLM, text reconstruction, memory token, controlled generation

**Relevance Score:** 8

**TL;DR:** The paper discusses generating reversible sentence embeddings allowing LLMs to reconstruct original text without modifying their weights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capability of LLMs in reconstructing original text using reversible sentence embeddings for potential applications.

**Method:** Introducing a special memory token whose embedding is specifically optimized through training on a fixed sequence, allowing the model to reconstruct the original text.

**Key Contributions:**

	1. Introduction of reversible sentence embeddings for LLMs
	2. Successful reconstruction of original text from embeddings
	3. Demonstration of LLM capabilities in memory-based applications

**Result:** Evaluation across English and Spanish datasets showed that Llama 3.1 8B successfully reconstructs sequences of up to approximately 240 tokens across various model scales.

**Limitations:** 

**Conclusion:** The findings suggest significant implications for memory-based retrieval, compression, and controlled text generation within LLMs.

**Abstract:** In this work, we observe an interesting phenomenon: it is possible to generate reversible sentence embeddings that allow an LLM to reconstruct the original text exactly, without modifying the model's weights. This is achieved by introducing a special memory token, whose embedding is optimized through training on a fixed sequence. When prompted with this embedding, the model reconstructs the fixed sequence exactly. We evaluate this phenomenon across English and Spanish datasets, sequences of up to approximately 240 tokens, and model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B successfully reconstructs all tested sequences. Our findings highlight an interesting capability of LLMs and suggest potential applications in memory-based retrieval, compression, and controlled text generation.

</details>


### [28] [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)

*Drew Walker, Swati Rajwal, Sudeshna Das, Snigdha Peddireddy, Abeed Sarker*

**Main category:** cs.CL

**Keywords:** social isolation, loneliness, NLP, suicide, classification

**Relevance Score:** 8

**TL;DR:** This paper applies NLP techniques to identify social isolation and loneliness in suicide narratives, revealing significant predictors and improving prevention efforts.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** Social isolation and loneliness are linked to increased suicide rates, yet they are not currently monitored in official reporting systems. This study aims to fill this gap by using NLP to analyze law enforcement and coroner narratives.

**Method:** The research utilizes topic modeling for lexicon development and supervised learning classifiers to create high-quality classifiers, achieving an average F1 score of .86 and accuracy of .82.

**Key Contributions:**

	1. Application of NLP techniques to suicide narratives for identifying social isolation
	2. Development of high-quality classifiers for chronic social isolation
	3. Identification of significant predictors for social isolation related to demographic factors.

**Result:** From analyzing over 300,000 suicides from 2002 to 2020, the study identified 1,198 cases mentioning chronic social isolation, with specific demographic groups, such as men and gay individuals, showing higher odds of classification related to social isolation.

**Limitations:** 

**Conclusion:** The methods developed in this study can enhance surveillance and preventative strategies for social isolation and loneliness, especially in the context of suicide prevention in the U.S.

**Abstract:** Social isolation and loneliness, which have been increasing in recent years strongly contribute toward suicide rates. Although social isolation and loneliness are not currently recorded within the US National Violent Death Reporting System's (NVDRS) structured variables, natural language processing (NLP) techniques can be used to identify these constructs in law enforcement and coroner medical examiner narratives. Using topic modeling to generate lexicon development and supervised learning classifiers, we developed high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over 300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic social isolation. Decedents had higher odds of chronic social isolation classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR = 3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001). We found significant predictors for other social isolation topics of recent or impending divorce, child custody loss, eviction or recent move, and break-up. Our methods can improve surveillance and prevention of social isolation and loneliness in the United States.

</details>


### [29] [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation](https://arxiv.org/abs/2506.15068)

*Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, Jordan Lee Boyd-Graber*

**Main category:** cs.CL

**Keywords:** open-ended generation, evaluation metrics, PrefBERT, long-form generation, human preferences

**Relevance Score:** 8

**TL;DR:** PrefBERT is a scoring model designed to evaluate open-ended long-form generation, improving semantic feedback compared to traditional metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation methods struggle to effectively separate good from bad outputs in long-form generation due to biases and lack of coherence and relevance assessment.

**Method:** PrefBERT is trained on diverse response evaluation datasets and uses distinct rewards for guiding long-form generation output quality in generative reinforcement policy optimization (GRPO).

**Key Contributions:**

	1. Introduction of PrefBERT for evaluating open-ended long-form outputs
	2. Demonstrates improved alignment with human preferences over traditional metrics
	3. Code availability for further research and application

**Result:** PrefBERT offers improved semantic reward feedback and aligns better with human preferences compared to traditional metrics like ROUGE-L and BERTScore.

**Limitations:** 

**Conclusion:** The use of PrefBERT in training policy models leads to responses that are more aligned with human quality assessments than those trained with conventional metrics.

**Abstract:** Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.

</details>


### [30] [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076)

*Ruihan Wu, Konstantin Garov, Kamalika Chaudhuri*

**Main category:** cs.CL

**Keywords:** large language models, unlearning, knowledge encoding, empirical investigation, factual knowledge

**Relevance Score:** 9

**TL;DR:** Investigates the impact of learning-time choices in knowledge encoding on the effectiveness of unlearning in large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the deployment of LLMs, the ability to unlearn knowledge has become critical due to privacy concerns and the need to correct outdated information.

**Method:** Empirical investigation of knowledge encoding methods and their impact on unlearning performance through experiments.

**Key Contributions:**

	1. Demonstrates the effect of paraphrased descriptions on unlearning efficiency.
	2. Identifies challenges in unlearning specific knowledge from larger text blocks.
	3. Highlights the importance of learning-time knowledge encoding.

**Result:** Learning with paraphrased descriptions enhances unlearning performance, while unlearning specific knowledge from chunks of text proves challenging.

**Limitations:** Focuses on empirical results without a theoretical framework for unlearning mechanisms.

**Conclusion:** Knowledge encoding during the learning phase significantly influences the post-hoc unlearning process in LLMs.

**Abstract:** As large language models (LLMs) are increasingly deployed in the real world, the ability to ``unlearn'', or remove specific pieces of knowledge post hoc, has become essential for a variety of reasons ranging from privacy regulations to correcting outdated or harmful content. Prior work has proposed unlearning benchmarks and algorithms, and has typically assumed that the training process and the target model are fixed. In this work, we empirically investigate how learning-time choices in knowledge encoding impact the effectiveness of unlearning factual knowledge. Our experiments reveal two key findings: (1) learning with paraphrased descriptions improves unlearning performance and (2) unlearning individual piece of knowledge from a chunk of text is challenging. Our results suggest that learning-time knowledge encoding may play a central role in enabling reliable post-hoc unlearning.

</details>


### [31] [Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification](https://arxiv.org/abs/2506.15081)

*Yaxin Fan, Peifeng Li, Qiaoming Zhu*

**Main category:** cs.CL

**Keywords:** Dialogue Discourse Parsing, Clarification Module, Discourse Relations, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper presents a Discourse-aware Clarification Module (DCM) that enhances dialogue discourse parsing by resolving ambiguities in utterance relations through clarification type and discourse goal reasoning, along with a Contribution-aware Preference Optimization (CPO) mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Dialogue discourse parsing is challenged by ambiguities introduced by linguistic features such as omission and idioms, which can obscure intended discourse relations between utterances.

**Method:** The proposed DCM employs two reasoning processes: clarification type reasoning to analyze linguistic features and discourse goal reasoning to distinguish intended relations from ambiguities. CPO is introduced to optimize the DCM by assessing the contributions of clarifications and providing feedback.

**Key Contributions:**

	1. Introduction of the Discourse-aware Clarification Module (DCM)
	2. Development of Contribution-aware Preference Optimization (CPO)
	3. Demonstration of significant performance improvement over state-of-the-art baselines

**Result:** Experiments on the STAC and Molweni datasets show that the proposed approach effectively resolves ambiguities and significantly outperforms state-of-the-art baselines in dialogue discourse parsing.

**Limitations:** 

**Conclusion:** The implementation of DCM and CPO demonstrates a robust improvement in the parser's performance, evidencing the effectiveness of discourse-aware techniques in addressing dialogue ambiguities.

**Abstract:** Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.

</details>


### [32] [CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records](https://arxiv.org/abs/2506.15118)

*Junke Wang, Hongshun Ling, Li Zhang, Longqian Zhang, Fang Wang, Yuan Gao, Zhi Li*

**Main category:** cs.CL

**Keywords:** Electronic Health Records, Disease Prediction, Knowledge Distillation, Clinical Efficiency, Machine Learning

**Relevance Score:** 9

**TL;DR:** The CKD-EHR framework enhances disease prediction in Electronic Health Records by embedding medical knowledge in language models, achieving better accuracy and efficiency in clinical settings.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve disease prediction models in EHRs which struggle with medical knowledge representation and clinical deployment efficiency.

**Method:** The study employs the CKD-EHR framework which involves fine-tuning the Qwen2.5-7B model with medical knowledge data, using a multi-granularity attention distillation mechanism to generate soft labels for a lightweight BERT student model.

**Key Contributions:**

	1. Proposes the CKD-EHR framework for disease risk prediction
	2. Uses knowledge distillation from a large model to a lightweight BERT model
	3. Demonstrates significant improvements in accuracy and efficiency on clinical data.

**Result:** CKD-EHR outperforms baseline models with a 9% increase in diagnostic accuracy, a 27% improvement in F1-score, and a 22.2 times speedup in inference over the MIMIC-III dataset.

**Limitations:** 

**Conclusion:** CKD-EHR provides significant improvements in diagnosis accuracy and resource utilization, representing a practical solution for clinical settings.

**Abstract:** Electronic Health Records (EHR)-based disease prediction models have demonstrated significant clinical value in promoting precision medicine and enabling early intervention. However, existing large language models face two major challenges: insufficient representation of medical knowledge and low efficiency in clinical deployment. To address these challenges, this study proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which achieves efficient and accurate disease risk prediction through knowledge distillation techniques. Specifically, the large language model Qwen2.5-7B is first fine-tuned on medical knowledge-enhanced data to serve as the teacher model.It then generates interpretable soft labels through a multi-granularity attention distillation mechanism. Finally, the distilled knowledge is transferred to a lightweight BERT student model. Experimental results show that on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and a 22.2 times inference speedup is achieved. This innovative solution not only greatly improves resource utilization efficiency but also significantly enhances the accuracy and timeliness of diagnosis, providing a practical technical approach for resource optimization in clinical settings. The code and data for this research are available athttps://github.com/209506702/CKD_EHR.

</details>


### [33] [Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs](https://arxiv.org/abs/2506.15131)

*Jing Yang Lee, Kong-Aik Lee, Woon-Seng Gan*

**Main category:** cs.CL

**Keywords:** Open-domain Dialogue, Multi-Response Generation, Preference-based Selection

**Relevance Score:** 7

**TL;DR:** This paper addresses the one-to-many property of open-domain dialogue by proposing a framework that includes Multi-Response Generation and Preference-based Selection, leveraging a new dialogue corpus for enhanced response diversity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance response diversity in open-domain dialogue systems, as most LLM-based agents do not explicitly model the one-to-many property of dialogues.

**Method:** The paper decomposes the generation of dialogues into two tasks: generating multiple diverse responses and selecting one based on preferences. A new corpus, o2mDial, is introduced to facilitate these tasks.

**Key Contributions:**

	1. Introduction of o2mDial corpus for Multi-Response Generation
	2. Proposed in-context learning and instruction-tuning strategies
	3. Development of new evaluation metrics for response generation

**Result:** The framework improves response diversity and quality, achieving up to a 90% increase in response quality in comparison to smaller LLMs, aligning their performance closer to larger models.

**Limitations:** 

**Conclusion:** The proposed two-stage framework effectively models the one-to-many property and improves the generation quality of open-domain dialogue systems.

**Abstract:** Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.

</details>


### [34] [Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models](https://arxiv.org/abs/2506.15138)

*Gyeongje Cho, Yeonkyoun So, Chanwoo Park, Sangmin Lee, Sungmok Jung, Jaejin Lee*

**Main category:** cs.CL

**Keywords:** tokenization, Korean language, language models, linguistic structure, efficiency

**Relevance Score:** 4

**TL;DR:** This paper introduces Thunder-Tok, a Korean tokenizer that reduces token fertility by 10% using a linguistically informed approach.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of token fertility in language modeling without sacrificing performance, particularly for the Korean language.

**Method:** A rule-based pre-tokenization method aligned with Korean linguistic structure, creating a seed vocabulary, and a branching entropy-based selection algorithm.

**Key Contributions:**

	1. Introduction of Thunder-Tok, a new Korean tokenizer
	2. Reduction of token fertility by 10%
	3. Linguistically informed pre-tokenization method

**Result:** Thunder-Tok reduces the number of tokens by approximately 10%, resulting in a 10% increase in inference speed, without compromising performance across downstream tasks.

**Limitations:** 

**Conclusion:** The proposed tokenizer effectively enhances the efficiency of language models for the Korean language while preserving linguistic information.

**Abstract:** This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.

</details>


### [35] [Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View](https://arxiv.org/abs/2506.15156)

*Muhammad Cendekia Airlangga, Hilal AlQuabeh, Munachiso S Nwadike, Kentaro Inui*

**Main category:** cs.CL

**Keywords:** state-space models, memory retention, primacy effects, recency effects, semantic regularity

**Relevance Score:** 7

**TL;DR:** This study investigates memory in state-space language models through primacy and recency effects, revealing mechanisms that influence information retention and forgetting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how information is retained and forgotten in state-space language models and to uncover the underlying mechanisms of memory performance.

**Method:** Structured recall tasks were applied to the Mamba architecture to assess accuracy profiles and identify memory mechanisms through targeted ablations and input perturbations.

**Key Contributions:**

	1. Identified mechanisms of long-term and short-term memory in language models.
	2. Demonstrated the impact of semantic regularity on memory allocation.
	3. Validated findings using large-scale language models with targeted experiments.

**Result:** A U-shaped accuracy profile indicates strong performance at the beginning and end of input sequences, with identified mechanisms of long-term memory being linked to selective state space channels, short-term memory being influenced by recent inputs, and dynamic modulation by semantic regularity.

**Limitations:** Limited to the specific architecture of the Mamba models and may not generalize to all state-space models.

**Conclusion:** Memory in state-space models is nuanced, with distinct mechanisms influencing retention and forgetting based on input structure and temporal factors.

**Abstract:** We study memory in state-space language models using primacy and recency effects as behavioral tools to uncover how information is retained and forgotten over time. Applying structured recall tasks to the Mamba architecture, we observe a consistent U-shaped accuracy profile, indicating strong performance at the beginning and end of input sequences. We identify three mechanisms that give rise to this pattern. First, long-term memory is supported by a sparse subset of channels within the model's selective state space block, which persistently encode early input tokens and are causally linked to primacy effects. Second, short-term memory is governed by delta-modulated recurrence: recent inputs receive more weight due to exponential decay, but this recency advantage collapses when distractor items are introduced, revealing a clear limit to memory depth. Third, we find that memory allocation is dynamically modulated by semantic regularity: repeated relations in the input sequence shift the delta gating behavior, increasing the tendency to forget intermediate items. We validate these findings via targeted ablations and input perturbations on two large-scale Mamba-based language models: one with 1.4B and another with 7B parameters.

</details>


### [36] [A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals](https://arxiv.org/abs/2506.15208)

*Andrea Cadeddu, Alessandro Chessa, Vincenzo De Leo, Gianni Fenu, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino, Luca Secchi*

**Main category:** cs.CL

**Keywords:** Sustainable Development Goals, text classification, large language models

**Relevance Score:** 8

**TL;DR:** This study examines the use of LLMs for text classification related to the UN's Sustainable Development Goals, comparing performance between various models and adaptation techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of tracking progress toward the 17 Sustainable Development Goals due to the complex data involved, highlighting the need for effective text classification models.

**Method:** The study analyzes multiple proprietary and open-source large language models (LLMs) for a single-label, multi-class text classification task focused on Sustainable Development Goals and evaluates in-context learning approaches such as Zero-Shot, Few-Shot Learning, and Fine-Tuning.

**Key Contributions:**

	1. Comparison of proprietary and open-source LLMs for SDG-focused text classification
	2. Evaluation of task adaptation techniques like Zero-Shot and Few-Shot Learning
	3. Demonstration that smaller models can match larger models through prompt optimization

**Result:** The findings suggest that smaller LLMs, when optimized with prompt engineering, can achieve performance comparable to larger models like GPT.

**Limitations:** 

**Conclusion:** Optimizing smaller models through prompt engineering is effective for text classification tasks, potentially providing cost-effective solutions for analyzing data related to the Sustainable Development Goals.

**Abstract:** In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).

</details>


### [37] [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs](https://arxiv.org/abs/2506.15211)

*Feng He, Zijun Chen, Xinnian Liang, Tingting Ma, Yunqi Qiu, Shuangzhi Wu, Junchi Yan*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, ProtoReasoning, Cross-domain generalization, Prototypical representations, Logical reasoning

**Relevance Score:** 8

**TL;DR:** The paper introduces ProtoReasoning, a framework that enhances the reasoning capabilities of Large Reasoning Models through prototypical representations, demonstrating improved performance across various reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and enhance cross-domain generalization in Large Reasoning Models, focusing on the role of shared abstract reasoning prototypes.

**Method:** ProtoReasoning utilizes an automated prototype construction pipeline, a comprehensive verification system with Prolog and PDDL, and ensures scalability for problem synthesis within prototype space.

**Key Contributions:**

	1. Introduction of ProtoReasoning framework
	2. Automated prototype construction and verification systems
	3. Demonstrated improvements in various reasoning tasks

**Result:** ProtoReasoning demonstrates a 4.7% improvement in logical reasoning, 6.3% in planning tasks, and 4.0% in general reasoning, showcasing the effectiveness of using prototypes.

**Limitations:** 

**Conclusion:** The study validates that reasoning prototypes are foundational for enhancing generalization in LLMs, outperforming traditional training on natural language representations.

**Abstract:** Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.

</details>


### [38] [MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs](https://arxiv.org/abs/2506.15215)

*Yongqi Fan, Yating Wang, Guandong Wang, Jie Zhai, Jingping Liu, Qi Ye, Tong Ruan*

**Main category:** cs.CL

**Keywords:** Open-ended Question Answering, Evaluation Metrics, Large Language Models, Semantic Similarity, Human Annotation

**Relevance Score:** 8

**TL;DR:** MinosEval is a novel evaluation method for open-ended question answering that distinguishes between factoid and non-factoid questions, using adaptive scoring for fact-based inquiries and instance-aware ranking for others, outperforming traditional methods in interpretability and alignment with human annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of open-ended question answering systems by addressing the limitations of existing metrics and methods, particularly in capturing semantic similarities and providing interpretable results.

**Method:** MinosEval distinguishes between factoid and non-factoid questions and utilizes adaptive scoring for factoid questions and instance-aware listwise ranking for non-factoid questions to evaluate candidate answers.

**Key Contributions:**

	1. Introduction of MinosEval as a tailored evaluation method for open-ended QA.
	2. Use of distinct scoring strategies for factoid and non-factoid questions.
	3. Improvement in alignment with human annotations in evaluation results.

**Result:** Experiments demonstrate that MinosEval aligns better with human annotations and provides more interpretable evaluation results compared to traditional metrics and existing LLM-based approaches.

**Limitations:** The proposed method may be limited by the datasets used for training and evaluation, and its effectiveness may vary across different QA contexts.

**Conclusion:** MinosEval offers a refined and interpretable method for evaluating open-ended question answering, improving the understanding of LLM performance on complex questions.

**Abstract:** Open-ended question answering (QA) is a key task for evaluating the capabilities of large language models (LLMs). Compared to closed-ended QA, it demands longer answer statements, more nuanced reasoning processes, and diverse expressions, making refined and interpretable automatic evaluation both crucial and challenging. Traditional metrics like ROUGE and BERTScore struggle to capture semantic similarities due to different patterns between model responses and reference answers. Current LLM-based evaluation approaches, such as pairwise or listwise comparisons of candidate answers, lack intuitive interpretability. While pointwise scoring of each response provides some descriptions, it fails to adapt across different question contents. Most notably, existing methods overlook the distinction between factoid and non-factoid questions. To address these challenges, we propose \textbf{MinosEval}, a novel evaluation method that first distinguishes open-ended questions and then ranks candidate answers using different evaluation strategies. For factoid questions, it applies an adaptive key-point scoring strategy, while for non-factoid questions, it uses an instance-aware listwise ranking strategy. Experiments on multiple open-ended QA datasets, including self-built ones with more candidate responses to complement community resources, show that MinosEval better aligns with human annotations and offers more interpretable results.

</details>


### [39] [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)

*Jaione Bengoetxea, Itziar Gonzalez-Dios, Rodrigo Agerri*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Basque, Spanish, Language Models, Linguistic Variation

**Relevance Score:** 6

**TL;DR:** This paper evaluates the performance of language technologies on Basque and Spanish, highlighting challenges due to linguistic variation with a novel dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well existing language technologies can handle linguistic variations in Basque and Spanish.

**Method:** The study employs Natural Language Inference (NLI) as a pivot task, supported by a newly curated parallel dataset focusing on variants of Basque and Spanish, and conducts experiments with both encoder-only and decoder-based LLMs.

**Key Contributions:**

	1. Introduction of a novel, manually-curated parallel dataset for Basque and Spanish variants.
	2. Empirical evidence of performance drops in LLMs with linguistic variation, especially in peripheral dialects.
	3. Public availability of data and code for further research.

**Result:** The analysis reveals a significant performance drop in handling linguistic variation, particularly in Basque, with encoder-only models struggling more with Western Basque.

**Limitations:** Focuses mainly on Basque and Spanish, limiting generalizability to other languages or dialects.

**Conclusion:** Linguistic variation presents notable challenges for language technologies, underscoring the need for further refinement of models to better handle dialectal differences.

**Abstract:** In this paper, we evaluate the capacity of current language technologies to understand Basque and Spanish language varieties. We use Natural Language Inference (NLI) as a pivot task and introduce a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Our empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models (LLMs) shows a performance drop when handling linguistic variation, especially in Basque. Error analysis suggests that this decline is not due to lexical overlap, but rather to the linguistic variation itself. Further ablation experiments indicate that encoder-only models particularly struggle with Western Basque, which aligns with linguistic theory that identifies peripheral dialects (e.g., Western) as more distant from the standard. All data and code are publicly available.

</details>


### [40] [Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs](https://arxiv.org/abs/2506.15241)

*Yang Fan, Zhang Qi, Xing Wenqian, Liu Chang, Liu Liu*

**Main category:** cs.CL

**Keywords:** large language models, historical text analysis, retrieval-augmented generation

**Relevance Score:** 4

**TL;DR:** The paper proposes the Graph RAG framework for enhancing large language models in historical text analysis, presenting a dataset with minimal manual annotation and evaluating its effectiveness in relation extraction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address domain knowledge gaps in large language models for computational humanities, specifically in historical text analysis.

**Method:** The framework combines chain-of-thought prompting, self-instruction generation, and process supervision to create a character relationship dataset with minimal manual effort, integrating knowledge graphs with retrieval-augmented generation.

**Key Contributions:**

	1. Introduction of the Graph RAG framework for historical text analysis.
	2. Creation of The First Four Histories character relationship dataset with minimal manual annotation.
	3. Demonstration of improved model performance in relation extraction using knowledge graphs.

**Result:** The domain-specific model Xunzi-Qwen1.5-14B achieved an F1 score of 0.68 in relation extraction. The DeepSeek model with GraphRAG improved F1 score by 11%, effectively reducing hallucination issues.

**Limitations:** 

**Conclusion:** The Graph RAG framework presents a low-resource solution for classical text knowledge extraction, enhancing historical knowledge services and humanities research.

**Abstract:** This article addresses domain knowledge gaps in general large language models for historical text analysis in the context of computational humanities and AIGC technology. We propose the Graph RAG framework, combining chain-of-thought prompting, self-instruction generation, and process supervision to create a The First Four Histories character relationship dataset with minimal manual annotation. This dataset supports automated historical knowledge extraction, reducing labor costs. In the graph-augmented generation phase, we introduce a collaborative mechanism between knowledge graphs and retrieval-augmented generation, improving the alignment of general models with historical knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B, with Simplified Chinese input and chain-of-thought prompting, achieves optimal performance in relation extraction (F1 = 0.68). The DeepSeek model integrated with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12), effectively alleviating hallucinations phenomenon, and improving interpretability. This framework offers a low-resource solution for classical text knowledge extraction, advancing historical knowledge services and humanities research.

</details>


### [41] [TopClustRAG at SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.15246)

*Juli Bakagianni, John Pavlopoulos, Aristidis Likas*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Question Answering, K-Means Clustering

**Relevance Score:** 8

**TL;DR:** TopClustRAG is a retrieval-augmented generation system designed for end-to-end question answering, utilizing a hybrid retrieval strategy and clustering to enhance answer quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve answer diversity, relevance, and faithfulness in retrieval-augmented generation systems for large-scale question answering.

**Method:** The system employs a hybrid retrieval strategy using sparse and dense indices, followed by K-Means clustering to group semantically similar passages, which are then used to create cluster-specific prompts for a large language model.

**Key Contributions:**

	1. Introduction of TopClustRAG for enhanced question answering
	2. Hybrid retrieval strategy combining sparse and dense indices
	3. Implementation of K-Means clustering for passage organization

**Result:** On the FineWeb Sample-10BT dataset, TopClustRAG achieved 2nd place in faithfulness and 7th in correctness on the leaderboard, indicating effective performance.

**Limitations:** 

**Conclusion:** The use of clustering-based context filtering and prompt aggregation enhances the overall performance of large-scale RAG systems.

**Abstract:** We present TopClustRAG, a retrieval-augmented generation (RAG) system developed for the LiveRAG Challenge, which evaluates end-to-end question answering over large-scale web corpora. Our system employs a hybrid retrieval strategy combining sparse and dense indices, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster are used to construct cluster-specific prompts for a large language model (LLM), generating intermediate answers that are filtered, reranked, and finally synthesized into a single, comprehensive response. This multi-stage pipeline enhances answer diversity, relevance, and faithfulness to retrieved evidence. Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in faithfulness and 7th in correctness on the official leaderboard, demonstrating the effectiveness of clustering-based context filtering and prompt aggregation in large-scale RAG systems.

</details>


### [42] [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)

*Sungen Hahm, Heejin Kim, Gyuseong Lee, Hyunji Park, Jaejin Lee*

**Main category:** cs.CL

**Keywords:** De-identification, Court judgments, Deep neural network, Data protection, Korean legal dataset

**Relevance Score:** 4

**TL;DR:** This paper presents Thunder-DeID, a de-identification framework designed for the South Korean judiciary to enhance the protection of personal data in court judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The South Korean judiciary's de-identification process is insufficient for handling court judgments at scale, requiring a solution that meets legal standards and effectively protects personal data.

**Method:** The authors developed Thunder-DeID, which includes the creation of a Korean legal dataset with annotated judgments, a systematic categorization of Personally Identifiable Information (PII), and an end-to-end deep neural network (DNN)-based de-identification pipeline.

**Key Contributions:**

	1. First Korean legal dataset with annotated court judgments
	2. Systematic categorization of PII
	3. End-to-end DNN-based de-identification pipeline

**Result:** The experiments showed that Thunder-DeID achieves state-of-the-art performance in de-identifying court judgments.

**Limitations:** The study may have limitations related to the generalizability of the model to other jurisdictions or legal systems.

**Conclusion:** The proposed framework addresses legal and technical challenges in the de-identification process, proving effective in optimizing data protection for court cases.

**Abstract:** To ensure a balance between open access to justice and personal data protection, the South Korean judiciary mandates the de-identification of court judgments before they can be publicly disclosed. However, the current de-identification process is inadequate for handling court judgments at scale while adhering to strict legal requirements. Additionally, the legal definitions and categorizations of personal identifiers are vague and not well-suited for technical solutions. To tackle these challenges, we propose a de-identification framework called Thunder-DeID, which aligns with relevant laws and practices. Specifically, we (i) construct and release the first Korean legal dataset containing annotated judgments along with corresponding lists of entity mentions, (ii) introduce a systematic categorization of Personally Identifiable Information (PII), and (iii) develop an end-to-end deep neural network (DNN)-based de-identification pipeline. Our experimental results demonstrate that our model achieves state-of-the-art performance in the de-identification of court judgments.

</details>


### [43] [Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment](https://arxiv.org/abs/2506.15301)

*Shrestha Ghosh, Moritz Schneider, Carina Reinicke, Carsten Eickhoff*

**Main category:** cs.CL

**Keywords:** clinical trial recruitment, LLM, trial-patient matching, NLP, evaluation frameworks

**Relevance Score:** 9

**TL;DR:** This survey analyzes the task of trial-patient matching in clinical trial recruitment and contextualizes LLM-based methods in this domain.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited adoption of LLMs in critical domains like clinical trial recruitment despite their advancement in general-domain NLP tasks, and to explore the potential benefits of LLMs in matching trials and patients.

**Method:** The paper conducts a survey of current LLM-assisted approaches to trial-patient matching, evaluates existing benchmarks and frameworks, and discusses the challenges and future directions for LLM adoption in clinical research.

**Key Contributions:**

	1. First comprehensive analysis of trial-patient matching using LLMs
	2. Critical examination of existing benchmarks and approaches
	3. Identification of challenges and future directions for LLM technologies in clinical research

**Result:** The analysis reveals that current applications often rely on proprietary models and lack robust evaluation benchmarks, highlighting the need for improved methodologies in LLM-assisted clinical trial recruitment.

**Limitations:** Limited exploration of proprietary models and existing frameworks without rigorous evaluation metrics.

**Conclusion:** The survey identifies significant gaps in current approaches and suggests necessary advancements in evaluation frameworks and methodologies, paving the way for more effective use of LLMs in clinical trials.

**Abstract:** Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.

</details>


### [44] [ConLID: Supervised Contrastive Learning for Low-Resource Language Identification](https://arxiv.org/abs/2506.15304)

*Negar Foroutan, Jakhongir Saydaliev, Ye Eun Kim, Antoine Bosselut*

**Main category:** cs.CL

**Keywords:** language identification, low-resource languages, supervised contrastive learning, domain-invariant representations, multilingual LLM

**Relevance Score:** 8

**TL;DR:** This paper presents a supervised contrastive learning approach to improve language identification performance for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the poor performance of language identification models on low-resource languages, which are often constrained by limited and single-domain data.

**Method:** The authors propose a novel approach based on supervised contrastive learning to learn domain-invariant representations for low-resource languages, aiming to resolve class imbalance and bias issues.

**Key Contributions:**

	1. Introduction of a novel supervised contrastive learning approach for low-resource language identification.
	2. Analysis showing a 3.2% improvement in performance on out-of-domain data.
	3. Focus on addressing class imbalance and bias in language identification training.

**Result:** The proposed method improves language identification performance on out-of-domain data for low-resource languages by 3.2%.

**Limitations:** 

**Conclusion:** The findings demonstrate the effectiveness of the supervised contrastive learning approach in enhancing language identification models for low-resource settings.

**Abstract:** Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.

</details>


### [45] [DeVisE: Behavioral Testing of Medical Large Language Models](https://arxiv.org/abs/2506.15339)

*Camila Zurdo Tagliabue, Heloisa Oss Boll, Aykut Erdem, Erkut Erdem, Iacer Calixto*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Decision Support, Behavioral Testing, Fairness in AI, Medical AI

**Relevance Score:** 9

**TL;DR:** Introducing DeVisE, a framework for evaluating the clinical reasoning of large language models in decision support using a dataset of ICU discharge notes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of current evaluation methods that fail to discern true medical reasoning in LLMs.

**Method:** Constructing a dataset of ICU discharge notes and evaluating five LLMs under zero-shot and fine-tuned settings while assessing input-level sensitivity and downstream reasoning.

**Key Contributions:**

	1. Introduction of DeVisE framework for evaluating clinical understanding in LLMs.
	2. Creation of a comprehensive dataset of ICU discharge notes with demographic and vital sign counterfactuals.
	3. Highlighting the impacts of demographic factors on LLM outputs and advocating for fairness-aware evaluations.

**Result:** Zero-shot models demonstrated coherent counterfactual reasoning, while fine-tuned models were stable but less responsive to changes influenced by demographic factors.

**Limitations:** The study's findings may depend on the quality and diversity of the ICU discharge notes used in the dataset.

**Conclusion:** Behavioral testing is essential for understanding clinical LLM reasoning and enhancing the safety and transparency of medical AI systems.

**Abstract:** Large language models (LLMs) are increasingly used in clinical decision support, yet current evaluation methods often fail to distinguish genuine medical reasoning from superficial patterns. We introduce DeVisE (Demographics and Vital signs Evaluation), a behavioral testing framework for probing fine-grained clinical understanding. We construct a dataset of ICU discharge notes from MIMIC-IV, generating both raw (real-world) and template-based (synthetic) versions with controlled single-variable counterfactuals targeting demographic (age, gender, ethnicity) and vital sign attributes. We evaluate five LLMs spanning general-purpose and medically fine-tuned variants, under both zero-shot and fine-tuned settings. We assess model behavior via (1) input-level sensitivity - how counterfactuals alter the likelihood of a note; and (2) downstream reasoning - how they affect predicted hospital length-of-stay. Our results show that zero-shot models exhibit more coherent counterfactual reasoning patterns, while fine-tuned models tend to be more stable yet less responsive to clinically meaningful changes. Notably, demographic factors subtly but consistently influence outputs, emphasizing the importance of fairness-aware evaluation. This work highlights the utility of behavioral testing in exposing the reasoning strategies of clinical LLMs and informing the design of safer, more transparent medical AI systems.

</details>


### [46] [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)

*Arijit Maji, Raghvendra Kumar, Akash Ghosh, Anushka, Sriparna Saha*

**Main category:** cs.CL

**Keywords:** Cultural Understanding, Language Models, Benchmark Dataset, Indian Culture, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** SANSKRITI is a benchmark dataset for evaluating language models' understanding of India's diverse culture, featuring 21,853 question-answer pairs across significant cultural attributes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance language models' performance by understanding local socio-cultural contexts, particularly in India.

**Method:** The study introduces the SANSKRITI dataset and evaluates it on various language models including LLMs, ILMs, and SLMs, assessing their understanding of culturally nuanced queries.

**Key Contributions:**

	1. Introduction of the SANSKRITI benchmark dataset for Indian cultural knowledge
	2. Evaluation of leading language models against this dataset
	3. Highlighting the disparities in model performance based on cultural contexts.

**Result:** The evaluation uncovers significant variations in the performance of different language models when tasked with region-specific cultural questions.

**Limitations:** 

**Conclusion:** SANSKRITI provides a comprehensive dataset that can help improve the cultural understanding of language models.

**Abstract:** Language Models (LMs) are indispensable tools shaping modern workflows, but their global effectiveness depends on understanding local socio-cultural contexts. To address this, we introduce SANSKRITI, a benchmark designed to evaluate language models' comprehension of India's rich cultural diversity. Comprising 21,853 meticulously curated question-answer pairs spanning 28 states and 8 union territories, SANSKRITI is the largest dataset for testing Indian cultural knowledge. It covers sixteen key attributes of Indian culture: rituals and ceremonies, history, tourism, cuisine, dance and music, costume, language, art, festivals, religion, medicine, transport, sports, nightlife, and personalities, providing a comprehensive representation of India's cultural tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic Language Models (ILMs), and Small Language Models (SLMs), revealing significant disparities in their ability to handle culturally nuanced queries, with many models struggling in region-specific contexts. By offering an extensive, culturally rich, and diverse dataset, SANSKRITI sets a new standard for assessing and improving the cultural understanding of LMs.

</details>


### [47] [COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation](https://arxiv.org/abs/2506.15372)

*Raghvendra Kumar, S. A. Mohammed Salman, Aryan Sahu, Tridib Nandi, Pragathi Y. P., Sriparna Saha, Jose G. Moreno*

**Main category:** cs.CL

**Keywords:** multimodal summarization, multilingual dataset, Indian languages, natural language generation, reader comments

**Relevance Score:** 7

**TL;DR:** COSMMIC is a dataset for multimodal and multilingual summarization in Indian languages, integrating text, images, and user comments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources for comment-aware multimodal summarization in Indian languages.

**Method:** The study introduces a dataset comprising 4,959 article-image pairs and 24,484 reader comments, assessing various configurations for summarization using state-of-the-art language models.

**Key Contributions:**

	1. Introduction of the COSMMIC dataset for nine Indian languages
	2. Integration of multimodal data (text, images, comments) for summarization
	3. Evaluation of summarization configurations using advanced language models

**Result:** The dataset effectively enhances summarization by incorporating reader insights, with a comprehensive evaluation of different configurations showing efficacy in natural language generation tasks.

**Limitations:** 

**Conclusion:** COSMMIC advances NLP resources for Indian languages by combining text, images, and user feedback, promoting inclusivity in research.

**Abstract:** Despite progress in comment-aware multimodal and multilingual summarization for English and Chinese, research in Indian languages remains limited. This study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC comprises 4,959 article-image pairs and 24,484 reader comments, with ground-truth summaries available in all included languages. Our approach enhances summaries by integrating reader insights and feedback. We explore summarization and headline generation across four configurations: (1) using article text alone, (2) incorporating user comments, (3) utilizing images, and (4) combining text, comments, and images. To assess the dataset's effectiveness, we employ state-of-the-art language models such as LLama3 and GPT-4. We conduct a comprehensive study to evaluate different component combinations, including identifying supportive comments, filtering out noise using a dedicated comment classifier using IndicBERT, and extracting valuable insights from images with a multilingual CLIP-based classifier. This helps determine the most effective configurations for natural language generation (NLG) tasks. Unlike many existing datasets that are either text-only or lack user comments in multimodal settings, COSMMIC uniquely integrates text, images, and user feedback. This holistic approach bridges gaps in Indian language resources, advancing NLP research and fostering inclusivity.

</details>


### [48] [Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning](https://arxiv.org/abs/2506.15415)

*Stanley Ngugi*

**Main category:** cs.CL

**Keywords:** Large Language Models, lexical alignment, fine-tuning, low-resource languages, contrastive learning

**Relevance Score:** 7

**TL;DR:** This paper introduces Targeted Lexical Injection (TLI), a fine-tuning approach to improve lexical alignment in Low-Resource Languages (LRLs) like Swahili. It leverages insights from early layers of the Lugha-Llama-8B-wura model to enhance performance in cross-lingual tasks.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often underperform in low-resource languages due to data scarcity, which affects tasks like translation and cross-lingual information retrieval. This research addresses the challenge of improving lexical alignment in LRLs.

**Method:** The paper proposes Targeted Lexical Injection (TLI), utilizing Low-Rank Adaptation (LoRA) and contrastive learning to fine-tune the model, specifically targeting early-layer embeddings that display strong lexical alignment.

**Key Contributions:**

	1. Introduction of Targeted Lexical Injection (TLI) as a novel fine-tuning method.
	2. Demonstration of improved lexical alignment using early-layer embeddings in LLMs.
	3. Evidence of strong generalization to unseen word pairs.

**Result:** TLI significantly improved lexical alignment performance, with cosine similarity for 623 Swahili-English word pairs increasing from 0.3211 to 0.4113 (+28.08%) and for 63 unseen control pairs from 0.3143 to 0.4033 (+28.32%).

**Limitations:** 

**Conclusion:** TLI effectively enhances the model's ability to maintain and propagate early-layer cross-lingual knowledge, marking a significant advancement in the fine-tuning of LRL-focused LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their performance in low-resource languages (LRLs), such as Swahili, often lags due to data scarcity and underrepresentation in pre-training. A key challenge is achieving robust cross-lingual lexical alignment, crucial for tasks like translation and cross-lingual information retrieval. This paper introduces Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach. We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits strong, near-perfect lexical alignment for Swahili-English word pairs in its early internal layers (specifically Layer 2, with ~0.99998 average cosine similarity based on a pilot study), a capability not fully reflected in its final output representations (baseline ~0.32 similarity on our evaluation set). TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a contrastive learning objective to fine-tune the model, specifically targeting embeddings from this empirically identified optimal early layer. Our experiments show that TLI significantly improves the output-level lexical alignment for 623 trained Swahili-English word pairs, increasing average cosine similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More importantly, these improvements generalize remarkably well to 63 unseen control word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17 x 10^-27). These findings suggest TLI enhances the model's ability to preserve and propagate its inherent early-layer cross-lingual knowledge, offering a parameter-efficient and effective strategy for improving lexical alignment in LRL-focused LLMs.

</details>


### [49] [Understanding GUI Agent Localization Biases through Logit Sharpness](https://arxiv.org/abs/2506.15425)

*Xingjian Tao, Yiwei Wang, Yujun Cai, Zhicheng Yang, Jing Tang*

**Main category:** cs.CL

**Keywords:** Multimodal large language models, GUI agents, localization errors, model uncertainty, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper presents a framework for evaluating multimodal large language models (MLLMs) in GUI interactions, focusing on reducing localization errors and improving model performance through novel metrics and techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unreliability of MLLMs in GUI interactions due to systematic localization errors and to enhance the interpretability and robustness of GUI agent behavior.

**Method:** The paper proposes a fine-grained evaluation framework that categorizes model predictions into four types and introduces the Peak Sharpness Score (PSS) to quantify model uncertainty. Additionally, it presents the Context-Aware Cropping technique to refine input context without additional training.

**Key Contributions:**

	1. Introduction of a fine-grained evaluation framework for MLLMs in GUI tasks.
	2. Development of the Peak Sharpness Score (PSS) for measuring model uncertainty.
	3. Proposal of Context-Aware Cropping as a training-free performance enhancement technique.

**Result:** Experiments demonstrate that the proposed framework and techniques improve model performance and provide insightful analysis of model behavior.

**Limitations:** 

**Conclusion:** The study highlights the importance of nuanced evaluation methods and training-free techniques to enhance the performance of MLLMs for GUI agents.

**Abstract:** Multimodal large language models (MLLMs) have enabled GUI agents to interact with operating systems by grounding language into spatial actions. Despite their promising performance, these models frequently exhibit hallucinations-systematic localization errors that compromise reliability. We propose a fine-grained evaluation framework that categorizes model predictions into four distinct types, revealing nuanced failure modes beyond traditional accuracy metrics. To better quantify model uncertainty, we introduce the Peak Sharpness Score (PSS), a metric that evaluates the alignment between semantic continuity and logits distribution in coordinate prediction. Building on this insight, we further propose Context-Aware Cropping, a training-free technique that improves model performance by adaptively refining input context. Extensive experiments demonstrate that our framework and methods provide actionable insights and enhance the interpretability and robustness of GUI agent behavior.

</details>


### [50] [PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice](https://arxiv.org/abs/2503.01903)

*Shuyu Liu, Ruoxi Wang, Ling Zhang, Xuequan Zhu, Rui Yang, Xinzhu Zhou, Fei Wu, Zhi Yang, Cheng Jin, Gang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Psychiatry, Benchmarking, Clinical Evaluation, Auxiliary Tools

**Relevance Score:** 9

**TL;DR:** This paper presents PsychBench, a benchmarking framework to evaluate the performance of Large Language Models (LLMs) in psychiatric clinical settings, addressing the need for tailored tools in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the shortage of medical resources and low diagnostic consistency in psychiatric practice by introducing a benchmarking framework for evaluating LLMs in this context.

**Method:** A comprehensive quantitative evaluation of 16 LLMs was conducted using PsychBench, focusing on factors such as prompt design, reasoning, input length, and domain-specific fine-tuning. Additionally, a clinical study with 60 psychiatrists assessed the practical benefits of these models.

**Key Contributions:**

	1. Introduction of PsychBench as a benchmarking framework for LLMs in psychiatry.
	2. Quantitative evaluation of 16 LLMs, analyzing impacts of prompt design and model tuning.
	3. Findings indicate substantial support for junior psychiatrists, but limitations for seasoned clinicians.

**Result:** The evaluation revealed that while existing LLMs show significant potential as auxiliary tools, they currently lack adequacy as decision-making aids in psychiatric practice, particularly for seasoned professionals compared to junior psychiatrists.

**Limitations:** Existing models are not yet adequate as decision-making tools; further improvements and tailored versions are necessary.

**Conclusion:** The dataset and evaluation framework will be made publicly available to foster research on the application of LLMs in psychiatric settings, highlighting the need for improved models in clinical environments.

**Abstract:** The advent of Large Language Models (LLMs) offers potential solutions to address problems such as shortage of medical resources and low diagnostic consistency in psychiatric clinical practice. Despite this potential, a robust and comprehensive benchmarking framework to assess the efficacy of LLMs in authentic psychiatric clinical environments is absent. This has impeded the advancement of specialized LLMs tailored to psychiatric applications. In response to this gap, by incorporating clinical demands in psychiatry and clinical data, we proposed a benchmarking system, PsychBench, to evaluate the practical performance of LLMs in psychiatric clinical settings. We conducted a comprehensive quantitative evaluation of 16 LLMs using PsychBench, and investigated the impact of prompt design, chain-of-thought reasoning, input text length, and domain-specific knowledge fine-tuning on model performance. Through detailed error analysis, we identified strengths and potential limitations of the existing models and suggested directions for improvement. Subsequently, a clinical reader study involving 60 psychiatrists of varying seniority was conducted to further explore the practical benefits of existing LLMs as supportive tools for psychiatrists of varying seniority. Through the quantitative and reader evaluation, we show that while existing models demonstrate significant potential, they are not yet adequate as decision-making tools in psychiatric clinical practice. The reader study further indicates that, as an auxiliary tool, LLM could provide particularly notable support for junior psychiatrists, effectively enhancing their work efficiency and overall clinical quality. To promote research in this area, we will make the dataset and evaluation framework publicly available, with the hope of advancing the application of LLMs in psychiatric clinical settings.

</details>


### [51] [AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need](https://arxiv.org/abs/2506.15451)

*Zhouhong Gu, Xiaoxuan Zhu, Yin Cai, Hao Shen, Xingzhou Chen, Qingyi Wang, Jialin Li, Xiaoran Shi, Haoran Guo, Wenxuan Huang, Hongwei Feng, Yanghua Xiao, Zheyu Ye, Yao Hu, Shaosheng Cao*

**Main category:** cs.CL

**Keywords:** large language models, multi-agent systems, task resolution, parallel architecture, adaptive collaboration

**Relevance Score:** 8

**TL;DR:** AgentGroupChat-V2 is a novel framework for large language model multi-agent systems, addressing challenges in architecture design and task resolution with core innovations in parallel processing and collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve system architecture and performance of multi-agent systems using large language models for complex tasks.

**Method:** Introduces a fully parallel architecture, an adaptive collaboration engine for heterogeneous LLMs, and optimization strategies for agent organization.

**Key Contributions:**

	1. Divide-and-conquer fully parallel architecture for user query decomposition.
	2. Adaptive collaboration engine for dynamic LLM selection.
	3. Agent organization optimization for efficient problem decomposition.

**Result:** AgentGroupChat-V2 achieves superior performance with 91.50% accuracy on GSM8K, 30.4% on AIME, and 79.20% on HumanEval, particularly excelling in difficult tasks.

**Limitations:** 

**Conclusion:** AgentGroupChat-V2 offers a comprehensive, efficient solution for multi-agent systems capable of handling complex reasoning tasks more effectively than existing methods.

**Abstract:** Large language model based multi-agent systems have demonstrated significant potential in social simulation and complex task resolution domains. However, current frameworks face critical challenges in system architecture design, cross-domain generalizability, and performance guarantees, particularly as task complexity and number of agents increases. We introduces AgentGroupChat-V2, a novel framework addressing these challenges through three core innovations: (1) a divide-and-conquer fully parallel architecture that decomposes user queries into hierarchical task forest structures enabling dependency management and distributed concurrent processing. (2) an adaptive collaboration engine that dynamically selects heterogeneous LLM combinations and interaction modes based on task characteristics. (3) agent organization optimization strategies combining divide-and-conquer approaches for efficient problem decomposition. Extensive experiments demonstrate AgentGroupChat-V2's superior performance across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME (nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance advantages become increasingly pronounced with higher task difficulty, particularly on Level 5 MATH problems where improvements exceed 11 percentage points compared to state-of-the-art baselines. These results confirm that AgentGroupChat-V2 provides a comprehensive solution for building efficient, general-purpose LLM multi-agent systems with significant advantages in complex reasoning scenarios. Code is available at https://github.com/MikeGu721/AgentGroupChat-V2.

</details>


### [52] [RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation](https://arxiv.org/abs/2506.15455)

*Xinnuo Xu, Rachel Lawrence, Kshitij Dubey, Atharva Pandey, Risa Ueno, Fabian Falck, Aditya V. Nori, Rahul Sharma, Amit Sharma, Javier Gonzalez*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Framework, Problem Variations, Statistical Recall

**Relevance Score:** 9

**TL;DR:** This paper introduces RE-IMAGINE, a framework for evaluating reasoning abilities in Large Language Models by generating problem variations that cannot be solved through memorization alone.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify whether observed high accuracy of LLMs on reasoning benchmarks arises from true reasoning skills or statistical recall of their training data.

**Method:** The RE-IMAGINE framework creates a hierarchy of reasoning abilities based on the ladder of causation and generates problem variations within this hierarchy using an intermediate symbolic representation.

**Key Contributions:**

	1. Introduction of the RE-IMAGINE framework for reasoning evaluation
	2. Generation of problem variations that require genuine reasoning beyond memorization
	3. Demonstration of performance reductions in LLMs when faced with generated problems

**Result:** The framework was tested on four popular benchmarks, revealing a decrease in performance when models were faced with generated problem variations, suggesting reliance on statistical recall rather than genuine reasoning.

**Limitations:** The approach relies on specific problem variations and does not explore all reasoning domains exhaustively.

**Conclusion:** RE-IMAGINE highlights the limitations of LLMs in reasoning tasks and sets the stage for further research into developing reasoning skills across different domains.

**Abstract:** Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.

</details>


### [53] [Context-Informed Grounding Supervision](https://arxiv.org/abs/2506.15480)

*Hyunji Lee, Seunghyun Yoon, Yunjae Won, Hanseok Oh, Geewook Kim, Trung Bui, Franck Dernoncourt, Elias Stengel-Eskin, Mohit Bansal, Minjoon Seo*

**Main category:** cs.CL

**Keywords:** Language Models, Grounding, Supervision, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper proposes Context-INformed Grounding Supervision (CINGS) to enhance grounded generation in large language models by training them with relevant context prepended to responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the grounding ability of large language models (LLMs) when using external knowledge, addressing limitations seen in prior methods where context is simply appended at inference time.

**Method:** CINGS involves post-training supervision where relevant context is prepended to the response while computing the loss only over the response tokens, effectively masking out the context.

**Key Contributions:**

	1. Introduction of CINGS for improved model grounding
	2. Demonstration of enhanced performance across 11 information-seeking datasets
	3. Reduction of hallucinations in vision-language models with factual consistency

**Result:** Models trained with CINGS show significantly improved grounding in both textual and visual domains compared to standard models, with better performance across diverse benchmarks and reduced hallucinations.

**Limitations:** 

**Conclusion:** CINGS enhances the model's reliance on external context without negatively impacting general performance, indicating a shift in knowledge and behavior that promotes factual consistency.

**Abstract:** Large language models (LLMs) are often supplemented with external knowledge to provide information not encoded in their parameters or to reduce hallucination. In such cases, we expect the model to generate responses by grounding its response in the provided external context. However, prior work has shown that simply appending context at inference time does not ensure grounded generation. To address this, we propose Context-INformed Grounding Supervision (CINGS), a post-training supervision in which the model is trained with relevant context prepended to the response, while computing the loss only over the response tokens and masking out the context. Our experiments demonstrate that models trained with CINGS exhibit stronger grounding in both textual and visual domains compared to standard instruction-tuned models. In the text domain, CINGS outperforms other training methods across 11 information-seeking datasets and is complementary to inference-time grounding techniques. In the vision-language domain, replacing a vision-language model's LLM backbone with a CINGS-trained model reduces hallucinations across four benchmarks and maintains factual consistency throughout the generated response. This improved grounding comes without degradation in general downstream performance. Finally, we analyze the mechanism underlying the enhanced grounding in CINGS and find that it induces a shift in the model's prior knowledge and behavior, implicitly encouraging greater reliance on the external context.

</details>


### [54] [SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling](https://arxiv.org/abs/2506.15498)

*Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Large Language Models, process supervision, annotation efficiency, reasoning performance, AI applications

**Relevance Score:** 8

**TL;DR:** The paper introduces SPARE, a framework for efficient process annotation in LLMs, improving reasoning performance and efficiency in multi-step reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Automated process annotation is essential for advancing the reasoning capabilities of LLMs but remains challenging.

**Method:** SPARE employs a single-pass annotation process that aligns solution steps to a reference solution while providing explicit reasoning for evaluation.

**Key Contributions:**

	1. Introduction of SPARE for single-pass annotation
	2. Improved reasoning performance across multiple domains
	3. Enhanced efficiency compared to existing methods

**Result:** SPARE shows improved reasoning performance across four datasets in mathematical reasoning, compositional question answering, and spatial reasoning, achieving competitive results with significantly lower runtime.

**Limitations:** 

**Conclusion:** SPARE enhances the efficiency of process supervision for LLMs, providing a practical solution to the annotation challenge and is made available for further research.

**Abstract:** Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables single-pass, per-step annotation by aligning each solution step to one or multiple steps in a reference solution, accompanied by explicit reasoning for evaluation. We show that reference-guided step-level evaluation effectively facilitates process supervision on four datasets spanning three domains: mathematical reasoning, multi-hop compositional question answering, and spatial reasoning. We demonstrate that SPARE, when compared to baselines, improves reasoning performance when used for: (1) fine-tuning models in an offline RL setup for inference-time greedy-decoding, and (2) training reward models for ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE achieves competitive performance on challenging mathematical datasets while offering 2.6 times greater efficiency, requiring only 38% of the runtime, compared to tree search-based automatic annotation. The codebase, along with a trained SPARE-PRM model, is publicly released to facilitate further research and reproducibility.

</details>


### [55] [Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge](https://arxiv.org/abs/2506.15504)

*Li Zheng, Sihang Wang, Hao Fei, Zuquan Peng, Fei Li, Jianming Fu, Chong Teng, Donghong Ji*

**Main category:** cs.CL

**Keywords:** hyperbole detection, metaphor detection, emotion analysis, natural language processing, NLP

**Relevance Score:** 7

**TL;DR:** The paper presents an emotion-guided framework, EmoBi, for detecting hyperbole and metaphor by leveraging emotion analysis and dynamic interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying hyperbole and metaphor poses challenges due to their semantic obscurity and the neglect of implicit emotions in existing detection methods.

**Method:** The EmoBi framework includes an emotion analysis module to mine emotion connotations, a domain mapping module for target and source understanding, and a dynamic interaction module to enhance detection of rhetorical devices.

**Key Contributions:**

	1. Emotion-guided framework for hyperbole and metaphor detection
	2. Deep analysis of emotion connotations
	3. Bidirectional dynamic interaction for mutual promotion between rhetorical devices

**Result:** EmoBi significantly outperforms baseline methods, achieving an F1 score increase of 28.1% for hyperbole detection and 23.1% for metaphor detection compared to the state of the art.

**Limitations:** 

**Conclusion:** The EmoBi framework demonstrates effectiveness in improving the detection of hyperbole and metaphor, showcasing potential advancements in NLP applications.

**Abstract:** Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.

</details>


### [56] [Lessons from Training Grounded LLMs with Verifiable Rewards](https://arxiv.org/abs/2506.15522)

*Shang Hong Sim, Tej Deep Pala, Vernon Toh, Hai Leong Chieu, Amir Zadeh, Chuan Li, Navonil Majumder, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, grounding, citation, reasoning

**Relevance Score:** 9

**TL;DR:** This study explores how reinforcement learning and internal reasoning can enhance the grounding of large language models (LLMs) through a method called GRPO, leading to significant improvements in generating well-cited responses and handling unanswerable queries.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Generating grounded and trustworthy responses in large language models remains a challenging task, with existing models often failing to provide correct citations and accurate answers.

**Method:** The authors employ the Group Relative Policy Optimization (GRPO) method to train LLMs with verifiable outcome-based rewards focusing on answer correctness, citation sufficiency, and refusal quality, without needing extensive annotations.

**Key Contributions:**

	1. Introduction of GRPO for training LLMs
	2. Demonstration of stage-wise optimization improving grounding
	3. Combining instruction tuning with GRPO to enhance QA tasks

**Result:** Experiments show that reasoning-augmented models outperform instruction-only variants, particularly in unanswerable queries and citation generation. The two-stage training method enhances grounding by stabilizing the learning signal, and combining GRPO with instruction tuning improves performance on long-form QA tasks.

**Limitations:** 

**Conclusion:** The study concludes that reinforcement learning, internal reasoning, and an outcome-driven training approach are effective in building more reliable and verifiable LLMs.

**Abstract:** Generating grounded and trustworthy responses remains a key challenge for large language models (LLMs). While retrieval-augmented generation (RAG) with citation-based grounding holds promise, instruction-tuned models frequently fail even in straightforward scenarios: missing explicitly stated answers, citing incorrectly, or refusing when evidence is available. In this work, we explore how reinforcement learning (RL) and internal reasoning can enhance grounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method to train models using verifiable outcome-based rewards targeting answer correctness, citation sufficiency, and refusal quality, without requiring gold reasoning traces or expensive annotations. Through comprehensive experiments across ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented models significantly outperform instruction-only variants, especially in handling unanswerable queries and generating well-cited responses. A two-stage training setup, first optimizing answer and citation behavior and then refusal, further improves grounding by stabilizing the learning signal. Additionally, we revisit instruction tuning via GPT-4 distillation and find that combining it with GRPO enhances performance on long-form, generative QA tasks. Overall, our findings highlight the value of reasoning, stage-wise optimization, and outcome-driven RL for building more verifiable and reliable LLMs.

</details>


### [57] [RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models](https://arxiv.org/abs/2506.15545)

*Bailin Wang, Chang Lan, Chong Wang, Ruoming Pang*

**Main category:** cs.CL

**Keywords:** local-global attention, RATTENTION, efficiency, performance, Transformers

**Relevance Score:** 8

**TL;DR:** This paper introduces RATTENTION, a new local-global attention model that improves efficiency without sacrificing performance by successfully handling out-of-window tokens and optimizing window size.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of local attention by enhancing its performance in short-context scenarios while maintaining efficiency.

**Method:** The authors propose RATTENTION, which incorporates a specialized linear attention mechanism to capture information from tokens outside the defined window and perform pretraining experiments.

**Key Contributions:**

	1. Introduction of RATTENTION model for local-global attention
	2. Demonstration of improved efficiency and performance with smaller window sizes
	3. Validation of model performance on the RULER benchmark

**Result:** RATTENTION achieves a superior performance-efficiency tradeoff, matching full-attention model performance with a significantly smaller window size (512) and maintaining training speeds comparable to existing methods.

**Limitations:** 

**Conclusion:** The proposed RATTENTION model successfully balances the tradeoff between performance and efficiency, especially in scenarios with shorter contexts, making it a promising alternative to traditional attention mechanisms.

**Abstract:** Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches.

</details>


### [58] [Approximating Language Model Training Data from Weights](https://arxiv.org/abs/2506.15553)

*John X. Morris, Junjie Oscar Yin, Woojeong Kim, Vitaly Shmatikov, Alexander M. Rush*

**Main category:** cs.CL

**Keywords:** language models, data approximation, machine learning, fine-tuning, gradient-based methods

**Relevance Score:** 8

**TL;DR:** The paper proposes a gradient-based method for approximating training data from existing language models using their weights, enhancing model performance significantly.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of recovering training data for language models, which often have open weights but closed training datasets.

**Method:** A gradient-based approach that identifies the highest-matching data from a large public text corpus is developed to approximate original training data.

**Key Contributions:**

	1. Introduces a method for data approximation from language model weights.
	2. Demonstrates significant performance improvements in classification tasks and perplexity measurements.
	3. Establishes baseline metrics for evaluating data approximation techniques.

**Result:** The proposed method improved classification performance on the AG News task from 65% to 80% and decreased perplexity on MSMARCO data from 3.3 to 2.3.

**Limitations:** 

**Conclusion:** The method shows potential in recovering effective training data to enhance model performance, even without access to the original dataset.

**Abstract:** Modern language models often have open weights but closed training data. We formalize the problem of data approximation from model weights and propose several baselines and metrics. We develop a gradient-based approach that selects the highest-matching data from a large public text corpus and show its effectiveness at recovering useful data given only weights of the original and finetuned models. Even when none of the true training data is known, our method is able to locate a small subset of public Web documents can be used to train a model to close to the original model performance given models trained for both classification and supervised-finetuning. On the AG News classification task, our method improves performance from 65% (using randomly selected data) to 80%, approaching the expert benchmark of 88%. When applied to a model trained with SFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3, compared to an expert LLAMA model's perplexity of 2.0.

</details>


### [59] [PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction](https://arxiv.org/abs/2506.15556)

*Shufan Li, Aditya Grover*

**Main category:** cs.CL

**Keywords:** Large Language Models, Voice Chat Applications, Predictive Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces Predictive Generation (PredGen), a framework to reduce latency in LLM-driven voice chat applications by generating responses while the user is speaking.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs combined with TTS systems face latency issues in voice chat applications, which negatively impacts user experience, especially on consumer-grade hardware.

**Method:** PredGen employs speculative decoding to generate candidate responses during user input, allowing TTS to start processing with minimal delay.

**Key Contributions:**

	1. Introduction of the PredGen framework for latency reduction in voice applications
	2. Demonstration of 2x latency improvement in simulated experiments
	3. Minimization of additional computational costs during response generation.

**Result:** PredGen reduces latency by around 2x with minimal additional computational cost in diverse scenarios according to simulated experiments.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances the responsiveness of voice assistants by addressing the latency in generating audio responses from LLMs.

**Abstract:** Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end of user input and the start of audio output, resulting in suboptimal user experiences. This latency is particularly evident when LLMs are deployed as single-user voice assistants on consumer-grade hardware with limited computing capacity. We discovered that this latency is primarily dominated by the time it takes for the LLMs to generate the first sentence, which is required as input by the TTS systems that synthesize audio responses on a sentence-by-sentence basis. To address this bottleneck, we propose Predictive Generation (PredGen), a novel framework that mitigates-or even eliminates-this delay through speculative decoding at input time. PredGen generates candidate responses while the user is still speaking, enabling the system to begin TTS processing with minimal delay. Simulated experiments on the Lmsys and MT-Bench datasets show that the proposed method can effectively reduce the latency by around 2x across a wide range of use cases, while incurring only minimal additional computation cost at input time-computation that would otherwise go unused.

</details>


### [60] [Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models](https://arxiv.org/abs/2506.15568)

*Zhengyang Shan, Emily Ruth Diana, Jiawei Zhou*

**Main category:** cs.CL

**Keywords:** gender fairness, large language models, gender inclusivity, bias evaluation, generative models

**Relevance Score:** 9

**TL;DR:** This paper evaluates gender fairness in large language models (LLMs) using a new metric called the Gender Inclusivity Fairness Index (GIFI) that assesses inclusivity for both binary and non-binary genders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous studies that primarily focus on binary gender distinctions and to improve the inclusivity of LLMs.

**Method:** The study introduces the Gender Inclusivity Fairness Index (GIFI), which includes various evaluations such as probing models with gender pronouns and assessing their generation and cognitive behaviors related to gender.

**Key Contributions:**

	1. Introduction of the Gender Inclusivity Fairness Index (GIFI) for evaluating gender inclusivity in LLMs
	2. Comprehensive assessments on 22 LLMs revealing significant bias variations
	3. Establishment of a benchmark for future advancements in gender fairness in AI models

**Result:** Extensive evaluations using GIFI on 22 LLMs showed significant variations in gender inclusivity among the models, highlighting the need for advancements in this area.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of enhancing inclusivity in LLMs and establish a benchmark for future work in gender fairness in generative models.

**Abstract:** We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers. We conduct extensive evaluations with GIFI on 22 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs' gender inclusivity. Our study highlights the importance of improving LLMs' inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.

</details>


### [61] [SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.15569)

*Chengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan, Yilun Zhao*

**Main category:** cs.CL

**Keywords:** multimodal, foundation models, scientific claim verification, benchmark, RAG

**Relevance Score:** 8

**TL;DR:** Introduction of SciVer, a benchmark for evaluating foundation models in multimodal scientific claim verification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for a specific benchmark to assess foundation models' capabilities in multimodal scientific contexts.

**Method:** Creation of SciVer with 3,000 expert-annotated examples spanning four reasoning types, evaluating 21 state-of-the-art multimodal foundation models.

**Key Contributions:**

	1. Introduction of a new benchmark (SciVer) for evaluating models in scientific contexts
	2. Detailed performance assessment of multimodal foundation models
	3. Analysis of critical limitations and error evaluation in existing models

**Result:** Found a significant performance gap between the evaluated models and human experts in multimodal scientific claim verification.

**Limitations:** The study reveals substantial performance gaps without presenting solutions for the identified limitations.

**Conclusion:** Identification of critical limitations in current models, providing insights to improve comprehension and reasoning in scientific literature tasks.

**Abstract:** We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.

</details>


### [62] [DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement](https://arxiv.org/abs/2506.15583)

*Shaoqing Lin, Chong Teng, Fei Li, Donghong Ji, Lizhen Qu, Zhuang Li*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, scene graph parsing, multi-sentence captions, DiscoSG-DS, DiscoSG-Refiner

**Relevance Score:** 6

**TL;DR:** This paper introduces a new task, Discourse-level text Scene Graph parsing (DiscoSG), aimed at improving the performance of Vision-Language Models by refining the way multi-sentence visual descriptions are parsed into scene graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current scene graph parsers struggle with discourse-level multi-sentence visual descriptions, leading to poor performance on downstream Vision-Language Model tasks due to issues like cross-sentence coreference.

**Method:** The authors present DiscoSG, a dataset and task that pairs multi-sentence captions with enhanced scene graphs. They propose a two-step graph generation method called DiscoSG-Refiner, which utilizes two Flan-T5-Base models to improve graph parsing efficiency.

**Key Contributions:**

	1. Introduction of DiscoSG dataset and task for discourse-level scene graph parsing.
	2. Development of DiscoSG-Refiner for efficient graph generation while maintaining high performance.
	3. Significant improvements in downstream evaluations such as caption assessment and hallucination detection.

**Result:** DiscoSG-Refiner improves SPICE by approximately 30% over existing baselines while achieving significantly faster inference times, demonstrating enhanced performance in downstream tasks.

**Limitations:** High inference costs for larger models and licensing restrictions hinder their open-source accessibility.

**Conclusion:** The proposed methods and dataset are effective in addressing the limitations of previous approaches, showing substantial improvements in discourse-level tasks.

**Abstract:** Vision-Language Models (VLMs) now generate discourse-level, multi-sentence visual descriptions, challenging text scene graph parsers originally designed for single-sentence caption-to-graph mapping. Current approaches typically merge sentence-level parsing outputs for discourse input, often missing phenomena like cross-sentence coreference, resulting in fragmented graphs and degraded downstream VLM task performance. To address this, we introduce a new task, Discourse-level text Scene Graph parsing (DiscoSG), supported by our dataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised multi-sentence caption-graph pairs for images. Each caption averages 9 sentences, and each graph contains at least 3 times more triples than those in existing datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS improves SPICE by approximately 48% over the best sentence-merging baseline, high inference cost and restrictive licensing hinder its open-source use, and smaller fine-tuned PLMs struggle with complex graphs. We propose DiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a second PLM to iteratively propose graph edits, reducing full-graph generation overhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE by approximately 30% over the best baseline while achieving 86 times faster inference than GPT-4. It also consistently improves downstream VLM tasks like discourse-level caption evaluation and hallucination detection. Code and data are available at: https://github.com/ShaoqLin/DiscoSG

</details>


### [63] [WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts](https://arxiv.org/abs/2506.15594)

*Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, RÃ©mi Lebret*

**Main category:** cs.CL

**Keywords:** vision-language models, document understanding, cross-modal reasoning

**Relevance Score:** 6

**TL;DR:** WikiMixQA introduces a benchmark for evaluating vision-language models' cross-modal reasoning skills over long document contexts, emphasizing complex reasoning with tables and charts from Wikipedia.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in automatic document understanding, particularly with long-context vision inputs and complex layouts.

**Method:** The authors present WikiMixQA, a benchmark that includes 1,000 multiple-choice questions based on tables and charts from 4,000 Wikipedia pages, focusing on cross-modal reasoning.

**Key Contributions:**

	1. Introduction of WikiMixQA benchmark for cross-modal reasoning
	2. Evaluation of multiple state-of-the-art vision-language models
	3. Demonstration of performance disparities in long-context processing

**Result:** Evaluation of 12 state-of-the-art vision-language models shows that while proprietary models perform better with direct context (~70% accuracy), their performance drops significantly with long documents. GPT-4-o is notably better, exceeding 50% accuracy, while open-source models achieve a maximum of 27%.

**Limitations:** The benchmark focuses on specific Wikipedia topics and may not generalize across all document types.

**Conclusion:** The findings highlight the difficulties in long-context, multi-modal reasoning and position WikiMixQA as an essential tool for advancing research in document understanding.

**Abstract:** Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.

</details>


### [64] [From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns](https://arxiv.org/abs/2506.15598)

*Bernardo Leite, Henrique Lopes Cardoso, Pedro Pinto, Abel Ferreira, LuÃ­s Abreu, Isabel Rangel, Sandra Monteiro*

**Main category:** cs.CL

**Keywords:** generative AI, MCQ generation, reading comprehension, psychometrics, Portuguese

**Relevance Score:** 5

**TL;DR:** The paper investigates the use of generative AI for automating the creation of multiple-choice questions (MCQs) in Portuguese, focusing on reading comprehension suitable for elementary education.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the time-consuming process of manually creating MCQs and the need for high-quality and reliable MCQs in educational settings, particularly in non-English languages.

**Method:** Generative models are used to produce MCQs aligned with curriculum and varying difficulty levels. The generated MCQs are evaluated through expert reviews and psychometric analysis based on student responses.

**Key Contributions:**

	1. Demonstrates the application of generative AI in non-English language MCQ generation.
	2. Evaluates the quality of AI-generated MCQs using expert reviews and psychometric data.
	3. Highlights the importance of reliable distractors in MCQ design.

**Result:** The study finds that the quality of AI-generated MCQs is comparable to human-created ones, although issues with semantic clarity, answerability, and distractor quality are identified.

**Limitations:** Focuses on Portuguese, limiting applicability to other languages; issues with semantic clarity and distractor quality were noted.

**Conclusion:** While generative AI shows promise in MCQ generation, overcoming challenges in clarity and engagement is essential for practical application in education.

**Abstract:** While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design.

</details>


### [65] [The Compositional Architecture of Regret in Large Language Models](https://arxiv.org/abs/2506.15617)

*Xiangxiang Cui, Shu Yang, Tianjin Huang, Wanyu Lin, Lijie Hu, Di Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, regret expression, neural networks, cognitive processes, machine learning

**Relevance Score:** 8

**TL;DR:** The paper explores the concept of regret in Large Language Models (LLMs) by identifying and analyzing regret expressions within model outputs to enhance reliability and understanding of cognitive processes in neural networks.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the mechanism of regret in LLMs is vital for improving model reliability and cognitive representation in neural networks.

**Method:** The study develops a workflow to construct a regret dataset through designed prompting scenarios, introduces the Supervised Compression-Decoupling Index (S-CDI) to find optimal representation layers for regret, and defines the Regret Dominance Score (RDS) to identify and analyze regret neurons.

**Key Contributions:**

	1. Workflow for constructing a comprehensive regret dataset
	2. Development of S-CDI metric for optimal layer identification
	3. Introduction of RDS metric for regret neuron analysis

**Result:** The proposed methods successfully identified optimal regret representation layers, leading to improved performance in probe classification. An M-shaped decoupling pattern was observed in model layers, and neurons were categorized into three functional types.

**Limitations:** Challenges include the absence of specialized datasets, lack of metrics for optimal layer identification, and limited methods for analyzing regret neurons.

**Conclusion:** The analysis of regret in LLMs provides insights into cognitive processing and highlights the potential for enhancing model reliability through better understanding of how different neurons function in relation to regret.

**Abstract:** Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the model's hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons.

</details>


### [66] [Minding the Politeness Gap in Cross-cultural Communication](https://arxiv.org/abs/2506.15623)

*Yuka Machino, Matthias Hofer, Max Siegel, Joshua B. Tenenbaum, Robert D. Hawkins*

**Main category:** cs.CL

**Keywords:** cross-cultural communication, intensifiers, computational cognitive model, British English, American English

**Relevance Score:** 4

**TL;DR:** This paper investigates cross-cultural communication issues in the interpretation of English intensifiers between British and American speakers through computational cognitive modeling.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the subtle differences in interpretation of intensifiers between British and American English speakers and understand whether these arise from literal meanings or pragmatic factors.

**Method:** The authors conducted three experiments and developed a computational cognitive model to analyze how listeners balance informativity, politeness, and utterance cost in their interpretations.

**Key Contributions:**

	1. Development of a computational cognitive model for cross-cultural communication analysis.
	2. Identification of the interaction between literal meaning and pragmatic factors in understanding intensifiers.
	3. New insights into the role of utterance cost in cross-cultural communication.

**Result:** The model comparisons indicated that differences in intensifier interpretation between cultures stem from both literal meaning variations and the weight assigned to utterance cost.

**Limitations:** 

**Conclusion:** Cross-cultural differences in interpretation are influenced by a complex interplay of semantic meanings and normative behaviors, challenging previous theories that focused on either solely.

**Abstract:** Misunderstandings in cross-cultural communication often arise from subtle differences in interpretation, but it is unclear whether these differences arise from the literal meanings assigned to words or from more general pragmatic factors such as norms around politeness and brevity. In this paper, we report three experiments examining how speakers of British and American English interpret intensifiers like "quite" and "very." To better understand these cross-cultural differences, we developed a computational cognitive model where listeners recursively reason about speakers who balance informativity, politeness, and utterance cost. Our model comparisons suggested that cross-cultural differences in intensifier interpretation stem from a combination of (1) different literal meanings, (2) different weights on utterance cost. These findings challenge accounts based purely on semantic variation or politeness norms, demonstrating that cross-cultural differences in interpretation emerge from an intricate interplay between the two.

</details>


### [67] [Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability](https://arxiv.org/abs/2506.15629)

*Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** generative LLMs, commonsense reasoning, benchmark, ordered coverage, instruction-following

**Relevance Score:** 8

**TL;DR:** This paper presents Ordered CommonGen, a benchmark for evaluating LLMs on their compositional generalization and instruction-following capabilities with a focus on generating sentences in specified concept order.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the capability of generative LLMs to follow instructions regarding the order of concepts in sentence generation and to measure their compositional generalization abilities.

**Method:** Introduced a benchmark called Ordered CommonGen that quantifies ordered coverage, assessing whether generated sentences include concepts in the specified order, while using 36 LLMs to analyze their performance.

**Key Contributions:**

	1. Development of the Ordered CommonGen benchmark
	2. Introduction of ordered coverage metrics
	3. Analysis of LLMs' instruction-following capabilities and their limitations

**Result:** The study revealed that LLMs understand instruction intent but often produce low-diversity outputs due to biases towards specific order patterns; the best-performing LLM achieved only 75% ordered coverage.

**Limitations:** The study primarily focuses on instruction-following and compositional generalization without addressing other aspects of LLM performance.

**Conclusion:** Improvements are necessary for LLMs regarding their instruction-following and compositional generalization capabilities.

**Abstract:** In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.

</details>


### [68] [Oldies but Goldies: The Potential of Character N-grams for Romanian Texts](https://arxiv.org/abs/2506.15650)

*Dana Lupsa, Sanda-Maria Avram*

**Main category:** cs.CL

**Keywords:** authorship attribution, machine learning, Romanian texts

**Relevance Score:** 4

**TL;DR:** This study evaluates machine learning techniques for authorship attribution of Romanian texts, finding that artificial neural networks perform best using character n-gram features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address authorship attribution for Romanian texts using a systematic evaluation of machine learning techniques.

**Method:** The study evaluates six machine learning techniques (SVM, LR, k-NN, DT, RF, ANN) using character n-gram features for authorship classification on the ROST corpus.

**Key Contributions:**

	1. Evaluation of multiple machine learning techniques for authorship attribution
	2. Demonstrated effectiveness of character n-gram features in Romanian texts
	3. Showcased the potential of simple stylometric approaches in under-studied languages

**Result:** The ANN model achieved the highest performance, with perfect classification in four out of fifteen runs using 5-gram features.

**Limitations:** 

**Conclusion:** Lightweight, interpretable character n-gram approaches can deliver state-of-the-art accuracy for Romanian authorship attribution and may be preferable in resource-constrained language settings.

**Abstract:** This study addresses the problem of authorship attribution for Romanian texts using the ROST corpus, a standard benchmark in the field. We systematically evaluate six machine learning techniques: Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT), Random Forests (RF), and Artificial Neural Networks (ANN), employing character n-gram features for classification. Among these, the ANN model achieved the highest performance, including perfect classification in four out of fifteen runs when using 5-gram features. These results demonstrate that lightweight, interpretable character n-gram approaches can deliver state-of-the-art accuracy for Romanian authorship attribution, rivaling more complex methods. Our findings highlight the potential of simple stylometric features in resource, constrained or under-studied language settings.

</details>


### [69] [CC-LEARN: Cohort-based Consistency Learning](https://arxiv.org/abs/2506.15662)

*Xiao Ye, Shaswat Shrivastava, Zhaonan Li, Jacob Dineen, Shijie Lu, Avneet Ahuja, Ming Shen, Zhikun Xu, Ben Zhou*

**Main category:** cs.CL

**Keywords:** Cohort-based Consistency Learning, Reinforcement Learning, Large Language Models

**Relevance Score:** 8

**TL;DR:** Cohort-based Consistency Learning (CC-Learn) enhances reasoning consistency in LLMs through a reinforcement learning framework that optimizes cohort-level consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with robust reasoning and consistency across tasks.

**Method:** Cohort-based Consistency Learning (CC-Learn) uses reinforcement learning to train on cohorts of similar questions, optimizing a composite objective that includes cohort accuracy and penalties for trivial lookups.

**Key Contributions:**

	1. Introduction of a framework for cohort-level consistency in reasoning
	2. Demonstration of improved accuracy and stability on reasoning tasks
	3. Optimized reinforcement learning objectives for LLMs

**Result:** Experiments show that CC-Learn improves both accuracy and reasoning stability on benchmarks like ARC-Challenge and StrategyQA compared to pretrained and SFT models.

**Limitations:** 

**Conclusion:** Cohort-level reinforcement learning effectively enhances reasoning consistency in large language models.

**Abstract:** Large language models excel at many tasks but still struggle with consistent, robust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a reinforcement learning framework that improves the reliability of LLM reasoning by training on cohorts of similar questions derived from shared programmatic abstractions. To enforce cohort-level consistency, we define a composite objective combining cohort accuracy, a retrieval bonus for effective problem decomposition, and a rejection penalty for trivial or invalid lookups that reinforcement learning can directly optimize, unlike supervised fine-tuning. Optimizing this reward guides the model to adopt uniform reasoning patterns across all cohort members. Experiments on challenging reasoning benchmarks (including ARC-Challenge and StrategyQA) show that CC-Learn boosts both accuracy and reasoning stability over pretrained and SFT baselines. These results demonstrate that cohort-level RL effectively enhances reasoning consistency in LLMs.

</details>


### [70] [Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674)

*Tommaso Green, Martin Gubri, Haritz Puerto, Sangdoo Yun, Seong Joon Oh*

**Main category:** cs.CL

**Keywords:** Privacy leakage, Reasoning traces, Large language models, Sensitive data, Prompt injections

**Relevance Score:** 9

**TL;DR:** The paper examines privacy leaks in the reasoning traces of large reasoning models, showing that these internal processes can contain sensitive user information and can be exploited through prompt injections.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the assumption that reasoning traces of large models are safe from privacy leakage.

**Method:** The authors conduct probing and agentic evaluations to analyze how reasoning steps in models can lead to sensitive data leakage.

**Key Contributions:**

	1. Demonstration that reasoning traces can leak sensitive user information.
	2. Identification of prompt injections as a method for extracting this information.
	3. Analysis of the trade-off between reasoning utility and privacy risks.

**Result:** The study finds that while increasing reasoning steps makes models more cautious in final outputs, it significantly increases the verbosity in reasoning, leading to more potential data leakage.

**Limitations:** The study primarily focuses on the leakage aspects and may not cover extensive solutions to mitigate these issues.

**Conclusion:** Safety measures should address not just final outputs but also the internal reasoning processes of models to mitigate privacy risks.

**Abstract:** We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.

</details>


### [71] [Gender-Neutral Machine Translation Strategies in Practice](https://arxiv.org/abs/2506.15676)

*Hillary Dawkins, Isar Nejadgholi, Chi-kiu Lo*

**Main category:** cs.CL

**Keywords:** machine translation, gender neutrality, gender ambiguity, representational harms, binary gender stereotypes

**Relevance Score:** 7

**TL;DR:** This paper evaluates the performance of 21 machine translation systems in preserving gender neutrality, particularly in the context of gender ambiguity in language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how machine translation systems handle gender ambiguity while considering the potential for misgendering and representational harms.

**Method:** The study analyzes 21 machine translation systems across three translation directions of varying difficulty, categorizing their gender-neutral strategies in response to gender ambiguity.

**Key Contributions:**

	1. Evaluation of 21 MT systems' response to gender ambiguity
	2. Categorization of gender-neutral strategies in MT
	3. Examination of the impact of binary gender stereotypes on MT

**Result:** The findings highlight a broad failure among the majority of MT systems to provide gender-neutral translations when faced with gender ambiguity, although a few systems do implement effective strategies based on the target language.

**Limitations:** The study's scope is limited to just 21 MT systems, which may not provide a comprehensive view of all systems available.

**Conclusion:** There is a pressing need for improvements in machine translation systems to ensure they maintain gender neutrality, as current implementations often fall short.

**Abstract:** Gender-inclusive machine translation (MT) should preserve gender ambiguity in the source to avoid misgendering and representational harms. While gender ambiguity often occurs naturally in notional gender languages such as English, maintaining that gender neutrality in grammatical gender languages is a challenge. Here we assess the sensitivity of 21 MT systems to the need for gender neutrality in response to gender ambiguity in three translation directions of varying difficulty. The specific gender-neutral strategies that are observed in practice are categorized and discussed. Additionally, we examine the effect of binary gender stereotypes on the use of gender-neutral translation. In general, we report a disappointing absence of gender-neutral translations in response to gender ambiguity. However, we observe a small handful of MT systems that switch to gender neutral translation using specific strategies, depending on the target language.

</details>


### [72] [GenRecal: Generation after Recalibration from Large to Small Vision-Language Models](https://arxiv.org/abs/2506.15681)

*Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu*

**Main category:** cs.CL

**Keywords:** vision-language models, distillation framework, knowledge transfer

**Relevance Score:** 8

**TL;DR:** The paper presents Generation after Recalibration (GenRecal), a distillation framework that improves the performance of vision-language models (VLMs) by facilitating knowledge transfer across different VLM architectures.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the challenges of deploying vision-language models (VLMs) on resource-constrained devices, particularly due to their high computational demands.

**Method:** GenRecal introduces a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, allowing for effective knowledge transfer.

**Key Contributions:**

	1. Introduction of GenRecal, a general-purpose distillation framework for VLMs
	2. Development of the Recalibrator for feature representation alignment
	3. Demonstration of superior performance on multiple challenging benchmarks.

**Result:** Extensive experiments show that GenRecal significantly enhances baseline performances on various benchmarks, surpassing both open-source and closed-source VLMs.

**Limitations:** 

**Conclusion:** GenRecal demonstrates a promising approach to create smaller, efficient VLMs without compromising performance, making it feasible for deployment in real-world scenarios.

**Abstract:** Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.

</details>


### [73] [PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning](https://arxiv.org/abs/2506.15683)

*Yuhui Shi, Yehan Yang, Qiang Sheng, Hao Mi, Beizhe Hu, Chaoxi Xu, Juan Cao*

**Main category:** cs.CL

**Keywords:** large language models, text detection, machine learning, privately-tuned models, family-aware learning

**Relevance Score:** 9

**TL;DR:** PhantomHunter detects LLM-generated text from privately-tuned models using a family-aware learning framework, outperforming existing baselines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large language models (LLMs) has led to increased issues like misinformation and academic misconduct, necessitating effective detection of LLM-generated text, particularly from privately tuned models.

**Method:** The paper introduces PhantomHunter, a specialized detector that employs a family-aware learning framework to identify shared traits across base models and their fine-tuned derivatives.

**Key Contributions:**

	1. Introduction of PhantomHunter for detecting LLM-generated text from privately-tuned models.
	2. Use of family-aware learning framework for improved detection accuracy.
	3. Experimental validation showing substantially higher performance than existing solutions.

**Result:** PhantomHunter demonstrates superior detection capabilities, achieving F1 scores over 96% compared to 7 baselines and 3 industrial services in experiments using various LLM families.

**Limitations:** 

**Conclusion:** The proposed method effectively addresses the challenges of detecting text from privately-tuned LLMs, indicating its potential importance in maintaining integrity in text generation.

**Abstract:** With the popularity of large language models (LLMs), undesirable societal problems like misinformation production and academic misconduct have been more severe, making LLM-generated text detection now of unprecedented importance. Although existing methods have made remarkable progress, a new challenge posed by text from privately tuned LLMs remains underexplored. Users could easily possess private LLMs by fine-tuning an open-source one with private corpora, resulting in a significant performance drop of existing detectors in practice. To address this issue, we propose PhantomHunter, an LLM-generated text detector specialized for detecting text from unseen, privately-tuned LLMs. Its family-aware learning framework captures family-level traits shared across the base models and their derivatives, instead of memorizing individual characteristics. Experiments on data from LLaMA, Gemma, and Mistral families show its superiority over 7 baselines and 3 industrial services, with F1 scores of over 96%.

</details>


### [74] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)

*Ho Yin 'Sam' Ng, Ting-Yao Hsu, Aashish Anantha Ramakrishnan, Branislav Kveton, Nedim Lipka, Franck Dernoncourt, Dongwon Lee, Tong Yu, Sungchul Kim, Ryan A. Rossi, Ting-Hao 'Kenneth' Huang*

**Main category:** cs.CL

**Keywords:** personalized caption generation, multimodal profiles, figure captions, language models

**Relevance Score:** 8

**TL;DR:** This paper presents LaMP-Cap, a dataset designed for generating personalized figure captions using multimodal figure profiles, showing significant improvements in caption quality when leveraging profiles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for personalized figure captions that match authors' writing styles, as existing AI-generated captions often require revision.

**Method:** The paper introduces LaMP-Cap, which includes multimodal inputs (figure images and additional figure profiles) to assist in generating personalized captions.

**Key Contributions:**

	1. Introduction of the LaMP-Cap dataset for personalized figure caption generation.
	2. Demonstration of the importance of multimodal profiles for enhancing caption quality.
	3. Ablation studies showing the superiority of image-based profiles over text-only profiles.

**Result:** Experiments with four LLMs demonstrate that using profile information improves caption generation, with images proving more beneficial than text when creating captions.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of multimodal profiles in generating captions that closely reflect the authors' styles, advocating for the use of such datasets in future research.

**Abstract:** Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.

</details>


### [75] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)

*Jesse Woo, Fateme Hashemi Chaleshtori, Ana MarasoviÄ, Kenneth Marino*

**Main category:** cs.CL

**Keywords:** Legal NLP, Legal briefs, Language models, Argumentation, Dataset

**Relevance Score:** 3

**TL;DR:** BRIEFME is a new dataset designed for improving legal brief writing and editing by language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in Legal NLP related to the writing and editing of legal briefs, aiming to assist legal professionals in making persuasive arguments.

**Method:** The paper introduces the BRIEFME dataset, which includes three tasks: argument summarization, argument completion, and case retrieval, aimed at evaluating the capabilities of language models in the legal domain.

**Key Contributions:**

	1. Introduction of the BRIEFME dataset for legal briefs
	2. Analysis of LLM performance on legal-specific tasks
	3. Identification of areas where LLMs struggle in Legal NLP

**Result:** Current large language models (LLMs) perform well in argument summarization and completion tasks but struggle with realistic argument completion and case retrieval.

**Limitations:** The dataset and tasks may not fully encapsulate the complexities of legal argumentation in practice.

**Conclusion:** The findings suggest a need for ongoing development in Legal NLP to support legal practitioners more effectively.

**Abstract:** A core part of legal work that has been under-explored in Legal NLP is the writing and editing of legal briefs. This requires not only a thorough understanding of the law of a jurisdiction, from judgments to statutes, but also the ability to make new arguments to try to expand the law in a new direction and make novel and creative arguments that are persuasive to judges. To capture and evaluate these legal skills in language models, we introduce BRIEFME, a new dataset focused on legal briefs. It contains three tasks for language models to assist legal professionals in writing briefs: argument summarization, argument completion, and case retrieval. In this work, we describe the creation of these tasks, analyze them, and show how current models perform. We see that today's large language models (LLMs) are already quite good at the summarization and guided completion tasks, even beating human-generated headings. Yet, they perform poorly on other tasks in our benchmark: realistic argument completion and retrieving relevant legal cases. We hope this dataset encourages more development in Legal NLP in ways that will specifically aid people in performing legal work.

</details>


### [76] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)

*Ha-Thanh Nguyen, Chaoran Liu, Koichi Takeda, Yusuke Miyao, Pontus Stenetorp, Qianying Liu, Su Myat Noe, Hideyuki Tachibana, Sadao Kurohashi*

**Main category:** cs.CL

**Keywords:** BIS Reasoning, syllogistic reasoning, large language models, belief-inconsistent reasoning, benchmarking

**Relevance Score:** 7

**TL;DR:** BIS Reasoning 1.0 is a dataset designed to evaluate belief-inconsistent reasoning in large language models, benchmarked against state-of-the-art models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate reasoning biases in large language models (LLMs) using syllogistic reasoning problems that are logically valid but belief-inconsistent.

**Method:** Introduced a new dataset of syllogistic reasoning problems and benchmarked performance across various state-of-the-art LLMs, including GPT and Claude models.

**Key Contributions:**

	1. Introduction of the BIS Reasoning 1.0 dataset
	2. Benchmarking of multiple state-of-the-art LLMs
	3. Identification of reasoning biases in LLMs

**Result:** GPT-4o achieved 79.54% accuracy, highlighting significant performance variance among models.

**Limitations:** Limited to evaluating Japanese LLMs; results may vary with additional contexts or languages.

**Conclusion:** Findings indicate critical weaknesses in LLMs related to logically valid but belief-conflicting inputs, with implications for their use in sensitive areas such as healthcare and law.

**Abstract:** We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.

</details>
