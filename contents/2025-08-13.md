# 2025-08-13

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 72]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Designing for Disclosure in Data Visualizations](https://arxiv.org/abs/2508.08383)

*Krisha Mehta, Gordon Kindlmann, Alex Kale*

**Main category:** cs.HC

**Keywords:** data visualization, disclosure tactics, taxonomy, design trade-offs, ethical design

**Relevance Score:** 6

**TL;DR:** The paper presents a taxonomy of disclosure tactics in data visualization derived from an analysis of 425 visualization techniques, highlighting how design choices affect data representation and access.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of explicit guidance on how data transformations in visualizations can hide or reveal information, which impacts understanding.

**Method:** Content analysis of 425 visualization examples from academic literature to develop a taxonomy that categorizes disclosure tactics based on their effects on data representation.

**Key Contributions:**

	1. Development of a taxonomy of disclosure tactics in data visualizations.
	2. A systematic way to evaluate design trade-offs related to information access.
	3. Guidance for various design scenarios where disclosure is crucial.

**Result:** The analysis resulted in a systematic taxonomy that aids designers in reasoning about trade-offs in information disclosure in visualization contexts.

**Limitations:** 

**Conclusion:** Framing disclosure as a critical aspect of visualization research introduces new insights into numerous areas including authoring tools, ethical design, and communication of uncertainty.

**Abstract:** Visualizing data often entails data transformations that can reveal and hide information, operations we dub disclosure tactics. Whether designers hide information intentionally or as an implicit consequence of other design choices, tools and frameworks for visualization offer little explicit guidance on disclosure. To systematically characterize how visualizations can limit access to an underlying dataset, we contribute a content analysis of 425 examples of visualization techniques sampled from academic papers in the visualization literature, resulting in a taxonomy of disclosure tactics. Our taxonomy organizes disclosure tactics based on how they change the data representation underlying a chart, providing a systematic way to reason about design trade-offs in terms of what information is revealed, distorted, or hidden. We demonstrate the benefits of using our taxonomy by showing how it can guide reasoning in design scenarios where disclosure is a first-order consideration. Adopting disclosure as a framework for visualization research offers new perspective on authoring tools, literacy, uncertainty communication, personalization, and ethical design.

</details>


### [2] [Empowering Children to Create AI-Enabled Augmented Reality Experiences](https://arxiv.org/abs/2508.08467)

*Lei Zhang, Shuyao Zhou, Amna Liaqat, Tinney Mak, Brian Berengard, Emily Qian, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** AI, Augmented Reality, Child Learning, Visual Programming, Generative Models

**Relevance Score:** 7

**TL;DR:** Capybara is an AR-based visual programming environment that enables children to create and animate 3D characters using AI technologies, promoting creative engagement with AR tools.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To shift the usage of AI-enabled AR technologies from positioning children as consumers to empowering them as creators.

**Method:** Capybara integrates AR and AI to allow children to create 3D characters using text-to-3D models, animate them with auto-rigging and body tracking, and interact with physical objects through vision-based AI recognition.

**Key Contributions:**

	1. Introduction of Capybara, an AR and AI-powered environment for child creators
	2. Implementation of text-to-3D generative AI for character customization
	3. Integration of vision-based AI for interactive programming with physical objects.

**Result:** User studies with 20 children showed that Capybara effectively empowers children to create personalized and interactive AR experiences that link virtual and physical worlds.

**Limitations:** 

**Conclusion:** Capybara fosters creative learning by enabling children to actively engage in the creation of AR environments, utilizing AI tools to enhance their learning experience.

**Abstract:** Despite their potential to enhance children's learning experiences, AI-enabled AR technologies are predominantly used in ways that position children as consumers rather than creators. We introduce Capybara, an AR-based and AI-powered visual programming environment that empowers children to create, customize, and program 3D characters overlaid onto the physical world. Capybara enables children to create virtual characters and accessories using text-to-3D generative AI models, and to animate these characters through auto-rigging and body tracking. In addition, our system employs vision-based AI models to recognize physical objects, allowing children to program interactive behaviors between virtual characters and their physical surroundings. We demonstrate the expressiveness of Capybara through a set of novel AR experiences. We conducted user studies with 20 children in the United States and Argentina. Our findings suggest that Capybara can empower children to harness AI in authoring personalized and engaging AR experiences that seamlessly bridge the virtual and physical worlds.

</details>


### [3] [AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns](https://arxiv.org/abs/2508.08502)

*Marta Robledo-Moreno, Ruben Vera-Rodriguez, Ruben Tolosana, Javier Ortega-Garcia, Andres Huergo, Julian Fierrez*

**Main category:** cs.HC

**Keywords:** behavioral biometrics, in-air signatures, privacy, sensor data, machine learning

**Relevance Score:** 7

**TL;DR:** This study presents AirSignatureDB, a dataset for verifying in-air signatures using smartphone motion sensors, and explores the implications for privacy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The popularity of behavioral biometrics for authentication necessitates reliable datasets and methods for evaluation, particularly in real-world scenarios.

**Method:** A dataset of in-air signatures from 108 participants was collected across various smartphone models. The study benchmarks traditional and deep learning methods for signature verification and reconstructs three-dimensional trajectories from sensor data.

**Key Contributions:**

	1. Introduction of the AirSignatureDB dataset
	2. Benchmarking of both traditional and deep learning methods for in-air signature verification
	3. Development of a method for reconstructing 3D trajectories from inertial sensors

**Result:** The research shows that accurate reconstruction of in-air signatures is possible, challenging previous assumptions about their traceless nature and demonstrating vulnerabilities in privacy.

**Limitations:** The reconstruction method's implications on user privacy are complex and require further exploration.

**Conclusion:** The findings call for a reevaluation of privacy considerations in behavioral biometrics using inertial sensor data, as it can expose user-specific information.

**Abstract:** Behavioral biometrics based on smartphone motion sensors are growing in popularity for authentication purposes. In this study, AirSignatureDB is presented: a new publicly accessible dataset of in-air signatures collected from 108 participants under real-world conditions, using 83 different smartphone models across four sessions. This dataset includes genuine samples and skilled forgeries, enabling a comprehensive evaluation of system robustness against realistic attack scenarios. Traditional and deep learning-based methods for in-air signature verification are benchmarked, while analyzing the influence of sensor modality and enrollment strategies. Beyond verification, a first approach to reconstructing the three-dimensional trajectory of in-air signatures from inertial sensor data alone is introduced. Using on-line handwritten signatures as a reference, we demonstrate that the recovery of accurate trajectories is feasible, challenging the long-held assumption that in-air gestures are inherently traceless. Although this approach enables forensic traceability, it also raises critical questions about the privacy boundaries of behavioral biometrics. Our findings underscore the need for a reevaluation of the privacy assumptions surrounding inertial sensor data, as they can reveal user-specific information that had not previously been considered in the design of in-air signature systems.

</details>


### [4] [Adaptique: Multi-objective and Context-aware Online Adaptation of Selection Techniques in Virtual Reality](https://arxiv.org/abs/2508.08505)

*Chao-Jung Lai, Mauricio Sousa, Tianyu Zhang, Ludwig Sidenmark, Tovi Grossman*

**Main category:** cs.HC

**Keywords:** virtual reality, adaptive selection, user interface, human movement, interaction techniques

**Relevance Score:** 8

**TL;DR:** Adaptique is an adaptive selection model for virtual reality that optimizes selection techniques based on user context and environmental factors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of selection in virtual reality, such as distant targets, occlusion, and target density, which complicate user interaction.

**Method:** Adaptique infers optimal selection techniques by analyzing user and environmental information, focusing on target size, distance, occlusion, and user posture, while balancing speed, accuracy, comfort, and familiarity.

**Key Contributions:**

	1. Development of Adaptique, an adaptive selection model for VR.
	2. Incorporation of contextual information for technique selection.
	3. Demonstration of improved user preference and performance in studies.

**Result:** User studies indicate that Adaptique outperforms traditional single selection techniques in both preference and performance.

**Limitations:** 

**Conclusion:** Adaptique demonstrates improved versatility and effectiveness in selection tasks within virtual reality environments.

**Abstract:** Selection is a fundamental task that is challenging in virtual reality due to issues such as distant and small targets, occlusion, and target-dense environments. Previous research has tackled these challenges through various selection techniques, but complicates selection and can be seen as tedious outside of their designed use case. We present Adaptique, an adaptive model that infers and switches to the most optimal selection technique based on user and environmental information. Adaptique considers contextual information such as target size, distance, occlusion, and user posture combined with four objectives: speed, accuracy, comfort, and familiarity which are based on fundamental predictive models of human movement for technique selection. This enables Adaptique to select simple techniques when they are sufficiently efficient and more advanced techniques when necessary. We show that Adaptique is more preferred and performant than single techniques in a user study, and demonstrate Adaptique's versatility in an application.

</details>


### [5] [StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI](https://arxiv.org/abs/2508.08524)

*Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsara, Shaun Kane*

**Main category:** cs.HC

**Keywords:** accessibility, human-computer interaction, multimodal AI, street view, blind users

**Relevance Score:** 9

**TL;DR:** StreetViewAI is an accessible street view tool designed for blind users, integrating multimodal AI and conversational speech for virtual navigation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to create an inclusive tool that allows blind users to interact with maps and navigate real-world environments that are typically inaccessible through standard street view applications.

**Method:** The authors iteratively designed StreetViewAI with input from a mixed-visual ability team and conducted evaluations with blind users to assess usability and functionality.

**Key Contributions:**

	1. Introduction of StreetViewAI, the first accessible street view tool for blind users
	2. Integration of context-aware, multimodal AI and conversational speech
	3. Guidelines for future work on making digital maps accessible

**Result:** The evaluation with eleven blind users revealed that StreetViewAI effectively supports point of interest (POI) investigations and remote route planning.

**Limitations:** The study is limited by the small sample size of participants for evaluation.

**Conclusion:** The study concludes that accessible street view tools are valuable for blind users and outlines guidelines for future enhancements in this area.

**Abstract:** Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.

</details>


### [6] [Explore, Listen, Inspect: Supporting Multimodal Interaction with 3D Surface and Point Data Visualizations](https://arxiv.org/abs/2508.08554)

*Sanchita S. Kamath, Aziz N. Zeidieh, JooYoung Seo*

**Main category:** cs.HC

**Keywords:** Blind and Low-Vision, 3D Data Visualization, Multimodal Interaction, Inclusive Design, Sonification

**Relevance Score:** 9

**TL;DR:** DIXTRAL is a web-native system designed to enhance access to 3D data visualizations for blind and low-vision users through multimodal interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Blind and low-vision users are excluded from 3D data visualizations which rely heavily on visual interactions.

**Method:** The study involved co-design with BLV researchers, iterative testing of prototypes, and feedback collection on spatial navigation, sonification, and usability.

**Key Contributions:**

	1. Introduction of DIXTRAL, a multimodal interaction system for BLV users
	2. Findings on effective auditory, visual, and textual feedback techniques
	3. Strategies for improving inclusivity in 3D visualizations

**Result:** Synchronized auditory, visual, and textual feedback, along with keyboard and gamepad navigation, improved structure discovery and orientation for BLV users.

**Limitations:** Study based on a small sample size with limited participant diversity.

**Conclusion:** DIXTRAL improves access to 3D scalar fields for BLV users and provides insights for inclusive 3D visualization practices.

**Abstract:** Blind and low-vision (BLV) users remain largely excluded from three-dimensional (3D) surface and point data visualizations due to the reliance on visual interaction. Existing approaches inadequately support non-visual access, especially in browser-based environments. This study introduces DIXTRAL, a hosted web-native system, co-designed with BLV researchers to address these gaps through multimodal interaction. Conducted with two blind and one sighted researcher, this study took place over sustained design sessions. Data were gathered through iterative testing of the prototype, collecting feedback on spatial navigation, sonification, and usability. Co-design observations demonstrate that synchronized auditory, visual, and textual feedback, combined with keyboard and gamepad navigation, enhances both structure discovery and orientation. DIXTRAL aims to improve access to 3D continuous scalar fields for BLV users and inform best practices for creating inclusive 3D visualizations.

</details>


### [7] [CoSight: Exploring Viewer Contributions to Online Video Accessibility Through Descriptive Commenting](https://arxiv.org/abs/2508.08582)

*Ruolin wang, Xingyu Liu, Biao Wang, Wayne Zhang, Ziqian Liao, Ziwen Li, Amy Pavel, Xiang 'Anthony' Chen*

**Main category:** cs.HC

**Keywords:** video accessibility, audio description, YouTube, community engagement, blind and low vision

**Relevance Score:** 7

**TL;DR:** CoSight enhances video accessibility for blind and low vision users by encouraging descriptive comments from sighted viewers on YouTube, resulting in more integrated visual descriptions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the accessibility gap in online video content for blind and low vision audiences, where traditional audio description methods are insufficient and unscalable.

**Method:** Developed CoSight, a Chrome extension that prompts users with visual indicators and reminders to add descriptions while watching and commenting on YouTube videos.

**Key Contributions:**

	1. Introduction of CoSight as a tool for enhancing video accessibility through community involvement.
	2. Successful integration of accessibility practices into ordinary commenting behavior on YouTube.
	3. Initial qualitative feedback indicating the value of user-generated descriptions for BLV audiences.

**Result:** In an exploratory study, CoSight led to 89% of comments including grounded visual descriptions. Follow-up interviews indicated that these comments, while not as rigorous as professional audio descriptions, offer valuable visual context.

**Limitations:** The quality of user-generated comments does not match that of professional audio descriptions and may vary greatly among users.

**Conclusion:** CoSight provides a feasible and scalable method to enhance video accessibility through community engagement, contributing to a more inclusive viewing experience.

**Abstract:** The rapid growth of online video content has outpaced efforts to make visual information accessible to blind and low vision (BLV) audiences. While professional Audio Description (AD) remains the gold standard, it is costly and difficult to scale across the vast volume of online media. In this work, we explore a complementary approach to broaden participation in video accessibility: engaging everyday video viewers at their watching and commenting time. We introduce CoSight, a Chrome extension that augments YouTube with lightweight, in-situ nudges to support descriptive commenting. Drawing from Fogg's Behavior Model, CoSight provides visual indicators of accessibility gaps, pop-up hints for what to describe, reminders to clarify vague comments, and related captions and comments as references. In an exploratory study with 48 sighted users, CoSight helped integrate accessibility contribution into natural viewing and commenting practices, resulting in 89% of comments including grounded visual descriptions. Follow-up interviews with four BLV viewers and four professional AD writers suggest that while such comments do not match the rigor of professional AD, they can offer complementary value by conveying visual context and emotional nuance for understanding the videos.

</details>


### [8] [Imposing AI: Deceptive design patterns against sustainability](https://arxiv.org/abs/2508.08672)

*Anaëlle Beignon, Thomas Thibault, Nolwenn Maudet*

**Main category:** cs.HC

**Keywords:** Generative AI, User Interface Design, Deceptive Patterns, Regulation, Environmental Impact

**Relevance Score:** 6

**TL;DR:** This paper discusses the environmental impact of generative AI and how tech companies are redesigning user interfaces to compel AI adoption, using deceptive strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the environmental consequences of widespread generative AI deployment and how it influences user behavior.

**Method:** Analysis of user interface design changes in tech companies, categorizing their strategies as deceptive patterns to encourage AI use.

**Key Contributions:**

	1. Identification of deceptive design strategies in AI adoption
	2. Discussion on environmental implications of generative AI
	3. Suggestions for regulatory measures on AI feature imposition

**Result:** Identified two main strategies: replacing existing features with AI ones and promoting narratives that hinder resistance to AI adoption.

**Limitations:** The paper focuses mainly on design strategies without extensive empirical evidence on user response to these changes.

**Conclusion:** There is a need for regulating the imposition of AI features to mitigate environmental harm.

**Abstract:** Generative AI is being massively deployed in digital services, at a scale that will result in significant environmental harm. We document how tech companies are transforming established user interfaces to impose AI use and show how and to what extent these strategies fit within established deceptive pattern categories. We identify two main design strategies that are implemented to impose AI use in both personal and professional contexts: imposing AI features in interfaces at the expense of existing non-AI features and promoting narratives about AI that make it harder to resist using it. We discuss opportunities for regulating the imposed adoption of AI features, which would inevitably lead to negative environmental effects.

</details>


### [9] [Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context](https://arxiv.org/abs/2508.08731)

*Mingyuan Zhong, Ajit Mallavarapu, Qing Nie*

**Main category:** cs.HC

**Keywords:** LLM, accessibility, mobile devices, content labels, screen readers

**Relevance Score:** 9

**TL;DR:** Caption is an LLM-powered tool that generates accurate content labels for visual interactive elements on mobile devices by using next-screen context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Content labels for visual elements are often missing or uninformative, hindering accessibility for screen readers. Current automated captioning systems are limited in context awareness.

**Method:** Caption collects next-screen context by navigating to the destination screen after an interaction, combining information from both the origin and destination screens for label generation.

**Key Contributions:**

	1. Development of Caption, an LLM-powered labeling tool
	2. Utilization of next-screen context for improved label accuracy
	3. Support for accessibility enhancements in mobile applications

**Result:** Preliminary results indicate that Caption produces more accurate labels than human annotators and a baseline LLM, thereby improving accessibility.

**Limitations:** 

**Conclusion:** Caption aims to empower developers with actionable accessibility suggestions and help screen reader users with on-demand repairs.

**Abstract:** We present Caption, an LLM-powered content label generation tool for visual interactive elements on mobile devices. Content labels are essential for screen readers to provide announcements for image-based elements, but are often missing or uninformative due to developer neglect. Automated captioning systems attempt to address this, but are limited to on-screen context, often resulting in inaccurate or unspecific labels. To generate more accurate and descriptive labels, Caption collects next-screen context on interactive elements by navigating to the destination screen that appears after an interaction and incorporating information from both the origin and destination screens. Preliminary results show Caption generates more accurate labels than both human annotators and an LLM baseline. We expect Caption to empower developers by providing actionable accessibility suggestions and directly support on-demand repairs by screen reader users.

</details>


### [10] [From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation](https://arxiv.org/abs/2508.08737)

*Jonathan C. Roberts, Peter Butcher, Panagiotis D. Ritsos*

**Main category:** cs.HC

**Keywords:** data visualization, pedagogical strategy, scenario-based learning, data communication, critical thinking

**Relevance Score:** 4

**TL;DR:** This paper discusses using scenario-based visualisation examples as a teaching method for data insight, representation, and interpretation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenges educators face in teaching data visualisation and to promote critical thinking about data representation in real-world contexts.

**Method:** The authors present a series of data-driven scenarios designed to reflect real-world complexities in data communication, focusing on aspects like chart selection and visual bias.

**Key Contributions:**

	1. Introduction of scenario-based visualisation for teaching data literacy.
	2. Development of concrete examples to aid data communication education.
	3. Framework for creating additional educational scenarios based on real-world contexts.

**Result:** The paper provides a collection of example scenarios, aiding educators in teaching data visualisation effectively while promoting critical assessment of data display methods.

**Limitations:** 

**Conclusion:** The approach aligns with authentic and scenario-based learning principles, allowing educators to create their own teaching scenarios derived from the principles outlined in the paper.

**Abstract:** This paper explores the use of scenario-based visualisation examples as a pedagogical strategy for teaching students the complexities of data insight, representation, and interpretation. Teaching data visualisation often involves explaining intricate issues related to data management and the challenges of presenting data meaningfully. In this work, we present a series of data-driven scenarios. These concise stories depict specific situations, and are created to help the educators highlight key concerns in data communication, such as chart selection, temporal versus categorical comparison, visual bias, and narrative framing. By grounding these examples in real-world contexts, students are encouraged to critically assess not only what the data shows, but how and why it is shown that way. The paper presents a collection of example scenarios, that educators can use for their own lessons; the work fits with a larger project on looking at critical thinking in the classroom, and developing appropriate tools. We also start to abstract principles, from our approach, so that others can develop their own scenarios for their teaching. Our approach aligns with principles of authentic and scenario-based learning, using real-world contexts to foster critical engagement with data.

</details>


### [11] [Addressing the Heterogeneity of Visualization in an Introductory PhD Course in the Swedish Context](https://arxiv.org/abs/2508.08958)

*Kostiantyn Kucher, Niklas Rönnberg, Jonas Löwgren*

**Main category:** cs.HC

**Keywords:** Visualization, PhD Education, Human-Computer Interaction

**Relevance Score:** 4

**TL;DR:** The paper reports on an introductory course designed for PhD students in Visualization Technology and Methodology to help them navigate the fragmented structure of the field at Linköping University.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To aid new PhD students in understanding the fragmented nature of the visualization field and to help them build connections within their institution.

**Method:** Designing an introductory course that includes interactions with other doctoral education activities and field trips to various research groups and units related to visualization.

**Key Contributions:**

	1. Design of an interdisciplinary introductory course on visualization for PhD students
	2. Incorporation of field trips to enhance learning and networking
	3. Insights into the challenges faced by new PhD students in visualization

**Result:** The course enabled students to better grasp the diversity in the field of visualization and facilitated building collegial relationships across the institution.

**Limitations:** 

**Conclusion:** The lessons learned from this course's preparation and execution can guide researchers and educators in establishing or improving similar doctoral courses.

**Abstract:** Visualization is a heterogeneous field, and this aspect is often reflected by the organizational structures at higher education institutions that academic researchers in visualization and related fields including computer graphics, human-computer interaction, and media design are typically affiliated with. It may thus be a challenge for new PhD students to grasp the fragmented structure of their new workplace, form collegial relations across the institution, and to build a coherent picture of the discipline as a whole. We report an attempt to address this challenge, in the form of an introductory course on the subject of Visualization Technology and Methodology for PhD students at the Division for Media and Information Technology, Link\"oping University, Sweden. We discuss the course design, including interactions with other doctoral education activities and field trips to multiple research groups and units within the division (ranging from scientific visualization and computer graphics to media design and visual communication). Lessons learned from the course preparation work as well as the first instance of the course offered during autumn term 2023 can be helpful to researchers and educators aiming to establish or improve similar doctoral courses.

</details>


### [12] [Envisioning Generative Artificial Intelligence in Cartography and Mapmaking](https://arxiv.org/abs/2508.09028)

*Yuhao Kang, Chenglong Wang*

**Main category:** cs.HC

**Keywords:** generative artificial intelligence, cartography, map design, ethical implications, symbolization

**Relevance Score:** 2

**TL;DR:** This paper explores the impact of generative artificial intelligence on cartography, discussing its applications, benefits, limitations, and ethical considerations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to examine how generative AI can enhance cartographic design decisions and practices.

**Method:** The paper employs case studies to illustrate how generative AI impacts aspects like symbolization, map evaluation, and map reading in cartography.

**Key Contributions:**

	1. Identification of benefits of GenAI in cartography
	2. Case studies demonstrating practical applications
	3. Discussion of ethical implications related to GenAI usage in mapping

**Result:** Generative AI can significantly improve mapmaking processes and usability but has limitations in scenarios requiring deep cartographic knowledge.

**Limitations:** Key scenarios where GenAI may not be suitable include tasks needing deep understanding of cartographic knowledge and precision-focused requirements.

**Conclusion:** The work highlights the potential of GenAI in cartography while cautioning against its use in precision-critical tasks and urging consideration of ethical issues.

**Abstract:** Generative artificial intelligence (GenAI), including large language models, diffusion-based image generation models, and GenAI agents, has provided new opportunities for advancements in mapping and cartography. Due to their characteristics including world knowledge and generalizability, artistic style and creativity, and multimodal integration, we envision that GenAI may benefit a variety of cartographic design decisions, from mapmaking (e.g., conceptualization, data preparation, map design, and map evaluation) to map use (such as map reading, interpretation, and analysis). This paper discusses several important topics regarding why and how GenAI benefits cartography with case studies including symbolization, map evaluation, and map reading. Despite its unprecedented potential, we identify key scenarios where GenAI may not be suitable, such as tasks that require a deep understanding of cartographic knowledge or prioritize precision and reliability. We also emphasize the need to consider ethical and social implications, such as concerns related to hallucination, reproducibility, bias, copyright, and explainability. This work lays the foundation for further exploration and provides a roadmap for future research at the intersection of GenAI and cartography.

</details>


### [13] [Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration](https://arxiv.org/abs/2508.09033)

*Tina Behzad, Nikolos Gurney, Ning Wang, David V. Pynadath*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, AI communication, Trust in AI, Decision-making, Explanations

**Relevance Score:** 8

**TL;DR:** This paper explores how AI communication affects human-AI team performance, particularly focusing on AI explanations regarding its strengths and weaknesses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the benefits of human-AI teaming through effective communication, enabling users to better understand and trust AI assistance.

**Method:** The authors trained a decision tree to identify and explain the model's mistakes. A user study was conducted on an income prediction task, examining the effects of varying information levels and AI explanations.

**Key Contributions:**

	1. Investigates the impact of AI explanations on human-AI team performance.
	2. Demonstrates how communicating AI's strengths and weaknesses affects user trust.
	3. Provides empirical evidence from a user study to support findings.

**Result:** The study found that providing insights into AI performance significantly enhanced task performance, and improving AI's communication about its strengths and limitations fostered better trust calibration among users.

**Limitations:** 

**Conclusion:** The findings emphasize the critical role of information delivery in shaping user trust and reliance on AI in decision-making processes.

**Abstract:** The promise of human-AI teaming lies in humans and AI working together to achieve performance levels neither could accomplish alone. Effective communication between AI and humans is crucial for teamwork, enabling users to efficiently benefit from AI assistance. This paper investigates how AI communication impacts human-AI team performance. We examine AI explanations that convey an awareness of its strengths and limitations. To achieve this, we train a decision tree on the model's mistakes, allowing it to recognize and explain where and why it might err. Through a user study on an income prediction task, we assess the impact of varying levels of information and explanations about AI predictions. Our results show that AI performance insights enhance task performance, and conveying AI awareness of its strengths and weaknesses improves trust calibration. These findings highlight the importance of considering how information delivery influences user trust and reliance in AI-assisted decision-making.

</details>


### [14] [Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks](https://arxiv.org/abs/2508.09043)

*Yanbing Chen, Jonathan Nelson, Bing Zhou, Ryan Zhenqi Zhou, Shan Ye, Haokun Liu, Zhining Gu, Armita Kar, Hoeyun Kwon, Pengyu Chen, Maoran Sun, Yuhao Kang*

**Main category:** cs.HC

**Keywords:** GIScience, faculty hiring networks, knowledge dissemination, academic equity, research themes

**Relevance Score:** 2

**TL;DR:** This study investigates faculty hiring networks in GIScience, mapping the connections of 946 faculty worldwide to reveal placement patterns, institutional influences, and evolving research themes.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexamined impacts of faculty hiring networks on knowledge dissemination and collaborative research in GIScience, filling a gap in existing literature.

**Method:** Analyzed placement patterns of 946 GIScience faculty by mapping connections between PhD-granting institutions and current faculty affiliations, using a comprehensive dataset compiled from volunteer contributions.

**Key Contributions:**

	1. Comprehensive mapping of GIScience faculty hiring networks
	2. Identification of influential academic programs
	3. Insight into evolving research themes in GIScience

**Result:** Identified influential programs in placing GIScience faculty, with heavy hiring concentration in western countries, significant internal retention, and evolving research themes focused on spatial data analytics, cartography, and environmental sciences.

**Limitations:** Potential limitations in the dataset's representativeness due to volunteer contributions.

**Conclusion:** The findings highlight the role of hiring practices in shaping global knowledge dissemination and advocate for academic equity in GIScience and Geography.

**Abstract:** Academia is profoundly influenced by faculty hiring networks, which serve as critical conduits for knowledge dissemination and the formation of collaborative research initiatives. While extensive research in various disciplines has revealed the institutional hierarchies inherent in these networks, their impacts within GIScience remain underexplored. To fill this gap, this study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping the connections between PhD-granting institutions and current faculty affiliations. Our dataset, which is compiled from volunteer-contributed information, is the most comprehensive collection available in this field. While there may be some limitations in its representativeness, its scope and depth provide a unique and valuable perspective on the global placement patterns of GIScience faculty. Our analysis reveals several influential programs in placing GIScience faculty, with hiring concentrated in the western countries. We examined the diversity index to assess the representation of regions and institutions within the global GIScience faculty network. We observe significant internal retention at both the continental and country levels, and a high level of non-self-hired ratio at the institutional level. Over time, research themes have also evolved, with growing research clusters emphasis on spatial data analytics, cartography and geovisualization, geocomputation, and environmental sciences, etc. These results illuminate the influence of hiring practices on global knowledge dissemination and contribute to promoting academic equity within GIScience and Geography.

</details>


### [15] [HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents](https://arxiv.org/abs/2403.02752)

*Sam Yu-Te Lee, Kwan-Liu Ma*

**Main category:** cs.HC

**Keywords:** sensemaking, visual analytics, large language models, hypergraph, user study

**Relevance Score:** 8

**TL;DR:** The paper presents HINTs, a visual analytics system that integrates topic- and entity-based techniques with Large Language Models (LLMs) to improve document sensemaking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of sensemaking in large document collections, which often lacks interpretability and trust due to model misalignment.

**Method:** Combine topic- and entity-based approaches by leveraging LLMs for data preparation, modeling the corpus as a hypergraph, and using an agglomerative clustering algorithm to organize the data semantically and based on connectivity.

**Key Contributions:**

	1. Introduction of HINTs, a novel visual analytics framework for document sensemaking
	2. Integration of LLMs as intelligent agents alongside traditional analytical methods
	3. Empirical evidence through case studies and user evaluations on the effectiveness of the system

**Result:** Two case studies demonstrate HINTs' generalizability and effectiveness, along with a user study revealing the behavior patterns and challenges in using intelligent agents for sensemaking.

**Limitations:** New challenges arise with the introduction of intelligent agents requiring refined interactive visualizations.

**Conclusion:** Intelligent agents can address many sensemaking challenges, but visual hints from visualizations are essential to tackle new problems introduced by these agents.

**Abstract:** Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.

</details>


### [16] [A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health](https://arxiv.org/abs/2411.02594)

*Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura M. Schwab Reese, Munmun De Choudhury*

**Main category:** cs.HC

**Keywords:** large language models, public health, risk assessment, health communication, focus groups

**Relevance Score:** 9

**TL;DR:** This paper explores the potential risks of adopting large language models (LLMs) in public health, presenting a risk taxonomy based on insights from focus groups.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The adoption of LLMs in public health presents unique challenges that require evaluation of potential risks, which have not been thoroughly explored yet.

**Method:** Focus groups were conducted with public health professionals and individuals with lived experience to gather their concerns about using LLMs for key public health issues.

**Key Contributions:**

	1. Development of a risk taxonomy for LLMs in public health
	2. Identification of four dimensions of risk
	3. Provision of reflection tools for practitioners

**Result:** A risk taxonomy was developed that categorizes risks into four dimensions: individual risks, human-centered care, information ecosystem, and technology accountability, and includes reflection questions for practitioners.

**Limitations:** 

**Conclusion:** The work emphasizes the importance of re-evaluating traditional information behaviors and incorporating lived experience and real-world practices to mitigate risks associated with LLMs in public health.

**Abstract:** Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.

</details>


### [17] [Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks](https://arxiv.org/abs/2502.09577)

*Qian Wan, Jiannan Li, Huanchen Wang, Zhicong Lu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Machine Learning, Language Models, Collaborative Writing, Prewriting

**Relevance Score:** 8

**TL;DR:** Polymind is a visual diagramming tool that utilizes multiple LLM-powered agents to support the prewriting process through a parallel collaboration workflow, offering enhanced customizability in task management compared to traditional chatbots.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of collaborating with LLMs in prewriting activities, specifically the limitations of turn-taking conversational interactions.

**Method:** The authors developed Polymind, which allows users to define and manage 'microtasks' in a parallel manner, simulating collaborative scenarios like brainstorming and writing.

**Key Contributions:**

	1. Introduction of the Polymind tool for prewriting collaboration
	2. Implementation of parallel collaboration workflows with microtasks
	3. User evaluation showing increased customizability and idea expansion over traditional LLM interfaces

**Result:** Users of Polymind experienced greater customizability and were able to expand their personalized writing ideas more quickly compared to using ChatGPT.

**Limitations:** The study does not explore the long-term impact of using Polymind on writing quality or user satisfaction across diverse writing tasks.

**Conclusion:** Polymind facilitates a more effective collaboration framework for prewriting by allowing simultaneous microtask management, enhancing user engagement and productivity.

**Abstract:** Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.

</details>


### [18] [AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot](https://arxiv.org/abs/2504.04299)

*Mohammad, Namvarpour, Harrison Pauwels, Afsaneh Razi*

**Main category:** cs.HC

**Keywords:** AI companionship, user reviews, thematic analysis, ethical guidelines, sexual harassment

**Relevance Score:** 8

**TL;DR:** Study investigates user experiences of harassment by the Replika chatbot through thematic analysis of negative reviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the inappropriate behaviors exhibited by AI conversational agents and their impact on users seeking emotional support.

**Method:** Thematic analysis of 35,105 negative reviews from the Google Play Store, identifying 800 cases of sexual harassment by the Replika chatbot.

**Key Contributions:**

	1. Identified prevalent instances of sexual harassment in AI chatbots.
	2. Highlighted user experiences regarding discomfort and privacy violations.
	3. Called for improved ethical standards in AI development.

**Result:** Findings revealed users frequently experienced unsolicited sexual advances, persistent inappropriate behavior, and boundary violations, leading to discomfort and privacy concerns.

**Limitations:** 

**Conclusion:** The study emphasizes the need for developers to implement safeguards and ethical guidelines to prevent harassment from AI companions.

**Abstract:** Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.

</details>


### [19] [When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices](https://arxiv.org/abs/2504.10265)

*Mariana Fernandez-Espinosa, Mariana Gonzalez-Bejar, Jacobo Wiesner, Diego Gomez-Zara*

**Main category:** cs.HC

**Keywords:** Domestic Work, Online Technologies, Human-Computer Interaction, Work Practices, Technology Design

**Relevance Score:** 4

**TL;DR:** This study explores the use of online technologies by domestic workers, revealing challenges and opportunities for better tool design.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how domestic workers use online technologies and how it impacts their work practices and relationships.

**Method:** Interviews with 30 domestic workers in the United States and thematic analysis of their experiences with digital tools.

**Key Contributions:**

	1. Examination of domestic workers' use of online technologies
	2. Identification of challenges and opportunities in current digital tools
	3. Recommendations for more suitable technology designs for domestic work.

**Result:** Findings highlight the limited transformative role of current technologies and characterize workers' approaches and avoidance of these tools.

**Limitations:** Focuses on a limited sample of 30 workers in the US; findings may not generalize to other contexts or populations.

**Conclusion:** The study identifies challenges and opportunities in technology use, which could inform better design for tools to support domestic workers.

**Abstract:** Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work practices, perceptions, and relationships with customers and employers. We interviewed 30 domestic workers residing in the United States, who provided examples that highlight the insufficient transformative role of current online technologies in their work. By conducting a thematic analysis, we characterize how they approach and avoid these digital tools at different stages of their work. Through these findings, we investigate the limitations of technology and identify challenges and opportunities that could inform the design of more suitable tools to improve the conditions of this marginalized group.

</details>


### [20] [Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students](https://arxiv.org/abs/2504.10961)

*Audrey Zhang, Yifei Gao, Wannapon Suraworachet, Tanya Nazaretsky, Mutlu Cukurova*

**Main category:** cs.HC

**Keywords:** Generative AI, Large Language Models, Feedback, Higher Education, AI Literacy

**Relevance Score:** 8

**TL;DR:** This study investigates undergraduate students' perceptions of feedback from large language models (LLMs), human sources, and human-AI co-produced sources, revealing biases based on source disclosure.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the gap in understanding student perceptions of various feedback sources in higher education, particularly as generative AI models are incorporated into educational practices.

**Method:** A within-subject experimental design involving 91 undergraduate participants was used to compare trust levels in LLM, human, and co-produced feedback, assessing factors such as feedback quality perception and biases according to the source.

**Key Contributions:**

	1. Investigates student trust in AI vs. human feedback sources
	2. Identifies biases in feedback perception based on source disclosure
	3. Emphasizes the need for feedback and AI literacy in higher education

**Result:** Findings indicated that students preferred AI and co-produced feedback over human feedback when the source was blinded, but expressed a bias against AI feedback when the source was revealed. Furthermore, educational AI experience enhanced students' ability to identify LLM-generated feedback and increased trust in feedback types.

**Limitations:** The study is limited to undergraduate students and may not generalize to other educational levels or contexts.

**Conclusion:** The study highlights the significance of source credibility and suggests enhancing feedback and AI literacy to reduce biases in perceptions of AI-generated feedback to facilitate its adoption in education.

**Abstract:** As generative AI models, particularly large language models (LLMs), transform educational feedback practices in higher education (HE) contexts, understanding students' perceptions of different sources of feedback becomes crucial for their effective implementation and adoption. This study addresses a critical gap by comparing undergraduate students' trust in LLM, human, and human-AI co-produced feedback in their authentic HE context. More specifically, through a within-subject experimental design involving 91 participants, we investigated factors that predict students' ability to distinguish between feedback types, their perceptions of feedback quality, and potential biases related to the source of feedback. Findings revealed that when the source was blinded, students generally preferred AI and co-produced feedback over human feedback regarding perceived usefulness and objectivity. However, they presented a strong bias against AI when the source of feedback was disclosed. In addition, only AI feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational AI experience improved students' ability to identify LLM-generated feedback and increased their trust in all types of feedback. More years of students' experience using AI for general purposes were associated with lower perceived usefulness and credibility of feedback. These insights offer substantial evidence of the importance of source credibility and the need to enhance both feedback literacy and AI literacy to mitigate bias in student perceptions for AI-generated feedback to be adopted and impact education.

</details>


### [21] [A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health](https://arxiv.org/abs/2411.02594)

*Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura M. Schwab Reese, Munmun De Choudhury*

**Main category:** cs.HC

**Keywords:** large language models, public health, risk assessment, health communication, information behavior

**Relevance Score:** 9

**TL;DR:** The paper explores the unique challenges and risks of adopting large language models (LLMs) in public health through focus groups with professionals and individuals, presenting a risk taxonomy to guide thoughtful implementation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the potential risks of using LLMs in public health, where misinformation can have severe consequences.

**Method:** Focus groups with public health professionals and individuals with lived experience to identify their concerns regarding LLMs in the context of public health issues.

**Key Contributions:**

	1. Development of a risk taxonomy for LLMs in public health
	2. Identification of four dimensions of risk
	3. Provision of reflection questions for practitioners to evaluate LLM implementation

**Result:** A risk taxonomy was synthesized, highlighting four dimensions of risk associated with LLM use: individual risks, human-centered care risks, information ecosystem risks, and technological accountability risks.

**Limitations:** 

**Conclusion:** The paper emphasizes the need for a risk-reflexive approach in adopting LLMs in public health, providing a framework for collaboration between computing and public health fields to mitigate potential harms.

**Abstract:** Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models](https://arxiv.org/abs/2508.08262)

*Alaa Alhamzeh, Mays Al Rebdawi*

**Main category:** cs.CL

**Keywords:** Financial Communications, Argument Quality, Gender Bias, LLMs, Annotation Methodologies

**Relevance Score:** 8

**TL;DR:** This paper evaluates the argument quality in financial communications using state-of-the-art LLMs and introduces an adversarial attack to assess gender bias.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The quality of financial arguments influences investment decisions and public trust, yet their assessment is underexplored.

**Method:** The study assesses the LLMs (GPT-4o, Llama 3.1, Gemma 2) by annotating financial communications with the FinArgQuality dataset and compares LLM-generated annotations to human annotations across different temperature settings.

**Key Contributions:**

	1. Evaluation of LLM-generated annotations against human benchmarks
	2. Introduction of an adversarial attack to explore gender bias
	3. Analysis of temperature settings on annotation stability

**Result:** LLM-generated annotations showed higher inter-annotator agreement compared to human annotations, though they still displayed varying levels of gender bias.

**Limitations:** The extent of gender bias in LLMs needs further investigation; models may still lack robustness in biased scenarios.

**Conclusion:** The findings suggest that while LLMs can provide reliable annotations, there is a need for caution regarding bias, leading to recommendations for future research in annotation methodologies.

**Abstract:** Financial arguments play a critical role in shaping investment decisions and public trust in financial institutions. Nevertheless, assessing their quality remains poorly studied in the literature. In this paper, we examine the capabilities of three state-of-the-art LLMs GPT-4o, Llama 3.1, and Gemma 2 in annotating argument quality within financial communications, using the FinArgQuality dataset. Our contributions are twofold. First, we evaluate the consistency of LLM-generated annotations across multiple runs and benchmark them against human annotations. Second, we introduce an adversarial attack designed to inject gender bias to analyse models responds and ensure model's fairness and robustness. Both experiments are conducted across three temperature settings to assess their influence on annotation stability and alignment with human labels. Our findings reveal that LLM-based annotations achieve higher inter-annotator agreement than human counterparts, though the models still exhibit varying degrees of gender bias. We provide a multifaceted analysis of these outcomes and offer practical recommendations to guide future research toward more reliable, cost-effective, and bias-aware annotation methodologies.

</details>


### [23] [TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection](https://arxiv.org/abs/2508.08265)

*Tarık Saraç, Selin Mergen, Mucahid Kutlu*

**Main category:** cs.CL

**Keywords:** scientific discourse, large language models, debate method, social media analysis, CheckThat! 2025

**Relevance Score:** 7

**TL;DR:** This paper presents a novel council debate method using multiple LLMs for detecting scientific discourse on Twitter, achieving top performance in identifying references to scientific studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to enhance the detection of scientific claims, references to studies, and mentions of scientific entities in web discourse.

**Method:** The study introduces three debating frameworks: single debate, team debate, and council debate, with the latter being the primary model as it showed superior performance.

**Key Contributions:**

	1. Development of a council debate framework for LLMs
	2. Demonstration of effective collaboration among LLMs in discourse detection
	3. Top performance achieved in reference identification of scientific studies

**Result:** The council debate method excelled in detecting references to scientific studies, while it was less effective for identifying scientific claims and entity mentions, ranking 8th and 9th respectively.

**Limitations:** Limited ranking performance for scientific claims and entity mentions compared to study references.

**Conclusion:** The council debate method provides a promising approach for enhancing scientific discourse detection but highlights areas for further improvement in other categories.

**Abstract:** In this paper, we present our work developed for the scientific web discourse detection task (Task 4a) of CheckThat! 2025. We propose a novel council debate method that simulates structured academic discussions among multiple large language models (LLMs) to identify whether a given tweet contains (i) a scientific claim, (ii) a reference to a scientific study, or (iii) mentions of scientific entities. We explore three debating methods: i) single debate, where two LLMs argue for opposing positions while a third acts as a judge; ii) team debate, in which multiple models collaborate within each side of the debate; and iii) council debate, where multiple expert models deliberate together to reach a consensus, moderated by a chairperson model. We choose council debate as our primary model as it outperforms others in the development test set. Although our proposed method did not rank highly for identifying scientific claims (8th out of 10) or mentions of scientific entities (9th out of 10), it ranked first in detecting references to scientific studies.

</details>


### [24] [Heartificial Intelligence: Exploring Empathy in Language Models](https://arxiv.org/abs/2508.08271)

*Victoria Williams, Benjamin Rosman*

**Main category:** cs.CL

**Keywords:** language models, empathy, human-computer interaction, virtual companionship, emotional support

**Relevance Score:** 8

**TL;DR:** This study evaluates cognitive and affective empathy in small and large language models, showing LLMs excel at cognitive empathy tasks but lag in affective empathy compared to humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how language models can simulate human empathy, particularly in roles as virtual companions.

**Method:** Psychological tests were applied to assess cognitive and affective empathy in small (SLMs) and large (LLMs) language models and human participants.

**Key Contributions:**

	1. Demonstrated LLMs' superiority in cognitive empathy compared to human participants.
	2. Highlighted the disparity between cognitive and affective empathy in language models.
	3. Provided insights into the implications for using LLMs in emotional support roles.

**Result:** LLMs outperformed humans, including psychology students, on cognitive empathy tasks but exhibited significantly lower affective empathy.

**Limitations:** The study mainly focuses on empathy assessment and does not address practical applications of these findings in real-world scenarios.

**Conclusion:** The findings indicate LLMs' potential for effective virtual companionship and emotional support, while highlighting their limitations in emotional empathy.

**Abstract:** Large language models have become increasingly common, used by millions of people worldwide in both professional and personal contexts. As these models continue to advance, they are frequently serving as virtual assistants and companions. In human interactions, effective communication typically involves two types of empathy: cognitive empathy (understanding others' thoughts and emotions) and affective empathy (emotionally sharing others' feelings). In this study, we investigated both cognitive and affective empathy across several small (SLMs) and large (LLMs) language models using standardized psychological tests. Our results revealed that LLMs consistently outperformed humans - including psychology students - on cognitive empathy tasks. However, despite their cognitive strengths, both small and large language models showed significantly lower affective empathy compared to human participants. These findings highlight rapid advancements in language models' ability to simulate cognitive empathy, suggesting strong potential for providing effective virtual companionship and personalized emotional support. Additionally, their high cognitive yet lower affective empathy allows objective and consistent emotional support without running the risk of emotional fatigue or bias.

</details>


### [25] [Real-time News Story Identification](https://arxiv.org/abs/2508.08272)

*Tadej Škvorc, Nikola Ivačič, Sebastjan Hribar, Marko Robnik-Šikonja*

**Main category:** cs.CL

**Keywords:** story identification, news monitoring, real-time processing, text clustering, topic modeling

**Relevance Score:** 4

**TL;DR:** This paper presents a real-time story identification approach for news monitoring systems to categorize articles based on specific events, places, and people instead of general text similarity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the reading experience on news sites by organizing content into topical collections, thereby facilitating automatic categorization of articles as they are published.

**Method:** The approach combines text representation techniques, clustering algorithms, and online topic modeling methods, utilizing methods like BERTopic, DBStream, and TextClust for effective story identification.

**Key Contributions:**

	1. Real-time story identification method for news articles
	2. Adaptation of online topic-modeling approaches for story discovery
	3. Evaluation using a dataset from Slovene media revealing efficacy of the approach

**Result:** Evaluation on a Slovene media news dataset demonstrates that the real-time story identification approach produces meaningful categorization of news articles.

**Limitations:** 

**Conclusion:** The study concludes that the proposed real-time method for story identification is effective and yields results that align well with human judgment.

**Abstract:** To improve the reading experience, many news sites organize news into topical collections, called stories. In this work, we present an approach for implementing real-time story identification for a news monitoring system that automatically collects news articles as they appear online and processes them in various ways. Story identification aims to assign each news article to a specific story that the article is covering. The process is similar to text clustering and topic modeling, but requires that articles be grouped based on particular events, places, and people, rather than general text similarity (as in clustering) or general (predefined) topics (as in topic modeling). We present an approach to story identification that is capable of functioning in real time, assigning articles to stories as they are published online. In the proposed approach, we combine text representation techniques, clustering algorithms, and online topic modeling methods. We combine various text representation methods to extract specific events and named entities necessary for story identification, showing that a mixture of online topic-modeling approaches such as BERTopic, DBStream, and TextClust can be adapted for story discovery. We evaluate our approach on a news dataset from Slovene media covering a period of 1 month. We show that our real-time approach produces sensible results as judged by human evaluators.

</details>


### [26] [TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning](https://arxiv.org/abs/2508.08273)

*Kristian Miok, Blaz Škrlj, Daniela Zaharie, Marko Robnik Šikonja*

**Main category:** cs.CL

**Keywords:** Clinical language models, Interpretability, Large language models

**Relevance Score:** 9

**TL;DR:** TT-XAI improves clinical language model interpretability and classification performance using keyword distillation and LLM reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of trustworthiness and interpretability in clinical language models applied to lengthy electronic health records.

**Method:** The method involves distilling raw discharge notes into concise keyword representations and generating chain-of-thought clinical explanations with LLMs, evaluated through various fidelity metrics and human assessments.

**Key Contributions:**

	1. Introduction of TT-XAI framework
	2. Use of domain-aware keyword distillation
	3. Improved interpretability and performance in clinical AI applications

**Result:** Keyword distillation enhances BERT classifier performance and improves explanation fidelity, as all evaluation methods favored the keyword-augmented approach.

**Limitations:** 

**Conclusion:** TT-XAI provides a scalable way to enhance the interpretability and trustworthiness of AI models in clinical decision support.

**Abstract:** Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.

</details>


### [27] [Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition](https://arxiv.org/abs/2508.08274)

*Roberto Labadie-Tamayo, Djordje Slijepčević, Xihui Chen, Adrian Jaques Böck, Andreas Babic, Liz Freimann, Christiane Atzmüller Matthias Zeppelzauer*

**Main category:** cs.CL

**Keywords:** hate speech, counter speech recognition, adjective-based representations, interpretability, large language models

**Relevance Score:** 7

**TL;DR:** This paper introduces a transparent method called the Speech Concept Bottleneck Model (SCBM) for automated hate and counter speech recognition, utilizing adjectives as interpretable concepts, achieving high accuracy and interpretability across multiple datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing prevalence of hate speech on social media and the need for effective automated detection methods that also provide interpretability.

**Method:** The SCBM uses large language models to transform input texts into an abstract adjective-based representation, which is then classified using a lightweight classifier. It is tested on five benchmark datasets in multiple languages and platforms.

**Key Contributions:**

	1. Introduction of the Speech Concept Bottleneck Model (SCBM) for hate speech detection.
	2. Use of adjective-based representations for improved interpretability in automated classification.
	3. Demonstration of competitive performance on multiple datasets while providing local and global interpretability.

**Result:** SCBM achieved an average macro-F1 score of 0.69 and outperformed existing models on four out of five datasets, demonstrating both high accuracy and interpretability.

**Limitations:** 

**Conclusion:** The adjective-based representations not only enhance the performance in hate speech detection but can also be adapted for various other NLP tasks, showcasing their versatility and effectiveness.

**Abstract:** The rapid increase in hate speech on social media has exposed an unprecedented impact on society, making automated methods for detecting such content important. Unlike prior black-box models, we propose a novel transparent method for automated hate and counter speech recognition, i.e., "Speech Concept Bottleneck Model" (SCBM), using adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to map input texts to an abstract adjective-based representation, which is then sent to a light-weight classifier for downstream tasks. Across five benchmark datasets spanning multiple languages and platforms (e.g., Twitter, Reddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which outperforms the most recently reported results from the literature on four out of five datasets. Aside from high recognition accuracy, SCBM provides a high level of both local and global interpretability. Furthermore, fusing our adjective-based concept representation with transformer embeddings, leads to a 1.8% performance increase on average across all datasets, showing that the proposed representation captures complementary information. Our results demonstrate that adjective-based concept representations can serve as compact, interpretable, and effective encodings for hate and counter speech recognition. With adapted adjectives, our method can also be applied to other NLP tasks.

</details>


### [28] [MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis](https://arxiv.org/abs/2508.08275)

*Haiyun Guo, ZhiYan Hou, Yu Chen, Jinghan He, Yandu Sun, Yuzhe Zhou, Shujing Guo, Kuan Zhu, Jinqiao Wang*

**Main category:** cs.CL

**Keywords:** Multimodal, Large Language Models, Continual Learning, Benchmarking, Instruction Tuning

**Relevance Score:** 8

**TL;DR:** Introduction of MLLM-CTBench, a benchmark for evaluating continual instruction tuning of Multimodal Large Language Models (MLLMs) with a focus on multidimensional evaluation, algorithm comparisons, and curated tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of rigorous benchmarking in continual instruction tuning for MLLMs, which is essential for adapting to real-world application demands.

**Method:** Introduced MLLM-CTBench that evaluates MLLMs based on accuracy, reasoning quality, and compares eight continual learning algorithms over twelve organized task datasets.

**Key Contributions:**

	1. Multidimensional Evaluation combining accuracy and CoT reasoning assessment
	2. Systematic comparison of eight continual learning algorithms
	3. Carefully curated tasks from multiple challenging domains

**Result:** Findings reveal that models with enhanced general capabilities show resilience to forgetting, reasoning chains diminish slower than answers, and continual learning effectiveness is linked to model capability and task order.

**Limitations:** 

**Conclusion:** MLLM-CTBench sets a robust standard for continual instruction tuning in MLLMs, providing insights for future algorithm design and evaluation.

**Abstract:** Multimodal Large Language Models (MLLMs) rely on continual instruction tuning to adapt to the evolving demands of real-world applications. However, progress in this area is hindered by the lack of rigorous and systematic benchmarks. To address this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark with three key contributions: (1) Multidimensional Evaluation: We combine final answer accuracy with fine-grained CoT reasoning quality assessment, enabled by a specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms and Training Paradigms: We benchmark eight continual learning algorithms across four major categories and systematically compare reinforcement learning with supervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and organize 16 datasets from existing work, covering six challenging domains. Our key findings include: (i) Models with stronger general capabilities exhibit greater robustness to forgetting during continual learning; (ii) Reasoning chains degrade more slowly than final answers, supporting the hierarchical forgetting hypothesis; (iii) The effectiveness of continual learning algorithms is highly dependent on both model capability and task order; (iv) In reinforcement learning settings, incorporating KL-divergence constraints helps maintain policy stability and plays a crucial role in mitigating forgetting. MLLM-CTBench establishes a rigorous standard for continual instruction tuning of MLLMs and offers practical guidance for algorithm design and evaluation.

</details>


### [29] [Evaluating Contrast Localizer for Identifying Causal Unitsin Social & Mathematical Tasks in Language Models](https://arxiv.org/abs/2508.08276)

*Yassine Jamaa, Badr AlKhamissi, Satrajit Ghosh, Martin Schrimpf*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Mathematical reasoning, Contrast localizer, Language models, Causal relevance

**Relevance Score:** 8

**TL;DR:** This paper investigates the causal relevance of units in LLMs and VLMs for Theory of Mind and mathematical reasoning tasks using a neuroscientific approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the causal relevance of specific units in AI models related to Theory of Mind and mathematical reasoning, challenging existing notions of localizers.

**Method:** Adapted a neuroscientific contrast localizer to identify and assess units in 11 LLMs and 5 VLMs through targeted ablations and performance benchmarking.

**Key Contributions:**

	1. Demonstrated a causal analysis approach using neuroscientific techniques in AI models
	2. Revealed unexpected performance impacts of low-activation units
	3. Questioned the efficacy of current localizers for determining unit relevance

**Result:** Findings showed that low-activation units sometimes caused larger performance drops than high-activation ones, questioning the reliability of contrast-based localizers.

**Limitations:** Focus on a limited set of models and tasks; results may not generalize to all scenarios.

**Conclusion:** The study suggests that task-specific units may not be accurately captured and advocates for more diverse stimulus sets in future research.

**Abstract:** This work adapts a neuroscientific contrast localizer to pinpoint causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated units using contrastive stimulus sets and assess their causal role via targeted ablations. We compare the effect of lesioning functionally selected units against low-activation and randomly selected units on downstream accuracy across established ToM and mathematical benchmarks. Contrary to expectations, low-activation units sometimes produced larger performance drops than the highly activated ones, and units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer. These findings call into question the causal relevance of contrast-based localizers and highlight the need for broader stimulus sets and more accurately capture task-specific units.

</details>


### [30] [Objective Metrics for Evaluating Large Language Models Using External Data Sources](https://arxiv.org/abs/2508.08277)

*Haoze Du, Richard Li, Edward Gehringer*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evaluation Framework, Subjective Metrics, Automation, Performance Assessment

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for objectively evaluating Large Language Models (LLMs) using subjective metrics from educational materials, ensuring transparent and consistent assessment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance evaluation of Large Language Models (LLMs) by minimizing subjectivity and enhancing consistency in assessments.

**Method:** The framework utilizes subjective metrics from class materials, established benchmarks, and structured evaluation pipelines to assess LLM outputs across various tasks.

**Key Contributions:**

	1. Introduction of a framework for objective LLM evaluation using subjective metrics
	2. Emphasis on automation and transparency in scoring methodology
	3. Development of a scalable solution for performance assessment across different domains

**Result:** The proposed method enables scalable, automated, and transparent performance assessments while addressing the limitations of traditional subjective evaluation methods.

**Limitations:** The framework may still depend on the quality of the underlying textual materials and defined metrics.

**Conclusion:** This framework provides a reliable and efficient approach for measuring LLM performance in educational and high-stakes contexts, reducing human bias.

**Abstract:** Evaluating the performance of Large Language Models (LLMs) is a critical yet challenging task, particularly when aiming to avoid subjective assessments. This paper proposes a framework for leveraging subjective metrics derived from the class textual materials across different semesters to assess LLM outputs across various tasks. By utilizing well-defined benchmarks, factual datasets, and structured evaluation pipelines, the approach ensures consistent, reproducible, and bias-minimized measurements. The framework emphasizes automation and transparency in scoring, reducing reliance on human interpretation while ensuring alignment with real-world applications. This method addresses the limitations of subjective evaluation methods, providing a scalable solution for performance assessment in educational, scientific, and other high-stakes domains.

</details>


### [31] [MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language](https://arxiv.org/abs/2508.08283)

*Andres Garcia Rincon, Eliseo Ferrante*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-agent systems, Behavior Trees, Synthetic dataset generation, Natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces MinionsLLM, a framework that combines LLMs with Behavior Trees and Formal Grammars for natural language control of multi-agent systems, achieving notable improvements in performance and syntactic validity.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance natural language control of multi-agent systems through the integration of LLMs, while providing standardized interfaces and synthetic dataset generation methods.

**Method:** Combines LLMs, Behavior Trees, and Formal Grammars; develops two synthetic dataset generation methods to fine-tune LLMs for better performance; validated using Google's Gemma 3 model family.

**Key Contributions:**

	1. Introduction of MinionsLLM framework for multi-agent systems
	2. Development of synthetic dataset generation methods
	3. Open-source release to promote reproducibility

**Result:** Achieved a 92.6% syntactic validity and a 33% mean task performance improvement over baseline across different parameter scales.

**Limitations:** 

**Conclusion:** Smaller models show the greatest improvement from fine-tuning, indicating potential for effective deployment in constrained environments.

**Abstract:** This paper presents MinionsLLM, a novel framework that integrates Large Language Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable natural language control of multi-agent systems within arbitrary, user-defined environments. MinionsLLM provides standardized interfaces for defining environments, agents, and behavioral primitives, and introduces two synthetic dataset generation methods (Method A and Method B) to fine-tune LLMs for improved syntactic validity and semantic task relevance. We validate our approach using Google's Gemma 3 model family at three parameter scales (1B, 4B, and 12B) and demonstrate substantial gains: Method B increases syntactic validity to 92.6% and achieves a mean task performance improvement of 33% over baseline. Notably, our experiments show that smaller models benefit most from fine-tuning, suggesting promising directions for deploying compact, locally hosted LLMs in resource-constrained multi-agent control scenarios. The framework and all resources are released open-source to support reproducibility and future research.

</details>


### [32] [The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs](https://arxiv.org/abs/2508.08285)

*Denis Janiak, Jakub Binkowski, Albert Sawczyn, Bogdan Gabrys, Ravid Schwartz-Ziv, Tomasz Kajdanowicz*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Detection, Evaluation Metrics, Natural Language Processing, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper critiques the use of ROUGE for evaluating hallucination detection in large language models (LLMs), highlighting a misalignment with human judgements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by hallucinations in LLMs and improve the evaluation of detection methods.

**Method:** Comprehensive human studies assessing the effectiveness of hallucination detection methods using both ROUGE and human-aligned metrics.

**Key Contributions:**

	1. Critique of ROUGE as an evaluation metric for hallucination detection
	2. Presentation of human-aligned evaluation metrics like LLM-as-Judge
	3. Demonstration that simple heuristics can outperform complex detection techniques.

**Result:** ROUGE shows high recall but low precision, leading to misleading performance estimates; several detection methods dropped performance by up to 45.9% when evaluated with human-aligned metrics.

**Limitations:** Focus primarily on evaluation methodologies without proposing new detection techniques.

**Conclusion:** There is a need for semantically aware evaluation frameworks to better assess hallucination detection methods and ensure LLM output trustworthiness.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing, yet their tendency to hallucinate poses serious challenges for reliable deployment. Despite numerous hallucination detection methods, their evaluations often rely on ROUGE, a metric based on lexical overlap that misaligns with human judgments. Through comprehensive human studies, we demonstrate that while ROUGE exhibits high recall, its extremely low precision leads to misleading performance estimates. In fact, several established detection methods show performance drops of up to 45.9\% when assessed using human-aligned metrics like LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based on response length can rival complex detection techniques, exposing a fundamental flaw in current evaluation practices. We argue that adopting semantically aware and robust evaluation frameworks is essential to accurately gauge the true performance of hallucination detection methods, ultimately ensuring the trustworthiness of LLM outputs.

</details>


### [33] [Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions](https://arxiv.org/abs/2508.08287)

*Farah Atif, Nursultan Askarbekuly, Kareem Darwish, Monojit Choudhury*

**Main category:** cs.CL

**Keywords:** Large Language Models, Islamic Jurisprudence, Abstention Behavior, Benchmark, Accuracy

**Relevance Score:** 4

**TL;DR:** This paper introduces FiqhQA, a benchmark for evaluating LLMs on Islamic rulings and their ability to abstain from answering inappropriately across Sunni schools of thought.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of evaluation on LLMs in religious domains, particularly regarding their reliability and acknowledgment of when not to answer questions.

**Method:** Zero-shot and abstention experiments were conducted to test LLMs' accuracy and their behavior in abstaining from providing answers when necessary.

**Key Contributions:**

	1. Introduction of the FiqhQA benchmark for Islamic jurisprudence
	2. Evaluation of LLMs' abstention behaviors in religious contexts
	3. Identification of performance disparities between languages and models.

**Result:** GPT-4o showed the highest accuracy, while Gemini and Fanar had better abstention behavior. All models performed worse in Arabic, indicating challenges in non-English language contexts.

**Limitations:** The performance of models drops in Arabic, indicating limitations in LLM’s religious reasoning abilities for non-English contexts.

**Conclusion:** The study highlights the need for task-specific evaluations of LLMs in religious settings and cautions against their deployment without thorough assessments.

**Abstract:** Despite the increasing usage of Large Language Models (LLMs) in answering questions in a variety of domains, their reliability and accuracy remain unexamined for a plethora of domains including the religious domains. In this paper, we introduce a novel benchmark FiqhQA focused on the LLM generated Islamic rulings explicitly categorized by the four major Sunni schools of thought, in both Arabic and English. Unlike prior work, which either overlooks the distinctions between religious school of thought or fails to evaluate abstention behavior, we assess LLMs not only on their accuracy but also on their ability to recognize when not to answer. Our zero-shot and abstention experiments reveal significant variation across LLMs, languages, and legal schools of thought. While GPT-4o outperforms all other models in accuracy, Gemini and Fanar demonstrate superior abstention behavior critical for minimizing confident incorrect answers. Notably, all models exhibit a performance drop in Arabic, highlighting the limitations in religious reasoning for languages other than English. To the best of our knowledge, this is the first study to benchmark the efficacy of LLMs for fine-grained Islamic school of thought specific ruling generation and to evaluate abstention for Islamic jurisprudence queries. Our findings underscore the need for task-specific evaluation and cautious deployment of LLMs in religious applications.

</details>


### [34] [Heartificial Intelligence: Exploring Empathy in Language Models](https://arxiv.org/abs/2508.08271)

*Victoria Williams, Benjamin Rosman*

**Main category:** cs.CL

**Keywords:** Large language models, Cognitive empathy, Affective empathy, Human-computer interaction, Virtual companionship

**Relevance Score:** 8

**TL;DR:** This study investigates cognitive and affective empathy capabilities in small and large language models, showing LLMs excel in cognitive empathy but lag in affective empathy compared to humans.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how advancements in large language models impact their ability to perform empathetic communication as virtual assistants.

**Method:** Psychological tests for cognitive and affective empathy were administered to both language models and human participants to compare performance.

**Key Contributions:**

	1. Demonstrated LLMs' superiority in cognitive empathy tasks compared to humans.
	2. Highlighted the emotional support potential of LLMs while acknowledging their limits in affective empathy.
	3. Revealed that LLMs can offer consistent emotional backing devoid of bias and fatigue.

**Result:** LLMs outperformed humans in cognitive empathy tasks, while both SLMs and LLMs showed significantly lower affective empathy than human participants.

**Limitations:** The study primarily focused on empathy testing and did not explore other facets of emotional interactions or practical applications in virtual companionship.

**Conclusion:** LLMs possess strong potential for virtual companionship through cognitive empathy but lack in affecting shared emotional understanding; they can provide objective emotional support without fatigue.

**Abstract:** Large language models have become increasingly common, used by millions of people worldwide in both professional and personal contexts. As these models continue to advance, they are frequently serving as virtual assistants and companions. In human interactions, effective communication typically involves two types of empathy: cognitive empathy (understanding others' thoughts and emotions) and affective empathy (emotionally sharing others' feelings). In this study, we investigated both cognitive and affective empathy across several small (SLMs) and large (LLMs) language models using standardized psychological tests. Our results revealed that LLMs consistently outperformed humans - including psychology students - on cognitive empathy tasks. However, despite their cognitive strengths, both small and large language models showed significantly lower affective empathy compared to human participants. These findings highlight rapid advancements in language models' ability to simulate cognitive empathy, suggesting strong potential for providing effective virtual companionship and personalized emotional support. Additionally, their high cognitive yet lower affective empathy allows objective and consistent emotional support without running the risk of emotional fatigue or bias.

</details>


### [35] [Putnam-AXIOM: A Functional and Static Benchmark](https://arxiv.org/abs/2508.08292)

*Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno Dumont, Elyas Obbad, Sanmi Koyejo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Benchmarking

**Relevance Score:** 6

**TL;DR:** A new benchmark called Putnam-AXIOM is introduced to assess mathematical reasoning in large language models (LLMs), providing a contamination-resilient evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for LLMs in mathematical reasoning are nearing saturation and may be compromised by training-set contamination, necessitating a more robust evaluation method.

**Method:** Putnam-AXIOM includes 522 competition problems from the William Lowell Putnam Mathematical Competition and 100 functional variants generated through perturbation, allowing for an unlimited number of unseen instances to prevent memorization.

**Key Contributions:**

	1. Introduction of a new benchmark for LLMs in mathematical reasoning
	2. Dynamic variations of problems to reduce contamination issues
	3. Implementation of Teacher-Forced Accuracy for scoring reasoning traces.

**Result:** The strongest model, OpenAI's o1-preview, scores 41.9% accuracy on the original set but drops to 22.3% on the variations, indicating significant challenges in reasoning and the impact of dynamic benchmarks.

**Limitations:** 

**Conclusion:** Putnam-AXIOM offers an effective framework for evaluating LLMs in mathematical reasoning by mitigating risks from training-set contamination, enhancing the understanding of model capabilities.

**Abstract:** Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.

</details>


### [36] [CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation](https://arxiv.org/abs/2508.08386)

*Shuzhou Yuan, William LaCroix, Hardik Ghoshal, Ercong Nie, Michael Färber*

**Main category:** cs.CL

**Keywords:** Large Language Models, Education, AI Tutoring, CoDAE, Data Augmentation

**Relevance Score:** 9

**TL;DR:** This paper presents CoDAE, a framework for adapting large language models for educational purposes, enhancing their effectiveness as AI tutors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of off-the-shelf large language models in educational settings, addressing limitations such as readiness to reveal answers, poor adaptivity, and vulnerability to emotional manipulation.

**Method:** The authors introduce Chain-of-Thought (CoT) data augmentation, enrich real-world dialogues with pedagogical guidance, and fine-tune four open-source LLMs using augmented datasets.

**Key Contributions:**

	1. Introduction of CoDAE framework for educational LLM adaptation
	2. Augmented datasets using Chain-of-Thought prompting
	3. Improved resistance to answer disclosure and better pedagogical alignment

**Result:** Models fine-tuned with CoDAE provide better pedagogical guidance, support reasoning processes, and resist premature answer disclosure in simulated educational scenarios.

**Limitations:** The study's findings are based on simulated scenarios; real-world effectiveness needs further validation.

**Conclusion:** The CoDAE framework significantly enhances the educational capabilities of LLMs, making them more effective AI tutors.

**Abstract:** Large Language Models (LLMs) are increasingly employed as AI tutors due to their scalability and potential for personalized instruction. However, off-the-shelf LLMs often underperform in educational settings: they frequently reveal answers too readily, fail to adapt their responses to student uncertainty, and remain vulnerable to emotionally manipulative prompts. To address these challenges, we introduce CoDAE, a framework that adapts LLMs for educational use through Chain-of-Thought (CoT) data augmentation. We collect real-world dialogues between students and a ChatGPT-based tutor and enrich them using CoT prompting to promote step-by-step reasoning and pedagogically aligned guidance. Furthermore, we design targeted dialogue cases to explicitly mitigate three key limitations: over-compliance, low response adaptivity, and threat vulnerability. We fine-tune four open-source LLMs on different variants of the augmented datasets and evaluate them in simulated educational scenarios using both automatic metrics and LLM-as-a-judge assessments. Our results show that models fine-tuned with CoDAE deliver more pedagogically appropriate guidance, better support reasoning processes, and effectively resist premature answer disclosure.

</details>


### [37] [Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery](https://arxiv.org/abs/2508.08401)

*Jiatong Li, Weida Wang, Qinggang Zhang, Junxian Li, Di Zhang, Changmeng Zheng, Shufei Zhang, Xiaoyong Wei, Qing Li*

**Main category:** cs.CL

**Keywords:** large language models, molecule discovery, reasoning performance, explainability, machine learning

**Relevance Score:** 9

**TL;DR:** Mol-R1 is a novel framework designed to enhance the explainability and reasoning performance of Explicit Long-CoT reasoning LLMs for text-based molecule generation, addressing limitations in knowledge-intensive domains like molecule discovery.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning capabilities of LLMs in molecule discovery, which requires a deep understanding of complex domain knowledge and suffers from low efficiency and limited ability in current models.

**Method:** The framework uses a high-quality reasoning dataset generated through Prior Regulation via In-context Distillation (PRID) and employs a training strategy called Molecular Iterative Adaptation (MoIA) that combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO).

**Key Contributions:**

	1. Introduction of the Mol-R1 framework for molecule generation using LLMs.
	2. Development of the PRID methodology for enhancing reasoning datasets.
	3. Implementation of MoIA to improve model training specific to molecular discovery.

**Result:** Mol-R1 demonstrates superior performance in text-based molecule reasoning generation tasks compared to existing models.

**Limitations:** The effectiveness of the framework is still subject to the quality and breadth of the underlying dataset and may not generalize well across all chemical domains.

**Conclusion:** The proposed Mol-R1 framework effectively bridges the gap between LLM capabilities and the rigorous demands of molecular data tasks, improving both reasoning and explainability.

**Abstract:** Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce Mol-R1, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our approach begins with a high-quality reasoning dataset curated through Prior Regulation via In-context Distillation (PRID), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce MoIA, Molecular Iterative Adaptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like reasoning models for molecule discovery. Finally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines.

</details>


### [38] [Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment](https://arxiv.org/abs/2508.08424)

*Saketh Reddy Vemula, Dipti Mishra Sharma, Parameswari Krishnamurthy*

**Main category:** cs.CL

**Keywords:** language modeling, tokenization, morphological alignment, downstream tasks, NLP

**Relevance Score:** 6

**TL;DR:** This paper investigates the impacts of morphological alignment in tokenization on language model performance across diverse languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether morphologically aligned tokenization improves performance for languages with complex morphology.

**Method:** A comprehensive evaluation of language models focusing on tokenizer training, finetuning, and downstream task evaluations across Telugu, Hindi, and English.

**Key Contributions:**

	1. Created a dataset for Telugu with gold morpheme segmentations
	2. Demonstrated the significance of tokenizer algorithm over morphological alignment
	3. Showed positive correlation between morphological alignment and performance in syntax tasks

**Result:** Better morphological alignment moderately correlates with performance improvements in syntax-based tasks, but tokenizer algorithm has a more significant impact than morphological alignment.

**Limitations:** 

**Conclusion:** Hybrid tokenizers that incorporate morphological segmentation enhance performance within the BPE framework, while intrinsic metrics did not correlate with downstream performance.

**Abstract:** Prior work on language modeling showed conflicting findings about whether morphologically aligned approaches to tokenization improve performance, particularly for languages with complex morphology. To investigate this, we select a typologically diverse set of languages: Telugu (agglutinative), Hindi (primarily fusional with some agglutination), and English (fusional). We conduct a comprehensive evaluation of language models -- starting from tokenizer training and extending through the finetuning and downstream task evaluation. To account for the consistent performance differences observed across tokenizer variants, we focus on two key factors: morphological alignment and tokenization quality. To assess morphological alignment of tokenizers in Telugu, we create a dataset containing gold morpheme segmentations of 600 derivational and 7000 inflectional word forms.   Our experiments reveal that better morphological alignment correlates positively -- though moderately -- with performance in syntax-based tasks such as Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing. However, we also find that the tokenizer algorithm (Byte-pair Encoding vs. Unigram) plays a more significant role in influencing downstream performance than morphological alignment alone. Naive Unigram tokenizers outperform others across most settings, though hybrid tokenizers that incorporate morphological segmentation significantly improve performance within the BPE framework. In contrast, intrinsic metrics like Corpus Token Count (CTC) and R\'enyi entropy showed no correlation with downstream performance.

</details>


### [39] [Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints](https://arxiv.org/abs/2508.08466)

*Daren Yao, Jinsong Yuan, Ruike Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human Preferences, Adaptive Margin-Sigmoid Loss, APO-hinge-zero, Preference-based Objectives

**Relevance Score:** 7

**TL;DR:** The paper proposes two lightweight DPO-based methods to enhance the alignment of small LLMs with human preferences under performance constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of small LLMs with human preferences, especially in scenarios of underperformance.

**Method:** Introduction of Adaptive Margin-Sigmoid Loss and APO-hinge-zero, utilizing margin-based objectives and selective update mechanisms.

**Key Contributions:**

	1. Introduction of two novel DPO-based models for small LLMs
	2. Demonstration of significant performance improvements in preference alignment
	3. Potential for efficient deployment of LLMs in resource-constrained environments

**Result:** APO-hinge-zero outperformed APO-zero baseline in AlpacaEval with a +2.0 points win rate improvement and showed competitive results in MT-Bench across various categories, particularly in STEM and Humanities.

**Limitations:** 

**Conclusion:** Simple adaptations of preference-based objectives can significantly improve the performance of small LLMs under resource constraints, paving the way for their efficient deployment.

**Abstract:** Small large language models (LLMs) often face difficulties in aligning output to human preferences, particularly when operating under severe performance gaps. In this work, we propose two lightweight DPO-based variants -- Adaptive Margin-Sigmoid Loss and APO-hinge-zero -- to better address underperformance scenarios by introducing margin-based objectives and selective update mechanisms.   Our APO-hinge-zero method, which combines hinge-induced hard-example mining with the chosen-focused optimization of APO-zero, achieves strong results. In AlpacaEval, APO-hinge-zero improves the win rate by +2.0 points and the length-controlled win rate by +1.4 points compared to the APO-zero baseline. In MT-Bench, our methods maintain competitive performance in diverse categories, particularly excelling in STEM and Humanities tasks.   These results demonstrate that simple modifications to preference-based objectives can significantly enhance small LLM alignment under resource constraints, offering a practical path toward more efficient deployment.

</details>


### [40] [Momentum Point-Perplexity Mechanics in Large Language Models](https://arxiv.org/abs/2508.08492)

*Lorenzo Tomaz, Judd Rosenblatt, Thomas Berry Jones, Diogo Schwerz de Lucena*

**Main category:** cs.CL

**Keywords:** language models, hidden states, Jacobian steering, interpretability, energy conservation

**Relevance Score:** 7

**TL;DR:** The paper investigates the dynamics of hidden states in large language models and proposes a method called Jacobian steering to improve token prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how internal hidden states of language models evolve during inference, and to enhance predictability and alignment with human intent.

**Method:** The study analyzes the hidden states of 20 transformer models using a physics-based analogy and develops Jacobian steering to minimally perturb hidden states to favor target tokens.

**Key Contributions:**

	1. Introduced a 'log-Lagrangian' perspective to analyze model behavior.
	2. Developed Jacobian steering as a control method for language models.
	3. Demonstrated improved semantic quality in model outputs using the proposed method.

**Result:** Jacobian steering maintained nearly constant 'energy' in the hidden states and produced higher quality text continuations compared to the models' original outputs.

**Limitations:** 

**Conclusion:** Approaching transformers through a mechanics lens provides insights for interpretability and control of models, potentially enhancing their alignment with human objectives.

**Abstract:** We take a physics-based approach to studying how the internal hidden states of large language models change from token to token during inference. Across 20 open-source transformer models (135M-3B parameters), we find that a quantity combining the rate of change in hidden states and the model's next-token certainty, analogous to energy in physics, remains nearly constant. Random-weight models conserve this "energy" more tightly than pre-trained ones, while training shifts models into a faster, more decisive regime with greater variability. Using this "log-Lagrangian" view, we derive a control method called Jacobian steering, which perturbs hidden states in the minimal way needed to favor a target token. This approach maintained near-constant energy in two tested models and produced continuations rated higher in semantic quality than the models' natural outputs. Viewing transformers through this mechanics lens offers a principled basis for interpretability, anomaly detection, and low-risk steering. This could help make powerful models more predictable and aligned with human intent.

</details>


### [41] [Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression](https://arxiv.org/abs/2508.08509)

*Jadie Adams, Brian Hu, Emily Veenhuis, David Joy, Bharadwaj Ravichandran, Aaron Bray, Anthony Hoogs, Arslan Basharat*

**Main category:** cs.CL

**Keywords:** Large Language Models, Pluralistic Alignment, Few-Shot Learning, Ethical AI, User Preferences

**Relevance Score:** 8

**TL;DR:** This paper introduces a steerable pluralistic model for aligning large language models (LLMs) according to diverse user preferences, using few-shot comparative regression.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current alignment techniques for LLMs primarily focus on scalar rewards, which fail to capture the diverse preferences of users. This paper aims to enhance LLM alignment by developing a model that better reflects individual user preferences.

**Method:** The proposed method employs few-shot comparative regression, leveraging in-context learning to evaluate and adapt to user preferences across multiple fine-grained attributes. Two new benchmarks, adapted from existing datasets, are used for evaluation.

**Key Contributions:**

	1. Introduction of a steerable pluralistic model for LLM alignment
	2. Development of two new benchmarks for evaluating pluralistic alignment
	3. Demonstration of superior performance over existing alignment methods

**Result:** The new model outperforms various baseline and state-of-the-art alignment methods, demonstrating improved interpretability and adaptability across different LLMs and attributes.

**Limitations:** 

**Conclusion:** This research paves the way for more representative and fair use of LLMs, contributing to the advancement of ethical AI by offering new insights into pluralistic alignment.

**Abstract:** Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI.

</details>


### [42] [DeCAL Tokenwise Compression](https://arxiv.org/abs/2508.08514)

*Sameer Panwar*

**Main category:** cs.CL

**Keywords:** tokenwise compression, language model, compressed representations, DeCAL, NLP

**Relevance Score:** 6

**TL;DR:** DeCAL is a new tokenwise compression method leveraging a language model to create high-quality compressed representations, achieving competitive performance on downstream tasks at high compression rates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an effective tokenwise compression method that maximizes compression quality for language models while maintaining performance on various tasks.

**Method:** DeCAL modifies an encoder-decoder language model trained with denoising to generate compressed representations, emphasizing quality over computation cost.

**Key Contributions:**

	1. Introduction of DeCAL for tokenwise compression
	2. Performance matching uncompressed models at 2x compression
	3. Significant savings for tasks utilizing pre-computed dense representations

**Result:** DeCAL achieves competitive results at 2x compression, maintaining performance with minor metric drops at up to 8x compression across tasks like question-answering and summarization.

**Limitations:** Focus on compression quality may increase computational requirements.

**Conclusion:** DeCAL demonstrates strong potential for high-quality compression in language models, with opportunities for broader application.

**Abstract:** This paper introduces DeCAL, a new method for tokenwise compression. DeCAL uses an encoder-decoder language model pretrained with denoising to learn to produce high-quality, general-purpose compressed representations by the encoder. DeCAL applies small modifications to the encoder, with the emphasis on maximizing compression quality, even at the expense of compute. We show that DeCAL at 2x compression can match uncompressed on many downstream tasks, with usually only minor dropoff in metrics up to 8x compression, among question-answering, summarization, and multi-vector retrieval tasks. DeCAL offers significant savings where pre-computed dense representations can be utilized, and we believe the approach can be further developed to be more broadly applicable.

</details>


### [43] [DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives](https://arxiv.org/abs/2508.08591)

*Sehwan Moon, Aram Lee, Jeong Eun Kim, Hee-Ju Kang, Il-Seon Shin, Sung-Wan Kim, Jae-Min Kim, Min Jhon, Ju-Wan Kim*

**Main category:** cs.CL

**Keywords:** Depression Prediction, Large Language Models, Interpretable AI, Health Informatics, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents DepressLLM, a large language model developed to predict depression using a novel dataset of autobiographical narratives, achieving high classification performance and providing interpretable predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of depression prediction due to a lack of large-scale and high-quality annotated datasets.

**Method:** Trained on a dataset of 3,699 autobiographical narratives, DepressLLM employs a Score-guided Token Probability Summation (SToPS) module for improved classification performance and confidence estimates.

**Key Contributions:**

	1. Introduction of DepressLLM for depression prediction
	2. Use of a novel corpus of autobiographical narratives
	3. Implementation of the SToPS module for improved prediction and confidence estimates

**Result:** DepressLLM achieved an AUC of 0.789 overall, improving to 0.904 with high-confidence predictions (≥ 0.95).

**Limitations:** Identified model and data limitations through a psychiatric review of misclassifications that suggest future research directions.

**Conclusion:** The study demonstrates that interpretable AI can support earlier depression diagnosis and emphasizes the potential of AI in psychiatric applications.

**Abstract:** Advances in large language models (LLMs) have enabled a wide range of applications. However, depression prediction is hindered by the lack of large-scale, high-quality, and rigorously annotated datasets. This study introduces DepressLLM, trained and evaluated on a novel corpus of 3,699 autobiographical narratives reflecting both happiness and distress. DepressLLM provides interpretable depression predictions and, via its Score-guided Token Probability Summation (SToPS) module, delivers both improved classification performance and reliable confidence estimates, achieving an AUC of 0.789, which rises to 0.904 on samples with confidence $\geq$ 0.95. To validate its robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets, including an Ecological Momentary Assessment (EMA) corpus of daily stress and mood recordings, and on public clinical interview data. Finally, a psychiatric review of high-confidence misclassifications highlighted key model and data limitations that suggest directions for future refinements. These findings demonstrate that interpretable AI can enable earlier diagnosis of depression and underscore the promise of medical AI in psychiatry.

</details>


### [44] [Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review](https://arxiv.org/abs/2508.08610)

*David Santandreu Calonge, Linda Smail*

**Main category:** cs.CL

**Keywords:** Parameter-Efficient Fine-Tuning, Retrieval-Augmented Generation, Low-Rank Adaptation, Cantonese language, domain adaptation

**Relevance Score:** 8

**TL;DR:** This review focuses on recent advances in Parameter-Efficient Fine-Tuning (PEFT) methods for optimizing Retrieval-Augmented Generation (RAG) systems, particularly in the context of Cantonese language processing.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by RAG systems in understanding and generating colloquial Cantonese expressions due to limited annotated data and linguistic variability.

**Method:** The review evaluates the integration of Low-Rank Adaptation (LoRA) within RAG frameworks, benchmarks the accuracy of various PEFT methods, and analyzes domain adaptation strategies with limited data.

**Key Contributions:**

	1. Analysis of LoRA variants in RAG systems.
	2. Evaluation of PEFT methods for dialectal language processing.
	3. Identification of strategies for improving model adaptability and user personalization.

**Result:** Dynamic and ensemble LoRA adaptations can significantly reduce the number of trainable parameters while maintaining both retrieval accuracy and generation quality in dialectal contexts. Nevertheless, challenges remain in preserving fine-grained linguistic nuances, particularly for low-resource languages.

**Limitations:** Limitations remain in preserving fine-grained linguistic nuances in low-resource settings, and the integration of real-time user feedback is underdeveloped.

**Conclusion:** The review emphasizes the potential of PEFT-enhanced RAG systems for domain-specific language tasks and identifies areas for future research, including dialectal authenticity and scalable fine-tuning pipelines.

**Abstract:** This review examines recent advances in Parameter-Efficient Fine-Tuning (PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize Retrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi. These systems face challenges in understanding and generating authentic Cantonese colloquial expressions due to limited annotated data and linguistic variability. The review evaluates the integration of LoRA within RAG frameworks, benchmarks PEFT methods for retrieval and generation accuracy, identify domain adaptation strategies under limited data, and compares fine-tuning techniques aimed at improving semantic fidelity under data-scarce conditions. A systematic analysis of recent studies employing diverse LoRA variants, synthetic data generation, user feedback integration, and adaptive parameter allocation was conducted to assess their impact on computational efficiency, retrieval precision, linguistic authenticity, and scalability. Findings reveal that dynamic and ensemble LoRA adaptations significantly reduce trainable parameters without sacrificing retrieval accuracy and generation quality in dialectal contexts. However, limitations remain in fully preserving fine-grained linguistic nuances, especially for low-resource settings like Cantonese. The integration of real-time user feedback and domain-specific data remains underdeveloped, limiting model adaptability and personalization. While selective parameter freezing and nonlinear adaptation methods offer better trade-offs between efficiency and accuracy, their robustness at scale remains an open challenge. This review highlights the promise of PEFT-enhanced RAG systems for domain-specific language tasks and calls for future work targeting dialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.

</details>


### [45] [InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling](https://arxiv.org/abs/2508.08636)

*Peiji Li, Jiasheng Ye, Yongkang Chen, Yichuan Ma, Zijie Yu, Kedi Chen, Ganqu Cui, Haozhan Li, Jiacheng Chen, Chengqi Lyu, Wenwei Zhang, Linyang Li, Qipeng Guo, Dahua Lin, Bowen Zhou, Kai Chen*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, task environments, synthetic data generation, model evaluation

**Relevance Score:** 9

**TL;DR:** InternBootcamp introduces a framework of 1000+ domain-diverse task environments to enhance large language model (LLM) reasoning capabilities, featuring automated task generation and integrated evaluation modules.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing benchmarks in capturing the diversity of real-world reasoning tasks for large language models (LLMs).

**Method:** An open-source framework with automatic generation of training/testing cases, configurable difficulty levels, and integrated verification modules for objective evaluation.

**Key Contributions:**

	1. Introduction of an open-source framework for LLM reasoning research with 1000+ diverse tasks.
	2. Automated generation of training and testing scenarios with adjustable difficulties.
	3. Development of Bootcamp-EVAL for performance assessment of reasoning capabilities.

**Result:** The framework improves LLM performance, evidenced by a 32B model achieving state-of-the-art results on the Bootcamp-EVAL benchmark by effectively using diverse training tasks.

**Limitations:** 

**Conclusion:** InternBootcamp is a vital infrastructure for optimizing RL-based models, generating synthetic data, and evaluating model performance through comprehensive task coverage.

**Abstract:** Large language models (LLMs) have revolutionized artificial intelligence by enabling complex reasoning capabilities. While recent advancements in reinforcement learning (RL) have primarily focused on domain-specific reasoning tasks (e.g., mathematics or code generation), real-world reasoning scenarios often require models to handle diverse and complex environments that narrow-domain benchmarks cannot fully capture. To address this gap, we present InternBootcamp, an open-source framework comprising 1000+ domain-diverse task environments specifically designed for LLM reasoning research. Our codebase offers two key functionalities: (1) automated generation of unlimited training/testing cases with configurable difficulty levels, and (2) integrated verification modules for objective response evaluation. These features make InternBootcamp fundamental infrastructure for RL-based model optimization, synthetic data generation, and model evaluation. Although manually developing such a framework with enormous task coverage is extremely cumbersome, we accelerate the development procedure through an automated agent workflow supplemented by manual validation protocols, which enables the task scope to expand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an automatically generated benchmark for comprehensive performance assessment. Evaluation reveals that frontier models still underperform in many reasoning tasks, while training with InternBootcamp provides an effective way to significantly improve performance, leading to our 32B model that achieves state-of-the-art results on Bootcamp-EVAL and excels on other established benchmarks. In particular, we validate that consistent performance gains come from including more training tasks, namely \textbf{task scaling}, over two orders of magnitude, offering a promising route towards capable reasoning generalist.

</details>


### [46] [Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents](https://arxiv.org/abs/2508.08645)

*Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang*

**Main category:** cs.CL

**Keywords:** mobile-use agents, human intention alignment, demonstration learning, query rewriting, personalization

**Relevance Score:** 9

**TL;DR:** This paper presents IFRAgent, a framework that improves mobile-use agents' alignment with human intent by considering both explicit and implicit intention flows.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance personalization in mobile-use agents by evaluating their understanding of human intent, especially focusing on both explicit and implicit intention flows.

**Method:** The paper introduces MobileIAR, a dataset for assessing human-intent-aligned actions. IFRAgent analyzes explicit intention flows to create a standard operating procedures library and implicit intention flows for a user-level habit repository, using retrieval-augmented generation to refine queries.

**Key Contributions:**

	1. Introduction of the MobileIAR dataset for human-intent alignment.
	2. Development of the IFRAgent framework that incorporates both explicit and implicit intention flows.
	3. Demonstration of significant performance improvements in human intention alignment and task completion rates.

**Result:** IFRAgent outperforms existing baselines in human intention alignment rate by 6.79% and improves step completion rates by 5.30%.

**Limitations:** 

**Conclusion:** The findings suggest that adequately addressing both explicit and implicit intentions can significantly enhance the performance of mobile-use agents in aligning with human actions.

**Abstract:** As multimodal large language models advance rapidly, the automation of mobile tasks has become increasingly feasible through the use of mobile-use agents that mimic human interactions from graphical user interface. To further enhance mobile-use agents, previous studies employ demonstration learning to improve mobile-use agents from human demonstrations. However, these methods focus solely on the explicit intention flows of humans (e.g., step sequences) while neglecting implicit intention flows (e.g., personal preferences), which makes it difficult to construct personalized mobile-use agents. In this work, to evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset containing human-intent-aligned actions and ground-truth actions. This enables a comprehensive assessment of the agents' understanding of human intent. Then we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention \textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes explicit intention flows from human demonstrations to construct a query-level vector library of standard operating procedures (SOP), and analyzes implicit intention flows to build a user-level habit repository. IFRAgent then leverages a SOP extractor combined with retrieval-augmented generation and a query rewriter to generate personalized query and SOP from a raw ambiguous query, enhancing the alignment between mobile-use agents and human intent. Experimental results demonstrate that IFRAgent outperforms baselines by an average of 6.79\% (32.06\% relative improvement) in human intention alignment rate and improves step completion rates by an average of 5.30\% (26.34\% relative improvement). The codes are available at https://github.com/MadeAgents/Quick-on-the-Uptake.

</details>


### [47] [LLaMA-Based Models for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.08649)

*Jakub Šmíd, Pavel Přibáň, Pavel Král*

**Main category:** cs.CL

**Keywords:** large language models, aspect-based sentiment analysis, LLaMA, fine-tuning, error analysis

**Relevance Score:** 9

**TL;DR:** This paper investigates the performance of fine-tuned large language models (LLMs) for compound aspect-based sentiment analysis (ABSA), specifically focusing on LLaMA-based models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the performance of open-source LLMs fine-tuned for ABSA, as their capabilities in this area remain under-researched.

**Method:** The paper evaluates the performance of LLaMA-based LLMs fine-tuned for ABSA across four tasks and eight English datasets, including error analysis to identify challenges.

**Key Contributions:**

	1. Demonstrated that fine-tuned LLaMA-based models can achieve state-of-the-art results in ABSA tasks.
	2. Conducted comprehensive evaluations across multiple datasets and tasks.
	3. Identified specific challenges faced by fine-tuned models through error analysis.

**Result:** The fine-tuned Orca~2 model outperforms state-of-the-art results in all evaluated tasks, though all models exhibit difficulties in zero-shot and few-shot scenarios.

**Limitations:** Models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned models.

**Conclusion:** Fine-tuning LLMs like Orca~2 shows promise for surpassing existing methods in ABSA, but further investigation is needed to address challenges in zero-shot and few-shot environments.

**Abstract:** While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca~2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.

</details>


### [48] [UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection](https://arxiv.org/abs/2508.08650)

*Jakub Šmíd, Pavel Přibáň, Pavel Král*

**Main category:** cs.CL

**Keywords:** emotion detection, cross-lingual, large language models, machine translation, social media

**Relevance Score:** 8

**TL;DR:** The paper outlines a system developed for cross-lingual emotion detection using fine-tuned large language models and demonstrates strong performance in detecting emotions and trigger words from tweets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of cross-lingual emotion detection in social media content across multiple languages.

**Method:** The approach involves fine-tuning quantized large language models like Orca 2, utilizing low-rank adapters (LoRA) and multilingual Transformer-based models such as XLM-R and mT5, along with machine translation for performance enhancement.

**Key Contributions:**

	1. Fine-tuning of quantized large language models for emotion detection.
	2. Use of low-rank adapters to enhance model performance.
	3. Incorporation of machine translation techniques for improving results.

**Result:** The system ranked 1st in numerical trigger words detection, 3rd in binary trigger words detection, and 7th in overall emotion detection.

**Limitations:** 

**Conclusion:** The proposed method shows significant potential in effectively detecting emotions and relevant trigger words from tweets across different languages, outperforming several benchmarks.

**Abstract:** This paper presents our system built for the WASSA-2024 Cross-lingual Emotion Detection Shared Task. The task consists of two subtasks: first, to assess an emotion label from six possible classes for a given tweet in one of five languages, and second, to predict words triggering the detected emotions in binary and numerical formats. Our proposed approach revolves around fine-tuning quantized large language models, specifically Orca~2, with low-rank adapters (LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We enhance performance through machine translation for both subtasks and trigger word switching for the second subtask. The system achieves excellent performance, ranking 1st in numerical trigger words detection, 3rd in binary trigger words detection, and 7th in emotion detection.

</details>


### [49] [Prompt-Based Approach for Czech Sentiment Analysis](https://arxiv.org/abs/2508.08651)

*Jakub Šmíd, Pavel Přibáň*

**Main category:** cs.CL

**Keywords:** aspect-based sentiment analysis, sentiment classification, prompt-based methods, Czech, zero-shot learning

**Relevance Score:** 6

**TL;DR:** This paper presents the first prompt-based methods for aspect-based sentiment analysis in Czech using sequence-to-sequence models, achieving better results than traditional approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve aspect-based sentiment analysis and sentiment classification in Czech by utilizing prompt-based methods that can perform better with limited training data.

**Method:** The authors employ sequence-to-sequence models for simultaneous aspect-based tasks and conduct zero-shot and few-shot learning experiments to compare their prompt-based technique with traditional fine-tuning methods.

**Key Contributions:**

	1. Introduction of prompt-based methods for Czech sentiment analysis
	2. Demonstration of improved performance in zero-shot and few-shot learning scenarios
	3. Evidence of significant domain pre-training benefits.

**Result:** The prompt-based approach outperforms traditional fine-tuning, showing significant improvements particularly in zero-shot settings when pre-trained on target domain data.

**Limitations:** Focuses specifically on the Czech language, limiting generalizability to other languages.

**Conclusion:** The findings suggest that prompt-based methods are superior for sentiment analysis in low-resource settings, successfully leveraging limited training samples.

**Abstract:** This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario.

</details>


### [50] [LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement](https://arxiv.org/abs/2508.08653)

*Rajmohan C, Sarthak Harne, Arvind Agarwal*

**Main category:** cs.CL

**Keywords:** Large Language Models, text-to-table, iterative self-feedback, task decomposition, structured data

**Relevance Score:** 8

**TL;DR:** This paper presents an efficient LLM-driven system for transforming unstructured text into structured data through novel prompting techniques and task decomposition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges faced by LLMs in converting ambiguous or domain-specific unstructured text into structured formats, particularly tables.

**Method:** The system utilizes novel prompting techniques, breaking down the text-to-table task into manageable sub-tasks and refining the outputs through iterative self-feedback.

**Key Contributions:**

	1. Introduction of task decomposition for LLM-driven text-to-table generation.
	2. Application of iterative self-feedback in the refinement process of generated tables.
	3. Comparative analysis showing enhanced performance against existing methods.

**Result:** The proposed methods show significant improvements in the quality of generated tables compared to existing baselines on two complex datasets.

**Limitations:** The potential risks associated with iterative self-feedback and trade-offs between performance and computational cost need further exploration.

**Conclusion:** The approach demonstrates effective performance enhancements while discussing the trade-offs between the approach's benefits and computational costs.

**Abstract:** Transforming unstructured text into structured data is a complex task, requiring semantic understanding, reasoning, and structural comprehension. While Large Language Models (LLMs) offer potential, they often struggle with handling ambiguous or domain-specific data, maintaining table structure, managing long inputs, and addressing numerical reasoning. This paper proposes an efficient system for LLM-driven text-to-table generation that leverages novel prompting techniques. Specifically, the system incorporates two key strategies: breaking down the text-to-table task into manageable, guided sub-tasks and refining the generated tables through iterative self-feedback. We show that this custom task decomposition allows the model to address the problem in a stepwise manner and improves the quality of the generated table. Furthermore, we discuss the benefits and potential risks associated with iterative self-feedback on the generated tables while highlighting the trade-offs between enhanced performance and computational cost. Our methods achieve strong results compared to baselines on two complex text-to-table generation datasets available in the public domain.

</details>


### [51] [TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation](https://arxiv.org/abs/2508.08680)

*Armel Zebaze, Benoît Sagot, Rachel Bawden*

**Main category:** cs.CL

**Keywords:** machine translation, low-resource languages, in-context learning, LLM, backtranslation

**Relevance Score:** 8

**TL;DR:** TopXGen is an LLM-based method for generating high-quality, topic-diverse parallel data for low-resource languages to improve machine translation performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Machine translation into low-resource languages (LRLs) is inadequate compared to high-resource languages (HRLs). Synthetic parallel data creation, particularly backtranslation, requires quality target-side texts that are scarce for many LRLs.

**Method:** The paper presents TopXGen, which utilizes LLMs to generate natural-sounding target-side texts in LRLs, which can be backtranslated to enhance parallel datasets for improved ICL and fine-tuning.

**Key Contributions:**

	1. Introduction of TopXGen for generating parallel text in low-resource languages
	2. Demonstration of improved translation performance using generated data
	3. Code availability enabling reproducibility and further research.

**Result:** TopXGen significantly boosts the performance of LLMs in translating low-resource languages during both fine-tuning and in-context learning.

**Limitations:** 

**Conclusion:** The approach demonstrates that LLMs can effectively generate useful parallel texts by leveraging their capabilities in high-resource languages, addressing the challenge of limited datasets for low-resource language translation.

**Abstract:** LLMs have been shown to perform well in machine translation (MT) with the use of in-context learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. A common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present \textsc{TopXGen}, an LLM-based approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into a high-resource source language. We show that \textsc{TopXGen} boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ArmelRandy/topxgen.

</details>


### [52] [Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults](https://arxiv.org/abs/2508.08684)

*Bram van Dijk, Tiberon Kuiper, Sirin Aoulad si Ahmed, Armel Levebvre, Jake Johnson, Jan Duin, Simon Mooijaart, Marco Spruit*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Older Adults, Chatbot, ASR Models, Health Informatics

**Relevance Score:** 7

**TL;DR:** This study evaluates automatic speech recognition (ASR) models for older Dutch adults using a chatbot in clinical contexts, revealing that generic multilingual models perform better than fine-tuned models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bottleneck of reliable ASR for underrepresented groups, particularly older adults, in clinical settings.

**Method:** The study benchmarks multi-lingual ASR models against models specifically fine-tuned for the speech of older Dutch adults, assessing performance and processing speed.

**Key Contributions:**

	1. Evaluation of ASR performance on older adults' speech
	2. Comparison between generic and fine-tuned ASR models
	3. Identification of accuracy-speed trade-offs in ASR architectures

**Result:** Generic multilingual ASR models outperform fine-tuned models for older adults, suggesting good generalizability. Truncating models can balance accuracy and processing speed, but some errors were noted due to hallucinations.

**Limitations:** Some high word error rates were observed due to hallucinations in responses.

**Conclusion:** Recent ASR models are effective for older adults, and adjustments can improve their application in geriatric contexts and chatbot interactions.

**Abstract:** Voice-controlled interfaces can support older adults in clinical contexts, with chatbots being a prime example, but reliable Automatic Speech Recognition (ASR) for underrepresented groups remains a bottleneck. This study evaluates state-of-the-art ASR models on language use of older Dutch adults, who interacted with the Welzijn.AI chatbot designed for geriatric contexts. We benchmark generic multilingual ASR models, and models fine-tuned for Dutch spoken by older adults, while also considering processing speed. Our results show that generic multilingual models outperform fine-tuned models, which suggests recent ASR models can generalise well out of the box to realistic datasets. Furthermore, our results suggest that truncating existing architectures is helpful in balancing the accuracy-speed trade-off, though we also identify some cases with high WER due to hallucinations.

</details>


### [53] [A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models](https://arxiv.org/abs/2508.08712)

*Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu*

**Main category:** cs.CL

**Keywords:** parallel text generation, large language models, autoregressive, non-autoregressive, inference efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents a systematic survey of parallel text generation methods, categorizing them into autoregressive and non-autoregressive paradigms, assessing their trade-offs in performance, and outlining future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of autoregressive text generation in large language models by exploring parallel text generation techniques that improve generation speed and efficiency.

**Method:** The authors categorize existing parallel text generation approaches into autoregressive-based and non-autoregressive-based methods, examining core techniques and assessing their trade-offs regarding speed, quality, and efficiency.

**Key Contributions:**

	1. Systematic survey of parallel text generation methods
	2. Categorization of techniques into AR-based and Non-AR-based paradigms
	3. Identification of theoretical trade-offs and future research directions

**Result:** The study categorizes and evaluates various parallel text generation methods, highlighting advancements, open challenges, and future research directions.

**Limitations:** The survey may not cover every emerging parallel generation method and lacks empirical comparison across all recognized techniques.

**Conclusion:** The findings emphasize the need for comprehensive understanding and further exploration of parallel text generation techniques to enhance inference performance in large language models.

**Abstract:** As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation.

</details>


### [54] [IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization](https://arxiv.org/abs/2508.08719)

*Yuzhuo Bai, Shitong Duan, Muhua Huang, Jing Yao, Zhenghao Liu, Peng Zhang, Tun Lu, Xiaoyuan Yi, Maosong Sun, Xing Xie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-like traits, Trait elicitation, Psychological theories, Trait reflection

**Relevance Score:** 9

**TL;DR:** The paper presents IROTE, a method for enhancing Large Language Models' (LLMs) ability to reflect human-like traits consistently across various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the superficial elicitation problem in existing methods for prompting LLMs to embody human-like traits more accurately and consistently.

**Method:** IROTE utilizes a novel in-context approach for stable and transferable trait elicitation by generating and optimizing a textual self-reflection, thereby stimulating LLMs' trait-driven behavior without the need for fine-tuning.

**Key Contributions:**

	1. Introduction of IROTE for generating stable and transferable trait elicitation in LLMs.
	2. Leveraging psychological theories of identity-related reflection for LLM prompting.
	3. Demonstration of significant performance improvements in trait-driven behavior across multiple tasks.

**Result:** Extensive experiments show that IROTE can induce LLMs' stable impersonation of desired traits across diverse tasks, outperforming existing baselines.

**Limitations:** The method is not tested on a wider variety of LLM architectures and may require adjustments for specific applications or traits.

**Conclusion:** The proposed method effectively enhances the connection between LLM behavior and target traits, resulting in better trait reflection in various applications.

**Abstract:** Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs' trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs' behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs' stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.

</details>


### [55] [Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation](https://arxiv.org/abs/2508.08730)

*Weibin Liao, Tianlong Wang, Yinghao Zhu, Yasha Wang, Junyi Gao, Liantao Ma*

**Main category:** cs.CL

**Keywords:** Medical Lay Language Generation, asymmetric LoRA, semantic fidelity

**Relevance Score:** 9

**TL;DR:** Magical is a novel asymmetric LoRA architecture designed for Medical Lay Language Generation (MLLG) that improves semantic fidelity and diverse style generation compared to standard LoRA.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance accessibility of complex scientific content through better Medical Lay Language Generation (MLLG) methods.

**Method:** The paper proposes Magical, which uses a shared matrix for summarization and multiple isolated matrices for style generation, while incorporating a semantic invariance constraint to maintain semantic fidelity and a recommendation-guided switch for style adaptability.

**Key Contributions:**

	1. Introduction of Magical, a new asymmetric LoRA architecture for MLLG.
	2. Incorporation of a Semantic Invariance Constraint to preserve semantic fidelity.
	3. Implementation of Recommendation-guided Switch to adapt to diverse lay styles.

**Result:** Experimental results show that Magical outperforms existing methods in MLLG tasks while reducing the number of trainable parameters by 31.66%.

**Limitations:** 

**Conclusion:** Magical addresses the limitations of traditional LoRA in MLLG by providing a more effective means of generating diverse and semantically accurate lay language.

**Abstract:** Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix $A$ for abstractive summarization, along with multiple isolated matrices $B$ for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix $A$. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices $B$. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%.

</details>


### [56] [ChatBench: From Static Benchmarks to Human-AI Evaluation](https://arxiv.org/abs/2504.07114)

*Serina Chang, Ashton Anderson, Jake M. Hofman*

**Main category:** cs.CL

**Keywords:** LLM, user interaction, machine learning, chatbot evaluation, dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces ChatBench, a dataset designed to evaluate user-AI interactions by converting MMLU benchmark questions into conversational formats, revealing that AI-alone metrics do not predict user-AI performance accurately.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLM-based chatbots, it is vital to assess how humans and AI can work together effectively, rather than measuring AI performance in isolation.

**Method:** A user study was conducted to transform MMLU questions into conversational scenarios where users collaborated with LLMs to find answers, leading to the creation of the ChatBench dataset.

**Key Contributions:**

	1. Introduction of ChatBench, a comprehensive dataset for user-AI interaction evaluation.
	2. Revealing the inadequacy of AI-alone metrics in predicting user-AI interaction success.
	3. Improvement of user simulator accuracy via fine-tuning on real conversational data.

**Result:** The study found that AI-alone accuracy does not predict user-AI accuracy, with substantial variation in performance across different subjects, highlighting the unique dynamics of user-AI conversations.

**Limitations:** Further research needed to address varying contexts in user-AI interactions and expand on the user simulator's adaptability.

**Conclusion:** By fine-tuning a user simulator on ChatBench, the ability to estimate user-AI accuracy significantly improved, suggesting potential for future advancements in interactive evaluation.

**Abstract:** With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.

</details>


### [57] [SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs](https://arxiv.org/abs/2508.08742)

*Haotian Chen, Qingqing Long, Meng Xiao, Xiao Luo, Wei Ju, Chengrui Wang, Xuezhi Wang, Yuanchun Zhou, Hengshu Zhu*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, scientific literature, question answering, evaluation benchmark

**Relevance Score:** 9

**TL;DR:** This paper introduces SciRerankBench, a benchmark for evaluating rerankers in RAG-LLM systems, addressing their potential and limitations in scientific literature question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the advancements and limitations of two-stage retrieval-augmented generated large language models (RAG-LLMs) in scientific literature question answering.

**Method:** Development of the SciRerankBench benchmark for evaluating the performance of rerankers in RAG-LLMs across five scientific subjects using three types of Q-C-A pairs.

**Key Contributions:**

	1. Introduction of SciRerankBench Benchmark for RAG-LLM evaluation
	2. Evaluation of reranker performance across multiple scientific subjects
	3. Insights into strengths and limitations of different rerankers.

**Result:** The systematic evaluation of 13 widely used rerankers on five families of LLMs demonstrates their strengths and limitations in terms of noise resilience, relevance disambiguation, and factual consistency.

**Limitations:** Potential issues with the generalizability of findings beyond the tested models and subjects.

**Conclusion:** SciRerankBench is the first benchmark specifically designed to evaluate rerankers within RAG-LLMs, providing insights and guidance for further development in the field.

**Abstract:** Scientific literature question answering is a pivotal step towards new scientific discoveries. Recently, \textit{two-stage} retrieval-augmented generated large language models (RAG-LLMs) have shown impressive advancements in this domain. Such a two-stage framework, especially the second stage (reranker), is particularly essential in the scientific domain, where subtle differences in terminology may have a greatly negative impact on the final factual-oriented or knowledge-intensive answers. Despite this significant progress, the potential and limitations of these works remain unexplored. In this work, we present a Scientific Rerank-oriented RAG Benchmark (SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning five scientific subjects. To rigorously assess the reranker performance in terms of noise resilience, relevance disambiguation, and factual consistency, we develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy Contexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI), and Counterfactual Contexts (CC). Through systematic evaluation of 13 widely used rerankers on five families of LLMs, we provide detailed insights into their relative strengths and limitations. To the best of our knowledge, SciRerankBench is the first benchmark specifically developed to evaluate rerankers within RAG-LLMs, which provides valuable observations and guidance for their future development.

</details>


### [58] [DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation](https://arxiv.org/abs/2508.08761)

*Stavros Doropoulos, Stavros Vologiannidis, Ioannis Magnisalis*

**Main category:** cs.CL

**Keywords:** Large Language Model, team dialogue, information systems management, automated task formalization, public benchmark dataset

**Relevance Score:** 8

**TL;DR:** DevNous is an LLM-based system that automates translating unstructured team dialogue into structured IT project governance artifacts, achieving high accuracy on a new benchmark.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bottleneck in translating unstructured team dialogues into structured formats necessary for IT project governance.

**Method:** DevNous uses a multi-agent expert system to integrate into team chat environments, extracting actionable intents from informal dialogues and managing workflows for tasks like task formalization and progress synthesis.

**Key Contributions:**

	1. Validated architectural pattern for developing ambient administrative agents
	2. First robust empirical baseline for the unstructured-to-structured dialogue translation problem
	3. Public benchmark dataset for evaluating similar systems

**Result:** DevNous achieved an exact match turn accuracy of 81.3% and a multiset F1-Score of 0.845 on a new benchmark with 160 annotated conversational turns.

**Limitations:** 

**Conclusion:** DevNous provides a validated architectural pattern for ambient administrative agents and offers a public benchmark dataset for further research in this field.

**Abstract:** The manual translation of unstructured team dialogue into the structured artifacts required for Information Technology (IT) project governance is a critical bottleneck in modern information systems management. We introduce DevNous, a Large Language Model-based (LLM) multi-agent expert system, to automate this unstructured-to-structured translation process. DevNous integrates directly into team chat environments, identifying actionable intents from informal dialogue and managing stateful, multi-turn workflows for core administrative tasks like automated task formalization and progress summary synthesis. To quantitatively evaluate the system, we introduce a new benchmark of 160 realistic, interactive conversational turns. The dataset was manually annotated with a multi-label ground truth and is publicly available. On this benchmark, DevNous achieves an exact match turn accuracy of 81.3\% and a multiset F1-Score of 0.845, providing strong evidence for its viability. The primary contributions of this work are twofold: (1) a validated architectural pattern for developing ambient administrative agents, and (2) the introduction of the first robust empirical baseline and public benchmark dataset for this challenging problem domain.

</details>


### [59] [Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering](https://arxiv.org/abs/2508.08785)

*Yunfeng Ning, Mayi Xu, Jintao Wen, Qiankun Pi, Yuanyuan Zhu, Ming Zhong, Jiawei Jiang, Tieyun Qian*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, privacy protection, knowledge graphs, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a novel ARoG framework to integrate private knowledge graphs (KGs) into retrieval-augmented generation (RAG) systems while maintaining privacy by anonymizing entity semantics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of external knowledge into large language models (LLMs) often leads to privacy risks with private KGs, necessitating a solution that balances knowledge retrieval and user privacy.

**Method:** The ARoG framework employs two abstraction strategies: relation-centric abstraction, which captures high-level concepts from entity relations, and structure-oriented abstraction, which converts natural language questions into structured paths compatible with KGs.

**Key Contributions:**

	1. Introduction of a privacy-protected RAG scenario with anonymous entities in KGs.
	2. Development of relation-centric and structure-oriented abstraction strategies.
	3. Demonstration of strong retrieval performance and privacy-robustness through experiments on multiple datasets.

**Result:** The ARoG framework effectively allows for the retrieval of knowledge from anonymous entities in KGs, leading to improved performance while strictly maintaining privacy.

**Limitations:** The paper does not explore the scalability of the ARoG framework for very large or complex KGs.

**Conclusion:** ARoG demonstrates that it is possible to enhance retrieval performance without compromising the privacy of knowledge in KGs by anonymizing entity semantics.

**Abstract:** LLMs often suffer from hallucinations and outdated or incomplete knowledge. RAG is proposed to address these issues by integrating external knowledge like that in KGs into LLMs. However, leveraging private KGs in RAG systems poses significant privacy risks due to the black-box nature of LLMs and potential insecure data transmission, especially when using third-party LLM APIs lacking transparency and control. In this paper, we investigate the privacy-protected RAG scenario for the first time, where entities in KGs are anonymous for LLMs, thus preventing them from accessing entity semantics. Due to the loss of semantics of entities, previous RAG systems cannot retrieve question-relevant knowledge from KGs by matching questions with the meaningless identifiers of anonymous entities. To realize an effective RAG system in this scenario, two key challenges must be addressed: (1) How can anonymous entities be converted into retrievable information. (2) How to retrieve question-relevant anonymous entities. Hence, we propose a novel ARoG framework including relation-centric abstraction and structure-oriented abstraction strategies. For challenge (1), the first strategy abstracts entities into high-level concepts by dynamically capturing the semantics of their adjacent relations. It supplements meaningful semantics which can further support the retrieval process. For challenge (2), the second strategy transforms unstructured natural language questions into structured abstract concept paths. These paths can be more effectively aligned with the abstracted concepts in KGs, thereby improving retrieval performance. To guide LLMs to effectively retrieve knowledge from KGs, the two strategies strictly protect privacy from being exposed to LLMs. Experiments on three datasets demonstrate that ARoG achieves strong performance and privacy-robustness.

</details>


### [60] [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791)

*Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Xuanjing Huang, Jiecao Chen*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, tool use, automated environment construction, verifiable reward mechanism

**Relevance Score:** 7

**TL;DR:** This paper presents a novel reinforcement learning framework for large language models (LLMs) to enhance their tool-use capabilities through automated environment construction and a verifiable reward mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ability of large language models (LLMs) to interact meaningfully with their environment via effective tool use, addressing the challenges in reinforcement learning frameworks.

**Method:** An automated environment construction pipeline that includes scenario decomposition, document generation, function integration, and a verifiable reward mechanism to assess tool use precision and task execution completeness.

**Key Contributions:**

	1. Automated environment construction pipeline for LLMs
	2. Verifiable reward mechanism for tool use evaluation
	3. Demonstrated robustness of tool-use performance across various LLM scales

**Result:** The proposed framework significantly enhances tool-use performance in LLMs without degrading their general capabilities, shown through experiments on models of varying scales.

**Limitations:** 

**Conclusion:** The improvements in tool use are linked to better context understanding and reasoning, attributed to updates in lower-layer MLP parameters.

**Abstract:** Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.

</details>


### [61] [TiMoE: Time-Aware Mixture of Language Experts](https://arxiv.org/abs/2508.08827)

*Robin Faro, Dongyang Fan, Tamar Alphaidze, Martin Jaggi*

**Main category:** cs.CL

**Keywords:** large language models, temporal leakage, NLP, machine learning, AI

**Relevance Score:** 8

**TL;DR:** This paper introduces TiMoE, a Time-aware Mixture of Language Experts designed to mitigate temporal leakage in large language models by pre-training on disjoint two-year slices of a 2013-2024 corpus.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the issue of temporal leakage in large language models, where outdated information may mislead predictions by relying on future data.

**Method:** The authors pre-train a set of GPT-style experts on distinct two-year sections of a corpus and employ TiMoE to mask out experts irrelevant to a given query timestamp at inference time, ensuring causal validity.

**Key Contributions:**

	1. Introduction of TiMoE for mitigating temporal leakage in LLMs
	2. Creation of TSQA, a 10k-question benchmark for measuring temporal accuracy
	3. Demonstration of improved performance on standard NLP tasks while maintaining causal validity

**Result:** Experiments show that TiMoE performs comparably to or better than the best single-period expert, reducing future-knowledge errors by up to 15% across various NLP tasks including the introduced TSQA benchmark.

**Limitations:** 

**Conclusion:** The study demonstrates that a modular, time-segmented approach coupled with causal routing can effectively produce LLMs that remain chronological while still delivering strong overall performance.

**Abstract:** Large language models (LLMs) are typically trained on fixed snapshots of the web, which means that their knowledge becomes stale and their predictions risk temporal leakage: relying on information that lies in the future relative to a query. We tackle this problem by pre-training from scratch a set of GPT-style experts on disjoint two-year slices of a 2013-2024 corpus and combining them through TiMoE, a Time-aware Mixture of Language Experts. At inference time, TiMoE masks all experts whose training window ends after the query timestamp and merges the remaining log-probabilities in a shared space, guaranteeing strict causal validity while retaining the breadth of multi-period knowledge. We also release TSQA, a 10k-question benchmark whose alternatives are explicitly labelled as past, future or irrelevant, allowing fine-grained measurement of temporal hallucinations. Experiments on eight standard NLP tasks plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best single-period expert and cuts future-knowledge errors by up to 15%. Our results demonstrate that modular, time-segmented pre-training paired with causal routing is a simple yet effective path toward LLMs that stay chronologically grounded without sacrificing general performance much. We open source our code at TiMoE (Github): https://github.com/epfml/TiMoE

</details>


### [62] [An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems](https://arxiv.org/abs/2508.08833)

*Yuren Hao, Xiang Wan, Chengxiang Zhai*

**Main category:** cs.CL

**Keywords:** LLMs, mathematical reasoning, robustness, PutnamGAP, benchmark

**Relevance Score:** 9

**TL;DR:** This paper presents a new framework to assess the robustness of LLMs' mathematical reasoning through stress-testing with transformed mathematical problems, resulting in the PutnamGAP benchmark dataset and insights into LLM performance variability.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the mathematical reasoning capabilities of LLMs more accurately by measuring their sensitivity to linguistic and parametric variations in math problems.

**Method:** A systematic framework was developed to stress-test LLMs on advanced math problems that are mathematically equivalent but linguistically varied, creating the PutnamGAP benchmark dataset for evaluation.

**Key Contributions:**

	1. Introduction of a systematic assessment framework for LLMs' mathematical reasoning
	2. Creation of the PutnamGAP benchmark dataset
	3. Demonstration of robust performance degradation across multiple LLMs under evaluation

**Result:** The evaluation of 18 different LLMs showed significant performance drops on the transformed variants, with OpenAI's O3 model experiencing a 10.5 percentage drop on core-step-based variants.

**Limitations:** The study primarily focuses on mathematical reasoning and may not encompass other dimensions of LLM performance.

**Conclusion:** The new evaluation methodology effectively improves the understanding of LLMs' mathematical reasoning robustness and provides insights for further enhancements.

**Abstract:** In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 49 % on the originals but drops by 4 percentage points on surface variants, and by 10.5 percentage points on core-step-based variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.

</details>


### [63] [Steering Towards Fairness: Mitigating Political Bias in LLMs](https://arxiv.org/abs/2508.08846)

*Afrozah Nadeem, Mark Dras, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias Mitigation, Political Compass Test, Internal Model Analysis, Debiasing Techniques

**Relevance Score:** 9

**TL;DR:** The paper presents a framework to analyze and reduce ideological biases in large language models (LLMs) using internal model representations and the Political Compass Test.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of ideological biases encoded in large language models that affect their real-world applications.

**Method:** The framework uses contrastive pairs to extract and compare hidden layer activations across multiple ideological axes in models like Mistral and DeepSeek, employing a comprehensive activation extraction pipeline.

**Key Contributions:**

	1. Proposed a framework for analyzing ideological biases in LLMs
	2. Introduced a layer-wise activation extraction pipeline
	3. Provided empirical insights into representational bias and mitigation strategies

**Result:** The analysis reveals that decoder LLMs encode representational bias across different layers, which can be mitigated through targeted interventions using steering vectors.

**Limitations:** 

**Conclusion:** This study enhances understanding of how political bias is represented in LLMs and provides strategies for effective debiasing, going beyond superficial fixes.

**Abstract:** Recent advancements in large language models (LLMs) have enabled their widespread use across diverse real-world applications. However, concerns remain about their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. In this paper, we propose a framework for probing and mitigating such biases in decoder-based LLMs through analysis of internal model representations. Grounded in the Political Compass Test (PCT), our method uses contrastive pairs to extract and compare hidden layer activations from models like Mistral and DeepSeek. We introduce a comprehensive activation extraction pipeline capable of layer-wise analysis across multiple ideological axes, revealing meaningful disparities linked to political framing. Our results show that decoder LLMs systematically encode representational bias across layers, which can be leveraged for effective steering vector-based mitigation. This work provides new insights into how political bias is encoded in LLMs and offers a principled approach to debiasing beyond surface-level output interventions.

</details>


### [64] [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855)

*Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** bias mitigation, Large Language Models, bias analysis, machine learning, human-computer interaction

**Relevance Score:** 9

**TL;DR:** BiasGym is a framework for identifying and mitigating biases in Large Language Models (LLMs) through bias injection and analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to understand and mitigate biases encoded in the weights of Large Language Models (LLMs) to develop effective strategies for addressing biased behavior.

**Method:** BiasGym comprises two components: BiasInject for injecting biases into the model via token-based fine-tuning, and BiasScope for analyzing and steering components responsible for bias.

**Key Contributions:**

	1. Introduction of BiasGym framework for bias analysis in LLMs
	2. Development of BiasInject and BiasScope components
	3. Demonstrated effectiveness in reducing real-world and fictional stereotypes.

**Result:** BiasGym allows for the consistent elicitation of biases, targeted debiasing, and generalizes to unseen biases. It effectively reduces stereotypes and probes fictional associations in LLMs.

**Limitations:** 

**Conclusion:** The framework demonstrates utility for safety interventions and interpretability research in LLMs.

**Abstract:** Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during training. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from a country being `reckless drivers') and in probing fictional associations (e.g., people from a country having `blue skin'), showing its utility for both safety interventions and interpretability research.

</details>


### [65] [Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance](https://arxiv.org/abs/2508.08876)

*Kaiyu Wang, Lin Mu, Zhiyao Yang, Ximing Li, Xiaotang Zhou Wanfu Gao, Huimao Zhang*

**Main category:** cs.CL

**Keywords:** Quality Assurance, Radiology Reports, Machine Learning, Text Span Analysis, Automated Evaluation

**Relevance Score:** 6

**TL;DR:** Proposal of Sqator for automatic QA scoring of radiology reports using fine-grained text span analysis.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The manual QA process for junior radiology reports is labor-intensive and can lead to inaccuracies due to various biases.

**Method:** Sqator measures QA scores by analyzing the importance of revised text spans between junior and senior reports, aiming for more granular analysis than traditional document-level methods.

**Key Contributions:**

	1. Introduction of Sqator for span-level QA scoring
	2. Comparison with traditional document-level methods
	3. Validation with a large dataset of radiology reports

**Result:** Sqator was evaluated on 12,013 radiology reports, achieving competitive QA scores and aligning span importance scores with senior doctors' judgments.

**Limitations:** 

**Conclusion:** Sqator offers an efficient and accurate alternative for evaluating QA scores in radiology reports, potentially reducing labor costs and improving reliability.

**Abstract:** Quality Assurance (QA) for radiology reports refers to judging whether the junior reports (written by junior doctors) are qualified. The QA scores of one junior report are given by the senior doctor(s) after reviewing the image and junior report. This process requires intensive labor costs for senior doctors. Additionally, the QA scores may be inaccurate for reasons like diagnosis bias, the ability of senior doctors, and so on. To address this issue, we propose a Span-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores automatically. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Unlike the common document-level semantic comparison method, we try to analyze the semantic difference by exploring more fine-grained text spans. Specifically, Sqator measures QA scores by measuring the importance of revised spans between junior and senior reports, and outputs the final QA scores by merging all revised span scores. We evaluate Sqator using a collection of 12,013 radiology reports. Experimental results show that Sqator can achieve competitive QA scores. Moreover, the importance scores of revised spans can be also consistent with the judgments of senior doctors.

</details>


### [66] [Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models](https://arxiv.org/abs/2508.08879)

*Haeun Yu, Seogyeong Jeong, Siddhesh Pawar, Jisu Shin, Jiho Jin, Junho Myung, Alice Oh, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Competence, Cultural Bias, Mechanistic Interpretability, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper presents Culturescope, a method to explore cultural biases in large language models (LLMs) by analyzing their internal representations and proposes a cultural flattening score to measure these biases.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of external evaluations in assessing LLMs' cultural competence and to better understand how cultural misrepresentation occurs within these models.

**Method:** Culturescope utilizes a mechanistic interpretability approach to probe LLMs' internal representations, employing a patching method to extract cultural knowledge and introducing a cultural flattening score to quantify cultural biases.

**Key Contributions:**

	1. Introduction of the Culturescope method for probing LLM cultural knowledge
	2. Development of the cultural flattening score for measuring biases
	3. Empirical evidence of Western-dominance bias in LLMs

**Result:** Experimental results demonstrate that LLMs exhibit Western-dominance bias and cultural flattening, while low-resource cultures show less susceptibility to these biases, presumably due to limited training resources.

**Limitations:** Focuses primarily on Western-dominance bias; more diverse case studies needed across various cultures.

**Conclusion:** The findings highlight the importance of understanding and mitigating cultural biases in LLMs to improve their cultural understanding, laying groundwork for future research in this area.

**Abstract:** The growing deployment of large language models (LLMs) across diverse cultural contexts necessitates a better understanding of how the overgeneralization of less documented cultures within LLMs' representations impacts their cultural understanding. Prior work only performs extrinsic evaluation of LLMs' cultural competence, without accounting for how LLMs' internal mechanisms lead to cultural (mis)representation. To bridge this gap, we propose Culturescope, the first mechanistic interpretability-based method that probes the internal representations of LLMs to elicit the underlying cultural knowledge space. CultureScope utilizes a patching method to extract the cultural knowledge. We introduce a cultural flattening score as a measure of the intrinsic cultural biases. Additionally, we study how LLMs internalize Western-dominance bias and cultural flattening, which allows us to trace how cultural biases emerge within LLMs. Our experimental results reveal that LLMs encode Western-dominance bias and cultural flattening in their cultural knowledge space. We find that low-resource cultures are less susceptible to cultural biases, likely due to their limited training resources. Our work provides a foundation for future research on mitigating cultural biases and enhancing LLMs' cultural understanding. Our codes and data used for experiments are publicly available.

</details>


### [67] [ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs](https://arxiv.org/abs/2508.08895)

*Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun*

**Main category:** cs.CL

**Keywords:** large language models, inference speed, parallel decoding, adaptive decoding, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper introduces Adaptive Serial-Parallel Decoding (ASPD), which enhances the inference speed of large language models (LLMs) by utilizing intrinsic parallelism, achieving significant performance improvements.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing scale and complexity of LLMs create inference latency challenges due to their autoregressive decoding paradigm, necessitating improvements in decoding efficiency.

**Method:** The paper proposes an ASPD framework that automates the construction of parallelizable data segments and implements a Hybrid Decoding Engine for adaptive decoding between serial and parallel modes.

**Key Contributions:**

	1. Introduction of intrinsic parallelism in LLM decoding
	2. Development of the Hybrid Decoding Engine for adaptive decoding
	3. Achieving significant speedup while maintaining generation quality

**Result:** ASPD achieves up to 3.19x speedup (1.85x on average) in inference speed while maintaining response quality within 1% of standard autoregressive models across various tasks.

**Limitations:** 

**Conclusion:** The ASPD framework sets a benchmark for efficient LLM parallel inference, suitable for latency-sensitive applications like customer service bots and answer retrieval engines.

**Abstract:** The increasing scale and complexity of large language models (LLMs) pose significant inference latency challenges, primarily due to their autoregressive decoding paradigm characterized by the sequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we observed that some segments exhibit parallelizable structures, which we term intrinsic parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel decoding) can significantly improve the overall inference speed of LLMs. In this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which addresses two core challenges: automated construction of parallelizable data and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive pipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive models. To empower efficient adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which enables seamless transitions between serial and parallel decoding modes while maintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical Reasoning, demonstrate that ASPD achieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up to 3.19x speedup (1.85x on average) while maintaining response quality within 1% difference compared to autoregressive models, realizing significant acceleration without compromising generation quality. Our framework sets a groundbreaking benchmark for efficient LLM parallel inference, paving the way for its deployment in latency-sensitive applications such as AI-powered customer service bots and answer retrieval engines.

</details>


### [68] [Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning](https://arxiv.org/abs/2508.08912)

*Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Weakly Supervised Learning, Dialectal Arabic

**Relevance Score:** 4

**TL;DR:** A scalable training pipeline for Arabic ASR using weakly supervised learning and supervised fine-tuning achieves state-of-the-art results.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of developing accurate ASR systems for low-resource languages like Arabic, which suffer from limited labeled data and diverse dialects.

**Method:** The method includes a pretraining stage on 15,000 hours of weakly labeled speech and a subsequent supervised fine-tuning stage using filtered weakly labeled data and a small annotated dataset.

**Key Contributions:**

	1. Scalable training pipeline for Arabic ASR
	2. Combination of weakly supervised learning with supervised fine-tuning
	3. State-of-the-art results in multi-dialectal Arabic ASR challenge

**Result:** Achieved state-of-the-art results in the multi-dialectal Arabic ASR challenge, demonstrating the effectiveness of the proposed approach.

**Limitations:** 

**Conclusion:** Weak supervision combined with fine-tuning can effectively address the data scarcity issue for low-resource, dialect-rich languages in ASR systems.

**Abstract:** Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages.

</details>


### [69] [Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation](https://arxiv.org/abs/2508.08933)

*Khondoker Ittehadul Islam, Gabriele Sarti*

**Main category:** cs.CL

**Keywords:** Bangla, multilingual models, multi-step reasoning, language models, evaluation

**Relevance Score:** 6

**TL;DR:** This paper presents a Bangla multi-step reasoning dataset, evaluates multilingual models on it, and discusses the challenges in utilizing relevant reasoning steps in Bangla.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the reasoning capabilities of language models in lower-resource languages, specifically Bangla, compared to English.

**Method:** The study involves creating a Bangla dataset by translating the English Reveal dataset and evaluating multilingual small language models on both the original and translated datasets.

**Key Contributions:**

	1. Introduction of a Bangla multi-step reasoning dataset
	2. Evaluation of multilingual small language models on this dataset
	3. Analysis of reasoning step usage in different languages.

**Result:** Models showed improved reasoning for non-binary questions but encountered difficulties utilizing relevant Bangla reasoning steps effectively.

**Limitations:** Focus is mainly on Bangla and may not generalize to other low-resource languages; limited model architecture evaluation.

**Conclusion:** The paper concludes that reasoning context aids in generating correct answers, although challenges remain in employing Bangla reasoning steps across different models.

**Abstract:** Language models have demonstrated remarkable performance on complex multi-step reasoning tasks. However, their evaluation has been predominantly confined to high-resource languages such as English. In this paper, we introduce a manually translated Bangla multi-step reasoning dataset derived from the English Reveal dataset, featuring both binary and non-binary question types. We conduct a controlled evaluation of English-centric and Bangla-centric multilingual small language models on the original dataset and our translated version to compare their ability to exploit relevant reasoning steps to produce correct answers. Our results show that, in comparable settings, reasoning context is beneficial for more challenging non-binary questions, but models struggle to employ relevant Bangla reasoning steps effectively. We conclude by exploring how reasoning steps contribute to models' predictions, highlighting different trends across models and languages.

</details>


### [70] [Train Long, Think Short: Curriculum Learning for Efficient Reasoning](https://arxiv.org/abs/2508.08940)

*Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** curriculum learning, large language models, token efficiency, Group Relative Policy Optimization, reasoning

**Relevance Score:** 7

**TL;DR:** This paper introduces a curriculum learning strategy for length-controlled reasoning in large language models (LLMs) using Group Relative Policy Optimization (GRPO), which improves token efficiency and accuracy by gradually tightening token budgets during training.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning abilities of LLMs while managing computational costs and promoting efficient learning through an adaptive length control approach.

**Method:** The authors propose a curriculum learning strategy using GRPO that starts with generous token budgets and gradually reduces them, encouraging LLMs to first explore effective strategies before refining them into concise reasoning.

**Key Contributions:**

	1. Introduction of curriculum learning for length-controlled reasoning
	2. Empirical validation showing improvements over fixed-budget training
	3. Release of code and checkpoints for reproducibility

**Result:** Experiments show that the proposed curriculum-based training surpasses fixed-budget baselines in accuracy and token efficiency across multiple benchmarks, including GSM8K and MATH500.

**Limitations:** 

**Conclusion:** Curriculum-based training with progressive constraints serves as an effective inductive bias for improving reasoning models, leading to better performance and resource usage.

**Abstract:** Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.

</details>


### [71] [Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens](https://arxiv.org/abs/2508.08942)

*Lucas Albarede, Jose Moreno, Lynda Tamine, Luce Lefeuvre*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, trustworthiness, attributed text generation, LoDIT

**Relevance Score:** 9

**TL;DR:** LoDIT is a new method for jointly generating and attributing answers in retrieval-augmented generation (RAG) using specific token logits, improving trustworthiness and efficiency compared to state-of-the-art models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for trustworthiness in Large Language Models (LLMs) due to their tendency to hallucinate answers.

**Method:** LoDIT generates and attributes answers by marking documents with specific token identifiers and using the logits of these tokens to gauge each document's contribution during answer generation.

**Key Contributions:**

	1. Introduction of LoDIT for joint answer generation and attribution
	2. Utilization of specific token logits for accurate document contributions
	3. Demonstration of efficiency and robustness compared to existing models

**Result:** LoDIT significantly outperforms state-of-the-art models on several metrics in a trustworthiness-focused attributed text-generation benchmark.

**Limitations:** 

**Conclusion:** LoDIT demonstrates both improved latency and robustness across various scenarios in generating trustworthy answers.

**Abstract:** Despite their impressive performances, Large Language Models (LLMs) remain prone to hallucination, which critically undermines their trustworthiness. While most of the previous work focused on tackling answer and attribution correctness, a recent line of work investigated faithfulness, with a focus on leveraging internal model signals to reflect a model's actual decision-making process while generating the answer. Nevertheless, these methods induce additional latency and have shown limitations in directly aligning token generation with attribution generation. In this paper, we introduce LoDIT, a method that jointly generates and faithfully attributes answers in RAG by leveraging specific token logits during generation. It consists of two steps: (1) marking the documents with specific token identifiers and then leveraging the logits of these tokens to estimate the contribution of each document to the answer during generation, and (2) aggregating these contributions into document attributions. Experiments on a trustworthiness-focused attributed text-generation benchmark, Trust-Align, show that LoDIT significantly outperforms state-of-the-art models on several metrics. Finally, an in-depth analysis of LoDIT shows both its efficiency in terms of latency and its robustness in different settings.

</details>


### [72] [Retrospective Sparse Attention for Efficient Long-Context Generation](https://arxiv.org/abs/2508.09001)

*Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Key-Value cache, Attention mechanisms, NLP, Long-context tasks

**Relevance Score:** 9

**TL;DR:** RetroAttention is a technique that improves long-context inference in Large Language Models by revising past attention outputs with new KV entries, enhancing efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with extended contexts due to the limitations of the Key-Value cache which increases both memory usage and latency.

**Method:** RetroAttention updates past attention outputs during long decoding by incorporating new KV entries, which enhances efficiency in accessing relevant context.

**Key Contributions:**

	1. Introduces a new update technique for the KV cache that revises past outputs
	2. Demonstrates significant performance improvements over existing KV compression methods
	3. Allows for continual correction of prior attention approximations

**Result:** RetroAttention shows up to 1.6× better effective KV exposure and up to 21.9% improved accuracy over state-of-the-art KV compression methods.

**Limitations:** 

**Conclusion:** The method breaks the standard fixed-attention-output model, allowing continuous correction of past approximations, leading to better performance in long-context tasks.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in long-context tasks such as reasoning, code generation, and multi-turn dialogue. However, inference over extended contexts is bottlenecked by the Key-Value (KV) cache, whose memory footprint grows linearly with sequence length and dominates latency at each decoding step. While recent KV cache compression methods identify and load important tokens, they focus predominantly on input contexts and fail to address the cumulative attention errors that arise during long decoding. In this paper, we introduce RetroAttention, a novel KV cache update technique that retrospectively revises past attention outputs using newly arrived KV entries from subsequent decoding steps. By maintaining a lightweight output cache, RetroAttention enables past queries to efficiently access more relevant context, while incurring minimal latency overhead. This breaks the fixed-attention-output paradigm and allows continual correction of prior approximations. Extensive experiments on long-generation benchmarks show that RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by up to 21.9\%.

</details>


### [73] [LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA](https://arxiv.org/abs/2508.09012)

*Adrián Gude, Roi Santos-Ríos, Francisco Prado-Valiño, Ana Ezquerro, Jesús Vilares*

**Main category:** cs.CL

**Keywords:** Tabular Question Answering, Zero-shot Learning, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper presents a zero-shot pipeline for Tabular Question Answering using a Large Language Model to generate functional code for information extraction from tabular data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of extracting relevant information from tabular data in response to natural language questions without task-specific fine-tuning.

**Method:** The approach involves a modular pipeline with a code generator, column relevance identification, and data type analysis, along with an iterative refinement process when errors occur.

**Key Contributions:**

	1. Development of a zero-shot pipeline for Tabular QA
	2. Integration of a modular approach with code generation
	3. Implementation of an iterative refinement process for error handling

**Result:** The zero-shot code generation approach ranked 33rd out of 53 in the test phase, demonstrating its potential for Tabular QA.

**Limitations:** The pipeline's performance is limited by the absence of task-specific fine-tuning and may not handle complex queries effectively.

**Conclusion:** The findings support the effectiveness of zero-shot code generation for Tabular Question Answering, highlighting its applicability without extensive fine-tuning.

**Abstract:** This paper describes our participation in SemEval 2025 Task 8, focused on Tabular Question Answering. We developed a zero-shot pipeline that leverages an Large Language Model to generate functional code capable of extracting the relevant information from tabular data based on an input question. Our approach consists of a modular pipeline where the main code generator module is supported by additional components that identify the most relevant columns and analyze their data types to improve extraction accuracy. In the event that the generated code fails, an iterative refinement process is triggered, incorporating the error feedback into a new generation prompt to enhance robustness. Our results show that zero-shot code generation is a valid approach for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of task-specific fine-tuning.

</details>


### [74] [A Survey on Training-free Alignment of Large Language Models](https://arxiv.org/abs/2508.09016)

*Birong Pan, Yongqi Li, Weiyu Zhang, Wenpeng Lu, Mayi Xu, Shen Zhou, Yuanyuan Zhu, Ming Zhong, Tieyun Qian*

**Main category:** cs.CL

**Keywords:** Large language models, Training-free alignment, Human values, Ethical standards, Decoding stages

**Relevance Score:** 9

**TL;DR:** This paper reviews training-free alignment methods for large language models, offering insights into pre-decoding, in-decoding, and post-decoding techniques without intensive retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of traditional fine-tuning methods in aligning large language models with human values and ethical standards, considering constraints in model accessibility and computational resources.

**Method:** The review categorizes training-free alignment methods into three stages: pre-decoding, in-decoding, and post-decoding, providing an examination of their mechanisms and limitations.

**Key Contributions:**

	1. First systematic review of training-free alignment methods for LLMs
	2. Categorization of alignment methods by decoding stages
	3. Identification of challenges and future research directions

**Result:** The paper synthesizes existing research on training-free alignment, identifying challenges and suggesting future directions to improve LLM safety and reliability.

**Limitations:** The paper primarily focuses on training-free methods, which may not cover all potential alignment scenarios.

**Conclusion:** The survey serves as a guidance for practitioners in developing more inclusive and effective alignment techniques for LLMs.

**Abstract:** The alignment of large language models (LLMs) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, decoding-time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining LLMs, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we provide a detailed examination from the viewpoint of LLMs and multimodal LLMs (MLLMs), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable LLMs.

</details>


### [75] [LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback](https://arxiv.org/abs/2508.09042)

*Chen Xu, Zhenyu Lv, Tian Lan, Xianyang Wang, Luyao Ji, Leyang Cui, Minqiang Yang, Jian Shen, Qunxi Dong, Xiuling Liu, Juan Wang, Bin Hu*

**Main category:** cs.CL

**Keywords:** human-computer interaction, therapist training, language models

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel paradigm where a large language model (LLM) is used as a supervisor to train therapists by providing targeted feedback on common therapeutic mistakes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical and safety concerns surrounding the application of LLMs in psychotherapy and to improve the training of real therapists.

**Method:** Developed a therapist-training paradigm that includes establishing standards for mistaken behaviors, creating a dialogue-feedback dataset where an agent makes standard mistakes, and training a supervisor model to provide targeted feedback.

**Key Contributions:**

	1. Establishment of standards for common therapeutic mistakes
	2. Creation of a human-in-the-loop dialogue-feedback dataset for training
	3. Development of a supervisor model that provides targeted feedback for therapist training

**Result:** The trained supervisor model can offer high-quality feedback aligning with clinical guidelines, showing a significant potential for enhancing therapist training.

**Limitations:** 

**Conclusion:** The study demonstrates the feasibility and effectiveness of using LLMs to improve therapist training, potentially leading to better therapeutic outcomes.

**Abstract:** Although large language models (LLMs) hold significant promise in psychotherapy, their direct application in patient-facing scenarios raises ethical and safety concerns. Therefore, this work shifts towards developing an LLM as a supervisor to train real therapists. In addition to the privacy of clinical therapist training data, a fundamental contradiction complicates the training of therapeutic behaviors: clear feedback standards are necessary to ensure a controlled training system, yet there is no absolute "gold standard" for appropriate therapeutic behaviors in practice. In contrast, many common therapeutic mistakes are universal and identifiable, making them effective triggers for targeted feedback that can serve as clearer evidence. Motivated by this, we create a novel therapist-training paradigm: (1) guidelines for mistaken behaviors and targeted correction strategies are first established as standards; (2) a human-in-the-loop dialogue-feedback dataset is then constructed, where a mistake-prone agent intentionally makes standard mistakes during interviews naturally, and a supervisor agent locates and identifies mistakes and provides targeted feedback; (3) after fine-tuning on this dataset, the final supervisor model is provided for real therapist training. The detailed experimental results of automated, human and downstream assessments demonstrate that models fine-tuned on our dataset MATE, can provide high-quality feedback according to the clinical guideline, showing significant potential for the therapist training scenario.

</details>


### [76] [MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions](https://arxiv.org/abs/2508.09057)

*Zeyu Huang, Juyuan Wang, Longfeng Chen, Boyi Xiao, Leng Cai, Yawen Zeng, Jin Xu*

**Main category:** cs.CL

**Keywords:** Large Vision Language Models, mobile agents, evaluation benchmark, user intent, bilingual benchmark

**Relevance Score:** 7

**TL;DR:** The paper presents MVISU-Bench, a new bilingual benchmark for evaluating mobile agents, and introduces Aider, a module that enhances user intent clarification, leading to significant improvements in task success rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of real-world relevance in current evaluation benchmarks for mobile agents and to meet diverse user automation needs.

**Method:** The study comprises the development of MVISU-Bench, which includes 404 tasks across 137 applications, and the implementation of Aider, a dynamic prompt prompter to enhance user intent understanding.

**Key Contributions:**

	1. Introduction of MVISU-Bench as a comprehensive evaluation benchmark for mobile agents.
	2. Development of Aider, which enhances user intent clarification and mitigates risks during interactions.
	3. Documented improvements in task success rates using Aider over existing methods.

**Result:** Aider improved overall success rates by 19.55%, specifically achieving 53.52% and 29.41% increases for unethical and interactive instructions respectively, compared to the state-of-the-art.

**Limitations:** 

**Conclusion:** The findings reveal a significant gap between existing mobile agent capabilities and real-world user expectations, highlighting the importance of improving evaluation methods and user intent understanding.

**Abstract:** Given the significant advances in Large Vision Language Models (LVLMs) in reasoning and visual understanding, mobile agents are rapidly emerging to meet users' automation needs. However, existing evaluation benchmarks are disconnected from the real world and fail to adequately address the diverse and complex requirements of users. From our extensive collection of user questionnaire, we identified five tasks: Multi-App, Vague, Interactive, Single-App, and Unethical Instructions. Around these tasks, we present \textbf{MVISU-Bench}, a bilingual benchmark that includes 404 tasks across 137 mobile applications. Furthermore, we propose Aider, a plug-and-play module that acts as a dynamic prompt prompter to mitigate risks and clarify user intent for mobile agents. Our Aider is easy to integrate into several frameworks and has successfully improved overall success rates by 19.55\% compared to the current state-of-the-art (SOTA) on MVISU-Bench. Specifically, it achieves success rate improvements of 53.52\% and 29.41\% for unethical and interactive instructions, respectively. Through extensive experiments and analysis, we highlight the gap between existing mobile agents and real-world user expectations.

</details>


### [77] [READER: Retrieval-Assisted Drafter for Efficient LLM Inference](https://arxiv.org/abs/2508.09072)

*Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi*

**Main category:** cs.CL

**Keywords:** Large Language Models, speculative decoding, retrieval-augmented generation

**Relevance Score:** 8

**TL;DR:** READER is a novel method for efficient LLM inference that enhances speculative decoding by leveraging self-repetitions and optimizing key-value cache sizes, achieving over 40% speedup without additional training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in LLM inference caused by the autoregressive nature of token generation and to improve deployment for industrial applications.

**Method:** READER employs a lossless speculative decoding technique that expands the decoding tree using tokens from a statistical search, focusing on large batch sizes.

**Key Contributions:**

	1. Introduction of READER, a lossless speculative decoding method for LLM inference
	2. Optimization of KV cache size for large batch processing
	3. Demonstration of high performance with no additional training and significant speed improvements

**Result:** READER outperforms existing speculative decoding methods, particularly excelling in search-based tasks where it shows more than 10x speedup.

**Limitations:** 

**Conclusion:** The proposed method presents a significant improvement in the efficiency of LLM inference, particularly for applications requiring robust speed and performance during retrieval-augmented generation tasks.

**Abstract:** Large Language Models (LLMs) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel lossless speculative decoding method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative decoding tree using tokens obtained through statistical search. This work focuses on large batch sizes (>= 8), an underexplored yet important area for industrial applications. We also analyze the key-value (KV) cache size during speculative decoding and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative decoding methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40\%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup.

</details>


### [78] [CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization](https://arxiv.org/abs/2508.09074)

*Xinge Ye, Rui Wang, Yuchuan Wu, Victor Ma, Feiteng Fang, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Comparative Evaluation, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper presents Comparative Policy Optimization (CPO) for improving evaluation in reinforcement learning for subjective tasks like role-playing dialogue by using comparative group-wise scoring.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges in reinforcement learning fine-tuning for subjective tasks, which traditional reward modeling fails to manage due to subjective evaluation and unstable reward signals.

**Method:** CPO shifts the evaluation paradigm from sample-wise scoring to comparative group-wise scoring. It introduces the CharacterArena evaluation framework, which includes a multi-turn role-playing simulation and trajectory-level comparative evaluation.

**Key Contributions:**

	1. Introduction of Comparative Policy Optimization (CPO)
	2. Development of the CharacterArena evaluation framework
	3. Empirical validation showing improved dialogue quality

**Result:** The empirical results demonstrate that CPO's approach leads to significant improvements in dialogue quality while minimizing contextual bias in performance evaluation.

**Limitations:** 

**Conclusion:** CPO effectively mitigates reward ambiguity and enhances the evaluation of dialogue systems, thus contributing to improved RLFT outcomes in subjective tasks.

**Abstract:** Reinforcement Learning Fine-Tuning (RLFT) has achieved notable success in tasks with objectively verifiable answers (e.g., code generation, mathematical reasoning), yet struggles with open-ended subjective tasks like role-playing dialogue. Traditional reward modeling approaches, which rely on independent sample-wise scoring, face dual challenges: subjective evaluation criteria and unstable reward signals.Motivated by the insight that human evaluation inherently combines explicit criteria with implicit comparative judgments, we propose Comparative Policy Optimization (CPO). CPO redefines the reward evaluation paradigm by shifting from sample-wise scoring to comparative group-wise scoring.Building on the same principle, we introduce the CharacterArena evaluation framework, which comprises two stages:(1) Contextualized Multi-turn Role-playing Simulation, and (2) Trajectory-level Comparative Evaluation. By operationalizing subjective scoring via objective trajectory comparisons, CharacterArena minimizes contextual bias and enables more robust and fair performance evaluation. Empirical results on CharacterEval, CharacterBench, and CharacterArena confirm that CPO effectively mitigates reward ambiguity and leads to substantial improvements in dialogue quality.

</details>


### [79] [Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages](https://arxiv.org/abs/2508.09091)

*Imalsha Puranegedara, Themira Chathumina, Nisal Ranathunga, Nisansa de Silva, Surangika Ranathunga, Mokanarangan Thayaparan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Low-resource languages, Multilingual processing, Transformer Softmax, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents a novel architecture that enhances the performance of Large Language Models (LLMs) on low-resource languages (LRLs) by fusing intermediate layers and using token-specific weighting strategies, leading to significant performance improvements across several benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of LLMs is significantly degraded on low-resource languages due to English-centric training, necessitating improved methods for multilingual processing.

**Method:** A novel architecture that fuses all intermediate layers of LLMs and employs two weighting strategies: Global Softmax for overall layer importance and Transformer Softmax for token-specific weights, without using parallel or multilingual data for training.

**Key Contributions:**

	1. Novel architecture fusing intermediate layers of LLMs
	2. Global Softmax and Transformer Softmax for layer importance and token-specific weights
	3. Significant performance gains in low-resource languages through a data-efficient training method.

**Result:** The proposed model outperforms the LangBridge baseline in various tasks, achieving improvements such as increasing Sinhala classification accuracy from 71.66% to 75.86% and enhancing average XNLI accuracy from 70.36% to 71.50%.

**Limitations:** 

**Conclusion:** The approach offers a scalable and data-efficient pathway toward developing more capable multilingual LLMs, particularly for low-resource languages.

**Abstract:** Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLM's embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs.

</details>


### [80] [Link Prediction for Event Logs in the Process Industry](https://arxiv.org/abs/2508.09096)

*Anastasia Zhukova, Thomas Walton, Christian E. Matt, Bela Gipp*

**Main category:** cs.CL

**Keywords:** Knowledge management, Record linking, Cross-document coreference resolution, Natural language inference, Semantic text similarity

**Relevance Score:** 4

**TL;DR:** The paper addresses the challenge of fragmented event logs in the process industry by applying record linking as a cross-document coreference resolution task enhanced with natural language inference and semantic text similarity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Knowledge management is crucial in the process industry to improve operations and ensure safety, but fragmented event logs hinder the recommendation of past solutions.

**Method:** The study adapts cross-document coreference resolution techniques to frame record linking at the passage level, accommodating both unstructured text and structured attributes in process industry texts.

**Key Contributions:**

	1. Adaptation of CDCR for record linking in process industry texts
	2. Improved performance over existing NLI and STS baselines
	3. Enhanced handling of fragmented data through advanced reasoning techniques

**Result:** The proposed record linking model outperformed traditional methods, improving results by 28% and 27% compared to NLI and STS baselines, respectively.

**Limitations:** 

**Conclusion:** The study demonstrates that adapting state-of-the-art CDCR models with reasoning capabilities can significantly enhance knowledge management processes in the process industry.

**Abstract:** Knowledge management (KM) is vital in the process industry for optimizing operations, ensuring safety, and enabling continuous improvement through effective use of operational data and past insights. A key challenge in this domain is the fragmented nature of event logs in shift books, where related records, e.g., entries documenting issues related to equipment or processes and the corresponding solutions, may remain disconnected. This fragmentation hinders the recommendation of previous solutions to the users. To address this problem, we investigate record linking (RL) as link prediction, commonly studied in graph-based machine learning, by framing it as a cross-document coreference resolution (CDCR) task enhanced with natural language inference (NLI) and semantic text similarity (STS) by shifting it into the causal inference (CI). We adapt CDCR, traditionally applied in the news domain, into an RL model to operate at the passage level, similar to NLI and STS, while accommodating the process industry's specific text formats, which contain unstructured text and structured record attributes. Our RL model outperformed the best versions of NLI- and STS-driven baselines by 28% (11.43 points) and 27% (11.21 points), respectively. Our work demonstrates how domain adaptation of the state-of-the-art CDCR models, enhanced with reasoning capabilities, can be effectively tailored to the process industry, improving data quality and connectivity in shift logs.

</details>


### [81] [AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators](https://arxiv.org/abs/2508.09101)

*Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang, Haotian Zhu, Jianwei Cai, Yue Mao, Chenchen Zhang, Lingyun Tan, Ziyan Xu, Bohui Zhai, Hengyi Liu, Speed Zhu, Wiggin Zhou, Fengzong Lian*

**Main category:** cs.CL

**Keywords:** Large Language Models, code generation, multilingual datasets

**Relevance Score:** 9

**TL;DR:** AutoCodeGen is an automated method for generating high-difficulty multilingual code generation datasets, resulting in AutoCodeBench, a benchmark for evaluating LLMs in diverse programming tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address critical limitations in existing benchmarks for evaluating LLMs' code generation capabilities, such as reliance on manual annotations and a lack of multilingual and diverse problem distributions.

**Method:** AutoCodeGen generates test inputs with LLMs and verifies test outputs through a multilingual sandbox, employing reverse-order problem generation and filtering to ensure data quality.

**Key Contributions:**

	1. Introduction of AutoCodeGen for automated dataset generation
	2. Creation of AutoCodeBench with 3,920 diverse problems
	3. Evaluation of multiple LLMs, revealing performance challenges

**Result:** AutoCodeBench contains 3,920 problems evenly distributed across 20 programming languages and evaluates over 30 LLMs, highlighting their struggles with complexity and diversity.

**Limitations:** 

**Conclusion:** The AutoCodeBench series aims to provide a valuable resource for evaluating LLMs on challenging multilingual code generation tasks, encouraging a shift in focus within the research community.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.

</details>


### [82] [SinLlama - A Large Language Model for Sinhala](https://arxiv.org/abs/2508.09115)

*H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur*

**Main category:** cs.CL

**Keywords:** Sinhala, Large Language Models, text classification, multilingual LLM, SinLlama

**Relevance Score:** 8

**TL;DR:** This paper presents SinLlama, the first open-source LLM with explicit support for the Sinhala language, which outperforms existing models in text classification tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of low-resource languages like Sinhala in the realm of open-source Large Language Models.

**Method:** The research involved enhancing the tokenizer of the multilingual LLM Llama-3-8B with a Sinhala specific vocabulary and performing continual pre-training on a cleaned corpus of 10 million Sinhala texts.

**Key Contributions:**

	1. Introduction of SinLlama as the first decoder-based open-source LLM with Sinhala support
	2. Enhancement of the LLM tokenizer with Sinhala specific vocabulary
	3. Demonstrated improved performance in text classification tasks compared to existing models.

**Result:** SinLlama significantly outperformed both the base and instruct variants of Llama-3-8B in three text classification tasks after instruction fine-tuning.

**Limitations:** 

**Conclusion:** SinLlama represents a significant advancement in multilingual LLMs, providing effective support for Sinhala and showcasing the potential improvements for text classification in low-resource languages.

**Abstract:** Low-resource languages such as Sinhala are often overlooked by open-source Large Language Models (LLMs). In this research, we extend an existing multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM tokenizer with Sinhala specific vocabulary and perform continual pre-training on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This is the very first decoder-based open-source LLM with explicit Sinhala support. When SinLlama was instruction fine-tuned for three text classification tasks, it outperformed base and instruct variants of Llama-3-8B by a significant margin.

</details>


### [83] [OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows](https://arxiv.org/abs/2508.09124)

*Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor Rühle, Saravan Rajmohan*

**Main category:** cs.CL

**Keywords:** large language models, benchmark, long-horizon workflows, office applications, multi-agent framework

**Relevance Score:** 9

**TL;DR:** OdysseyBench is a comprehensive benchmark for evaluating LLM agents on long-horizon workflows, addressing the limitations of existing benchmarks that focus on atomic tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for LLM agents focus on atomic tasks, failing to capture complex, multi-interaction workflows needed in real-world applications.

**Method:** We introduce OdysseyBench, which includes two splits: OdysseyBench+ (300 real-world tasks) and OdysseyBench-Neo (302 newly synthesized tasks), and a framework called HomerAgents for automated benchmark creation.

**Key Contributions:**

	1. Introduction of OdysseyBench as a benchmark for long-horizon workflows
	2. Creation of HomerAgents for automated benchmark generation
	3. Evaluation showing superior assessment capabilities of LLM agents in real-world contexts

**Result:** OdysseyBench effectively challenges state-of-the-art LLM agents and provides a more accurate assessment of their capabilities in complex, real-world contexts.

**Limitations:** 

**Conclusion:** OdysseyBench will advance the development and evaluation of LLM agents in productivity scenarios, and we release it along with HomerAgents to facilitate research.

**Abstract:** Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line.

</details>


### [84] [Complex Logical Instruction Generation](https://arxiv.org/abs/2508.09125)

*Mian Zhang, Shujian Liu, Sixun Dong, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang, Sathish Reddy Indurthi, Haoyun Deng, Zhiyu Zoey Chen, Kaiqiang Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, Logic Instructions, Benchmarking LLMs, Instruction Following, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces LogicIFGen and LogicIFEval to assess LLMs' performance on complex, logic-rich instructions defined by code functions, revealing current LLMs struggle with such tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating LLM performance on intricate, logic-conveying instructions is essential as instruction following is critical for advanced agentic behaviors.

**Method:** LogicIFGen is an automated framework for generating verifiable instructions from code functions. LogicIFEval is a benchmark created from these instructions, consisting of 426 entries to evaluate LLM instruction-following capabilities.

**Key Contributions:**

	1. Introduction of LogicIFGen for generating instructions from code functions
	2. Creation of LogicIFEval, a benchmark for evaluating LLMs on logic-rich tasks
	3. Demonstration of LLMs' limitations in instruction following.

**Result:** Experiments show most LLMs can only accurately follow fewer than 60% of the logic-rich instructions, indicating significant gaps in their instruction-following abilities.

**Limitations:** The benchmark may not cover all potential complexities in natural language reasoning.

**Conclusion:** The study highlights the challenges faced by state-of-the-art LLMs in understanding and executing complex instructions, emphasizing the need for further advancements in instruction following.

**Abstract:** Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF

</details>


### [85] [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models](https://arxiv.org/abs/2508.09138)

*Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Temporal Consistency, Text Generation, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper addresses the issue of intermediate predictions being discarded in diffusion large language models (dLLMs) and introduces methods to leverage temporal consistency for improved text generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current decoding strategies in dLLMs overlook valuable intermediate predictions, leading to suboptimal outputs. This work aims to mitigate this loss by exploiting temporal consistency in predictions.

**Method:** The paper introduces two methods: 1) Temporal Self-Consistency Voting, which aggregates predictions across denoising steps; 2) Temporal Consistency Reinforcement, using Temporal Semantic Entropy (TSE) as a reward signal for stable generation.

**Key Contributions:**

	1. Introduction of Temporal Self-Consistency Voting
	2. Development of Temporal Consistency Reinforcement with TSE
	3. Demonstration of significant empirical improvements across benchmarks

**Result:** The proposed methods yield significant improvements across multiple benchmarks, with an average gain of 24.7% on the Countdown dataset when using the negative TSE reward, and notable gains on GSM8K, MATH500, and SVAMP when combined with accuracy rewards.

**Limitations:** 

**Conclusion:** The study highlights the importance of temporal dynamics in dLLMs and provides effective methodologies to leverage these dynamics for enhanced text generation quality.

**Abstract:** Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.

</details>


### [86] [Quantifying Gender Biases Towards Politicians on Reddit](https://arxiv.org/abs/2112.12014)

*Sara Marjanovic, Karolina Stańczak, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** gender bias, political representation, online discourse, Reddit, misogyny

**Relevance Score:** 4

**TL;DR:** This study examines gender biases in online political discussions by analyzing 10 million Reddit comments about male and female politicians, revealing significant differences in the treatment of female politicians compared to their male counterparts.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to address the persistent issue of gender bias in political representation and to understand the role of online discourse in perpetuating these biases.

**Method:** The methodology involves collecting a large dataset of comments from Reddit, focusing on conversations about both male and female politicians, and conducting a detailed analysis of various forms of gender bias, including linguistic and extra-linguistic factors.

**Key Contributions:**

	1. Comprehensive analysis of gender biases in online political discussions
	2. Release of a large curated dataset for future research
	3. Identification of various forms of gender bias in social media language.

**Result:** The findings indicate that while there is equal public interest in female politicians, the language used to describe them is often less professional and more personal compared to male politicians, including references to their appearance and familial relations.

**Limitations:** The study focuses solely on Reddit comments and may not capture the full spectrum of online discourse on gender biases across other platforms.

**Conclusion:** The study concludes that implicit gender biases contribute to unequal representation, highlighting the need for awareness and further research in addressing these biases in digital discourse.

**Abstract:** Despite attempts to increase gender parity in politics, global efforts have struggled to ensure equal female representation. This is likely tied to implicit gender biases against women in authority. In this work, we present a comprehensive study of gender biases that appear in online political discussion. To this end, we collect 10 million comments on Reddit in conversations about male and female politicians, which enables an exhaustive study of automatic gender bias detection. We address not only misogynistic language, but also other manifestations of bias, like benevolent sexism in the form of seemingly positive sentiment and dominance attributed to female politicians, or differences in descriptor attribution. Finally, we conduct a multi-faceted study of gender bias towards politicians investigating both linguistic and extra-linguistic cues. We assess 5 different types of gender bias, evaluating coverage, combinatorial, nominal, sentimental, and lexical biases extant in social media language and discourse. Overall, we find that, contrary to previous research, coverage and sentiment biases suggest equal public interest in female politicians. Rather than overt hostile or benevolent sexism, the results of the nominal and lexical analyses suggest this interest is not as professional or respectful as that expressed about male politicians. Female politicians are often named by their first names and are described in relation to their body, clothing, or family; this is a treatment that is not similarly extended to men. On the now banned far-right subreddits, this disparity is greatest, though differences in gender biases still appear in the right and left-leaning subreddits. We release the curated dataset to the public for future studies.

</details>


### [87] [Utilizing Large Language Models for Information Extraction from Real Estate Transactions](https://arxiv.org/abs/2404.18043)

*Yu Zhao, Haoxiang Gao, Jinghan Cao, Shiqi Yang*

**Main category:** cs.CL

**Keywords:** real estate, large language models, data extraction, transformer architectures, information retrieval

**Relevance Score:** 4

**TL;DR:** This paper investigates using large language models for automated data extraction from real estate sales contracts, aiming to enhance efficiency and accuracy in analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Real estate sales contracts hold essential data, but traditional manual extraction is inefficient and prone to errors.

**Method:** The study employs transformer-based large language models and synthetically generated contracts for fine-tuning the model.

**Key Contributions:**

	1. Application of LLMs in real estate contract extraction
	2. Creation of synthetic contracts for model fine-tuning
	3. Significant improvements in data extraction metrics

**Result:** Fine-tuning on synthetic contracts led to improvements in metrics and qualitative aspects of information retrieval and reasoning tasks.

**Limitations:** 

**Conclusion:** Applying large language models can significantly enhance the efficiency and accuracy of real estate contract analysis.

**Abstract:** Real estate sales contracts contain crucial information for property transactions, but manual data extraction can be time-consuming and error-prone. This paper explores the application of large language models, specifically transformer-based architectures, for automated information extraction from real estate contracts. We discuss challenges, techniques, and future directions in leveraging these models to improve efficiency and accuracy in real estate contract analysis. We generated synthetic contracts using the real-world transaction dataset, thereby fine-tuning the large-language model and achieving significant metrics improvements and qualitative improvements in information retrieval and reasoning tasks.

</details>


### [88] [From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2410.06795)

*Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, Hallucinations, Feature Extraction, Multi-modal, Adaptive Virtual Tokens

**Relevance Score:** 6

**TL;DR:** This paper investigates hallucinations in large vision-language models (LVLMs) and proposes a novel tuning strategy called PATCH to mitigate them by improving feature extraction and decoupling.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in LVLMs, which generate inaccuracies not present in the visual input, and to investigate the underlying causes from an architectural perspective.

**Method:** The paper proposes a new tuning strategy named PATCH, which utilizes adaptive virtual tokens to enhance feature extraction from bounding boxes in LVLMs.

**Key Contributions:**

	1. Proposes a novel tuning strategy named PATCH for LVLMs.
	2. Demonstrates state-of-the-art performance on hallucination datasets.
	3. Investigates architectural aspects of visual encoders and modal alignment modules.

**Result:** PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets for LVLMs.

**Limitations:** 

**Conclusion:** The proposed method provides insights into the causes of hallucinations in LVLMs and fosters further advancements in the field.

**Abstract:** Hallucinations in large vision-language models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field.

</details>


### [89] [AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models](https://arxiv.org/abs/2501.13983)

*Yang Fan*

**Main category:** cs.CL

**Keywords:** Large Language Models, data contamination, dynamic evaluation, Bloom's taxonomy, fairness in evaluation

**Relevance Score:** 7

**TL;DR:** This paper presents AdEval, a dynamic evaluation method for Large Language Models (LLMs) to combat data contamination and improve evaluation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of data contamination in the evaluation of LLMs, which can lead to overestimated performance due to reliance on static benchmarks.

**Method:** AdEval extracts knowledge points from static datasets for dynamic alignment, utilizes online searches for background information, and formulates questions based on Bloom's cognitive hierarchy to enable multi-level assessments.

**Key Contributions:**

	1. Introduction of AdEval for dynamic evaluation of LLMs
	2. Use of Bloom's cognitive hierarchy for multi-level assessment
	3. Demonstration of improved evaluation fairness and diversity

**Result:** Experimental results indicate that AdEval reduces the impact of data contamination, enhances complexity control, and improves the fairness and diversity of LLM evaluations.

**Limitations:** The paper faces criticisms regarding potential data falsification and plagiarism in its methodology.

**Conclusion:** AdEval offers a more reliable and comprehensive method to evaluate LLMs, effectively addressing common pitfalls in traditional static evaluation methods.

**Abstract:** As Large Language Models (LLMs) are pre-trained on ultra-large-scale corpora, the problem of data contamination is becoming increasingly serious, and there is a risk that static evaluation benchmarks overestimate the performance of LLMs. To address this, this paper proposes a dynamic data evaluation method called AdEval (Alignment-based Dynamic Evaluation). AdEval first extracts knowledge points and main ideas from static datasets to achieve dynamic alignment with the core content of static benchmarks, and by avoiding direct reliance on static datasets, it inherently reduces the risk of data contamination from the source. It then obtains background information through online searches to generate detailed descriptions of the knowledge points. Finally, it designs questions based on Bloom's cognitive hierarchy across six dimensions-remembering, understanding, applying, analyzing, evaluating, and creating to enable multi-level cognitive assessment. Additionally, AdEval controls the complexity of dynamically generated datasets through iterative question reconstruction. Experimental results on multiple datasets show that AdEval effectively alleviates the impact of data contamination on evaluation results, solves the problems of insufficient complexity control and single-dimensional evaluation, and improves the fairness, reliability and diversity of LLMs evaluation.

</details>


### [90] [EvoP: Robust LLM Inference via Evolutionary Pruning](https://arxiv.org/abs/2502.14910)

*Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan, Chun Jason Xue*

**Main category:** cs.CL

**Keywords:** Large Language Models, model pruning, evolutionary algorithms, calibration dataset, practical deployment

**Relevance Score:** 9

**TL;DR:** EvoP is an evolutionary pruning framework designed to enhance the efficiency of Large Language Models (LLMs) by creating diverse calibration datasets and optimizing pruning patterns.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve deployment of large language models in resource-constrained environments by addressing limitations of existing pruning methods.

**Method:** EvoP employs a cluster-based calibration dataset sampling strategy to enhance dataset diversity, followed by an evolutionary pruning pattern searching method to identify optimal pruning configurations.

**Key Contributions:**

	1. Introduction of the cluster-based calibration dataset sampling strategy for LLMs.
	2. Development of evolutionary pruning pattern searching to optimize model efficiency.
	3. Validation of EvoP's effectiveness through experiments on various LLMs and tasks.

**Result:** EvoP outperforms existing model pruning techniques, achieving superior performance and efficiency across multiple LLMs and downstream tasks.

**Limitations:** 

**Conclusion:** EvoP presents a practical and scalable solution for deploying large language models effectively in real-world applications.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing model pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model.   To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing model pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.

</details>


### [91] [CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation](https://arxiv.org/abs/2504.00043)

*Jixuan Leng, Chengsong Huang, Langlin Huang, Bill Yuchen Lin, William W. Cohen, Haohan Wang, Jiaxin Huang*

**Main category:** cs.CL

**Keywords:** crossword puzzles, large language models, multimodal evaluation, reasoning, benchmarking

**Relevance Score:** 7

**TL;DR:** CrossWordBench is a benchmark for evaluating reasoning capabilities of LLMs and LVLMs using crossword puzzles, revealing performance differences between reasoning and non-reasoning models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing frameworks fail to assess both text and visual reasoning capabilities together, necessitating a new approach to evaluate multimodal reasoning.

**Method:** CrossWordBench introduces a puzzle generation framework that creates crossword puzzles in text and image formats with adjustable difficulty and various evaluation strategies.

**Key Contributions:**

	1. Introduction of CrossWordBench for multimodal reasoning evaluation
	2. Development of a controllable puzzle generation framework
	3. Identification of performance gaps between LLMs and LVLMs

**Result:** Over 20 models were evaluated, showing reasoning LLMs outperform non-reasoning models, while LVLMs exhibited difficulties linked to grid-parsing accuracy.

**Limitations:** 

**Conclusion:** Current LLMs and LVLMs have limitations in reasoning capabilities; CrossWordBench provides a new method for multimodal task evaluation.

**Abstract:** Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly assess either text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles -- a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in two formats (text and image), supports adjustable difficulty through prefill ratio control, and offers different evaluation strategies, ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs substantially outperform non-reasoning models by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings highlight limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.

</details>


### [92] [ChatBench: From Static Benchmarks to Human-AI Evaluation](https://arxiv.org/abs/2504.07114)

*Serina Chang, Ashton Anderson, Jake M. Hofman*

**Main category:** cs.CL

**Keywords:** LLM, user-AI conversations, ChatBench, evaluation, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a user study to evaluate human-LLM collaboration using a new dataset, ChatBench, which highlights discrepancies between AI-alone and user-AI performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid adoption of LLM-based chatbots raises the need to assess their efficacy in human-LLM collaborations, beyond isolated evaluations.

**Method:** A user study that converts standard MMLU questions into interactive user-AI conversations, creating the ChatBench dataset, which includes distinct data for user-alone, AI-alone, and user-AI interactions.

**Key Contributions:**

	1. Introduction of ChatBench dataset with comprehensive user-AI interaction data
	2. Demonstrated the divergence of AI-alone and user-AI performance
	3. Improved user simulator correlation through fine-tuning on ChatBench data

**Result:** AI-alone accuracy does not correlate with user-AI accuracy, revealing significant performance variances across subjects like math and physics; furthermore, fine-tuning a user simulator enhances predictive accuracy for user-AI interactions.

**Limitations:** The dataset is limited to 396 questions and two LLMs, which may not generalize to all conversational contexts.

**Conclusion:** The findings emphasize the necessity of evaluating chatbot performance in collaborative contexts and suggest that tailored evaluations can significantly improve accuracy estimations.

**Abstract:** With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.

</details>


### [93] [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)

*Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, ambiguity, misinformation, multi-agent systems

**Relevance Score:** 9

**TL;DR:** This paper introduces RAMDocs, a dataset for evaluating LLM performance in the presence of ambiguity and misinformation, and MADAM-RAG, a multi-agent method to improve LLM responses to conflicting information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the factuality of LLM responses amidst ambiguity, misinformation, and conflicting information from various sources.

**Method:** The authors propose a new dataset (RAMDocs) for complex query scenarios and a multi-agent debate mechanism (MADAM-RAG) for LLMs to determine the best response while filtering out noise and inaccuracies.

**Key Contributions:**

	1. Introduced RAMDocs dataset simulating complex query scenarios.
	2. Developed MADAM-RAG, a multi-agent approach for improved LLM responses.
	3. Demonstrated substantial performance gains over existing RAG methods.

**Result:** MADAM-RAG demonstrates improvements over traditional RAG baselines, achieving up to 11.40% better performance on AmbigDocs and 15.80% on FaithEval with Llama3.3-70B-Instruct.

**Limitations:** The analysis indicates a remaining gap in performance when evidence imbalance and misinformation increase.

**Conclusion:** While MADAM-RAG starts to address issues of ambiguity and misinformation for LLM agents, significant performance gaps remain, particularly when the supporting evidence is imbalanced.

**Abstract:** Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.

</details>
