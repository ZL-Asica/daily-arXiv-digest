# 2025-06-04

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 186]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)

*Vassilis Lyberatos, Spyridon Kantarelis, Ioanna Zioga, Christina Anagnostopoulou, Giorgos Stamou, Anastasia Georgaki*

**Main category:** cs.HC

**Keywords:** emotional expression, music performance, audience engagement, improvisation, neurophysiological measurements

**Relevance Score:** 2

**TL;DR:** The study explores emotional expression in music performance through computational and neurophysiological methods, revealing the impact of performance settings and expressiveness on emotional communication and audience reaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how different performance settings and levels of expressiveness in music influence emotional expression and listener perception.

**Method:** The study employed both computational audio analysis and neurophysiological measurements to assess emotional communication in music performances, using different repertoires and expressiveness levels.

**Key Contributions:**

	1. Introduced a multimodal approach to study emotional expression in music performance.
	2. Demonstrated the unique acoustic features of expressive and improvisational music performances.
	3. Provided insights into the neurophysiological responses elicited by varying levels of expressiveness in music.

**Result:** Expressive and improvisational performances exhibited unique acoustic features, leading to stronger emotional responses from listeners, while neurophysiological data indicated greater relaxation in improvisational settings.

**Limitations:** 

**Conclusion:** Expressivity in music performance significantly enhances emotional communication and audience engagement, emphasizing its importance in musical interpretation.

**Abstract:** This study investigates emotional expression and perception in music performance using computational and neurophysiological methods. The influence of different performance settings, such as repertoire, diatonic modal etudes, and improvisation, as well as levels of expressiveness, on performers' emotional communication and listeners' reactions is explored. Professional musicians performed various tasks, and emotional annotations were provided by both performers and the audience. Audio analysis revealed that expressive and improvisational performances exhibited unique acoustic features, while emotion analysis showed stronger emotional responses. Neurophysiological measurements indicated greater relaxation in improvisational performances. This multimodal study highlights the significance of expressivity in enhancing emotional communication and audience engagement.

</details>


### [2] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)

*Takao Fujii, Katie Seaborn, Madeleine Steeds, Jun Kato*

**Main category:** cs.HC

**Keywords:** Conversational agents, Anthropomorphism, Voice identity, Intersectionality, Cultural sensitivity

**Relevance Score:** 7

**TL;DR:** This study investigates the impact of voice and non-pronominal self-referents in shaping perceptions of digital agents’ identities among Japanese users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the ethics of anthropomorphizing machines and the implications of using culturally specific self-referents in conversational agents.

**Method:** A crowdsourcing study involving 204 Japanese participants evaluated three ChatGPT voices with seven different self-referents to analyze perceptions of gender, age, and formality.

**Key Contributions:**

	1. Identifies the role of non-pronominal self-referents in shaping identity perceptions of conversational agents.
	2. Reveals the intersectionality of gender, age, and formality in the context of voice agents.
	3. Calls for culturally-sensitive approaches in the design of human-like agents.

**Result:** The study found strong evidence of voice gendering and showed how intersectional self-referents can create ambiguity, evading conventional gendering assumptions.

**Limitations:** Limited to Japanese participants; findings may not generalize across other cultures or languages.

**Conclusion:** The findings highlight the need for culturally-sensitive design in voice agents, emphasizing the intricate relationship between self-referents and user perceptions of identity.

**Abstract:** Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other "neutral" non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents.

</details>


### [3] [Composable Building Blocks for Controllable and Transparent Interactive AI Systems](https://arxiv.org/abs/2506.02262)

*Sebe Vanbrabant, Gustavo Rovelo Ruiz, Davy Vanacken*

**Main category:** cs.HC

**Keywords:** Explainable AI, Interactive Systems, Interpretability, Structural Building Blocks, Human-Machine Communication

**Relevance Score:** 8

**TL;DR:** This paper proposes a structured approach to represent interactive systems using building blocks that improve the interpretability of AI models and their architectures, addressing the black box problem in AI systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI technologies in interactive systems has led to increased complexity, where the black box nature of AI models hampers understanding. Explainable AI (XAI) techniques can clarify individual models, but the overall system architecture remains opaque.

**Method:** The authors propose a method to represent interactive systems as sequences of structural building blocks which include AI models and control mechanisms. These blocks can be explained through visual aids and XAI techniques, forming a comprehensive overview of the system's structure and flow.

**Key Contributions:**

	1. Development of a flow-based architecture for interactive systems using structural building blocks.
	2. Enhancement of interpretability for both individual AI models and the overall system architecture through visual explanations.
	3. Creation of a prototype interactive system to showcase the proposed approach.

**Result:** The proposed approach facilitates alignment between human and machine interpretability, allowing for clearer communication regarding the system's architecture and functioning. A prototype interactive system was developed to demonstrate the approach.

**Limitations:** The approach may require further validation in diverse scenarios and with various types of interactive systems to assess its generalizability and effectiveness.

**Conclusion:** By utilizing a flow-based architecture and structural building blocks, the proposed method enhances the understanding of interactive systems that employ AI technologies, making them more accessible to users.

**Abstract:** While the increased integration of AI technologies into interactive systems enables them to solve an equally increasing number of tasks, the black box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. To this end, we propose an approach to represent interactive systems as sequences of structural building blocks, such as AI models and control mechanisms grounded in the literature. These can then be explained through accompanying visual building blocks, such as XAI techniques. The flow and APIs of the structural building blocks form an explicit overview of the system. This serves as a communication basis for both humans and automated agents like LLMs, aligning human and machine interpretability of AI models. We discuss a selection of building blocks and concretize our flow-based approach in an architecture and accompanying prototype interactive system.

</details>


### [4] [Visualization for interactively adjusting the de-bias effect of word embedding](https://arxiv.org/abs/2506.02447)

*Arisa Sugino, Takayuki Itoh*

**Main category:** cs.HC

**Keywords:** word embeddings, gender bias, debiasing, interactive visualization, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper addresses gender bias in Japanese word embeddings by proposing an interactive visualization method to adjust debiasing levels per word category while monitoring model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the impact of learned biases in word embeddings, particularly gender bias, without degrading model performance.

**Method:** An interactive visualization tool allows users to adjust debiasing parameters based on the accuracy of a category classification task, alongside presenting multiple debiasing configurations using an optimization scheme.

**Key Contributions:**

	1. Interactive visualization for debiasing word embeddings
	2. User-driven adjustment of debiasing parameters
	3. Optimization scheme for presenting debiasing options

**Result:** The study shows significant variation in debiasing influence across different categories of words, emphasizing the need for tailored debiasing approaches.

**Limitations:** The focus is solely on gender bias in Japanese, which may not generalize to other types of biases or languages.

**Conclusion:** The proposed method allows for user-specific adjustments in debiasing processes, balancing bias removal and performance retention in word embeddings.

**Abstract:** Word embedding, which converts words into numerical values, is an important natural language processing technique and widely used. One of the serious problems of word embedding is that the bias will be learned and affect the model if the dataset used for pre-training contains bias. On the other hand, indiscriminate removal of bias from word embeddings may result in the loss of information, even if the bias is undesirable to us. As a result, a risk of model performance degradation due to bias removal will be another problem. As a solution to this problem, we focus on gender bias in Japanese and propose an interactive visualization method to adjust the degree of debias for each word category. Specifically, we visualize the accuracy in a category classification task after debiasing, and allow the user to adjust the parameters based on the visualization results, so that the debiasing can be adjusted according to the user's objectives. In addition, considering a trade-off between debiasing and preventing degradation of model performance, and that different people perceive gender bias differently, we developed a mechanism to present multiple choices of debiasing configurations applying an optimization scheme. This paper presents the results of an experiment in which we removed the gender bias for word embeddings learned from the Japanese version of Wikipedia. We classified words into five categories based on a news corpus, and observed that the degree of influence of debiasing differed greatly among the categories. We then adjusted the degree of debiasing for each category based on the visualization results.

</details>


### [5] [To Embody or Not: The Effect Of Embodiment On User Perception Of LLM-based Conversational Agents](https://arxiv.org/abs/2506.02514)

*Kyra Wang, Boon-Kiat Quek, Jessica Goh, Dorien Herremans*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Embodiment, Large Language Models, User Perception, Sycophancy

**Relevance Score:** 9

**TL;DR:** This paper investigates how the embodiment of conversational agents (CAs) using large language models affects user perception during cooperative tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the research gap regarding the impact of embodiment on user perception of LLM-based conversational agents in cooperative contexts.

**Method:** A mixed-methods within-subjects study was conducted to compare user perceptions of embodied versus non-embodied CAs during cooperative tasks.

**Key Contributions:**

	1. Investigation of embodiment effects on LLM-based conversational agents
	2. Findings suggesting embodiment can hinder perceived credibility
	3. Qualitative insights into user perceptions of sycophancy in embodied CAs

**Result:** The non-embodied agent was rated significantly more competent than the embodied one; qualitative feedback indicated that the embodied agent was perceived as more sycophantic.

**Limitations:** The study is limited to specific tasks and settings, and may not generalize to all contexts of CA use.

**Conclusion:** Embodiment may negatively impact the perceived credibility of conversational agents if they are perceived to be sycophantic, challenging existing beliefs about the positive effects of embodiment.

**Abstract:** Embodiment in conversational agents (CAs) refers to the physical or visual representation of these agents, which can significantly influence user perception and interaction. Limited work has been done examining the effect of embodiment on the perception of CAs utilizing modern large language models (LLMs) in non-hierarchical cooperative tasks, a common use case of CAs as more powerful models become widely available for general use. To bridge this research gap, we conducted a mixed-methods within-subjects study on how users perceive LLM-based CAs in cooperative tasks when embodied and non-embodied. The results show that the non-embodied agent received significantly better quantitative appraisals for competence than the embodied agent, and in qualitative feedback, many participants believed that the embodied CA was more sycophantic than the non-embodied CA. Building on prior work on users' perceptions of LLM sycophancy and anthropomorphic features, we theorize that the typically-positive impact of embodiment on perception of CA credibility can become detrimental in the presence of sycophancy. The implication of such a phenomenon is that, contrary to intuition and existing literature, embodiment is not a straightforward way to improve a CA's perceived credibility if there exists a tendency to sycophancy.

</details>


### [6] [Cognitive Load-Driven VR Memory Palaces: Personalizing Focus and Recall Enhancement](https://arxiv.org/abs/2506.02700)

*Zhengyang Li, Hailin Deng*

**Main category:** cs.HC

**Keywords:** Cognitive Load, Virtual Reality, Memory Performance, EEG, Personalized Experience

**Relevance Score:** 7

**TL;DR:** This study investigates using Virtual Reality to enhance memory through personalized environments based on individual cognitive load profiles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize Virtual Reality environments tailored to individual cognitive load levels to improve focus and memory performance.

**Method:** EEG devices were used to monitor participants' Beta wave activity, while polynomial regression modeled their cognitive load profiles, adjusting spatial variables in the VR environment dynamically.

**Key Contributions:**

	1. Integration of VR with memory palace techniques
	2. Dynamic adjustment of VR spatial variables based on cognitive load
	3. Empirical evidence of improved cognitive performance using personalized VR settings

**Result:** 8 out of 10 participants exhibited an increase in Beta wave activity, indicating improved focus and cognitive performance in customized VR settings.

**Limitations:** 

**Conclusion:** The findings highlight the potential of VR-based memory environments that consider cognitive load, offering new insights for VR memory research.

**Abstract:** Cognitive load, which varies across individuals, can significantly affect focus and memory performance.This study explores the integration of Virtual Reality (VR) with memory palace techniques, aiming to optimize VR environments tailored to individual cognitive load levels to improve focus and memory. We utilized EEG devices, specifically the Oculus Quest 2, to monitor Beta wave activity in 10 participants.By modeling their cognitive load profiles through polynomial regression, we dynamically adjusted spatial variables within a VR environment using Grasshopper, creating personalized experiences. Results indicate that 8 participants showed a notable increase in Beta wave activity, demonstrating improved focus and cognitive performance in the customized VR settings.These findings underscore the potential of VR-based memory environments, driven by cognitive load considerations, and provide valuable insights for advancing VR memory research

</details>


### [7] [Heatables: Effects of Infrared-LED-Induced Ear Heating on Thermal Perception, Comfort, and Cognitive Performance](https://arxiv.org/abs/2506.02714)

*Valeria Zitz, Michael Küttner, Jonas Hummel, Michael T. Knierim, Michael Beigl, Tobias Röddiger*

**Main category:** cs.HC

**Keywords:** thermal comfort, wearable technology, heat stimulation, NIR-IR radiation, user-centered design

**Relevance Score:** 7

**TL;DR:** This paper explores a novel in-ear wearable device, Heatables, that uses Near-Infrared (NIR) and Infrared (IR) radiation to enhance thermal comfort by providing localized heating.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Maintaining thermal comfort in shared indoor environments is challenging, especially with traditional HVAC systems, and cold exposure can impair cognitive performance.

**Method:** The study used Heatables, an in-ear device emitting NIR and IR radiation, in a placebo-controlled experiment with 24 participants exposed to a cool office environment for 150 minutes.

**Key Contributions:**

	1. Introduction of the Heatables wearable device.
	2. Demonstration of effective localized heating for thermal comfort enhancement.
	3. Validation of benefits beyond the immediate area of application.

**Result:** Heatables significantly increased perceived ambient temperature by approximately 1.5 degrees Celsius and delayed cold discomfort; thermal benefits were observed beyond the ear region, enhancing whole-body comfort.

**Limitations:** The sample size was limited to 24 participants, and longer-term effects were not assessed.

**Conclusion:** In-ear NIR-IR-LED stimulation shows promise for enhancing thermal comfort in everyday situations unobtrusively.

**Abstract:** Maintaining thermal comfort in shared indoor environments remains challenging, as centralized HVAC systems are slow to adapt and standardized to group norms. Cold exposure not only reduces subjective comfort but can impair cognitive performance, particularly under moderate to severe cold stress. Personal Comfort Systems (PCS) have shown promise by providing localized heating, yet many designs target distal body parts with low thermosensitivity and often lack portability. In this work, we investigate whether targeted thermal stimulation using in-ear worn devices can manipulate thermal perception and enhance thermal comfort. We present Heatables, a novel in-ear wearable that emits Near-Infrared (NIR) and Infrared (IR) radiation via integrated LEDs to deliver localized optical heating. This approach leverages NIR-IR's ability to penetrate deeper tissues, offering advantages over traditional resistive heating limited to surface warming. In a placebo-controlled study with 24 participants, each exposed for 150 minutes in a cool office environment (approximately 17.5 degrees Celsius) to simulate sustained cold stress during typical sedentary office activities, Heatables significantly increased the perceived ambient temperature by around 1.5 degrees Celsius and delayed cold discomfort. Importantly, thermal benefits extended beyond the ear region, improving both whole-body comfort and thermal acceptability. These findings position in-ear NIR-IR-LED-based stimulation as a promising modality for unobtrusive thermal comfort enhancement in everyday contexts.

</details>


### [8] [Exploring listeners' perceptions of AI-generated and human-composed music for functional emotional applications](https://arxiv.org/abs/2506.02856)

*Kimaya Lecamwasam, Tishya Ray Chaudhuri*

**Main category:** cs.HC

**Keywords:** AI-generated music, human-composed music, emotional resonance, music appraisal, emotional authenticity

**Relevance Score:** 4

**TL;DR:** The study explores listener perceptions of AI-generated vs human-composed music, focusing on emotional resonance and preference under various conditions.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI-generated music is evaluated compared to human music, particularly in emotional contexts.

**Method:** A mixed-methods design was employed where participants rated music classified as AI or human, under different labeling and emotion conditions.

**Key Contributions:**

	1. Found differences in preference versus emotional impact between AI and human music
	2. Qualitative insights linking humanness in music to emotional qualities
	3. Challenges the notion of AI as a replacement for human creativity in music.

**Result:** Participants preferred AI-generated music despite rating human music as more effective in eliciting specific emotional states, highlighting a contradiction in emotional authenticity perceptions.

**Limitations:** Quantitative analyses showed no significant differences in emotional response despite preference ratings.

**Conclusion:** The findings suggest that preference for AI music does not equate to emotional efficacy, prompting a re-evaluation of how AI in music should be designed to prioritize human values in emotional contexts.

**Abstract:** This work investigates how listeners perceive and evaluate AI-generated as compared to human-composed music in the context of emotional resonance and regulation. Across a mixed-methods design, participants were exposed to both AI and human music under various labeling conditions (music correctly labeled as AI- or human-origin, music incorrectly labeled as AI- or human-origin, and unlabeled music) and emotion cases (Calm and Upbeat), and were asked to rate preference, efficacy of target emotion elicitation, and emotional impact. Participants were significantly more likely to rate human-composed music, regardless of labeling, as more effective at eliciting target emotional states, though quantitative analyses revealed no significant differences in emotional response. However, participants were significantly more likely to indicate preference for AI-generated music, yielding further questions regarding the impact of emotional authenticity and perceived authorship on musical appraisal. Qualitative data underscored this, with participants associating humanness with qualities such as imperfection, flow, and 'soul.' These findings challenge the assumption that preference alone signals success in generative music systems. Rather than positioning AI tools as replacements for human creativity or emotional expression, they point toward a more careful design ethos that acknowledges the limits of replication and prioritizes human values such as authenticity, individuality, and emotion regulation in wellness and affective technologies.

</details>


### [9] [Unpacking Graduate Students' Learning Experience with Generative AI Teaching Assistant in A Quantitative Methodology Course](https://arxiv.org/abs/2506.02966)

*Zhanxin Hao, Haifeng Luo, Yongyi Chen, Yu Zhang*

**Main category:** cs.HC

**Keywords:** AI tools, graduate education, Bloom's taxonomy, CLEAR framework, student inquiry

**Relevance Score:** 4

**TL;DR:** The study analyzes AI usage among graduate students in a research methods course, revealing varying patterns of inquiry based on students' backgrounds and understanding of course content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how students engage with AI assistants during a quantitative research methods course and to assess the impact of their academic backgrounds on AI usage.

**Method:** The study involved coding student inquiries using Bloom's taxonomy and the CLEAR framework, followed by statistical analyses including t-tests and poisson regression, and post-course interviews for qualitative insights.

**Key Contributions:**

	1. Identified patterns in AI usage among students of different academic strengths
	2. Utilized Bloom's taxonomy and CLEAR framework for inquiry coding
	3. Provided insights for improving AI tool engagement in education

**Result:** A U-shaped pattern was observed in AI usage, with higher engagement at both the start and end of the course, and lower usage mid-term. Students with weaker mathematical backgrounds utilized the AI more, but with less structured inquiries.

**Limitations:** Study limited to a specific course and small sample size; findings may not generalize beyond this context.

**Conclusion:** Targeted guidance is necessary to better assist students with varying academic proficiency, aiming to enhance AI tool effectiveness in educational settings.

**Abstract:** The study was conducted in an Advanced Quantitative Research Methods course involving 20 graduate students. During the course, student inquiries made to the AI were recorded and coded using Bloom's taxonomy and the CLEAR framework. A series of independent sample t-tests and poisson regression analyses were employed to analyse the characteristics of different questions asked by students with different backgrounds. Post course interviews were conducted with 10 students to gain deeper insights into their perceptions. The findings revealed a U-shaped pattern in students' use of the AI assistant, with higher usage at the beginning and towards the end of the course, and a decrease in usage during the middle weeks. Most questions posed to the AI focused on knowledge and comprehension levels, with fewer questions involving deeper cognitive thinking. Students with a weaker mathematical foundation used the AI assistant more frequently, though their inquiries tended to lack explicit and logical structure compared to those with a strong mathematical foundation, who engaged less with the tool. These patterns suggest the need for targeted guidance to optimise the effectiveness of AI tools for students with varying levels of academic proficiency.

</details>


### [10] [Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps](https://arxiv.org/abs/2506.02993)

*Zhanxin Hao, Jie Cao, Ruimiao Li, Jifan Yu, Zhiyuan Liu, Yu Zhang*

**Main category:** cs.HC

**Keywords:** multi-agent AI, personalized learning, student engagement, technology acceptance, learning outcomes

**Relevance Score:** 8

**TL;DR:** This study examines the interaction patterns of university students with multi-agent AI systems and their impact on learning outcomes and motivation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how student engagement with multiple AI agents influences cognitive outcomes and non-cognitive factors in personalized learning environments.

**Method:** The research used a dataset of 305 university students interacting with an online platform (MAIC) featuring multi-agent AI, analyzing pre- and post-test scores, self-reported motivation, and technology acceptance across 19,365 lines of dialogue.

**Key Contributions:**

	1. Identified engagement patterns in student-AI interactions
	2. Demonstrated varying impacts based on students' prior knowledge
	3. Showed increased technology acceptance across diverse student groups

**Result:** Two engagement patterns were identified: co-construction of knowledge (beneficial for less knowledgeable students) and co-regulation (more common among students with higher knowledge), with technology acceptance increasing for all participants.

**Limitations:** The study is limited by its focus on one learning platform and may not generalize to all educational contexts.

**Conclusion:** Multi-agent AI can cater to diverse student needs, enhancing personalized learning and diminishing performance gaps, suggesting directions for future research.

**Abstract:** Multi-agent AI systems, which simulate diverse instructional roles such as teachers and peers, offer new possibilities for personalized and interactive learning. Yet, student-AI interaction patterns and their pedagogical implications remain unclear. This study explores how university students engaged with multiple AI agents, and how these interactions influenced cognitive outcomes (learning gains) and non-cognitive factors (motivation, technology acceptance). Based on MAIC, an online learning platform with multi-agent, the research involved 305 university students and 19,365 lines of dialogue data. Pre- and post-test scores, self-reported motivation and technology acceptance were also collected. The study identified two engagement patterns: co-construction of knowledge and co-regulation. Lag sequential analysis revealed that students with lower prior knowledge relied more on co-construction of knowledge sequences, showing higher learning gains and post-course motivation. In contrast, students with higher prior knowledge engaged more in co-regulation behaviors but exhibited limited learning improvement. Technology acceptance increased across all groups. These findings suggest that multi-agent AI systems can adapt to students' varying needs, support differentiated engagement, and reduce performance gaps. Implications for personalized system design and future research directions are discussed.

</details>


### [11] [Feedstack: Layering Structured Representations over Unstructured Feedback to Scaffold Human AI Conversation](https://arxiv.org/abs/2506.03052)

*Hannah Vy Nguyen, Yu-Chun Grace Yen, Omar Shakir, Hang Huynh, Sebastian Gutierrez, June A. Smith, Sheila Jimenez, Salma Abdelgelil, Stephen MacNeil*

**Main category:** cs.HC

**Keywords:** Conversational User Interfaces, Feedback Systems, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Feedstack is a speculative interface designed to enhance feedback conversations through layered interaction techniques and structured representations, promoting exploration and understanding between users and AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To move beyond simple turn-based dialogue in digital conversations to allow for richer interactions and shared understanding between users and AI systems.

**Method:** A research-through-design approach was taken to develop Feedstack, and formative studies were conducted with novice designers to explore their engagement with the system's layered features.

**Key Contributions:**

	1. Introduction of Feedstack, a novel conversational user interface.
	2. Exploratory insights into layered affordances for feedback conversations.
	3. New directions for future conversational feedback systems.

**Result:** The findings indicate how layered structures in conversations can surface user intent and reveal design principles, providing insights into potential new directions for conversational feedback systems.

**Limitations:** The studies conducted were formative and not conclusive, limiting the evaluation of Feedstack's effectiveness.

**Conclusion:** Feedstack acts as a design probe, reflecting on its potential to enhance conversational feedback rather than delivering final evaluations.

**Abstract:** Many conversational user interfaces facilitate linear conversations with turn-based dialogue, similar to face-to-face conversations between people. However, digital conversations can afford more than simple back-and-forth; they can be layered with interaction techniques and structured representations that scaffold exploration, reflection, and shared understanding between users and AI systems. We introduce Feedstack, a speculative interface that augments feedback conversations with layered affordances for organizing, navigating, and externalizing feedback. These layered structures serve as a shared representation of the conversation that can surface user intent and reveal underlying design principles. This work represents an early exploration of this vision using a research-through-design approach. We describe system features and design rationale, and present insights from two formative (n=8, n=8) studies to examine how novice designers engage with these layered supports. Rather than presenting a conclusive evaluation, we reflect on Feedstack as a design probe that opens up new directions for conversational feedback systems.

</details>


### [12] [Assessing Workers Neuro-physiological Stress Responses to Augmented Reality Safety Warnings in Immersive Virtual Roadway Work Zones](https://arxiv.org/abs/2506.03113)

*Fatemeh Banani Ardecani, Omidreza Shoghli*

**Main category:** cs.HC

**Keywords:** Virtual Reality, neuro-physiological stress, augmented reality, construction safety, sensor integration

**Relevance Score:** 4

**TL;DR:** The paper presents a framework integrating VR simulations and sensors to study construction workers' stress responses to AR-enabled warnings during roadway tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the neuro-physiological stress responses of construction workers under multi-sensory AR-enabled warnings within a VR environment.

**Method:** Participants engaged in roadway maintenance tasks of varying intensities while physiological metrics (EDA, HRV, EEG) were monitored and analyzed.

**Key Contributions:**

	1. Integration of VR and wearable sensors for stress research
	2. Identification of physiological markers correlating with task intensity
	3. Insights into the cognitive and autonomic interplay during stress

**Result:** Task intensity significantly affected stress indicators, with moderate tasks increasing heart rate and producing identifiable changes in EEG patterns.

**Limitations:** 

**Conclusion:** The study underscores the connection between mental processing and physical stress responses in hazardous work conditions, with implications for AR safety systems.

**Abstract:** This paper presents a multi-stage experimental framework that integrates immersive Virtual Reality (VR) simulations, wearable sensors, and advanced signal processing to investigate construction workers neuro-physiological stress responses to multi-sensory AR-enabled warnings. Participants performed light- and moderate-intensity roadway maintenance tasks within a high-fidelity VR roadway work zone, while key stress markers of electrodermal activity (EDA), heart rate variability (HRV), and electroencephalography (EEG) were continuously measured. Statistical analyses revealed that task intensity significantly influenced physiological and neurological stress indicators. Moderate-intensity tasks elicited greater autonomic arousal, evidenced by elevated heart rate measures (mean-HR, std-HR, max-HR) and stronger electrodermal responses, while EEG data indicated distinct stress-related alpha suppression and beta enhancement. Feature-importance analysis further identified mean EDR and short-term HR metrics as discriminative for classifying task intensity. Correlation results highlighted a temporal lag between immediate neural changes and subsequent physiological stress reactions, emphasizing the interplay between cognition and autonomic regulation during hazardous tasks.

</details>


### [13] [A Comparative Study of Scanpath Models in Graph-Based Visualization](https://arxiv.org/abs/2503.24160)

*Angela Lopez-Cardona, Parvin Emami, Sebastian Idesis, Saravanakumar Duraisamy, Luis A. Leiva, Ioannis Arapakis*

**Main category:** cs.HC

**Keywords:** Information Visualization, Eye-tracking, Gaze prediction, Model evaluation, Visual analytics

**Relevance Score:** 7

**TL;DR:** This study evaluates computational models for predicting gaze patterns in Information Visualization (InfoVis) systems using eye-tracking data from participants analyzing graphs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to understand visual attention allocation to optimize interface design and the challenges of collecting eye-tracking data.

**Method:** Conducted an eye-tracking experiment with 40 participants, comparing human scanpaths to synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.

**Key Contributions:**

	1. Evaluation of gaze prediction models in InfoVis
	2. Analysis of the impact of question complexity on gaze patterns
	3. Advancement of predictive modeling in visual analytics

**Result:** The study found that the computational models could approximate human gaze patterns, with performance influenced by question complexity and node number.

**Limitations:** Potential limitations include the specificity of the context (digital forensics) and variability in individual gaze behavior.

**Conclusion:** This research contributes to predictive modeling in visual analytics, providing insights for better InfoVis system design.

**Abstract:** Information Visualization (InfoVis) systems utilize visual representations to enhance data interpretation. Understanding how visual attention is allocated is essential for optimizing interface design. However, collecting Eye-tracking (ET) data presents challenges related to cost, privacy, and scalability. Computational models provide alternatives for predicting gaze patterns, thereby advancing InfoVis research. In our study, we conducted an ET experiment with 40 participants who analyzed graphs while responding to questions of varying complexity within the context of digital forensics. We compared human scanpaths with synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer. Our research evaluates the accuracy of these models and examines how question complexity and number of nodes influence performance. This work contributes to the development of predictive modeling in visual analytics, offering insights that can enhance the design and effectiveness of InfoVis systems.

</details>


### [14] [An AI-powered Public Health Automated Kiosk System for Personalized Care: An Experimental Pilot Study](https://arxiv.org/abs/2504.13880)

*Sonya Falahati, Morteza Alizadeh, Fatemeh Ghazipour, Zhino Safahi, Navid Khaledian, Mohammad R. Salmanpour*

**Main category:** cs.HC

**Keywords:** HERMES Kiosk, OTC medication, Artificial Intelligence, Federated Learning, Public Health

**Relevance Score:** 9

**TL;DR:** The HERMES Kiosk provides personalized OTC medication recommendations using an AI model enhanced with GAT and MHCA, while ensuring user privacy through federated learning.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of traditional health kiosks by providing AI-driven personalized OTC medication recommendations in public areas.

**Method:** HERMES analyzes self-reported symptoms and anonymized medical histories using AI; trained on EHR and DDI data, with emphasis on user privacy and accessibility features.

**Key Contributions:**

	1. Implementation of GAMENet model with GAT and MHCA
	2. Privacy-preserving design using federated learning
	3. Multilingual support and accessibility features for public health kiosks

**Result:** The enhanced GAMENet model achieved a PRAUC of 0.74, indicating improved accuracy in healthcare recommendations over previous models.

**Limitations:** 

**Conclusion:** HERMES showcases the potential of AI-powered kiosks to enhance public health access and reduce healthcare system burdens, with future efforts aimed at deployment and scalability.

**Abstract:** Background: The HERMES Kiosk (Healthcare Enhanced Recommendations through Artificial Intelligence & Expertise System) is designed to provide personalized Over-the-Counter (OTC) medication recommendations, addressing the limitations of traditional health kiosks. It integrates an advanced GAMENet model enhanced with Graph Attention Networks (GAT) and Multi-Head Cross-Attention (MHCA) while ensuring user privacy through federated learning. This paper outlines the conceptual design and architecture of HERMES, with a focus on deployment in high-traffic public areas. Methods: HERMES analyzes self-reported symptoms and anonymized medical histories using AI algorithms to generate context-aware OTC medication recommendations. The system was initially trained using Electronic Health Records (EHR) from the MIMIC-III dataset (6,350 patients) and Drug-Drug Interaction (DDI) data from the TWOSIDES database, incorporating the top 90 severity DDI types. Real-time DDI checks and ATC-mapped drug codes further improve safety. The kiosk is designed for accessibility, offering multilingual support, large fonts, voice commands, and Braille compatibility. A built-in health education library promotes preventive care and health literacy. A survey was conducted among 10 medical professionals to evaluate its potential applications in medicine. Results: Preliminary results show that the enhanced GAMENet model achieved a Precision-Recall AUC (PRAUC) of 0.74, outperforming the original model. These findings suggest a strong potential for delivering accurate and secure healthcare recommendations in public settings. Conclusion: HERMES demonstrates how AI-driven, privacy-preserving kiosks can enhance public health access, empower users, and alleviate burdens on healthcare systems. Future work will focus on real-world deployment, usability testing, and scalability for broader adoption.

</details>


### [15] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)

*Takao Fujii, Katie Seaborn, Madeleine Steeds, Jun Kato*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Identity, Voice Gendering, Intersectionality, Cultural Sensitivity

**Relevance Score:** 8

**TL;DR:** The paper explores how voices of conversational agents can evoke gender perceptions and discusses the impact of Japanese self-referents on agent identity perception.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ethics and implications of using human social identity cues in conversational agents, highlighting the role of cultural aspects in shaping user perceptions.

**Method:** A crowdsourcing study with 204 Japanese participants evaluated three ChatGPT voices while using seven different non-pronominal self-referents.

**Key Contributions:**

	1. Demonstrates the impact of voice on gender perception in conversational agents.
	2. Introduces the concept of non-pronominal self-referents in understanding agent identity.
	3. Reveals the intersectionality of age, formality, and gendering in agent perception.

**Result:** The study found significant evidence of voice gendering and highlighted that intersectional self-referents can create ambiguity in gender perception, enriching the understanding of identity neutrality in voice agents.

**Limitations:** Limited to Japanese cultural context and voices; findings may not generalize to other languages or cultures.

**Conclusion:** The findings underline the necessity for culturally sensitive approaches in designing conversational agents, advocating for consideration of intersectionality in agent identity.

**Abstract:** Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other "neutral" non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)

*Jinzhu Yang*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Prompt Learning, Medical Informatics

**Relevance Score:** 9

**TL;DR:** This study presents the Prompt-bioMRC model for improving Named Entity Recognition in the medical domain using prompt learning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Named Entity Recognition (NER) within medical texts by leveraging prompt learning techniques, especially in light of advancements in large-scale models like BioBERT.

**Method:** The study introduces the Prompt-bioMRC model, which employs both hard template and soft prompt designs, and conducts extensive experiments on various medical datasets.

**Key Contributions:**

	1. Introduction of the Prompt-bioMRC model combining hard and soft prompts for NER
	2. Demonstrated superior performance over traditional NER models in medical contexts
	3. Potential applications in intelligent diagnosis systems and healthcare decision support.

**Result:** The findings show that the Prompt-bioMRC model consistently outperforms traditional NER models across multiple medical datasets.

**Limitations:** 

**Conclusion:** The study validates the effectiveness of the Prompt-bioMRC model and emphasizes its potential to support intelligent diagnosis systems and improve healthcare decision-making through better automated medical data processing.

**Abstract:** This study is dedicated to exploring the application of prompt learning methods to advance Named Entity Recognition (NER) within the medical domain. In recent years, the emergence of large-scale models has driven significant progress in NER tasks, particularly with the introduction of the BioBERT language model, which has greatly enhanced NER capabilities in medical texts. Our research introduces the Prompt-bioMRC model, which integrates both hard template and soft prompt designs aimed at refining the precision and efficiency of medical entity recognition. Through extensive experimentation across diverse medical datasets, our findings consistently demonstrate that our approach surpasses traditional models. This enhancement not only validates the efficacy of our methodology but also highlights its potential to provide reliable technological support for applications like intelligent diagnosis systems. By leveraging advanced NER techniques, this study contributes to advancing automated medical data processing, facilitating more accurate medical information extraction, and supporting efficient healthcare decision-making processes.

</details>


### [17] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)

*Lukas Rauch, Moritz Wirth, Denis Huseljic, Marek Herde, Bernhard Sick, Matthias Aßenmacher*

**Main category:** cs.CL

**Keywords:** active learning, large language models, text classification, embeddings, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of large language model (LLM) embeddings on deep active learning strategies for text classification.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the practicality of deep active learning (AL) using frozen LLM embeddings and evaluate their influence on query strategies.

**Method:** Five leading models from the MTEB leaderboard were tested against two baselines across ten text classification tasks to assess the effectiveness of various query strategies in active learning.

**Key Contributions:**

	1. Established a benchmark for LLM embedding quality in active learning.
	2. Demonstrated the importance of synergy between sampling methods and embedding quality.
	3. Provided insights into the sensitivity of query strategies to embedding quality.

**Result:** The study found that diversity-based sampling for initializing labeled pools significantly boosts performance, and the effectiveness of query strategies depends heavily on the quality of embeddings used.

**Limitations:** The study is limited to ten text classification tasks and may not generalize to other domains.

**Conclusion:** A context-specific evaluation of AL strategies is necessary, as performance varies based on embedding quality and the task.

**Abstract:** The advent of large language models (LLMs) capable of producing general-purpose representations lets us revisit the practicality of deep active learning (AL): By leveraging frozen LLM embeddings, we can mitigate the computational costs of iteratively fine-tuning large backbones. This study establishes a benchmark and systematically investigates the influence of LLM embedding quality on query strategies in deep AL. We employ five top-performing models from the massive text embedding benchmark (MTEB) leaderboard and two baselines for ten diverse text classification tasks. Our findings reveal key insights: First, initializing the labeled pool using diversity-based sampling synergizes with high-quality embeddings, boosting performance in early AL iterations. Second, the choice of the optimal query strategy is sensitive to embedding quality. While the computationally inexpensive Margin sampling can achieve performance spikes on specific datasets, we find that strategies like Badge exhibit greater robustness across tasks. Importantly, their effectiveness is often enhanced when paired with higher-quality embeddings. Our results emphasize the need for context-specific evaluation of AL strategies, as performance heavily depends on embedding quality and the target task.

</details>


### [18] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)

*Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma*

**Main category:** cs.CL

**Keywords:** multi-hop reasoning, large language models, benchmark, natural language processing, health informatics

**Relevance Score:** 9

**TL;DR:** NovelHopQA introduces a benchmark for evaluating multi-hop question answering in large language models over extensive text excerpts, revealing significant reasoning challenges even for state-of-the-art models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks in evaluating multi-hop reasoning in long-context environments, especially for large language models.

**Method:** The paper presents NovelHopQA, a benchmark that combines context length and reasoning depth, using 64k-128k-token excerpts from 83 public-domain novels and a keyword-guided pipeline to create hop-separated chains.

**Key Contributions:**

	1. Introduction of NovelHopQA benchmark for multi-hop QA over large-context narratives
	2. Controlled evaluation methodology with human annotation for alignment and hop depth
	3. Analysis of failure modes in current SOTA models regarding long-range reasoning

**Result:** Evaluation of six state-of-the-art models showed consistent accuracy drops with increased hop count and context length, revealing that larger models do not necessarily succeed in complex reasoning tasks.

**Limitations:** The study focuses on specific narrative contexts and may not generalize across all types of texts or QA tasks.

**Conclusion:** NovelHopQA serves as a valuable diagnostic tool to assess and enhance multi-hop reasoning capabilities in large language models, highlighting areas where these models struggle.

**Abstract:** Current large language models (LLMs) struggle to answer questions that span tens of thousands of tokens, especially when multi-hop reasoning is involved. While prior benchmarks explore long-context comprehension or multi-hop reasoning in isolation, none jointly vary context length and reasoning depth in natural narrative settings. We introduce NovelHopQA, the first benchmark to evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length public-domain novels. A keyword-guided pipeline builds hop-separated chains grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models and apply oracle-context filtering to ensure all questions are genuinely answerable. Human annotators validate both alignment and hop depth. We noticed consistent accuracy drops with increased hops and context length, even in frontier models-revealing that sheer scale does not guarantee robust reasoning. Our failure mode analysis highlights common breakdowns, such as missed final-hop integration and long-range drift. NovelHopQA offers a controlled diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [19] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)

*Timothy Do, Pranav Saran, Harshita Poojary, Pranav Prabhu, Sean O'Brien, Vasu Sharma, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Metaphor Classification, Attention Head Pruning, Low-resource Languages, Multilingual Models

**Relevance Score:** 7

**TL;DR:** The paper presents a hybrid NLP model using mBERT and LSTM for metaphor classification in low-resource languages, achieving significant accuracy with a focus on attention head pruning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle challenges posed by figurative language in NLP systems, especially for low-resource languages like Konkani.

**Method:** A hybrid model combining pre-trained Multilingual BERT with a bidirectional LSTM and a linear classifier, fine-tuned on a new annotated metaphor classification dataset. An attention head pruning strategy was implemented for efficiency.

**Key Contributions:**

	1. Introduction of a new annotated dataset for metaphor classification in Konkani
	2. Development of a hybrid model integrating mBERT and LSTM
	3. Implementation of attention head pruning strategy for improved model efficiency

**Result:** The model achieved 78% accuracy in metaphor classification and 83% accuracy in idiom classification after pruning.

**Limitations:** 

**Conclusion:** Attention head pruning proved effective in enhancing the efficiency of NLP tools for underrepresented languages.

**Abstract:** In this paper, we address the persistent challenges that figurative language expressions pose for natural language processing (NLP) systems, particularly in low-resource languages such as Konkani. We present a hybrid model that integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM and a linear classifier. This architecture is fine-tuned on a newly introduced annotated dataset for metaphor classification, developed as part of this work. To improve the model's efficiency, we implement a gradient-based attention head pruning strategy. For metaphor classification, the pruned model achieves an accuracy of 78%. We also applied our pruning approach to expand on an existing idiom classification task, achieving 83% accuracy. These results demonstrate the effectiveness of attention head pruning for building efficient NLP tools in underrepresented languages.

</details>


### [20] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)

*Christopher Lee Lübbers*

**Main category:** cs.CL

**Keywords:** Paraphrasing, Machine Translation, Question-Answering, Human-Centric Models, Preference Optimization

**Relevance Score:** 9

**TL;DR:** This study improves paraphrase-type generation using a human-ranked dataset and Direct Preference Optimization (DPO), enhancing accuracy and human preference ratings for better semantic accuracy in language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing paraphrase-type generation methods struggle to align with human preferences due to reliance on automated metrics and limited training data, impacting semantic fidelity.

**Method:** The study utilizes a human-ranked paraphrase-type dataset and applies Direct Preference Optimization (DPO) to enhance the alignment of model outputs with human judgments.

**Key Contributions:**

	1. Developed a human-ranked paraphrase-type dataset for training
	2. Applied Direct Preference Optimization (DPO) for better alignment with human judgments
	3. Achieved high F1 scores for paraphrase-type detection models.

**Result:** DPO-based training improves paraphrase-type generation accuracy by 3 percentage points and human preference ratings by 7 percentage points, with the detection model achieving high F1 scores for various paraphrase types.

**Limitations:** 

**Conclusion:** The findings indicate that preference data and DPO training yield more reliable and semantically accurate paraphrases, supporting improved text summarization and question-answering, thus advancing the field toward user-aligned language generation.

**Abstract:** Paraphrasing re-expresses meaning to enhance applications like text simplification, machine translation, and question-answering. Specific paraphrase types facilitate accurate semantic analysis and robust language models. However, existing paraphrase-type generation methods often misalign with human preferences due to reliance on automated metrics and limited human-annotated training data, obscuring crucial aspects of semantic fidelity and linguistic transformations.   This study addresses this gap by leveraging a human-ranked paraphrase-type dataset and integrating Direct Preference Optimization (DPO) to align model outputs directly with human judgments. DPO-based training increases paraphrase-type generation accuracy by 3 percentage points over a supervised baseline and raises human preference ratings by 7 percentage points. A newly created human-annotated dataset supports more rigorous future evaluations. Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for addition/deletion, 0.78 for same polarity substitution, and 0.70 for punctuation changes.   These findings demonstrate that preference data and DPO training produce more reliable, semantically accurate paraphrases, enabling downstream applications such as improved summarization and more robust question-answering. The PTD model surpasses automated metrics and provides a more reliable framework for evaluating paraphrase quality, advancing paraphrase-type research toward richer, user-aligned language generation and establishing a stronger foundation for future evaluations grounded in human-centric criteria.

</details>


### [21] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)

*E Fan, Weizong Wang, Tianhan Zhang*

**Main category:** cs.CL

**Keywords:** Computational Fluid Dynamics, ChatCFD, OpenFOAM, large language models, automation

**Relevance Score:** 3

**TL;DR:** ChatCFD is a large language model-driven pipeline that automates CFD workflows, simplifying complex simulations for users with minimal expertise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the operational complexity and expertise required for Computational Fluid Dynamics (CFD) simulations.

**Method:** A structured pipeline integrating large language models with the OpenFOAM framework to automate configuration and execution of CFD workflows from natural language prompts.

**Key Contributions:**

	1. Automation of CFD workflows using large language models
	2. Integration with OpenFOAM for natural language configuration
	3. Validation against published CFD results with complex configurations

**Result:** ChatCFD can autonomously reproduce published CFD results and effectively manage complex configurations that are challenging for traditional language models.

**Limitations:** 

**Conclusion:** ChatCFD bridges the gap between language processing and CFD expertise, enabling broader accessibility and efficiency in simulation tasks.

**Abstract:** Computational Fluid Dynamics (CFD) is essential for scientific and engineering advancements but is limited by operational complexity and the need for extensive expertise. This paper presents ChatCFD, a large language model-driven pipeline that automates CFD workflows within the OpenFOAM framework. It enables users to configure and execute complex simulations from natural language prompts or published literature with minimal expertise. The innovation is its structured approach to database construction, configuration validation, and error reflection, integrating CFD and OpenFOAM knowledge with general language models to improve accuracy and adaptability. Validation shows ChatCFD can autonomously reproduce published CFD results, handling complex, unseen configurations beyond basic examples, a task challenging for general language models.

</details>


### [22] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)

*Feng Wang, Yiding Sun, Jiaxin Mao, Wei Xue, Danqing Xu*

**Main category:** cs.CL

**Keywords:** Large language models, Financial RAG benchmark, Financial applications, NLP, Dynamic data integration

**Relevance Score:** 6

**TL;DR:** The paper presents FinS-Pilot, a benchmark for evaluating retrieval-augmented generation (RAG) systems in financial applications, addressing issues of data confidentiality and dynamic data integration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in standardized benchmarks for evaluating RAG systems in the financial domain due to challenges in data confidentiality and dynamic integration.

**Method:** The authors developed FinS-Pilot using real-world financial assistant interactions, incorporating real-time API data and structured text, organized via intent classification for critical financial tasks.

**Key Contributions:**

	1. Development of FinS-Pilot, a novel benchmark for financial RAG systems
	2. Integration of real-time data and structured sources in evaluations
	3. Accessibility of the dataset and evaluation code on GitHub

**Result:** FinS-Pilot was shown to effectively evaluate the capabilities of leading Chinese LLMs, identifying models that are well-suited for financial applications and addressing the deficit in specialized evaluation tools for this area.

**Limitations:** 

**Conclusion:** The introduction of FinS-Pilot offers both a practical framework for evaluation and a dataset designed to propel research in financial NLP systems.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduces FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains such as equity analysis and macroeconomic forecasting. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information. Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [23] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)

*Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, Jinfeng Bai*

**Main category:** cs.CL

**Keywords:** multimodal, instruction tuning, large language models, catastrophic forgetting, machine learning

**Relevance Score:** 6

**TL;DR:** This paper presents BranchLoRA, a new framework designed to improve the efficiency and performance of Multimodal Large Language Models in continual instruction tuning, addressing the issue of Catastrophic Forgetting in existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency in the Mixture-of-Experts LoRA framework within the context of Multimodal Continual Instruction Tuning and to mitigate the challenges posed by Catastrophic Forgetting.

**Method:** The paper proposes BranchLoRA, which incorporates a flexible tuning-freezing mechanism and task-specific routers to improve task specialization and collaboration among branches.

**Key Contributions:**

	1. Introduction of BranchLoRA framework
	2. Flexible tuning-freezing mechanism to reduce CF
	3. Task-specific routers for optimal branch distribution

**Result:** BranchLoRA significantly outperforms the existing MoELoRA framework across various Multimodal Large Language Model sizes in terms of efficiency and performance while mitigating Catastrophic Forgetting.

**Limitations:** The framework's performance across unfamiliar tasks or domains has not been evaluated.

**Conclusion:** The proposed BranchLoRA framework effectively addresses the critical parameter inefficiency seen in existing continual instruction tuning methods and demonstrates superior performance in maintaining alignment with human intent over sequential tasks.

**Abstract:** Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [24] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)

*Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su*

**Main category:** cs.CL

**Keywords:** large language models, evaluation, KnowSum, unseen knowledge, information retrieval

**Relevance Score:** 9

**TL;DR:** The paper introduces KnowSum, a framework for better evaluating LLMs by quantifying unseen knowledge that current evaluations often overlook.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies in current LLM evaluations which fail to fully capture model capabilities due to the oversight of unseen knowledge.

**Method:** KnowSum statistically extrapolates unobserved knowledge from the frequencies of observed knowledge instances to provide a more thorough evaluation of LLMs.

**Key Contributions:**

	1. Introduction of the KnowSum framework for evaluating unseen knowledge in LLMs.
	2. Demonstrated effectiveness across total knowledge assessment, retrieval effectiveness, and output diversity.
	3. Revised comparative rankings for common LLMs based on internal knowledge assessment.

**Result:** The framework was tested across multiple tasks, revealing that significant amounts of knowledge were neglected when based solely on observed performance and that KnowSum led to different comparative rankings of LLMs.

**Limitations:** The framework's effectiveness may vary based on the specific tasks and knowledge types being evaluated.

**Conclusion:** KnowSum enhances the evaluation of LLMs by revealing the amount of unseen knowledge, impacting how we understand model capabilities and performance ranking.

**Abstract:** Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.

</details>


### [25] [IMPersona: Evaluating Individual Level LM Impersonation](https://arxiv.org/abs/2504.04332)

*Quan Shi, Carlos E. Jimenez, Stephen Dong, Brian Seo, Caden Yao, Adam Kelch, Karthik Narasimhan*

**Main category:** cs.CL

**Keywords:** language models, impersonation, human-like text generation, supervised fine-tuning, ethical implications

**Relevance Score:** 9

**TL;DR:** The paper introduces IMPersona, a framework to evaluate language models' ability to impersonate individual writing styles and knowledge, achieving notable success with modestly sized models.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** With language models becoming increasingly capable of human-like text generation, it is crucial to understand their ability to mimic individual identities and the implications of such capabilities.

**Method:** The study employs supervised fine-tuning and a hierarchical memory-inspired retrieval system to enhance language models like Llama-3.1-8B-Instruct for impersonation tasks.

**Key Contributions:**

	1. Introduction of the IMPersona framework for evaluating language model impersonation
	2. Demonstration of effective impersonation by modestly sized language models
	3. Analysis of ethical implications and defense strategies against impersonation

**Result:** In blind tests, fine-tuned models with memory integration were misidentified as human 44.44% of the time, outperforming the best existing prompting-based strategies at 25.00%.

**Limitations:** Focused on fine-tuned models; may not generalize across all language models or contexts of use.

**Conclusion:** The results highlight potential risks and raise ethical concerns regarding the use of personalized language models, urging the need for detection and defense strategies against impersonation.

**Abstract:** As language models achieve increasingly human-like capabilities in conversational text generation, a critical question emerges: to what extent can these systems simulate the characteristics of specific individuals? To evaluate this, we introduce IMPersona, a framework for evaluating LMs at impersonating specific individuals' writing style and personal knowledge. Using supervised fine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate that even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can achieve impersonation abilities at concerning levels. In blind conversation experiments, participants (mis)identified our fine-tuned models with memory integration as human in 44.44% of interactions, compared to just 25.00% for the best prompting-based approach. We analyze these results to propose detection methods and defense strategies against such impersonation attempts. Our findings raise important questions about both the potential applications and risks of personalized language models, particularly regarding privacy, security, and the ethical deployment of such technologies in real-world contexts.

</details>


### [26] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)

*Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning, medical domain, machine learning, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper investigates the internal reasoning processes of reasoning-enhanced Large Language Models (LLMs) in medical and mathematical domains, proposing a new evaluation framework to assess knowledge and reasoning quality.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexplored quality and transparency of internal reasoning processes in reasoning-enhanced LLMs, especially in the medical and mathematical domains.

**Method:** A fine-grained evaluation framework is introduced that assesses two aspects: the correctness of knowledge (Knowledge Index - KI) and the quality of reasoning (Information Gain - InfoGain). The study examines R1-distilled and base Qwen models with supervised fine-tuning and/or reinforcement learning.

**Key Contributions:**

	1. Introduced a new evaluation framework for assessing knowledge and reasoning in LLMs.
	2. Identified the limitations of model transferability to medical reasoning tasks.
	3. Demonstrated the benefits of reinforcement learning in improving reasoning accuracy.

**Result:** The study finds that (1) general reasoning abilities do not transfer well to the medical domain, (2) supervised fine-tuning increases accuracy but reduces reasoning quality, and (3) reinforcement learning improves reasoning accuracy by eliminating irrelevant knowledge.

**Limitations:** The generalizability of findings to other domains beyond medical and mathematical has not been established.

**Conclusion:** The research highlights the complexity of reasoning in medical tasks and the trade-offs between accuracy and reasoning quality, suggesting the importance of refining training methodologies in LLMs.

**Abstract:** Recent advances in reasoning-enhanced Large Language Models such as OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex tasks. However, the quality and transparency of their internal reasoning processes remain underexplored. This work moves beyond the final-answer accuracy and investigates step-by-step reasoning in the medical and mathematical domains by explicitly decomposing the thinking trajectories into two parts: knowledge and reasoning. Specifically, we introduce a fine-grained evaluation framework that judges: (1) the correctness of knowledge used (measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured by Information Gain (InfoGain)). Using this framework, we study R1-distilled and base Qwen models trained with supervised fine-tuning (SFT) and/or reinforcement learning (RL) in the medical and math domains. Three intriguing findings emerge: (1) The general reasoning abilities in R1-distilled models do not transfer effectively to the medical domain through either SFT or RL. (2) SFT raises final-answer accuracy in both domains, but often at the cost of reasoning quality: InfoGain drops by 38.9% on average compared with untrained models; In the medical domain, however, SFT remains crucial because domain knowledge is indispensable. (3) RL enhances medical reasoning by pruning inaccurate or irrelevant knowledge from reasoning paths, thereby improving both reasoning accuracy and knowledge correctness.

</details>


### [27] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)

*Michael Li, Nishant Subramani*

**Main category:** cs.CL

**Keywords:** transformer models, lexical identity, inflectional morphology, language representation, BERT

**Relevance Score:** 8

**TL;DR:** This paper analyzes how various transformer-based language models encode lexical identity and inflectional morphology, finding consistent patterns in information representation across multiple models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how contemporary language models represent linguistic information beyond early models like BERT and GPT-2.

**Method:** The study trains classifiers on layer-wise activations of models to predict word lemmas and inflectional features, examining both classical and contemporary architectures.

**Key Contributions:**

	1. Analysis of lexical and inflectional information across various language models
	2. Identification of linear and nonlinear encoding patterns in different layers
	3. Demonstration of consistency in linguistic information representation across 16 models tested

**Result:** It was found that models linearly represent lexical information in early layers and nonlinearly in later layers while encoding inflectional features consistently across all layers.

**Limitations:** 

**Conclusion:** Despite advances in architecture and training regimes, the way transformer models represent linguistic information appears fundamentally similar.

**Abstract:** Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today's language models, we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) represent lexical identity and inflectional morphology. We train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features. We discover that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout the layers. Further analysis reveals that these models encode inflectional morphology through generalizable abstractions, but rely predominantly on memorization to encode lexical identity. Remarkably, these patterns emerge across all 16 models we test, despite differences in architecture, size, and training regime (including pretrained and instruction-tuned variants). This consistency suggests that, despite substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties could be fundamental for next token prediction and are learned early during pretraining. Our code is available at https://github.com/ml5885/model_internal_sleuthing.

</details>


### [28] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)

*Joshua Rozner, Leonie Weissweiler, Cory Shain*

**Main category:** cs.CL

**Keywords:** Construction Grammar, Pretrained Language Models, BabyLM Challenge, Language Learning, Neural Models

**Relevance Score:** 6

**TL;DR:** This paper evaluates construction learning in language models trained on developmentally plausible data, showing that they can represent diverse constructions and perform better on benchmarks when they do.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well language models trained on realistic data can represent linguistic constructions similar to human language acquisition.

**Method:** The study replicates methods from Rozner et al. (2025) to assess constructional learning in models from the 2024 BabyLM challenge.

**Key Contributions:**

	1. Demonstration that language models can learn constructions with less training data than typically used.
	2. Correlation between constructional performance and success on BabyLM benchmarks.
	3. Insight into the implications of construction representation for language learning in humans.

**Result:** Models trained on developmentally plausible data demonstrate an ability to represent diverse constructions and correlate performance on benchmarks with constructional representation.

**Limitations:** The study is limited to specific models and may not generalize across all types of language models or constructions.

**Conclusion:** Even with limited data, models can learn to represent complex linguistic constructions, suggesting relevance to human language learning processes.

**Abstract:** Construction grammar posits that children acquire constructions (form-meaning pairings) from the statistics of their environment. Recent work supports this hypothesis by showing sensitivity to constructions in pretrained language models (PLMs), including one recent study (Rozner et al., 2025) demonstrating that constructions shape the PLM's output distribution. However, models under study have generally been trained on developmentally implausible amounts of data, casting doubt on their relevance to human language learning. Here we use Rozner et al.'s methods to evaluate constructional learning in models from the 2024 BabyLM challenge. Our results show that even when trained on developmentally plausible quantities of data, models represent diverse constructions, even hard cases that are superficially indistinguishable. We further find correlational evidence that constructional performance may be functionally relevant: models that better represent constructions perform better on the BabyLM benchmarks.

</details>


### [29] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)

*Bo Peng, Zhiheng Wang, Heyang Gong, Chaochao Lu*

**Main category:** cs.CL

**Keywords:** dialogue systems, synthetic data generation, personalized assistance, user background, evaluation framework

**Relevance Score:** 8

**TL;DR:** A novel method for automatic synthetic data generation in dialogue systems, introducing the IP-Dialog benchmark and a systematic evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve personalized assistance in dialogue systems by inferring user backgrounds while addressing challenges in data scarcity and traditional dataset creation methods.

**Method:** A synthetic data generation approach is proposed along with the creation of the Implicit Personalized Dialogue benchmark, which includes a training dataset for 10 tasks and 12 user attribute types, supplemented by a systematic evaluation framework with four assessment metrics.

**Key Contributions:**

	1. Introduction of the Implicit Personalized Dialogue benchmark
	2. Automatic synthetic data generation for dialogue systems
	3. Development of a systematic evaluation framework for attribute awareness and reasoning

**Result:** Experiments validate the reliability of the synthetic dataset and provide insights into models' performance in user attribute awareness and reasoning.

**Limitations:** 

**Conclusion:** The proposed method improves the capabilities of dialogue systems to provide personalized assistance while overcoming traditional data limitations.

**Abstract:** In modern dialogue systems, the ability to implicitly infer user backgrounds from conversations and leverage this information for personalized assistance is crucial. However, the scarcity of high-quality data remains a fundamental challenge to evaluating and improving this capability. Traditional dataset construction methods are labor-intensive, resource-demanding, and raise privacy concerns. To address these issues, we propose a novel approach for automatic synthetic data generation and introduce the Implicit Personalized Dialogue (IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12 user attribute types. Additionally, we develop a systematic evaluation framework with four metrics to assess both attribute awareness and reasoning capabilities. We further propose five causal graphs to elucidate models' reasoning pathways during implicit personalization. Extensive experiments yield insightful observations and prove the reliability of our dataset.

</details>


### [30] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)

*Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur*

**Main category:** cs.CL

**Keywords:** Neural Transducer, Speech Recognition, Speech Translation

**Relevance Score:** 4

**TL;DR:** HENT-SRT is a novel framework for speech recognition and translation using Neural Transducers, addressing challenges in word reordering and computational efficiency.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Current Neural Transducer approaches struggle with word reordering and performance when modeling ASR and ST together, leading to performance gaps with attention-based models.

**Method:** HENT-SRT separates ASR and translation tasks, utilizes self-distillation with CTC consistency, and incorporates efficient ASR practices for reduced training complexity.

**Key Contributions:**

	1. Introduction of HENT-SRT framework for improved speech translation.
	2. Use of self-distillation with CTC regularization for robust ST.
	3. Incorporation of efficient methods from ASR to reduce training complexity.

**Result:** HENT-SRT achieved state-of-the-art performance on Arabic, Spanish, and Mandarin conversational datasets, significantly narrowing the performance gap with AED systems.

**Limitations:** 

**Conclusion:** The proposed framework shows substantial improvements in translation quality and efficiency while maintaining ASR performance.

**Abstract:** Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems.

</details>


### [31] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)

*Maike Behrendt, Stefan Sylvius Wagner, Carina Weinmann, Marike Bormann, Mira Warne, Stefan Harmeling*

**Main category:** cs.CL

**Keywords:** political participation, machine learning, deliberation, online discussions, platform design

**Relevance Score:** 3

**TL;DR:** This paper examines the challenges in political online discussions and proposes machine learning solutions to improve deliberative quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing importance of political online participation necessitates high-quality discussions characterized by civil argument exchange, dependent on platform design.

**Method:** The paper explores the issues arising in political online discussions and discusses the application of machine learning methods to enhance deliberation.

**Key Contributions:**

	1. Identification of key issues in political online discussions.
	2. Application of machine learning methods to enhance deliberation.
	3. Recommendations for designing platforms that support civil exchanges.

**Result:** Machine learning has the potential to address specific challenges in political online discussions, improving the overall quality of deliberation.

**Limitations:** 

**Conclusion:** By leveraging machine learning, platforms can facilitate better online communication among citizens, enhancing deliberative processes.

**Abstract:** Political online participation in the form of discussing political issues and exchanging opinions among citizens is gaining importance with more and more formats being held digitally. To come to a decision, a careful discussion and consideration of opinions and a civil exchange of arguments, which is defined as the act of deliberation, is desirable. The quality of discussions and participation processes in terms of their deliberativeness highly depends on the design of platforms and processes. To facilitate online communication for both participants and initiators, machine learning methods offer a lot of potential. In this work we want to showcase which issues occur in political online discussions and how machine learning can be used to counteract these issues and enhance deliberation.

</details>


### [32] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)

*Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli, Andre Martins, Giuseppe Attanasio*

**Main category:** cs.CL

**Keywords:** speech translation, gender encoding, interpretability, translation bias, machine learning

**Relevance Score:** 7

**TL;DR:** This paper examines gender encoding in speech translation models and its implications for speaker gender assignment in translations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how speech translation models capture and encode gender characteristics, and the bias introduced in translations based on model architecture.

**Method:** Using probing methods to assess the gender encoding capabilities of various speech translation models across three language directions: English-French, English-Italian, and English-Spanish.

**Key Contributions:**

	1. Assessment of gender encoding in speech translation models
	2. Comparison of traditional and newer speech translation architectures
	3. Identification of translation bias towards masculine default in newer models.

**Result:** Traditional encoder-decoder models effectively capture gender information, whereas newer hybrid architectures tend to lack this capability, leading to a translation bias towards masculine defaults.

**Limitations:** 

**Conclusion:** The study highlights the need for awareness of gender biases in speech translation systems and suggests that improvements are necessary in newer architectures to avoid such biases.

**Abstract:** Recent studies on interpreting the hidden states of speech models have shown their ability to capture speaker-specific features, including gender. Does this finding also hold for speech translation (ST) models? If so, what are the implications for the speaker's gender assignment in translation? We address these questions from an interpretability perspective, using probing methods to assess gender encoding across diverse ST models. Results on three language directions (English-French/Italian/Spanish) indicate that while traditional encoder-decoder models capture gender information, newer architectures -- integrating a speech encoder with a machine translation system via adapters -- do not. We also demonstrate that low gender encoding capabilities result in systems' tendency toward a masculine default, a translation bias that is more pronounced in newer architectures.

</details>


### [33] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)

*Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel*

**Main category:** cs.CL

**Keywords:** AI debate, fact-checking, COVID-19 misinformation

**Relevance Score:** 9

**TL;DR:** This paper explores the use of AI debate to improve fact-checking and judgment accuracy on controversial topics like COVID-19 claims, highlighting the effectiveness of AI systems in countering biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As AI influences our understanding of the world, it risks amplifying misinformation, particularly in public health, necessitating methods to ensure AI truthfulness and effective human oversight.

**Method:** The study consists of two parts: one with human judges evaluating COVID-19 claims through AI-assisted debate vs. consultancy, and another using personalized AI judges designed to mimic different human biases.

**Key Contributions:**

	1. Demonstrates that AI debate can enhance human judgment in assessing factual claims.
	2. Shows that personalized AI judges can outperform human judges in judgment accuracy.
	3. Highlights the potential of leveraging AI systems for truth-seeking in biased domains.

**Result:** The human study shows AI debate improves judgment accuracy by 10%, especially aiding judges with mainstream beliefs (+15.2% accuracy) and slightly assisting skeptical judges (+4.7% accuracy). The AI judge study indicates AI judges with personas achieve higher accuracy (78.5%) than human judges (70.1%).

**Limitations:** 

**Conclusion:** AI debate frameworks can serve as effective tools for scalable oversight, enhancing judgment accuracy and potentially offering bias-resilient supervision of AI systems.

**Abstract:** As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics like public health where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI truthfulness by enabling humans to supervise systems that may exceed human capabilities--yet humans themselves hold different beliefs and biases that impair their judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial COVID-19 factuality claims where people hold strong prior beliefs. We conduct two studies: one with human judges holding either mainstream or skeptical beliefs evaluating factuality claims through AI-assisted debate or consultancy protocols, and a second examining the same problem with personalized AI judges designed to mimic these different human belief systems. In our human study, we find that debate-where two AI advisor systems present opposing evidence-based arguments-consistently improves judgment accuracy and confidence calibration, outperforming consultancy with a single-advisor system by 10% overall. The improvement is most significant for judges with mainstream beliefs (+15.2% accuracy), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In our AI judge study, we find that AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without personas (69.8%), suggesting their potential for supervising frontier AI models. These findings highlight AI debate as a promising path toward scalable, bias-resilient oversight--leveraging both diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [34] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)

*Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli*

**Main category:** cs.CL

**Keywords:** ASR, feature attribution, Conformer-based

**Relevance Score:** 4

**TL;DR:** This paper uses feature attribution to identify acoustic cues in a Conformer-based ASR system, revealing insights into the model's reliance on phonemic characteristics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the specific acoustic cues that modern ASR models depend on, addressing limitations found in prior studies with outdated models and limited phoneme analysis.

**Method:** Feature attribution techniques were applied to analyze acoustic properties of plosives, fricatives, and vowels in a modern Conformer-based ASR system.

**Key Contributions:**

	1. Identification of specific acoustic cues used by ASR models
	2. Enhanced interpretability of modern ASR systems
	3. New insights into phoneme processing, particularly sibilants and plosives

**Result:** The analysis revealed that the ASR model relies heavily on the full time spans of vowels, particularly their first two formants, and shows greater sensitivity to male speech. It also distinguishes better spectral characteristics of sibilant fricatives relatively to non-sibilants and emphasizes the release phase in plosives.

**Limitations:** 

**Conclusion:** The insights gained from this study improve the interpretability of ASR systems and point to important areas for further research on model robustness.

**Abstract:** Despite significant advances in ASR, the specific acoustic cues models rely on remain unclear. Prior studies have examined such cues on a limited set of phonemes and outdated models. In this work, we apply a feature attribution technique to identify the relevant acoustic cues for a modern Conformer-based ASR system. By analyzing plosives, fricatives, and vowels, we assess how feature attributions align with their acoustic properties in the time and frequency domains, also essential for human speech perception. Our findings show that the ASR model relies on vowels' full time spans, particularly their first two formants, with greater saliency in male speech. It also better captures the spectral characteristics of sibilant fricatives than non-sibilants and prioritizes the release phase in plosives, especially burst characteristics. These insights enhance the interpretability of ASR models and highlight areas for future research to uncover potential gaps in model robustness.

</details>


### [35] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)

*Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu*

**Main category:** cs.CL

**Keywords:** cell type annotation, single-cell RNA sequencing, large language models, batch-level context, CellPuzzles

**Relevance Score:** 7

**TL;DR:** The paper introduces the CellPuzzles task for assigning unique cell types in single-cell RNA sequencing data, and presents Cell-o1, a fine-tuned LLM that outperforms existing models in this context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a methodology for unique cell type annotation considering batch-level cellular context, addressing limitations of existing models that annotate cells independently.

**Method:** Introduces the CellPuzzles benchmark for unique cell type assignment, uses a 7B LLM (Cell-o1) trained with supervised fine-tuning on reasoning traces and reinforcement learning with batch-level rewards.

**Key Contributions:**

	1. Introduction of the CellPuzzles benchmark for batch-level cell type annotation
	2. Development of Cell-o1, which significantly improves annotation accuracy
	3. Insights into reasoning behaviors relevant to batch-level cellular context

**Result:** Cell-o1 achieves state-of-the-art performance, surpassing OpenAI's o1 model by over 73% and generalizing well across various tissues and conditions.

**Limitations:** 

**Conclusion:** The findings highlight the potential of Cell-o1 in improving batch-level cell type annotation, with insights into training dynamics and reasoning behaviors contributing to understanding performance.

**Abstract:** Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.

</details>


### [36] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)

*Lindia Tjuatja, Graham Neubig*

**Main category:** cs.CL

**Keywords:** language models, automated evaluation, contextual embeddings, performance comparison, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces BehaviorBox, a methodology for the automated comparison of language models that identifies fine-grained text features where one model outperforms another.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the performance differences between language models is challenging due to the limitations of existing evaluation methods.

**Method:** BehaviorBox uses performance-aware contextual embeddings to extract coherent features of text that demonstrate performance differences between two language models.

**Key Contributions:**

	1. Introduction of the BehaviorBox methodology for automated LM comparison
	2. Identification of fine-grained textual features where model performance differs
	3. Insights into model performance that traditional metrics fail to capture

**Result:** The method successfully identifies specific contexts where one model outperforms another, providing insights that are not captured by traditional measures like corpus-level perplexity.

**Limitations:** 

**Conclusion:** BehaviorBox enhances the evaluation of language models by revealing meaningful performance differences in fine-grained contexts.

**Abstract:** Language model evaluation is a daunting task: prompts are brittle, corpus-level perplexities are vague, and the choice of benchmarks are endless. Finding examples that show meaningful, generalizable differences between two LMs is crucial to understanding where one model succeeds and another fails. Can this process be done automatically? In this work, we propose methodology for automated comparison of language models that uses performance-aware contextual embeddings to find fine-grained features of text where one LM outperforms another. Our method, which we name BehaviorBox, extracts coherent features that demonstrate differences with respect to the ease of generation between two LMs. Specifically, BehaviorBox finds features that describe groups of words in fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'" and "exclamation marks after emotional statements", where one model outperforms another within a particular datatset. We apply BehaviorBox to compare models that vary in size, model family, and post-training, and enumerate insights into specific contexts that illustrate meaningful differences in performance which cannot be found by measures such as corpus-level perplexity alone.

</details>


### [37] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)

*Richard Armitage*

**Main category:** cs.CL

**Keywords:** Large language models, Primary care, Medical education, Clinical performance, LLM applications

**Relevance Score:** 9

**TL;DR:** This paper evaluates the performance of leading large language models in answering primary care examination questions, showing they perform significantly better than GPs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of advanced LLMs in the medical field, particularly in the context of primary care education and examination.

**Method:** Four LLMs (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) were tested on 100 multiple choice questions from the Royal College of General Practitioners GP SelfTest, with responses scored against correct answers.

**Key Contributions:**

	1. Demonstrated the capabilities of leading LLMs in answering primary care exam questions.
	2. Showed significant performance improvement over average GP scores.
	3. Strengthened the argument for integrating reasoning LLMs in clinical practice.

**Result:** The models achieved scores of 99.0%, 95.0%, 95.0%, and 95.0% respectively, outperforming the average peer score of 73.0%.

**Limitations:** 

**Conclusion:** The results indicate that LLMs can effectively support primary care, particularly those trained on relevant clinical data, with implications for their integration in medical training and practice.

**Abstract:** Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.   Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.   Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.   Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.

</details>


### [38] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)

*Ella Rannon, David Burstein*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, biological sequences, bioinformatics, genomics, NLP applications

**Relevance Score:** 4

**TL;DR:** This review discusses the application of NLP techniques to biological sequence data, including genomics and proteomics, and evaluates various models and their effectiveness in this context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how NLP methods, traditionally used for human language, can be beneficial in analyzing biological sequences and large-scale genomic data.

**Method:** The review examines different NLP methods such as word2vec and transformer models, along with tokenization strategies and model architectures suited for biological tasks.

**Key Contributions:**

	1. Assessment of various NLP models for biological applications
	2. Evaluation of tokenization strategies relevant to genomics
	3. Discussion of recent advancements in NLP for bioinformatics

**Result:** The paper highlights the integration of NLP methods into bioinformatics and their advancements in applications like gene expression and structure prediction.

**Limitations:** 

**Conclusion:** The integration of advanced NLP techniques into bioinformatics shows great potential to advance our understanding of biological processes.

**Abstract:** Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.

</details>


### [39] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)

*Sofoklis Kakouros*

**Main category:** cs.CL

**Keywords:** emotion recognition, speech processing, word informativeness, acoustic features, language model

**Relevance Score:** 8

**TL;DR:** This paper presents a method for enhancing emotion recognition from speech by identifying semantically important segments using word informativeness from a pre-trained language model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenge of effectively identifying speech segments that carry relevant acoustic variations for emotion recognition.

**Method:** The approach combines standard acoustic prosodic features with word informativeness derived from a pre-trained language model to selectively compute features on important speech segments.

**Key Contributions:**

	1. Introduction of word informativeness for segment selection in emotion recognition
	2. Demonstration of improved accuracy in emotion recognition tasks
	3. Utilization of self-supervised representations in analysis

**Result:** The method resulted in a significant improvement in emotion recognition accuracy by focusing on semantically informative segments rather than entire sentences.

**Limitations:** 

**Conclusion:** The use of word informativeness for segment selection enhances emotion recognition performance, demonstrating a novel approach in speech processing.

**Abstract:** In emotion recognition from speech, a key challenge lies in identifying speech signal segments that carry the most relevant acoustic variations for discerning specific emotions. Traditional approaches compute functionals for features such as energy and F0 over entire sentences or longer speech portions, potentially missing essential fine-grained variation in the long-form statistics. This research investigates the use of word informativeness, derived from a pre-trained language model, to identify semantically important segments. Acoustic features are then computed exclusively for these identified segments, enhancing emotion recognition accuracy. The methodology utilizes standard acoustic prosodic features, their functionals, and self-supervised representations. Results indicate a notable improvement in recognition performance when features are computed on segments selected based on word informativeness, underscoring the effectiveness of this approach.

</details>


### [40] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)

*Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu*

**Main category:** cs.CL

**Keywords:** dialogue systems, human-computer interaction, LLM, task-oriented dialogue, expert feedback

**Relevance Score:** 8

**TL;DR:** CoDial is a framework enabling non-technical experts to define and refine dialogue systems using structured graphs and supports zero-shot task specification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate the creation of dialogue systems for specialized domains like law and medicine without requiring extensive technical expertise or large amounts of training data.

**Method:** CoDial converts expert knowledge into executable conversation logic using a structured heterogeneous graph and integrates with guardrailing languages for task-oriented dialogue.

**Key Contributions:**

	1. Introduction of CoDial framework for dialogue systems
	2. Utilization of a structured heterogeneous graph for task specification
	3. Demonstration of iterative improvement through manual and LLM-aided feedback

**Result:** CoDial achieves state-of-the-art performance on the STAR dataset and competes well on the MultiWOZ dataset while allowing for iterative improvements through expert and LLM feedback.

**Limitations:** 

**Conclusion:** CoDial is a practical tool that supports the expert-guided alignment of LLMs in critical domains, making dialogue systems more accessible to non-experts.

**Abstract:** It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDial's iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains.

</details>


### [41] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)

*Wenzheng Zhang, Xi Victoria Lin, Karl Stratos, Wen-tau Yih, Mingda Chen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, query-free system, language models

**Relevance Score:** 9

**TL;DR:** ImpRAG is a unified RAG system that integrates retrieval and generation without explicit queries, improving model generalization across diverse tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations in traditional RAG systems that treat retrieval and generation separately, which can hinder model generalization.

**Method:** ImpRAG integrates retrieval and generation into a single model, allowing implicit expression of information needs. It utilizes a two-stage inference process with shared model parameters to optimize both tasks simultaneously.

**Key Contributions:**

	1. Development of ImpRAG, a query-free RAG system
	2. Unified model for simultaneous retrieval and generation
	3. Demonstrated improvements in task generalization through empirical experiments

**Result:** ImpRAG achieved 3.6-11.5 improvements in exact match scores on diverse knowledge-intensive tasks, demonstrating improved generalization capabilities.

**Limitations:** 

**Conclusion:** The approach highlights the necessity of balancing retrieval and generation parameters and using generation perplexities as training objectives to enhance model performance.

**Abstract:** Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval and generation as separate processes, requiring explicit textual queries to connect them. This separation can limit the ability of models to generalize across diverse tasks. In this work, we propose a query-free RAG system, named ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG allows models to implicitly express their information needs, eliminating the need for human-specified queries. By dividing pretrained decoder-only language models into specialized layer groups, ImpRAG optimizes retrieval and generation tasks simultaneously. Our approach employs a two-stage inference process, using the same model parameters and forward pass for both retrieval and generation, thereby minimizing the disparity between retrievers and language models. Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves 3.6-11.5 improvements in exact match scores on unseen tasks with diverse formats, highlighting its effectiveness in enabling models to articulate their own information needs and generalize across tasks. Our analysis underscores the importance of balancing retrieval and generation parameters and leveraging generation perplexities as retrieval training objectives for enhanced performance.

</details>


### [42] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)

*Sofoklis Kakouros, Haoyu Chen*

**Main category:** cs.CL

**Keywords:** prosody, self-supervised learning, speech classification, tennis, emotional states

**Relevance Score:** 4

**TL;DR:** This study analyzes prosodic features in post-match tennis interviews to classify match outcomes using self-supervised learning models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore how prosodic characteristics can indicate the emotional states of athletes based on match outcomes.

**Method:** The study analyzes pitch and intensity in interview recordings, applying self-supervised learning models like Wav2Vec 2.0 and HuBERT to classify winning and losing outcomes.

**Key Contributions:**

	1. Introduces the classification of tennis match outcomes based on prosodic features of post-match interviews
	2. Demonstrates the efficacy of self-supervised learning models in emotional recognition through speech
	3. Highlights the importance of pitch variability in determining winners and losers.

**Result:** The results show that SSL representations successfully differentiate between winning and losing outcomes by capturing subtle speech patterns, with prosodic cues such as pitch variability being strong indicators.

**Limitations:** 

**Conclusion:** The research concludes that prosodic features are significant for indicating match outcomes and can be classified using advanced SSL techniques.

**Abstract:** This study examines the prosodic characteristics associated with winning and losing in post-match tennis interviews. Additionally, this research explores the potential to classify match outcomes solely based on post-match interview recordings using prosodic features and self-supervised learning (SSL) representations. By analyzing prosodic elements such as pitch and intensity, alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine whether an athlete has won or lost their match. Traditional acoustic features and deep speech representations are extracted from the data, and machine learning classifiers are employed to distinguish between winning and losing players. Results indicate that SSL representations effectively differentiate between winning and losing outcomes, capturing subtle speech patterns linked to emotional states. At the same time, prosodic cues -- such as pitch variability -- remain strong indicators of victory.

</details>


### [43] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)

*Thai Hoang, Kung-Hsiang Huang, Shirley Kokane, Jianguo Zhang, Zuxin Liu, Ming Zhu, Jake Grigsby, Tian Lan, Michael S Ryoo, Chien-Sheng Wu, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles*

**Main category:** cs.CL

**Keywords:** Large Action Models, AI Agents, Training Data Generation, Feedback Mechanism, Interactive Environment

**Relevance Score:** 7

**TL;DR:** LAM SIMULATOR is a framework for generating high-quality training data for Large Action Models (LAMs) and AI agents through online exploration of tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating high-quality training data for LAMs, especially for complex multi-step tasks requiring effective planning and execution.

**Method:** The framework includes a dynamic task query generator, a collection of tools, and an interactive environment for LLM Agents to autonomously explore tasks and receive feedback.

**Key Contributions:**

	1. Introduces LAM SIMULATOR for autonomous task-solving using LLM Agents.
	2. Demonstrates significant performance improvements in LAMs with self-generated datasets.
	3. Offers a minimal human input solution for AI agent training data generation.

**Result:** Models trained with datasets generated by LAM SIMULATOR showed up to a 49.3% improvement over original performance baselines on benchmarks ToolBench and CRMArena.

**Limitations:** 

**Conclusion:** LAM SIMULATOR is efficient and effective for speeding up the development of AI agents, requiring minimal human input for dataset creation.

**Abstract:** Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.

</details>


### [44] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)

*Russell Scheinberg, Ameeta Agrawal, Amber Shore, So Young Lee*

**Main category:** cs.CL

**Keywords:** Grammar prompting, Large language models, Syntactic phenomena

**Relevance Score:** 8

**TL;DR:** This paper introduces 'grammar prompting', a method to enhance large language models' (LLMs) ability to judge sentence acceptability by first generating explanations of syntactic rules before applying them to sentence comparison tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs can explain grammatical rules but struggle to apply them accurately when assessing sentence acceptability. This research aims to address this gap and improve grammar judgment performance.

**Method:** The study employs a method called 'grammar prompting', where an LLM explains the relevant syntactic phenomenon, and this explanation is then used as context for a target model (either another LLM or a smaller language model, SLM) to make decisions about sentence acceptability.

**Key Contributions:**

	1. Introduction of grammar prompting as a novel technique in LLMs
	2. Demonstrated performance gains across multiple language benchmarks
	3. The lightweight method allows SLMs to achieve near state-of-the-art results

**Result:** Grammar prompting significantly enhances performance on syntactic judgment tasks across multiple language benchmarks, reducing the LLM-SLM accuracy gap by approximately 20%, and when combined with chain-of-thought reasoning, by 56%.

**Limitations:** 

**Conclusion:** The findings demonstrate that providing metalinguistic explanations to SLMs allows them to perform comparably to larger LLMs in multilingual environments with minimal computational cost.

**Abstract:** Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present "grammar prompting", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.

</details>


### [45] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)

*Pegah Alipoormolabashi, Ajay Patel, Niranjan Balasubramanian*

**Main category:** cs.CL

**Keywords:** authorship attribution, fairness, misattribution, machine learning, forensic analysis

**Relevance Score:** 5

**TL;DR:** The paper introduces the Misattribution Unfairness Index (MAUIk) to measure unfairness in authorship attribution systems, highlighting unequal risks of misattribution among authors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fairness of authorship attribution systems and the risks of misattribution for different authors in forensic contexts.

**Method:** The authors introduce the Misattribution Unfairness Index (MAUIk) to quantify unfairness by measuring how often authors are incorrectly attributed as the top writers for texts they did not write, across five models and two datasets.

**Key Contributions:**

	1. Introduction of the Misattribution Unfairness Index (MAUIk) for evaluating authorship attribution models.
	2. Demonstration of unfairness in existing models across various datasets.
	3. Identification of the relationship between author embedding positions and misattribution risk.

**Result:** All tested authorship attribution models displayed high levels of unfairness, with risks disproportionately affecting some authors, particularly those closer to the centroid of the author embedding space.

**Limitations:** The study is limited to five models and two datasets, which may not represent all cases of authorship attribution.

**Conclusion:** The study underscores the need to communicate misattribution risks and to calibrate models for ethical usage, especially considering potential harm to authors at higher risk.

**Abstract:** Authorship misattribution can have profound consequences in real life. In forensic settings simply being considered as one of the potential authors of an evidential piece of text or communication can result in undesirable scrutiny. This raises a fairness question: Is every author in the candidate pool at equal risk of misattribution? Standard evaluation measures for authorship attribution systems do not explicitly account for this notion of fairness. We introduce a simple measure, Misattribution Unfairness Index (MAUIk), which is based on how often authors are ranked in the top k for texts they did not write. Using this measure we quantify the unfairness of five models on two different datasets. All models exhibit high levels of unfairness with increased risks for some authors. Furthermore, we find that this unfairness relates to how the models embed the authors as vectors in the latent search space. In particular, we observe that the risk of misattribution is higher for authors closer to the centroid (or center) of the embedded authors in the haystack. These results indicate the potential for harm and the need for communicating with and calibrating end users on misattribution risk when building and providing such models for downstream use.

</details>


### [46] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)

*Berk Atil, Namrata Sureddy, Rebecca J. Passonneau*

**Main category:** cs.CL

**Keywords:** toxicity detection, language models, social reasoning, dataset, benchmarking

**Relevance Score:** 9

**TL;DR:** This paper presents TRuST, a dataset for toxicity detection, benchmarking LLMs on various toxicity-related tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical issue of toxicity in online content, especially that generated by language models.

**Method:** A comprehensive dataset merging existing ones, labeled for toxicity, target social groups, and toxic spans; benchmarking was done with state-of-the-art LLMs on toxicity detection tasks.

**Key Contributions:**

	1. Introduction of the TRuST dataset for toxicity detection.
	2. Benchmarking of LLMs on toxicity tasks.
	3. Insights into the social reasoning capabilities of LLMs.

**Result:** Fine-tuned models outperform zero-shot and few-shot prompting, but still show low performance for certain social groups, revealing weaknesses in social reasoning skills of LLMs.

**Limitations:** Performance remains low for specific social groups and reasoning capabilities do not significantly boost outcomes.

**Conclusion:** Improved datasets are needed to enhance performance in toxicity detection across diverse social groups, and LLMs require better social reasoning abilities.

**Abstract:** Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human/machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills.

</details>


### [47] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)

*Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee*

**Main category:** cs.CL

**Keywords:** Long Chain-of-Thought, Large Reasoning Models, Dataset, Reinforcement Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces the Long CoT Collection, a dataset of 100K chain-of-thought rationales designed to support independent development of large reasoning models (LRMs) without relying on existing models like R1.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on existing large reasoning models (LRMs) such as R1 limits the advancement of the field; thus, developing independent LRM capabilities is essential.

**Method:** The authors present a dataset consisting of long chain-of-thought (CoT) rationales annotated by existing short CoT LLMs and develop a pipeline to induce novel reasoning strategies to enhance long reasoning abilities.

**Key Contributions:**

	1. Introduction of the Long CoT Collection dataset
	2. Induction of novel reasoning strategies into short CoT LLMs
	3. Demonstration of significant gains in RL models initialized on the dataset.

**Result:** The Long CoT Collection yields CoT rationales of quality comparable to R1 and demonstrates that training on this dataset significantly boosts general reasoning skills, allowing substantial gains in reinforcement learning.

**Limitations:** The dataset's quality is slightly below R1, indicating room for improvement.

**Conclusion:** Training on the Long CoT Collection not only strengthens reasoning skills but also lays a strong foundation for improved reinforcement learning performance.

**Abstract:** With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [48] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)

*Jiaming Li, Yukun Chen, Ziqiang Liu, Minghuan Tan, Lei Zhang, Yunshui Li, Run Luo, Longze Chen, Jing Luo, Ahmadreza Argha, Hamid Alinejad-Rokny, Wei Zhou, Min Yang*

**Main category:** cs.CL

**Keywords:** automatic story generation, narrative coherence, machine learning, human preference evaluation, storytelling

**Relevance Score:** 5

**TL;DR:** Storyteller is a novel approach for automatic story generation that enhances narrative coherence and consistency by utilizing a plot node structure based on SVO triplets and integrating dynamic modules for improved interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for substantial improvements in narrative coherence and logical consistency in automatic story generation, as existing methods fail to provide a satisfactory storytelling experience.

**Method:** The introduction of Storyteller, which employs a plot node structure grounded in SVO triplets and integrates STORYLINE and NEKG modules that interact dynamically during the story generation process.

**Key Contributions:**

	1. Introduction of a SVO triplet-based plot node structure
	2. Dynamic interaction of STORYLINE and NEKG modules
	3. Demonstrated superior performance in creativity and coherence compared to existing methods.

**Result:** Storyteller significantly outperforms existing methods, achieving an 84.33% average win rate in human preference evaluations, and excels in creativity, coherence, engagement, and relevance metrics.

**Limitations:** 

**Conclusion:** The proposed Storyteller approach offers a substantial advancement in creating coherent and immersive narratives through innovative structural and dynamic integrations.

**Abstract:** Stories are central to human culture, serving to share ideas, preserve traditions, and foster connections. Automatic story generation, a key advancement in artificial intelligence (AI), offers new possibilities for creating personalized content, exploring creative ideas, and enhancing interactive experiences. However, existing methods struggle to maintain narrative coherence and logical consistency. This disconnect compromises the overall storytelling experience, underscoring the need for substantial improvements. Inspired by human cognitive processes, we introduce Storyteller, a novel approach that systemically improves the coherence and consistency of automatically generated stories. Storyteller introduces a plot node structure based on linguistically grounded subject verb object (SVO) triplets, which capture essential story events and ensure a consistent logical flow. Unlike previous methods, Storyteller integrates two dynamic modules, the STORYLINE and narrative entity knowledge graph (NEKG),that continuously interact with the story generation process. This integration produces structurally sound, cohesive and immersive narratives. Extensive experiments demonstrate that Storyteller significantly outperforms existing approaches, achieving an 84.33% average win rate through human preference evaluation. At the same time, it is also far ahead in other aspects including creativity, coherence, engagement, and relevance.

</details>


### [49] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)

*Herun Wan, Jiaying Wu, Minnan Luo, Zhi Zeng, Zhixiong Su*

**Main category:** cs.CL

**Keywords:** misinformation detection, shortcut learning, data augmentation, large language models, semantic understanding

**Relevance Score:** 8

**TL;DR:** The paper introduces TruthOverTricks, a framework for evaluating and addressing shortcut learning in misinformation detection models, and proposes a data augmentation method (SMF) to improve robustness against various shortcuts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing misinformation detection models are limited by their reliance on superficial cues that do not generalize well to the real-world context of misinformation.

**Method:** The paper presents TruthOverTricks for evaluating shortcut learning, categorizes shortcut behaviors, and introduces SMF for data augmentation that includes paraphrasing and factual summarization.

**Key Contributions:**

	1. Introduction of TruthOverTricks evaluation paradigm
	2. Identification of intrinsic and extrinsic shortcut behaviors
	3. Development of SMF data augmentation framework

**Result:** Empirical results show existing detectors experience significant performance drops when facing intrinsic and extrinsic shortcuts, while SMF improves robustness across 16 benchmarks.

**Limitations:** The evaluation may not cover all possible types of misinformation shortcuts, and the generalizability of the proposed methods to all contexts is yet to be established.

**Conclusion:** The findings emphasize the need for deep semantic understanding in misinformation detection and provide publicly available resources to aid in research.

**Abstract:** Misinformation detection models often rely on superficial cues (i.e., \emph{shortcuts}) that correlate with misinformation in training data but fail to generalize to the diverse and evolving nature of real-world misinformation. This issue is exacerbated by large language models (LLMs), which can easily generate convincing misinformation through simple prompts. We introduce TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning in misinformation detection. TruthOverTricks categorizes shortcut behaviors into intrinsic shortcut induction and extrinsic shortcut injection, and evaluates seven representative detectors across 14 popular benchmarks, along with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo. Empirical results reveal that existing detectors suffer severe performance degradation when exposed to both naturally occurring and adversarially crafted shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation framework that mitigates shortcut reliance through paraphrasing, factual summarization, and sentiment normalization. SMF consistently enhances robustness across 16 benchmarks, encouraging models to rely on deeper semantic understanding rather than shortcut cues. To promote the development of misinformation detectors, we have published the resources publicly at https://github.com/whr000001/TruthOverTricks.

</details>


### [50] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)

*Jeonghun Kang, Soonmok Kwon, Joonseok Lee, Byung-Hak Kim*

**Main category:** cs.CL

**Keywords:** LLM, highlight summarization, sports analytics, natural language reasoning, baseball

**Relevance Score:** 6

**TL;DR:** DIAMOND is an LLM-driven agent for baseball highlight summarization that combines structured sports analytics with natural language reasoning to improve play importance assessment and storytelling in sports highlights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional approaches to sports highlight summarization often miss strategic depth and momentum shifts. Manual curation is resource-intensive and not scalable, necessitating an automated solution that combines analytics and narrative.

**Method:** DIAMOND integrates sabermetric features like Win Expectancy and Leverage Index with an LLM for context-aware play selection, ensuring both quantitative rigor and qualitative storytelling.

**Key Contributions:**

	1. Introduction of a hybrid LLM-driven summarization approach for sports highlights
	2. Integration of structured analytics with natural language reasoning
	3. Significant improvement in summarization performance metrics over traditional methods

**Result:** DIAMOND achieves an F1-score improvement from 42.9% to 84.8% compared to traditional WPA-only approaches and outperforms commercial and statistical baselines on Korean Baseball Organization League games.

**Limitations:** Limited in scale; results based on a small set of games.

**Conclusion:** The results suggest that modular, interpretable agent-based frameworks have the potential for effective event-level summarization in sports and potentially other areas.

**Abstract:** Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.

</details>


### [51] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)

*Hisami Suzuki, Satoru Katsumata, Takashi Kodama, Tetsuro Takahashi, Kouta Nakayama, Satoshi Sekine*

**Main category:** cs.CL

**Keywords:** Japanese LLM, safety evaluation, fine-tuning, dataset, multilingual applications

**Relevance Score:** 8

**TL;DR:** This paper introduces AnswerCarefully, a dataset designed to enhance the safety and appropriateness of Japanese LLM outputs through a set of 1,800 question-answer pairs reflecting local socio-cultural contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the safety and appropriateness of outputs from Japanese LLMs by providing a tailored dataset that reflects socio-cultural contexts.

**Method:** The paper presents a dataset of 1,800 question-answer pairs specifically designed for Japanese LLMs, followed by a fine-tuning process of a Japanese LLM using this dataset and a safety evaluation of 12 Japanese LLMs.

**Key Contributions:**

	1. Introduction of AnswerCarefully dataset for Japanese LLMs.
	2. Demonstration of improved safety in LLM outputs after fine-tuning.
	3. Provision of English translations to aid the development of multilingual datasets.

**Result:** The fine-tuned Japanese LLM demonstrated improved output safety without compromising the quality of general responses, and the dataset serves as a benchmark for evaluating LLM safety.

**Limitations:** 

**Conclusion:** The dataset not only improves safety for Japanese LLMs but also promotes the creation of similar datasets for other languages and regions through its English translations and annotations.

**Abstract:** In this paper we present AnswerCarefully, a dataset for promoting the safety and appropriateness of Japanese LLM outputs. The dataset consists of 1,800 pairs of questions and reference answers, where the questions require special attention in answering. It covers a wide range of risk categories established in prior English-language datasets, but the data samples are original in that they are manually created to reflect the socio-cultural context of LLM usage in Japan. We show that using this dataset for instruction to fine-tune a Japanese LLM led to improved output safety without compromising the utility of general responses. We also report the results of a safety evaluation of 12 Japanese LLMs using this dataset as a benchmark. Finally, we describe the latest update on the dataset which provides English translations and annotations of the questions, aimed at facilitating the derivation of similar datasets in different languages and regions.

</details>


### [52] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)

*Ukyo Honda, Tatsushi Oka*

**Main category:** cs.CL

**Keywords:** In-context learning, Large language models, Robustness, Natural language understanding, Explanations

**Relevance Score:** 9

**TL;DR:** Introduction of X²-ICL, enhancing robustness of large language models by exploring explanations for all possible labels.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve generalization and robustness of in-context learning in large language models, especially when faced with out-of-distribution data.

**Method:** The paper proposes X²-ICL, which systematically explores explanations for all possible labels compared to the traditional X-ICL approach.

**Key Contributions:**

	1. Introduction of the X²-ICL framework
	2. Demonstration of improved robustness in LLMs
	3. Systematic exploration of all possible label explanations

**Result:** X²-ICL shows significantly improved robustness on various natural language understanding datasets, outperforming existing in-context learning methods.

**Limitations:** 

**Conclusion:** The studies validate that X²-ICL enhances decision-making capabilities in language models, making them more reliable amidst data variability.

**Abstract:** In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.

</details>


### [53] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)

*Chuanghao Ding, Jiaping Wang, Ziqing Yang, Xiaoliang Wang, Dahua Lin, Cam-Tu Nguyen, Fei Tan*

**Main category:** cs.CL

**Keywords:** Consultant Decoding, Speculative Decoding, Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces Consultant Decoding (CD), a new mechanism that improves the efficiency of inference speed for large language models while maintaining performance quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiency caused by high rejection rates in Speculative Decoding (SD) for large language models (LLMs) and aims to provide a more effective verification mechanism.

**Method:** CD verifies candidate drafts using token-level likelihoods computed solely by the LLM, rather than using importance sampling as in SD.

**Key Contributions:**

	1. Introduction of Consultant Decoding (CD) as a novel verification mechanism.
	2. Demonstration of a 2.5-fold speed increase in LLM inference compared to traditional methods.
	3. Reduction of call frequency for large models to below 10% in challenging tasks.

**Result:** CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining performance quality at about 100% of the target model's performance.

**Limitations:** 

**Conclusion:** CD significantly enhances inference efficiency by combining models of varying parameter sizes and reducing reliance on the large model in lower than 10% of calls in demanding tasks.

**Abstract:** The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.

</details>


### [54] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)

*Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qianwen Zhang, Di Yin, Xing Sun, Xiao Huang*

**Main category:** cs.CL

**Keywords:** Graph Retrieval Augmented Generation, GraphRAG-Bench, benchmarking

**Relevance Score:** 9

**TL;DR:** GraphRAG-Bench is introduced as a benchmark for evaluating Graph Retrieval Augmented Generation models in a structured way that emphasizes multi-hop reasoning and diverse task coverage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limited evaluations of GraphRAG models that rely on traditional datasets, which do not comprehensively assess reasoning capabilities.

**Method:** Developing a large-scale, domain-specific benchmark with challenging questions across 16 disciplines and a holistic evaluation framework for GraphRAG models.

**Key Contributions:**

	1. Introduction of a new benchmark for GraphRAG evaluations
	2. Inclusion of multi-hop reasoning in question design
	3. Comprehensive evaluation framework covering the full GraphRAG process.

**Result:** The applied benchmark revealed insights into the relationship between graph-based structuring and improved reasoning capabilities across various GraphRAG methods.

**Limitations:** 

**Conclusion:** GraphRAG-Bench enhances the evaluation of GraphRAG models by focusing on complex reasoning tasks and providing a comprehensive assessment of the entire GraphRAG pipeline.

**Abstract:** Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.

</details>


### [55] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)

*Zhengyuan Liu, Geyu Lin, Hui Li Tan, Huayun Zhang, Yanfeng Lu, Xiaoxue Gao, Stella Xin Yin, He Sun, Hock Huan Goh, Lung Hsiang Wong, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** generative AI, language acquisition, dialogic tutor, multilingual education, interactive learning

**Relevance Score:** 7

**TL;DR:** SingaKids is a dialogic tutor that enhances language acquisition for children through picture description tasks and multilingual interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve personalized and interactive learning experiences for young learners and promote language acquisition through the integration of generative AI.

**Method:** SingaKids incorporates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation, and is enhanced through multilingual pre-training and task-specific tuning.

**Key Contributions:**

	1. Development of SingaKids as a dialogic tutor for language learning
	2. Integration of multilingual interactions and speech generation
	3. Empirical validation with elementary school students for effective learning outcomes

**Result:** Empirical studies show that SingaKids effectively facilitates language learning for elementary school students across four languages, benefiting learners of varying performance levels.

**Limitations:** 

**Conclusion:** The system demonstrates promise in providing engaging and effective dialogic teaching to young learners in diverse cultural contexts.

**Abstract:** The integration of generative artificial intelligence into educational applications has enhanced personalized and interactive learning experiences, and it shows strong potential to promote young learners language acquisition. However, it is still challenging to ensure consistent and robust performance across different languages and cultural contexts, and kids-friendly design requires simplified instructions, engaging interactions, and age-appropriate scaffolding to maintain motivation and optimize learning outcomes. In this work, we introduce SingaKids, a dialogic tutor designed to facilitate language learning through picture description tasks. Our system integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive learning environment in four languages: English, Mandarin, Malay, and Tamil. We further improve the system through multilingual pre-training, task-specific tuning, and scaffolding optimization. Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels.

</details>


### [56] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)

*Tairan Liu*

**Main category:** cs.CL

**Keywords:** gender inequality, natural language processing, textbooks, cross-cultural analysis, education

**Relevance Score:** 2

**TL;DR:** This study analyzes gender inequality in English textbooks from 22 countries using NLP techniques, revealing consistent male overrepresentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine cross-cultural gender inequality in textbooks and understand how it impacts children's worldviews.

**Method:** Applied natural language processing methods to analyze character count, first mention, TF-IDF word associations, and GloVe embeddings for gendered keywords in English textbooks.

**Key Contributions:**

	1. Quantitative analysis of gender representation in international textbooks using NLP techniques.
	2. Identification of gender patterns in proper names and character mentions across cultures.
	3. Establishment of benchmarks for future studies on gender inequality in educational resources.

**Result:** Findings indicate a persistent overrepresentation of male characters across all regions, with the Latin cultural sphere exhibiting the least disparity.

**Limitations:** The study focuses solely on English textbooks and may not account for nuances in non-English contexts.

**Conclusion:** The study highlights the need for awareness of gender biases in educational materials globally.

**Abstract:** Textbooks play a critical role in shaping children's understanding of the world. While previous studies have identified gender inequality in individual countries' textbooks, few have examined the issue cross-culturally. This study applies natural language processing methods to quantify gender inequality in English textbooks from 22 countries across 7 cultural spheres. Metrics include character count, firstness (which gender is mentioned first), and TF-IDF word associations by gender. The analysis also identifies gender patterns in proper names appearing in TF-IDF word lists, tests whether large language models can distinguish between gendered word lists, and uses GloVe embeddings to examine how closely keywords associate with each gender. Results show consistent overrepresentation of male characters in terms of count, firstness, and named entities. All regions exhibit gender inequality, with the Latin cultural sphere showing the least disparity.

</details>


### [57] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)

*Maryam Berijanian, Kuldeep Singh, Amin Sehati*

**Main category:** cs.CL

**Keywords:** entity relationship classification, large language models, multi-agent systems, prompting techniques, information extraction

**Relevance Score:** 7

**TL;DR:** This paper analyzes three AI agent architectures for entity relationship classification using large language models, demonstrating that multi-agent coordination outperforms standard prompting techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Entity relationship classification faces challenges due to limited labeled data and complex relational structures, making it essential to improve methods for relation classification using LLMs.

**Method:** The study compares three architectures: reflective self-evaluation, hierarchical task decomposition, and a multi-agent dynamic example generation mechanism, assessing their performance across multiple domains and backends.

**Key Contributions:**

	1. Introduction of multi-agent dynamic example generation for relation classification.
	2. Systematic evaluation of diverse AI agent architectures targeting entity relationship classification.
	3. Demonstration of performance improvements over standard few-shot prompting methods.

**Result:** Multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models in relation classification tasks.

**Limitations:** 

**Conclusion:** The findings provide practical guidance for designing LLM-based systems for structured relation extraction, with implications for improving AI agents' capabilities in handling complex relational tasks.

**Abstract:** Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at \href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [58] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)

*Mahammed Kamruzzaman, Abdullah Al Monsur, Gene Louis Kim, Anshuman Chhabra*

**Main category:** cs.CL

**Keywords:** Large Language Models, emotional stereotypes, nationality, HCI, bias

**Relevance Score:** 7

**TL;DR:** This paper investigates emotional stereotypes in Large Language Models (LLMs) based on nationality-specific personas, revealing significant biases and misalignment with human emotional responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how LLMs represent emotions across different nationalities and to identify potential biases in their outputs.

**Method:** Analysis of pre-trained LLMs by assigning nationality-specific personas and evaluating emotional attributions against cultural norms.

**Key Contributions:**

	1. Identification of nationality-based emotional biases in LLMs
	2. Demonstration of misalignment between LLM and human emotional responses
	3. Analysis of emotional stereotypes in LLM outputs across cultures

**Result:** Significant differences in emotion attribution among nationalities were found, with notable misalignments between LLM-generated emotions and human emotional responses, especially for negative emotions.

**Limitations:** Focus on a limited number of countries; the study may not account for all cultural contexts.

**Conclusion:** LLMs exhibit reductive and biased emotional stereotypes linked to nationalities, which can misrepresent human emotional diversity.

**Abstract:** Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.

</details>


### [59] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)

*Utsav Maskey, Mark Dras, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety mechanisms, long-tail distribution, instruction refusal, encrypted texts

**Relevance Score:** 8

**TL;DR:** Systematic evaluation of LLM behavior on long-tail encrypted texts and safety implications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the safety of LLMs in processing long-tail distributed encrypted texts and understand potential vulnerabilities.

**Method:** Introduced a two-dimensional framework assessing LLM safety on instruction refusal and generation safety, with extensive experimental evaluation.

**Key Contributions:**

	1. Introduction of a two-dimensional safety framework for LLMs
	2. Identification of mismatched-generalization attacks in LLMs
	3. Evaluation of pre-LLM and post-LLM safeguards for model safety.

**Result:** Demonstrated that LLMs capable of decrypting ciphers can be vulnerable to mismatched-generalization attacks, leading to unsafe responses or over-refusal.

**Limitations:** Focuses solely on long-tail encrypted texts and may not generalize to all text types.

**Conclusion:** The findings indicate the need for robust safety mechanisms for LLMs, particularly in long-tail text scenarios, while evaluating existing safeguards.

**Abstract:** This paper presents a systematic evaluation of Large Language Models' (LLMs) behavior on long-tail distributed (encrypted) texts and their safety implications. We introduce a two-dimensional framework for assessing LLM safety: (1) instruction refusal-the ability to reject harmful obfuscated instructions, and (2) generation safety-the suppression of generating harmful responses. Through comprehensive experiments, we demonstrate that models that possess capabilities to decrypt ciphers may be susceptible to mismatched-generalization attacks: their safety mechanisms fail on at least one safety dimension, leading to unsafe responses or over-refusal. Based on these findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss their strengths and limitations. This work contributes to understanding the safety of LLM in long-tail text scenarios and provides directions for developing robust safety mechanisms.

</details>


### [60] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)

*Bo Peng, Zhiheng Wang, Heyang Gong, Chaochao Lu*

**Main category:** cs.CL

**Keywords:** dialogue systems, synthetic data generation, personalized assistance, evaluation framework, causal graphs

**Relevance Score:** 8

**TL;DR:** This paper introduces a method for automatic synthetic data generation to enhance personalized assistance in dialogue systems, along with a new benchmark and evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for personalized assistance in dialogue systems, hindered by the lack of high-quality training data, and the challenges of traditional dataset construction methods.

**Method:** The authors propose automatic synthetic data generation methods for creating a training dataset for the Implicit Personalized Dialogue (IP-Dialog) benchmark, which comprises 10 tasks and 12 user attribute types. They also develop a systematic evaluation framework and causal graphs to analyze reasoning pathways.

**Key Contributions:**

	1. Introduction of automatic synthetic data generation for dialogue systems.
	2. Creation of the Implicit Personalized Dialogue (IP-Dialog) benchmark with 10 tasks and 12 user attributes.
	3. Development of a systematic evaluation framework with causal graphs for reasoning pathways.

**Result:** The experiments demonstrate the reliability of the synthetic dataset and provide insights into the models' performances in attribute awareness and reasoning.

**Limitations:** The effectiveness of synthetic data may vary based on the complexity of user interactions and potential bias in generated data.

**Conclusion:** The proposed data generation method and IP-Dialog benchmark enable significant advancements in building more effective personalized dialogue systems.

**Abstract:** In modern dialogue systems, the ability to implicitly infer user backgrounds from conversations and leverage this information for personalized assistance is crucial. However, the scarcity of high-quality data remains a fundamental challenge to evaluating and improving this capability. Traditional dataset construction methods are labor-intensive, resource-demanding, and raise privacy concerns. To address these issues, we propose a novel approach for automatic synthetic data generation and introduce the Implicit Personalized Dialogue (IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12 user attribute types. Additionally, we develop a systematic evaluation framework with four metrics to assess both attribute awareness and reasoning capabilities. We further propose five causal graphs to elucidate models' reasoning pathways during implicit personalization. Extensive experiments yield insightful observations and prove the reliability of our dataset.

</details>


### [61] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)

*Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, Wei Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, visualization generation, multimodal reports, HCI, AI in health informatics

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for generating multimodal reports that integrate texts and visualizations using Large Language Models (LLMs).

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Existing frameworks for generating comprehensive reports primarily focus on text, neglecting the integration of visualizations, which is crucial for effective communication.

**Method:** The paper proposes a structured representation of charts called Formal Description of Visualization (FDV) and introduces Multimodal DeepResearcher, a framework that breaks report generation into researching, textualization, planning, and multimodal generation.

**Key Contributions:**

	1. Introduction of Formal Description of Visualization (FDV) for chart representation.
	2. Development of Multimodal DeepResearcher for multimodal report generation.
	3. Creation of MultimodalReportBench for evaluation of generated reports.

**Result:** Multimodal DeepResearcher is evaluated using MultimodalReportBench, demonstrating an 82% win rate over baseline methods based on extensive experiments.

**Limitations:** The paper may not address all potential design challenges for a wide range of complex visualizations.

**Conclusion:** The proposed framework significantly enhances the capability of LLMs in generating integrated multimodal reports, bridging the gap in automated visualization generation.

**Abstract:** Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.

</details>


### [62] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)

*Yupeng Qi, Ziyu Lyu, Min Yang, Yanlin Wang, Lu Bai, Lixin Cui*

**Main category:** cs.CL

**Keywords:** large language models, Mixture of Experts, safety optimization, helpfulness optimization, dynamic routing

**Relevance Score:** 9

**TL;DR:** MidPO is a Mixture of Experts (MoE) framework that optimizes the balance between safety and helpfulness in large language models (LLMs) by using independent safety and helpfulness experts with dynamic routing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Enhance the safety and helpfulness of LLMs, addressing limitations of existing safety-constrained optimization methods.

**Method:** Develops a Mixture of Experts framework that creates two independent experts for safety and helpfulness. It employs a dynamic routing mechanism for adaptively balancing contributions from each expert.

**Key Contributions:**

	1. Introduction of a Mixture of Experts framework for LLMs.
	2. Development of a single-preference enhanced direct preference optimization.
	3. Implementation of a dynamic routing mechanism for expert contribution allocation.

**Result:** MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness on three popular datasets.

**Limitations:** 

**Conclusion:** The proposed framework effectively addresses the trade-off between safety and helpfulness in LLMs, and code/models will be publicly available.

**Abstract:** As large language models (LLMs) are increasingly applied across various domains, enhancing safety while maintaining the helpfulness of LLMs has become a critical challenge. Recent studies solve this problem through safety-constrained online preference optimization or safety-constrained offline preference optimization. However, the safety-constrained online methods often suffer from excessive safety, which might reduce helpfulness, while the safety-constrained offline methods perform poorly in adaptively balancing safety and helpfulness. To address these limitations, we propose MidPO, a \textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness \textbf{\underline{d}}ual \textbf{\underline{P}}reference \textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference enhanced direct preference optimization approach to transform the base model into two independent experts, termed safety and helpfulness experts, and fine-tunes the two independent experts for optimal safety or helpfulness performance. Secondly, to achieve an effective balance between safety and helpfulness, MidPO incorporates the two experts into the MoE framework and designs a dynamic routing mechanism to allocate contributions from each expert adaptively. We conduct quantitative and qualitative experiments on three popular datasets to demonstrate the proposed MidPO significantly outperforms state-of-the-art approaches in both safety and helpfulness. The code and models will be released.

</details>


### [63] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)

*Chunkit Chan, Yauwai Yim, Hongchuan Zeng, Zhiying Zou, Xinyuan Cheng, Zhifan Sun, Zheye Deng, Kawai Chung, Yuzhuo Ao, Yixiang Fan, Cheng Jiayang, Ercong Nie, Ginny Y. Wong, Helmut Schmid, Hinrich Schütze, Simon See, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Theory of Mind, LLMs, multilingual, cognitive modeling, language understanding

**Relevance Score:** 8

**TL;DR:** This paper presents XToM, a multilingual benchmark evaluating Theory of Mind (ToM) in LLMs across five languages, revealing performance discrepancies in ToM relative to language understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the capacity of LLMs to demonstrate Multilingual Theory of Mind, addressing the neglect of linguistic diversity in existing ToM evaluations.

**Method:** The authors developed XToM, a benchmark assessing LLMs' ToM capabilities in five languages through various contextually rich tasks.

**Key Contributions:**

	1. Introduction of XToM as a benchmark for multilingual ToM evaluation in LLMs.
	2. Demonstration of the variation in ToM performance of LLMs across languages.
	3. Highlighting the impact of linguistic diversity on cognitive modeling in AI.

**Result:** Evaluation of LLMs using XToM showed that, while they perform well in multilingual language understanding, their ToM abilities vary significantly across different languages.

**Limitations:** The benchmark is evaluated only on five languages, which may not represent the full scope of linguistic diversity.

**Conclusion:** LLMs do not replicate human-like mentalizing across linguistic contexts, exposing their limitations in this area despite strong language performance.

**Abstract:** Theory of Mind (ToM), the ability to infer mental states in others, is pivotal for human social cognition. Existing evaluations of ToM in LLMs are largely limited to English, neglecting the linguistic diversity that shapes human cognition. This limitation raises a critical question: can LLMs exhibit Multilingual Theory of Mind, which is the capacity to reason about mental states across diverse linguistic contexts? To address this gap, we present XToM, a rigorously validated multilingual benchmark that evaluates ToM across five languages and incorporates diverse, contextually rich task scenarios. Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a pronounced dissonance: while models excel in multilingual language understanding, their ToM performance varies across languages. Our findings expose limitations in LLMs' ability to replicate human-like mentalizing across linguistic contexts.

</details>


### [64] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)

*Zijian Li, Xiaocheng Feng, Huixin Liu, Yichong Huang, Ting Liu, Bing Qin*

**Main category:** cs.CL

**Keywords:** model merging, fine-tuning, Frobenius norm, task interference, adaptive merging

**Relevance Score:** 7

**TL;DR:** This paper presents FroM, an adaptive merging method that enhances model merging by directly measuring model parameters and alleviating task interference.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of task interference when merging fine-tuning models, particularly in parameter-efficient scenarios.

**Method:** FroM utilizes the Frobenius norm to measure model parameters directly, without requiring training data, and introduces an additional hyperparameter for control.

**Key Contributions:**

	1. Introduction of FroM, an adaptive merging method that requires no training data.
	2. Effectively alleviates task interference during model merging.
	3. Demonstrates superior performance over baseline methods across multiple scenarios.

**Result:** FroM outperforms baseline methods in various fine-tuning scenarios, effectively mitigating task interference.

**Limitations:** 

**Conclusion:** The proposed FroM method offers a promising solution for parameter-efficient fine-tuning model merging, ensuring better performance without the usual drawbacks of traditional methods.

**Abstract:** With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.

</details>


### [65] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)

*Yifan Duan, Yihong Tang, Kehai Chen, Liqiang Nie, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Optimization, Role-Playing Prompts

**Relevance Score:** 8

**TL;DR:** This paper introduces ORPP, a framework for generating optimized role-playing prompts to improve the performance of large language models (LLMs) by confining the prompt search space to role-playing scenarios.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The existing model-driven strategies for prompt optimization often suffer from high computational overhead and limited applicability due to their requirements for strong optimization capabilities from the model.

**Method:** ORPP performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts and uses few-shot learning to apply this optimization to other samples.

**Key Contributions:**

	1. Introduction of the ORPP framework for prompt optimization
	2. Effective use of role-playing scenarios to generate prompts
	3. Demonstration of superior performance compared to existing methods.

**Result:** ORPP matches or surpasses existing prompt optimization methods, displaying superior performance and integration with other prompt techniques.

**Limitations:** 

**Conclusion:** ORPP effectively enhances LLM performance through optimized role-playing prompts and offers a flexible, plug-and-play capability.

**Abstract:** High-quality prompts are crucial for eliciting outstanding performance from large language models (LLMs) on complex tasks. Existing research has explored model-driven strategies for prompt optimization. However, these methods often suffer from high computational overhead or require strong optimization capabilities from the model itself, which limits their broad applicability.To address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a framework that enhances model performance by optimizing and generating role-playing prompts. The core idea of ORPP is to confine the prompt search space to role-playing scenarios, thereby fully activating the model's intrinsic capabilities through carefully crafted, high-quality role-playing prompts. Specifically, ORPP first performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts. Then, leveraging the model's few-shot learning capability, it transfers the optimization experience to efficiently generate suitable prompts for the remaining samples.Our experimental results show that ORPP not only matches but in most cases surpasses existing mainstream prompt optimization methods in terms of performance. Notably, ORPP demonstrates superior "plug-and-play" capability. In most cases, it can be integrated with various other prompt methods and further enhance their effectiveness.

</details>


### [66] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)

*Inderjeet Nair, Lu Wang*

**Main category:** cs.CL

**Keywords:** LLMs, Ethical risks, Value preferences, Long-form responses, Value expression

**Relevance Score:** 8

**TL;DR:** This paper investigates whether value preferences inferred from short-form assessments align with those expressed in longer, open-ended responses generated by LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the alignment of ethical value preferences expressed in short-form tests with those articulated in long-form LLM outputs, addressing the inadequacies of current evaluation methods.

**Method:** The study compares value preferences from short-form reactions with long-form responses from five LLMs, varying argument counts to examine verbosity preferences.

**Key Contributions:**

	1. Identification of weak correlations between short-form and long-form value preferences
	2. Insights into how verbosity affects value expression consistency
	3. Highlighting the need for more robust evaluation methods in LLM assessments

**Result:** The analysis reveals a weak correlation between short-form and long-form value preferences, and similar weak correlations across distinct long-form settings, indicating that alignment yields only modest consistency gains in value expression.

**Limitations:** The study only examines five specific LLMs and may not generalize to others or wider contexts.

**Conclusion:** The findings highlight the necessity for improved methods to ensure consistent value expression across varied applications of LLMs.

**Abstract:** Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications.

</details>


### [67] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)

*Sina Bagheri Nezhad, Ameeta Agrawal*

**Main category:** cs.CL

**Keywords:** NeuroSymbolic Reasoning, Large Language Models, Python Code Generation, Multilingual Reasoning, Symbolic Operations

**Relevance Score:** 8

**TL;DR:** This paper introduces NeuroSymbolic Augmented Reasoning (NSAR), a method that enhances multi-target reasoning in long-context scenarios using a combination of neural and symbolic reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models in multi-target reasoning with scattered information in extensive documents.

**Method:** NSAR extracts symbolic facts from text and generates executable Python code for complex reasoning tasks, tested across multiple languages and context lengths.

**Key Contributions:**

	1. Introduction of NeuroSymbolic Augmented Reasoning (NSAR) for improved reasoning in LLMs
	2. Demonstration of substantial performance gains over baseline methods
	3. Evaluation across diverse languages and context lengths for broader applicability

**Result:** NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in reasoning accuracy.

**Limitations:** 

**Conclusion:** Combining explicit symbolic operations with neural inference leads to robust, interpretable, and scalable reasoning in multilingual contexts.

**Abstract:** Large language models (LLMs) often struggle to perform multi-target reasoning in long-context scenarios where relevant information is scattered across extensive documents. To address this challenge, we introduce NeuroSymbolic Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic reasoning during inference. NSAR explicitly extracts symbolic facts from text and generates executable Python code to handle complex reasoning steps. Through extensive experiments across seven languages and diverse context lengths, we demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and advanced prompting strategies in accurately identifying and synthesizing multiple pieces of information. Our results highlight the effectiveness of combining explicit symbolic operations with neural inference for robust, interpretable, and scalable reasoning in multilingual settings.

</details>


### [68] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)

*Junzhe Zhang, Huixuan Zhang, Xinyu Hu, Li Lin, Mingqi Gao, Shi Qiu, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** multimodal evaluation, text-to-image generation, human evaluation data

**Relevance Score:** 8

**TL;DR:** The paper introduces Minos-Corpus, a large-scale multimodal evaluation dataset and the Minos model which outperforms existing evaluation models on text-to-image generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of evaluation capabilities in multimodal generation tasks, particularly for text-to-image (T2I) generation, and to incorporate large-scale human evaluation data.

**Method:** The authors propose a large-scale dataset, Minos-Corpus, that includes human and GPT evaluation data for I2T and T2I tasks. They develop Data Selection and Balance, Mix-SFT training methods, and the Minos evaluation model based on a 7B backbone, leveraging high-quality human evaluation data.

**Key Contributions:**

	1. Introduction of Minos-Corpus dataset for multimodal evaluation
	2. Development of the Minos evaluation model with state-of-the-art performance
	3. Demonstration of the importance of human evaluation data in model training

**Result:** Minos achieves state-of-the-art performance on multimodal evaluation tasks, particularly outperforming existing models in T2I generation evaluation.

**Limitations:** 

**Conclusion:** Leveraging large-scale human evaluation data is crucial for developing effective multimodal evaluation systems, and Minos demonstrates significant advancements in evaluation performance across tasks.

**Abstract:** Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks.

</details>


### [69] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)

*Yongjian Li, HaoCheng Chu, Yukun Yan, Zhenghao Liu, Shi Yu, Zheni Zeng, Ruobing Wang, Sen Song, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Knowledge-Aware Refinement, Dense Direct Preference Optimization, Factual Consistency, Semantic Consistency

**Relevance Score:** 10

**TL;DR:** This paper introduces KARE-RAG, an approach to enhance retrieval-augmented generation (RAG) by improving the processing of noisy content and correcting factual inaccuracies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There are persistent factual inconsistencies in retrieval-augmented generation (RAG) due to noise in retrieved documents, highlighting the need for improved handling of such content.

**Method:** KARE-RAG employs structured knowledge representations for error detection, a refined training objective called Dense Direct Preference Optimization (DDPO) that focuses on correcting critical errors, and a contrastive data generation pipeline for maintaining semantic consistency while addressing factual inaccuracies.

**Key Contributions:**

	1. Structured knowledge representations for error detection
	2. Dense Direct Preference Optimization (DDPO) for error correction
	3. A contrastive data generation pipeline for semantic consistency

**Result:** Experiments show that KARE-RAG significantly enhances standard RAG pipelines across various model scales, improving performance on both in-domain and out-of-domain tasks without compromising general capabilities.

**Limitations:** 

**Conclusion:** The study establishes a new direction in improving retrieval-augmented generation by enhancing models' ability to process retrieved knowledge, leading to better performance across different inference paradigms.

**Abstract:** Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access broader knowledge sources, yet factual inconsistencies persist due to noise in retrieved documents-even with advanced retrieval methods. We demonstrate that enhancing generative models' capacity to process noisy content is equally critical for robust performance. In this paper, we present KARE-RAG (Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge utilization through three key innovations: (1) structured knowledge representations that facilitate error detection during training, (2) Dense Direct Preference Optimization (DDPO)-a refined training objective that prioritizes correction of critical errors, and (3) a contrastive data generation pipeline that maintains semantic consistency while rectifying factual inaccuracies. Experiments show our method significantly enhances standard RAG pipelines across model scales, improving both in-domain and out-of-domain task performance without compromising general capabilities. Notably, these gains are achieved with modest training data, suggesting data-efficient optimization is possible through targeted learning strategies. Our findings establish a new direction for RAG improvement: by improving how models learn to process retrieved content, we can enhance performance across diverse inference paradigms. All data and code will be publicly available on Github.

</details>


### [70] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)

*Jie Zhu, Junhui Li, Yalong Wen, Xiandong Li, Lifan Guo, Feng Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial Meeting Understanding, Benchmark Datasets, Multilingual, AI Applications

**Relevance Score:** 8

**TL;DR:** Introducing the $	exttt{M$^3$FinMeeting}$ benchmark for evaluating LLMs in financial meeting comprehension, featuring multilingual support and multi-task capabilities.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing financial benchmarks that fail to capture real-world dynamics of financial meetings, which are crucial for evaluating LLMs in a practical context.

**Method:** Developed a dataset called $	exttt{M$^3$FinMeeting}$ that supports three languages (English, Chinese, Japanese) and includes tasks like summarization, QA pair extraction, and question answering.

**Key Contributions:**

	1. A multilingual benchmark for financial meeting understanding in English, Chinese, and Japanese.
	2. Inclusion of various industry sectors to cover a wide range of financial activities.
	3. Three distinct evaluation tasks tailored to better assess LLM capabilities.

**Result:** Experimental evaluations show that even advanced long-context LLMs have significant room for improvement when using the $	exttt{M$^3$FinMeeting}$ benchmark.

**Limitations:** 

**Conclusion:** The $	exttt{M$^3$FinMeeting}$ benchmark is effective for assessing LLMs' understanding of financial meetings, providing insights into their performance and areas for enhancement.

**Abstract:** Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting comprehension skills.

</details>


### [71] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)

*Zhuohan Xie, Dhruv Sahnan, Debopriyo Banerjee, Georgi Georgiev, Rushil Thareja, Hachem Madmoun, Jinyan Su, Aaryamonvikram Singh, Yuxia Wang, Rui Xing, Fajri Koto, Haonan Li, Ivan Koychev, Tanmoy Chakraborty, Salem Lahlou, Veselin Stoyanov, Preslav Nakov*

**Main category:** cs.CL

**Keywords:** Financial Reasoning, Benchmarking, Chain-of-Thought, LLMs, Symbolic Reasoning

**Relevance Score:** 6

**TL;DR:** FinChain is a new benchmark for evaluating multi-step symbolic reasoning in financial tasks, addressing the lack of datasets that assess intermediate reasoning steps.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate multi-step symbolic reasoning in financial tasks, where existing benchmarks fail to consider the reasoning processes leading to final answers.

**Method:** Introduced FinChain, a benchmark that spans 54 topics across 12 financial domains with varied complexity and expertise levels; includes executable Python traces for data generation and adaptation.

**Key Contributions:**

	1. FinChain benchmark for multi-step financial reasoning
	2. ChainEval evaluation metric for assessing intermediate reasoning
	3. Python traces for automatic data generation and model adaptation

**Result:** 30 LLMs were benchmarked on FinChain, showing a significant gap in multi-step reasoning performance, even among the best models.

**Limitations:** 

**Conclusion:** The FinChain benchmark and ChainEval metric provide a structured approach to enhance the evaluation of financial reasoning capabilities of LLMs.

**Abstract:** Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.

</details>


### [72] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)

*Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy*

**Main category:** cs.CL

**Keywords:** COLLATE, small language models, reasoning, preference optimization, natural language inference

**Relevance Score:** 8

**TL;DR:** COLLATE is a framework that improves the reasoning abilities of small language models without relying on larger models, using diverse rationales to enhance task performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of applying large language models (LLMs) in commercial settings due to transparency and copyright issues, while improving smaller LLMs' reasoning abilities.

**Method:** COLLATE tunes a small LLM by generating outputs from a pool of diverse rationales, utilizing preference optimization to select the rationale that maximizes the likelihood of the ground-truth answer.

**Key Contributions:**

	1. Introduction of the COLLATE framework
	2. Improvement of reasoning capabilities in smaller LLMs without size distillation
	3. Demonstration of efficacy across multiple datasets and domains

**Result:** COLLATE outperforms existing trainable and prompting baselines across 5 datasets in 3 domains, showing improvements in reasoning tasks.

**Limitations:** 

**Conclusion:** The framework demonstrates the value of using multiple rationale providers to enhance the capabilities of smaller LLMs, applicable across various parameter scales.

**Abstract:** LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions by generating step-by-step rationales. Prior works have utilized this capability to improve smaller and cheaper LMs (say, with 7B parameters). However, various practical constraints, such as copyright and legal issues, owing to lack of transparency in the pre-training data of large (often closed) models, prevent their use in commercial settings. Little focus has been given to improving the innate reasoning ability of smaller models without distilling information from larger LLMs. To address this, we propose COLLATE, a trainable framework that tunes a (small) LLM to generate those outputs from a pool of diverse rationales that selectively improves the downstream task. COLLATE enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs. The LLM is then tuned via preference optimization to choose the candidate rationale which maximizes the likelihood of ground-truth answer. COLLATE outperforms several trainable and prompting baselines on 5 datasets across 3 domains: maths problem solving, natural language inference, and commonsense reasoning. We show the eff icacy of COLLATE on LLMs from different model families across varying parameter scales (1B to 8B) and demonstrate the benefit of multiple rationale providers guided by the end task through ablations. Code is released here (https://github.com/Sohanpatnaik106/collate).

</details>


### [73] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)

*Yingying Zhuang, Aman Gupta, Anurag Beniwal*

**Main category:** cs.CL

**Keywords:** multilingual information retrieval, embedding models, contrastive learning

**Relevance Score:** 4

**TL;DR:** This paper presents a strategy to fine-tune multilingual embedding models for better information retrieval across languages, particularly aiding low-resource languages using a monolingual knowledge base.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge sharing across languages, especially for low-resource languages, by developing an effective embedding model for multilingual information retrieval.

**Method:** The authors propose a novel strategy that employs weighted sampling for contrastive learning to fine-tune multilingual embedding models.

**Key Contributions:**

	1. Novel strategy for fine-tuning multilingual embedding models
	2. Use of weighted sampling for contrastive learning
	3. Demonstrated performance gains in information retrieval tasks

**Result:** The proposed method shows performance improvements of up to 31.03% in Mean Reciprocal Rank (MRR) and up to 33.98% in Recall@3 compared to standard methods.

**Limitations:** 

**Conclusion:** The methodology is proven to be language agnostic and can be applied to both multilingual and code-switching scenarios, enhancing cross-language knowledge sharing.

**Abstract:** Multilingual information retrieval has emerged as powerful tools for expanding knowledge sharing across languages. On the other hand, resources on high quality knowledge base are often scarce and in limited languages, therefore an effective embedding model to transform sentences from different languages into a feature vector space same as the knowledge base language becomes the key ingredient for cross language knowledge sharing, especially to transfer knowledge available in high-resource languages to low-resource ones. In this paper we propose a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, enabling multilingual information retrieval with a monolingual knowledge base. We demonstrate that the weighted sampling strategy produces performance gains compared to standard ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our proposed methodology is language agnostic and applicable for both multilingual and code switching use cases.

</details>


### [74] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)

*Jinu Lee, Sagnik Mukherjee, Dilek Hakkani-Tur, Julia Hockenmaier*

**Main category:** cs.CL

**Keywords:** Reasoning models, Semantic analysis, Graphs, Human-computer interaction, Machine learning

**Relevance Score:** 7

**TL;DR:** ReasoningFlow introduces a schema for analyzing reasoning structures in large reasoning models (LRMs), transforming reasoning traces into directed acyclic graphs for improved understanding and evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding, evaluation, and enhancement of reasoning processes in large reasoning models.

**Method:** The paper presents ReasoningFlow, which parses reasoning traces from LRMs into directed acyclic graphs to characterize reasoning patterns as subgraph structures.

**Key Contributions:**

	1. Introduction of ReasoningFlow schema for analyzing reasoning patterns.
	2. Transformation of reasoning traces into directed acyclic graphs.
	3. Improvement in understanding and evaluation of LRM reasoning processes.

**Result:** ReasoningFlow offers a human-interpretable framework that facilitates the analysis of complex reasoning traces.

**Limitations:** 

**Conclusion:** The framework potentially enhances the transparency and effectiveness of reasoning processes in LRMs.

**Abstract:** Large reasoning models (LRMs) generate complex reasoning traces with planning, reflection, verification, and backtracking. In this work, we introduce ReasoningFlow, a unified schema for analyzing the semantic structures of these complex traces. ReasoningFlow parses traces into directed acyclic graphs, enabling the characterization of distinct reasoning patterns as subgraph structures. This human-interpretable representation offers promising applications in understanding, evaluating, and enhancing the reasoning processes of LRMs.

</details>


### [75] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)

*Maike Behrendt, Stefan Sylvius Wagner, Carina Weinmann, Marike Bormann, Mira Warne, Stefan Harmeling*

**Main category:** cs.CL

**Keywords:** political participation, machine learning, online discourse, deliberation, platform design

**Relevance Score:** 3

**TL;DR:** This paper discusses the role of machine learning in improving the quality of online political discussions by addressing issues related to deliberation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of digital formats for discussing political issues, enhancing the quality of online deliberation has become increasingly important.

**Method:** The paper analyzes the challenges in political online discussions and explores machine learning techniques to improve deliberative practices.

**Key Contributions:**

	1. Identification of key issues in online political discussions
	2. Application of machine learning methods to counteract these issues
	3. Recommendations for platform design improvements based on findings

**Result:** Machine learning can help identify issues in discussions and propose solutions to facilitate better argument exchange and civil discourse.

**Limitations:** 

**Conclusion:** By leveraging machine learning, we can enhance the deliberative quality of online political discussions.

**Abstract:** Political online participation in the form of discussing political issues and exchanging opinions among citizens is gaining importance with more and more formats being held digitally. To come to a decision, a careful discussion and consideration of opinions and a civil exchange of arguments, which is defined as the act of deliberation, is desirable. The quality of discussions and participation processes in terms of their deliberativeness highly depends on the design of platforms and processes. To facilitate online communication for both participants and initiators, machine learning methods offer a lot of potential. In this work we want to showcase which issues occur in political online discussions and how machine learning can be used to counteract these issues and enhance deliberation.

</details>


### [76] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)

*Xin Liu, Lu Wang*

**Main category:** cs.CL

**Keywords:** chain-of-thought prompting, large language models, inference efficiency, early stopping, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper investigates the minimum reasoning steps necessary for large language models (LLMs) to produce accurate answers and proposes strategies to enhance inference efficiency by reducing unnecessary reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the problem of verbose and redundant outputs in large language models when using chain-of-thought prompting, which increases inference costs.

**Method:** The authors conduct a systematic study to determine the minimal reasoning required for stable decision-making in math tasks and propose three strategies: early stopping via answer consistency, boosting end-of-reasoning signals, and a supervised method for stopping based on internal activations.

**Key Contributions:**

	1. Identify redundant reasoning steps in LLMs during math tasks.
	2. Propose novel inference-time strategies to enhance efficiency in LLMs.
	3. Demonstrate significant reductions in token usage without accuracy loss.

**Result:** Experiments on five benchmarks and five open-weight LLMs show a significant reduction in token usage with minimal accuracy impact, particularly with Answer Consistency reducing tokens by over 40% while improving accuracy on NaturalQuestions.

**Limitations:** The study primarily focuses on math reasoning tasks, and results may not generalize across all domains of reasoning.

**Conclusion:** The findings highlight the value of efficient reasoning methods at inference time, with practical advantages for applying LLMs in real-world scenarios.

**Abstract:** Chain-of-thought (CoT) prompting enhances reasoning in large language models (LLMs) but often leads to verbose and redundant outputs, thus increasing inference cost. We hypothesize that many reasoning steps are unnecessary for producing correct answers. To investigate this, we start with a systematic study to examine what is the minimum reasoning required for a model to reach a stable decision. We find that on math reasoning tasks like math, models typically converge to their final answers after 60\% of the reasoning steps, suggesting substantial redundancy in the remaining content. Based on these insights, we propose three inference-time strategies to improve efficiency: (1) early stopping via answer consistency, (2) boosting the probability of generating end-of-reasoning signals, and (3) a supervised method that learns when to stop based on internal activations. Experiments across five benchmarks and five open-weights LLMs show that our methods significantly reduce token usage with little or no accuracy drop. In particular, on NaturalQuestions, Answer Consistency reduces tokens by over 40\% while further improving accuracy. Our work underscores the importance of cost-effective reasoning methods that operate at inference time, offering practical benefits for real-world applications.

</details>


### [77] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)

*Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie*

**Main category:** cs.CL

**Keywords:** Multimodal Retrieval-Augmented Generation, CoRe-MMRAG, Knowledge Inconsistency, Multimodal Integration, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper presents CoRe-MMRAG, a framework to reconcile inconsistencies in Multimodal Retrieval-Augmented Generation, improving the reliability of multimodal large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of knowledge inconsistency in multimodal retrieval-augmented generation, especially the discrepancies between parametric and retrieved knowledge and misalignments in visual-textual sources.

**Method:** CoRe-MMRAG employs a four-stage pipeline: generating an internal response, selecting relevant multimodal evidence, generating an external response, and integrating both for a reliable answer.

**Key Contributions:**

	1. Introduction of CoRe-MMRAG framework for knowledge reconciliation in MMRAG.
	2. Improvements in multimodal integration and unified answer generation.
	3. Substantial performance gains on relevant benchmarks.

**Result:** The proposed framework achieves significant performance improvements on KB-VQA benchmarks, with 5.6% and 9.3% gains on InfoSeek and Encyclopedic-VQA respectively.

**Limitations:** 

**Conclusion:** CoRe-MMRAG enhances the robustness of multimodal large language models by reconciling source inconsistencies and improves overall performance in multimodal tasks.

**Abstract:** Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge \textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG} (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6\% and 9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We release code and data at \href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [78] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)

*Yirao Zhao, Guizhen Chen, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang*

**Main category:** cs.CL

**Keywords:** large language models, pruning, expert models, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper presents a new pruning method for large language models that focuses on creating compact expert models without extensive post-training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce computational resource requirements and improve inference speed for large language models by developing a method that prunes redundant parameters effectively.

**Method:** The paper introduces a custom pruning method called Cus-Prun, which identifies and prunes irrelevant neurons based on language, domain, and task dimensions, creating lightweight expert models without the need for post-training.

**Key Contributions:**

	1. Introduction of Cus-Prun for efficient pruning of large language models
	2. Demonstrated performance improvements over existing pruning methods
	3. Facilitates the creation of expert models without requiring post-training adjustments.

**Result:** Cus-Prun consistently outperforms existing pruning techniques, demonstrating minimal loss in capabilities while transitioning from large general models to smaller expert models.

**Limitations:** 

**Conclusion:** The proposed Cus-Prun method allows for efficient pruning of large language models, making it possible to tailor them to specific tasks without degradation in performance.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to prune a large general model into a smaller lightweight expert model, which is positioned along the "language", "domain" and "task" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates expert models without any post-training. Our experiments demonstrate that $\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.

</details>


### [79] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)

*Muhammad Falensi Azmi, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto*

**Main category:** cs.CL

**Keywords:** safety evaluation, large language models, cultural sensitivity, Indonesian languages, multilingual LLMs

**Relevance Score:** 8

**TL;DR:** The paper introduces IndoSafety, a safety evaluation dataset for Indonesian LLMs, emphasizing culturally sensitive safety measures.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of safety evaluations for region-specific LLMs, particularly in culturally diverse environments like Indonesia, where understanding local norms is critical.

**Method:** Developed a high-quality, human-verified dataset that evaluates the safety of Indonesian LLMs across various language varieties and established a taxonomy for safety evaluation that reflects Indonesia's sociocultural context.

**Key Contributions:**

	1. Introduction of IndoSafety dataset tailored for Indonesian LLMs
	2. Cultural context incorporated into safety evaluation framework
	3. Demonstrated effectiveness of the dataset in improving LLM safety

**Result:** IndoSafety demonstrated that existing LLMs for Indonesian often output unsafe content, especially in colloquial and local languages, but showed improvement in safety with fine-tuning on this new dataset without sacrificing performance.

**Limitations:** Focus limited to Indonesian languages and may not be generalizable to other cultural contexts; examples included may be offensive or biased.

**Conclusion:** The study emphasizes the importance of culturally grounded safety measures for LLMs and the potential of IndoSafety to enhance responsible LLM deployment in multilingual contexts.

**Abstract:** Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia's sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased.

</details>


### [80] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)

*Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell*

**Main category:** cs.CL

**Keywords:** self-supervised learning, prosody, speech comprehension, acoustic features, emotion recognition

**Relevance Score:** 6

**TL;DR:** This study investigates the role of prosody in speech comprehension using self-supervised learning to analyze acoustic structures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how prosodic features like intonation, tempo, and loudness contribute to speech comprehension beyond lexical content.

**Method:** The study employs a self-supervised learning approach with the Masked Prosody Model to analyze and predict perceptual labels from prosodic features.

**Key Contributions:**

	1. Introduces the Masked Prosody Model for analyzing prosody in speech.
	2. Demonstrates relative gains in predicting perceptual labels using SSL compared to traditional methods.
	3. Highlights the importance of timescale in SSL training objectives for prosodic analysis.

**Result:** The Masked Prosody Model shows improved prediction capabilities for longer-term structures, such as emotion recognition, over traditional acoustic features.

**Limitations:** 

**Conclusion:** The research highlights the significance of training objectives in self-supervised learning and the advantages of complex SSL-encoded structures.

**Abstract:** People exploit the predictability of lexical structures during text comprehension. Though predictable structure is also present in speech, the degree to which prosody, e.g. intonation, tempo, and loudness, contributes to such structure independently of the lexical content is unclear. This study leverages self-supervised learning (SSL) to examine the temporal granularity of structures in the acoustic correlates of prosody. Representations from our proposed Masked Prosody Model can predict perceptual labels dependent on local information, such as word boundaries, but provide the most value for labels involving longer-term structures, like emotion recognition. Probing experiments across various perceptual labels show strong relative gains over untransformed pitch, energy, and voice activity features. Our results reveal the importance of SSL training objective timescale and highlight the value of complex SSL-encoded structures compared to more constrained classical structures.

</details>


### [81] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)

*Maria Levchenko*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Russian language, cultural events, Large Language Models, NER models

**Relevance Score:** 6

**TL;DR:** This paper explores Named Entity Recognition (NER) for person names in Russian news regarding cultural events, using a dataset from Saint Petersburg. It evaluates various models, finding GPT-4o performs best with specific prompts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address NER challenges in Russian cultural event texts and evaluate the effectiveness of various NER models.

**Method:** Comparison of multiple NER models, including transformer-based architectures (DeepPavlov, RoBERTa, SpaCy) and recent LLMs (GPT-3.5, GPT-4, GPT-4o), on the unique SPbLitGuide dataset.

**Key Contributions:**

	1. Introduction of the SPbLitGuide dataset for cultural event NER
	2. Demonstration of superior performance in recent LLMs for NER tasks
	3. Insights into NER model limitations and capabilities in Russian.

**Result:** GPT-4o achieved an F1 score of 0.93 with specific JSON prompting, while GPT-4 had the highest precision at 0.99.

**Limitations:** 

**Conclusion:** The study enhances understanding of NER models for morphologically rich languages like Russian and shows advancements in model capabilities and deployment.

**Abstract:** This paper addresses the challenge of Named Entity Recognition (NER) for person names within the specialized domain of Russian news texts concerning cultural events. The study utilizes the unique SPbLitGuide dataset, a collection of event announcements from Saint Petersburg spanning 1999 to 2019. A comparative evaluation of diverse NER models is presented, encompassing established transformer-based architectures such as DeepPavlov, RoBERTa, and SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4, and GPT-4o. Key findings highlight the superior performance of GPT-4o when provided with specific prompting for JSON output, achieving an F1 score of 0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The research contributes to a deeper understanding of current NER model capabilities and limitations when applied to morphologically rich languages like Russian within the cultural heritage domain, offering insights for researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025) achieves F1=0.94 for both simple and structured prompts, demonstrating rapid progress across model families and simplified deployment requirements.

</details>


### [82] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)

*Minh Duc Bui, Kyung Eun Park, Goran Glavaš, Fabian David Schmidt, Katharina von der Wense*

**Main category:** cs.CL

**Keywords:** Large Language Models, Measurement Systems, Cultural Differences, Chain-of-Thought, Accuracy Variance

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of large language models (LLMs) to provide accurate conversions across different measurement systems, focusing on their default responses and accuracy variations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As measurement systems vary across cultures, it's crucial for LLMs to effectively handle these differences in order to provide accurate and relevant information to diverse users.

**Method:** The study tests seven open-source LLMs using newly compiled datasets, addressing three research questions related to default measurement systems, accuracy variations across systems, and the effectiveness of reasoning methods.

**Key Contributions:**

	1. Identification of default measurement systems used by LLMs
	2. Analysis of accuracy variance across different measurement systems
	3. Evaluation of reasoning methods to improve performance for underrepresented systems.

**Result:** LLMs default to the measurement system predominantly present in their training data, show significant performance instability across systems, and can mitigate some challenges through chain-of-thought reasoning, though at a cost of increased response length and compute resources.

**Limitations:** The study primarily focuses on open-source LLMs and may not generalize to all models; increased compute cost can discourage use in practical applications.

**Conclusion:** While reasoning methods can help stabilize responses across underrepresented measurement systems, the increased costs may limit usability for affected cultural groups.

**Abstract:** Measurement systems (e.g., currencies) differ across cultures, but the conversions between them are well defined so that humans can state facts using any measurement system of their choice. Being available to users from diverse cultural backgrounds, large language models (LLMs) should also be able to provide accurate information irrespective of the measurement system at hand. Using newly compiled datasets we test if this is the case for seven open-source LLMs, addressing three key research questions: (RQ1) What is the default system used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate potential challenges w.r.t. underrepresented systems via reasoning? Our findings show that LLMs default to the measurement system predominantly used in the data. Additionally, we observe considerable instability and variance in performance across different measurement systems. While this instability can in part be mitigated by employing reasoning methods such as chain-of-thought (CoT), this implies longer responses and thereby significantly increases test-time compute (and inference costs), marginalizing users from cultural backgrounds that use underrepresented measurement systems.

</details>


### [83] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)

*Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin*

**Main category:** cs.CL

**Keywords:** self-preference bias, large language models, DBG score

**Relevance Score:** 8

**TL;DR:** This paper introduces the DBG score to measure self-preference bias in large language models (LLMs) using gold judgments to account for response quality, and explores the impact of response style and training data on bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To accurately measure self-preference bias in LLMs without conflating it with response quality.

**Method:** The DBG score is introduced, which measures self-preference bias by comparing model scores to gold judgments, followed by extensive experiments across various LLMs and analysis of contributing factors to bias.

**Key Contributions:**

	1. Introduction of the DBG score for measuring self-preference bias
	2. Comprehensive experiments across various LLMs
	3. Insights into factors influencing self-preference bias

**Result:** The DBG score effectively isolates self-preference bias from response quality influences, and findings indicate that response style and post-training data can mitigate this bias.

**Limitations:** 

**Conclusion:** Using the DBG score allows for a clearer understanding of self-preference bias in LLMs, with important insights on how to alleviate it through specific intervention strategies.

**Abstract:** Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [84] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)

*Fan Gao, Dongyuan Li, Ding Xia, Fei Mi, Yasheng Wang, Lifeng Shang, Baojun Wang*

**Main category:** cs.CL

**Keywords:** Chinese essay writing, Large Language Models, benchmark, education, evaluation

**Relevance Score:** 7

**TL;DR:** This paper introduces a multi-genre benchmark for evaluating Chinese essay writing using Large Language Models (LLMs), addressing structural and rhetorical complexities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of LLMs in generating and evaluating Chinese essays, which are critical in educational contexts but underexplored in existing benchmarks.

**Method:** The authors propose enchName, a multi-genre benchmark for Chinese essays across four genres: Argumentative, Narrative, Descriptive, and Expository, including 728 real-world prompts and a genre-specific scoring framework.

**Key Contributions:**

	1. Development of a multi-genre benchmark for Chinese essay writing
	2. Creation of a fine-grained, genre-specific scoring framework
	3. Comprehensive benchmarking of 15 LLMs across various essay genres.

**Result:** The evaluation framework allows for fine-grained assessment of essays, and the benchmarking of 15 large LLMs provides an analysis of their performance and limitations across genres.

**Limitations:** 

**Conclusion:** The study aims to enhance LLM-based evaluation of Chinese essays and encourages further research to improve essay generation in educational settings.

**Abstract:** Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings.

</details>


### [85] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)

*Ömer Tarik Özyilmaz, Matt Coler, Matias Valdenegro-Toro*

**Main category:** cs.CL

**Keywords:** Arabic ASR, Dialectal Speech, Whisper, Machine Learning, Speech Recognition

**Relevance Score:** 5

**TL;DR:** Fine-tuning OpenAI's Whisper on Arabic dialects improves ASR performance, finding that dialect-pooled models can match dialect-specific ones without significant performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of commercial Arabic ASR systems in understanding dialectal Arabic and improve their performance.

**Method:** Investigated the effect of fine-tuning Whisper on five Arabic dialects using Mozilla Common Voice and MASC dataset, assessing MSA training size and model types.

**Key Contributions:**

	1. Fine-tuning Whisper for dialectal Arabic ASR
	2. Insights on MSA pre-training benefits
	3. Comparison of dialect-pooled and dialect-specific models

**Result:** Small MSA fine-tuning data substantially improves smaller models, and dialect-pooled models perform comparably to dialect-specific ones.

**Limitations:** Limited benefits from MSA pre-training suggest minimal shared features between MSA and dialects.

**Conclusion:** Pooling dialectal data can effectively mitigate data scarcity issues in low-resource ASR systems with minimal performance loss.

**Abstract:** Although commercial Arabic automatic speech recognition (ASR) systems support Modern Standard Arabic (MSA), they struggle with dialectal speech. We investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA training size effects, benefits of pre-training on MSA data, and dialect-specific versus dialect-pooled models. We find that small amounts of MSA fine-tuning data yield substantial improvements for smaller models, matching larger non-fine-tuned models. While MSA pre-training shows minimal benefit, suggesting limited shared features between MSA and dialects, our dialect-pooled models perform comparably to dialect-specific ones. This indicates that pooling dialectal data, when properly balanced, can help address data scarcity in low-resource ASR without significant performance loss.

</details>


### [86] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)

*Manon Reusens, Bart Baesens, David Jurgens*

**Main category:** cs.CL

**Keywords:** Large Language Models, persona consistency, human-computer interaction, natural language processing, task evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces a standardized framework to analyze the consistency of personalized Large Language Models (LLMs) when assigned specific personas across various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how consistently LLMs adhere to predefined personas in different writing styles and tasks.

**Method:** A new framework is developed to evaluate persona consistency across four categories—happiness, occupation, personality, and political stance—through various task dimensions.

**Key Contributions:**

	1. Development of a standardized framework for evaluating persona consistency in LLMs
	2. Identification of factors influencing persona consistency
	3. Insights into how task structure impacts LLM responses

**Result:** Findings indicate that consistency in responses is influenced by factors such as the assigned persona, stereotypes, and model design, with consistency varying across tasks and increasing with more structured tasks.

**Limitations:** 

**Conclusion:** The study highlights the importance of understanding persona consistency in LLMs, providing insights into model behavior across different contexts.

**Abstract:** Personalized Large Language Models (LLMs) are increasingly used in diverse applications, where they are assigned a specific persona - such as a happy high school teacher - to guide their responses. While prior research has examined how well LLMs adhere to predefined personas in writing style, a comprehensive analysis of consistency across different personas and task types is lacking. In this paper, we introduce a new standardized framework to analyze consistency in persona-assigned LLMs. We define consistency as the extent to which a model maintains coherent responses when assigned the same persona across different tasks and runs. Our framework evaluates personas across four different categories (happiness, occupation, personality, and political stance) spanning multiple task dimensions (survey writing, essay generation, social media post generation, single turn, and multi-turn conversations). Our findings reveal that consistency is influenced by multiple factors, including the assigned persona, stereotypes, and model design choices. Consistency also varies across tasks, increasing with more structured tasks and additional context. All code is available on GitHub.

</details>


### [87] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)

*Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** large language models, learning capability, benchmark, evaluation metrics, AI research

**Relevance Score:** 9

**TL;DR:** EvaLearn is a benchmark for assessing the learning capability and efficiency of large language models (LLMs) using 648 challenging problems across six task types, with performance evaluation based on sequential problem-solving.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate LLMs not just on static performance but on their ability to learn and improve through experience, addressing an underexplored aspect of model potential.

**Method:** EvaLearn requires models to solve problems sequentially rather than in parallel, using five comprehensive automated metrics to assess learning capability and efficiency.

**Key Contributions:**

	1. Introduction of a novel benchmark (EvaLearn) for evaluating LLM learning capabilities.
	2. In-depth analysis of model performance based on sequential problem-solving.
	3. Availability of datasets and evaluation framework to promote future research.

**Result:** Benchmarking nine models showed diverse performance profiles, highlighting that models with strong static abilities do not necessarily excel in learning capability.

**Limitations:** 

**Conclusion:** EvaLearn provides a new evaluation perspective that may help bridge the gap between model performance and human capabilities, encouraging improved evaluation methodologies in LLM research.

**Abstract:** We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository.

</details>


### [88] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)

*Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Chain-of-Thought, language reasoning, dynamic training

**Relevance Score:** 9

**TL;DR:** This paper presents a dynamic ratio-based training pipeline for improving language reasoning in LLMs without extensive data annotations, achieving significant output reduction while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of efficient language reasoning with long outputs in Large Language Models (LLMs).

**Method:** A dynamic ratio-based training pipeline that balances weights between System-1 and System-2 data to eliminate redundant reasoning processes.

**Key Contributions:**

	1. Dynamic ratio-based training pipeline for LLMs
	2. Reduction of output tokens by nearly 40%
	3. Validation across multiple models and benchmarks

**Result:** The proposed method reduces the number of output tokens by nearly 40% while preserving reasoning accuracy across various models and benchmarks.

**Limitations:** 

**Conclusion:** The dynamic training pipeline effectively enhances language reasoning efficiency in LLMs without needing complex data annotations.

**Abstract:** Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.

</details>


### [89] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)

*Zhengdong Lu, Weikai Lu, Yiling Tao, Yun Dai, ZiXuan Chen, Huiping Zhuang, Cen Chen, Hao Peng, Ziqian Zeng*

**Main category:** cs.CL

**Keywords:** Large Language Models, planning, subtasks, parallel processing, travel planning

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel parallel planning paradigm (DPPM) for LLM-based agents to improve performance in planning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM planning methods struggle with heavy constraints and cascading errors.

**Method:** The DPPM approach decomposes complex tasks into subtasks, plans for each subtask in parallel, and merges them into a final plan. It also includes a verification and refinement module.

**Key Contributions:**

	1. Introduction of the DPPM paradigm
	2. Verification and refinement module for error correction
	3. Improvement of LLM planning tasks in travel planning

**Result:** DPPM significantly outperforms existing methods in travel planning tasks.

**Limitations:** 

**Conclusion:** The DPPM paradigm effectively addresses the limitations of current planning methods in LLMs, enhancing their utility in complex planning scenarios.

**Abstract:** Despite significant advances in Large Language Models (LLMs), planning tasks still present challenges for LLM-based agents. Existing planning methods face two key limitations: heavy constraints and cascading errors. To address these limitations, we propose a novel parallel planning paradigm, which Decomposes, Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM). Specifically, DPPM decomposes the complex task based on constraints into subtasks, generates the subplan for each subtask in parallel, and merges them into a global plan. In addition, our approach incorporates a verification and refinement module, enabling error correction and conflict resolution. Experimental results demonstrate that DPPM significantly outperforms existing methods in travel planning tasks.

</details>


### [90] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)

*Liang Yue, Yihong Tang, Kehai Chen, Jie Liu, Min Zhang*

**Main category:** cs.CL

**Keywords:** data augmentation, instruction fine-tuning, multi-agent interactions, NLP, reasoning abilities

**Relevance Score:** 7

**TL;DR:** MASTER is a novel data augmentation method for enhancing instruction-following capabilities in NLP models through simulated multi-agent interactions, creating the BOOST-QA dataset for fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Improving instruction-following capabilities and task-specific performance of NLP models by generating high-quality fine-tuning data.

**Method:** Introducing MASTER, a data augmentation approach using multiple agents with varied cognitive levels to create teacher-student interaction data.

**Key Contributions:**

	1. Development of MASTER for data augmentation using multi-agent interactions.
	2. Creation of BOOST-QA dataset from existing datasets for fine-tuning.
	3. Improvement in reasoning abilities of models on complex tasks.

**Result:** Models fine-tuned with BOOST-QA demonstrate excellent performance across benchmarks and significant enhancement in reasoning abilities for complex tasks.

**Limitations:** 

**Conclusion:** MASTER provides a promising avenue for generating high-quality training data, improving multitask generalization in NLP models.

**Abstract:** Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models' instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models' reasoning abilities in complex tasks, providing valuable insights for future research.

</details>


### [91] [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)

*Masaki Sakata, Sho Yokoi, Benjamin Heinzerling, Takumi Ito, Kentaro Inui*

**Main category:** cs.CL

**Keywords:** language models, named entities, cluster analysis, Transformer, entity representation

**Relevance Score:** 9

**TL;DR:** This paper analyzes how language models represent and distinguish named entities through cluster analysis of their internal representations, providing insights into their effectiveness and organization of entity-related information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore how language models identify and manage named entities within their internal representations, focusing on the challenges of ambiguity and variability in entity mentions.

**Method:** The authors introduce a framework that applies cluster analysis to evaluate the internal representations of five Transformer-based autoregressive models, measuring their ability to cluster identical entities and separate different entities using precision and recall metrics.

**Key Contributions:**

	1. Development of a framework for analyzing entity mention representations in language models
	2. Empirical assessment of Transformer-based models' effectiveness in distinguishing entities
	3. Insights into the dimensionality and structure of entity-related information in language models.

**Result:** The experiments demonstrate that the models achieve precision and recall scores between 0.66 and 0.9, indicating effective differentiation between entity mentions, with significant findings around low-dimensional linear subspace representation in early layers of LMs.

**Limitations:** 

**Conclusion:** The research reveals that internal representations in LMs effectively manage entity information, influencing predictive performance and paralleling real-world entity-centric knowledge structures.

**Abstract:** We analyze the extent to which internal representations of language models (LMs) identify and distinguish mentions of named entities, focusing on the many-to-many correspondence between entities and their mentions. We first formulate two problems of entity mentions -- ambiguity and variability -- and propose a framework analogous to clustering quality metrics. Specifically, we quantify through cluster analysis of LM internal representations the extent to which mentions of the same entity cluster together and mentions of different entities remain separated. Our experiments examine five Transformer-based autoregressive models, showing that they effectively identify and distinguish entities with metrics analogous to precision and recall ranging from 0.66 to 0.9. Further analysis reveals that entity-related information is compactly represented in a low-dimensional linear subspace at early LM layers. Additionally, we clarify how the characteristics of entity representations influence word prediction performance. These findings are interpreted through the lens of isomorphism between LM representations and entity-centric knowledge structures in the real world, providing insights into how LMs internally organize and use entity information.

</details>


### [92] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)

*Qihang Yan, Xinyu Zhang, Luming Guo, Qi Zhang, Feifan Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Chain-of-Thought, Traditional Chinese Medicine, Knowledge Retrieval

**Relevance Score:** 9

**TL;DR:** This paper presents RACE-Align, a framework that enhances Large Language Models in vertical domains by integrating retrieval-augmented knowledge and Chain-of-Thought reasoning for improved accuracy and interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs face challenges with accuracy, reasoning, and interpretability in specific domains, necessitating new alignment strategies that leverage knowledge and reasoning.

**Method:** RACE-Align utilizes a binary preference dataset with external knowledge and Chain-of-Thought reasoning to align LLMs through Direct Preference Optimization, supported by an AI-driven refinement pipeline.

**Key Contributions:**

	1. Introduction of RACE-Align framework
	2. Integration of retrieval mechanisms and Chain-of-Thought reasoning
	3. Demonstrated effectiveness in Traditional Chinese Medicine domain

**Result:** Experimental results demonstrate that RACE-Align outperforms the base model and a fine-tuned model (SFT) in Traditional Chinese Medicine, showing improvements in accuracy, reasoning depth, and interpretability.

**Limitations:** 

**Conclusion:** RACE-Align provides a viable solution to enhance the knowledge application and reasoning of LLMs in complex domains, addressing key limitations of traditional alignment methods.

**Abstract:** Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains.

</details>


### [93] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)

*Amaç Herdağdelen, Marco Baroni*

**Main category:** cs.CL

**Keywords:** gender bias, commonsense knowledge, natural language processing, Twitter, stereotypes

**Relevance Score:** 4

**TL;DR:** This paper explores gender biases in actions derived from text corpora and social media, leveraging commonsense knowledge to evaluate stereotypes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how gender stereotypes manifest in actions and to evaluate their alignment with common sense using natural language sources.

**Method:** A comparison of gender-specific actions extracted from Twitter and other text corpora against stereotypical expectations, employing gender heuristics derived from Web corpora.

**Key Contributions:**

	1. Developed a dataset of 441 commonsense actions with gender ratings
	2. Created a larger dataset of 21,442 actions rated automatically for gender bias
	3. Demonstrated feasibility of using social media data for augmenting commonsense knowledge with gender stereotypes.

**Result:** Achieved a Spearman correlation of 0.47 with a human gold standard and an ROC curve of 0.76 for predicting action polarity, indicating a good fit between corpus data and human judgment.

**Limitations:** The study relies on Twitter data which may not fully represent broader societal views; potential biases in user demographics are acknowledged.

**Conclusion:** Natural text data, particularly from Twitter, can effectively augment commonsense understanding of gender expectations in actions; datasets of both manually and automatically rated actions were developed.

**Abstract:** We extracted gender-specific actions from text corpora and Twitter, and compared them to stereotypical expectations of people. We used Open Mind Common Sense (OMCS), a commonsense knowledge repository, to focus on actions that are pertinent to common sense and daily life of humans. We use the gender information of Twitter users and Web-corpus-based pronoun/name gender heuristics to compute the gender bias of the actions. With high recall, we obtained a Spearman correlation of 0.47 between corpus-based predictions and a human gold standard, and an area under the ROC curve of 0.76 when predicting the polarity of the gold standard. We conclude that it is feasible to use natural text (and a Twitter-derived corpus in particular) in order to augment commonsense repositories with the stereotypical gender expectations of actions. We also present a dataset of 441 commonsense actions with human judges' ratings on whether the action is typically/slightly masculine/feminine (or neutral), and another larger dataset of 21,442 actions automatically rated by the methods we investigate in this study.

</details>


### [94] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)

*Aisha Alansari, Hamzah Luqman*

**Main category:** cs.CL

**Keywords:** offensive speech detection, multi-task learning, active learning, Arabic text, natural language processing

**Relevance Score:** 3

**TL;DR:** A novel framework combining multi-task learning and active learning to improve offensive speech detection in Arabic social media text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing issues of offensive speech on social media, particularly in Arabic, where complexities arise from dialectal variations and limited labeled data.

**Method:** The framework integrates multi-task learning with active learning, employing several uncertainty sampling techniques to select the most informative samples for model training, and dynamically adjusts task weights during training.

**Key Contributions:**

	1. Integration of multi-task learning with active learning for Arabic offensive speech detection
	2. Dynamic task weight adjustment for improved model performance
	3. Novel handling of emojis to enhance semantic understanding

**Result:** Achieved a state-of-the-art macro F1-score of 85.42% on the OSACT2022 dataset, outperforming existing methods while requiring fewer fine-tuning samples.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of combining MTL and active learning for accurate offensive language detection in Arabic, particularly in resource-constrained environments.

**Abstract:** The rapid growth of social media has amplified the spread of offensive, violent, and vulgar speech, which poses serious societal and cybersecurity concerns. Detecting such content in Arabic text is particularly complex due to limited labeled data, dialectal variations, and the language's inherent complexity. This paper proposes a novel framework that integrates multi-task learning (MTL) with active learning to enhance offensive speech detection in Arabic social media text. By jointly training on two auxiliary tasks, violent and vulgar speech, the model leverages shared representations to improve the detection accuracy of the offensive speech. Our approach dynamically adjusts task weights during training to balance the contribution of each task and optimize performance. To address the scarcity of labeled data, we employ an active learning strategy through several uncertainty sampling techniques to iteratively select the most informative samples for model training. We also introduce weighted emoji handling to better capture semantic cues. Experimental results on the OSACT2022 dataset show that the proposed framework achieves a state-of-the-art macro F1-score of 85.42%, outperforming existing methods while using significantly fewer fine-tuning samples. The findings of this study highlight the potential of integrating MTL with active learning for efficient and accurate offensive language detection in resource-constrained settings.

</details>


### [95] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)

*Stefano Bannò, Kate Knill, Mark Gales*

**Main category:** cs.CL

**Keywords:** vocabulary assessment, language proficiency, large language models, second language learning, contextual evaluation

**Relevance Score:** 7

**TL;DR:** This paper presents a novel method for assessing second language vocabulary proficiency using large language models (LLMs) and the English Vocabulary Profile (EVP).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to improve the assessment of vocabulary use in second language proficiency by utilizing contextual information rather than relying solely on part-of-speech tagging.

**Method:** The method combines LLMs with the EVP to evaluate vocabulary use in context, focusing on challenges like polysemy and contextual variation. The performance of LLMs is compared to a baseline that uses part-of-speech-based assessments.

**Key Contributions:**

	1. Introduces a fine-grained vocabulary assessment method using LLMs
	2. Links in-context vocabulary use with proficiency levels using EVP
	3. Demonstrates the advantages of LLMs over traditional PoS-based approaches in language assessment.

**Result:** LLMs demonstrate improved performance in assigning proficiency levels to words based on contextual use in L2 learner writing compared to traditional PoS-based methods.

**Limitations:** 

**Conclusion:** LLMs are effective tools for vocabulary assessment, providing better insights into both word-level and essay-level proficiency in second language writing.

**Abstract:** Vocabulary use is a fundamental aspect of second language (L2) proficiency. To date, its assessment by automated systems has typically examined the context-independent, or part-of-speech (PoS) related use of words. This paper introduces a novel approach to enable fine-grained vocabulary evaluation exploiting the precise use of words within a sentence. The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP is a standard lexical resource that enables in-context vocabulary use to be linked with proficiency level. We evaluate the ability of LLMs to assign proficiency levels to individual words as they appear in L2 learner writing, addressing key challenges such as polysemy, contextual variation, and multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to exploit additional semantic information that yields improved performance. We also explore correlations between word-level proficiency and essay-level proficiency. Finally, the approach is applied to examine the consistency of the EVP proficiency levels. Results show that LLMs are well-suited for the task of vocabulary assessment.

</details>


### [96] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)

*Sifan Li, Yujun Cai, Yiwei Wang*

**Main category:** cs.CL

**Keywords:** vision-language models, hidden content detection, semantic visual thinking

**Relevance Score:** 7

**TL;DR:** This paper introduces HC-Bench, a benchmark to evaluate vision-language models (VLMs) on their ability to detect hidden content in images, highlighting their reliance on high-level semantics and proposing a solution that significantly improves accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the ability of vision-language models in detecting hidden content in various images, revealing their limitations compared to human perception.

**Method:** The authors created HC-Bench, a benchmark consisting of 112 images with hidden elements, and tested leading VLMs, finding they performed poorly. They then proposed SemVink, which involves scaling images to low resolutions to improve accuracy.

**Key Contributions:**

	1. Development of HC-Bench for evaluating VLMs on hidden content detection
	2. Introduction of SemVink for improved image processing
	3. Identification of a critical flaw in VLMs related to semantic overreliance

**Result:** Leading VLMs achieved near-zero accuracy on the benchmark, but using SemVink resulted in over 99% accuracy by reducing visual noise.

**Limitations:** The study primarily focuses on a specific type of visual task and may not generalize across all visual perception challenges.

**Conclusion:** The study highlights a critical limitation in VLMs, suggesting a need for hybrid models that integrate multi-scale processing to enhance robustness in real-world applications.

**Abstract:** Vision-language models (VLMs) excel in semantic tasks but falter at a core human capability: detecting hidden content in optical illusions or AI-generated images through perceptual adjustments like zooming. We introduce HC-Bench, a benchmark of 112 images with hidden text, objects, and illusions, revealing that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to an overreliance on high-level semantics. Strikingly, we propose SemVink (Semantic Visual Thinking) by simply scaling images to low resolutions (32-128 pixels), which unlocks >99% accuracy by eliminating redundant visual noise. This exposes a critical architectural flaw: VLMs prioritize abstract reasoning over low-level visual operations crucial for real-world robustness. Our work urges a shift toward hybrid models integrating multi-scale processing, bridging the gap between computational vision and human cognition for applications in medical imaging, security, and beyond.

</details>


### [97] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)

*Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba*

**Main category:** cs.CL

**Keywords:** large language models, compression, orthogonal transformations, structured matrices, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper discusses a method for improving the compressibility of large language models (LLMs) by applying orthogonal transformations to weight matrices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of high computational and memory demands of LLMs while maintaining performance.

**Method:** The authors explore leveraging orthogonal transformations of weight matrices to enhance the compressibility of the parameters in structured matrix representations, facilitating effective model size reduction without fine-tuning.

**Key Contributions:**

	1. Introduction of orthogonal transformations to enhance weight compressibility in LLMs.
	2. Demonstration of the method's applicability across various structured matrix types.
	3. Release of code for practical implementation.

**Result:** The proposed approach shows that certain orthogonal transformations can significantly improve the compressibility of weights in structured matrices, making LLMs more efficient.

**Limitations:** The approach relies on the assumption that pretrained weights can be transformed without fine-tuning, which may not hold in all cases.

**Conclusion:** This work presents a feasible strategy for reducing parameter counts in LLMs, which could lead to more resource-efficient NLP models.

**Abstract:** Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at https://github.com/GrishKate/ProcrustesGPT

</details>


### [98] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)

*Yulin Dou, Jiangming Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Preference Elicitation, Question Generation, Trajectory Optimization, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** TO-GATE enhances question generation for LLMs by optimizing questioning trajectories, improving preference elicitation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches struggle with optimal dialogue trajectories and may generate irrelevant questions.

**Method:** The paper presents TO-GATE, which consists of a clarification resolver to generate optimal questioning trajectories and a summarizer for task-aligned final responses.

**Key Contributions:**

	1. Introduction of TO-GATE framework for trajectory optimization in question generation
	2. Enhanced task-alignment of final responses through a summarizer
	3. Demonstrated significant improvement over existing baseline methods in preference elicitation tasks

**Result:** TO-GATE significantly outperformed baseline methods with a 9.32% improvement in efficacy on preference elicitation tasks.

**Limitations:** 

**Conclusion:** The trajectory optimization improves the ability of models to produce effective questions and aligned summary responses.

**Abstract:** Large language models (LLMs) can effectively elicit human preferences through multi-turn dialogue. Complex tasks can be accomplished through iterative clarifying questions and final responses generated by an LLM acting as a questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches based on self-taught reasoning struggle to identify optimal dialogue trajectories and avoid irrelevant questions to the tasks. To address this limitation, we propose TO-GATE, a novel framework that enhances question generation through trajectory optimization, which consists of two key components: a clarification resolver that generates optimal questioning trajectories, and a summarizer that ensures task-aligned final responses. The trajectory optimization enables the model to produce effective elicitation questions and summary responses tailored to specific tasks. Experimental results demonstrate that TO-GATE significantly outperforms baseline methods, achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [99] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)

*Ludovic Moncla, Hédi Zeghidi*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, historical texts, generative models, transformer-based architectures, Natural Language Processing

**Relevance Score:** 4

**TL;DR:** This paper benchmarks various Named Entity Recognition (NER) approaches on historical texts, addressing challenges of non-standard language and nested entities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the unique challenges of Named Entity Recognition in historical texts due to non-standard language and archaic orthography, alongside complex nested and overlapping entities.

**Method:** The study benchmarks classical CRFs, spaCy models, transformer architectures like CamemBERT, and sequence-labeling models such as Flair on the GeoEDdA dataset, proposing both token-level and span-level classification.

**Key Contributions:**

	1. Benchmarking diverse NER approaches on a historical dataset
	2. Proposing a dual framing of NER as token-level and span-level classification
	3. Exploring few-shot prompting with generative models for low-resource NER

**Result:** Transformer-based models deliver state-of-the-art performance on nested entities, while few-shot prompting with generative models shows promise in low-resource scenarios.

**Limitations:** Challenges in representing the complexities of historical texts persist, and the effectiveness of generative models in various contexts needs further exploration.

**Conclusion:** The research underscores the need for hybrid approaches that integrate symbolic and neural methods to address the complexities of early modern French texts and historical NER challenges.

**Abstract:** Named Entity Recognition (NER) in historical texts presents unique challenges due to non-standardized language, archaic orthography, and nested or overlapping entities. This study benchmarks a diverse set of NER approaches, ranging from classical Conditional Random Fields (CRFs) and spaCy-based models to transformer-based architectures such as CamemBERT and sequence-labeling models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly annotated corpus derived from 18th-century French encyclopedias. We propose framing NER as both token-level and span-level classification to accommodate complex nested entity structures typical of historical documents. Additionally, we evaluate the emerging potential of few-shot prompting with generative language models for low-resource scenarios. Our results demonstrate that while transformer-based models achieve state-of-the-art performance, especially on nested entities, generative models offer promising alternatives when labeled data are scarce. The study highlights ongoing challenges in historical NER and suggests avenues for hybrid approaches combining symbolic and neural methods to better capture the intricacies of early modern French text.

</details>


### [100] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)

*Jintian Shao, Yiming Cheng*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Large Language Models, reasoning, sequence prediction, pattern matching

**Relevance Score:** 8

**TL;DR:** This paper critiques Chain-of-Thought (CoT) prompting in Large Language Models (LLMs), arguing it enhances performance by imitating reasoning rather than showcasing genuine reasoning capability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the widespread belief that Chain-of-Thought prompting leads to real abstract reasoning in Large Language Models.

**Method:** The authors theoretically analyze the function of Chain-of-Thought prompting, suggesting it serves as a structural constraint rather than a true reasoning enhancer.

**Key Contributions:**

	1. Theoretical critique of Chain-of-Thought prompting
	2. Reinterpretation of LLM performance enhancements as imitation of reasoning
	3. Insight into model behavior regarding sequence generation and pattern matching

**Result:** The analysis indicates that CoT prompting primarily helps models to generate sequences that mimic reasoning by leveraging their capacity for pattern recognition, rather than fostering abstract reasoning.

**Limitations:** 

**Conclusion:** The findings question the narrative of emergent reasoning in models prompted with Chain-of-Thought and highlight the importance of understanding the mechanisms behind these model outputs.

**Abstract:** Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes.

</details>


### [101] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)

*Verena Blaschke, Miriam Winkler, Constantin Förster, Gabriele Wenger-Glemser, Barbara Plank*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, German dialects, ASR models, speech translation, linguistic analysis

**Relevance Score:** 6

**TL;DR:** A dataset named Betthupferl is introduced to improve ASR models' performance on dialects in Germany, containing read speech samples and transcriptions from different dialects and Standard German.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of diverse German dialects in automatic speech recognition (ASR) research and to study model robustness towards dialectal variation.

**Method:** The study presents the Betthupferl dataset comprising four hours of speech data across three dialects and Standard German. It benchmarks several ASR models for their ability to translate speech into Standard German and analyzes linguistic differences in transcriptions.

**Key Contributions:**

	1. Introduction of the Betthupferl dataset for dialectal ASR research
	2. Benchmarking multilingual ASR models on dialects
	3. Qualitative analysis of ASR output differences between dialectal and Standard German.

**Result:** The analysis reveals that ASR outputs exhibit variations in resemblance to dialectal versus standardized transcriptions, with some models normalizing grammatical differences while others retain dialectal constructions.

**Limitations:** The dataset is limited to dialects spoken in Southeast Germany and may not encompass all German dialectical variations.

**Conclusion:** The findings highlight the need to incorporate dialectal variations in ASR research to improve model robustness and accuracy across diverse linguistic features.

**Abstract:** Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions.

</details>


### [102] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)

*Yusuke Sakai, Takumi Goto, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** grammatical error correction, error detection, language model, evaluation method, natural language processing

**Relevance Score:** 7

**TL;DR:** IMPARA-GED is a reference-free model for automatic grammatical error correction evaluation that outperforms existing methods by effectively integrating grammatical error detection capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of automatic grammatical error correction systems by providing a method that does not depend on reference sentences.

**Method:** IMPARA-GED leverages a pre-trained language model to assess the quality of grammatical error corrections while detecting errors simultaneously.

**Key Contributions:**

	1. Introduction of a reference-free evaluation method for GEC
	2. Enhancement of error detection through the use of a pre-trained language model
	3. High correlation with human evaluations, validating the proposed approach

**Result:** IMPARA-GED demonstrates the highest correlation with human evaluations on the SEEDA dataset, indicating its effectiveness in estimating correction quality.

**Limitations:** 

**Conclusion:** The findings suggest that IMPARA-GED offers a superior approach for evaluating GEC systems, potentially improving the development of such technologies.

**Abstract:** We propose IMPARA-GED, a novel reference-free automatic grammatical error correction (GEC) evaluation method with grammatical error detection (GED) capabilities. We focus on the quality estimator of IMPARA, an existing automatic GEC evaluation method, and construct that of IMPARA-GED using a pre-trained language model with enhanced GED capabilities. Experimental results on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods, demonstrate that IMPARA-GED achieves the highest correlation with human sentence-level evaluations.

</details>


### [103] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)

*Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu*

**Main category:** cs.CL

**Keywords:** cell type annotation, single-cell RNA sequencing, large language models, CellPuzzles, batch-level reasoning

**Relevance Score:** 8

**TL;DR:** The paper presents CellPuzzles, a task for cell type annotation in single-cell RNA sequencing data, and introduces Cell-o1, a large language model that outperforms previous models in this task by leveraging batch-level reasoning.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** To improve cell type annotation in single-cell RNA sequencing data by mimicking human expert workflows and addressing limitations of existing large language models.

**Method:** Introduction of the CellPuzzles benchmark for batch-level cell type assignment, using a state-of-the-art 7B LLM (Cell-o1) trained on specialized reasoning techniques and reinforced with batch-level rewards.

**Key Contributions:**

	1. Introduction of the CellPuzzles task for batch-level cell annotation.
	2. Development of Cell-o1, an LLM specifically fine-tuned for this task.
	3. Demonstration of improved accuracy in cell type prediction through novel training methods.

**Result:** Cell-o1 significantly outperforms the best baseline model (OpenAI's o1), achieving over 73% improvement in batch-level accuracy, with a final accuracy surpassing 19.0%.

**Limitations:** The model's performance is still limited compared to human experts, indicating room for further improvements and research.

**Conclusion:** Cell-o1 demonstrates enhanced performance in cell type annotation by incorporating batch-level reasoning, which is crucial for accurate and contextualized cell type assignments.

**Abstract:** Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.

</details>


### [104] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)

*Yijun Yang, Zeyu Huang, Wenhao Zhu, Zihan Qiu, Fei Yuan, Jeff Z. Pan, Ivan Titov*

**Main category:** cs.CL

**Keywords:** long-context models, evaluation framework, machine learning, natural language processing, bioinformatics

**Relevance Score:** 8

**TL;DR:** This paper introduces LongBioBench, a benchmark for evaluating long-context language models using artificially generated biographies to address limitations in existing evaluation frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current frameworks for evaluating long-context language models (LCLM), which suffer from complexities and contamination in real-world tasks and lack of coherence in synthetic tasks.

**Method:** The study introduces LongBioBench, which uses artificially generated biographies in a controlled environment to assess LCLMs across understanding, reasoning, and trustworthiness.

**Key Contributions:**

	1. Introduction of LongBioBench as a novel evaluation framework for LCLMs
	2. Demonstration of LCLMs' deficiencies in understanding, reasoning, and trustworthiness
	3. Analysis of design choices in existing benchmarks that undermine model evaluations

**Result:** Experimental evaluation of 18 LCLMs shows deficiencies in semantic understanding and reasoning, with models becoming less trustworthy as context length increases.

**Limitations:** 

**Conclusion:** LongBioBench offers a better trade-off between mirroring authentic language tasks and maintaining controllability, being more interpretable and configurable than previous synthetic benchmarks.

**Abstract:** Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: $\textit{seamless context}$, $\textit{controllable setting}$, and $\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of $\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$. Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.

</details>


### [105] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)

*Diogo A. P. Nunes, Eugénio Ribeiro*

**Main category:** cs.CL

**Keywords:** Depression, Information Retrieval, Foundation Models, Classification, Health Informatics

**Relevance Score:** 9

**TL;DR:** A team's approach to classifying depression symptoms using Information Retrieval metrics and foundation model fine-tuning, which led to successful outcomes in an eRisk competition.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To effectively identify symptoms of depression through the lens of Information Retrieval, leveraging a structured approach to categorize sentences based on relevance to specific symptoms in the Beck's Depression Inventory.

**Method:** The study framed the task as a binary classification problem, using labeled data to train and validate models. It involved techniques such as fine-tuning foundation models, sentence similarity analysis, LLM prompting, and ensemble methods, with a focus on minimizing class imbalance with synthetic data.

**Key Contributions:**

	1. Framework for sentence classification related to depression symptoms
	2. Use of foundation model fine-tuning with synthetic data
	3. Competitive performance in an official evaluation setting

**Result:** The approach demonstrated that fine-tuning foundation models provided the best results, particularly when synthetic data was utilized to address class imbalance. The team achieved the highest scores in the official evaluation against 16 other teams.

**Limitations:** The reliance on a labeled dataset may restrict generalizability; optimal methods varied by symptom, indicating a need for further exploration.

**Conclusion:** The results indicate that different symptoms may require tailored approaches, highlighting the importance of model adaptability and the effectiveness of ensemble techniques in achieving optimal performance.

**Abstract:** In this work, we describe our team's approach to eRisk's 2025 Task 1: Search for Symptoms of Depression. Given a set of sentences and the Beck's Depression Inventory - II (BDI) questionnaire, participants were tasked with submitting up to 1,000 sentences per depression symptom in the BDI, sorted by relevance. Participant submissions were evaluated according to standard Information Retrieval (IR) metrics, including Average Precision (AP) and R-Precision (R-PREC). The provided training data, however, consisted of sentences labeled as to whether a given sentence was relevant or not w.r.t. one of BDI's symptoms. Due to this labeling limitation, we framed our development as a binary classification task for each BDI symptom, and evaluated accordingly. To that end, we split the available labeled data into training and validation sets, and explored foundation model fine-tuning, sentence similarity, Large Language Model (LLM) prompting, and ensemble techniques. The validation results revealed that fine-tuning foundation models yielded the best performance, particularly when enhanced with synthetic data to mitigate class imbalance. We also observed that the optimal approach varied by symptom. Based on these insights, we devised five independent test runs, two of which used ensemble methods. These runs achieved the highest scores in the official IR evaluation, outperforming submissions from 16 other teams.

</details>


### [106] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)

*Aishwarya Sahoo, Jeevana Kruthi Karnuthala, Tushar Parmanand Budhwani, Pranchal Agarwal, Sankaran Vaidyanathan, Alexa Siu, Franck Dernoncourt, Jennifer Healey, Nedim Lipka, Ryan Rossi, Uttaran Bhattacharya, Branislav Kveton*

**Main category:** cs.CL

**Keywords:** LLM, evaluation, regression models, human feedback, quantitative judges

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for LLM-as-a-judge, where a large language model evaluates another LLM's output through quantitative regression models that align LLM evaluation scores with human assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for efficient evaluation methods for LLM outputs, especially when human feedback is scarce.

**Method:** The authors train regression models that use judgments from existing LLM judges to improve their alignment with human evaluators, demonstrating the method's effectiveness across different feedback types.

**Key Contributions:**

	1. Introduction of the LLM-as-a-judge framework
	2. Development of regression models for aligning LLM scores with human assessments
	3. Demonstration of effectiveness across various feedback types

**Result:** Empirical validation on four datasets indicates that the proposed quantitative judges enhance the predictive capabilities of existing judges more efficiently than traditional supervised learning methods.

**Limitations:** The empirical validation relies on specific datasets and may not generalize universally across all LLM applications.

**Conclusion:** Quantitative judges provide a scalable and statistically efficient solution for LLM evaluations, improving output assessments in situations with limited human feedback.

**Abstract:** LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.

</details>


### [107] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)

*Boyi Li, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang*

**Main category:** cs.CL

**Keywords:** Multi-Agent Systems, Large Language Models, Adaptive Graph Pruning, Collaboration Framework, Token Efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents Adaptive Graph Pruning (AGP), a novel framework that optimizes the number of agents and communication structures in multi-agent systems using Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current multi-agent systems are limited by fixed agent numbers and static communication structures, which hinder adaptability to complex tasks.

**Method:** The framework employs a two-stage training strategy to determine optimal agent quantities and communication topologies, optimizing both hard-pruning and soft-pruning in a dynamic manner.

**Key Contributions:**

	1. Proposed a task-adaptive multi-agent collaboration framework.
	2. Achieved state-of-the-art performance across multiple benchmarks.
	3. Reduced token consumption significantly while maintaining high performance.

**Result:** AGP achieves state-of-the-art results with a performance increase of 2.58% to 9.84% across six benchmarks, adapts efficiently to specific tasks, and reduces token consumption by over 90%.

**Limitations:** 

**Conclusion:** The AGP framework outperforms existing methods across multiple benchmarks with fewer training requirements, demonstrating its effectiveness in adaptive multi-agent collaboration.

**Abstract:** Large Language Model (LLM) based multi-agent systems have shown remarkable performance in various tasks, especially when enhanced through collaborative communication. However, current methods often rely on a fixed number of agents and static communication structures, limiting their ability to adapt to varying task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes agent quantity (hard-pruning) and communication topology (soft-pruning). Specifically, our method employs a two-stage training strategy: firstly, independently training soft-pruning networks for different agent quantities to determine optimal agent-quantity-specific complete graphs and positional masks across specific tasks; and then jointly optimizing hard-pruning and soft-pruning within a maximum complete graph to dynamically configure the number of agents and their communication topologies per task. Extensive experiments demonstrate that our approach is: (1) High-performing, achieving state-of-the-art results across six benchmarks and consistently generalizes across multiple mainstream LLM architectures, with a increase in performance of $2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized communication topologies tailored to specific tasks, with an extremely high performance in all three task categories (general reasoning, mathematical reasoning, and code generation); (3) Token-economical, having fewer training steps and token consumption at the same time, with a decrease in token consumption of $90\%+$; and (4) Training-efficient, achieving high performance with very few training steps compared with other methods. The performance will surpass the existing baselines after about ten steps of training under six benchmarks.

</details>


### [108] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)

*Zhixiong Su, Yichen Wang, Herun Wan, Zhaohan Zhang, Minnan Luo*

**Main category:** cs.CL

**Keywords:** machine-generated text detection, human-AI coauthorship, fine-grained detection, HACo-Det dataset, NLP

**Relevance Score:** 9

**TL;DR:** The paper addresses the gap in fine-grained machine-generated text (MGT) detection in human-AI coauthored works, proposing a new dataset and evaluating existing detection methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing MGT detection primarily focuses on binary, document-level scenarios, ignoring the complexities of texts jointly written by humans and LLMs.

**Method:** The authors introduce the HACo-Det dataset for human-AI coauthored texts and retrofit seven existing document-level detectors to perform word-level detection. They evaluate these on the proposed dataset for both word- and sentence-level detection.

**Key Contributions:**

	1. Introduction of the HACo-Det dataset for coauthored text detection
	2. Evaluation of existing document-level detectors in a fine-grained context
	3. Analysis of performance influences and identification of limitations in current methods.

**Result:** Detectors exhibit difficulty with fine-grained detection, achieving an average F1 score of 0.462, with fine-tuned models demonstrating superior performance.

**Limitations:** Current methods struggle with fine-grained detection and have notable performance issues, necessitating further research and development.

**Conclusion:** Fine-grained co-authored text detection remains a challenging problem, requiring further analysis of performance factors and improvements to existing methods.

**Abstract:** The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.

</details>


### [109] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)

*Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane*

**Main category:** cs.CL

**Keywords:** Federated Learning, Large Language Models, Benchmarking, Domain-specific LLMs, Privacy-preserving AI

**Relevance Score:** 9

**TL;DR:** Introduction of the FlowerTune LLM Leaderboard for benchmarking federated fine-tuning of LLMs across multiple domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address data scarcity and the challenge of developing domain-specific models by utilizing Federated Learning for fine-tuning LLMs without sharing sensitive data.

**Method:** A benchmarking suite called FlowerTune LLM Leaderboard is introduced, evaluating federated fine-tuning of LLMs in four domains: NLP, finance, medical, and coding, using community-driven datasets and specific evaluation metrics.

**Key Contributions:**

	1. First comprehensive benchmarking suite for federated fine-tuning of LLMs
	2. Comparison across diverse domains and LLMs
	3. Insights into model performance and domain adaptation strategies

**Result:** The study compares 26 pre-trained LLMs using various aggregation and fine-tuning strategies, offering insights into their performance and domain adaptability under federated settings.

**Limitations:** 

**Conclusion:** The findings establish a foundational framework for the creation of privacy-preserving, domain-specialized LLMs suitable for practical applications.

**Abstract:** Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications.

</details>


### [110] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)

*Dingwei Chen, Ziqiang Liu, Feiteng Fang, Chak Tou Leong, Shiwen Ni, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, factual consistency, hallucinations, training-free method, interpolation

**Relevance Score:** 9

**TL;DR:** This paper presents PLI, a method to reduce hallucinations in Large Language Models by enhancing factuality through premature layers interpolation, improving coherence without training overhead.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the critical issue of factual inconsistency in Large Language Models, known as 'hallucinations', which existing methods fail to effectively mitigate.

**Method:** The proposed method, PLI (Premature Layers Interpolation), is a training-free technique that enhances the output of LLMs by interpolating mathematical formations of premature layers with adjacent layers to extend information processing.

**Key Contributions:**

	1. Introduction of PLI as a training-free method to improve LLM factuality
	2. Demonstration of improved performance over existing methods in reducing hallucinations
	3. Insights into the relationship between layer interpolation and LLM internal mechanisms

**Result:** Experiments on four publicly available datasets show that PLI significantly reduces hallucinations and often outperforms existing baseline methods.

**Limitations:** 

**Conclusion:** The findings indicate that the success of PLI is linked to the internal mechanisms of LLMs, enhancing factual coherence without the resource demands of traditional fine-tuning methods.

**Abstract:** Large Language Models (LLMs) demonstrate remarkable capabilities in text understanding and generation. However, their tendency to produce factually inconsistent outputs, commonly referred to as ''hallucinations'', remains a critical challenge. Existing approaches, such as retrieval-based and inference-time correction methods, primarily address this issue at the input or output level, often overlooking the intrinsic information refinement process and the role of premature layers. Meanwhile, alignment- and fine-tuning-based methods are resource-intensive. In this paper, we propose PLI (Premature Layers Interpolation), a novel, training-free, and plug-and-play intervention designed to enhance factuality. PLI mitigates hallucinations by inserting premature layers formed through mathematical interpolation with adjacent layers. Inspired by stable diffusion and sampling steps, PLI extends the depth of information processing and transmission in LLMs, improving factual coherence. Experiments on four publicly available datasets demonstrate that PLI effectively reduces hallucinations while outperforming existing baselines in most cases. Further analysis suggests that the success of layer interpolation is closely linked to LLMs' internal mechanisms. To promote reproducibility, we will release our code and data upon acceptance.

</details>


### [111] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)

*Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka*

**Main category:** cs.CL

**Keywords:** full-duplex dialogue, spoken dialogue systems, Japanese language

**Relevance Score:** 6

**TL;DR:** This paper presents the first publicly available full-duplex spoken dialogue model in Japanese, demonstrating improved naturalness and meaningfulness over baseline models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Full-duplex spoken dialogue systems can replicate the bidirectional nature of human conversations, but research in Japanese dialogue systems has been limited.

**Method:** The model is built using a two-stage training process: pre-training on large-scale Japanese spoken dialogue data and fine-tuning on high-quality stereo spoken dialogue data, supplemented with synthetic data from a multi-stream text-to-speech system.

**Key Contributions:**

	1. First publicly available full-duplex spoken dialogue model in Japanese.
	2. Two-stage training process utilizing both real and synthetic data.
	3. Demonstrated performance improvement over existing Japanese models.

**Result:** The model shows superior performance compared to existing Japanese baseline models, in terms of both naturalness and meaningfulness of conversation.

**Limitations:** 

**Conclusion:** The development of this full-duplex spoken dialogue model is a significant step forward for conversational AI in the Japanese language.

**Abstract:** Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model's performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness.

</details>


### [112] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)

*Richard Armitage*

**Main category:** cs.CL

**Keywords:** Large language models, Primary care, Clinical practice, Artificial intelligence, Medical education

**Relevance Score:** 10

**TL;DR:** This paper evaluates the performance of leading large language models (LLMs) in answering primary care examination questions to support clinical practice.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of advanced LLMs in the context of primary care education and their potential role in clinical practice.

**Method:** Four leading LLMs (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) were assessed on 100 multiple choice questions from the MRCGP exam context, simulating responses expected from UK general practitioners.

**Key Contributions:**

	1. Demonstrated high efficacy of LLMs in answering primary care exam questions
	2. Establishes performance benchmarks for LLMs against human practitioners
	3. Advocated for the integration of reasoning models in clinical education and practice

**Result:** o3 achieved a score of 99%, while Claude Opus 4, Grok3, and Gemini 2.5 Pro scored 95.0% each, all far exceeding the average peer score of 73.0%.

**Limitations:** The assessment covered a specific range of examination questions and may not generalize across all aspects of primary care education.

**Conclusion:** The study indicates that advanced LLMs, particularly o3, can significantly support primary care delivery by outperforming average practitioners in clinical assessments.

**Abstract:** Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.   Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.   Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.   Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.

</details>


### [113] [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)

*Iuliia Zaitova, Badr M. Abdullah, Wei Xue, Dietrich Klakow, Bernd Möbius, Tania Avgustinova*

**Main category:** cs.CL

**Keywords:** idiom translation, speech-to-text, machine translation, large language models, semantic understanding

**Relevance Score:** 4

**TL;DR:** This paper investigates the challenges of translating idioms in speech-to-text (SLT) and text-to-text (MT) systems, highlighting significant performance drops in SLT systems compared to MT and large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the major challenge of idiom translation in modern machine translation systems, particularly in the context of speech-to-text applications where research is lacking.

**Method:** The study systematically evaluates idiom translation performance versus conventional news translation in both text-to-text and speech-to-text systems across German-English and Russian-English language pairs, comparing various state-of-the-art SLT and MT systems.

**Key Contributions:**

	1. Systematic evaluation of idiom translation in SLT vs MT systems
	2. Comparison of state-of-the-art SLT and MT systems across multiple language pairs
	3. Highlighting the necessity for improved idiomatic handling strategies in SLT architectures

**Result:** The evaluation shows that SLT systems exhibit a marked performance decline on idiomatic data, often resulting in literal translations, while MT systems and large language models perform comparatively better in managing idioms.

**Limitations:** 

**Conclusion:** The findings emphasize the necessity for idiom-specific strategies and enhancements in the internal representations of SLT architectures to improve translation accuracy.

**Abstract:** Idioms are defined as a group of words with a figurative meaning not deducible from their individual components. Although modern machine translation systems have made remarkable progress, translating idioms remains a major challenge, especially for speech-to-text systems, where research on this topic is notably sparse. In this paper, we systematically evaluate idiom translation as compared to conventional news translation in both text-to-text machine translation (MT) and speech-to-text translation (SLT) systems across two language pairs (German to English, Russian to English). We compare state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal that SLT systems experience a pronounced performance drop on idiomatic data, often reverting to literal translations even in higher layers, whereas MT systems and Large Language Models demonstrate better handling of idioms. These findings underscore the need for idiom-specific strategies and improved internal representations in SLT architectures.

</details>


### [114] [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)

*Đorđe Klisura, Astrid R Bernaga Torres, Anna Karen Gárate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios*

**Main category:** cs.CL

**Keywords:** privacy policies, dialectal biases, NLP, machine learning, human-centered design

**Relevance Score:** 4

**TL;DR:** A multi-agent framework is proposed to reduce dialectal biases in privacy policy question answering systems, enhancing accessibility for diverse populations.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities of privacy policies that limit accessibility and reduce performance disparities for speakers of non-standard English dialects in Privacy Policy QA systems.

**Method:** The proposed framework includes a Dialect Agent for translating queries to Standard American English while preserving dialectal intent and a Privacy Policy Agent for improving predictions with domain expertise, avoiding the need for retraining or dialect-specific fine-tuning.

**Key Contributions:**

	1. Introduction of a multi-agent framework to address dialect biases in QA systems
	2. Integration of a Dialect Agent and a Privacy Policy Agent for improved query handling
	3. Demonstration of substantial accuracy improvements without additional training

**Result:** The framework improved GPT-4o-mini's zero-shot accuracy significantly on PrivacyQA (from 0.394 to 0.601) and PolicyQA (from 0.352 to 0.464), outperforming or matching few-shot baselines without extra training data.

**Limitations:** 

**Conclusion:** The results indicate that structured collaboration in agent systems can effectively reduce dialectal biases, emphasizing the urgency for NLP systems to consider linguistic diversity for fair access to privacy information.

**Abstract:** Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.

</details>


### [115] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)

*Florian Ludwig, Torsten Zesch, Frederike Zufall*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hate Speech, Legal Abstraction, German Criminal Code, Natural Language Processing

**Relevance Score:** 3

**TL;DR:** The paper investigates conditioning Large Language Models (LLMs) at different levels of abstraction to assess legal problems, particularly hate speech classification in social media posts according to the German Criminal Code.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well LLMs can internalize various levels of legal abstraction and their ability to classify hate speech under legal definitions.

**Method:** The study involves conditioning LLMs using different levels of legal abstraction and testing their performance on classifying social media posts related to incitement to hatred as defined by German law.

**Key Contributions:**

	1. Investigation of LLMs' ability to handle legal abstraction levels
	2. Evaluation of hate speech classification based on German Criminal Law
	3. Insights into the limitations of LLMs in legal contexts

**Result:** The findings indicate a significant performance gap between LLMs and legal experts, with models lacking depth in understanding legal tasks and often producing contradictory answers, despite reasonable performance in identifying target groups.

**Limitations:** LLMs struggle with deep understanding of legal tasks, often leading to contradictions and hallucinations in their responses.

**Conclusion:** While some models showed potential in applying concrete legal knowledge, significant improvements are needed for LLMs to effectively assess hate speech and align with expert legal reasoning.

**Abstract:** The assessment of legal problems requires the consideration of a specific legal system and its levels of abstraction, from constitutional law to statutory law to case law. The extent to which Large Language Models (LLMs) internalize such legal systems is unknown. In this paper, we propose and investigate different approaches to condition LLMs at different levels of abstraction in legal systems. This paper examines different approaches to conditioning LLMs at multiple levels of abstraction in legal systems to detect potentially punishable hate speech. We focus on the task of classifying whether a specific social media posts falls under the criminal offense of incitement to hatred as prescribed by the German Criminal Code. The results show that there is still a significant performance gap between models and legal experts in the legal assessment of hate speech, regardless of the level of abstraction with which the models were conditioned. Our analysis revealed, that models conditioned on abstract legal knowledge lacked deep task understanding, often contradicting themselves and hallucinating answers, while models using concrete legal knowledge performed reasonably well in identifying relevant target groups, but struggled with classifying target conducts.

</details>


### [116] [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)

*Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, Graham Neubig*

**Main category:** cs.CL

**Keywords:** generalist agent, AI tools, performance benchmarking

**Relevance Score:** 6

**TL;DR:** OpenHands-Versa is a generalist AI agent that utilizes a minimal set of tools to achieve high performance across various tasks, outperforming specialized agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the minimal set of general tools required for high performance across diverse tasks, addressing limitations of specialized AI agents.

**Method:** The research involves the development and evaluation of OpenHands-Versa, a generalist agent, using benchmarks such as SWE-Bench Multimodal, GAIA, and The Agent Company.

**Key Contributions:**

	1. Introduction of OpenHands-Versa as a generalist agent
	2. Demonstration of superior performance over specialized agents
	3. Establishment of new performance benchmarks for generalist agents

**Result:** OpenHands-Versa outperformed specialized agents, showing absolute improvements in success rates of 9.1, 1.3, and 9.1 points across the three benchmarks.

**Limitations:** 

**Conclusion:** The study demonstrates the feasibility of creating a generalist agent like OpenHands-Versa, which provides a strong baseline for future research in AI agents.

**Abstract:** Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.

</details>


### [117] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)

*Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset*

**Main category:** cs.CL

**Keywords:** Spoken Language Understanding, Information Retrieval, Large Language Models, Prompt Engineering

**Relevance Score:** 9

**TL;DR:** This paper explores the enhancement of Spoken Language Understanding (SLU) tasks through example selection using Information Retrieval (IR) techniques, demonstrating improved performance with existing prompt lengths.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the reliability of SLU systems, which are critical in many applications and typically rely on large amounts of training data that may not be available for all tasks or languages.

**Method:** The authors propose leveraging Information Retrieval methods to select examples that enhance prompts used in SLU tasks, thus adapting instruction-tuned large language models for better performance in few-shot scenarios.

**Key Contributions:**

	1. Introduction of example selection using IR for SLU tasks
	2. Demonstration of improved SLU performance with enhanced prompts
	3. Validation of methods across various SLU benchmarks

**Result:** Experimental results indicate that the incorporation of lexical IR methods significantly improves SLU performance on various benchmarks without the need to increase the prompt length.

**Limitations:** 

**Conclusion:** The findings suggest that IR techniques can be effectively utilized for prompt enhancement in SLU tasks, providing a valuable tool for improving model performance in scenarios with limited data.

**Abstract:** Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.   In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length.

</details>


### [118] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)

*Jintian Shao, Yiming Cheng*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Long-Term Value Modeling, Policy Guidance, Theoretical Analysis

**Relevance Score:** 6

**TL;DR:** This paper analyzes the limitations of the VAPO framework in reinforcement learning for long-chain reasoning in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore fundamental limitations in the modeling of deep, long-term value and step-by-step policy guidance in reinforcement learning for complex reasoning tasks.

**Method:** The paper employs a theoretical analysis to examine issues related to credit assignment, value function capacity with abstracted goals, and the translation of global value signals into local policy improvements.

**Key Contributions:**

	1. Theoretical exploration of VAPO's limitations in modeling long-term value.
	2. Identification of challenges in credit assignment and policy guidance in RL for LLMs.
	3. Suggestions for future research directions to improve LLM agents.

**Result:** The analysis identifies inherent difficulties in utilizing VAPO for long-term reasoning tasks, emphasizing the challenges of dealing with sparse rewards.

**Limitations:** Focused on theoretical aspects and might not include empirical validation of the findings.

**Conclusion:** Understanding VAPO's limitations can inform future research directions for enhancing the robustness of LLM agents in reinforcement learning contexts.

**Abstract:** Reinforcement learning (RL) enhances large language models (LLMs) in complex, long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework, despite sophisticated mechanisms like Decoupled GAE, theoretically faces fundamental limitations in comprehensively modeling and leveraging deep, long-term value for fine-grained, step-by-step policy guidance in extended reasoning chains. We argue these limitations stem from inherent difficulties in credit assignment, value function representational capacity with temporally abstracted goals, and translating global value signals into local policy improvements, especially with sparse rewards. Our theoretical analysis examines these aspects to illuminate VAPO's boundaries in long-term value modeling, aiming to deepen understanding of current RL for advanced reasoning and suggest future research for more robust LLM agents.

</details>


### [119] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)

*Yuval Kansal, Shmuel Berman, Lydia Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, factual accuracy, education, language bias, Llama3.1

**Relevance Score:** 8

**TL;DR:** This paper evaluates the factual accuracy of the Llama3.1 family of Large Language Models (LLMs) in answering educational questions in multiple languages, highlighting issues of correctness and biases in language performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for factuality in educational tools that utilize LLMs, particularly as their adoption in education increases.

**Method:** The paper assesses the ability of the Llama3.1 models to answer factual questions relevant to middle and high school curricula across various languages.

**Key Contributions:**

	1. Evaluation of Llama3.1 models' factual accuracy in education-related contexts.
	2. Identification of language bias in LLMs against rare languages.
	3. Recommendations for improving LLM performance in educational applications.

**Result:** The evaluation revealed that the Llama3.1 models frequently produce inaccurate responses and show an increase in biases against less commonly spoken languages.

**Limitations:** The study does not cover all LLM models and focuses specifically on the Llama3.1 family, which may not represent all educational LLMs.

**Conclusion:** Ensuring the correctness of LLMs in educational contexts is crucial, as performance varies significantly across languages and is prone to biases.

**Abstract:** Factuality is a necessary precursor to useful educational tools. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. Despite their strong English capabilities, LLM performance in other languages is largely untested. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages.

</details>


### [120] [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)

*Katherine Thai, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** long-context language models, literary evidence retrieval, narrative reasoning

**Relevance Score:** 7

**TL;DR:** The paper examines how well long-context language models can understand literary fiction through literary evidence retrieval tasks, revealing gaps in performance between different model types.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the capabilities of long-context language models in understanding and analyzing literary texts, specifically in literary evidence retrieval.

**Method:** The authors repurposed the RELiC dataset and created a benchmark requiring LLMs to generate missing quotations from literary works, assessing their ability to perform narrative reasoning and textual examination.

**Key Contributions:**

	1. Introduction of a benchmark for literary evidence retrieval with LLMs
	2. Demonstration of model performance exceeding human experts
	3. Release of a curated literary dataset for further research

**Result:** Experiments indicate that Gemini Pro 2.5 outperforms human experts in accuracy (62.5% vs. 50%), while the best open-weight model only achieves 29.1% accuracy, revealing significant gaps in interpretive reasoning capabilities between model types.

**Limitations:** The paper notes that even strong models have limitations in understanding nuanced literary signals and face issues with overgeneration, suggesting areas for improvement.

**Conclusion:** The study highlights the proficiency of advanced models in literary tasks while emphasizing their ongoing struggles with nuanced signals and the issue of overgeneration, posing challenges for future literary analysis applications.

**Abstract:** How well do modern long-context language models understand literary fiction? We explore this question via the task of literary evidence retrieval, repurposing the RELiC dataset of That et al. (2022) to construct a benchmark where the entire text of a primary source (e.g., The Great Gatsby) is provided to an LLM alongside literary criticism with a missing quotation from that work. This setting, in which the model must generate the missing quotation, mirrors the human process of literary analysis by requiring models to perform both global narrative reasoning and close textual examination. We curate a high-quality subset of 292 examples through extensive filtering and human verification. Our experiments show that recent reasoning models, such as Gemini Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In contrast, the best open-weight model achieves only 29.1% accuracy, highlighting a wide gap in interpretive reasoning between open and closed-weight models. Despite their speed and apparent accuracy, even the strongest models struggle with nuanced literary signals and overgeneration, signaling open challenges for applying LLMs to literary analysis. We release our dataset and evaluation code to encourage future work in this direction.

</details>


### [121] [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)

*Jonas F. Lotz, António V. Lopes, Stephan Peitz, Hendra Setiawan, Leonardo Emili*

**Main category:** cs.CL

**Keywords:** tokenizer, language model, evaluation, multilingual, Zipf's law

**Relevance Score:** 7

**TL;DR:** This paper addresses the impact of tokenizer choice on language model performance, especially in multilingual settings, and proposes a framework for evaluating tokenizer quality without requiring extensive computational resources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluations of tokenizer quality and its effect on language model performance, particularly for multilingual tasks.

**Method:** The authors use smaller models to predict the impact of different tokenizers on larger models, developing new intrinsic metrics based on Zipf's law for evaluation.

**Key Contributions:**

	1. Demonstrated the use of smaller models to evaluate tokenizer impact on larger models
	2. Introduced new intrinsic metrics inspired by Zipf's law for tokenizer evaluation
	3. Developed a reliable framework for intrinsic tokenizer evaluations

**Result:** Tokenizer choice is shown to have a minor effect on English tasks, but significant performance variations occur in multilingual contexts. The proposed metrics correlate more with downstream performance than traditional methods.

**Limitations:** The evaluations primarily focus on multilingual settings and may not encompass all aspects of tokenizer performance across various tasks.

**Conclusion:** The framework presented allows for informed tokenizer selection, simplifying the process of tokenizer evaluation in the development of language models.

**Abstract:** The choice of tokenizer can profoundly impact language model performance, yet accessible and reliable evaluations of tokenizer quality remain an open challenge. Inspired by scaling consistency, we show that smaller models can accurately predict significant differences in tokenizer impact on larger models at a fraction of the compute cost. By systematically evaluating both English-centric and multilingual tokenizers, we find that tokenizer choice has negligible effects on tasks in English but results in consistent performance differences in multilingual settings. We propose new intrinsic tokenizer metrics inspired by Zipf's law that correlate more strongly with downstream performance than text compression when modeling unseen languages. By combining several metrics to capture multiple aspects of tokenizer behavior, we develop a reliable framework for intrinsic tokenizer evaluations. Our work offers a more efficient path to informed tokenizer selection in future language model development.

</details>


### [122] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)

*Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Natural Language Feedback, Large Language Models, Critique-Guided Learning, Policy Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents Critique-GRPO, an online reinforcement learning framework that integrates numerical and natural language feedback to improve the reasoning capabilities of large language models (LLMs), demonstrating superior performance on various reasoning tasks compared to existing methods.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by reinforcement learning with solely numerical feedback, including performance plateaus and failures, by integrating natural language critiques.

**Method:** Critique-GRPO combines natural language feedback with numerical rewards in a reinforcement learning framework, enabling simultaneous learning from initial responses and critiques while maintaining exploration.

**Key Contributions:**

	1. Introduction of Critique-GRPO framework that integrates natural language feedback into RL processes.
	2. Demonstration of enhanced performance of LLMs in reasoning tasks using critique-guided refinements.
	3. Insights that challenge assumptions about exploration efficiency in RL.

**Result:** Critique-GRPO outperforms both supervised and RL-based fine-tuning methods across multiple reasoning tasks, improving pass@1 scores by approximately 4.5% and 5% on Qwen2.5 and Qwen3 models, respectively.

**Limitations:** The paper does not address the scalability of the Critique-GRPO framework in larger, more complex settings or its applicability to other domains beyond reasoning tasks.

**Conclusion:** The proposed framework deepens our understanding of policy exploration in RL, emphasizing that higher entropy and longer responses do not always lead to more efficient learning.

**Abstract:** Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.

</details>


### [123] [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)

*Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang*

**Main category:** cs.CL

**Keywords:** Analog Circuit Synthesis, Reinforcement Learning, Large Language Models

**Relevance Score:** 4

**TL;DR:** Proposes AUTOCIRCUIT-RL, a reinforcement learning framework for automated analog circuit synthesis using Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses the challenges in analog circuit topology synthesis due to vast design search space and strict constraints.

**Method:** Utilizes a two-phase approach combining instruction tuning with a large language model and reinforcement learning refinement.

**Key Contributions:**

	1. Introduction of AUTOCIRCUIT-RL framework
	2. Improvement in valid circuit generation
	3. Significant reduction in duplicate generation rates

**Result:** AUTOCIRCUIT-RL generates ~12% more valid circuits, improves efficiency by ~14%, and reduces duplicate generation rates by ~38%.

**Limitations:** 

**Conclusion:** Demonstrates the effectiveness of the framework in scaling to complex circuits while maintaining efficiency and adherence to constraints.

**Abstract:** Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework's effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design.

</details>


### [124] [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)

*Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, code generation, unit testing

**Relevance Score:** 6

**TL;DR:** CURE is a reinforcement learning framework that co-evolves code and test generation, improving code generation accuracy through interaction-based learning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a reinforcement learning framework that enhances coding and unit test generation capabilities without relying on ground-truth supervision.

**Method:** CURE employs a unique reward design, allowing the unit tester to learn directly from the coder's mistakes, thus enabling scalable training.

**Key Contributions:**

	1. Introduction of the CURE framework for co-evolving coding and testing capabilities
	2. Development of ReasonFlux-Coder models with improved accuracy
	3. Extensibility of the model to downstream tasks and agentic coding.

**Result:** The ReasonFlux-Coder models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0%, outperforming existing models.

**Limitations:** 

**Conclusion:** CURE demonstrates effective improvements in code generation and unit testing, and can also serve as a reward model for reinforcement learning.

**Abstract:** We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE

</details>


### [125] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)

*Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, Jianfeng Gao*

**Main category:** cs.CL

**Keywords:** VLM, GUI agents, visual grounding, action execution, machine learning

**Relevance Score:** 8

**TL;DR:** The paper presents GUI-Actor, a method for coordinate-free visual grounding in VLM-powered GUI agents, demonstrating improved action region localization and performance on multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenges of localizing screen regions for action execution in VLM-powered GUI agents due to weak spatial-semantic alignment and limitations of existing text-based approaches raise the need for more effective grounding methods.

**Method:** GUI-Actor introduces an attention-based action head that aligns a <ACTOR> token with relevant visual patch tokens, allowing the model to propose action regions in one pass, supplemented by a grounding verifier to select the best candidates for action.

**Key Contributions:**

	1. Introduction of a coordinate-free method for GUI grounding
	2. An attention-based action head that enhances spatial alignment
	3. Demonstrated performance improvements over existing models.

**Result:** GUI-Actor outperforms previous state-of-the-art methods on various benchmarks, achieving higher scores and better generalization to new screen layouts compared to models like UI-TARS.

**Limitations:** 

**Conclusion:** GUI-Actor effectively enhances VLM capabilities for GUI grounding without sacrificing general-purpose performance, demonstrating that fine-tuning a small number of parameters can yield state-of-the-art results.

**Abstract:** One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.

</details>


### [126] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)

*Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam*

**Main category:** cs.CL

**Keywords:** knowledge graph, large language models, neuroscience, information retrieval, entity extraction

**Relevance Score:** 4

**TL;DR:** This work presents novel methods for constructing a knowledge graph from unlabeled neuroscience literature using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To accurately retrieve information and discover insights from the extensive and dispersed neuroscience literature, addressing the limitations of current retrieval methods.

**Method:** The proposed methods involve constructing a knowledge graph using LLM, neuroscience ontology, and text embeddings from a large-scale unlabeled neuroscience research corpus.

**Key Contributions:**

	1. Novel methods for knowledge graph construction from unlabeled data
	2. Integration of LLMs and neuroscience ontology
	3. Enhanced information retrieval through an entity-augmented algorithm

**Result:** The methods achieved an F1 score of 0.84 for entity extraction and improved answers to over 54% of questions posed using the knowledge extracted from the KG.

**Limitations:** 

**Conclusion:** The proposed approaches significantly enhance knowledge discovery from the neuroscience corpus, demonstrating the effectiveness of using LLMs for knowledge graph construction.

**Abstract:** Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


### [127] [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)

*Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel*

**Main category:** cs.CL

**Keywords:** tokenisation bias, language models, causal effect, subwords, ACL 2025

**Relevance Score:** 8

**TL;DR:** This study quantifies tokenisation bias in language models by analyzing how the inclusion of subwords in tokenisers affects the probability assigned to character-strings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the mismatch in probabilities assigned to character-strings by language models due to variations in tokenisation methods.

**Method:** The authors frame tokenisation bias as a causal effect and estimate it using regression discontinuity design, comparing similar subwords around a cutoff point in tokeniser vocabulary.

**Key Contributions:**

	1. Quantification of tokenisation bias in language models.
	2. Methodology using regression discontinuity design to estimate causal effects of tokenisation.
	3. Demonstration of significant output variations due to tokenisation choices.

**Result:** Tokenisation significantly affects model outputs, with the inclusion of a subword in a small model's vocabulary increasing the probability of its characters by up to 17 times, indicating the importance of tokeniser design in language modelling.

**Limitations:** 

**Conclusion:** The findings reveal that tokenisation is a critical choice in language model design due to its substantial impact on probability assignments.

**Abstract:** Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \textit{``hello''}). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.

</details>


### [128] [Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?](https://arxiv.org/abs/2110.13658)

*Arij Riabi, Benoît Sagot, Djamé Seddah*

**Main category:** cs.CL

**Keywords:** NLP, low-resource languages, character-based models, NArabizi, language variability

**Relevance Score:** 6

**TL;DR:** This paper examines the effectiveness of character-based language models for NLP tasks in low-resource North-African dialectal Arabic (NArabizi), finding comparable performance to multilingual models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenges of building NLP systems for low-resource languages, focusing specifically on North-African colloquial Arabic written in NArabizi.

**Method:** The authors compare the performance of a character-based language model on tasks like part-of-speech tagging and dependency parsing against monolingual and multilingual models, using 99k sentences of NArabizi and additional data from noisy French content.

**Key Contributions:**

	1. Analysis of character-based models in low-resource NLP
	2. Performance comparison with multilingual models
	3. Insights on NArabizi as a low-resource language

**Result:** The character-based model demonstrates performance close to that of larger multilingual and monolingual models, suggesting its viability for low-resource languages

**Limitations:** 

**Conclusion:** Character-based language models can provide effective solutions for NLP in low-resource and high variability language scenarios.

**Abstract:** Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high-resource languages. Building language models and, more generally, NLP systems for non-standardized and low-resource languages remains a challenging task. In this work, we focus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data displaying a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre-trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set-tings.

</details>


### [129] [TransAug: Translate as Augmentation for Sentence Embeddings](https://arxiv.org/abs/2111.00157)

*Jue Wang*

**Main category:** cs.CL

**Keywords:** Contrastive Learning, Data Augmentation, Sentence Embeddings, Natural Language Processing, Semantic Textual Similarity

**Relevance Score:** 6

**TL;DR:** This paper introduces TransAug, a method that utilizes translated sentence pairs for data augmentation in sentence embeddings, successfully achieving state-of-the-art results in semantic textual similarity tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations in current contrastive learning techniques for sentence embeddings, particularly the scarcity of large sentence datasets.

**Method:** TransAug involves distilling a Chinese encoder from a pre-trained English SimCSE encoder, followed by a cross-lingual contrastive learning approach where the Chinese encoder remains frozen while the English one is updated.

**Key Contributions:**

	1. Introduction of Translate as Augmentation (TransAug) for data augmentation in NLP.
	2. The novel two-stage paradigm for improving sentence embeddings.
	3. Setting new state-of-the-art results in semantic textual similarity and transfer tasks.

**Result:** The approach achieves state-of-the-art performance on standard semantic textual similarity tasks and excels in transfer tasks evaluated with SentEval.

**Limitations:** 

**Conclusion:** TransAug demonstrates that translated sentence pairs can effectively improve sentence embedding representations, suggesting new avenues for data augmentation in NLP.

**Abstract:** While contrastive learning greatly advances the representation of sentence embeddings, it is still limited by the size of the existing sentence datasets. In this paper, we present TransAug (Translate as Augmentation), which provide the first exploration of utilizing translated sentence pairs as data augmentation for text, and introduce a two-stage paradigm to advances the state-of-the-art sentence embeddings. Instead of adopting an encoder trained in other languages setting, we first distill a Chinese encoder from a SimCSE encoder (pretrained in English), so that their embeddings are close in semantic space, which can be regraded as implicit data augmentation. Then, we only update the English encoder via cross-lingual contrastive learning and frozen the distilled Chinese encoder. Our approach achieves a new state-of-art on standard semantic textual similarity (STS), outperforming both SimCSE and Sentence-T5, and the best performance in corresponding tracks on transfer tasks evaluated by SentEval.

</details>


### [130] [Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset](https://arxiv.org/abs/2303.12892)

*Thanh-Dung Le, Philippe Jouvet, Rita Noumeir*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, Transformer, clinical text classification, low-resource computation, French clinical narratives

**Relevance Score:** 8

**TL;DR:** The study introduces a Mixture of Expert Transformer model tailored for classifying small-scale French clinical texts, optimized for low-resource settings, achieving competitive performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by existing Transformer models in classifying small clinical texts with limited computational resources.

**Method:** A customized Mixture of Expert (MoE) Transformer model was developed and tested on small-scale clinical texts at CHU Sainte-Justine Hospital.

**Key Contributions:**

	1. Customized MoE-Transformer model for clinical text classification
	2. Significant speed advantage in training (190x faster)
	3. Demonstrated effectiveness with limited resources and data in clinical settings

**Result:** The MoE-Transformer achieved 87% accuracy, 87% precision, 85% recall, and 86% F1-score, outperforming DistillBERT and others, while being trainable at 190 times the speed of biomedical pre-trained models.

**Limitations:** Limited performance compared to advanced biomedical pre-trained BERT models; some challenges in generalization and accuracy remain.

**Conclusion:** The MoE-Transformer provides a viable alternative for classifying clinical texts in settings with limited data and computational resources, despite some limitations in performance compared to advanced biomedical models.

**Abstract:** Transformer-based models have shown outstanding results in natural language processing but face challenges in applications like classifying small-scale clinical texts, especially with constrained computational resources. This study presents a customized Mixture of Expert (MoE) Transformer models for classifying small-scale French clinical texts at CHU Sainte-Justine Hospital. The MoE-Transformer addresses the dual challenges of effective training with limited data and low-resource computation suitable for in-house hospital use. Despite the success of biomedical pre-trained models such as CamemBERT-bio, DrBERT, and AliBERT, their high computational demands make them impractical for many clinical settings. Our MoE-Transformer model not only outperforms DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset but also achieves impressive results: an accuracy of 87\%, precision of 87\%, recall of 85\%, and F1-score of 86\%. While the MoE-Transformer does not surpass the performance of biomedical pre-trained BERT models, it can be trained at least 190 times faster, offering a viable alternative for settings with limited data and computational resources. Although the MoE-Transformer addresses challenges of generalization gaps and sharp minima, demonstrating some limitations for efficient and accurate clinical text classification, this model still represents a significant advancement in the field. It is particularly valuable for classifying small French clinical narratives within the privacy and constraints of hospital-based computational resources.

</details>


### [131] [UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities](https://arxiv.org/abs/2403.04247)

*Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Hui Wang*

**Main category:** cs.CL

**Keywords:** Entity Set Expansion, Ultra-fine-grained Entities, Negative Seed Entities, Contrastive Learning, Chain-of-Thought Reasoning

**Relevance Score:** 5

**TL;DR:** The paper introduces a novel approach to Entity Set Expansion (ESE) by incorporating negative seed entities, addressing challenges in ultra-fine-grained ESE, and presenting a large-scale dataset called UltraWiki.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional ESE methods struggle with ultra-fine-grained classes due to reliance solely on positive seed entities, causing ambiguity and insufficient unwanted semantics representation.

**Method:** The authors propose two frameworks, RetExpan (retrieval-based) and GenExpan (generation-based), and introduce contrastive learning and chain-of-thought reasoning to enhance understanding of ultra-fine-grained entities' semantics while utilizing a new dataset, UltraWiki.

**Key Contributions:**

	1. Introduction of negative seed entities for ESE
	2. Creation of UltraWiki dataset for Ultra-ESE
	3. Development of RetExpan and GenExpan frameworks for ESE tasks

**Result:** The proposed methods significantly improve model performance in Ultra-ESE and validate the importance of negative seed entities in eliminating ambiguity and defining unwanted semantics.

**Limitations:** While the proposed methods show improvement, the authors note that there remain opportunities for enhancing Ultra-ESE models further.

**Conclusion:** The paper presents effective strategies for Ultra-ESE but acknowledges that there is still considerable potential for further improvements.

**Abstract:** Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as the given set of seed entities. Traditional methods solely relied on positive seed entities to represent the target fine-grained semantic class, rendering them tough to represent ultra-fine-grained semantic classes. Specifically, merely relying on positive seed entities leads to two inherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods struggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this issue, we first introduce negative seed entities in the inputs, which jointly describe the ultra-fine-grained semantic class with positive seed entities. Negative seed entities eliminate the semantic ambiguity by providing a contrast between positive and negative attributes. Meanwhile, it provides a straightforward way to express ``unwanted''. To assess model performance in Ultra-ESE and facilitate further research, we also constructed UltraWiki, the first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973 entities and 394,097 sentences, alongside 236 ultra-fine-grained semantic classes, where each class is represented with 3-5 positive and negative seed entities. Moreover, a retrieval-based framework RetExpan and a generation-based framework GenExpan are proposed to provide powerful baselines for Ultra-ESE. Additionally, we devised two strategies to enhance models' comprehension of ultra-fine-grained entities' semantics: contrastive learning and chain-of-thought reasoning. Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a large space for improvement in Ultra-ESE.

</details>


### [132] [Revealing the Parallel Multilingual Learning within Large Language Models](https://arxiv.org/abs/2403.09073)

*Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, In-context learning, Neural activation, Language processing, Synaptic pruning

**Relevance Score:** 9

**TL;DR:** This study investigates the in-context learning capabilities of multilingual large language models (LLMs) using parallel input in multiple languages, revealing significant enhancements in comprehension and neuron activation patterns.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how multilingual input affects the comprehension abilities of LLMs and to demonstrate the potential of in-context learning (ICL) using multiple languages.

**Method:** The study involves designing extensive experiments that utilize 8 typical datasets, 7 languages, and 8 state-of-the-art multilingual LLMs to test the effectiveness of Parallel Input in Multiple Languages (PiM).

**Key Contributions:**

	1. Introduces the concept of Parallel Input in Multiple Languages (PiM) for enhancing LLM comprehension.
	2. Demonstrates the counterintuitive finding that PiM can lead to more precise neuron activation rather than increased activation.
	3. Aligns findings with neuroscience principles, providing insight into neural efficiency in LLMs.

**Result:** Experiments show that using PiM surpasses conventional ICL, with the incorporation of more languages improving performance even when combined with less effective translations. The research also uncovers a phenomenon where PiM activates fewer neurons for more precise usage, akin to synaptic pruning in neuroscience.

**Limitations:** 

**Conclusion:** The findings suggest that multilingual inputs not only enhance comprehension in LLMs but also refine their neural activation patterns, indicating a more efficient processing mechanism.

**Abstract:** In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.

</details>


### [133] [Checkpoint Merging via Bayesian Optimization in LLM Pretraining](https://arxiv.org/abs/2403.19390)

*Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui*

**Main category:** cs.CL

**Keywords:** large language models, checkpoint merging, Bayesian optimization

**Relevance Score:** 8

**TL;DR:** This paper proposes a method for checkpoint merging in pretraining large language models to reduce computational costs while improving performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing demand for resources in training large language models like GPT-4 and Gemini presents challenges related to computational and environmental costs.

**Method:** The proposed method leverages LLM checkpoints with shared training trajectories and utilizes Bayesian optimization to explore merging weights effectively.

**Key Contributions:**

	1. Proposed checkpoint merging method for pretraining LLMs
	2. Effective exploration of merging weights using Bayesian optimization
	3. Demonstrated improved generalization capabilities across domains

**Result:** Experiments show that the methodology not only enhances pretraining efficiency but also maintains strong generalization across different domains.

**Limitations:** 

**Conclusion:** Checkpoint merging presents an opportunity to gain substantial benefits in pretraining without significant computational overhead.

**Abstract:** The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.

</details>


### [134] [LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://arxiv.org/abs/2405.06705)

*Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li*

**Main category:** cs.CL

**Keywords:** self-correction, large language models, prompting strategies, mathematical reasoning, educational theory

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel prompting strategy called Pedagogical Chain-of-Thought (PedCoT) to enhance self-correction in LLMs by improving the identification of reasoning mistakes, especially in mathematics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs struggle to identify reasoning mistakes reliably using simplistic prompting strategies, leading to hallucinations in responses.

**Method:** The PedCoT approach incorporates pedagogical principles for prompt design, a two-stage interaction process, and ground prompts based on educational theories to guide LLMs in identifying mistakes.

**Key Contributions:**

	1. Introduction of the Pedagogical Chain-of-Thought (PedCoT) prompting strategy.
	2. Demonstration of improved mistake detection performance on math problems through a novel prompting approach.
	3. Theoretical foundation based on educational principles to inform prompting strategies.

**Result:** The experiments conducted on two public datasets show that PedCoT significantly outperforms existing baselines in identifying mathematical reasoning mistakes.

**Limitations:** 

**Conclusion:** The study highlights the critical role of educational theory in enhancing prompting strategies for LLMs to tackle complex tasks effectively.

**Abstract:** Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.

</details>


### [135] [SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale](https://arxiv.org/abs/2406.06907)

*Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu*

**Main category:** cs.CL

**Keywords:** Sign Language, Self-supervised Learning, Video Processing, Translation, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper proposes a self-supervised approach to learning representations of sign language that focuses on relevant visual attributes like the face, hands, and body pose, achieving competitive performance with significantly lower compute resources.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve representation learning in sign language video processing while focusing on the most relevant visual features, avoiding reliance on inconsistent pose estimation models.

**Method:** The proposed method processes individual frames from signing videos to learn handshapes and facial expressions in a self-supervised manner, improving efficiency over prior techniques.

**Key Contributions:**

	1. Self-supervised learning approach for handshapes and expressions
	2. Focus on relevant visual components (face, hands, body pose)
	3. High efficiency with competitive translation performance

**Result:** The approach reaches translation performance comparable to state-of-the-art methods on the How2Sign dataset while using less than 3% of the compute resources.

**Limitations:** 

**Conclusion:** This self-supervised method provides an efficient alternative for sign language to written language translation by focusing on critical aspects of the signer's video.

**Abstract:** A persistent challenge in sign language video processing, including the task of sign to written language translation, is how we learn representations of sign language in an effective and efficient way that preserves the important attributes of these languages, while remaining invariant to irrelevant visual differences. Informed by the nature and linguistics of signed languages, our proposed method focuses on just the most relevant parts in a signing video: the face, hands and body pose of the signer. However, instead of fully relying on pose estimation from off-the-shelf pose tracking models, which have inconsistent performance for hands and faces, we propose to learn a representation of the complex handshapes and facial expressions of sign languages in a self-supervised fashion. Our approach is based on learning from individual frames (rather than video sequences) and is therefore much more efficient than prior work on sign language pre-training. Compared to a recent model that established a new state of the art in sign language translation on the How2Sign dataset, our approach yields similar translation performance, using less than 3\% of the compute.

</details>


### [136] [Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems](https://arxiv.org/abs/2406.14545)

*Đorđe Klisura, Anthony Rios*

**Main category:** cs.CL

**Keywords:** text-to-SQL, schema inference attacks, GPT-4, database security, natural language processing

**Relevance Score:** 6

**TL;DR:** This paper presents a zero-knowledge framework to uncover database schema from text-to-SQL models, demonstrating significant vulnerabilities in such systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Text-to-SQL systems face vulnerabilities due to their dependence on database schema information, leading to schema inference attacks.

**Method:** We systematically probe text-to-SQL models with specially crafted questions and utilize a surrogate GPT-4 model to interpret outputs, uncovering hidden schema elements.

**Key Contributions:**

	1. Introduced a novel zero-knowledge framework for schema reconstruction
	2. Achieved high accuracy in identifying hidden schema elements
	3. Outlined a protection mechanism with noted limitations

**Result:** Our method achieves high accuracy in reconstructing table names with F1 scores of up to .99 for generative models and .78 for fine-tuned models, revealing serious risks of schema leakage.

**Limitations:** The proposed protection mechanism has limitations in effectively mitigating schema inference attacks.

**Conclusion:** While we propose a protection mechanism for generative models, our analysis shows its limitations in effectively mitigating these attacks.

**Abstract:** Text-to-SQL systems empower users to interact with databases using natural language, automatically translating queries into executable SQL code. However, their reliance on database schema information for SQL generation exposes them to significant security vulnerabilities, particularly schema inference attacks that can lead to unauthorized data access or manipulation. In this paper, we introduce a novel zero-knowledge framework for reconstructing the underlying database schema of text-to-SQL models without any prior knowledge of the database. Our approach systematically probes text-to-SQL models with specially crafted questions and leverages a surrogate GPT-4 model to interpret the outputs, effectively uncovering hidden schema elements -- including tables, columns, and data types. We demonstrate that our method achieves high accuracy in reconstructing table names, with F1 scores of up to .99 for generative models and .78 for fine-tuned models, underscoring the severity of schema leakage risks. We also show that our attack can steal prompt information in non-text-to-SQL models. Furthermore, we propose a simple protection mechanism for generative models and empirically show its limitations in mitigating these attacks.

</details>


### [137] [Free-text Rationale Generation under Readability Level Control](https://arxiv.org/abs/2407.01384)

*Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov*

**Main category:** cs.CL

**Keywords:** large language models, rationale generation, readability levels, explanation, human-annotated quality

**Relevance Score:** 8

**TL;DR:** This study examines how large language models generate explanations at different readability levels and their effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the readability level of generated rationales affects their effectiveness and user perception.

**Method:** A perturbation test was conducted where large language models were instructed to generate rationales aimed at specific readability levels, with a focus on their adaptability and quality correlation.

**Key Contributions:**

	1. Investigates LLM rationale generation across varying readability levels.
	2. Identifies misalignment between generated rationales' complexity and traditional readability scores.
	3. Highlights human perceptions of the effectiveness of different readability levels in rationales.

**Result:** The study found that explanations are adaptable to readability prompts, but the differentiation in complexity levels does not entirely align with traditional readability metrics. Generated rationales exhibited medium complexity, correlating with quality metrics, and received favorable feedback from human annotators, particularly at high-school readability levels.

**Limitations:** Results may be limited by the subjective nature of human annotation and the specific contexts of readability assessment.

**Conclusion:** Rationales generated by LLMs are versatile across different readability levels, yet the expected complexity does not fully comply with traditional metrics; high-school readability is often favored.

**Abstract:** Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform rationale generation under the effects of readability level control, i.e., being prompted for an explanation targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, though the observed distinction between readability levels does not fully match the defined complexity scores according to traditional readability metrics. Furthermore, the generated rationales tend to feature medium level complexity, which correlates with the measured quality using automatic metrics. Finally, our human annotators confirm a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored.

</details>


### [138] [UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization](https://arxiv.org/abs/2407.03525)

*Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral*

**Main category:** cs.CL

**Keywords:** time-sensitive question-answering, temporal reasoning, large language models

**Relevance Score:** 7

**TL;DR:** UnSeenTimeQA is a new benchmark for time-sensitive question-answering designed to enhance LLMs' temporal reasoning without relying on real-world factual knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to address the limitations of existing time-sensitive question-answering benchmarks by introducing a contamination-free approach that avoids the use of real-world, web-searchable queries.

**Method:** The authors created a data generation framework that synthetically generates event scenarios and designs three types of time-sensitive questions for evaluation.

**Key Contributions:**

	1. Introduction of UnSeenTimeQA benchmark for time-sensitive question-answering
	2. Data generation framework for creating synthetic event scenarios
	3. Evaluation highlighting the limitations of LLMs in temporal reasoning tasks

**Result:** Evaluation of five large language models shows mixed performance, excelling on simpler tasks but struggling with complex temporal reasoning over long-range dependencies and parallel events.

**Limitations:** LLMs perform inadequately on complex temporal reasoning tasks compared to real-world benchmarks.

**Conclusion:** The findings indicate that while there are capabilities in handling simpler temporal reasoning tasks, LLMs still struggle with complex scenarios in the TSQA domain.

**Abstract:** This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. Our data generation framework enables on-demand generation of new samples, mitigating the risk of data leakage. We designed three types of time-sensitive questions to test LLMs' temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real world fact-based TSQA. Error analysis indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel events.

</details>


### [139] [Localizing and Mitigating Errors in Long-form Question Answering](https://arxiv.org/abs/2407.11930)

*Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** long-form question answering, hallucinations, feedback model, dataset, error refinement

**Relevance Score:** 9

**TL;DR:** HaluQuestQA introduces a dataset for analyzing hallucinations in long-form question answering and proposes a method to improve answer quality by modeling error feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Long-form question answering can lead to hallucinations and factual inaccuracies, making evaluation difficult and impacting user comprehension.

**Method:** The authors developed HaluQuestQA, a dataset with annotated error spans, and trained a model to predict these error spans. They also proposed a prompt-based refinement approach to enhance answer quality.

**Key Contributions:**

	1. Introduction of HaluQuestQA, the first hallucination dataset for LFQA with localized error annotations.
	2. Development of an automatic feedback model for predicting error spans.
	3. Proposal of a prompt-based refinement method that improves answer quality in LFQA.

**Result:** The analysis revealed that long-form answers often lack comprehensiveness and relevancy. The refined answers showed a preference of 84% among humans over baseline answers, indicating significant quality improvement.

**Limitations:** The dataset is limited to specific error types and may not cover all hallucination forms encountered in LFQA.

**Conclusion:** The proposed Error-informed refinement approach successfully reduces errors in LFQA responses, leading to better quality answers preferred by human evaluators.

**Abstract:** Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves answer quality across multiple models. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers.

</details>


### [140] [A Survey on Employing Large Language Models for Text-to-SQL Tasks](https://arxiv.org/abs/2407.15186)

*Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Text-to-SQL, prompt engineering, finetuning, benchmark analysis

**Relevance Score:** 8

**TL;DR:** This paper reviews the development of LLM-based Text-to-SQL methods, outlining benchmarks, methodologies, and future challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of the advancements and methods in LLM-based Text-to-SQL techniques, highlighting their applications and assessment metrics.

**Method:** The paper utilizes a comparative analysis of the two main approaches in Text-to-SQL: prompt engineering and finetuning, supplemented by a taxonomy of methods and evaluation on various datasets.

**Key Contributions:**

	1. Comprehensive taxonomy of prompt engineering and finetuning methods in Text-to-SQL
	2. Analysis of benchmark datasets and evaluation metrics
	3. Discussion of challenges and future research directions in LLM-based Text-to-SQL

**Result:** An analysis of classic benchmarks, evaluation metrics, and characteristics of different models in the Text-to-SQL space, informed by practical insights.

**Limitations:** 

**Conclusion:** The paper concludes by outlining existing challenges in LLM-based Text-to-SQL and proposing future research directions in the area.

**Abstract:** With the development of the Large Language Models (LLMs), a large range of LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a comprehensive review of LLM-based Text2SQL studies. We first enumerate classic benchmarks and evaluation metrics. For the two mainstream methods, prompt engineering and finetuning, we introduce a comprehensive taxonomy and offer practical insights into each subcategory. We present an overall analysis of the above methods and various models evaluated on well-known datasets and extract some characteristics. Finally, we discuss the challenges and future directions in this field.

</details>


### [141] [Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes](https://arxiv.org/abs/2407.21050)

*Yao-Shun Chuang, Chun-Teh Lee, Oluwabunmi Tokede, Guo-Hao Lin, Ryan Brandon, Trung Duong Tran, Xiaoqian Jiang, Muhammad F. Walji*

**Main category:** cs.CL

**Keywords:** AI, NLP, Dental Diagnostics, RoBERTa, Healthcare

**Relevance Score:** 9

**TL;DR:** This study explores using AI and NLP to improve the extraction of diagnostic information from unstructured dental texts, demonstrating high accuracy with a fine-tuned RoBERTa model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the problem of incomplete or missing structured data in dental records due to the complexity of modern diagnostic classifications.

**Method:** Advanced AI and NLP techniques were employed, utilizing GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model, which was evaluated on clinical notes for extracting diagnostic accuracy.

**Key Contributions:**

	1. Utilization of GPT-4 for generating synthetic training data
	2. Improved diagnostic extraction accuracy in dentistry
	3. Potential broader healthcare applications for AI and NLP methods

**Result:** The model achieved diagnostic extraction accuracy of 0.99 for Site 1 and 0.98 for Site 2, with perfect scores in subtype categories in Site 2, thus confirming the method's effectiveness.

**Limitations:** 

**Conclusion:** The innovative AI and NLP methods improved the extraction of complex dental diagnostics and have the potential for wider applications across healthcare, enhancing patient care quality.

**Abstract:** This research addresses the issue of missing structured data in dental records by extracting diagnostic information from unstructured text. The updated periodontology classification system's complexity has increased incomplete or missing structured diagnoses. To tackle this, we use advanced AI and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model. This significantly enhances the model's ability to understand medical and dental language. We evaluated the model using 120 randomly selected clinical notes from two datasets, demonstrating its improved diagnostic extraction accuracy. The results showed high accuracy in diagnosing periodontal status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In the subtype category, Site 2 achieved perfect scores, outperforming Site 1. This method enhances extraction accuracy and broadens its use across dental contexts. The study underscores AI and NLP's transformative impact on healthcare delivery and management. Integrating AI and NLP technologies enhances documentation and simplifies administrative tasks by precisely extracting complex clinical information. This approach effectively addresses challenges in dental diagnostics. Using synthetic training data from LLMs optimizes the training process, improving accuracy and efficiency in identifying periodontal diagnoses from clinical notes. This innovative method holds promise for broader healthcare applications, potentially improving patient care quality.

</details>


### [142] [Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused](https://arxiv.org/abs/2408.08769)

*Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Xiping Hu, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, contrastive decoding, hallucinations, truthfulness, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper introduces LOL, a novel contrastive decoding framework that reduces hallucinations in Large Language Models by integrating information from lower layers and adding a truthfulness module.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in LLMs, which lead to inaccurate outputs and disrupt the original model's performance.

**Method:** The LOL framework employs multi-layer fusion during contrastive decoding by incorporating information from lower layers along with a truthfulness refocused module.

**Key Contributions:**

	1. Introduction of the LOL framework for multi-layer fusion in contrastive decoding.
	2. Integration of a truthfulness refocused module to enhance output validity.
	3. Demonstration of superior performance on four publicly available datasets.

**Result:** Extensive experiments show that LOL significantly reduces hallucinations and outperforms traditional baselines across various datasets.

**Limitations:** 

**Conclusion:** The LOL framework presents a more effective approach to contrastive decoding that enhances the reliability of LLM outputs.

**Abstract:** Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, they occasionally generate inaccurate and counterfactual outputs, a phenomenon commonly referred to as "hallucinations''. To tackle this issue, recent studies have explored contrastive decoding between the original model and an amateur model with induced hallucination, showing promising results. Nevertheless, this approach can disrupt the original LLM's output distribution due to coarse contrast and simple subtraction operations, potentially leading to errors. In this paper, we introduce a novel contrastive decoding framework, termed LOL (LOwer Layer Matters). Unlike prior methods that focus solely on the final layer, our approach integrates contrastive information from lower layers to enable multi-layer fusion during contrastive decoding. Additionally, we incorporate a truthfulness refocused module that leverages instruction guidance to further improve truthfulness in contrastive decoding. Extensive experiments on four publicly available datasets demonstrate that the LOL framework significantly mitigates hallucination while outperforming existing baselines in most cases. For reproducibility, we will release our code and data upon acceptance.

</details>


### [143] [XTRUST: On the Multilingual Trustworthiness of Large Language Models](https://arxiv.org/abs/2409.15762)

*Yahan Li, Yi Wang, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** trustworthiness, large language models, multilingual, healthcare, benchmark

**Relevance Score:** 9

**TL;DR:** This paper introduces XTRUST, a multilingual benchmark for evaluating the trustworthiness of large language models (LLMs) in various sensitive domains.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing concern over the trustworthiness of LLMs, especially when applied in critical sectors such as healthcare and finance, where errors can have significant repercussions.

**Method:** The authors developed XTRUST, a comprehensive multilingual benchmark that assesses the trustworthiness of LLMs across diverse topics and languages. The empirical evaluation included five popular LLMs and covered their performance on various tasks.

**Key Contributions:**

	1. Introduction of the first comprehensive multilingual trustworthiness benchmark, XTRUST.
	2. Empirical evaluation of five widely used LLMs across multiple languages and topics.
	3. Identification of performance gaps in LLMs for low-resource languages.

**Result:** The evaluation revealed that many LLMs exhibit poor performance in low-resource languages like Arabic and Russian, indicating a significant need for improvement in their multilingual capabilities.

**Limitations:** The study primarily focuses on five LLMs and may not represent the entire spectrum of available models; additionally, the benchmark may need to be expanded for broader evaluation.

**Conclusion:** The study emphasizes the necessity for enhanced trustworthiness in multilingual LLMs and suggests improvements to address the deficiencies identified in the evaluation.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing (NLP) tasks, capturing the attention of both practitioners and the broader public. A key question that now preoccupies the AI community concerns the capabilities and limitations of these models, with trustworthiness emerging as a central issue, particularly as LLMs are increasingly applied in sensitive fields like healthcare and finance, where errors can have serious consequences. However, most previous studies on the trustworthiness of LLMs have been limited to a single language, typically the predominant one in the dataset, such as English. In response to the growing global deployment of LLMs, we introduce XTRUST, the first comprehensive multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of topics, including illegal activities, hallucination, out-of-distribution (OOD) robustness, physical and mental health, toxicity, fairness, misinformation, privacy, and machine ethics, across 10 different languages. Using XTRUST, we conduct an empirical evaluation of the multilingual trustworthiness of five widely used LLMs, offering an in-depth analysis of their performance across languages and tasks. Our results indicate that many LLMs struggle with certain low-resource languages, such as Arabic and Russian, highlighting the considerable room for improvement in the multilingual trustworthiness of current language models. The code is available at https://github.com/LluckyYH/XTRUST.

</details>


### [144] [How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not](https://arxiv.org/abs/2409.17044)

*Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane*

**Main category:** cs.CL

**Keywords:** Large Language Models, Speech-to-Text, Speech Foundational Models

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of various components (Speech Foundational Models, adapters, and Large Language Models) on the performance of speech-to-text tasks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of understanding regarding the influence of individual components on speech-to-text downstream task performance.

**Method:** Evaluate combinations of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on Automatic Speech Recognition and Speech Translation tasks.

**Key Contributions:**

	1. Evaluates the performance impact of different components in speech-to-text tasks
	2. Demonstrates the pivotal role of Speech Foundational Models
	3. Analyzes the dependency of adapter design on selected SFM and LLM

**Result:** The evaluation shows that the Speech Foundational Model significantly affects downstream performance, while the adapter choice has a moderate impact that varies based on the SFM and LLM used.

**Limitations:** 

**Conclusion:** The findings highlight the importance of the SFM in achieving high performance in speech-to-text tasks, suggesting that careful selection of components is crucial for optimizing results.

**Abstract:** The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.

</details>


### [145] [Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning](https://arxiv.org/abs/2410.00382)

*Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Unlearning, In-context Learning, AI Ethics, Information Privacy

**Relevance Score:** 9

**TL;DR:** Proposes in-context knowledge unlearning for LLMs, allowing selective forgetting of information during inference while maintaining unrelated data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are used in various fields, the need to unlearn specific information while preserving privacy is crucial.

**Method:** The proposed method fine-tunes pre-trained LLMs to enable prompt unlearning of knowledge based on query context, focusing on test-time adjustments.

**Key Contributions:**

	1. Introduction of in-context knowledge unlearning for LLMs
	2. Demonstrated significant improvements in forget accuracy
	3. Insights into LLM internal behavior regarding knowledge retention and forgetting.

**Result:** Achieves up to 95% forget accuracy and retains 80% of unrelated knowledge, outperforming existing methods in both in-domain and out-of-domain tests on common datasets.

**Limitations:** 

**Conclusion:** The study highlights insights into LLMs' internal functioning, indicating that forgetting decisions occur at the last layer, which suggests room for enhancing unlearning mechanisms.

**Abstract:** As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.

</details>


### [146] [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming](https://arxiv.org/abs/2410.02677)

*Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi*

**Main category:** cs.CL

**Keywords:** Cultural knowledge, Language models, Benchmarking, Human-AI Red-Teaming, Diversity

**Relevance Score:** 9

**TL;DR:** CulturalBench is a benchmark of 1,696 questions designed to assess cultural knowledge of language models (LMs), with findings that highlight the challenges faced by LMs in accurately answering culturally diverse questions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate progress in developing language models that are culturally knowledgeable and useful across diverse global contexts, particularly for underrepresented cultures.

**Method:** A set of 1,696 human-written and verified questions spanning 45 regions and 17 topics, constructed using Human-AI Red-Teaming methods, with multiple annotators assessing each question's validity.

**Key Contributions:**

	1. Introduction of a comprehensive cultural knowledge benchmark for LMs.
	2. Human verification of questions ensuring quality and reliability.
	3. Insights into LM performance across a diverse set of cultural contexts.

**Result:** Top-performing LMs struggle with cultural knowledge, achieving accuracy between 28.7% to 61.5% on the hard version of the benchmark, and showing significant variance in performance across different cultures.

**Limitations:** The performance of LMs varies significantly across different global regions, highlighting limitations in their cultural understanding and applicability.

**Conclusion:** CulturalBench reveals substantial gaps in LMs' abilities to address culturally nuanced questions, suggesting that models like GPT-4o perform best but still struggle significantly with certain regions and topics.

**Abstract:** Robust, diverse, and challenging cultural knowledge benchmarks are essential for measuring our progress towards making LMs that are helpful across diverse cultures. We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs' cultural knowledge, covering 45 global regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions are each verified by five independent annotators and span 17 diverse topics ranging from food preferences to greeting etiquette. We construct CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human performance (92.4% accuracy), the hard version of CulturalBench is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy. We find that LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to overfit to a single answer. Our results indicate that GPT-4o substantially outperform other models across cultures, besting local providers (e.g., Mistral on European culture and DeepSeek on Chinese culture). Across the board, models under-perform on questions related to North Africa, South America and Middle East.

</details>


### [147] [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/abs/2410.03869)

*Wenxuan Wang, Kuiyi Gao, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu*

**Main category:** cs.CL

**Keywords:** image generation, safety assessment, jailbreaking, machine learning, AI safety

**Relevance Score:** 4

**TL;DR:** The paper presents a novel jailbreaking method, Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process; it also proposes a defense method to enhance model safety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant potential for abuse in text-based image generation models by assessing and improving their safety mechanisms.

**Method:** The CoJ attack decomposes harmful queries into sub-queries, enabling iterative image generation and editing, while a new dataset called CoJ-Bench is constructed for evaluation.

**Key Contributions:**

	1. Introduction of the Chain-of-Jailbreak (CoJ) attack method
	2. Creation of the CoJ-Bench dataset for evaluating model safety
	3. Development of Think Twice Prompting as a defense strategy against CoJ attacks.

**Result:** The CoJ attack successfully bypassed safeguards in over 60% of cases across four popular image generation models, significantly outperforming traditional jailbreaking methods.

**Limitations:** 

**Conclusion:** The research highlights vulnerabilities in image generation models and proposes a robust defense method, Think Twice Prompting, that can defend against over 95% of CoJ attacks.

**Abstract:** Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack. We release our dataset and code to facilitate the AI safety research.

</details>


### [148] [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/abs/2410.10672)

*Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Matrix Nuclear-Norm, Data Compression, Computational Efficiency, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** The paper introduces the Matrix Nuclear-Norm as an efficient evaluation metric for large language models, reducing the computational complexity of assessing data compression capabilities from O(n^3) to O(n^2).

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional metrics like Matrix Entropy, which are computationally intensive for large language models (LLMs).

**Method:** The Matrix Nuclear-Norm is introduced as a metric to quantify data compression proficiency and provide a convex approximation of matrix rank, utilizing the L_{1,2}-norm to approximate the nuclear norm for efficient evaluation.

**Key Contributions:**

	1. Introduction of the Matrix Nuclear-Norm as an efficient evaluation metric for LLMs.
	2. Reduction of time complexity from O(n^3) to O(n^2) for model evaluations.
	3. Demonstrated performance speed improvements for large models compared to traditional metrics.

**Result:** The Matrix Nuclear-Norm achieves evaluation speeds that are 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model, with performance improvements becoming more pronounced for larger models, as validated by tests with other models and benchmarks.

**Limitations:** 

**Conclusion:** The proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLM performance, balancing accuracy with computational efficiency.

**Abstract:** As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}\text{-norm} \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.

</details>


### [149] [Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation](https://arxiv.org/abs/2410.10995)

*Emmanouil Zaranis, Giuseppe Attanasio, Sweta Agrawal, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** Quality Estimation, gender bias, machine translation, translation quality, bias in metrics

**Relevance Score:** 7

**TL;DR:** This paper investigates the gender bias in Quality Estimation (QE) metrics for machine translation, revealing that these metrics favor masculine translations and penalize gender-neutral ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the overlooked issue of social biases in QE metrics that can favor certain demographic groups and impact translation quality.

**Method:** Experiments were conducted with state-of-the-art QE metrics across various domains, datasets, and languages to assess their bias regarding gender.

**Key Contributions:**

	1. Identification of gender bias in QE metrics
	2. Analysis of impact on machine translation quality
	3. Call for gender-focused evaluation of QE metrics

**Result:** Significant gender bias was found in QE metrics, with masculine-inflected translations receiving higher scores and gender-neutral translations penalized in scenarios of undisclosed gender.

**Limitations:** The study primarily focuses on gender bias, leaving other possible biases unexamined.

**Conclusion:** The findings highlight the critical need for the development and evaluation of gender-neutral QE metrics to ensure fairer machine translation outcomes.

**Abstract:** Quality estimation (QE)-the automatic assessment of translation quality-has recently become crucial across several stages of the translation pipeline, from data curation to training and decoding. While QE metrics have been optimized to align with human judgments, whether they encode social biases has been largely overlooked. Biased QE risks favoring certain demographic groups over others, e.g., by exacerbating gaps in visibility and usability. This paper defines and investigates gender bias of QE metrics and discusses its downstream implications for machine translation (MT). Experiments with state-of-the-art QE metrics across multiple domains, datasets, and languages reveal significant bias. When a human entity's gender in the source is undisclosed, masculine-inflected translations score higher than feminine-inflected ones, and gender-neutral translations are penalized. Even when contextual cues disambiguate gender, using context-aware QE metrics leads to more errors in selecting the correct translation inflection for feminine referents than for masculine ones. Moreover, a biased QE metric affects data filtering and quality-aware decoding. Our findings underscore the need for a renewed focus on developing and evaluating QE metrics centered on gender.

</details>


### [150] [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/abs/2410.11020)

*Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Understanding, Proximal Policy Optimization, Reinforcement Learning, Machine Learning

**Relevance Score:** 9

**TL;DR:** Exploring Proximal Policy Optimization (PPO) to enhance NLU capabilities of instruction-fine-tuned large language models (LLMs), yielding significant performance improvements over traditional supervised fine-tuning.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs under 14B parameters underperform on NLU tasks compared to smaller models like BERT-base; this research seeks to improve their performance using reinforcement learning techniques.

**Method:** The study uses Proximal Policy Optimization (PPO) to frame NLU as a reinforcement learning environment, treating token generation as a series of actions optimized for reward signals based on ground-truth labels.

**Key Contributions:**

	1. Introduces PPO for enhancing NLU in LLMs
	2. Demonstrates significant performance gains on established benchmarks
	3. Highlights the benefits of using reinforcement learning for task adaptation

**Result:** PPO outperforms supervised fine-tuning by an average of 6.3 points on GLUE and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. PPO-tuned models also outperformed GPT-4o by over 4% on average across tasks.

**Limitations:** 

**Conclusion:** Reframing LLM training as reinforcement learning problems allows for effective adaptation to new tasks using simple rewards instead of extensive data curation, highlighting a new approach to improving NLU abilities in LLMs.

**Abstract:** Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4\% on average across sentiment and natural language inference tasks, including gains of 7.3\% on the Mental Health dataset and 10.9\% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation.

</details>


### [151] [BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks](https://arxiv.org/abs/2410.12974)

*Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla*

**Main category:** cs.CL

**Keywords:** large language models, benchmarking, HCI, transparency, evaluation

**Relevance Score:** 8

**TL;DR:** This paper introduces BenchmarkCards, a framework to standardize and simplify the evaluation of benchmarks for large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for systematic evaluation methods for selecting appropriate LLMs for tasks due to varying model capabilities and the difficulty in finding suitable benchmarks.

**Method:** The authors developed BenchmarkCards, which documents critical benchmark attributes and conducted user studies with benchmark creators and users.

**Key Contributions:**

	1. Introduction of BenchmarkCards for documenting benchmark attributes
	2. Evidence from user studies indicating improved selection transparency
	3. Facilitates informed decision-making for LLM evaluations

**Result:** User studies indicated that BenchmarkCards facilitate easier benchmark selection and enhance transparency in LLM evaluation.

**Limitations:** No significant limitations mentioned; focuses on the utility of BenchmarkCards.

**Conclusion:** BenchmarkCards simplifies the process of benchmark selection, leading to more informed decisions when evaluating LLM performance.

**Abstract:** Large language models (LLMs) are powerful tools capable of handling diverse tasks. Comparing and selecting appropriate LLMs for specific tasks requires systematic evaluation methods, as models exhibit varying capabilities across different domains. However, finding suitable benchmarks is difficult given the many available options. This complexity not only increases the risk of benchmark misuse and misinterpretation but also demands substantial effort from LLM users, seeking the most suitable benchmarks for their specific needs. To address these issues, we introduce \texttt{BenchmarkCards}, an intuitive and validated documentation framework that standardizes critical benchmark attributes such as objectives, methodologies, data sources, and limitations. Through user studies involving benchmark creators and users, we show that \texttt{BenchmarkCards} can simplify benchmark selection and enhance transparency, facilitating informed decision-making in evaluating LLMs. Data & Code: https://github.com/SokolAnn/BenchmarkCards

</details>


### [152] [A Complexity-Based Theory of Compositionality](https://arxiv.org/abs/2410.14817)

*Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie*

**Main category:** cs.CL

**Keywords:** compositionality, AI, cognitive science, deep learning, algorithmic information theory

**Relevance Score:** 6

**TL;DR:** This paper proposes a formal definition of representational compositionality, providing a quantifiable framework for understanding compositionality in AI and cognitive science.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to establish a measurable and mathematical definition of compositionality, which is fundamental to intelligence and crucial for effective AI generalization.

**Method:** The authors introduce 'representational compositionality' defined by three properties: expressiveness, the ability to describe the representation using combinable symbolic sequences, and the simplicity of the function relating these sequences to the representation.

**Key Contributions:**

	1. Proposes a novel formal definition of representational compositionality.
	2. Validates the definition through both synthetic and real-world data experiments.
	3. Demonstrates the application of standard deep learning tools to estimate compositionality.

**Result:** Experiments validate the proposed definition and show how it can unify various intuitions from AI and cognitive science, demonstrating that it can be estimated using existing deep learning tools.

**Limitations:** The theoretical aspects of representational compositionality are intractable, which may hinder practical applications.

**Conclusion:** The definition could inspire the creation of new models that better reflect the mechanisms underlying compositional thought.

**Abstract:** Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. We hope that our definition can inspire the design of novel, theoretically-driven models that better capture the mechanisms of compositional thought. We make our code available at https://github.com/EricElmoznino/complexity_compositionality.

</details>


### [153] [EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/abs/2410.21271)

*Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen*

**Main category:** cs.CL

**Keywords:** Compression, Large Language Models, Low-rank Matrices, Fine-tuning-free, CUDA

**Relevance Score:** 9

**TL;DR:** EoRA is a fine-tuning-free method that enhances the performance of compressed Large Language Models (LLMs) using low-rank matrices, improving accuracy while maintaining computational efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of compressed LLMs without fine-tuning, addressing the issues of accuracy degradation and deployment constraints.

**Method:** EoRA augments compressed LLMs with low-rank matrices, allowing for a flexible trade-off between accuracy and computational overhead without additional fine-tuning.

**Key Contributions:**

	1. Development of the EoRA method for augmenting compressed LLMs
	2. Demonstration of significant accuracy recovery on benchmark datasets
	3. Introduction of an optimized CUDA kernel for faster inference

**Result:** EoRA outperforms previous training-free low-rank methods, achieving significant accuracy improvements on benchmark tasks such as ARC-Challenge, MathQA, and GSM8K, alongside enhancements in inference speed and reduced memory usage.

**Limitations:** 

**Conclusion:** EoRA provides an efficient solution for optimizing compressed LLM performance, facilitating their deployment across diverse user requirements without sacrificing accuracy or computational efficiency.

**Abstract:** While post-training compression techniques effectively reduce the memory footprint, latency, and power consumption of Large Language Models (LLMs), they often result in noticeable accuracy degradation and remain limited by hardware and kernel constraints that restrict supported compression formats ultimately reducing flexibility across a wide range of deployment scenarios. In this work, we propose EoRA, a novel fine-tuning-free method that augments compressed LLMs with low-rank matrices, allowing users to rapidly enhance task-specific performance and freely balance the trade-off between accuracy and computational overhead beyond the constraints of compression formats. EoRA consistently outperforms prior training-free low rank methods in recovering the accuracy of compressed LLMs, achieving notable accuracy improvements (e.g., $\mathbf{10.84\%}$ on ARC-Challenge, $\mathbf{6.74\%}$ on MathQA, and $\mathbf{6.74\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also introduce an optimized CUDA kernel, accelerating inference by up to 1.4x and reducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt solution for improving the accuracy of compressed models under varying user requirements, enabling more efficient and flexible deployment of LLMs. Code is available at https://github.com/NVlabs/EoRA.

</details>


### [154] [Self-Evolved Reward Learning for LLMs](https://arxiv.org/abs/2411.00418)

*Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Reward Model, Self-Evolved Learning, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces Self-Evolved Reward Learning (SER), a method that enhances the training of reward models in reinforcement learning by generating additional data from self-feedback, improving LLM performance with less reliance on human annotations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional reward models that rely on human feedback, which can be costly and introduce biases, this work explores a self-improving approach to training reward models.

**Method:** The authors propose Self-Evolved Reward Learning (SER), where the reward model autonomously generates training data to iteratively refine its performance, tested on datasets like HH-RLHF and UltraFeedback, with models including Mistral and Llama 3.

**Key Contributions:**

	1. Introduction of Self-Evolved Reward Learning (SER) for reward model improvement.
	2. Demonstration of enhanced performance of LLMs with limited human feedback.
	3. Extensive experimental validation on multiple datasets and comparisons against existing baselines.

**Result:** Experiments show that SER can significantly improve reward model performance even with limited human-annotated data, enhancing the efficacy of large language models like GPT-4 and ChatGPT.

**Limitations:** The effectiveness of SER may depend on the initial quality of the reward model and the datasets used for training.

**Conclusion:** SER presents a promising alternative to traditional human-centric approaches for training reward models, demonstrating robustness in LLM performance improvement through self-feedback mechanisms.

**Abstract:** Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs). Resources of this paper can be found at https://aka.ms/ser

</details>


### [155] [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/abs/2411.02430)

*Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Zhitao Zhang*

**Main category:** cs.CL

**Keywords:** Multimodal conversation, Emotional causation, Large Language Models, Emotion recognition, Dataset ECEM

**Relevance Score:** 8

**TL;DR:** This paper addresses the multimodal emotion cause explanation in conversations by introducing a novel task and dataset, ECEM, and proposing FAME-Net for analyzing emotional cues via Large Language Models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of emotional causation in multimodal conversations, which is under-explored in existing research focused on single modalities.

**Method:** Developed a new dataset (ECEM) leveraging video clips and emotional explanations, and proposed FAME-Net, a model utilizing LLMs to analyze visual data for interpreting emotions from facial expressions in conversations.

**Key Contributions:**

	1. Introduction of the MECEC task for multimodal emotion cause exploration.
	2. Development of the ECEM dataset that includes video clips and emotional responses.
	3. Proposal of FAME-Net, leveraging LLMs for facial emotion analysis.

**Result:** FAME-Net demonstrates superior performance in identifying emotional causes in conversations compared to existing methods on the ECEM dataset.

**Limitations:** 

**Conclusion:** The introduction of the MECEC task and the innovative approach of FAME-Net significantly advances the field of multimodal emotion recognition in conversations.

**Abstract:** Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically employs an utterance selection method within a single textual modality to locate causal utterances. This approach remains limited to coarse-grained assessments, lacks nuanced explanations of emotional causation, and demonstrates inadequate capability in identifying multimodal emotional triggers. Therefore, we introduce a task-\textbf{Multimodal Emotion Cause Explanation in Conversation (MECEC)}. This task aims to generate a summary based on the multimodal context of conversations, clearly and intuitively describing the reasons that trigger a given emotion. To adapt to this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM combines video clips with detailed explanations of character emotions, helping to explore the causal factors behind emotional expression in multimodal conversations. A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net outperforms several excellent baselines. Code and dataset are available at https://github.com/3222345200/FAME-Net.

</details>


### [156] [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://arxiv.org/abs/2411.02528)

*Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao*

**Main category:** cs.CL

**Keywords:** language models, human acceptability, MORCELA, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces MORCELA, a new linking theory for comparing language model probabilities to human acceptability judgments, proposing data-driven adjustments for factors like length and unigram frequency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the comparison between language models and human linguistic judgments by addressing the limitations of existing linking theories that treat adjustment factors uniformly across models.

**Method:** MORCELA applies learned parameters to estimate optimal adjustments for length and unigram frequency based on data, and its performance is compared to the SLOR linking theory.

**Key Contributions:**

	1. Introduction of MORCELA as a new linking theory for LM probabilities to human judgments.
	2. Demonstration of MORCELA's superiority over SLOR in performance across different models.
	3. Insights into the relationship between model size and adjustment needs for unigram frequency.

**Result:** MORCELA outperforms SLOR across two families of transformer language models (Pythia and OPT), highlighting that larger models require less adjustment for unigram frequency.

**Limitations:** The paper primarily focuses on transformer models and may not generalize to other architectures.

**Conclusion:** The study concludes that larger language models exhibit lower susceptibility to frequency effects due to their improved ability to predict rarer words in context, indicating the necessity of adjusting for these confounds.

**Abstract:** When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability - SLOR (Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs' lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context.

</details>


### [157] [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)

*Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao*

**Main category:** cs.CL

**Keywords:** Speculative Decoding, Large Language Models, Suffix Trees

**Relevance Score:** 8

**TL;DR:** SuffixDecoding is a new method using suffix trees to enhance the efficiency of speculative decoding in LLMs, achieving significant speedups for repetitive inference requests common in LLM-based agentic frameworks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve latency in large language model inference by addressing workload characteristics of LLM-based agents that require repetitive inference requests.

**Method:** Introduces SuffixDecoding, which leverages efficient suffix trees to cache token sequences and adaptively speculates based on acceptance likelihood.

**Key Contributions:**

	1. Introduction of SuffixDecoding method
	2. Utilization of suffix trees for caching sequences
	3. Demonstrated speedups over existing methods in agentic benchmarks

**Result:** SuffixDecoding achieves speedups of up to 5.3x on benchmarks like SWE-Bench and Text-to-SQL, outperforming state-of-the-art methods by a significant margin.

**Limitations:** 

**Conclusion:** SuffixDecoding effectively optimizes performance for agentic frameworks where predictability of tasks allows for more efficient speculative decoding.

**Abstract:** Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference.

</details>


### [158] [Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts](https://arxiv.org/abs/2411.11479)

*Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Value-Spectrum, Visual Question Answering, human values, multimodal applications

**Relevance Score:** 8

**TL;DR:** The paper introduces Value-Spectrum, a benchmark for evaluating Vision-Language Models (VLMs) based on personality traits and human values, using a dataset of over 50,000 videos for enhanced VQA assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to evaluate VLMs not just on functional tasks but also on abstract dimensions like personality traits and human values, which are often overlooked.

**Method:** The study develops a VLM agent pipeline for video browsing and creates a vector database with over 50,000 videos from platforms like TikTok and Instagram, aimed at assessing VLMs on value-oriented content.

**Key Contributions:**

	1. Introduction of the Value-Spectrum benchmark for VLM evaluation
	2. Creation of a large-scale video dataset for multimodal assessment
	3. Insights into VLM adaptability and persona simulation capabilities.

**Result:** Benchmarking reveals significant variations in how VLMs interpret value-oriented content and their adaptability in role-playing personas when prompted.

**Limitations:** 

**Conclusion:** Value-Spectrum provides a comprehensive evaluation framework for assessing VLM preferences in value-based tasks, offering insights into model adaptability and persona simulation.

**Abstract:** The recent progress in Vision-Language Models (VLMs) has broadened the scope of multimodal applications. However, evaluations often remain limited to functional tasks, neglecting abstract dimensions such as personality traits and human values. To address this gap, we introduce Value-Spectrum, a novel Visual Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's value dimensions that capture core human values guiding people's preferences and actions. We design a VLM agent pipeline to simulate video browsing and construct a vector database comprising over 50,000 short videos from TikTok, YouTube Shorts, and Instagram Reels. These videos span multiple months and cover diverse topics, including family, health, hobbies, society, technology, etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs handle value-oriented content. Beyond identifying VLMs' intrinsic preferences, we also explore the ability of VLM agents to adopt specific personas when explicitly prompted, revealing insights into the adaptability of the model in role-playing scenarios. These findings highlight the potential of Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences in value-based tasks and abilities to simulate diverse personas. The complete code and data are available at: https://github.com/Jeremyyny/Value-Spectrum.

</details>


### [159] [SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction](https://arxiv.org/abs/2411.16765)

*Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu, Alexander H. Liu*

**Main category:** cs.CL

**Keywords:** sign language, self-supervised learning, contextual representation, machine learning, transfer learning

**Relevance Score:** 5

**TL;DR:** SHuBERT is a self-supervised model for processing sign language, achieving state-of-the-art results across several tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of task-specific models in sign language processing and enhance the transfer learning potential across different tasks.

**Method:** SHuBERT implements self-supervised masked token prediction applied to multi-stream visual inputs from American Sign Language videos.

**Key Contributions:**

	1. Introduction of SHuBERT for sign language processing.
	2. Utilization of self-supervised learning with multiple visual streams.
	3. State-of-the-art performance across various sign language tasks.

**Result:** The model has achieved state-of-the-art performance in sign language translation, isolated sign language recognition, and fingerspelling detection.

**Limitations:** 

**Conclusion:** SHuBERT demonstrates significant advancements in sign language processing through contextual representation and self-supervised learning techniques.

**Abstract:** Sign language processing has traditionally relied on task-specific models, limiting the potential for transfer learning across tasks. Pre-training methods for sign language have typically focused on either supervised pre-training, which cannot take advantage of unlabeled data, or context-independent (frame or video segment) representations, which ignore the effects of relationships across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a self-supervised contextual representation model learned from approximately 1,000 hours of American Sign Language video. SHuBERT adapts masked token prediction objectives to multi-stream visual sign language input, learning to predict multiple targets corresponding to clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art performance across multiple tasks including sign language translation, isolated sign language recognition, and fingerspelling detection.

</details>


### [160] [Is it the end of (generative) linguistics as we know it?](https://arxiv.org/abs/2412.12797)

*Cristiano Chesi*

**Main category:** cs.CL

**Keywords:** generative linguistics, Poverty of Stimulus, theoretical perspectives, empirical adequacy, formalization

**Relevance Score:** 2

**TL;DR:** This paper critiques generative linguistics based on computational, theoretical, and experimental perspectives, arguing for a necessary update to the field.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address criticisms of generative linguistics and its adherence to the Poverty of Stimulus hypothesis, and to advocate for improved empirical standards.

**Method:** Adopts three idealized perspectives (computational, theoretical, and experimental) to analyze critiques of generative linguistics.

**Key Contributions:**

	1. Critically analyzes Piantadosi's critique of generative linguistics.
	2. Suggests a need for precise formalizations in generative linguistics.
	3. Advocates for the creation of standardized datasets for empirical evaluation.

**Result:** The paper finds that generative linguistics needs improvement in formalization and empirical evidence to maintain its relevance in language studies.

**Limitations:** 

**Conclusion:** An update of generative linguistics is necessary for improved precision and empirical adequacy to ensure its continued relevance in theoretical language studies.

**Abstract:** A significant debate has emerged in response to a paper written by Steven Piantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open archive for generative linguistics. Piantadosi's dismissal of Chomsky's approach is ruthless, but generative linguists deserve it. In this paper, I will adopt three idealized perspectives -- computational, theoretical, and experimental -- to focus on two fundamental issues that lend partial support to Piantadosi's critique: (a) the evidence challenging the Poverty of Stimulus (PoS) hypothesis and (b) the notion of simplicity as conceived within mainstream Minimalism. In conclusion, I argue that, to reclaim a central role in language studies, generative linguistics -- representing a prototypical theoretical perspective on language -- needs a serious update leading to (i) more precise, consistent, and complete formalizations of foundational intuitions and (ii) the establishment and utilization of a standardized dataset of crucial empirical evidence to evaluate the theory's adequacy. On the other hand, ignoring the formal perspective leads to major drawbacks in both computational and experimental approaches. Neither descriptive nor explanatory adequacy can be easily achieved without the precise formulation of general principles that can be challenged empirically.

</details>


### [161] [Can Input Attributions Explain Inductive Reasoning in In-Context Learning?](https://arxiv.org/abs/2412.15628)

*Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki*

**Main category:** cs.CL

**Keywords:** neural models, in-context learning, input attribution, interpretability, inductive reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates the interpretability of in-context learning in large language models through synthetic diagnostic tasks that test inductive reasoning, evaluating the effectiveness of input attribution methods in identifying influential examples.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the internal processes of neural models, particularly in the context of in-context learning where determining the influence of few-shot examples is crucial.

**Method:** The authors design synthetic diagnostic tasks that contain ambiguous examples, assessing various input attribution methods to see if they can accurately identify the critical example that disambiguates the task.

**Key Contributions:**

	1. Creation of synthetic diagnostic tasks inspired by psycholinguistic generalization tests.
	2. Evaluation of input attribution methods in identifying critical examples for inductive reasoning.
	3. Insights into the challenges of interpreting larger language models.

**Result:** Experiments show that a particular simple input attribution method outperforms others, and larger models generally present greater challenges for interpretation using gradient-based methods.

**Limitations:** Focuses on synthetic tasks which may not fully represent real-world scenarios; results may vary with different model architectures.

**Conclusion:** Interpretability remains a complex issue in in-context learning, with findings suggesting that simpler methods may be more effective than expected for understanding model reasoning.

**Abstract:** Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying/solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests typically adopted in psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates it. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods.

</details>


### [162] [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/abs/2412.17063)

*Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend*

**Main category:** cs.CL

**Keywords:** character development, narrative analysis, Holocaust testimonies, religious trajectory, natural language processing

**Relevance Score:** 3

**TL;DR:** This paper presents a computational method for analyzing character development, focusing on Holocaust survivor testimonies to explore changes in religious belief and practice.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to understand character evolution within narratives, particularly in the context of Holocaust survivor testimonies, providing insights into their religious trajectories.

**Method:** The study employs natural language processing techniques to analyze transcripts of survivor testimonies, focusing on the evolution of their religious beliefs and practices.

**Key Contributions:**

	1. Introduction of a computational approach to character analysis in narratives.
	2. Identification of common religiosity structures in Holocaust survivor testimonies.
	3. Demonstration of NLP techniques' applicability in historical narrative analysis.

**Result:** Clustering the trajectories reveals common narratives in belief, which tend to remain constant, while practices often oscillate, indicating different structures of religiosity among survivors.

**Limitations:** 

**Conclusion:** The findings show the effectiveness of NLP methods in analyzing character development, contributing to both historical and sociological understanding of individual narratives.

**Abstract:** This work presents a computational approach to analyze character development along the narrative timeline. The analysis characterizes the inner and outer changes the protagonist undergoes within a narrative, and the interplay between them. We consider transcripts of Holocaust survivor testimonies as a test case, each telling the story of an individual in first-person terms. We focus on the survivor's religious trajectory, examining the evolution of their disposition toward religious belief and practice along the testimony. Clustering the resulting trajectories in the dataset, we identify common sequences in the data. Our findings highlight multiple common structures of religiosity across the narratives: in terms of belief, most present a constant disposition, while for practice, most present an oscillating structure, serving as valuable material for historical and sociological research. This work demonstrates the potential of natural language processing techniques for analyzing character evolution through thematic trajectories in narratives.

</details>


### [163] [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)

*Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, self-evolving training, reinforcement learning, M-STAR, performance saturation

**Relevance Score:** 8

**TL;DR:** This paper explores self-evolving training for multimodal reasoning, identifying critical factors for its effectiveness and proposing a framework (M-STAR) that improves performance while addressing saturation issues.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of high-quality chain-of-thought data for complex reasoning tasks, particularly in multimodal reasoning where traditional methods struggle.

**Method:** The paper reframes self-evolving training using principles from reinforcement learning, analyzing Training Method, Reward Model, and Prompt Variation to improve efficacy.

**Key Contributions:**

	1. Introduction of M-STAR framework for multimodal reasoning
	2. Identification of key factors affecting self-evolving training
	3. Development of an automatic balancing mechanism to mitigate saturation.

**Result:** Establishes optimal design principles for multimodal reasoning and introduces a new balancing mechanism to overcome the saturation challenge.

**Limitations:** 

**Conclusion:** The proposed M-STAR framework demonstrates consistent performance improvements across various models and benchmarks, enhancing multimodal reasoning capabilities.

**Abstract:** Self-evolving trainin--where models iteratively learn from their own outputs--has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: Training Method, Reward Model, and Prompt Variation. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose M-STAR (Multimodal Self-evolving Training for Reasoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources are made publicly available at https://mstar-lmm.github.io.

</details>


### [164] [Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse](https://arxiv.org/abs/2412.17533)

*Anna Kołos, Katarzyna Lorenc, Emilia Wiśnios, Agnieszka Karlińska*

**Main category:** cs.CL

**Keywords:** content detection, Polish language, dataset, transformer models, content moderation

**Relevance Score:** 4

**TL;DR:** Development of forePLay, a Polish language dataset for detecting erotic content, showcasing superior performance of specialized models over multilingual ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the urgent demand for effective detection systems in non-English contexts, specifically for erotic content in Polish, where existing tools are lacking.

**Method:** Creation of a dataset containing over 24,000 annotated Polish sentences based on a multidimensional taxonomy that includes ambiguity, violence, and social unacceptability.

**Key Contributions:**

	1. Introduction of forePLay dataset with rich annotations for Polish language
	2. Demonstration of superior model performance for specialized Polish models
	3. Establishment of frameworks for developing content moderation systems in linguistically complex environments.

**Result:** Specialized Polish language models outperform multilingual alternatives in detecting erotic content, with transformer architectures excelling in handling imbalanced categories.

**Limitations:** 

**Conclusion:** The forePLay dataset and analysis provide frameworks for linguistically-aware content moderation systems while drawing attention to the complexities of morphologically rich languages.

**Abstract:** The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We present forePLay, a novel Polish language dataset for erotic content detection, featuring over 24k annotated sentences with a multidimensional taxonomy encompassing ambiguity, violence, and social unacceptability dimensions. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages.

</details>


### [165] [Instruction-Following Pruning for Large Language Models](https://arxiv.org/abs/2501.02086)

*Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei*

**Main category:** cs.CL

**Keywords:** large language models, dynamic pruning, instruction-following pruning

**Relevance Score:** 10

**TL;DR:** This paper introduces a dynamic pruning approach for large language models (LLMs) that selects relevant parameters based on user instructions, improving performance over static pruning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency and performance of large language models beyond traditional static pruning methods.

**Method:** The authors propose 'instruction-following pruning', where a sparse mask predictor adapts the pruning mask based on input user instructions, optimizing both the predictor and the LLM together.

**Key Contributions:**

	1. Introduction of instruction-following pruning for dynamic parameter selection
	2. Joint optimization of sparse mask predictor and LLM
	3. Demonstrated improvements on evaluation benchmarks compared to static pruning

**Result:** The dynamic pruning approach yields a 5-8 point improvement on benchmarks in math and coding tasks compared to a 3B dense model and matches the performance of a 9B model.

**Limitations:** 

**Conclusion:** The proposed method demonstrates significant advantages in adapting LLMs to specific tasks without losing efficiency or performance.

**Abstract:** With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed "instruction-following pruning", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.

</details>


### [166] [FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings](https://arxiv.org/abs/2501.06645)

*Tong Liu, Xiao Yu, Wenxuan Zhou, Jindong Gu, Volker Tresp*

**Main category:** cs.CL

**Keywords:** Direct Preference Optimization, Focal Loss, Large Language Models

**Relevance Score:** 9

**TL;DR:** FocalPO, a variant of Direct Preference Optimization, improves preference optimization for LLMs by down-weighting misranked pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of Direct Preference Optimization (DPO) where it rarely improves misranked preference pairs despite its focus on them, leading to ineffective training.

**Method:** FocalPO modifies the DPO approach by adding a modulating factor to its loss function, allowing it to down-weight misranked preference pairs and focus on enhancing the model's understanding of correctly ranked pairs.

**Key Contributions:**

	1. Introduction of FocalPO as a DPO variant
	2. Demonstrated superiority of FocalPO over traditional DPO
	3. Detailed empirical results showing FocalPO's effect on training dynamics

**Result:** FocalPO outperforms DPO and its variants on Alpaca Eval 2.0 benchmarks when using Mistral-Base-7B and Llama-3-Instruct-8B models.

**Limitations:** 

**Conclusion:** FocalPO is more effective than DPO for training LLMs in preference optimization, particularly in distinguishing between correct and incorrect sample groups.

**Abstract:** Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\citep{chen2024preference} empirically finds that DPO training \textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.

</details>


### [167] [Large Language Models to Diffusion Finetuning](https://arxiv.org/abs/2501.15781)

*Edoardo Cetin, Tianyu Zhao, Yujin Tang*

**Main category:** cs.CL

**Keywords:** large language models, finetuning, diffusion framework, autoregressive models, adaptive ODE solvers

**Relevance Score:** 9

**TL;DR:** A new finetuning method for large language models enhances performance through a diffusion framework, enabling adaptive compute requirements and improved accuracy on downstream tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and performance of pre-trained large language models by integrating diffusion methods into their finetuning process.

**Method:** The proposed method involves increasing the number of diffusion steps during finetuning, allowing models to autonomously adjust compute requirements based on specific tasks using adaptive ODE solvers without altering original model weights.

**Key Contributions:**

	1. A novel finetuning technique leveraging adaptive ODE solvers with diffusion steps.
	2. Demonstrated consistency in accuracy improvements related to increased compute at test-time.
	3. Compatibility with existing autoregressive frameworks while maintaining original model performance.

**Result:** Models finetuned with this method demonstrated monotonically increasing accuracy across various downstream tasks and improved question-answering capabilities on specified topics with the integration of guidance techniques.

**Limitations:** 

**Conclusion:** The method effectively combines strengths of autoregressive and diffusion frameworks for better performance in large language models, remaining compatible with traditional finetuning approaches.

**Abstract:** We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.

</details>


### [168] [Inference-time sparse attention with asymmetric indexing](https://arxiv.org/abs/2502.08246)

*Pierre-Emmanuel Mazaré, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Hervé Jégou, Matthijs Douze*

**Main category:** cs.CL

**Keywords:** self-attention, transformer models, asymmetric partitions, memory efficiency, Llama 3.1

**Relevance Score:** 5

**TL;DR:** The paper presents Saap, an asymmetric indexing technique for self-attention in transformer models, significantly improving efficiency by optimizing memory usage and processing time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the speed of self-attention in transformer models by addressing the limitations of existing GPU-compatible vector search algorithms that use standard partitioning methods.

**Method:** Saap introduces asymmetrical partitions for keys and queries, approximating self-attention with a data-adaptive sparsity pattern, requiring only an offline trained small query classifier.

**Key Contributions:**

	1. Introduction of asymmetric partitions for keys and queries
	2. Significant reduction in memory lookup and processing time
	3. Application to pretrained language models with minimal retraining requirements

**Result:** Saap reduces the memory lookup fraction by a factor of 20 and provides a 60% time saving compared to FlashAttention-v2, particularly effective on models like Llama 3.1-8b with long context sequences.

**Limitations:** 

**Conclusion:** Saap effectively improves self-attention efficiency in pretrained language models without extensive retraining, making it a promising method for large-scale applications.

**Abstract:** Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compatible vector search algorithms based on standard partitioning methods such as k-means. However, such partitioning methods yield poor results in this context because (1) the keys and queries follow different distributions, and (2) the RoPE positional encoding hinders the bucket assignment.   This paper introduces Saap (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern. It works on pretrained language models and only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, Saap typically reduces by a factor of 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\% when compared to FlashAttention-v2.

</details>


### [169] [Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?](https://arxiv.org/abs/2502.09416)

*Takumi Goto, Yusuke Sakai, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** grammatical error correction, automatic evaluation, human evaluation, ranking method, SEEDA benchmark

**Relevance Score:** 8

**TL;DR:** This paper proposes a new aggregation method for automatic evaluation metrics in grammatical error correction (GEC) to align better with human preferences.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current automatic evaluations in GEC do not match human evaluation methods, leading to discrepancies in system rankings.

**Method:** The study introduces an aggregation method that aligns automatic evaluation metrics with human evaluation rankings, using experimental validation on various metrics including edit-based and n-gram based metrics.

**Key Contributions:**

	1. Introduces a novel aggregation method for GEC evaluation metrics.
	2. Demonstrates improved alignment of automatic metrics with human evaluations.
	3. Provides empirical evidence using the SEEDA benchmark.

**Result:** The new method improves results for most existing metrics on the SEEDA benchmark, with BERT-based metrics outperforming GPT-4 metrics in some cases.

**Limitations:** The study may not cover all potential GEC metrics and their interactions with human evaluators.

**Conclusion:** The proposed method enhances the correlation between automatic and human evaluations in GEC, contributing to more accurate system rankings.

**Abstract:** One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, n-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. The proposed ranking method is integrated gec-metrics.

</details>


### [170] [Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues](https://arxiv.org/abs/2502.10973)

*David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter*

**Main category:** cs.CL

**Keywords:** Akan language, emotion recognition, multimodal dataset, prosodic annotations, low-resource languages

**Relevance Score:** 4

**TL;DR:** Introducing the Akan Conversation Emotion (ACE) dataset, a multimodal resource for emotion recognition in the Akan language, addressing the underrepresentation of low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in emotion recognition resources for low-resource African languages.

**Method:** Development of a multimodal dataset containing 385 emotion-labeled dialogues and annotations in audio, visual, and textual modalities, with added prosodic prominence labels.

**Key Contributions:**

	1. First multimodal emotion dialogue dataset for the Akan language.
	2. Includes word-level prosodic prominence annotations.
	3. Establishes baselines for emotion recognition using state-of-the-art methods.

**Result:** Demonstrated the quality and utility of the ACE dataset through experiments, establishing solid baselines for emotion recognition methods.

**Limitations:** 

**Conclusion:** ACE serves as a foundational resource for future research in inclusive NLP focusing on linguistically and culturally diverse data.

**Abstract:** In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.

</details>


### [171] [Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models](https://arxiv.org/abs/2502.11075)

*Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Numerical Reasoning, Benchmarking, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Proposal of a benchmark called NumericBench aimed at evaluating numerical reasoning capabilities of Large Language Models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the poor performance of LLMs on numerical reasoning tasks by providing a comprehensive evaluation framework.

**Method:** Introduction of NumericBench, which evaluates six fundamental numerical capabilities through diverse datasets.

**Key Contributions:**

	1. Introduction of the NumericBench benchmark for numerical reasoning evaluation
	2. Coverage of various numerical tasks and real-world context challenges
	3. Release of extensive datasets for robust evaluation

**Result:** Numerous state-of-the-art LLMs demonstrate persistent weaknesses in tasks involving numerical reasoning, underscoring the need for improvements in numerically-aware language modeling.

**Limitations:** 

**Conclusion:** The NumericBench benchmark aims to enhance the evaluation and development of LLMs' numerical reasoning abilities.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.

</details>


### [172] [Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/abs/2502.11184)

*Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, safety awareness, benchmark, MMSafeAware, image-prompt pairs

**Relevance Score:** 9

**TL;DR:** The paper presents MMSafeAware, a benchmark for evaluating safety awareness in Multimodal Large Language Models (MLLMs) across various safety scenarios, revealing significant deficiencies in current models' safety performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved safety awareness in MLLMs, specifically in correctly identifying unsafe content and reducing over-sensitivity.

**Method:** Introduces MMSafeAware benchmark with 1500 image-prompt pairs to evaluate safety across 29 scenarios; evaluates nine widely used MLLMs.

**Key Contributions:**

	1. Introduction of the MMSafeAware benchmark for MLLM safety evaluation.
	2. Evaluation of nine popular MLLMs demonstrating their deficiencies in safety awareness.
	3. Exploration of methods to enhance safety awareness that proved largely ineffective.

**Result:** Current MLLMs exhibit poor safety performance, with high misclassification rates of both unsafe and benign inputs; the methods proposed for improvement were ineffective.

**Limitations:** Limited success of proposed methods to improve safety awareness; further research needed to address safety challenges in MLLMs.

**Conclusion:** Current MLLMs struggle with safety awareness, indicating a critical need for better solutions and further research.

**Abstract:** Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.

</details>


### [173] [A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](https://arxiv.org/abs/2502.12665)

*Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li*

**Main category:** cs.CL

**Keywords:** long context, KV cache, vector quantization, attention scores, retrieval-based

**Relevance Score:** 9

**TL;DR:** This paper introduces A$^2$ATS, a novel method for efficient retrieval-based KV cache reduction in long context LLMs, achieving improved performance with reduced overhead.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Long context LLMs face challenges related to memory footprint and KV cache access overhead during inference, necessitating effective reduction methods.

**Method:** The paper proposes A$^2$ATS, which utilizes vector quantization on key states for efficient top-K token retrieval, with techniques like Windowed Rotary Position Embedding and query-aware vector quantization to enhance attention score approximation.

**Key Contributions:**

	1. Introduction of A$^2$ATS for KV cache reduction
	2. Windowed Rotary Position Embedding for decoupling positional dependencies
	3. Query-aware vector quantization for optimizing attention score approximation

**Result:** A$^2$ATS reduces performance degradation and overhead compared to existing approaches, improving long context serving throughput by up to 2.7 times.

**Limitations:** The study might still face challenges in real-world application performance and scalability beyond current benchmarks.

**Conclusion:** A$^2$ATS presents a promising solution for enhancing the efficiency of long context LLMs, facilitating better performance with larger batch sizes.

**Abstract:** Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \times$.

</details>


### [174] [Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison](https://arxiv.org/abs/2502.12921)

*George-Kirollos Saad, Scott Sanner*

**Main category:** cs.CL

**Keywords:** contrastive summarization, query-driven recommendation, large language models, debate prompting, user understanding

**Relevance Score:** 7

**TL;DR:** Q-STRUM Debate is a new method for contrastive summarization in query-driven recommendations, improving upon existing methods by utilizing debate-style prompting to generate clearer contrasts between items.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user understanding of why certain items are suitable for their needs in query-driven recommendations, addressing shortcomings of existing contrastive summarization methods.

**Method:** Q-STRUM Debate employs debate-style prompting in large language models to create focused and effective contrastive summaries of item aspects relevant to user queries.

**Key Contributions:**

	1. Introduction of a novel debate prompting methodology for contrastive summarization.
	2. Significant performance improvements over existing methods.
	3. Enhanced user understanding of item relevance through improved summarization techniques.

**Result:** Experiments show that Q-STRUM Debate significantly outperforms existing contrastive summarization methods on key criteria across three datasets.

**Limitations:** 

**Conclusion:** The Q-STRUM Debate methodology introduces an innovative way to generate clearer contrastive summaries, demonstrating enhanced performance in query-driven recommendation scenarios.

**Abstract:** Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.

</details>


### [175] [A Similarity Paradigm Through Textual Regularization Without Forgetting](https://arxiv.org/abs/2502.14376)

*Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu*

**Main category:** cs.CL

**Keywords:** prompt learning, visual-language models, generalization, textual regularization, machine learning

**Relevance Score:** 6

**TL;DR:** A novel method named Similarity Paradigm with Textual Regularization (SPTR) for improving prompt learning in visual-language models, aimed at enhancing generalization without losing essential knowledge.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generalization performance of visual-language models in prompt learning while avoiding overfitting on specific downstream data distributions.

**Method:** SPTR utilizes optimal transport for textual regularization to retain hand-crafted features while tuning textual features, alongside a similarity paradigm to enhance model robustness through alignment scores.

**Key Contributions:**

	1. Introduction of Similarity Paradigm with Textual Regularization (SPTR)
	2. Optimal transport for retaining hand-crafted features
	3. Improved generalization capabilities across multiple tasks

**Result:** SPTR outperformed existing prompt learning methods in four key tasks across 11 datasets, showcasing improved generalization capabilities.

**Limitations:** 

**Conclusion:** The proposed method addresses generalization issues in prompt learning by effectively combining textual regularization and alignment through multi-hand-crafted prompts.

**Abstract:** Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.

</details>


### [176] [Social Genome: Grounded Social Reasoning Abilities of Multimodal Models](https://arxiv.org/abs/2502.15109)

*Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency*

**Main category:** cs.CL

**Keywords:** social reasoning, multimodal models, benchmark, external knowledge, AI communication

**Relevance Score:** 6

**TL;DR:** SOCIAL GENOME is a benchmark for evaluating social reasoning abilities in multimodal AI models, featuring videos and human-annotated reasoning traces.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable AI systems to better interpret and respond to human communication and interactions in social contexts.

**Method:** Introduced SOCIAL GENOME benchmark with 272 videos and 1,486 human-annotated reasoning traces, which include 5,777 reasoning steps referencing multiple cue types and external knowledge.

**Key Contributions:**

	1. First benchmark for fine-grained social reasoning in AI
	2. Modeling challenge focusing on external knowledge in social reasoning
	3. Holistic evaluation metrics for social reasoning traces

**Result:** Experiments with state-of-the-art models revealed performance gaps in social reasoning abilities, highlighting areas for improvement.

**Limitations:** 

**Conclusion:** SOCIAL GENOME provides a comprehensive framework for evaluating and improving grounded social reasoning in multimodal models.

**Abstract:** Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.

</details>


### [177] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2502.17110)

*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

**Main category:** cs.CL

**Keywords:** Mobile automation, operational knowledge, video-based learning, AI frameworks, benchmarking

**Relevance Score:** 6

**TL;DR:** Mobile-Agent-V is a novel framework that improves mobile automation by using video to inject operational knowledge, reducing manual effort and increasing performance by 36%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise in mobile device usage creates a need for efficient task management automation, which is currently lacking in many AI frameworks due to limited operational expertise.

**Method:** The framework utilizes video as a knowledge source to streamline the process of acquiring operational knowledge for mobile automation, eliminating the burden of manual knowledge writing.

**Key Contributions:**

	1. Introduction of the Mobile-Agent-V framework for mobile automation using video knowledge.
	2. Development of the Mobile-Knowledge benchmark for evaluating external knowledge impact on mobile agents.
	3. Demonstrated 36% performance improvement in mobile automation tasks.

**Result:** Mobile-Agent-V demonstrates a 36% performance improvement over existing mobile automation methods when assessed using the new Mobile-Knowledge benchmark.

**Limitations:** 

**Conclusion:** This framework significantly enhances mobile automation by effortlessly integrating operational knowledge, showcasing advantages in efficiency and performance.

**Abstract:** The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.

</details>


### [178] [CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought](https://arxiv.org/abs/2502.17214)

*Boxuan Zhang, Ruqi Zhang*

**Main category:** cs.CL

**Keywords:** Uncertainty Quantification, Large Language Models, Chain-of-Thought, Reasoning, Misinformation

**Relevance Score:** 9

**TL;DR:** This paper introduces CoT-UQ, a response-wise uncertainty quantification framework for large language models that integrates reasoning capabilities to enhance decision-making reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often fail to accurately quantify uncertainty in their responses, leading to challenges in detecting misinformation and making reliable decisions.

**Method:** We propose CoT-UQ, a framework that utilizes Chain-of-Thought reasoning from LLMs to extract and assess the importance of keywords during inference, yielding a response-wise uncertainty estimate.

**Key Contributions:**

	1. Development of CoT-UQ for response-wise uncertainty quantification
	2. Integration of reasoning through Chain-of-Thought into the UQ process
	3. Demonstrated significant performance improvements over existing UQ methods

**Result:** CoT-UQ demonstrates a significant performance increase, with an average improvement of 5.9% AUROC when compared to existing uncertainty quantification methods across various reasoning tasks using the Llama Family models.

**Limitations:** 

**Conclusion:** The new framework improves reliability in LLM output by providing a more precise uncertainty quantification, making LLMs more effective in critical applications.

**Abstract:** Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on Llama Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ.

</details>


### [179] [Towards Enhanced Immersion and Agency for LLM-based Interactive Drama](https://arxiv.org/abs/2502.17878)

*Hongqiu Wu, Weiqi Wu, Tianyang Xu, Jiameng Zhang, Hai Zhao*

**Main category:** cs.CL

**Keywords:** Interactive Drama, LLM, Immersion, Agency, Storytelling

**Relevance Score:** 8

**TL;DR:** This paper presents LLM-based Interactive Drama, improving immersion and agency in AI-driven storytelling through novel methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance the user experience in interactive drama by focusing on immersion and agency, which have previously been underexplored.

**Method:** The authors propose Playwriting-guided Generation to create structured stories, and Plot-based Reflection for LLM agents to improve their responses to player intentions.

**Key Contributions:**

	1. Introduction of Playwriting-guided Generation for better story structure.
	2. Development of Plot-based Reflection for LLM agents to improve player interaction.
	3. Empirical evaluation demonstrating enhanced immersion and agency.

**Result:** The evaluation shows significant improvements in immersion and agency through the proposed methods as assessed by human judges.

**Limitations:** The evaluation is based on human judgment which may introduce subjectivity; further quantitative metrics could strengthen the findings.

**Conclusion:** The proposed methods advance the capabilities of LLMs in interactive storytelling, enhancing player engagement and experience.

**Abstract:** LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the user (i.e. the player) plays the role of a character in the story, has conversations with characters played by LLM agents, and experiences an unfolding story. This paper begins with understanding interactive drama from two aspects: Immersion, the player's feeling of being present in the story, and Agency, the player's ability to influence the story world. Both are crucial to creating an enjoyable interactive experience, while they have been underexplored in previous work. To enhance these two aspects, we first propose Playwriting-guided Generation, a novel method that helps LLMs craft dramatic stories with substantially improved structures and narrative quality. Additionally, we introduce Plot-based Reflection for LLM agents to refine their reactions to align with the player's intentions. Our evaluation relies on human judgment to assess the gains of our methods in terms of immersion and agency.

</details>


### [180] [DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers](https://arxiv.org/abs/2502.18460)

*Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen*

**Main category:** cs.CL

**Keywords:** dense retrievers, large language models, contrastive learning, multilingual retrieval, long-context capabilities

**Relevance Score:** 8

**TL;DR:** DRAMA is a training framework that effectively combines large language models with smaller dense retrievers to enhance performance and efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the computational challenges associated with large language models in retrieval tasks, while improving generalization with smaller models.

**Method:** The approach trains smaller dense retrievers using LLMs as a backbone, employing a single-stage contrastive learning on LLM-augmented data.

**Key Contributions:**

	1. Introduction of the DRAMA training framework
	2. Utilization of pruned LLMs to enhance smaller retrievers
	3. Demonstrated superior performance across tasks and languages compared to traditional methods.

**Result:** DRAMA outperforms traditional retrieval models in multilingual scenarios and long-context applications, demonstrating enhanced efficiency and generalization.

**Limitations:** 

**Conclusion:** The framework bridges the gap between model performance and resource efficiency, making dense retrieval more viable in practical applications.

**Abstract:** Large language models (LLMs) have demonstrated strong effectiveness and robustness while fine-tuned as dense retrievers. However, their large parameter size brings significant inference time computational challenges, including high encoding costs for large-scale corpora and increased query latency, limiting their practical deployment. While smaller retrievers offer better efficiency, they often fail to generalize effectively with limited supervised fine-tuning data. In this work, we introduce DRAMA, a training framework that leverages LLMs to train smaller generalizable dense retrievers. In particular, we adopt pruned LLMs as the backbone and train on diverse LLM-augmented data in a single-stage contrastive learning setup. Experiments show that DRAMA offers better multilingual and long-context capabilities than traditional encoder-based retrievers, and achieves strong performance across multiple tasks and languages. These highlight the potential of connecting the training of smaller retrievers with the growing advancements in LLMs, bridging the gap between efficiency and generalization.

</details>


### [181] [PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation](https://arxiv.org/abs/2502.19756)

*Nathan Roll*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Learning, Gradient-Based Search, PolyPrompt, MMLU Benchmark

**Relevance Score:** 8

**TL;DR:** PolyPrompt enhances multilingual capabilities of LLMs using a parameter-efficient framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs show inconsistent performance in multilingual contexts, prompting the need for improved methods.

**Method:** PolyPrompt learns language-specific trigger tokens through a gradient-based search, appending them to prompts based on the identified language during inference.

**Key Contributions:**

	1. Introduction of PolyPrompt framework for LLMs
	2. Use of gradient-based search for trigger token learning
	3. Demonstrated multilingual accuracy gains on a diverse language benchmark

**Result:** Experiments show accuracy improvements of 3.7%-19.9% on the global MMLU benchmark across fifteen languages compared to naive and translation-pipeline methods.

**Limitations:** 

**Conclusion:** PolyPrompt effectively increases the multilingual accuracy of LLMs, addressing significant performance gaps.

**Abstract:** Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.

</details>


### [182] [Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking](https://arxiv.org/abs/2502.20129)

*Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Transformer models, state tracking

**Relevance Score:** 8

**TL;DR:** The paper evaluates the effectiveness of Chain-of-Thought (CoT) in Transformer models, highlighting its state tracking capabilities and discovering the role of late-layer MLP neurons.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms behind Transformer+CoT and its effectiveness in learning state tracking algorithms.

**Method:** Evaluated Transformer+CoT's state tracking capabilities, identified key circuits in the model, and proposed metrics to analyze neuron performance.

**Key Contributions:**

	1. Evaluation of state tracking in Transformer+CoT
	2. Identification of late-layer MLP neuron role
	3. Proposing metrics for analyzing model circuits

**Result:** Demonstrated that late-layer MLP neurons play a crucial role in state tracking, achieving nearly 100% accuracy in implicit finite state automata (FSAs) identification.

**Limitations:** 

**Conclusion:** Transformer+CoT can learn robust algorithms that are resilient in challenging scenarios, suggesting a strong mechanistic understanding of its capabilities.

**Abstract:** Chain-of-thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. Our key contributions are: (1) We evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit (a subset of model components, responsible for tracking the world state), indicating that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three challenging settings: skipping intermediate steps, introducing data noises, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting its resilience in challenging scenarios. Our code is available at https://github.com/IvanChangPKU/FSA.

</details>


### [183] [Unnatural Languages Are Not Bugs but Features for LLMs](https://arxiv.org/abs/2503.01926)

*Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, J. Zico Kolter, Michael Qizhe Shieh*

**Main category:** cs.CL

**Keywords:** Large Language Models, unnatural languages, human-computer interaction, text processing, semantic meaning

**Relevance Score:** 7

**TL;DR:** This paper investigates the utility of unnatural languages in Large Language Models (LLMs), demonstrating that they have latent features useful for various tasks and can match the performance of models trained on natural languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study challenges the notion that LLMs should not process non-human-readable text sequences, seeking to understand the latent features of unnatural languages.

**Method:** A systematic investigation comparing the performance of LLMs trained on unnatural languages to those trained on natural languages, with evaluations on Length-controlled AlpacaEval 2.0.

**Key Contributions:**

	1. Demonstrated that unnatural languages have latent features usable by LLMs.
	2. Showed that models trained on unnatural instruction datasets can perform on par with those trained on natural language.
	3. Provided comprehensive analysis on how LLMs process and infer meaning from unnatural text.

**Result:** Models fine-tuned on unnatural versions of instruction datasets achieved performance comparable to those trained on natural language, with 49.71 win rates on average across different models.

**Limitations:** Limited to specific LLM architectures and datasets; further research needed to generalize findings across all model types.

**Conclusion:** Unnatural languages contain interpretable features for LLMs, and models can effectively process these languages while filtering out noise to extract contextual meanings.

**Abstract:** Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.

</details>


### [184] [Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent](https://arxiv.org/abs/2503.02519)

*Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Rollback Mechanism, Decision-Making

**Relevance Score:** 8

**TL;DR:** This paper presents GA-Rollback, a new framework for improving decision-making in LLM agents by allowing for rollback on incorrect actions, thus mitigating error propagation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the issue of error propagation in LLM agents that arises from the one-pass decision-making process where incorrect intermediate thoughts affect outcomes.

**Method:** The proposed GA-Rollback framework utilizes a generator to take actions and an assistant to evaluate those actions, allowing rollbacks on detected mistakes to improve overall decision-making.

**Key Contributions:**

	1. Introduction of the GA-Rollback framework for LLM decision-making
	2. Implementation of a rollback mechanism to correct errors in real-time
	3. Demonstration of improved performance on benchmark tasks compared to existing methods.

**Result:** GA-Rollback shows significant performance enhancements over strong baselines across three standard benchmarks in LLM decision-making tasks.

**Limitations:** 

**Conclusion:** GA-Rollback is positioned as an effective, robust, and modular addition that can enhance existing LLM methodologies without extensive modifications.

**Abstract:** Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.

</details>


### [185] [Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence](https://arxiv.org/abs/2503.05037)

*Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** Dense Retrieval, Information Retrieval, Retrieval-Augmented Generation, Heuristic Biases, Performance Analysis

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of heuristic biases in dense retrieval models used in Information Retrieval applications, particularly Retrieval-Augmented Generation, revealing significant vulnerabilities in the selection of documents by these models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and quantify the impact of heuristic biases on retrieval models to prevent downstream failures in applications like RAG.

**Method:** The authors repurpose a relation extraction dataset to conduct controlled experiments and analyze the performance of the retrievers Dragon+ and Contriever under various biases.

**Key Contributions:**

	1. Quantified the impact of heuristic biases in dense retrieval models.
	2. Demonstrated significant performance degradation due to the combination of biases.
	3. Highlighted the implications of biases on downstream AI applications like RAG.

**Result:** The experiments reveal that retrievers show a strong preference for shorter documents and biases that cause catastrophic performance degradation, with the ability to select the correct answer-containing document dropping to less than 10% under combined biases.

**Limitations:** The study focuses on specific heuristic biases and may not cover all possible biases in retrieval models.

**Conclusion:** Biases in retrieval models not only affect their performance but also have a detrimental impact on downstream applications, leading to significant performance drops in LLM outcomes when biased documents are prioritized.

**Abstract:** Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid downstream failures. In this work, we repurpose a relation extraction dataset (e.g., Re-DocRED) to design controlled experiments that quantify the impact of heuristic biases, such as a preference for shorter documents, on retrievers like Dragon+ and Contriever. We uncover major vulnerabilities, showing retrievers favor shorter documents, early positions, repeated entities, and literal matches, all while ignoring the answer's presence! Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 10% of cases over a synthetic biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than providing no documents at all. https://huggingface.co/datasets/mohsenfayyaz/ColDeR

</details>


### [186] [OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses](https://arxiv.org/abs/2503.10927)

*Angela Lopez-Cardona, Sebastian Idesis, Miguel Barreda-Ángeles, Sergi Abadal, Ioannis Arapakis*

**Main category:** cs.CL

**Keywords:** eye-tracking, Large Language Models, cognitive processing, alignment methods, human preferences

**Relevance Score:** 9

**TL;DR:** This paper introduces OASST-ETC, a novel eye-tracking corpus that captures reading patterns from participants evaluating LLM-generated responses, revealing distinct cognitive processing differences and correlations between human preferences and transformer model attention patterns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of aligning Large Language Models (LLMs) with human preferences by utilizing eye-tracking data to better understand cognitive processing during reading.

**Method:** The study involves the collection of eye-tracking data from 24 participants while they evaluate LLM-generated responses from the OASST1 dataset, analyzing patterns and correlations with synthetic eye-tracking data.

**Key Contributions:**

	1. Introduction of the OASST-ETC eye-tracking corpus
	2. Analysis of reading patterns related to LLM responses
	3. Correlation findings between human preferences and model attention patterns

**Result:** Distinct reading patterns were observed between preferred and non-preferred LLM responses, with stronger correlations found between human reading measures and attention from transformer models for preferred responses.

**Limitations:** 

**Conclusion:** The work provides a unique dataset for human cognitive processing study in LLM evaluation and proposes future directions for integrating eye-tracking data into LLM alignment methods.

**Abstract:** While Large Language Models (LLMs) have significantly advanced natural language processing, aligning them with human preferences remains an open challenge. Although current alignment methods rely primarily on explicit feedback, eye-tracking (ET) data offers insights into real-time cognitive processing during reading. In this paper, we present OASST-ETC, a novel eye-tracking corpus capturing reading patterns from 24 participants, while evaluating LLM-generated responses from the OASST1 dataset. Our analysis reveals distinct reading patterns between preferred and non-preferred responses, which we compare with synthetic eye-tracking data. Furthermore, we examine the correlation between human reading measures and attention patterns from various transformer-based models, discovering stronger correlations in preferred responses. This work introduces a unique resource for studying human cognitive processing in LLM evaluation and suggests promising directions for incorporating eye-tracking data into alignment methods. The dataset and analysis code are publicly available.

</details>


### [187] [The time scale of redundancy between prosody and linguistic context](https://arxiv.org/abs/2503.11630)

*Tamar I. Regev, Chiebuka Ohams, Shaylee Xie, Lukas Wolf, Evelina Fedorenko, Alex Warstadt, Ethan G. Wilcox, Tiago Pimentel*

**Main category:** cs.CL

**Keywords:** prosody, spoken communication, context integration, language production, predictive processing

**Relevance Score:** 4

**TL;DR:** The paper investigates the role of prosody in spoken communication, focusing on how prosodic features are influenced by past and future context lengths during speech.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the interaction between words and prosodic features can enhance insights into how non-verbal signals contribute to effective communication.

**Method:** The study systematically examines the relationship between prosodic features of words and their preceding and following contexts, analyzing varying time scales of 3-8 words for past context and 1-2 words for future context.

**Key Contributions:**

	1. Demonstrates the need for extended past contexts to predict prosody reliably.
	2. Identifies distinct roles of prosody in integrating past contexts and in predicting future words.
	3. Highlights the redundancy between prosody and future words only within short contexts.

**Result:** The research finds that prosodic features require extended past contexts for reliable prediction and show redundancy with short future contexts, indicating distinct roles of prosody in facilitating communication.

**Limitations:** The study focuses on specific time scales and does not consider all possible aspects of communication signaling.

**Conclusion:** Prosody adds unique information locally and aids in predicting upcoming speech, suggesting its importance in language processing.

**Abstract:** In spoken communication, information is transmitted not only via words, but also through a rich array of non-verbal signals, including prosody--the non-segmental auditory features of speech. Do these different communication channels carry distinct information? Prior work has shown that the information carried by prosodic features is substantially redundant with that carried by the surrounding words. Here, we systematically examine the time scale of this relationship, studying how it varies with the length of past and future contexts. We find that a word's prosodic features require an extended past context (3-8 words across different features) to be reliably predicted. Given that long-scale contextual information decays in memory, prosody may facilitate communication by adding information that is locally unique. We also find that a word's prosodic features show some redundancy with future words, but only with a short scale of 1-2 words, consistent with reports of incremental short-term planning in language production. Thus, prosody may facilitate communication by helping listeners predict upcoming material. In tandem, our results highlight potentially distinct roles that prosody plays in facilitating integration of words into past contexts and in helping predict upcoming words.

</details>


### [188] [Splintering Nonconcatenative Languages for Better Tokenization](https://arxiv.org/abs/2503.14433)

*Bar Gazit, Shaltiel Shmidman, Avi Shmidman, Yuval Pinter*

**Main category:** cs.CL

**Keywords:** tokenization, morphology, SPLINTER, BERT, NLP

**Relevance Score:** 7

**TL;DR:** SPLINTER is introduced as a pre-processing technique for better tokenization in languages with nonconcatenative morphologies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Common tokenization algorithms fail to adequately process languages with unique morphological structures, leading to suboptimal performance.

**Method:** SPLINTER rearranges text into a linear form to enhance the tokenization process for languages like Hebrew, Arabic, and Malay.

**Key Contributions:**

	1. Introduction of SPLINTER for nonconcatenative morphologies
	2. Demonstration of intrinsic measures for token vocabularies
	3. Evaluation on downstream BERT tasks in Hebrew

**Result:** SPLINTER shows improved token vocabularies and effective performance in downstream tasks related to Hebrew when evaluated against BERT models.

**Limitations:** 

**Conclusion:** By utilizing SPLINTER, tokenization becomes more effective for languages with complex morphological patterns, enhancing NLP model performance.

**Abstract:** Common subword tokenization algorithms like BPE and UnigramLM assume that text can be split into meaningful units by concatenative measures alone. This is not true for languages such as Hebrew and Arabic, where morphology is encoded in root-template patterns, or Malay and Georgian, where split affixes are common. We present SPLINTER, a pre-processing step which rearranges text into a linear form that better represents such nonconcatenative morphologies, enabling meaningful contiguous segments to be found by the tokenizer. We demonstrate SPLINTER's merit using both intrinsic measures evaluating token vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using BERT-architecture models trained for Hebrew.

</details>


### [189] [Meta-Learning Neural Mechanisms rather than Bayesian Priors](https://arxiv.org/abs/2503.16048)

*Michael Goodale, Salvador Mascarenhas, Yair Lakretz*

**Main category:** cs.CL

**Keywords:** meta-learning, neural networks, formal languages, cognitive mechanics, symbolic theories

**Relevance Score:** 7

**TL;DR:** The paper investigates meta-learning in neural networks and formal languages, revealing that it teaches neural mechanisms rather than simplicity-based priors, with implications for efficient learning paradigms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand what meta-learning imparts to neural network models and how they learn from formal languages.

**Method:** The authors explored meta-training on datasets organized by simplicity and assessed the resulting models' capabilities on downstream tasks.

**Key Contributions:**

	1. Demonstrated that meta-training imparts neural mechanisms to models.
	2. Showed that training on one formal language can be as beneficial as training on many.
	3. Provided insights linking symbolic theories with neural mechanisms.

**Result:** Meta-trained models were found to learn cognitive primitives like counters, achieving significant performance improvements even with training on a single formal language.

**Limitations:** 

**Conclusion:** The findings challenge previous assumptions about meta-learning, highlighting its potential for linking symbolic theories with neural mechanisms and improving meta-learning strategies.

**Abstract:** Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.

</details>


### [190] [Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility](https://arxiv.org/abs/2503.17579)

*Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt*

**Main category:** cs.CL

**Keywords:** large language models,  language processing,  human-like behavior

**Relevance Score:** 9

**TL;DR:** This paper investigates whether large language models (LLMs) reflect human-like patterns in language processing, specifically focusing on the production-interpretation distinction in pronoun usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the similarities and differences between LLMs and human language processing, particularly regarding how pronouns are produced and interpreted.

**Method:** The researchers examine the asymmetry in pronoun production and interpretation related to implicit causality verbs in both humans and instruction-tuned LLMs, assessing their performance through various meta-linguistic prompts.

**Key Contributions:**

	1. Demonstrates that LLMs can reflect human-like production-interpretation asymmetries.
	2. Establishes a relationship between model size and the reflection of human language patterns.
	3. Provides empirical data on LLM performance in language processing tasks.

**Result:** The findings indicate that certain LLMs demonstrate human-like asymmetries in pronoun usage, with larger models being more likely to replicate these patterns.

**Limitations:** The analysis is limited to specific models and types of prompts, and further research is needed across a broader range of models and scenarios.

**Conclusion:** Model size and the type of prompts significantly influence the LLMs' ability to mimic human-like language processing behavior.

**Abstract:** Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available at https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.

</details>


### [191] [Negation: A Pink Elephant in the Large Language Models' Room?](https://arxiv.org/abs/2503.22395)

*Tereza Vrabcová, Marek Kadlčík, Petr Sojka, Michal Štefánik, Michal Spiegel*

**Main category:** cs.CL

**Keywords:** negation, language models, textual entailment, multilingual, LLM robustness

**Relevance Score:** 6

**TL;DR:** This paper introduces two datasets for studying the challenges of negation in language models, revealing key insights about the linguistic properties affecting LLM performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Negations are crucial for logical reasoning but pose significant challenges for large language models (LLMs), which necessitates further investigation.

**Method:** Two new textual entailment datasets, NoFEVER-ML and NoSNLI-ML, were constructed in four languages (English, Czech, German, Ukrainian) to analyze how LLMs handle negation.

**Key Contributions:**

	1. Introduction of new datasets for negation challenges in LLMs
	2. Demonstration of the impact of model size on negation handling
	3. Insights on language-dependent performance in LLMs

**Result:** Larger model sizes may improve handling of negations; model accuracy and robustness are language-dependent, with projective languages outperforming non-projective ones.

**Limitations:** Focus is primarily on negation, which may not address other reasoning challenges in LLMs.

**Conclusion:** The datasets will aid in understanding and improving LLM reasoning about negation and reducing hallucinations in multilingual contexts.

**Abstract:** Negations are key to determining sentence meaning, making them essential for logical reasoning. Despite their importance, negations pose a substantial challenge for large language models (LLMs) and remain underexplored.   We constructed and published two new textual entailment datasets NoFEVER-ML and NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with   examples differing in negation. It allows investigation of the root causes of the negation problem and its exemplification: how popular LLM model properties and language impact their inability to handle negation correctly.   Contrary to previous work, we show that increasing the model size may improve the models' ability to handle negations. Furthermore, we find that both the models' reasoning accuracy and robustness to negation are language-dependent and that the length and explicitness of the premise have an impact on robustness. There is better accuracy in projective language with fixed order, such as English, than in non-projective ones, such as German or Czech.   Our entailment datasets pave the way to further research for explanation and exemplification of the negation problem, minimization of LLM hallucinations, and improvement of LLM reasoning in multilingual settings.

</details>


### [192] [Efficient Annotator Reliability Assessment with EffiARA](https://arxiv.org/abs/2504.00589)

*Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song*

**Main category:** cs.CL

**Keywords:** data annotation, machine learning, transformer models, annotator reliability, EffiARA

**Relevance Score:** 8

**TL;DR:** EffiARA is a novel annotation framework designed to enhance the machine learning annotation process by supporting the entire pipeline and ensuring evaluative insights on annotators and datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a standardized framework for document-level annotation in machine learning, addressing high costs and time consumption associated with data annotation.

**Method:** Introduction of the EffiARA annotation framework, which encompasses resources understanding, annotation dataset compilation, and insights into annotator reliability, plus the release of a Python package and webtool.

**Key Contributions:**

	1. First comprehensive annotation framework supporting all stages of the annotation process
	2. Includes a graphical user interface for easier access
	3. Open-source availability of both Python package and webtool to promote community use.

**Result:** The efficacy of the EffiARA framework is evidenced by improvements in classification performance through reliable labeling and increased agreement among annotators.

**Limitations:** 

**Conclusion:** EffiARA offers an efficient and structured approach to data annotation, with tools that enhance reliability and quality of annotated datasets.

**Abstract:** Data annotation is an essential component of the machine learning pipeline; it is also a costly and time-consuming process. With the introduction of transformer-based models, annotation at the document level is increasingly popular; however, there is no standard framework for structuring such tasks. The EffiARA annotation framework is, to our knowledge, the first project to support the whole annotation pipeline, from understanding the resources required for an annotation task to compiling the annotated dataset and gaining insights into the reliability of individual annotators as well as the dataset as a whole. The framework's efficacy is supported by two previous studies: one improving classification performance through annotator-reliability-based soft-label aggregation and sample weighting, and the other increasing the overall agreement among annotators through removing identifying and replacing an unreliable annotator. This work introduces the EffiARA Python package and its accompanying webtool, which provides an accessible graphical user interface for the system. We open-source the EffiARA Python package at https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at https://effiara.gate.ac.uk.

</details>


### [193] [A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models](https://arxiv.org/abs/2504.08961)

*Kseniia Petukhova, Ekaterina Kochmar*

**Main category:** cs.CL

**Keywords:** Large Language Models, discourse annotation, automated pipelines

**Relevance Score:** 9

**TL;DR:** The paper presents a fully automated pipeline utilizing Large Language Models (LLMs) to create and perform discourse annotation, aiming to improve efficiency and quality in comparison to traditional methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The manual design of tree annotation schemes is time-consuming and necessitates expert knowledge, prompting the need for an automated solution.

**Method:** We developed an automated pipeline that constructs annotation schemes using LLMs and evaluates it against established taxonomies like SWBD-DAMSL, analyzing different design choices to optimize annotation quality.

**Key Contributions:**

	1. Introduction of a fully automated LLM-based discourse annotation pipeline
	2. Demonstration of superior performance over manually designed annotation schemes
	3. Public release of code and data to support ongoing research

**Result:** The automated pipeline, particularly with frequency-guided decision trees and advanced LLMs, outperforms traditionally designed trees and can match or exceed human annotators while significantly reducing annotation time.

**Limitations:** 

**Conclusion:** This approach not only enhances the efficiency of discourse annotation but also maintains or improves quality, with all related resources made publicly available for further research.

**Abstract:** Recent advances in Large Language Models (LLMs) have shown promise in automating discourse annotation for conversations. While manually designing tree annotation schemes significantly improves annotation quality for humans and models, their creation remains time-consuming and requires expert knowledge. We propose a fully automated pipeline that uses LLMs to construct such schemes and perform annotation. We evaluate our approach on speech functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our experiments compare various design choices, and we show that frequency-guided decision trees, paired with an advanced LLM for annotation, can outperform previously manually designed trees and even match or surpass human annotators while significantly reducing the time required for annotation. We release all code and resultant schemes and annotations to facilitate future research on discourse annotation.

</details>


### [194] [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)

*Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, diffusion models, reasoning, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper proposes d1, a framework for enhancing reasoning capabilities in non-autoregressive large language models (dLLMs) through a combination of supervised finetuning and reinforcement learning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore if diffusion-based large language models can leverage recent advances in reasoning observed in autoregressive models, thereby enhancing their performance in reasoning tasks.

**Method:** The authors adapt pre-trained masked dLLMs using a masked supervised finetuning technique for distillation of knowledge and introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO.

**Key Contributions:**

	1. Introduction of the d1 framework for masked dLLMs
	2. Development of the diffu-GRPO RL algorithm
	3. Demonstration of improved performance on reasoning benchmarks

**Result:** Empirical studies demonstrate that the d1 framework significantly improves the reasoning performance of a state-of-the-art dLLM on various mathematical and planning benchmarks.

**Limitations:** 

**Conclusion:** The proposed d1 framework effectively enhances reasoning in masked dLLMs, outperforming previous models in key benchmarks.

**Abstract:** Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM. Our code is released at https://dllm-reasoning.github.io/.

</details>


### [195] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)

*Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri*

**Main category:** cs.CL

**Keywords:** close reading, benchmarking, large language models, interpretive reasoning, literary analysis

**Relevance Score:** 7

**TL;DR:** The paper presents KRISTEVA, a benchmark for evaluating close reading skills in LLMs, uncovering their strengths and weaknesses in interpretive reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of large language models (LLMs) in close reading tasks amidst the lack of suitable evaluation benchmarks.

**Method:** The study introduces KRISTEVA, a close reading benchmark comprising 1331 multiple-choice questions designed to evaluate LLMs on tasks related to literary analysis.

**Key Contributions:**

	1. Introduction of KRISTEVA, the first close reading benchmark for LLM evaluation
	2. Development of three task sets reflecting stages of the close reading process
	3. Empirical results showing LLM accuracy compared to human evaluators

**Result:** LLMs show college-level close reading competency with an accuracy of 49.7% - 69.7%, but human evaluators outperform them in 10 of 11 tasks.

**Limitations:** The benchmark may not cover all aspects of close reading and relies on specific classroom data for questions.

**Conclusion:** LLMs have some understanding of literary works, but their performance in close reading falls short of that of experienced human evaluators.

**Abstract:** Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.

</details>


### [196] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)

*Alexander Gill, Abhilasha Ravichander, Ana Marasović*

**Main category:** cs.CL

**Keywords:** large language models, benchmark creation, reading comprehension, reasoning, evaluation datasets

**Relevance Score:** 7

**TL;DR:** This paper investigates the effectiveness of large language models (LLMs) in generating evaluation benchmarks for reasoning tasks, finding that while LLMs can produce valid datasets at lower costs, these datasets are less challenging than human-authored ones.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs for data generation has created a need for rigorous evaluation benchmarks that effectively test reasoning capabilities and discourage shortcut exploitation.

**Method:** The study involves two case studies that compare LLM-generated versions of high-quality reading comprehension datasets (CondaQA and DROP) to their human-authored counterparts, assessing validity and difficulty.

**Key Contributions:**

	1. Demonstration of LLMs generating valid reasoning benchmarks at lower costs.
	2. Comparison of LLM-generated and human-authored datasets for reading comprehension tasks.
	3. Call for critical reassessment of using LLMs for benchmark creation.

**Result:** LLM-generated datasets often meet annotation guidelines for validity and are cheaper to produce, but they are found to be less challenging for LLMs than human-generated datasets.

**Limitations:** LLM-generated datasets are less challenging than their human-created equivalents, suggesting potential shortcomings in their effectiveness.

**Conclusion:** While LLMs can generate valid evaluation data, the resulting datasets may be insufficiently challenging, prompting a reassessment of LLM benchmarks in evaluation tasks.

**Abstract:** Large language models (LLMs) are increasingly used for data generation. However, creating evaluation benchmarks raises the bar for this emerging paradigm. Benchmarks must target specific phenomena, penalize exploiting shortcuts, and be challenging. Through two case studies, we investigate whether LLMs can meet these demands by generating reasoning over-text benchmarks and comparing them to those created through careful crowdsourcing. Specifically, we evaluate both the validity and difficulty of LLM-generated versions of two high-quality reading comprehension datasets: CondaQA, which evaluates reasoning about negation, and DROP, which targets reasoning about quantities. We find that prompting LLMs can produce variants of these datasets that are often valid according to the annotation guidelines, at a fraction of the cost of the original crowdsourcing effort. However, we show that they are less challenging for LLMs than their human-authored counterparts. This finding sheds light on what may have been lost by generating evaluation data with LLMs, and calls for critically reassessing the immediate use of this increasingly prevalent approach to benchmark creation.

</details>


### [197] [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)

*Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Human Label Variation, Taxonomy, Explanation Generation, e-SNLI

**Relevance Score:** 8

**TL;DR:** The paper introduces LITEX, a taxonomy for categorizing free-text explanations in Natural Language Inference (NLI) to address within-label variation, and demonstrates its effectiveness in generating linguistically closer explanations to humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of within-label variation in Natural Language Inference (NLI), where annotators may agree on labels but diverge in reasoning.

**Method:** A new taxonomy, LITEX, was introduced to systematically categorize free-text explanations, followed by annotation of the e-SNLI dataset, validation of the taxonomy's reliability, and analysis of its alignment with NLI labels, highlights, and explanations.

**Key Contributions:**

	1. Introduction of LITEX taxonomy for free-text explanation categorization in NLI
	2. Systematic annotation and validation of e-SNLI dataset based on the taxonomy
	3. Demonstration of improved explanatory generation by conditioning on LITEX.

**Result:** LITEX annotations showed how different rationales align with the same NLI labels and highlighted the effectiveness of using LITEX for generating explanations that resemble human reasoning more closely than traditional methods.

**Limitations:** 

**Conclusion:** The LITEX taxonomy captures within-label variation in NLI and enhances explanation generation, leading to better alignment between human and model-generated explanations.

**Abstract:** There is increasing evidence of Human Label Variation (HLV) in Natural Language Inference (NLI), where annotators assign different labels to the same premise-hypothesis pair. However, within-label variation--cases where annotators agree on the same label but provide divergent reasoning--poses an additional and mostly overlooked challenge. Several NLI datasets contain highlighted words in the NLI item as explanations, but the same spans on the NLI item can be highlighted for different reasons, as evidenced by free-text explanations, which offer a window into annotators' reasoning. To systematically understand this problem and gain insight into the rationales behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for categorizing free-text explanations. Using this taxonomy, we annotate a subset of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it aligns with NLI labels, highlights, and explanations. We further assess the taxonomy's usefulness in explanation generation, demonstrating that conditioning generation on LITEX yields explanations that are linguistically closer to human explanations than those generated using only labels or highlights. Our approach thus not only captures within-label variation but also shows how taxonomy-guided generation for reasoning can bridge the gap between human and model explanations more effectively than existing strategies.

</details>


### [198] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)

*Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi*

**Main category:** cs.CL

**Keywords:** backdoor attacks, test set contamination, large language models, reproducibility, transparency

**Relevance Score:** 6

**TL;DR:** DyePack is a framework designed to detect if models have been contaminated by benchmark test sets during training using backdoor attacks without needing model internals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of test set contamination in large language models which compromises reproducibility and transparency in evaluations.

**Method:** DyePack mixes backdoor samples with test data and employs multiple backdoors with stochastic targets to compute false positive rates (FPR) for flagged models.

**Key Contributions:**

	1. Introduction of the DyePack framework for test set contamination detection.
	2. Use of backdoor attacks to flag models without internal access.
	3. Evaluation across multiple models and datasets ensuring low false positive rates.

**Result:** DyePack successfully detects all contaminated models in multiple-choice and open-ended tasks with extremely low false positive rates, ensuring reliable identification of contamination.

**Limitations:** The framework's effectiveness may vary with the complexity of the models used or the nature of the datasets.

**Conclusion:** DyePack provides a principled method for identifying test set contamination, maintaining low false positive rates and offering strong evidence for flagged models.

**Abstract:** Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.

</details>


### [199] [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)

*Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang*

**Main category:** cs.CL

**Keywords:** LLM alignment, preference data, GPT-4o, data efficiency, diagnosis tools

**Relevance Score:** 9

**TL;DR:** The paper introduces the Alignment Data Map, a tool to efficiently analyze preference data for LLM alignment, significantly improving data collection by identifying high-quality samples without explicit annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the high costs and inefficiencies in collecting human preference data necessary for aligning LLMs with human values.

**Method:** Utilizing GPT-4o to compute alignment scores for LLM responses based on existing preference datasets, leading to the formation of an Alignment Data Map which shows data's mean and variance.

**Key Contributions:**

	1. Introduction of the Alignment Data Map for LLM preference analysis
	2. Demonstration of improved efficiency in data collection using reduced datasets
	3. Capability to diagnose preference datasets for quality.

**Result:** With only 33% of the data, specifically from high-mean, low-variance regions, the tool achieves comparable or superior performance to using the entire dataset.

**Limitations:** 

**Conclusion:** The Alignment Data Map enhances data collection efficiency and can also diagnose existing preference datasets, identifying low-impact or misannotated samples.

**Abstract:** Human preference data plays a critical role in aligning large language models (LLMs) with human values. However, collecting such data is often expensive and inefficient, posing a significant scalability challenge. To address this, we introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we compute alignment scores for LLM-generated responses to instructions from existing preference datasets. These scores are then used to construct an Alignment Data Map based on their mean and variance. Our experiments show that using only 33 percent of the data, specifically samples in the high-mean, low-variance region, achieves performance comparable to or better than using the entire dataset. This finding suggests that the Alignment Data Map can significantly improve data collection efficiency by identifying high-quality samples for LLM alignment without requiring explicit annotations. Moreover, the Alignment Data Map can diagnose existing preference datasets. Our analysis shows that it effectively detects low-impact or potentially misannotated samples. Source code is available online.

</details>


### [200] [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)

*Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Human Label Variation, Evaluation Framework, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel LLM-based pipeline for enhancing answer option evaluation through forward reasoning with chains of thought (CoTs), improving alignment with human label distributions.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address human label variation in multiple annotators and improve model predictions alignment with human labeling through better reasoning processes.

**Method:** The proposed pipeline uses reasoning-tuned LLMs to generate chains of thought for each answer option and employs discourse segmenters to extract supporting and opposing statements, followed by a rank-based evaluation framework.

**Key Contributions:**

	1. Development of an LLM-based pipeline utilizing chains of thought for answer evaluation.
	2. Introduction of a rank-based evaluation framework that emphasizes ranking over exact scores.
	3. Demonstration of improved accuracy and alignment with human label distributions across multiple datasets.

**Result:** The method outperforms previous direct generation techniques and baselines across three datasets, leading to better alignment of ranked answers with human judgments.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates significant improvements in reasoning accuracy and human alignment for labeling tasks.

**Abstract:** The recent rise of reasoning-tuned Large Language Models (LLMs)--which generate chains of thought (CoTs) before giving the final answer--has attracted significant attention and offers new opportunities for gaining insights into human label variation, which refers to plausible differences in how multiple annotators label the same data instance. Prior work has shown that LLM-generated explanations can help align model predictions with human label distributions, but typically adopt a reverse paradigm: producing explanations based on given answers. In contrast, CoTs provide a forward reasoning path that may implicitly embed rationales for each answer option, before generating the answers. We thus propose a novel LLM-based pipeline enriched with linguistically-grounded discourse segmenters to extract supporting and opposing statements for each answer option from CoTs with improved accuracy. We also propose a rank-based HLV evaluation framework that prioritizes the ranking of answers over exact scores, which instead favor direct comparison of label distributions. Our method outperforms a direct generation method as well as baselines on three datasets, and shows better alignment of ranking methods with humans, highlighting the effectiveness of our approach.

</details>


### [201] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)

*Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

**Main category:** cs.CL

**Keywords:** theorem proving, large language models, reinforcement learning, mathematical reasoning, benchmark datasets

**Relevance Score:** 9

**TL;DR:** DeepTheorem is a novel informal theorem-proving framework that enhances LLM mathematical reasoning through natural language, featuring a large benchmark dataset and a reinforcement learning strategy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional automated theorem proving approaches do not align well with the informal knowledge that large language models (LLMs) acquire during pre-training, limiting their mathematical reasoning capabilities.

**Method:** The framework includes a comprehensive dataset with 121K informal theorems and proofs, and employs a novel reinforcement learning strategy (RL-Zero) tailored for informal theorem proving.

**Key Contributions:**

	1. Introduction of a large-scale benchmark dataset for informal theorem proving.
	2. Development of a novel reinforcement learning strategy for enhancing mathematical reasoning in LLMs.
	3. Comprehensive evaluation metrics for assessing proof correctness and reasoning quality.

**Result:** DeepTheorem significantly improves LLM theorem-proving performance, achieving state-of-the-art accuracy and reasoning quality after extensive experiments.

**Limitations:** 

**Conclusion:** DeepTheorem has the potential to advance automated informal theorem proving and facilitate deeper mathematical exploration.

**Abstract:** Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration.

</details>
