# 2025-07-25

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 35]

- [cs.CL](#cs.CL) [Total: 62]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)

*Liang Zhang, Xiaoming Zhai, Jionghao Lin, Jionghao Lin, Jennifer Kleiman, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, Xiangen Hu, Arthur C. Graesser*

**Main category:** cs.HC

**Keywords:** Large Language Model, AI education, communication strategies, collaborative problem-solving, peer-to-peer collaboration

**Relevance Score:** 7

**TL;DR:** This study evaluates communication strategies among LLM agents in a dual-agent math problem-solving context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective communication strategies among LLM agents to enhance collaborative problem-solving efficiency in AI-aided education.

**Method:** Examined four communication modes in a dual-agent, chat-based problem-solving environment using the OpenAI GPT-4o model on the MATH dataset.

**Key Contributions:**

	1. Systematic evaluation of communication strategies among LLM agents
	2. Identification of peer-to-peer collaboration as the most effective strategy
	3. Insights into the role of dialogue acts in collaborative problem-solving

**Result:** Dual-agent setups outperformed single agents, with peer-to-peer collaboration achieving the highest accuracy.

**Limitations:** 

**Conclusion:** Effective communication strategies are essential for improving complex problem-solving in AI education.

**Abstract:** Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \textit{teacher-student interaction}, \textit{peer-to-peer collaboration}, \textit{reciprocal peer teaching}, and \textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.

</details>


### [2] [A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians](https://arxiv.org/abs/2507.17754)

*Justin Morse, Kurt Gilbert, Kyle Shin, Rick Cooke, Peyton Rose, Jack Sullivan, Angelo Sisante*

**Main category:** cs.HC

**Keywords:** Ambient Scribes, EHR Integration, Clinician Burnout, AI in Healthcare, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents an ambient medical scribe application integrated into an EHR system that uses AI to assist clinicians by generating high-quality notes, resulting in reduced cognitive load and documentation burden.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing clinician burnout necessitated the development of automated solutions to alleviate administrative burdens in healthcare settings.

**Method:** An ambient scribe application was developed and integrated into the EHR system, utilizing Whisper for transcription and a modular in-context learning pipeline with GPT-4o for generating SOAP notes and patient instructions.

**Key Contributions:**

	1. Introduction of a custom ambient scribe application integrated into EHR systems.
	2. Demonstrated improved note quality compared to expert-written notes.
	3. Survey results showing significant reductions in cognitive load and documentation burden for clinicians.

**Result:** The generated notes surpassed the quality of expert-written notes, with 94% of clinicians reporting reduced cognitive load and 97% reporting less documentation burden. Post-processing with a fine-tuned BART model improved note conciseness.

**Limitations:** 

**Conclusion:** The findings demonstrate the effectiveness of AI systems in supporting clinicians and enhancing the quality of care by reducing administrative tasks.

**Abstract:** Clinician burnout has motivated the growing adoption of ambient medical scribes in the clinic. In this work, we introduce a custom-built ambient scribe application integrated into the EHR system at Included Health, a personalized all-in-one healthcare company offering telehealth services. The application uses Whisper for transcription and a modular in-context learning pipeline with GPT-4o to automatically generate SOAP notes and patient instructions. Testing on mock visit data shows that the notes generated by the application exceed the quality of expert-written notes as determined by an LLM-as-a-judge. The application has been widely adopted by the clinical practice, with over 540 clinicians at Included Health using the application at least once. 94% (n = 63) of surveyed clinicians report reduced cognitive load during visits and 97% (n = 66) report less documentation burden when using the application. Additionally, we show that post-processing notes with a fine-tuned BART model improves conciseness. These findings highlight the potential for AI systems to ease administrative burdens and support clinicians in delivering efficient, high-quality care.

</details>


### [3] [Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image](https://arxiv.org/abs/2507.17755)

*Jianfeng Lan, Yingjia Huang*

**Main category:** cs.HC

**Keywords:** social media, body image, Douyin, WeChat, adolescents

**Relevance Score:** 3

**TL;DR:** This study investigates how Douyin and WeChat affect body image perceptions among Chinese male adolescents, finding Douyin's content has a significant impact.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the influence of social media platforms on body image perceptions among adolescents, particularly focusing on the differences between Douyin and WeChat.

**Method:** A survey of 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction.

**Key Contributions:**

	1. Identifies the differing impacts of Douyin and WeChat on body image among adolescents.
	2. Highlights the importance of platform characteristics in shaping psychological outcomes.
	3. Offers insights for addressing body image concerns related to social media usage.

**Result:** Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat shows no significant correlation with body image dimensions.

**Limitations:** Study focused solely on male adolescents in China; findings may not generalize to other demographics or regions.

**Conclusion:** The findings indicate that Douyinâ€™s algorithm-driven environment intensifies exposure to idealized body standards, influencing cognitive perceptions of body image.

**Abstract:** In the digital era, social media platforms play a pivotal role in shaping adolescents' body image perceptions. This study examines how Douyin and WeChat, two contrasting Chinese social media platforms, influence body image among Chinese male adolescents. Employing a platformization perspective, we surveyed 395 male adolescents aged 10 to 24 using the Multidimensional Body-Self Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation and body satisfaction. Our findings reveal that Douyin usage is significantly correlated with appearance evaluation and body area satisfaction, while WeChat usage shows no significant correlation with any body image dimensions. These results suggest that Douyin's algorithm-driven, video-centric environment intensifies exposure to idealized body standards, impacting users at a cognitive level. This study underscores the importance of considering platform-specific characteristics in understanding social media's impact on body image. It contributes to the broader discourse on how technological design and content modalities mediate psychological outcomes, offering insights for addressing body image concerns among male adolescents in China.

</details>


### [4] [Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy](https://arxiv.org/abs/2507.17756)

*Josh Hunter, John McDermid, Simon Burton*

**Main category:** cs.HC

**Keywords:** railway safety, automation, assistive technologies, human factors, system of systems

**Relevance Score:** 3

**TL;DR:** This study examines how railway professionals perceive safety, discussing the integration of human, systematic, and technological elements for future automation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To inform future technological developments in railway safety by understanding professionals' perspectives.

**Method:** Interviews with drivers, route planners, and administrative personnel were conducted to gather data on safety practices and automation.

**Key Contributions:**

	1. Investigates professional perceptions of safety in railways
	2. Identifies the need for a railway-specific causation model
	3. Emphasizes the importance of assistive technologies over full automation

**Result:** Findings indicate a cautious approach to automation, a preference for assistive technologies, and a complex understanding of safety in rail systems.

**Limitations:** Challenges exist in transferring automotive automation tactics to railway systems.

**Conclusion:** The study identifies the need for a railway-specific causation model for better safety evaluation and highlights limitations in applying automotive automation to railways.

**Abstract:** This study investigates how railway professionals perceive safety as a concept within rail, with the intention to help inform future technological developments within the industry. Through a series of interviews with drivers, route planners,and administrative personnel, the research explores the currentstate of safety practices, the potential for automation and the understanding of the railway as a system of systems. Key findings highlight a cautious attitude towards automation, a preference for assistive technologies, and a complex understanding of safety that integrates human, systematic and technological factors. The study also addresses the limitations of transferring automotive automation technologies to railways and the need for a railway-specific causation model to better evaluate and enhance safety in an evolving technological landscape. This study aims to bridge thegap between contemporary research and practical applications, contributing to the development of more effective safety metrics.

</details>


### [5] [BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches](https://arxiv.org/abs/2507.17757)

*Sam Gordon James, Miranda Elaine Glynis Armstrong, Aisling Ann O'Kane, Harry Emerson, Zahraa S. Abdallah*

**Main category:** cs.HC

**Keywords:** Type 1 diabetes, smartwatch, dataset, data analysis, mixed-methods research

**Relevance Score:** 6

**TL;DR:** The BrisT1D dataset, developed from a longitudinal study of young adults with Type 1 diabetes using smartwatches, provides both quantitative and qualitative data aimed at improving T1D management.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the real-world use of management technology for Type 1 diabetes and to provide an easily accessible dataset for further research.

**Method:** The dataset was created from a longitudinal study involving 24 young adults in the UK, integrating data from T1D management systems and smartwatches, along with interview transcripts.

**Key Contributions:**

	1. Creation of the BrisT1D dataset combining quantitative and qualitative data.
	2. Facilitation of research on blood glucose and hypoglycemia prediction models.
	3. Enhancement of mixed-methods studies in chronic disease management.

**Result:** The dataset includes processed and raw device data, supporting various applications such as blood glucose and hypoglycemia prediction, as well as closed-loop algorithm development.

**Limitations:** 

**Conclusion:** The BrisT1D dataset has significant potential for mixed-methods research into T1D management and the role of smartwatches, enhancing understanding of user experiences and improving predictive algorithms.

**Abstract:** Background: Type 1 diabetes (T1D) has seen a rapid evolution in management technology and forms a useful case study for the future management of other chronic conditions. Further development of this management technology requires an exploration of its real-world use and the potential of additional data streams. To facilitate this, we contribute the BrisT1D Dataset to the growing number of public T1D management datasets. The dataset was developed from a longitudinal study of 24 young adults in the UK who used a smartwatch alongside their usual T1D management. Findings: The BrisT1D dataset features both device data from the T1D management systems and smartwatches used by participants, as well as transcripts of monthly interviews and focus groups conducted during the study. The device data is provided in a processed state, for usability and more rapid analysis, and in a raw state, for in-depth exploration of novel insights captured in the study. Conclusions: This dataset has a range of potential applications. The quantitative elements can support blood glucose prediction, hypoglycaemia prediction, and closed-loop algorithm development. The qualitative elements enable the exploration of user experiences and opinions, as well as broader mixed-methods research into the role of smartwatches in T1D management.

</details>


### [6] [DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation](https://arxiv.org/abs/2507.17759)

*Riddhi Heda, Sidhant Singh, Umair Yasir, Tanmay Jaiswal, Anil Mokhade*

**Main category:** cs.HC

**Keywords:** Digital Hostel Management System, AI, Cloud Infrastructure, Predictive Analytics, User Experience

**Relevance Score:** 4

**TL;DR:** The paper presents a Digital Hostel Management System (DHMS) designed to enhance operational efficiency and student satisfaction in hostel management using modern web technologies, AI, and cloud infrastructure.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies and operational burdens in traditional hostel management systems that do not meet the needs of digitally native students.

**Method:** The paper introduces a modular and integrated platform that automates key hostel management functions such as room allotment, grievance redressal, and communication via a natural language chatbot, using simulation tests for validation.

**Key Contributions:**

	1. Introduction of a modular and integrated Digital Hostel Management System (DHMS)
	2. Use of AI and cloud infrastructure for automating key management functions
	3. Implementation of predictive analytics and sentiment analysis for proactive management

**Result:** DHMS achieved a 92% student satisfaction rate in room allocation and an average chatbot response time below one second during simulation tests.

**Limitations:** Requires further testing for integration across multiple blocks, user acceptance, scalability under load, and ERP compatibility.

**Conclusion:** Further testing for integration, user acceptance, scalability, and ERP compatibility is needed before deploying DHMS campus-wide; the work outlines the system architecture and implementation approach for enhancing user experience and efficiency.

**Abstract:** Traditional hostel management practices in academic institutions often suffer from inefficiencies, delays, and fragmented communication. These systems fail to meet the expectations of digitally native students and place a significant operational burden on hostel staff. This paper introduces DHMS (Digital Hostel Management System), a modular and integrated platform designed to digitize and streamline essential hostel management functions. DHMS leverages modern web technologies, artificial intelligence, and cloud infrastructure to automate room allotment, grievance redressal, gate pass logistics, and communication via a natural language chatbot. In simulation tests, DHMS achieved a 92% student satisfaction rate in room allocation and maintained an average chatbot response time below one second. Additional features include predictive analytics for proactive maintenance planning and sentiment analysis for feedback processing. While promising, the system requires further testing for integration across multiple hostel blocks, user acceptance, scalability under load, and ERP compatibility before campus-wide deployment. This work discusses the system architecture, implementation approach, and factors critical to improving user experience, administrative efficiency, and decision-making processes.

</details>


### [7] [Co-constructing Explanations for AI Systems using Provenance](https://arxiv.org/abs/2507.17761)

*Jan-Christoph Kalo, Fina Polat, Shubha Guha, Paul Groth*

**Main category:** cs.HC

**Keywords:** AI explainability, data provenance, interactive agent, user simulations, large language model

**Relevance Score:** 8

**TL;DR:** This paper presents an interactive agent designed to help users understand AI system outputs through contextualized explanations grounded in data provenance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** AI systems are complex, and their outputs can be difficult for users to understand. Data provenance often lacks context for users.

**Method:** The paper introduces an interactive agent that collaborates with users to construct explanations. It includes a prototype demonstration and a scalable evaluation framework using user simulations and a large language model as a judge.

**Key Contributions:**

	1. Introduction of an interactive agent for explanation co-construction
	2. Demonstration of a prototype for user interaction
	3. Development of a scalable evaluation framework using simulations and LLMs

**Result:** The initial prototype shows promise in facilitating user understanding of AI system outputs through structured explanations.

**Limitations:** The prototype is in an early stage and may require further development based on user feedback.

**Conclusion:** The interactive agent represents a step towards making data provenance more accessible and meaningful for users, improving the explainability of AI systems.

**Abstract:** Modern AI systems are complex workflows containing multiple components and data sources. Data provenance provides the ability to interrogate and potentially explain the outputs of these systems. However, provenance is often too detailed and not contextualized for the user trying to understand the AI system. In this work, we present our vision for an interactive agent that works together with the user to co-construct an explanation that is simultaneously useful to the user as well as grounded in data provenance. To illustrate this vision, we present: 1) an initial prototype of such an agent; and 2) a scalable evaluation framework based on user simulations and a large language model as a judge approach.

</details>


### [8] [Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems](https://arxiv.org/abs/2507.17774)

*Zhangqi Liu*

**Main category:** cs.HC

**Keywords:** human-AI co-creation, design workflows, creative agents

**Relevance Score:** 8

**TL;DR:** This paper discusses the role of AI, particularly LLMs and multimodal diffusion models, in human-centered design, focusing on co-creation in early-stage design processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI in design workflows necessitates a new understanding of human-centered design, moving beyond automation to a collaborative co-creation model.

**Method:** The paper examines how designers can use LLMs like GPT-4 and multimodal models such as Stable Diffusion as active participants in the design process, engaging in iterative cycles of proposal, critique, and revision.

**Key Contributions:**

	1. Introduction of the human-AI co-creation paradigm
	2. Exploration of LLMs and multimodal models in design
	3. Framework for integrating AI in iterative design processes

**Result:** The investigation reveals that AI can enhance creativity and decision-making in design by fostering collaborative interactions between human designers and AI agents.

**Limitations:** 

**Conclusion:** AI's role is shifting from a tool for efficiency to an interactive collaborator, requiring designers to adapt their workflows for effective human-AI co-creation.

**Abstract:** As artificial intelligence (AI) continues to evolve from a back-end computational tool into an interactive, generative collaborator, its integration into early-stage design processes demands a rethinking of traditional workflows in human-centered design. This paper explores the emergent paradigm of human-AI co-creation, where AI is not merely used for automation or efficiency gains, but actively participates in ideation, visual conceptualization, and decision-making. Specifically, we investigate the use of large language models (LLMs) like GPT-4 and multimodal diffusion models such as Stable Diffusion as creative agents that engage designers in iterative cycles of proposal, critique, and revision.

</details>


### [9] [Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization](https://arxiv.org/abs/2507.17898)

*Connor Scully-Allison, Kevin Menear, Kristin Potter, Andrew McNutt, Katherine E. Isaacs, Dmitry Duplyakin*

**Main category:** cs.HC

**Keywords:** visualization, human-computer interaction, machine learning, supercomputer queue data, notebook-embedded visualization

**Relevance Score:** 8

**TL;DR:** The paper presents a design study of 'Guidepost', a notebook-embedded visualization tool for supercomputer queue data, aimed at serving multiple user groups and their unique tasks.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The limitation of domain-specific visualizations to narrow tasks prompts the exploration of designs that support multiple user groups, enhancing overall utility.

**Method:** A design study approach using personas from HCI and software engineering literature to categorize tasks for scientists, machine learning researchers, and system maintainers, followed by user evaluation with expert analysts.

**Key Contributions:**

	1. Development of 'Guidepost' for multi-user group support in visualizations.
	2. Adaption of personas from HCI for task categorization in visualization design.
	3. Evaluation of visualization effectiveness across different user groups.

**Result:** The visualization successfully enables users to complete shared tasks via point-and-click interactions while supporting individual programmatic analysis needs for unique tasks.

**Limitations:** 

**Conclusion:** Adopting a broader approach in visualization design can satisfy the varying needs of different user groups, thereby enhancing the overall effectiveness of data interaction.

**Abstract:** Domain-specific visualizations sometimes focus on narrow, albeit important, tasks for one group of users. This focus limits the utility of a visualization to other groups working with the same data. While tasks elicited from other groups can present a design pitfall if not disambiguated, they also present a design opportunity -- development of visualizations that support multiple groups. This development choice presents a trade off of broadening the scope but limiting support for the more narrow tasks of any one group, which in some cases can enhance the overall utility of the visualization. We investigate this scenario through a design study where we develop \textit{Guidepost}, a notebook-embedded visualization of supercomputer queue data that helps scientists assess supercomputer queue wait times, machine learning researchers understand prediction accuracy, and system maintainers analyze usage trends. We adapt the use of personas for visualization design from existing literature in the HCI and software engineering domains and apply them in categorizing tasks based on their uniqueness across the stakeholder personas. Under this model, tasks shared between all groups should be supported by interactive visualizations and tasks unique to each group can be deferred to scripting with notebook-embedded visualization design. We evaluate our visualization with nine expert analysts organized into two groups: a "research analyst" group that uses supercomputer queue data in their research (representing the Machine Learning researchers and Jobs Data Analyst personas) and a "supercomputer user" group that uses this data conditionally (representing the HPC User persona). We find that our visualization serves our three stakeholder groups by enabling users to successfully execute shared tasks with point-and-click interaction while facilitating case-specific programmatic analysis workflows.

</details>


### [10] [Automated Brake Onset Detection in Naturalistic Driving Data](https://arxiv.org/abs/2507.17943)

*Shu-Yuan Liu, Johan EngstrÃ¶m, Gustav Markkula*

**Main category:** cs.HC

**Keywords:** automated driving systems, response timing, brake onset estimation, collision avoidance, machine learning

**Relevance Score:** 6

**TL;DR:** The paper presents an algorithm for estimating brake onset in automated driving systems using vehicle longitudinal time series data, addressing the limitations of traditional methods that rely on manual annotation or vehicle control signals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve evaluation techniques for automated driving systems by enabling response timing measurements even when vehicle control signals are unavailable.

**Method:** A piecewise linear acceleration model was developed to automatically estimate brake onset from longitudinal time series data, complemented by a manual annotation method for validation.

**Key Contributions:**

	1. Development of an efficient algorithm for estimating brake onset
	2. Validation against human manual annotation methods
	3. Applicability to diverse driving data sets and scenarios

**Result:** The algorithm's classification performance was validated against human manual annotation using naturalistic collision avoidance data, demonstrating high accuracy.

**Limitations:** The algorithm has certain limitations but is generally efficient and adaptable.

**Conclusion:** Despite some limitations, the algorithm is efficient, generalizable, and applicable across various road user and scenario types.

**Abstract:** Response timing measures play a crucial role in the assessment of automated driving systems (ADS) in collision avoidance scenarios, including but not limited to establishing human benchmarks and comparing ADS to human driver response performance. For example, measuring the response time (of a human driver or ADS) to a conflict requires the determination of a stimulus onset and a response onset. In existing studies, response onset relies on manual annotation or vehicle control signals such as accelerator and brake pedal movements. These methods are not applicable when analyzing large scale data where vehicle control signals are not available. This holds in particular for the rapidly expanding sets of ADS log data where the behavior of surrounding road users is observed via onboard sensors. To advance evaluation techniques for ADS and enable measuring response timing when vehicle control signals are not available, we developed a simple and efficient algorithm, based on a piecewise linear acceleration model, to automatically estimate brake onset that can be applied to any type of driving data that includes vehicle longitudinal time series data. We also proposed a manual annotation method to identify brake onset and used it as ground truth for validation. R2 was used as a confidence metric to measure the accuracy of the algorithm, and its classification performance was analyzed using naturalistic collision avoidance data of both ADS and humans, where our method was validated against human manual annotation. Although our algorithm is subject to certain limitations, it is efficient, generalizable, applicable to any road user and scenario types, and is highly configurable.

</details>


### [11] [Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale](https://arxiv.org/abs/2507.17985)

*Alex Liu, Lief Esbenshade, Shawon Sarkar, Victor Tian, Zachary Zhang, Kevin He, Min Sun*

**Main category:** cs.HC

**Keywords:** Large Language Models, AI in Education, Qualitative Analysis, Human-AI Collaboration, Teacher Development

**Relevance Score:** 8

**TL;DR:** This paper explores the integration of large language models (LLMs) in educational tools and analyzes over 140,000 messages between educators and AI, revealing patterns in their engagement and AI's effectiveness in supporting teaching.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how educators use AI tools in practice and study their interactions with AI at scale.

**Method:** A human-AI collaborative methodology involving a four-phase coding pipeline to analyze educator-AI messages, combined with thematic analysis and model benchmarking.

**Key Contributions:**

	1. Development of a hierarchical codebook aligned with teacher evaluation frameworks.
	2. Demonstration of LLMs' effectiveness in qualitative coding tasks.
	3. Insights into educator engagement patterns and AI-related competencies.

**Result:** LLMs, especially Claude 3.5 Haiku, can effectively support educators in identifying themes and performing qualitative coding tasks, illustrating a significant engagement in enhancing instructional practices and professional development.

**Limitations:** 

**Conclusion:** The study provides insights into the integration of generative AI in education, highlighting emerging AI competencies and offering a scalable model for qualitative research.

**Abstract:** The integration of large language models (LLMs) into educational tools has the potential to substantially impact how teachers plan instruction, support diverse learners, and engage in professional reflection. Yet little is known about how educators actually use these tools in practice and how their interactions with AI can be meaningfully studied at scale. This paper presents a human-AI collaborative methodology for large-scale qualitative analysis of over 140,000 educator-AI messages drawn from a generative AI platform used by K-12 teachers. Through a four-phase coding pipeline, we combined inductive theme discovery, codebook development, structured annotation, and model benchmarking to examine patterns of educator engagement and evaluate the performance of LLMs in qualitative coding tasks. We developed a hierarchical codebook aligned with established teacher evaluation frameworks, capturing educators' instructional goals, contextual needs, and pedagogical strategies. Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably support theme identification, extend human recognition in complex scenarios, and outperform open-weight models in both accuracy and structural reliability. The analysis also reveals substantive patterns in how educators inquire AI to enhance instructional practices (79.7 percent of total conversations), create or adapt content (76.1 percent), support assessment and feedback loop (46.9 percent), attend to student needs for tailored instruction (43.3 percent), and assist other professional responsibilities (34.2 percent), highlighting emerging AI-related competencies that have direct implications for teacher preparation and professional development. This study offers a scalable, transparent model for AI-augmented qualitative research and provides foundational insights into the evolving role of generative AI in educational practice.

</details>


### [12] [Evaluating judgment of spatial correlation in visual displays of scalar field distributions](https://arxiv.org/abs/2507.17997)

*Yayan Zhao, Matthew Berger*

**Main category:** cs.HC

**Keywords:** Visualization, Spatial Correlation, 2D Scalar Fields, Judgment, Human Perception

**Relevance Score:** 5

**TL;DR:** The study investigates how different visual displays affect human identification of spatial correlation in 2D scalar fields, comparing animation and juxtaposed views with various color scales.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how visual design influences human perception of spatial correlation in scalar fields.

**Method:** The study involved experimental design comparing animation-based displays and juxtaposed views, while controlling for distribution characteristics related to spatial correlation and scale discriminability.

**Key Contributions:**

	1. Comparison of animation-based displays versus juxtaposed views
	2. Assessment of visual display impacts on human judgment
	3. Insights into the effects of distribution characteristics on perception of spatial correlation.

**Result:** Results demonstrate how distribution characteristics and visual display types influence judgments on spatial correlation recognition among participants.

**Limitations:** The study may have limitations in generalizability beyond the specific types of displays investigated.

**Conclusion:** The findings underscore the significance of visualization design in effectively conveying spatial correlation information in scalar fields.

**Abstract:** In this work we study the identification of spatial correlation in distributions of 2D scalar fields, presented across different forms of visual displays. We study simple visual displays that directly show color-mapped scalar fields, namely those drawn from a distribution, and whether humans can identify strongly correlated spatial regions in these displays. In this setting, the recognition of correlation requires making judgments on a set of fields, rather than just one field. Thus, in our experimental design we compare two basic visualization designs: animation-based displays against juxtaposed views of scalar fields, along different choices of color scales. Moreover, we investigate the impacts of the distribution itself, controlling for the level of spatial correlation and discriminability in spatial scales. Our study's results illustrate the impacts of these distribution characteristics, while also highlighting how different visual displays impact the types of judgments made in assessing spatial correlation. Supplemental material is available at https://osf.io/zn4qy

</details>


### [13] ["I Would Not Be This Version of Myself Today": Elaborating on the Effects of Eudaimonic Gaming Experiences](https://arxiv.org/abs/2507.18084)

*Nisha Devasia, Georgia Kenderova, Michele Newman, Julie Kientz, Jin Ha Lee*

**Main category:** cs.HC

**Keywords:** eudaimonic gaming, game effects, meaningful experiences, mixed-methods approach, positive outcomes

**Relevance Score:** 7

**TL;DR:** This paper investigates the perceived outcomes of eudaimonic gaming experiences and how various components influence these effects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched effects of eudaimonic gaming, focusing on personal meaningfulness and growth within game experiences.

**Method:** A survey was conducted with 166 respondents recounting meaningful gaming experiences, utilizing a mixed-methods approach to classify effects and identify significant subcomponents.

**Key Contributions:**

	1. Empirical understanding of effects from eudaimonic gaming
	2. Identification of significant subcomponents of gaming experiences
	3. Implications for researchers and practitioners to enhance player outcomes

**Result:** Findings indicate that meaningful gaming experiences can lead to positive reflective, learning, social, health, and career effects.

**Limitations:** 

**Conclusion:** The study extends current theoretical models of eudaimonic gaming experiences and suggests practical implications for promoting positive outcomes in gaming.

**Abstract:** While much of the research in digital games has emphasized hedonic experiences, such as flow, enjoyment, and positive affect, recent years have seen increased interest in eudaimonic gaming experiences, typically mixed-affect and associated with personal meaningfulness and growth. The formation of such experiences in games is theorized to have four constituent elements: motivation, game use, experience, and effects. However, while the first three elements have been relatively well explored in the literature, the effects - and how they may influence positive individual outcomes - have been underexplored thus far. To this end, in this work, we investigate the perceived outcomes of eudaimonic gaming and how different components of the experience influence these effects. We conducted a survey (n = 166) in which respondents recounted meaningful gaming experiences and how they affected their present lives. We used a mixed-methods approach to classify effects and identify significant subcomponents of their formation. We contribute an empirical understanding of how meaningful gaming experiences can lead to positive reflective, learning, social, health, and career effects, extending current theoretical models of eudaimonic gaming experiences and offering implications for how researchers and practitioners might use these findings to promote positive outcomes for players.

</details>


### [14] [Effects of variation in system responsiveness on user performance in virtual environments](https://arxiv.org/abs/2507.18085)

*Benjamin Watson, Neff Walker, William Ribarsky, Victoria Spaulding*

**Main category:** cs.HC

**Keywords:** system responsiveness, virtual environments, human-computer interaction, task performance, visual feedback

**Relevance Score:** 8

**TL;DR:** This paper examines system responsiveness (SR) in virtual environments, focusing on how mean (MSR) and standard deviation (SDSR) affect task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how variability in system responsiveness impacts user performance in virtual environments and improve human-computer interfaces.

**Method:** Three studies are conducted with participants to measure the effects of MSR and SDSR on performance in grasp and placement tasks, using within-subjects designs.

**Key Contributions:**

	1. Detailed examination of system responsiveness (SR) components in virtual environments.
	2. Results indicating specific SDSR thresholds affecting task performance.
	3. Guidance for VE designers on managing SR and SDSR for user interfaces.

**Result:** Results indicate that SDSR only impacts performance when it exceeds 82 ms, with placement tasks being particularly sensitive to SR.

**Limitations:** 

**Conclusion:** Designers of virtual environments should not overly restrict SDSR but adjust SR based on the frequency of required visual feedback for optimal performance.

**Abstract:** System responsiveness (SR) is defined as the elapsed time until a system responds to user control. SR fluctuates over time, so it must be described statistically with mean (MSR) and standard deviation (SDSR). In this paper, we examine SR in virtual environments (VEs), outlining its components and methods of experimental measurement and manipulation. Three studies of MSR and SDSR effects on performance of grasp and placement tasks are then presented. The studies used within-subjects designs with 11, 12, and 10 participants, respectively. Results showed that SDSR affected performance only if it was above 82 ms. Placement required more frequent visual feedback and was more sensitive to SR. We infer that VE designers need not tightly control SDSR and may wish to vary SR control based on required visual feedback frequency. These results may be used to improve the human-computer interface in a wide range of interactive graphical applications, including scientific visualization, training, mental health, and entertainment.

</details>


### [15] [Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality](https://arxiv.org/abs/2507.18151)

*Shizhen Zhang, Shengxin Li, Quan Li*

**Main category:** cs.HC

**Keywords:** ADHD, Mixed Reality, communication support, real-time assistance, user study

**Relevance Score:** 7

**TL;DR:** Understood is a Mixed Reality system for adults with ADHD, aimed at facilitating communication by providing real-time support features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Adults with ADHD face communication challenges due to executive dysfunction and emotional dysregulation, and existing interventions do not adequately support them in daily communication contexts.

**Method:** Formative semi-structured interviews and a design workshop were conducted to identify communication barriers, leading to the development of the Understood system, which includes features like real-time conversation summarization, context-aware word suggestions, and topic shifting detection.

**Key Contributions:**

	1. Introduction of a Mixed Reality system for adult ADHD communication support
	2. Real-time conversation summarization and context-aware word suggestions
	3. Detection and reminding of topic shifts during conversations

**Result:** User studies and expert interviews indicated that Understood supports communication effectively, with high usability, complementing traditional therapeutic interventions.

**Limitations:** The paper does not address long-term effectiveness or the system's impact on different communication contexts.

**Conclusion:** The Understood system provides significant support for adults with ADHD in real-world communication, filling a critical gap left by existing interventions.

**Abstract:** Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience communication challenges, primarily due to executive dysfunction and emotional dysregulation, even after years of social integration. While existing interventions predominantly target children through structured or intrusive methods, adults lack tools that translate clinical strategies into daily communication support. To address this gap, we present Understood, a Mixed Reality (MR) system implemented on Microsoft HoloLens 2, designed to assist adults with ADHD in real-world communication. Through formative semi-structured interviews and a design workshop, we identified critical communication barriers and derived design goals for the system. Understood combines three key features: (1) real-time conversation summarization to reduce cognitive load, (2) context-aware subsequent word suggestions during moments of disfluency, and (3) topic shifting detection and reminding to mitigate off-topic transitions. A within-subjects user study and expert interviews demonstrate that Understood effectively supports communication with high usability, offering a complement to therapist-mediated interventions.

</details>


### [16] [ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent](https://arxiv.org/abs/2507.18165)

*Yuheng Zhao, Xueli Shu, Liwen Fan, Lin Gao, Yu Zhang, Siming Chen*

**Main category:** cs.HC

**Keywords:** visual analytics, proactive assistance, LLM, human-computer interaction, user experience

**Relevance Score:** 9

**TL;DR:** ProactiveVA is a framework that enhances visual analytics by using LLM-assisted agents to provide proactive, context-aware assistance during user interactions, improving the effectiveness of data analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Analysts using visual analytics can become overwhelmed by complexity, which highlights the need for intelligent assistance mechanisms that go beyond reactive help.

**Method:** A formative study analyzed user interaction logs to understand help-seeking behaviors, leading to the design of a three-stage UI agent pipeline that perceives user needs and offers tailored suggestions.

**Key Contributions:**

	1. Introduction of the ProactiveVA framework for proactive assistance
	2. Analysis of user help-seeking behaviors to inform design
	3. Implementation and evaluation across different visual analytics systems

**Result:** The framework was implemented in various visual analytics systems, demonstrating generalizability and effectiveness as evaluated by algorithm, case studies, expert assessments, and user studies.

**Limitations:** The study focuses on specific VA systems and may not address all potential user interactions or contexts.

**Conclusion:** This research emphasizes current design trade-offs in proactive visual analytics and suggests directions for further research in intelligent assistance.

**Abstract:** Visual analytics (VA) is typically applied to complex data, thus requiring complex tools. While visual analytics empowers analysts in data analysis, analysts may get lost in the complexity occasionally. This highlights the need for intelligent assistance mechanisms. However, even the latest LLM-assisted VA systems only provide help when explicitly requested by the user, making them insufficiently intelligent to offer suggestions when analysts need them the most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors user interactions and delivers context-aware assistance proactively. To design effective proactive assistance, we first conducted a formative study analyzing help-seeking behaviors in user interaction logs, identifying when users need proactive help, what assistance they require, and how the agent should intervene. Based on this analysis, we distilled key design requirements in terms of intent recognition, solution generation, interpretability and controllability. Guided by these requirements, we develop a three-stage UI agent pipeline including perception, reasoning, and acting. The agent autonomously perceives users' needs from VA interaction logs, providing tailored suggestions and intuitive guidance through interactive exploration of the system. We implemented the framework in two representative types of VA systems, demonstrating its generalizability, and evaluated the effectiveness through an algorithm evaluation, case and expert study and a user study. We also discuss current design trade-offs of proactive VA and areas for further exploration.

</details>


### [17] [Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners](https://arxiv.org/abs/2507.18169)

*Lorenzo Porcaro, Chiara Monaldi*

**Main category:** cs.HC

**Keywords:** recommender systems, music listening, cultural analysis, algorithmic awareness, digital literacy

**Relevance Score:** 4

**TL;DR:** This study examines music listeners' perspectives on recommender systems through interviews analyzed with Emotional Textual Analysis, highlighting cultural repertoires and the need for algorithmic awareness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the psychosocial and cultural implications of recommender systems in music listening, moving beyond traditional cognitive-behaviorism perspectives.

**Method:** Interviews with Italian music listeners analyzed using Emotional Textual Analysis.

**Key Contributions:**

	1. Introduced a psychosocial and cultural analysis of recommender systems in music.
	2. Identified gaps in listeners' understanding of gender disparities in music recommendations.
	3. Emphasized the importance of interdisciplinary research for improving digital literacy.

**Result:** Identified shared cultural repertoires and noted listeners' lack of critical understanding concerning recommender systems and representational issues, especially regarding gender disparities.

**Limitations:** Focus on a specific cultural group (Italian music listeners) may limit generalizability.

**Conclusion:** Interdisciplinary research is needed to tackle representational harms and promote algorithmic awareness and digital literacy in developing better recommender systems.

**Abstract:** Recommender systems shape music listening worldwide due to their widespread adoption in online platforms. Growing concerns about representational harms that these systems may cause are nowadays part of the scientific and public debate, wherein music listener perspectives are oftentimes reported and discussed from a cognitive-behaviorism perspective, but rarely contextualised under a psychosocial and cultural lens. We proceed in this direction, by interviewing a group of Italian music listeners and analysing their narratives through Emotional Textual Analysis. Thanks to this, we identify shared cultural repertoires that reveal people's complex relationship with listening practices: even when familiar with online platforms, listeners may still lack a critical understanding of recommender systems. Moreover, representational issues, particularly gender disparities, seem not yet fully grasped in the context of online music listening. This study underscores the need for interdisciplinary research to address representational harms, and the role of algorithmic awareness and digital literacy in developing trustworthy recommender systems.

</details>


### [18] [Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning](https://arxiv.org/abs/2507.18252)

*Dongyang Guo, Yasmeen Abdrabou, Enkeleda Thaqi, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** eye-tracking, human-AI collaboration, large language models, cognitive modeling, anomaly detection

**Relevance Score:** 8

**TL;DR:** This paper presents a multimodal human-AI collaborative framework for enhanced cognitive pattern extraction from eye-tracking data, utilizing LLMs and hybrid anomaly detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Eye-tracking data provides insights into users' cognitive states but is challenging to analyze due to its structured nature; LLMs face difficulties with non-linguistic data.

**Method:** The framework consists of a multi-stage pipeline for gaze pattern extraction, an Expert-Model Co-Scoring Module to integrate expert judgment with LLM output, and a hybrid anomaly detection module using LSTM and LLM-driven analysis.

**Key Contributions:**

	1. Multimodal framework integrating LLMs with eye-tracking analysis
	2. Expert-Model Co-Scoring Module for trust score generation
	3. Hybrid anomaly detection combining LSTM and LLM-driven methods

**Result:** The framework shows improvements in consistency, interpretability, and performance, achieving up to 50% accuracy in difficulty prediction tasks across several LLMs and prompt strategies.

**Limitations:** 

**Conclusion:** The presented approach provides a scalable and interpretable solution for cognitive modeling, with potential applications in areas like adaptive learning and human-computer interaction.

**Abstract:** Eye-tracking data reveals valuable insights into users' cognitive states but is difficult to analyze due to its structured, non-linguistic nature. While large language models (LLMs) excel at reasoning over text, they struggle with temporal and numerical data. This paper presents a multimodal human-AI collaborative framework designed to enhance cognitive pattern extraction from eye-tracking signals. The framework includes: (1) a multi-stage pipeline using horizontal and vertical segmentation alongside LLM reasoning to uncover latent gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert judgment with LLM output to generate trust scores for behavioral interpretations; and (3) a hybrid anomaly detection module combining LSTM-based temporal modeling with LLM-driven semantic analysis. Our results across several LLMs and prompt strategies show improvements in consistency, interpretability, and performance, with up to 50% accuracy in difficulty prediction tasks. This approach offers a scalable, interpretable solution for cognitive modeling and has broad potential in adaptive learning, human-computer interaction, and educational analytics.

</details>


### [19] [Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking](https://arxiv.org/abs/2507.18315)

*Rhys Jacka, Paola R. PeÃ±a, Sophie Leonard, Ã‰va SzÃ©kely, Benjamin R. Cowan*

**Main category:** cs.HC

**Keywords:** speech disfluencies, human-machine dialogue, competence perception, egocentric communication, language production

**Relevance Score:** 7

**TL;DR:** This paper investigates the impact of speech disfluencies on human-machine dialogue, showing that participants perceive disfluent speech agents as more competent despite no differences in fluency ratings.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how speech disfluencies affect human-machine dialogue and audience design, given their known role in human-human communication.

**Method:** An online Namer-Matcher task where 61 participants interacted with a speech agent that used either fluent or disfluent speech, followed by a partner-modelling questionnaire.

**Key Contributions:**

	1. Investigates the role of disfluencies in human-machine dialogue
	2. Demonstrates perceptions of competence linked to disfluent speech agents
	3. Analyzes the impact on egocentric vs allocentric language production

**Result:** Participants rated the disfluent speech agent as more competent post-interaction, although pre-task ratings showed no significant differences. No notable differences in conversational flexibility or human-likeness were recorded.

**Limitations:** Credibility intervals are wide, suggesting the effects observed may not be clear-cut.

**Conclusion:** Disfluent agents may enhance egocentric communication but the findings are not definitive due to wide credibility intervals; implications for partner models and language production in HMD are discussed.

**Abstract:** Speech disfluencies play a role in perspective-taking and audience design in human-human communication (HHC), but little is known about their impact in human-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one participants interacted with a speech agent using either fluent or disfluent speech. Participants completed a partner-modelling questionnaire (PMQ) both before and after the task. Post-interaction evaluations indicated that participants perceived the disfluent agent as more competent, despite no significant differences in pre-task ratings. However, no notable differences were observed in assessments of conversational flexibility or human-likeness. Our findings also reveal evidence of egocentric and allocentric language production when participants interact with speech agents. Interaction with disfluent speech agents appears to increase egocentric communication in comparison to fluent agents. Although the wide credibility intervals mean this effect is not clear-cut. We discuss potential interpretations of this finding, focusing on how disfluencies may impact partner models and language production in HMD.

</details>


### [20] [PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses](https://arxiv.org/abs/2507.18393)

*Mahiro Ozaki, Li Chen, Shotaro Naganuma, Valdemar Å vÃ¡benskÃ½, Fumiya Okubo, Atsushi Shimada*

**Main category:** cs.HC

**Keywords:** Learning Analytics, Educational Data, Curriculum Mapping

**Relevance Score:** 6

**TL;DR:** This study presents PALM, a learning analytics dashboard that enhances students' awareness of their learning behaviors and academic progression by integrating curriculum-level information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional learning analytics research is limited by focusing on individual courses or learners without considering the broader curriculum context.

**Method:** The PALM system integrates multilayered educational data into a curriculum map, and its effectiveness was evaluated based on its impact on students' learning awareness and its performance relative to existing systems.

**Key Contributions:**

	1. Introduction of a comprehensive learning analytics dashboard (PALM)
	2. Enhanced understanding of student learning behaviors and academic progression
	3. Significantly better visual appeal and usability compared to existing systems.

**Result:** PALM significantly improves learners' awareness of study planning and reflection and performs better in visual appeal and usability compared to existing systems.

**Limitations:** PALM requires ongoing refinement as a system.

**Conclusion:** PALM represents a substantial advancement in learning analytics, promoting self-regulated learning and engagement through improved insights into academic progress.

**Abstract:** This study proposes and evaluates the PAnoramic Learning Map (PALM), a learning analytics (LA) dashboard designed to address the scalability challenges of LA by integrating curriculum-level information. Traditional LA research has predominantly focused on individual courses or learners and often lacks a framework that considers the relationships between courses and the long-term trajectory of learning. To bridge this gap, PALM was developed to integrate multilayered educational data into a curriculum map, enabling learners to intuitively understand their learning records and academic progression. We conducted a system evaluation to assess PALM's effectiveness in two key areas: (1) its impact on students' awareness of their learning behaviors, and (2) its comparative performance against existing systems. The results indicate that PALM enhances learners' awareness of study planning and reflection, particularly by improving perceived behavioral control through the visual presentation of individual learning histories and statistical trends, which clarify the links between learning actions and outcomes. Although PALM requires ongoing refinement as a system, it received significantly higher evaluations than existing systems in terms of visual appeal and usability. By serving as an information resource with previously inaccessible insights, PALM enhances self-regulated learning and engagement, representing a significant step beyond conventional LA toward a comprehensive and scalable approach.

</details>


### [21] [Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols](https://arxiv.org/abs/2507.18401)

*Andrew Jeyathasan, Swati Banerjee*

**Main category:** cs.HC

**Keywords:** multisensory integration, human-computer interaction, stimulus timing

**Relevance Score:** 6

**TL;DR:** This article explores the complexities of multisensory integration (MSI) by examining factors that affect the interaction of three or more sensory modalities, focusing on designs for effective MSI study protocols.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand how multiple senses work together in creating cohesive perception, especially for immersive technologies.

**Method:** The article discusses factors such as cross-modal correspondence, congruence, cognitive load, and stimulus timing relevant to the integration of multiple sensory modalities.

**Key Contributions:**

	1. Exploration of interactions between three or more sensory modalities
	2. Identification of relevant factors affecting MSI
	3. Recommendations for designing effective MSI study protocols

**Result:** Identifies key factors affecting multisensory integration and proposes ways to design effective study protocols that take these factors into account.

**Limitations:** 

**Conclusion:** A better understanding of multi-sensory integration can enhance the design of immersive technologies and contribute to more effective MSI studies.

**Abstract:** We experience the world through multiple senses that work together to create a cohesive perception, whether in daily life or immersive technologies. Understanding this multisensory integration (MSI) requires examining the interactions between sensory modalities, each with unique temporal dynamics and characteristics. While most research focuses on unimodal or bimodal cues, the integration of three or more modalities remains underexplored. MSI studies must account for factors like cross-modal correspondence, congruence, cognitive load, and stimulus timing, which become increasingly complex as modalities multiply. This article examines these key factors and how they can be applied to 8 design effective MSI study protocols.

</details>


### [22] [Towards Understanding Decision Problems As a Goal of Visualization Design](https://arxiv.org/abs/2507.18428)

*Lena Cibulski, Stefan Bruckner*

**Main category:** cs.HC

**Keywords:** decision-making, visualization research, decision-support, characterization scheme, interaction design

**Relevance Score:** 6

**TL;DR:** The paper proposes a characterization scheme for decision-making in visualization research, focusing on data, users, and task context to enhance decision-support designs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the support for decision-making tasks in visualization research, which is often under-defined.

**Method:** The authors propose a characterization scheme that outlines key properties of the data, users, and task context relating to decision problems.

**Key Contributions:**

	1. A new characterization scheme for decision problems in visualization
	2. Improved precision in specifying decision-support claims
	3. Identification of future research opportunities in decision-centric visualization.

**Result:** The scheme highlights the conditions framing decisions and informs the design of visual encodings and interactions, showcasing its utility through examples from existing design studies.

**Limitations:** 

**Conclusion:** This characterization scheme can help visualization researchers more accurately specify decision-support claims and identify future research opportunities.

**Abstract:** Decision-making is a central yet under-defined goal in visualization research. While existing task models address decision processes, they often neglect the conditions framing a decision. To better support decision-making tasks, we propose a characterization scheme that describes decision problems through key properties of the data, users, and task context. This scheme helps visualization researchers specify decision-support claims more precisely and informs the design of appropriate visual encodings and interactions. We demonstrate the utility of our approach by applying it to characterize decision tasks targeted by existing design studies, highlighting opportunities for future research in decision-centric visualization.

</details>


### [23] [High-Dimensional Data Classification in Concentric Coordinates](https://arxiv.org/abs/2507.18450)

*Alice Williams, Boris Kovalerchuk*

**Main category:** cs.HC

**Keywords:** multi-dimensional data, visualization, machine learning, Concentric Coordinates, Human-Computer Interaction

**Relevance Score:** 6

**TL;DR:** This paper proposes a framework for visualizing multi-dimensional data using lossless Concentric Coordinates, enhancing interpretability and interaction in machine learning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The visualization of high-dimensional data faces challenges such as occlusion and the need for computationally feasible methods to support machine learning visualization.

**Method:** The authors introduce Concentric Coordinates as a generalization of existing visualization techniques like Parallel and Circular Coordinates, aimed at improving multi-dimensional data interpretation.

**Key Contributions:**

	1. Introduction of lossless Concentric Coordinates for high-dimensional data
	2. Improvement of human interaction with machine learning visualizations
	3. Generalization of existing visualization methods to enhance clarity

**Result:** This new framework allows for lossless visualization of data across varying dimensions, enhancing clarity and user interaction.

**Limitations:** 

**Conclusion:** The proposed method significantly improves the visualization capabilities for machine learning algorithms, leading to better interpretability for users.

**Abstract:** The visualization of multi-dimensional data with interpretable methods remains limited by capabilities for both high-dimensional lossless visualizations that do not suffer from occlusion and that are computationally capable by parameterized visualization. This paper proposes a low to high dimensional data supporting framework using lossless Concentric Coordinates that are a more compact generalization of Parallel Coordinates along with former Circular Coordinates. These are forms of the General Line Coordinate visualizations that can directly support machine learning algorithm visualization and facilitate human interaction.

</details>


### [24] [ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR](https://arxiv.org/abs/2507.18510)

*Chenyang Zhang, Tiffany S Ma, John Andrews, Eric J Gonzalez, Mar Gonzalez-Franco, Yalong Yang*

**Main category:** cs.HC

**Keywords:** Spatial Interaction, Force-responsive, 3D environments, User Study, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** ForcePinch introduces a force-responsive spatial interaction method that adjusts pointer tracking speed based on users' pinching force, improving efficiency and precision in 3D object manipulation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance spatial interaction in 3D environments by allowing users to intuitively modulate tracking speed without sacrificing flexibility.

**Method:** Development of a hardware prototype that integrates a pressure sensor with a customizable mapping function to translate pinching force into tracking speed.

**Key Contributions:**

	1. Introduction of a novel force-responsive interaction method for 3D environments.
	2. Development of a prototype that links physical force with digital interaction.
	3. Empirical evidence of effectiveness through user studies comparing existing techniques.

**Result:** User studies showed favorable outcomes for ForcePinch compared to distance-responsive and speed-responsive techniques, highlighting its contextual advantages in interaction.

**Limitations:** Limited sample size in user study, specifics of mapping function customization not fully explored.

**Conclusion:** ForcePinch demonstrates the potential of force-responsive interactions to inform and inspire future designs in spatial interaction.

**Abstract:** Spatial interaction in 3D environments requires balancing efficiency and precision, which requires dynamic tracking speed adjustments. However, existing techniques often couple tracking speed adjustments directly with hand movements, reducing interaction flexibility. Inspired by the natural friction control inherent in the physical world, we introduce ForcePinch, a novel force-responsive spatial interaction method that enables users to intuitively modulate pointer tracking speed and smoothly transition between rapid and precise movements by varying their pinching force. To implement this concept, we developed a hardware prototype integrating a pressure sensor with a customizable mapping function that translates pinching force into tracking speed adjustments. We conducted a user study with 20 participants performing well-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch against the distance-responsive technique Go-Go and speed-responsive technique PRISM. Results highlight distinctive characteristics of the force-responsive approach across different interaction contexts. Drawing on these findings, we highlight the contextual meaning and versatility of force-responsive interactions through four illustrative examples, aiming to inform and inspire future spatial interaction design.

</details>


### [25] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)

*Donghoon Shin, Daniel Lee, Gary Hsieh, Gromit Yeuk-Yin Chan*

**Main category:** cs.HC

**Keywords:** Poster design, Generative AI, User feedback, Persona agents, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** PosterMate is a generative AI poster design assistant that facilitates collaborative feedback using audience-driven persona agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Gathering diverse feedback in poster designing is challenging, and generative AI may improve this process.

**Method:** PosterMate creates persona agents from marketing documents to gather feedback on poster components, stimulating discussion to reconcile perspectives.

**Key Contributions:**

	1. Introduction of PosterMate, an audience-driven design assistant
	2. Demonstration of effective collaboration through persona agents
	3. Evidence of performance through user studies and controlled evaluations

**Result:** User studies revealed PosterMate captures overlooked viewpoints and serves as an effective prototyping tool; feedback aligns with persona identity, and discussions synthesize diverse perspectives.

**Limitations:** 

**Conclusion:** PosterMate shows potential for improving design feedback by utilizing generative AI to simulate audience interactions and facilitate agreement in design edits.

**Abstract:** Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.

</details>


### [26] [MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss](https://arxiv.org/abs/2507.18619)

*Yichen Yu, Qiaoran Wang*

**Main category:** cs.HC

**Keywords:** multi-sensory feedback, virtual reality, hearing impairments, rehabilitation, fNIRS

**Relevance Score:** 4

**TL;DR:** This study investigates the effectiveness of multi-sensory feedback technology using virtual reality to aid rehabilitation in children with hearing impairments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Children with hearing impairments struggle with language and motor development, necessitating innovative rehabilitation approaches.

**Method:** We utilized functional near-infrared spectroscopy (fNIRS) to assess cortical activation patterns in children during pitch-matching tasks with varying interaction modes.

**Key Contributions:**

	1. Integration of multi-sensory feedback in VR for rehabilitation
	2. Assessment of cortical activation using fNIRS in real-time
	3. Evidence supporting design of interactive rehabilitation systems

**Result:** The study found distinct cortical activation patterns that suggest enhanced cognitive engagement and motor control when employing VR-based multi-sensory feedback.

**Limitations:** 

**Conclusion:** The results support the use of personalized, interactive rehabilitation systems to improve outcomes for children with hearing impairments.

**Abstract:** Children with hearing impairments face ongoing challenges in language and motor development. This study explores how multi-sensory feedback technology based on virtual reality (VR), integrating auditory, visual, and tactile stimuli, can enhance rehabilitation outcomes. Using functional near-infrared spectroscopy (fNIRS) technology, we assessed cortical activation patterns in children during pitch-matching tasks across different interaction modes. Our findings aim to provide evidence for designing personalized, interactive rehabilitation systems that enhance cognitive engagement and motor control in children with hearing impairments.

</details>


### [27] [Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork](https://arxiv.org/abs/2507.18622)

*Armin Bernstetter, Tom Kwasnitschka, Isabella Peters*

**Main category:** cs.HC

**Keywords:** provenance management, reproducibility, geospatial data, immersive visualization, user study

**Relevance Score:** 4

**TL;DR:** The paper evaluates a provenance management tool for tracking research workflows in immersive and non-immersive environments, demonstrating perceived usefulness and ease of use, particularly in immersive settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To support reproducibility in research by managing provenance of workflows, particularly in immersive geospatial data visualization scenarios.

**Method:** A user study was conducted to assess how researchers interacted with a Digital Lab Book (DLB) for recording their engagements with a virtual fieldwork tool.

**Key Contributions:**

	1. Evaluation of a provenance management tool for research workflows
	2. User study comparing immersive and non-immersive settings
	3. Insights into perceived usefulness and ease of use of the DLB among researchers.

**Result:** Participants found the DLB useful and easy to use, with indicators of higher perceived ease of use in the immersive setting; however, usage patterns did not reveal significant differences between the two settings.

**Limitations:** No significant group differences in actual usage patterns were found despite perceived differences in ease of use.

**Conclusion:** While both immersive and non-immersive users found the DLB beneficial, the study highlights the need for further investigation into the impacts of context on tool usage.

**Abstract:** Ensuring reproducibility of research is an integral part of good scientific practice. One way to support this is through provenance: information about research workflows from data gathering to researchers' sensemaking processes leading to published results. This is highly important in disciplines such as geosciences, where researchers use software for interactive and immersive visualizations of geospatial data, doing virtual measurements in simulated fieldwork on 3D models. We evaluated a provenance management tool, which allows recording of interactions with a virtual fieldwork tool and annotating different states of the visualization. The user study investigated how researchers used this Digital Lab Book (DLB) and whether perceived ease of use and perceived usefulness differed between groups in immersive or non-immersive settings. Participants perceived the DLB as both useful and easy to use. While there were indications of differences in perceived ease of use (higher for immersive setting), usage patterns showed no significant group differences.

</details>


### [28] [A Scoping Review of Functional Near-Infrared Spectroscopy (fNIRS) Applications in Game-Based Learning Environments](https://arxiv.org/abs/2411.02650)

*Shayla Sharmin, Gael Lucero-Palacios, Behdokht Kiafar, Mohammad Fahim Abrar, Mohammad Al-Ratrout, Aditya Raikwar, Roghayeh Leila Barmaki*

**Main category:** cs.HC

**Keywords:** fNIRS, game-integrated learning systems, adaptive learning, cognitive states, learning technology

**Relevance Score:** 6

**TL;DR:** This paper is a scoping review of the use of functional Near-Infrared Spectroscopy (fNIRS) in game-integrated learning systems, focusing on how fNIRS can enhance understanding of cognitive states to aid in adaptive learning.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the use of fNIRS in understanding cognitive and emotional processes during learning in game-integrated learning environments, which enhance engagement and interactivity.

**Method:** A scoping review using the PRISMA-ScR framework, screening 1300 papers and analyzing 21 empirical studies categorized by study type, learning platform, game device, fNIRS configuration, and outcome measures.

**Key Contributions:**

	1. Review of current fNIRS applications in game-integrated learning systems
	2. Identification of gaps in real-time implementation of adaptive learning
	3. Insights into cognitive states for improving learning system design

**Result:** The review indicates that game-integrated learning systems can improve engagement comparably to traditional methods, and fNIRS provides valuable insights into cognitive states, though it is not yet fully integrated into real-time adaptive systems.

**Limitations:** Limited implementation in real-time adaptive systems and challenges in standardization and data interpretation.

**Conclusion:** Standardization and data interpretation challenges remain, but fNIRS presents potential for brain-aware, interactive learning systems, providing insights for future adaptive learning research.

**Abstract:** Functional Near-Infrared Spectroscopy (fNIRS) has emerged as a valuable tool to investigate cognitive and emotional processes during learning. We focus specifically on game-integrated learning systems as the context for fNIRS-based brain data analysis. We selected game-integrated learning systems because such systems make learning more engaging, interactive, and immersive, all of which are critical features for adaptive learning design. The goal of this scoping review is to help researchers understand how fNIRS has been used so far to study brain activity in game-integrated learning systems. We also aim to show how brain data captured through fNIRS can support the development of adaptive learning systems by monitoring learners' cognitive states. Using the PRISMA-ScR framework, 1300 papers were screened, and 21 empirical studies were selected for in-depth analysis. Studies were categorized as affective/cognitive response studies or comparative studies, and further analyzed by learning platform, game device, fNIRS configuration, outcome measures, and study design. The findings reveal that game-integrated learning systems can be as effective as traditional methods in improving engagement and involvement. The findings also show that fNIRS offers valuable insights into cognitive states, but it has not yet been widely implemented in real-time adaptive systems. We identify key challenges in standardization and data interpretation and highlight the potential of fNIRS for developing brain-aware, interactive learning environments. This review offers insights to guide future research on using brain data to support adaptive learning and intelligent system design.

</details>


### [29] [Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction](https://arxiv.org/abs/2412.14209)

*Peter E. D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan*

**Main category:** cs.HC

**Keywords:** Explainable AI, decision support systems, construction industry, evidence-based framework, transparency

**Relevance Score:** 4

**TL;DR:** This paper presents a framework for Explainable AI in the construction industry, focusing on evidence-based decision support systems (DSS) to enhance AI outputs' reliability and accountability.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for transparency and interpretability in AI decision support systems, particularly in the construction industry, to improve trustworthiness of AI-generated recommendations.

**Method:** A theoretical, evidence-based means-end framework developed through a narrative review.

**Key Contributions:**

	1. Introduction of an evidence-based framework for Explainable AI in the construction industry.
	2. Focus on user knowledge needs and decision contexts in AI explanations.
	3. Applicability to various stakeholders in the construction sector including professionals and project managers.

**Result:** The framework evaluates the strength, relevance, and utility of evidence supporting AI explanations, designed to cater to the knowledge needs of users in decision-making contexts.

**Limitations:** 

**Conclusion:** The framework aids in creating meaningful explanations for AI outputs tailored to specific end-user requirements while being adaptable to various stakeholders in the construction sector.

**Abstract:** Explainable Artificial Intelligence seeks to make the reasoning processes of AI models transparent and interpretable, particularly in complex decision making environments. In the construction industry, where AI based decision support systems are increasingly adopted, limited attention has been paid to the integration of supporting evidence that underpins the reliability and accountability of AI generated outputs. The absence of such evidence undermines the validity of explanations and the trustworthiness of system recommendations. This paper addresses this gap by introducing a theoretical, evidence based means end framework developed through a narrative review. The framework offers an epistemic foundation for designing XAI enabled DSS that generate meaningful explanations tailored to users knowledge needs and decision contexts. It focuses on evaluating the strength, relevance, and utility of different types of evidence supporting AI generated explanations. While developed with construction professionals as primary end users, the framework is also applicable to developers, regulators, and project managers with varying epistemic goals.

</details>


### [30] [WigglyEyes: Inferring Eye Movements from Keypress Data](https://arxiv.org/abs/2412.15669)

*Yujun Zhu, Danqing Shi, Hee-Seung Moon, Antti Oulasvirta*

**Main category:** cs.HC

**Keywords:** eye tracking, user gaze inference, keypress data, HCI, touchscreen typing

**Relevance Score:** 8

**TL;DR:** A model infers user gaze during typing based on keypress data, offering an alternative to direct eye tracking.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a method for estimating user gaze without the costs or challenges of traditional eye tracking.

**Method:** The model uses keypress data to output a scanpath, integrating individual user characteristics through a low-dimensional parameter vector and a new loss function for synchronization.

**Key Contributions:**

	1. Inference of gaze from keypress data
	2. Use of a low-dimensional parameter vector for individuals
	3. Novel loss function for gaze and keypress synchronization

**Result:** Evaluations show accurate gaze inference during touchscreen typing sessions.

**Limitations:** The model relies solely on keypress data, which may not capture all nuances of user behavior.

**Conclusion:** This model effectively estimates gaze movements, suggesting wide applications in HCI where direct data collection is difficult.

**Abstract:** We present a model for inferring where users look during interaction based on keypress data only. Given a key log, it outputs a scanpath that tells, moment-by-moment, how the user had moved eyes while entering those keys. The model can be used as a proxy for human data in cases where collecting real eye tracking data is expensive or impossible. Our technical insight is an inference architecture that considers the individual characteristics of the user, inferred as a low-dimensional parameter vector. We present a novel loss function for synchronizing inferred eye movements with the keypresses. Evaluations on touchscreen typing demonstrate accurate gaze inference.

</details>


### [31] [HandProxy: Expanding the Affordances of Speech Interfaces in Immersive Environments with a Virtual Proxy Hand](https://arxiv.org/abs/2503.10029)

*Chen Liang, Yuxuan Liu, Martez Mott, Anhong Guo*

**Main category:** cs.HC

**Keywords:** Hand Interaction, Speech Interfaces, Virtual Environments, Human-Computer Interaction, User Study

**Relevance Score:** 8

**TL;DR:** HandProxy enhances speech interfaces to enable expressive hand interactions in virtual environments, achieving high task completion and command execution accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of hand interactions in immersive environments due to impairments and environmental constraints, exploring speech interfaces as an alternative.

**Method:** Developing HandProxy, a system that translates natural speech into a sequence of hand controls for real-time execution in virtual environments.

**Key Contributions:**

	1. Introduction of HandProxy for expressive hand interactions via speech
	2. Demonstrated high efficiency in task completion and command accuracy
	3. Support for flexible and natural speech input

**Result:** A user study with 20 participants showed HandProxy enabled diverse hand interactions, achieving a 100% task completion rate and 91.8% command execution accuracy.

**Limitations:** 

**Conclusion:** HandProxy allows natural speech to control hand movements, enhancing interaction flexibility in virtual settings.

**Abstract:** Hand interactions are increasingly used as the primary input modality in immersive environments, but they are not always feasible due to situational impairments, motor limitations, and environmental constraints. Speech interfaces have been explored as an alternative to hand input in research and commercial solutions, but are limited to initiating basic hand gestures and system controls. We introduce HandProxy, a system that expands the affordances of speech interfaces to support expressive hand interactions. Instead of relying on predefined speech commands directly mapped to possible interactions, HandProxy enables users to control the movement of a virtual hand as an interaction proxy, allowing them to describe the intended interactions naturally while the system translates speech into a sequence of hand controls for real-time execution. A user study with 20 participants demonstrated that HandProxy effectively enabled diverse hand interactions in virtual environments, achieving a 100% task completion rate with an average of 1.09 attempts per speech command and 91.8% command execution accuracy, while supporting flexible, natural speech input with varying levels of control and granularity.

</details>


### [32] [Designing Effective Human-Swarm Interaction Interfaces: Insights from a User Study on Task Performance](https://arxiv.org/abs/2504.02250)

*Wasura D. Wattearachchi, Erandi Lakshika, Kathryn Kasmarik, Michael Barlow*

**Main category:** cs.HC

**Keywords:** human-swarm interaction, interface design, robotics, user study, hazard analysis

**Relevance Score:** 8

**TL;DR:** This paper presents a systematic method for designing human-swarm interaction interfaces through empirical evaluation, demonstrating its effectiveness in guiding robotic swarms while minimizing robot deactivation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-swarm interaction interfaces by deriving design principles and empirically evaluating their effectiveness in guiding robotic swarms in risky environments.

**Method:** The researchers applied ten design principles from literature to develop a tablet-based interface and conducted a user study with 31 participants guiding a robotic swarm in the presence of various hazards.

**Key Contributions:**

	1. Development of a systematic design method for human-swarm interfaces.
	2. Conducting empirical evaluations that demonstrate the interface's effectiveness.
	3. Identifying performance variations across different types of hazards.

**Result:** At least one robot was brought closer to the target in 98% of tasks, with more than 50% of the robots reaching the target in 67% of cases, and 94% of tasks maintained over 50% of the robots active.

**Limitations:** The effectiveness of the interface varied depending on the type of hazard, indicating potential areas for further optimization.

**Conclusion:** The interface significantly enhances performance in target search tasks under various hazardous conditions, particularly in scenarios with moving hazards.

**Abstract:** In this paper, we present a systematic method of design for human-swarm interaction interfaces, combining theoretical insights with empirical evaluation. We first derived ten design principles from existing literature, applying them to key information dimensions identified through goal-directed task analysis and developed a tablet-based interface for a target search task. We then conducted a user study with 31 participants where humans were required to guide a robotic swarm to a target in the presence of three types of hazards that pose a risk to the robots: Distributed, Moving, and Spreading. Performance was measured based on the proximity of the robots to the target and the number of deactivated robots at the end of the task. Results indicate that at least one robot was brought closer to the target in 98% of tasks, demonstrating the interface's success in fulfilling the primary objective of the task. Additionally, in nearly 67% of tasks, more than 50% of the robots reached the target. Moreover, particularly better performance was noted in moving hazards. Additionally, the interface appeared to help minimise robot deactivation, as evidenced by nearly 94% of tasks where participants managed to keep more than 50% of the robots active, ensuring that most of the swarm remained operational. However, its effectiveness varied across hazards, with robot deactivation being lowest in distributed hazard scenarios, suggesting that the interface provided the most support in these conditions.

</details>


### [33] [On the Complexities of Testing for Compliance with Human Oversight Requirements in AI Regulation](https://arxiv.org/abs/2504.03300)

*Markus Langer, Veronika Lazar, Kevin Baum*

**Main category:** cs.HC

**Keywords:** AI governance, human oversight, compliance testing, European AI Act, sociotechnical systems

**Relevance Score:** 4

**TL;DR:** The paper discusses challenges in ensuring compliance with human oversight requirements in AI governance, emphasizing the balance between checklist methods and empirical testing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address key challenges in testing compliance with human oversight requirements set by the European AI Act.

**Method:** The paper analyzes the effectiveness of current compliance testing methods, highlighting the difficulties in balancing simple checklist approaches with more nuanced empirical testing.

**Key Contributions:**

	1. Identification of compliance testing challenges within AI governance
	2. Discussion of the balance between checklist methods and empirical testing
	3. Argument for a shift towards sociotechnical system governance.

**Result:** It reveals that many challenges in compliance testing are due to the contextual nature of human oversight, the need for continuous updates, and the complexity of operationalizing standards.

**Limitations:** 

**Conclusion:** The findings indicate a need for a shift towards developing sociotechnical systems rather than focusing solely on technological products in AI governance.

**Abstract:** Human oversight requirements are a core component of the European AI Act and in AI governance. In this paper, we highlight key challenges in testing for compliance with these requirements. A central difficulty lies in balancing simple, but potentially ineffective checklist-based approaches with resource-intensive and context-sensitive empirical testing of the effectiveness of human oversight of AI. Questions regarding when to update compliance testing, the context-dependent nature of human oversight requirements, and difficult-to-operationalize standards further complicate compliance testing. We argue that these challenges illustrate broader challenges in the future of sociotechnical AI governance, i.e. a future that shifts from ensuring good technological products to good sociotechnical systems.

</details>


### [34] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)

*Liang Zhang, Xiaoming Zhai, Jionghao Lin, Jionghao Lin, Jennifer Kleiman, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, Xiangen Hu, Arthur C. Graesser*

**Main category:** cs.HC

**Keywords:** Large Language Models, AI education, collaborative problem-solving, dual-agent systems, communication strategies

**Relevance Score:** 8

**TL;DR:** This study evaluates the impact of different communication strategies among LLM agents in AI-supported education, particularly in collaborative math problem-solving environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve collaborative problem-solving efficiency in AI-aided education through effective communication strategies among LLM agents.

**Method:** The research examines four communication modes: teacher-student interaction, peer-to-peer collaboration, reciprocal peer teaching, and critical debate within a dual-agent, chat-based mathematical problem-solving system utilizing the OpenAI GPT-4o model.

**Key Contributions:**

	1. Systematic evaluation of different communication strategies among LLM agents in education
	2. Demonstration of dual-agent setups outperforming single agents
	3. Identification of key dialogue acts that facilitate collaborative problem-solving.

**Result:** The results indicate that dual-agent setups perform better than single agents, with peer-to-peer collaboration achieving the highest accuracy on the MATH dataset.

**Limitations:** 

**Conclusion:** While multi-agent frameworks can enhance computational tasks, successful communication strategies are vital for addressing complex issues in AI education.

**Abstract:** Large Language Model (LLM) agents are increasingly utilized in AI-aided education to support tutoring and learning. Effective communication strategies among LLM agents improve collaborative problem-solving efficiency and facilitate cost-effective adoption in education. However, little research has systematically evaluated the impact of different communication strategies on agents' problem-solving. Our study examines four communication modes, \textit{teacher-student interaction}, \textit{peer-to-peer collaboration}, \textit{reciprocal peer teaching}, and \textit{critical debate}, in a dual-agent, chat-based mathematical problem-solving environment using the OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that dual-agent setups outperform single agents, with \textit{peer-to-peer collaboration} achieving the highest accuracy. Dialogue acts like statements, acknowledgment, and hints play a key role in collaborative problem-solving. While multi-agent frameworks enhance computational tasks, effective communication strategies are essential for tackling complex problems in AI education.

</details>


### [35] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)

*Donghoon Shin, Daniel Lee, Gary Hsieh, Gromit Yeuk-Yin Chan*

**Main category:** cs.HC

**Keywords:** poster design, AI feedback, collaborative design, persona agents, user study

**Relevance Score:** 6

**TL;DR:** PosterMate is a generative AI-based poster design assistant that enhances collaboration and feedback from diverse audiences through audience-driven persona agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the poster design process by facilitating synchronous feedback from target audiences with diverse perspectives.

**Method:** PosterMate constructs persona agents from marketing documents and gathers feedback through these agents, stimulating discussion to reach design edit conclusions, followed by integration into the poster.

**Key Contributions:**

	1. Introduces the PosterMate assistant for poster design feedback.
	2. Uses generative AI to simulate human-like interactions for audience-driven feedback.
	3. Demonstrates effectiveness in capturing diverse viewpoints through persona agents.

**Result:** User studies showed PosterMate captures overlooked viewpoints and serves as an effective prototyping tool, with feedback deemed appropriate based on persona identities.

**Limitations:** 

**Conclusion:** PosterMate enhances collaboration in poster design by effectively synthesizing feedback from various persona agents, integrating agreed edits into the design.

**Abstract:** Poster designing can benefit from synchronous feedback from target audiences. However, gathering audiences with diverse perspectives and reconciling them on design edits can be challenging. Recent generative AI models present opportunities to simulate human-like interactions, but it is unclear how they may be used for feedback processes in design. We introduce PosterMate, a poster design assistant that facilitates collaboration by creating audience-driven persona agents constructed from marketing documents. PosterMate gathers feedback from each persona agent regarding poster components, and stimulates discussion with the help of a moderator to reach a conclusion. These agreed-upon edits can then be directly integrated into the poster design. Through our user study (N=12), we identified the potential of PosterMate to capture overlooked viewpoints, while serving as an effective prototyping tool. Additionally, our controlled online evaluation (N=100) revealed that the feedback from an individual persona agent is appropriate given its persona identity, and the discussion effectively synthesizes the different persona agents' perspectives.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [36] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)

*Yimeng Zhang, Tian Wang, Jiri Gesi, Ziyi Wang, Yuxuan Lu, Jiacheng Lin, Sinong Zhan, Vianne Gao, Ruochen Jiao, Junze Liu, Kun Qian, Yuxin Tang, Ran Xue, Houyu Zhang, Qingjun Cui, Yufan Guo, Dakuo Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Human Behavior Simulation

**Relevance Score:** 9

**TL;DR:** Shop-R1 is a novel reinforcement learning framework that enhances LLMs' reasoning for simulating human behavior in online shopping, achieving over 65% improvement in performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements, the reasoning abilities of LLMs in simulating human behavior in web environments are limited by the models generating training rationales.

**Method:** Introduces a two-stage RL framework where the first stage focuses on rationale generation using internal model signals, and the second stage involves action prediction guided by a hierarchical reward structure.

**Key Contributions:**

	1. Introduction of Shop-R1 as a novel RL framework for LLMs
	2. Implementation of self-supervised rationale generation
	3. Development of a difficulty-aware hierarchical reward structure for action prediction.

**Result:** Shop-R1 demonstrates a relative improvement of over 65% compared to baseline models in generating human-like behavior during online shopping.

**Limitations:** 

**Conclusion:** Our framework significantly enhances LLMs' simulation capabilities, providing a more effective approach to modeling human behavior in digital environments.

**Abstract:** Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline.

</details>


### [37] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)

*Zhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Process Reward Models, Large Language Models, Dynamic Generalization, Pareto Dominance, Machine Learning

**Relevance Score:** 9

**TL;DR:** Dynamic and Generalizable Process Reward Modeling (DG-PRM) improves reward signals for Large Language Models by employing a reward tree and Pareto dominance estimation to enhance model performance and generalizability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the limitations of existing Process Reward Models (PRMs) that rely on heuristic methods and struggle with cross-domain generalization.

**Method:** The paper proposes DG-PRM, utilizing a reward tree for fine-grained, multi-dimensional reward criteria and dynamic selection of reward signals for step-wise scoring alongside Pareto dominance estimation for identifying positive and negative pairs.

**Key Contributions:**

	1. Introduction of Dynamic and Generalizable Process Reward Modeling (DG-PRM)
	2. Development of a reward tree for capturing multi-dimensional reward criteria
	3. Application of Pareto dominance estimation for reward signal evaluation

**Result:** Experimental results indicate that DG-PRM significantly boosts model performance across various tasks and adapts effectively to out-of-distribution scenarios, demonstrating notable generalizability.

**Limitations:** 

**Conclusion:** DG-PRM offers a more robust framework for guiding LLMs with dense reward signals, enhancing their effectiveness in complex tasks.

**Abstract:** Process Reward Models (PRMs) are crucial for guiding Large Language Models (LLMs) in complex scenarios by providing dense reward signals. However, existing PRMs primarily rely on heuristic approaches, which struggle with cross-domain generalization. While LLM-as-judge has been proposed to provide generalized rewards, current research has focused mainly on feedback results, overlooking the meaningful guidance embedded within the text. Additionally, static and coarse-grained evaluation criteria struggle to adapt to complex process supervision. To tackle these challenges, we propose Dynamic and Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to capture and store fine-grained, multi-dimensional reward criteria. DG-PRM dynamically selects reward signals for step-wise reward scoring. To handle multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation to identify discriminative positive and negative pairs. Experimental results show that DG-PRM achieves stunning performance on prevailing benchmarks, significantly boosting model performance across tasks with dense rewards. Further analysis reveals that DG-PRM adapts well to out-of-distribution scenarios, demonstrating exceptional generalizability.

</details>


### [38] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)

*Shubham Mohole, Sainyam Galhotra*

**Main category:** cs.CL

**Keywords:** natural language interfaces, data analysis, cognitive biases, interactive systems, prompt generation

**Relevance Score:** 9

**TL;DR:** VeriMinder is an interactive system designed to detect and mitigate cognitive biases in analytical questions formed by users of natural language interfaces to databases (NLIDBs), enhancing the quality of data analysis.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of formulating bias-free analytical questions by users without a background in statistical analysis when using NLIDBs.

**Method:** VeriMinder employs a contextual semantic mapping framework to identify relevant biases, operationalizes the Hard-to-Vary principle for systematic analysis, and utilizes an optimized LLM-powered system for generating high-quality, task-specific prompts.

**Key Contributions:**

	1. Contextual semantic mapping framework for identifying biases
	2. Analytical framework operationalizing the Hard-to-Vary principle
	3. LLM-powered generation of task-specific prompts

**Result:** User testing revealed that 82.5% of participants felt positively impacted by the quality of their analysis, and VeriMinder outperformed alternative methods by at least 20% in key metrics of concreteness, comprehensiveness, and accuracy.

**Limitations:** 

**Conclusion:** VeriMinder helps users avoid 'wrong question' vulnerabilities in data analysis by improving analytical questioning and is available as open-source software for broader community use.

**Abstract:** Application systems using natural language interfaces to databases (NLIDBs) have democratized data analysis. This positive development has also brought forth an urgent challenge to help users who might use these systems without a background in statistical analysis to formulate bias-free analytical questions. Although significant research has focused on text-to-SQL generation accuracy, addressing cognitive biases in analytical questions remains underexplored. We present VeriMinder, https://veriminder.ai, an interactive system for detecting and mitigating such analytical vulnerabilities. Our approach introduces three key innovations: (1) a contextual semantic mapping framework for biases relevant to specific analysis contexts (2) an analytical framework that operationalizes the Hard-to-Vary principle and guides users in systematic data analysis (3) an optimized LLM-powered system that generates high-quality, task-specific prompts using a structured process involving multiple candidates, critic feedback, and self-reflection.   User testing confirms the merits of our approach. In direct user experience evaluation, 82.5% participants reported positively impacting the quality of the analysis. In comparative evaluation, VeriMinder scored significantly higher than alternative approaches, at least 20% better when considered for metrics of the analysis's concreteness, comprehensiveness, and accuracy. Our system, implemented as a web application, is set to help users avoid "wrong question" vulnerability during data analysis. VeriMinder code base with prompts, https://reproducibility.link/veriminder, is available as an MIT-licensed open-source software to facilitate further research and adoption within the community.

</details>


### [39] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)

*Nhan Phan, Anusha Porwal, Yaroslav Getman, Ekaterina Voskoboinik, TamÃ¡s GrÃ³sz, Mikko Kurimo*

**Main category:** cs.CL

**Keywords:** Automatic Speaking Assessment, Language Learning, Machine Learning, Data Efficiency

**Relevance Score:** 6

**TL;DR:** The paper introduces an efficient end-to-end system for Automatic Speaking Assessment (ASA) that processes multiple spoken responses with a single encoder and a lightweight aggregator.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a practical and efficient solution for scoring multi-part second-language tests in Computer-Assisted Language Learning systems.

**Method:** A single Whisper-small encoder processes all four spoken responses, and a lightweight aggregator combines the information to predict scores, eliminating the need for separate transcription and models per part.

**Key Contributions:**

	1. Unique single-encoder architecture for processing multiple responses
	2. Data sampling strategy that improves performance with fewer training samples
	3. Achievement of lower RMSE compared to traditional text-based models

**Result:** The system achieved a RMSE of 0.384, outperforming a text-based baseline while using fewer parameters (168M), and demonstrated strong data efficiency by using only 44.8% of the speakers in training while achieving RMSE of 0.383.

**Limitations:** 

**Conclusion:** The proposed system enhances the efficiency and scalability of ASA for language learning applications, addressing challenges in performance and data utilization.

**Abstract:** We present an efficient end-to-end approach for holistic Automatic Speaking Assessment (ASA) of multi-part second-language tests, developed for the 2025 Speak & Improve Challenge. Our system's main novelty is the ability to process all four spoken responses with a single Whisper-small encoder, combine all information via a lightweight aggregator, and predict the final score. This architecture removes the need for transcription and per-part models, cuts inference time, and makes ASA practical for large-scale Computer-Assisted Language Learning systems.   Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming the text-based baseline (0.44) while using at most 168M parameters (about 70% of Whisper-small). Furthermore, we propose a data sampling strategy, allowing the model to train on only 44.8% of the speakers in the corpus and still reach 0.383 RMSE, demonstrating improved performance on imbalanced classes and strong data efficiency.

</details>


### [40] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)

*Hulayyil Alshammari, Praveen Rao*

**Main category:** cs.CL

**Keywords:** AI detection, large language models, adversarial attacks, DeepSeek, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of six AI detection tools in identifying text generated by the LLM DeepSeek, especially after adversarial attacks like paraphrasing and humanization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in literature regarding the detection of text generated by the recently published LLM DeepSeek and the impact of adversarial attacks on detection accuracy.

**Method:** The study utilized six AI detection tools and exposed them to human-authored and DeepSeek-generated texts, applying adversarial attacks to assess their performance.

**Key Contributions:**

	1. Evaluated the performance of six detection tools on DeepSeek-generated text.
	2. Demonstrated the impact of adversarial attacks on detection accuracy.
	3. Introduced few-shot prompting and CoT reasoning as effective strategies for better detection.

**Result:** QuillBot and Copyleaks demonstrated near-perfect performance on original and paraphrased texts, while other tools like AI Text Classifier and GPT-2 showed variable detection accuracy, particularly struggling with humanization attacks.

**Limitations:** The study primarily focuses on a limited number of detection tools and the accuracy may vary with different versions of LLMs or under different conditions.

**Conclusion:** The study highlights the challenges and variations in detecting DeepSeek-generated text, especially under various adversarial conditions. Few-shot prompting and CoT methods proved to be effective for detection accuracy.

**Abstract:** Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools -- AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others -- particularly AI Text Classifier and GPT-2 -- showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%).

</details>


### [41] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)

*Sohaib Imran, Ihor Kendiukhov, Matthew Broerman, Aditya Thomas, Riccardo Campanella, Rob Lamb, Peter M. Atkinson*

**Main category:** cs.CL

**Keywords:** language models, Bayes' theorem, Bayesian Coherence Coefficient, belief updating, AI governance

**Relevance Score:** 8

**TL;DR:** This paper investigates how the size and capability of language models influence their adherence to Bayes' theorem when updating beliefs with evidence, proposing a new metric, the Bayesian Coherence Coefficient (BCC).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether larger language models are better at updating beliefs in accordance with Bayes' theorem when given in-context evidence.

**Method:** We formulated a Bayesian Coherence Coefficient (BCC) metric and generated a dataset to measure BCC across various pre-trained language models from five different families, analyzing their performance in relation to model size, training data, and benchmark scores.

**Key Contributions:**

	1. Introduction of the Bayesian Coherence Coefficient (BCC) metric for evaluating belief updates in language models.
	2. Empirical analysis comparing belief coherence of various language model families based on size and training data.
	3. Insights on the implications of model architecture for the governance of language models.

**Result:** Our findings suggest that larger pre-trained language models exhibit more coherent belief updates in line with Bayes' theorem, as evidenced by their BCC scores.

**Limitations:** The analysis is based on pre-trained models and may not fully reflect the performance of fine-tuned models in belief updating tasks.

**Conclusion:** The results highlight the significance of model size and architecture in belief updating processes in language models, which has important implications for the understanding and governance of LLMs.

**Abstract:** Do larger and more capable language models learn to update their "beliefs" about propositions more consistently with Bayes' theorem when presented with evidence in-context? To test this, we formulate a Bayesian Coherence Coefficient (BCC) metric and generate a dataset with which to measure the BCC. We measure BCC for multiple pre-trained-only language models across five model families, comparing against the number of model parameters, the amount of training data, and model scores on common benchmarks. Our results provide evidence for our hypothesis that larger and more capable pre-trained language models assign credences that are more coherent with Bayes' theorem. These results have important implications for our understanding and governance of LLMs.

</details>


### [42] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)

*Fitsum Gaim, Jong C. Park*

**Main category:** cs.CL

**Keywords:** Tigrinya, NLP, Natural Language Processing, morphological processing, machine translation

**Relevance Score:** 3

**TL;DR:** This paper surveys the state of NLP research for Tigrinya, analyzing over 40 studies and highlighting challenges and future directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the severe underrepresentation of Tigrinya in Natural Language Processing research.

**Method:** Systematic review of over 40 studies from 2011 to 2025 encompassing various NLP tasks such as morphological processing, machine translation, and speech recognition.

**Key Contributions:**

	1. Comprehensive survey of Tigrinya NLP research
	2. Identification of key challenges and promising research directions
	3. Publicly available curated metadata of surveyed studies and resources

**Result:** The analysis shows a transition from rule-based systems to modern neural architectures, driven by advancements in resource creation.

**Limitations:** Focuses exclusively on Tigrinya, limiting broader applicability to other languages.

**Conclusion:** The paper highlights the challenges in Tigrinya NLP while proposing future research directions and providing a curated repository of resources.

**Abstract:** Despite being spoken by millions of people, Tigrinya remains severely underrepresented in Natural Language Processing (NLP) research. This work presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40 studies spanning more than a decade of work from 2011 to 2025. We systematically review the current state of computational resources, models, and applications across ten distinct downstream tasks, including morphological processing, machine translation, speech recognition, and question-answering. Our analysis reveals a clear trajectory from foundational, rule-based systems to modern neural architectures, with progress consistently unlocked by resource creation milestones. We identify key challenges rooted in Tigrinya's morphological complexity and resource scarcity, while highlighting promising research directions, including morphology-aware modeling, cross-lingual transfer, and community-centered resource development. This work serves as both a comprehensive reference for researchers and a roadmap for advancing Tigrinya NLP. A curated metadata of the surveyed studies and resources is made publicly available.\footnote{Tigrinya NLP Anthology: https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [43] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)

*Zihan Wang, Xinzhang Liu, Yitong Yao, Chao Wang, Yu Zhao, Zhihao Yang, Wenmin Deng, Kaipeng Jia, Jiaxin Peng, Yuyao Huang, Sishi Xiong, Zhuo Jiang, Kaidong Yu, Xiaohui Hu, Fubei Yao, Ruiyu Fang, Zhuoru Jiang, Ruiting Song, Qiyi Xie, Rui Xue, Xuewei He, Yanlei Xue, Zhu Yuan, Zhaoxi Zhang, Zilu Huang, Shiquan Wang, Xin Wang, Hanming Wu, Mingyuan Wang, Xufeng Zhan, Yuhan Sun, Zhaohu Xing, Yuhao Jiang, Bingkai Yang, Shuangyong Song, Yongxiang Li, Zhongjiang He, Xuelong Li*

**Main category:** cs.CL

**Keywords:** TeleChat, Language Models, Machine Learning, Reinforcement Learning, Reasoning

**Relevance Score:** 8

**TL;DR:** Introduction of new TeleChat models (TeleChat2, TeleChat2.5, T1) offering performance upgrades through advanced training strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of language models with significant performance gains while addressing specific applications like code generation and mathematical reasoning.

**Method:** The models utilize a training pipeline starting with pretraining on a large dataset followed by Supervised Fine-Tuning and Direct Preference Optimization, with the addition of continual pretraining for domain-specific tasks and reinforcement learning.

**Key Contributions:**

	1. Introduction of new models TeleChat2, TeleChat2.5, and T1 with advanced training methodologies
	2. Demonstrated significant improvements in reasoning and task performance
	3. Public release of models tailored for diverse applications

**Result:** TeleChat2 series achieves notable improvements in reasoning and task performance, outperforming proprietary models, particularly in complex reasoning tasks and rapid inference.

**Limitations:** 

**Conclusion:** The TeleChat models are released for public use, enabling developers and researchers to leverage cutting-edge language models for a variety of applications.

**Abstract:** We introduce the latest series of TeleChat models: \textbf{TeleChat2}, \textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The \textbf{T1} variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2}, \textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.

</details>


### [44] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)

*Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, neural databases, gated retrieval, L& E

**Relevance Score:** 9

**TL;DR:** NeuralDB is an editing framework for LLMs, enhancing efficient knowledge updates while preserving general capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Efficiently editing knowledge in LLMs is crucial for timely model updates, but current methods risk compromising overall performance and causing memory issues.

**Method:** NeuralDB models linear Locate-and-Edit methods as querying a Key-Value database, utilizing a non-linear gated retrieval module to selectively edit facts.

**Key Contributions:**

	1. Introduction of NeuralDB as a neural KV database for LLM editing
	2. Utilization of a non-linear gated retrieval module to preserve model performance
	3. Demonstration of effectiveness in extensive editing scenarios up to 100,000 facts.

**Result:** NeuralDB demonstrated superior performance in editing efficacy, generalization, specificity, fluency, and consistency while preserving overall performance across multiple tasks, even when expanded to edit 100,000 facts.

**Limitations:** 

**Conclusion:** NeuralDB effectively enables large-scale editing in LLMs without degrading their general abilities, showcasing scalability and effectiveness in multiple datasets.

**Abstract:** Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [45] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)

*Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** inference-time steering, large language models, vision-language models, gradient-based attribution, token influence

**Relevance Score:** 8

**TL;DR:** GrAInS is an inference-time steering method for LLMs and VLMs that uses gradient-based attribution to modify activations for improved task performance without model fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Most existing inference steering methods are limited by fixed interventions and do not leverage informative gradient signals from input tokens, especially in multimodal contexts.

**Method:** GrAInS utilizes contrastive, gradient-based attribution via Integrated Gradients to identify influential tokens, constructs steering vectors, and adjusts hidden activations in transformer layers based on token-level attribution signals.

**Key Contributions:**

	1. Introduced GrAInS for flexible inference-time steering of multimodal models
	2. Demonstrated empirical superiority over fine-tuning and existing steering methods
	3. Provided a framework for token-level influence assessment in model responses

**Result:** GrAInS demonstrates significant performance improvements, including a 13.22% accuracy gain on TruthfulQA, a reduction of hallucination rates on MMHal-Bench, and an 8.11% increase in alignment win rates on SPA-VL, while maintaining fluency.

**Limitations:** 

**Conclusion:** GrAInS offers a novel and effective method for steering LLM and VLM behavior at inference time, enabling modular and interpretable control without the need for retraining.

**Abstract:** Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.

</details>


### [46] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)

*Hoyeon Lee, Sejung Son, Ye-Eun Kang, Jong-Hwan Kim*

**Main category:** cs.CL

**Keywords:** phrase break prediction, large language models, synthetic data generation, text-to-speech systems, speech domain

**Relevance Score:** 8

**TL;DR:** This paper explores using large language models to generate synthetic phrase break annotations, reducing reliance on costly human annotations in text-to-speech systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for phrase break prediction rely on extensive human annotations, which are costly and labor-intensive, while variability in speech complicates data consistency.

**Method:** We leverage large language models to create synthetic phrase break annotations and compare these with traditional human annotations across multiple languages.

**Key Contributions:**

	1. Introduction of LLMs for generating synthetic phrase break annotations
	2. Comparison of LLM-based annotations with traditional methods
	3. Assessment of LLM effectiveness across multiple languages

**Result:** The use of LLM-generated data effectively mitigates the challenges of phrase break prediction, demonstrating its applicability in the speech domain and across various languages.

**Limitations:** 

**Conclusion:** LLMs provide a promising alternative for generating high-quality synthetic data in phrase break prediction, potentially reducing manual efforts in speech tasks.

**Abstract:** Current approaches to phrase break prediction address crucial prosodic aspects of text-to-speech systems but heavily rely on vast human annotations from audio or text, incurring significant manual effort and cost. Inherent variability in the speech domain, driven by phonetic factors, further complicates acquiring consistent, high-quality data. Recently, large language models (LLMs) have shown success in addressing data challenges in NLP by generating tailored synthetic data while reducing manual annotation needs. Motivated by this, we explore leveraging LLM to generate synthetic phrase break annotations, addressing the challenges of both manual annotation and speech-related tasks by comparing with traditional annotations and assessing effectiveness across multiple languages. Our findings suggest that LLM-based synthetic data generation effectively mitigates data challenges in phrase break prediction and highlights the potential of LLMs as a viable solution for the speech domain.

</details>


### [47] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)

*Tevin Atwal, Chan Nam Tieu, Yefeng Yuan, Zhan Shi, Yuhong Liu, Liang Cheng*

**Main category:** cs.CL

**Keywords:** Synthetic Data, Large Language Models, Diversity Metrics, Privacy Risks, Text Generation

**Relevance Score:** 9

**TL;DR:** This paper assesses the diversity and privacy risks of synthetic data generated by Large Language Models (LLMs) and proposes enhancements for better data quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing challenges in evaluating the diversity and privacy risks associated with synthetic data generated by LLMs in data-driven applications.

**Method:** The paper proposes a comprehensive set of metrics to quantify the diversity and privacy of text-based synthetic data, followed by an experimental evaluation and a prompt-based approach to improve data generation.

**Key Contributions:**

	1. Development of metrics for assessing diversity and privacy of synthetic data
	2. Empirical evaluation revealing LLMs' limitations
	3. Proposed prompt-based enhancement strategy for generating better synthetic data

**Result:** The findings indicate significant limitations in LLMs regarding the generation of diverse and privacy-preserving synthetic data.

**Limitations:** Focused primarily on text-based synthetic data and LLMs; may not generalize to other forms of synthetic data.

**Conclusion:** A prompt-based approach is recommended to enhance the diversity of synthetic data while safeguarding user privacy.

**Abstract:** The increasing use of synthetic data generated by Large Language Models (LLMs) presents both opportunities and challenges in data-driven applications. While synthetic data provides a cost-effective, scalable alternative to real-world data to facilitate model training, its diversity and privacy risks remain underexplored. Focusing on text-based synthetic data, we propose a comprehensive set of metrics to quantitatively assess the diversity (i.e., linguistic expression, sentiment, and user perspective), and privacy (i.e., re-identification risk and stylistic outliers) of synthetic datasets generated by several state-of-the-art LLMs. Experiment results reveal significant limitations in LLMs' capabilities in generating diverse and privacy-preserving synthetic data. Guided by the evaluation results, a prompt-based approach is proposed to enhance the diversity of synthetic reviews while preserving reviewer privacy.

</details>


### [48] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)

*Zehan Li, Hongjie Chen, Yuxin Zhang, Jing Zhou, Xuening Wang, Hang Lv, Mengjie Du, Yaodong Song, Jie Lian, Jian Kang, Jie Li, Yongxiang Li, Zhongjiang He, Xuelong Li*

**Main category:** cs.CL

**Keywords:** Spoken language models, Benchmarking, Conversational agents, User-centered evaluation, Natural language processing

**Relevance Score:** 7

**TL;DR:** TELEVAL is a dynamic benchmark designed to evaluate spoken language models (SLMs) as conversational agents in realistic settings, focusing on natural user interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for spoken language models often fail to reflect how users interact in real-world conversations, necessitating a more user-centered evaluation framework.

**Method:** TELEVAL defines three evaluation dimensions (Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities) and uses a dialogue format consistent with real-world usage to assess both text and audio outputs.

**Key Contributions:**

	1. Introduction of TELEVAL as a new benchmark for evaluating SLMs in realistic settings
	2. Focus on user-centered evaluation reflecting natural interactions
	3. Identification of evaluation dimensions relevant to conversational abilities

**Result:** Experiments reveal that current SLMs still have significant limitations in natural conversational tasks, indicating a need for further improvements.

**Limitations:** TELEVAL is specifically focused on Chinese interactive settings, which may limit its applicability to other languages or contexts.

**Conclusion:** TELEVAL aims to provide a framework that reflects user experience, facilitating the development of more capable dialogue-oriented SLMs.

**Abstract:** Spoken language models (SLMs) have seen rapid progress in recent years, along with the development of numerous benchmarks for evaluating their performance. However, most existing benchmarks primarily focus on evaluating whether SLMs can perform complex tasks comparable to those tackled by large language models (LLMs), often failing to align with how users naturally interact in real-world conversational scenarios. In this paper, we propose TELEVAL, a dynamic benchmark specifically designed to evaluate SLMs' effectiveness as conversational agents in realistic Chinese interactive settings. TELEVAL defines three evaluation dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It adopts a dialogue format consistent with real-world usage and evaluates text and audio outputs separately. TELEVAL particularly focuses on the model's ability to extract implicit cues from user speech and respond appropriately without additional instructions. Our experiments demonstrate that despite recent progress, existing SLMs still have considerable room for improvement in natural conversational tasks. We hope that TELEVAL can serve as a user-centered evaluation framework that directly reflects the user experience and contributes to the development of more capable dialogue-oriented SLMs.

</details>


### [49] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)

*Haomin Qi, Zihan Dai, Chengbo Huang*

**Main category:** cs.CL

**Keywords:** large language models, parameter-efficient fine-tuning, gradient stability, unitary RNN, hybrid methods

**Relevance Score:** 9

**TL;DR:** This paper evaluates parameter-efficient fine-tuning techniques for large language models and introduces a hybrid method that combines the strengths of existing techniques to improve efficiency and resource management during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Fine-tuning large language models is computationally intensive, necessitating more efficient methods for real-world applications and deployment.

**Method:** The paper reviews various parameter-efficient fine-tuning techniques and proposes a hybrid method that combines BOFT's stability with LoRA-GA's convergence speed, using per-layer adaptive updates based on gradient norms.

**Key Contributions:**

	1. Introduction of a hybrid fine-tuning strategy combining BOFT and LoRA-GA techniques.
	2. Demonstration of the adaptation of unitary RNN principles to transformer LLMs for increased gradient stability.
	3. Empirical validation on multiple benchmarks showing efficiency improvements in fine-tuning.

**Result:** Empirical evaluations demonstrate that the hybrid method outperforms existing PEFT techniques, achieving similar accuracy to full fine-tuning while significantly reducing training time and memory usage.

**Limitations:** 

**Conclusion:** The proposed hybrid approach represents a scalable and practical solution for fine-tuning large language models, making it viable for resource-constrained environments.

**Abstract:** Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench, and HumanEval -- using models ranging from 7B to 405B parameters demonstrate that our hybrid method consistently outperforms individual PEFT baselines, approaching full fine-tuning accuracy while reducing resource consumption by up to 2.1 times in training time and 50 percent in memory usage. These findings establish the hybrid approach as a practical and scalable fine-tuning solution for real-world deployment of LLMs under resource constraints.

</details>


### [50] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)

*Riley Carlson, John Bauer, Christopher D. Manning*

**Main category:** cs.CL

**Keywords:** GloVe, word embeddings, NER, natural language processing, language evolution

**Relevance Score:** 6

**TL;DR:** The report evaluates new 2024 English GloVe models, highlighting improvements in word representation and cultural relevance compared to the 2014 versions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide updated word embeddings that reflect current language usage and improve upon the inadequacies of the original 2014 GloVe models.

**Method:** Two sets of word embeddings were trained using data from Wikipedia, Gigaword, and Dolma, followed by evaluations through vocabulary comparison, testing, and NER tasks.

**Key Contributions:**

	1. Introduction of 2024 GloVe models with updated linguistic relevance
	2. Comprehensive documentation of data versions and preprocessing
	3. Demonstrated improved performance on recent NER tasks.

**Result:** The 2024 GloVe models incorporate new relevant words and perform well in structural tasks, showing improved results on recent NER datasets, especially for non-Western data.

**Limitations:** 

**Conclusion:** The new models enhance word representation by including contemporary language and context, thereby improving NLP applications.

**Abstract:** This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.

</details>


### [51] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)

*Hongjie Chen, Zehan Li, Yaodong Song, Wenming Deng, Yitong Yao, Yuxin Zhang, Hang Lv, Xuechao Zhu, Jian Kang, Jie Lian, Jie Li, Chao Wang, Shuangyong Song, Yongxiang Li, Zhongjiang He*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, Paralinguistic Awareness, Emotion Recognition

**Relevance Score:** 7

**TL;DR:** GOAT-SLM is a novel spoken language model designed to incorporate paralinguistic and speaker characteristic cues for improved spoken interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Most existing spoken language models overlook essential non-linguistic cues such as emotion and dialect. This work aims to fill that gap by enhancing spoken language modeling to include such characteristics.

**Method:** GOAT-SLM uses a dual-modality head architecture to separate linguistic modeling from acoustic realization, employing a modular training strategy that progressively aligns various information using large-scale speech-text datasets.

**Key Contributions:**

	1. Introduction of GOAT-SLM with paralinguistic awareness
	2. Dual-modality head architecture for separating linguistic from acoustic features
	3. Modular training strategy for aligning linguistic and non-linguistic cues

**Result:** Experimental results show that GOAT-SLM achieves balanced performance across semantic and non-semantic tasks, outperforming other models in aspects like emotion and dialect sensitivity.

**Limitations:** 

**Conclusion:** The introduction of GOAT-SLM emphasizes the need for spoken language models to consider both linguistic and paralinguistic elements, paving the way for more socially aware systems.

**Abstract:** Recent advances in end-to-end spoken language models (SLMs) have significantly improved the ability of AI systems to engage in natural spoken interactions. However, most existing models treat speech merely as a vehicle for linguistic content, often overlooking the rich paralinguistic and speaker characteristic cues embedded in human speech, such as dialect, age, emotion, and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel spoken language model with paralinguistic and speaker characteristic awareness, designed to extend spoken language modeling beyond text semantics. GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation. To enhance model efficiency and versatility, we propose a modular, staged training strategy that progressively aligns linguistic, paralinguistic, and speaker characteristic information using large-scale speech-text corpora. Experimental results on TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM achieves well-balanced performance across both semantic and non-semantic tasks, and outperforms existing open-source models in handling emotion, dialectal variation, and age-sensitive interactions. This work highlights the importance of modeling beyond linguistic content and advances the development of more natural, adaptive, and socially aware spoken language systems.

</details>


### [52] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)

*Xiaoyuan Li, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Multi-modal Large Language Models, mathematical reasoning, code generation, visual operations, evaluation framework

**Relevance Score:** 7

**TL;DR:** This work evaluates multi-modal large language models (MLLMs) on their ability to perform mathematical reasoning via code in visual operations, revealing a gap in their performance compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluations of MLLMs mainly focus on text-only reasoning, which overlooks their capabilities in executing accurate visual operations through code.

**Method:** The study introduces a framework to assess MLLMs through two tasks: Multi-modal Code Generation (MCG) for creating visualizations and Multi-modal Code Editing (MCE) for executing fine-grained operations (deletion, modification, annotation) on existing visualizations.

**Key Contributions:**

	1. Introduction of a framework for evaluating MLLMs in multi-modal mathematical reasoning
	2. Identification of the gap in performance between MLLMs and human abilities in visual reasoning tasks
	3. Development of a dataset covering various types of mathematical figures for robust evaluation

**Result:** The experiments involving nine mainstream MLLMs show that these models significantly underperform compared to human capabilities in executing fine-grained visual operations.

**Limitations:** The study is limited to the capabilities of current popular MLLMs and does not explore new model architectures or approaches.

**Conclusion:** This work highlights the need for further development in MLLMs' code-based visual operation capabilities to reach human-level performance.

**Abstract:** Recent progress in Multi-modal Large Language Models (MLLMs) has enabled step-by-step multi-modal mathematical reasoning by performing visual operations based on the textual instructions. A promising approach uses code as an intermediate representation to precisely express and manipulate the images in the reasoning steps. However, existing evaluations focus mainly on text-only reasoning outputs, leaving the MLLM's ability to perform accurate visual operations via code largely unexplored. This work takes a first step toward addressing that gap by evaluating MLLM's code-based capabilities in multi-modal mathematical reasoning.Specifically, our framework focuses on two key evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's ability to accurately understand and construct visualizations from scratch. (2) Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained operations, which include three types: Deletion, Modification and Annotation. To evaluate the above tasks, we incorporate a dataset that covers the five most popular types of mathematical figures, including geometric diagrams, function plots, and three types of statistical charts, to provide a comprehensive and effective measurement of existing MLLMs. Our experimental evaluation involves nine mainstream MLLMs, and the results reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations.

</details>


### [53] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)

*Gonzalo Cardenal Antolin, Jacques Fellay, Bashkim Jaha, Roger Kouyos, Niko Beerenwinkel, Diane Duroux*

**Main category:** cs.CL

**Keywords:** Large Language Models, HIV Management, Clinical Practice, Medical Question Answering, AI in Health

**Relevance Score:** 9

**TL;DR:** This study evaluates LLM capabilities in HIV management, introducing the HIVMedQA benchmark for assessing medical question answering performance, and highlights the need for careful integration of LLMs in clinical practice.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of HIV management presents a compelling use case for integrating LLMs into clinical decision-making, but concerns about accuracy and clinician acceptance exist.

**Method:** The study utilized prompt engineering and evaluated seven general-purpose and three medically specialized LLMs against a benchmark dataset of curated clinical questions, assessing performance based on comprehension, reasoning, recall, bias, harm, and accuracy.

**Key Contributions:**

	1. Introduction of HIVMedQA benchmark for HIV-related question answering
	2. Evaluation of LLM performance using clinical relevance parameters
	3. Insights on the performance trade-offs of general-purpose vs. medically fine-tuned models

**Result:** Gemini 2.5 Pro outperformed all other evaluated models, though performance declined with increased question complexity, and no strong correlation was found between model size and performance.

**Limitations:** Limited exploration of AI applications specifically for HIV care and concerns about the accuracy and safety of LLMs in clinical practice.

**Conclusion:** The findings indicate the necessity for targeted development to ensure the safe and effective use of LLMs in clinical settings, particularly in complex domains like HIV care.

**Abstract:** Large language models (LLMs) are emerging as valuable tools to support clinicians in routine decision-making. HIV management is a compelling use case due to its complexity, including diverse treatment options, comorbidities, and adherence challenges. However, integrating LLMs into clinical practice raises concerns about accuracy, potential harm, and clinician acceptance. Despite their promise, AI applications in HIV care remain underexplored, and LLM benchmarking studies are scarce. This study evaluates the current capabilities of LLMs in HIV management, highlighting their strengths and limitations. We introduce HIVMedQA, a benchmark designed to assess open-ended medical question answering in HIV care. The dataset consists of curated, clinically relevant questions developed with input from an infectious disease physician. We evaluated seven general-purpose and three medically specialized LLMs, applying prompt engineering to enhance performance. Our evaluation framework incorporates both lexical similarity and an LLM-as-a-judge approach, extended to better reflect clinical relevance. We assessed performance across key dimensions: question comprehension, reasoning, knowledge recall, bias, potential harm, and factual accuracy. Results show that Gemini 2.5 Pro consistently outperformed other models across most dimensions. Notably, two of the top three models were proprietary. Performance declined as question complexity increased. Medically fine-tuned models did not always outperform general-purpose ones, and larger model size was not a reliable predictor of performance. Reasoning and comprehension were more challenging than factual recall, and cognitive biases such as recency and status quo were observed. These findings underscore the need for targeted development and evaluation to ensure safe, effective LLM integration in clinical care.

</details>


### [54] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)

*Kexin Chen, Dongxia Wang, Yi Liu, Haonan Zhang, Wenhai Wang*

**Main category:** cs.CL

**Keywords:** Sticky tokens, Text embeddings, NLP, Tokenization, Model performance

**Relevance Score:** 8

**TL;DR:** This paper investigates 'sticky tokens' in Transformer-based text embeddings, which distort sentence similarity and degrade performance in NLP tasks. It introduces a detection method and reveals significant adverse effects on downstream applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the negative impact of sticky tokens on the reliability of embeddings used in NLP tasks.

**Method:** Introduces a detection method called Sticky Token Detector (STD), analyzing 40 checkpoints across 14 model families to identify anomalous tokens.

**Key Contributions:**

	1. Formal definition and detection method for sticky tokens
	2. Analysis revealing the origin of these tokens
	3. Evaluation of their impact on NLP task performance

**Result:** Discovered 868 sticky tokens that primarily originate from unused vocabulary entries and subwords in multilingual corpora. Their presence can cause performance drops up to 50% in downstream tasks like clustering and retrieval.

**Limitations:** The study focuses on specific models and may not generalize to all NLP frameworks or embeddings.

**Conclusion:** The study underscores the necessity for improved tokenization strategies and model design to combat the detrimental effects of sticky tokens in text embedding applications.

**Abstract:** Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.

</details>


### [55] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)

*Wonjun Jeong, Dongseok Kim, Taegkeun Whangbo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias Mitigation, Evaluation Framework, Selection Bias, Debiasing Methods

**Relevance Score:** 8

**TL;DR:** SCOPE is a framework that addresses selection bias in LLM evaluations by estimating and mitigating position-bias distributions, leading to more reliable performance assessments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the inflated scores of LLMs on multiple-choice tasks that arise from selection bias rather than true understanding of the material.

**Method:** SCOPE uses a null prompt to estimate models' position-bias, redistributes answer slots based on inverse-bias distributions, and prevents proximity cues from leading to incorrect guesses.

**Key Contributions:**

	1. Introduces SCOPE framework for bias mitigation in LLM evaluations
	2. Establishes new methods for measuring and redistributing selection bias
	3. Demonstrates superior performance and reliability compared to existing methods.

**Result:** SCOPE showed consistent performance improvements over existing debiasing methods and clearer confidence distributions across multiple benchmark experiments.

**Limitations:** 

**Conclusion:** SCOPE establishes a new standard for fairness and reliability in LLM evaluations, improving assessment outcomes by mitigating biases.

**Abstract:** Large Language Models (LLMs) can achieve inflated scores on multiple-choice tasks by exploiting inherent biases in option positions or labels, rather than demonstrating genuine understanding. This study introduces SCOPE, an evaluation framework designed to measure and mitigate such selection bias in a dataset-independent manner. By repeatedly invoking a null prompt that lacks semantic content, SCOPE estimates each model's unique position-bias distribution. It then redistributes the answer slot according to the inverse-bias distribution, thereby equalizing the lucky-rate, the probability of selecting the correct answer by chance. Furthermore, it prevents semantically similar distractors from being placed adjacent to the answer, thereby blocking near-miss guesses based on superficial proximity cues. Across multiple benchmark experiments, SCOPE consistently outperformed existing debiasing methods in terms of stable performance improvements and showed clearer confidence distributions over correct options. This framework thus offers a new standard for enhancing the fairness and reliability of LLM evaluations.

</details>


### [56] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)

*Keyu Wu, Qianjin Yu, Manlin Mei, Ruiting Liu, Jun Wang, Kailai Zhang, Yelun Bao*

**Main category:** cs.CL

**Keywords:** Root Cause Analysis, Artificial Intelligence, Telecommunication Networks, Graph-based Reasoning, Benchmarks

**Relevance Score:** 4

**TL;DR:** The paper discusses the challenges of applying AI to Root Cause Analysis in telecommunication networks, focusing on graph-based reasoning and the lack of benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Root Cause Analysis (RCA) is essential in maintaining effective telecommunication networks but poses significant difficulties for AI systems due to its intricate reasoning demands and insufficient realistic datasets for training and evaluation.

**Method:** 

**Key Contributions:**



**Result:** 

**Limitations:** 

**Conclusion:** 

**Abstract:** Root Cause Analysis (RCA) in telecommunication networks is a critical task, yet it presents a formidable challenge for Artificial Intelligence (AI) due to its complex, graph-based reasoning requirements and the scarcity of realistic benchmarks.

</details>


### [57] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)

*Aline Belloni, Patrick Prieur*

**Main category:** cs.CL

**Keywords:** ISO 9001, ISO 30401, Knowledge Management System, SECI model, PDCA cycles

**Relevance Score:** 3

**TL;DR:** This paper discusses the integration of Knowledge Management Systems (KMS) with ISO 9001 compliant operational processes, emphasizing the SECI model and PDCA cycles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explain the integration of knowledge development and management activities within existing operational processes in organizations compliant with ISO 9001 and ISO 30401.

**Method:** The article recaps process modeling principles under ISO 9001 and explores the integration of ISO 30401-compliant KMS with operational processes using the SECI model and PDCA cycles.

**Key Contributions:**

	1. Integration of ISO 30401 into existing operational processes
	2. Application of SECI model in organizational settings
	3. Alignment of Knowledge Management with ISO 9001 standards

**Result:** The study illustrates how ISO 30401 principles can enhance the effectiveness of organizational processes by synchronizing knowledge management and workflow activities.

**Limitations:** 

**Conclusion:** A well-implemented KMS based on ISO 30401 not only streamlines knowledge activities but aligns them with strategic goals, improving overall organizational efficiency.

**Abstract:** Business process modeling is used by most organizations as an essential framework for ensuring efficiency and effectiveness of the work and workflow performed by its employees and for ensuring the alignment of such work with its strategic goals. For organizations that are compliant or near-compliant with ISO 9001, this approach involves the detailed mapping of processes, sub-processes, activities, and tasks. ISO30401 is a Management System Standard, introduced in 2018, establishing universal requirements for the set up of a Knowledge Management System in an organization. As ``ISO30401 implementers'' we regularly face the challenge of explaining our clients how the knowledge development, transformation and conveyances activities depicted in ISO30401 do integrate with existing operational processes. This article recaps process modelling principles in the context of ISO9001 and explores, based on our experience, how an ISO30401-compliant Knowledge Management System (KMS) entwines with all other processes of an Integrated Management System and in particular how it can be implemented by deploying the mechanisms of the SECI model through the steps of PDCA cycles.

</details>


### [58] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)

*San Kim, Jonghwi Kim, Yejin Jeon, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, adversarial defense, poisoning attacks, masked language modeling

**Relevance Score:** 9

**TL;DR:** This paper presents GMTP, a defense mechanism to detect and filter adversarial documents in Retrieval-Augmented Generation systems using gradient analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of external sources in LLMs poses security risks from attackers injecting poisoned documents, leading to harmful outputs.

**Method:** The proposed GMTP method examines the gradients of the retriever's similarity function to identify high-impact tokens, which are then masked and their probabilities checked using an MLM to detect poisoned content.

**Key Contributions:**

	1. Introduction of GMTP as a novel defense method
	2. Demonstration of high-precision filtering of poisoned documents
	3. Validation of GMTP's effectiveness across diverse datasets and adversarial settings.

**Result:** GMTP can eliminate over 90% of poisoned documents while maintaining the relevance of legitimate content, thus ensuring effective retrieval and generation.

**Limitations:** 

**Conclusion:** This paper showcases GMTP as a robust solution for enhancing the security of RAG systems against adversarial attacks without compromising performance.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by providing external knowledge for accurate and up-to-date responses. However, this reliance on external sources exposes a security risk, attackers can inject poisoned documents into the knowledge base to steer the generation process toward harmful or misleading outputs. In this paper, we propose Gradient-based Masked Token Probability (GMTP), a novel defense method to detect and filter out adversarially crafted documents. Specifically, GMTP identifies high-impact tokens by examining gradients of the retriever's similarity function. These key tokens are then masked, and their probabilities are checked via a Masked Language Model (MLM). Since injected tokens typically exhibit markedly low masked-token probabilities, this enables GMTP to easily detect malicious documents and achieve high-precision filtering. Experiments demonstrate that GMTP is able to eliminate over 90% of poisoned content while retaining relevant documents, thus maintaining robust retrieval and generation performance across diverse datasets and adversarial settings.

</details>


### [59] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)

*Kyubeen Han, Junseo Jang, Hongjin Kim, Geunyeong Jeong, Harksoo Kim*

**Main category:** cs.CL

**Keywords:** instruction-tuning, large language models, misinformation susceptibility, user input, system prompts

**Relevance Score:** 9

**TL;DR:** This paper investigates how instruction-tuning affects large language models' susceptibility to misinformation, showing an increased reliance on user input leading to greater misinformation acceptance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to understand the effects of instruction-tuning on the behavior of large language models, specifically their handling of misinformation presented by users.

**Method:** The study conducts an analysis comparing instruction-tuned LLMs with base models to measure their susceptibility to misinformation under various conditions.

**Key Contributions:**

	1. Investigates the effects of instruction-tuning on misinformation susceptibility in LLMs
	2. Compares instruction-tuned models with base models
	3. Examines user prompt structure and its impacts on misinformation acceptance.

**Result:** Instruction-tuned LLMs are significantly more likely to accept misinformation from users compared to base models, indicating a shift in susceptibility from the model to the user.

**Limitations:** The study primarily focuses on the impact of instruction-tuning without exploring other potential influences on misinformation acceptance comprehensively.

**Conclusion:** The findings highlight the need for systematic approaches to mitigate the unintended consequences of instruction-tuning, emphasizing the importance of improving LLM reliability in practical applications.

**Abstract:** Instruction-tuning enhances the ability of large language models (LLMs) to follow user instructions more accurately, improving usability while reducing harmful outputs. However, this process may increase the model's dependence on user input, potentially leading to the unfiltered acceptance of misinformation and the generation of hallucinations. Existing studies primarily highlight that LLMs are receptive to external information that contradict their parametric knowledge, but little research has been conducted on the direct impact of instruction-tuning on this phenomenon. In our study, we investigate the impact of instruction-tuning on LLM's susceptibility to misinformation. Our analysis reveals that instruction-tuned LLMs are significantly more likely to accept misinformation when it is presented by the user. A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role. Furthermore, we explore additional factors influencing misinformation susceptibility, such as the role of the user in prompt structure, misinformation length, and the presence of warnings in the system prompt. Our findings underscore the need for systematic approaches to mitigate unintended consequences of instruction-tuning and enhance the reliability of LLMs in real-world applications.

</details>


### [60] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)

*Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan*

**Main category:** cs.CL

**Keywords:** layer pruning, large language models, magnitude compensation

**Relevance Score:** 6

**TL;DR:** This paper introduces Prune&Comp, a layer pruning scheme for large language models that mitigates performance degradation due to layer removal by rescaling weights offline while maintaining performance metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant performance degradation caused by the removal of layers in large language models during pruning.

**Method:** Prune&Comp estimates the magnitude gap from layer removal and compensates by rescaling remaining weights offline without runtime overhead, additionally employing an iterative pruning strategy.

**Key Contributions:**

	1. Introduction of the Prune&Comp method for layer pruning
	2. Demonstration of performance retention in LLaMA-3-8B
	3. Establishment of a new iterative pruning strategy

**Result:** Prune&Comp achieves nearly half the perplexity and retains 93.19% of the original model's question-answering performance when 5 layers of LLaMA-3-8B are pruned, outperforming the baseline by 4.01%.

**Limitations:** 

**Conclusion:** Prune&Comp offers a significant improvement in layer pruning for large language models by addressing the hidden state magnitude gap effectively, leading to better performance metrics.

**Abstract:** Layer pruning has emerged as a promising technique for compressing large language models (LLMs) while achieving acceleration proportional to the pruning ratio. In this work, we identify that removing any layer induces a significant magnitude gap in hidden states, resulting in substantial performance degradation. To address this issue, we propose Prune&Comp, a novel plug-and-play layer pruning scheme that leverages magnitude compensation to mitigate such gaps in a training-free manner. Specifically, we first estimate the magnitude gap caused by layer removal and then eliminate this gap by rescaling the remaining weights offline, with zero runtime overhead incurred. We further demonstrate the advantages of Prune&Comp through an iterative pruning strategy. When integrated with an iterative prune-and-compensate loop, Prune&Comp consistently enhances existing layer pruning metrics. For instance, when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the original model's question-answering performance, outperforming the baseline by 4.01%.

</details>


### [61] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)

*Suhang Wu, Jialong Tang, Chengyi Yang, Pei Zhang, Baosong Yang, Junhui Li, Junfeng Yao, Min Zhang, Jinsong Su*

**Main category:** cs.CL

**Keywords:** speech translation, terminology translation, Locate-and-Focus

**Relevance Score:** 6

**TL;DR:** This paper introduces a Locate-and-Focus method for improving terminology translation in speech translation models by effectively locating terminologies in speech clips and minimizing irrelevant information.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The accurate translation of terminology in direct speech translation is a significant challenge, especially with interference from irrelevant noise.

**Method:** The proposed Locate-and-Focus method locates speech clips containing terminologies to construct translation knowledge and associates it with the utterance from both audio and textual modalities.

**Key Contributions:**

	1. Introduction of the Locate-and-Focus method for speech translation.
	2. Effective locating of terminologies within utterances.
	3. Improved success rate of terminology translation in diverse datasets.

**Result:** The method effectively locates terminologies, enhances terminology translation success rates, and maintains general translation performance across various datasets.

**Limitations:** 

**Conclusion:** The proposed method demonstrates improved terminology translation accuracy without compromising overall translation effectiveness.

**Abstract:** Direct speech translation (ST) has garnered increasing attention nowadays, yet the accurate translation of terminology within utterances remains a great challenge. In this regard, current studies mainly concentrate on leveraging various translation knowledge into ST models. However, these methods often struggle with interference from irrelevant noise and can not fully utilize the translation knowledge. To address these issues, in this paper, we propose a novel Locate-and-Focus method for terminology translation. It first effectively locates the speech clips containing terminologies within the utterance to construct translation knowledge, minimizing irrelevant information for the ST model. Subsequently, it associates the translation knowledge with the utterance and hypothesis from both audio and textual modalities, allowing the ST model to better focus on translation knowledge during translation. Experimental results across various datasets demonstrate that our method effectively locates terminologies within utterances and enhances the success rate of terminology translation, while maintaining robust general translation performance.

</details>


### [62] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)

*Nevidu Jayatilleke, Nisansa de Silva*

**Main category:** cs.CL

**Keywords:** Optical Character Recognition, Low-Resourced Languages, Sinhala, Tamil, OCR benchmarking

**Relevance Score:** 4

**TL;DR:** This study analyzes the performance of six OCR engines on Low-Resourced Languages (Sinhala and Tamil), revealing key findings on their effectiveness and introducing a novel Tamil OCR benchmarking dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective OCR solutions for Low-Resourced Languages (LRLs) like Sinhala and Tamil, where existing research and technology are limited compared to High-Resourced Languages.

**Method:** The study conducts a comparative analysis of six OCR engines (Cloud Vision API, Surya, Document AI, Tesseract, Subasa OCR, EasyOCR) on their performance in processing Sinhala and Tamil text, utilizing five measurement techniques for evaluation.

**Key Contributions:**

	1. A comparative analysis of multiple OCR engines on LRLs.
	2. Identification of the best-performing OCR solutions for Sinhala and Tamil.
	3. Introduction of a novel synthetic Tamil OCR benchmarking dataset.

**Result:** Surya performed the best for Sinhala with a WER of 2.61%, while Document AI had the best results for Tamil with a CER of 0.78%.

**Limitations:** The limitations of some OCR engines restricted their evaluation to only one language, which may affect generalizability.

**Conclusion:** The findings highlight significant disparities in OCR engine performance for different Low-Resourced Languages, emphasizing the need for further development and evaluation in this area, alongside the introduction of a novel benchmarking dataset for Tamil OCR.

**Abstract:** Solving the problem of Optical Character Recognition (OCR) on printed text for Latin and its derivative scripts can now be considered settled due to the volumes of research done on English and other High-Resourced Languages (HRL). However, for Low-Resourced Languages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative analysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The selected engines include both commercial and open-source systems, aiming to evaluate the strengths of each category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations. The performance of these systems was rigorously analysed using five measurement techniques to assess accuracy at both the character and word levels. According to the findings, Surya delivered the best performance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled across all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above analysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.

</details>


### [63] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)

*Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu*

**Main category:** cs.CL

**Keywords:** LLMs, stylistic adaptation, instruction-following models, LoRA, enterprise communication

**Relevance Score:** 8

**TL;DR:** StyleAdaptedLM is a framework for adapting LLMs to specific stylistic characteristics without compromising instruction adherence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for stylistic adaptation in enterprise communication poses challenges, especially when working with unstructured corpora.

**Method:** StyleAdaptedLM employs Low-Rank Adaptation (LoRA) to train stylistic traits on a base model, which is then merged with an instruction-following model.

**Key Contributions:**

	1. Introduces StyleAdaptedLM for stylistic adaptation of LLMs.
	2. Utilizes LoRA for efficient training without paired stylistic data.
	3. Demonstrates robust stylistic customization with preserved instruction adherence.

**Result:** Experiments show improved stylistic consistency and human evaluations confirm successful uptake of brand-specific conventions without paired data.

**Limitations:** 

**Conclusion:** StyleAdaptedLM provides an efficient means for achieving stylistic personalization in language models while maintaining performance on tasks.

**Abstract:** Adapting LLMs to specific stylistic characteristics, like brand voice or authorial tones, is crucial for enterprise communication but challenging to achieve from corpora which lacks instruction-response formatting without compromising instruction adherence. We introduce StyleAdaptedLM, a framework that efficiently transfers stylistic traits to instruction-following models using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base model with diverse unstructured stylistic corpora, then merged with a separate instruction-following model. This enables robust stylistic customization without paired data or sacrificing task performance. Experiments across multiple datasets and models demonstrate improved stylistic consistency while preserving instruction adherence, with human evaluations confirming brand-specific convention uptake. StyleAdaptedLM offers an efficient path for stylistic personalization in LLMs.

</details>


### [64] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)

*Biao Yi, Zekun Fei, Jianing Geng, Tong Li, Lihai Nie, Zheli Liu, Yiming Li*

**Main category:** cs.CL

**Keywords:** large reasoning models, data poisoning, backdoor attacks, chain-of-thought, resource consumption

**Relevance Score:** 6

**TL;DR:** The paper introduces a novel attack on large reasoning models (LRMs) called 'overthinking backdoors', which allows an attacker to control the verbosity of reasoning outputs while maintaining answer correctness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large reasoning models (LRMs) in AI, there is a need to understand potential vulnerabilities that can be exploited to manipulate their reasoning capabilities.

**Method:** The authors propose a tunable backdoor attack achieved through data poisoning, where the number of repetitions of a trigger correlates to the increase in reasoning verbosity.

**Key Contributions:**

	1. Introduction of 'overthinking backdoors' as a new attack vector for LRMs
	2. Development of a tunable backdoor approach using data poisoning
	3. Demonstration of the effectiveness of the attack on various LRMs without loss of output correctness

**Result:** Extensive empirical results show that the proposed attack can cause a multi-fold increase in reasoning process length without degrading the correctness of the final answer.

**Limitations:** 

**Conclusion:** The proposed method introduces a stealthy attack vector that can consume resources without compromising output accuracy, highlighting vulnerabilities in LRMs.

**Abstract:** Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term "overthinking backdoors". We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model's reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer's correctness. Our source code is available at https://github.com/FZaKK/BadReasoner.

</details>


### [65] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)

*Maciej Skorski, Alina Landowska*

**Main category:** cs.CL

**Keywords:** moral foundation detection, large language models, fine-tuned transformers, ethical AI, moral reasoning

**Relevance Score:** 8

**TL;DR:** This study investigates the effectiveness of large language models (LLMs) in moral foundation detection versus fine-tuned transformers, revealing significant performance gaps and indicating that fine-tuning is more reliable for this specific task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the ability of AI systems, particularly LLMs, in moral reasoning and to improve their alignment with ethical considerations in social discourse.

**Method:** A comparative analysis of state-of-the-art LLMs and fine-tuned transformers on Twitter and Reddit datasets using ROC, PR, and DET curve analysis.

**Key Contributions:**

	1. First comprehensive comparison of LLMs with fine-tuned transformers for moral foundation detection.
	2. Identification of significant performance gaps in LLMs regarding moral reasoning.
	3. Demonstration of the advantages of fine-tuning over prompt engineering in LLMs.

**Result:** The study found that LLMs had high false negative rates and systematically failed to detect moral content, showing the limitations of their prompting capabilities.

**Limitations:** 

**Conclusion:** Task-specific fine-tuning is more effective than prompting for moral reasoning tasks in AI applications.

**Abstract:** Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear.   This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.   Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.

</details>


### [66] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)

*Ieva Raminta StaliÅ«naitÄ—, Julius Cheng, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** machine translation, gender bias, semantic uncertainty, debiasing, contextual information

**Relevance Score:** 7

**TL;DR:** The paper explores biases in machine translation models regarding gender specification, suggesting models should recognize ambiguity in gender contexts while maintaining uncertainty when necessary.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bias present in machine translation models that rely on stereotypes instead of contextual information for gender specification.

**Method:** The study employs newly proposed metrics of semantic uncertainty to evaluate the performance of machine translation models in translating gender-ambiguous and gender-specific instances.

**Key Contributions:**

	1. Introduces the concept of maintaining uncertainty in gender-ambiguous translations
	2. Demonstrates the disparity between model accuracy and uncertainty levels
	3. Evaluates the independent effects of debiasing in ambiguous vs unambiguous cases

**Result:** Findings reveal that high translation and gender accuracy does not correlate with expected uncertainty levels in ambiguous gender cases; debiasing affects these instances differently.

**Limitations:** The study mainly focuses on gender biases, limiting its applicability to other domains of bias in machine translation.

**Conclusion:** The research indicates that machine translation models should not only prioritize accurate gender translation but also appropriately express uncertainty in ambiguous contexts to reduce biases.

**Abstract:** In machine translation (MT), when the source sentence includes a lexeme whose gender is not overtly marked, but whose target-language equivalent requires gender specification, the model must infer the appropriate gender from the context and/or external knowledge. Studies have shown that MT models exhibit biased behaviour, relying on stereotypes even when they clash with contextual information. We posit that apart from confidently translating using the correct gender when it is evident from the input, models should also maintain uncertainty about the gender when it is ambiguous. Using recently proposed metrics of semantic uncertainty, we find that models with high translation and gender accuracy on unambiguous instances do not necessarily exhibit the expected level of uncertainty in ambiguous ones. Similarly, debiasing has independent effects on ambiguous and unambiguous translation instances.

</details>


### [67] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)

*Yifu Chen, Bingchen Huang, Zhiling Wang, Yuanchao Du, Junfeng Luo, Lei Shen, Zhineng chen*

**Main category:** cs.CL

**Keywords:** in-context learning, example retrieval, language models, NLP tasks, fine-grained feedback

**Relevance Score:** 9

**TL;DR:** The paper presents TDR, a framework that enhances example retrieval for in-context learning (ICL) in language models by decoupling task examples and incorporating LLM feedback to improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in retrieving high-quality examples for in-context learning from multi-task datasets, specifically by distinguishing cross-task distributions and connecting retriever outputs to LLM feedback.

**Method:** The TDR framework separates ICL examples by task and utilizes feedback from LLMs to train the retrieval module, enhancing the quality of examples retrieved for specific tasks.

**Key Contributions:**

	1. Decoupling ICL examples across different tasks to improve retrieval accuracy.
	2. Incorporating fine-grained feedback from LLMs for better supervised training of the retrieval module.
	3. Demonstrating state-of-the-art performance on a wide array of NLP tasks.

**Result:** Experiments on 30 NLP tasks showed that TDR consistently improved results across all datasets and achieved state-of-the-art performance.

**Limitations:** 

**Conclusion:** TDR is a plug-and-play method that can be integrated with various LLMs to enhance their example retrieval capabilities for ICL.

**Abstract:** In-context learning (ICL) has become a classic approach for enabling LLMs to handle various tasks based on a few input-output examples. The effectiveness of ICL heavily relies on the quality of these examples, and previous works which focused on enhancing example retrieval capabilities have achieved impressive performances. However, two challenges remain in retrieving high-quality examples: (1) Difficulty in distinguishing cross-task data distributions, (2) Difficulty in making the fine-grained connection between retriever output and feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR decouples the ICL examples from different tasks, which enables the retrieval module to retrieve examples specific to the target task within a multi-task dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise and guide the training of the retrieval module, which helps to retrieve high-quality examples. We conducted extensive experiments on a suite of 30 NLP tasks, the results demonstrate that TDR consistently improved results across all datasets and achieves state-of-the-art performance. Meanwhile, our approach is a plug-and-play method, which can be easily combined with various LLMs to improve example retrieval abilities for ICL. The code is available at https://github.com/Nnn-s/TDR.

</details>


### [68] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)

*Ariana Sahitaj, Premtim Sahitaj, Veronika Solopova, Jiaao Li, Sebastian MÃ¶ller, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** propaganda detection, large language models, human-computer interaction, annotation consistency, knowledge distillation

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework for propaganda detection on social media that integrates human expertise with LLM assistance, focusing on improved annotation consistency and scalability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in propaganda detection on social media, specifically the complexity of the task and the scarcity of high-quality labeled data.

**Method:** The study introduces a hierarchical taxonomy for 14 propaganda techniques, conducts a human annotation study on the HQP dataset, and implements an LLM-assisted pre-annotation pipeline for efficient annotation processes. It also includes verification studies for agreement and efficiency improvements, and fine-tunes smaller language models using LLM-generated data.

**Key Contributions:**

	1. Introduction of a hierarchical taxonomy for fine-grained propaganda techniques
	2. Development of an LLM-assisted annotation pipeline
	3. Demonstration of scalability through knowledge distillation from LLMs to smaller models.

**Result:** The LLM-assisted approach resulted in significant improvements in inter-annotator agreement and annotation speed, demonstrating the feasibility of using LLMs for enhancing the quality of propaganda detection.

**Limitations:** Further exploration is needed to validate the framework across diverse social media platforms and languages.

**Conclusion:** The research showcases a scalable method for propaganda detection that could contribute to creating transparent media ecosystems, promoting accountability and adherence to Sustainable Development Goals (SDG) 16.

**Abstract:** Propaganda detection on social media remains challenging due to task complexity and limited high-quality labeled data. This paper introduces a novel framework that combines human expertise with Large Language Model (LLM) assistance to improve both annotation consistency and scalability. We propose a hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into three broader categories, conduct a human annotation study on the HQP dataset that reveals low inter-annotator agreement for fine-grained labels, and implement an LLM-assisted pre-annotation pipeline that extracts propagandistic spans, generates concise explanations, and assigns local labels as well as a global label. A secondary human verification study shows significant improvements in both agreement and time-efficiency. Building on this, we fine-tune smaller language models (SLMs) to perform structured annotation. Instead of fine-tuning on human annotations, we train on high-quality LLM-generated data, allowing a large model to produce these annotations and a smaller model to learn to generate them via knowledge distillation. Our work contributes towards the development of scalable and robust propaganda detection systems, supporting the idea of transparent and accountable media ecosystems in line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [69] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)

*Asaf Yehudai, Lilach Eden, Yotam Perlitz, Roy Bar-Haim, Michal Shmueli-Scheuer*

**Main category:** cs.CL

**Keywords:** Large Language Models, error analysis, interactive dashboard, benchmarking, feedback

**Relevance Score:** 9

**TL;DR:** Introducing CLEAR, an interactive package for LLM-based error analysis that provides actionable feedback and visualizations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide clearer insights into model evaluation by addressing the limitations of current LLM evaluation paradigms that yield single scores without explaining model performance.

**Method:** CLEAR generates per-instance feedback, creates system-level error issues, and quantifies their prevalence. It features an interactive dashboard for comprehensive error analysis.

**Key Contributions:**

	1. Interactive, open-source error analysis package for LLMs
	2. Generates actionable feedback and system-level insights
	3. User-friendly dashboard with visualizations and filtering options

**Result:** Demonstrated CLEAR analysis effectiveness through RAG and Math benchmarks, providing valuable insights into model behavior and specific error types.

**Limitations:** 

**Conclusion:** CLEAR offers a solution for in-depth evaluation of LLMs, enabling users to understand model performance beyond singular scores.

**Abstract:** The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.

</details>


### [70] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)

*Silvia Cappa, Lingxiao Kong, Pille-Riin Peet, Fanfu Wei, Yuchen Zhou, Jan-Christoph Kalo*

**Main category:** cs.CL

**Keywords:** cross-lingual inconsistencies, Wikipedia, structured content, tabular data, multilingual alignment

**Relevance Score:** 4

**TL;DR:** This study analyzes cross-lingual inconsistencies in Wikipedia's structured content, focusing on tabular data, and proposes a methodology for their assessment and alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses factual inconsistencies in Wikipedia's various language versions, which can affect its neutrality and reliability, especially for AI systems that use Wikipedia as a training source.

**Method:** The authors developed a methodology to collect, align, and analyze tables from multilingual Wikipedia articles, defining categories of inconsistency and applying quantitative and qualitative metrics to assess multilingual alignment.

**Key Contributions:**

	1. Development of a novel methodology for aligning tables in multilingual Wikipedia articles.
	2. Identification of categories of inconsistency in structured content across languages.
	3. Assessment metrics for evaluating multilingual alignment of Wikipedia content.

**Result:** The investigation highlights significant cross-lingual inconsistencies in Wikipedia's structured content, emphasizing the need for improved methodologies in factual verification and AI system design.

**Limitations:** The study is limited to tabular data and may not generalize to all types of content in Wikipedia.

**Conclusion:** The findings underscore the implications for multilingual knowledge interaction and the importance of addressing inconsistencies to enhance the reliability of AI systems that depend on Wikipedia.

**Abstract:** Wikipedia serves as a globally accessible knowledge source with content in over 300 languages. Despite covering the same topics, the different versions of Wikipedia are written and updated independently. This leads to factual inconsistencies that can impact the neutrality and reliability of the encyclopedia and AI systems, which often rely on Wikipedia as a main training source. This study investigates cross-lingual inconsistencies in Wikipedia's structured content, with a focus on tabular data. We developed a methodology to collect, align, and analyze tables from Wikipedia multilingual articles, defining categories of inconsistency. We apply various quantitative and qualitative metrics to assess multilingual alignment using a sample dataset. These insights have implications for factual verification, multilingual knowledge interaction, and design for reliable AI systems leveraging Wikipedia content.

</details>


### [71] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)

*Giorgos Iacovides, Wuyang Zhou, Danilo Mandic*

**Main category:** cs.CL

**Keywords:** sentiment analysis, finance, large language models, Direct Preference Optimization, portfolio strategies

**Relevance Score:** 4

**TL;DR:** FinDPO is a finance-specific LLM framework that enhances sentiment analysis for trading decisions by addressing the limitations of supervised fine-tuned models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing impact of online finance-related opinions on trading decisions necessitates effective sentiment analysis to quantify these opinions.

**Method:** FinDPO utilizes post-training human preference alignment via Direct Preference Optimization to enhance LLM performance specifically in financial sentiment analysis.

**Key Contributions:**

	1. Introduction of FinDPO as a finance-specific LLM framework
	2. Improvement over existing models with an 11% performance increase
	3. Ability to integrate sentiment analysis into portfolio strategies with significant returns.

**Result:** FinDPO outperforms existing SFT models by 11% on sentiment classification benchmarks and achieves a 67% annual return in simulations even under transaction costs.

**Limitations:** 

**Conclusion:** FinDPO proves effective in adapting sentiment analysis to the financial domain, enabling practical application in portfolio strategies and demonstrating strong financial performance metrics.

**Abstract:** Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).

</details>


### [72] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)

*Rana Alshaikh, Israa Alghanmi, Shelan Jeawak*

**Main category:** cs.CL

**Keywords:** Large Language Models, Arabic Tabular Data, Benchmarking, Natural Language Processing, Cognitive Reasoning

**Relevance Score:** 8

**TL;DR:** AraTable is a benchmark for evaluating LLMs' understanding of Arabic tabular data, highlighting significant reasoning challenges and providing a novel evaluation framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap of large language models (LLMs) in interpreting Arabic tabular data due to limited public resources and unique language features.

**Method:** A hybrid pipeline where task content is generated by LLMs and then filtered and verified by human experts, including tasks for direct question answering, fact verification, and complex reasoning.

**Key Contributions:**

	1. Introduction of AraTable as a benchmark for Arabic tabular data
	2. Development of a new fully automated evaluation framework
	3. Insights into LLM performance limitations and opportunities for improvement in complex reasoning tasks

**Result:** Initial analyses reveal LLMs perform adequately on simpler tasks but struggle with deeper reasoning; a fully automated evaluation framework shows comparable results to human judges.

**Limitations:** The study is focused solely on Arabic tabular data and may not generalize to other languages or data types.

**Conclusion:** AraTable offers a valuable benchmark and framework to enhance LLM performance on Arabic structured data, paving the way for future studies.

**Abstract:** The cognitive and reasoning abilities of large language models (LLMs) have enabled remarkable progress in natural language processing. However, their performance in interpreting structured data, especially in tabular formats, remains limited. Although benchmarks for English tabular data are widely available, Arabic is still underrepresented because of the limited availability of public resources and its unique language features. To address this gap, we present AraTable, a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. AraTable consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources. Our methodology follows a hybrid pipeline, where initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality. Initial analyses using AraTable show that, while LLMs perform adequately on simpler tabular tasks such as direct question answering, they continue to face significant cognitive challenges when tasks require deeper reasoning and fact verification. This indicates that there are substantial opportunities for future work to improve performance on complex tabular reasoning tasks. We also propose a fully automated evaluation framework that uses a self-deliberation mechanism and achieves performance nearly identical to that of human judges. This research provides a valuable, publicly available resource and evaluation framework that can help accelerate the development of foundational models for processing and analysing Arabic structured data.

</details>


### [73] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)

*Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu*

**Main category:** cs.CL

**Keywords:** Punctuation Restoration, XLM-RoBERTa, Bangla, Automatic Speech Recognition, Low-resource NLP

**Relevance Score:** 6

**TL;DR:** This study investigates using transformer models for punctuation restoration in unpunctuated Bangla text, achieving high accuracy and providing resources for future research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance the readability of Bangla text, especially for Automatic Speech Recognition applications in low-resource contexts.

**Method:** The study uses XLM-RoBERTa-large, trained on a large corpus with data augmentation techniques to predict punctuation marks in Bangla text.

**Key Contributions:**

	1. Application of a transformer model for punctuation restoration in Bangla
	2. Creation of a diverse training corpus for low-resource languages
	3. Public availability of datasets and code for future research

**Result:** The best model achieved 97.1% accuracy on the News test set and strong generalization to noisy ASR transcripts.

**Limitations:** 

**Conclusion:** This work establishes a robust baseline for punctuation restoration in Bangla and contributes datasets and code for future low-resource NLP research.

**Abstract:** Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.   Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.

</details>


### [74] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)

*Basel Alshaikhdeeb, Ahmed Abdelmonem Hemedan, Soumyabrata Ghosh, Irina Balaur, Venkata Satagopam*

**Main category:** cs.CL

**Keywords:** Synthetic medical text, Natural Language Processing, Transformer architectures

**Relevance Score:** 8

**TL;DR:** This paper presents a systematic review of techniques and evaluation methods for generating synthetic medical text, emphasizing the use of transformer architectures like GPTs and the need for privacy considerations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address common clinical NLP issues like text sparsity and privacy concerns by generating synthetic medical free-text.

**Method:** Conducted a systematic review by searching major databases for relevant articles and analyzing them based on purpose, techniques, and evaluation methods.

**Key Contributions:**

	1. Systematic review of synthetic medical text generation techniques
	2. Analysis of evaluation methods for synthetic medical texts
	3. Identification of key purposes for generating synthetic medical text

**Result:** Identified 94 relevant articles, with a focus on text augmentation, assistive writing, and privacy-preserving methods, primarily using transformer architectures.

**Limitations:** More human assessments are needed to ensure the privacy of generated texts; some concerns regarding the use of synthetic text in real applications remain.

**Conclusion:** Synthetic medical texts can effectively assist in healthcare NLP tasks, but privacy issues need more attention; further advances can enhance workflow efficiency.

**Abstract:** Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.

</details>


### [75] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)

*Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** Large Language Models, tabular data, dependency graphs, attention mechanism, data generation

**Relevance Score:** 7

**TL;DR:** GraDe integrates sparse dependency graphs into LLMs' attention to improve tabular data generation by prioritizing critical feature interactions while reducing focus on irrelevant ones.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with tabular data due to the sparse feature-level dependencies that dilute attention on important feature interactions.

**Method:** GraDe uses a dynamic graph learning module to integrate functional dependencies into the attention mechanism of LLMs, enhancing structure-aware modeling of tabular data.

**Key Contributions:**

	1. Introduction of GraDe for integrating sparse dependency graphs into LLMs
	2. Improved performance on complex datasets
	3. Minimal intrusion into the existing LLM architecture

**Result:** GraDe outperforms existing LLM-based methods by up to 12% on complex datasets and is competitive with state-of-the-art methods in synthetic data quality.

**Limitations:** 

**Conclusion:** GraDe offers an effective and practical method for improving tabular data generation with LLMs by leveraging dependency graphs.

**Abstract:** Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.

</details>


### [76] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)

*Maciej Skorski, Alina Landowska*

**Main category:** cs.CL

**Keywords:** moral foundation detection, large language models, fine-tuning, ethical AI, social discourse

**Relevance Score:** 8

**TL;DR:** This study compares LLMs and fine-tuned transformers for moral foundation detection in social media discourse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze social discourse and develop ethically-aligned AI systems.

**Method:** Comparison of state-of-the-art LLMs and fine-tuned transformers using ROC, PR, and DET curve analysis across Twitter and Reddit datasets.

**Key Contributions:**

	1. First comprehensive comparison of LLMs and fine-tuned models for moral reasoning
	2. Identification of performance gaps in LLMs for moral content detection
	3. Demonstration of the superiority of fine-tuning over prompting in this domain.

**Result:** LLMs show high false negative rates and under-detection of moral content despite prompt engineering.

**Limitations:** Focuses primarily on Twitter and Reddit datasets, which may limit generalizability.

**Conclusion:** Task-specific fine-tuning is superior to prompting for moral reasoning applications in LLMs.

**Abstract:** Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear.   This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.   Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.

</details>


### [77] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)

*JoÃ£o Ruano, GonÃ§alo M. Correia, Leonor Barreiros, Afonso Mendes*

**Main category:** cs.CL

**Keywords:** Biomedical Named Entity Recognition, Named Entity Recognition, Multi-task learning

**Relevance Score:** 7

**TL;DR:** This paper introduces SRU-NER, a novel approach for Biomedical Named Entity Recognition that effectively handles nested entities and integrates multiple datasets using multi-task learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of biomedical terminology and inconsistencies in annotation across datasets pose significant challenges in Biomedical Named Entity Recognition.

**Method:** SRU-NER uses a Slot-based Recurrent Unit approach to manage nested named entities and employs a multi-task learning strategy to better integrate multiple datasets, adjusting loss computation dynamically to minimize the impact of absent entity types.

**Key Contributions:**

	1. Introduction of a novel Slot-based Recurrent Unit for nested entity recognition.
	2. Implementation of multi-task learning to enhance dataset integration.
	3. Dynamic loss computation to mitigate annotation gaps across datasets.

**Result:** SRU-NER demonstrates competitive performance in both biomedical and general-domain NER tasks, with improvements seen in cross-domain generalization through extensive experiments and human assessments.

**Limitations:** Potential limitations not explicitly stated in the abstract.

**Conclusion:** The proposed SRU-NER method effectively addresses challenges in biomedical NER, showing robustness and adaptability across different datasets and improving on existing methods.

**Abstract:** Biomedical Named Entity Recognition presents significant challenges due to the complexity of biomedical terminology and inconsistencies in annotation across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to handle nested named entities while integrating multiple datasets through an effective multi-task learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments, including a cross-corpus evaluation and human assessment of the model's predictions, SRU-NER achieves competitive performance in biomedical and general-domain NER tasks, while improving cross-domain generalization.

</details>


### [78] [Identity-related Speech Suppression in Generative AI Content Moderation](https://arxiv.org/abs/2409.13725)

*Grace Proebsting, Oghenefejiro Isaacs Anigboro, Charlie M. Crawford, DanaÃ© Metaxa, Sorelle A. Friedler*

**Main category:** cs.CL

**Keywords:** content moderation, generative AI, speech suppression, identity-related speech, automated systems

**Relevance Score:** 8

**TL;DR:** This paper investigates the suppression of identity-related speech by automated content moderation systems and the impact of generative AI on content creation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of automated content moderation incorrectly filtering content related to marginalized identities and to ensure that generative AI allows appropriate text generation.

**Method:** The paper introduces measures of speech suppression, creating a benchmark for measuring suppression across different identity groups using both user-generated datasets and generative AI-focused data.

**Key Contributions:**

	1. Introduction of measures of speech suppression for identity groups
	2. Creation of benchmark datasets for measuring speech suppression
	3. Analysis of identity-related bias in automated content moderation systems.

**Result:** The study finds that identity-related speech is more likely to be incorrectly suppressed across various content moderation services, with different reasons for suppression linked to identity stereotypes and text associations.

**Limitations:** 

**Conclusion:** As generative AI becomes more prevalent in creative contexts, it is crucial to consider the implications of speech suppression on the creation of identity-related content.

**Abstract:** Automated content moderation has long been used to help identify and filter undesired user-generated content online. But such systems have a history of incorrectly flagging content by and about marginalized identities for removal. Generative AI systems now use such filters to keep undesired generated content from being created by or shown to users. While a lot of focus has been given to making sure such systems do not produce undesired outcomes, considerably less attention has been paid to making sure appropriate text can be generated. From classrooms to Hollywood, as generative AI is increasingly used for creative or expressive text generation, whose stories will these technologies allow to be told, and whose will they suppress?   In this paper, we define and introduce measures of speech suppression, focusing on speech related to different identity groups incorrectly filtered by a range of content moderation APIs. Using both short-form, user-generated datasets traditional in content moderation and longer generative AI-focused data, including two datasets we introduce in this work, we create a benchmark for measurement of speech suppression for nine identity groups. Across one traditional and four generative AI-focused automated content moderation services tested, we find that identity-related speech is more likely to be incorrectly suppressed than other speech. We find that reasons for incorrect flagging behavior vary by identity based on stereotypes and text associations, with, e.g., disability-related content more likely to be flagged for self-harm or health-related reasons while non-Christian content is more likely to be flagged as violent or hateful. As generative AI systems are increasingly used for creative work, we urge further attention to how this may impact the creation of identity-related content.

</details>


### [79] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)

*Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis*

**Main category:** cs.CL

**Keywords:** Information Extraction, Natural Language Processing, Transformers

**Relevance Score:** 8

**TL;DR:** GLiNER2 is a unified framework for information extraction that offers efficient named entity recognition, text classification, and data extraction in a single model. It improves deployment efficiency compared to existing large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of requiring specialized models and high computational costs in existing information extraction solutions.

**Method:** GLiNER2 enhances the original GLiNER architecture to support multiple tasks within a single pretrained transformer model, utilizing a schema-based interface for multi-task composition.

**Key Contributions:**

	1. Unified framework supporting multiple NLP tasks
	2. Improved CPU efficiency and model compactness
	3. Open-source availability with pre-trained models

**Result:** GLiNER2 shows competitive performance in extraction and classification tasks while being more accessible for deployment than LLM-based approaches.

**Limitations:** 

**Conclusion:** GLiNER2 is released as an open-source library, providing an efficient alternative for information extraction tasks.

**Abstract:** Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.

</details>


### [80] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)

*Jiafeng Xiong, Yuting Zhao*

**Main category:** cs.CL

**Keywords:** Multimodal Machine Translation, Graph-guided Inductive Learning, Image-free Translation

**Relevance Score:** 6

**TL;DR:** The paper introduces GIIFT, an innovative two-stage framework for Multimodal Machine Translation (MMT) that leverages graphs for inductive image-free translation, demonstrating state-of-the-art results on various benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Multimodal Machine Translation (MMT) by addressing the challenges of modality gap and rigid visual-linguistic alignment while enabling inference beyond trained multimodal domains.

**Method:** GIIFT employs novel multimodal scene graphs and a cross-modal Graph Attention Network adapter to integrate modality-specific information and generalize to broader translation domains without relying on images.

**Key Contributions:**

	1. Introduction of GIIFT framework for inductive image-free MMT
	2. Development of multimodal scene graphs for better modality integration
	3. State-of-the-art results on translation tasks without images

**Result:** GIIFT surpasses existing MMT methods on the Multi30K dataset for English-to-French and English-to-German tasks and achieves state-of-the-art performance in image-free translation on the WMT benchmark.

**Limitations:** 

**Conclusion:** The GIIFT framework significantly enhances image-free inference in MMT, indicating that effective integration of modality-specific information can lead to better performance.

**Abstract:** Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.

</details>


### [81] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)

*Ganesh Sapkota, Md Hasibur Rahman*

**Main category:** cs.CL

**Keywords:** DNA Language Models, tokenization, Byte Pair Encoding, genomic analysis, hybrid approach

**Relevance Score:** 2

**TL;DR:** The paper introduces a hybrid tokenization strategy for DNA Language Models that combines 6-mer tokenization and Byte Pair Encoding, improving prediction accuracy for DNA sequence modeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of traditional k-mer tokenization in capturing local structures and understanding global context in DNA sequences.

**Method:** The authors propose a hybrid approach that merges unique 6-mer tokens with optimally selected BPE tokens from 600 BPE cycles to create a balanced, context-aware vocabulary.

**Key Contributions:**

	1. Introduced a novel hybrid tokenization strategy for DNA Language Models.
	2. Demonstrated improved performance on next-k-mer prediction tasks.
	3. Highlighted the importance of advanced tokenization for genomic modeling.

**Result:** The foundational DLM showed significant performance gains, achieving prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming existing models like NT, DNABERT2, and GROVER.

**Limitations:** 

**Conclusion:** This hybrid tokenization method is crucial for preserving local and global sequence information in DNA modeling, serving as a strong basis for future genomic applications.

**Abstract:** This paper presents a novel hybrid tokenization strategy that enhances the performance of DNA Language Models (DLMs) by combining 6-mer tokenization with Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence structures but often faces challenges, including uneven token distribution and a limited understanding of global sequence context. To address these limitations, we propose merging unique 6mer tokens with optimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a balanced and context-aware vocabulary, enabling the model to capture both short and long patterns within DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was evaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved performance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These results highlight the ability of the hybrid tokenization strategy to preserve both the local sequence structure and global contextual information in DNA modeling. This work underscores the importance of advanced tokenization methods in genomic language modeling and lays a robust foundation for future applications in downstream DNA sequence analysis and biological research.

</details>


### [82] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)

*Feng Hong, Geng Yu, Yushi Ye, Haicheng Huang, Huangjie Zheng, Ya Zhang, Yanfeng Wang, Jiangchao Yao*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Large Language Models, Decoding Algorithm, Quality-Speed Trade-off, Deep Learning

**Relevance Score:** 8

**TL;DR:** WINO is a new decoding algorithm for DLLMs that improves the quality-speed trade-off by allowing revokable decoding through a draft-and-verify mechanism.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the quality-speed trade-off in existing Diffusion Large Language Models during parallel decoding.

**Method:** WINO employs a draft-and-verify mechanism where multiple tokens are drafted concurrently while using bidirectional context to verify and mask suspicious tokens for refinement.

**Key Contributions:**

	1. Introduction of the WINO decoding algorithm for DLLMs
	2. Demonstrated improvements in speed and accuracy on benchmark tasks
	3. Validation on open-source DLLMs like LLaDA and MMaDA

**Result:** WINO shows a 6x acceleration in inference on the GSM8K benchmark with a 2.58% improvement in accuracy and a 10x speedup on the Flickr30K task with enhanced performance.

**Limitations:** 

**Conclusion:** The proposed WINO algorithm significantly enhances the performance and efficiency of DLLMs, offering a viable solution to the quality-speed trade-off.

**Abstract:** Diffusion Large Language Models (DLLMs) have emerged as a compelling alternative to Autoregressive models, designed for fast parallel generation. However, existing DLLMs are plagued by a severe quality-speed trade-off, where faster parallel decoding leads to significant performance degradation. We attribute this to the irreversibility of standard decoding in DLLMs, which is easily polarized into the wrong decoding direction along with early error context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO), a training-free decoding algorithm that enables revokable decoding in DLLMs. WINO employs a parallel draft-and-verify mechanism, aggressively drafting multiple tokens while simultaneously using the model's bidirectional context to verify and re-mask suspicious ones for refinement. Verified in open-source DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the quality-speed trade-off. For instance, on the GSM8K math benchmark, it accelerates inference by 6$\times$ while improving accuracy by 2.58%; on Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance. More comprehensive experiments are conducted to demonstrate the superiority and provide an in-depth understanding of WINO.

</details>


### [83] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)

*Jiahao Wang, Ramen Liu, Longhui Zhang, Jing Li*

**Main category:** cs.CL

**Keywords:** Hate Speech Recognition, Self-Retrieval-Augmented Generation, Multi-Round Voting, Chinese Language Processing, Natural Language Processing

**Relevance Score:** 3

**TL;DR:** The paper presents a system for Fine-Grained Chinese Hate Speech Recognition using a novel framework that improves task performance through various methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Fine-Grained Chinese Hate Speech Recognition (FGCHSR) tasks using innovative computational approaches.

**Method:** The authors proposed the SRAG-MAV framework, which integrates task reformulation, retrieval-augmented generation, and multi-round voting inference to improve stability and accuracy in hate speech recognition.

**Key Contributions:**

	1. Introduction of the SRAG-MAV framework for FGCHSR.
	2. Implementation of dynamic retrieval to create contextual prompts for improved accuracy.
	3. Demonstrated significant performance improvement over existing models.

**Result:** The system achieved superior performance metrics on the STATE ToxiCN dataset compared to leading models, with a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505.

**Limitations:** 

**Conclusion:** The novel framework significantly enhances hate speech recognition capabilities, marking a step forward in addressing hate speech in Chinese.

**Abstract:** This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [84] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)

*Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain-specific Performance, Data Synthesis

**Relevance Score:** 8

**TL;DR:** AQuilt is a framework for creating instruction-tuning data tailored to specialized domains from unlabeled data, enhancing LLM performance with reduced costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance of LLMs in specialized domains while minimizing computational costs and addressing generalization issues across tasks.

**Method:** AQuilt constructs instruction-tuning data by utilizing unlabeled data and incorporating logic and inspection elements to facilitate reasoning processes.

**Key Contributions:**

	1. Introduction of AQuilt framework for instruction-tuning data generation.
	2. Reduction of production costs to 17% while maintaining performance.
	3. High relevance of generated data to downstream specialized tasks.

**Result:** AQuilt has been shown to perform comparably to DeepSeek-V3, achieving similar results at only 17% of the cost, with generated data demonstrating increased relevance for downstream tasks.

**Limitations:** 

**Conclusion:** The framework allows for effective data generation for various tasks, sustaining high-quality outputs with fewer resources.

**Abstract:** Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [85] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)

*Andreea Nica, Ivan Zakazov, Nicolas Mario Baldwin, Saibo Geng, Robert West*

**Main category:** cs.CL

**Keywords:** Prompt Optimization, Large Language Models, Textual Feedback

**Relevance Score:** 8

**TL;DR:** The paper introduces TRPrompt, a framework for optimizing prompts for large language models using textual feedback instead of prior dataset collection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning abilities of large language models (LLMs) via prompt optimization without requiring parameter updates.

**Method:** The Textual Reward Prompt (TRPrompt) framework integrates textual feedback into the training of a prompt model, improving it iteratively based on feedback on generated prompts.

**Key Contributions:**

	1. Introduction of the TRPrompt framework
	2. Unified approach of textual feedback and reward-based learning
	3. State-of-the-art performance on math datasets using the proposed framework

**Result:** TRPrompt leads to state-of-the-art query-specific prompts for challenging math problems from datasets like GSMHard and MATH.

**Limitations:** 

**Conclusion:** The framework successfully combines heuristic-based and reward-based approaches to create effective prompts, demonstrating high performance on specified math datasets.

**Abstract:** Prompt optimization improves the reasoning abilities of large language models (LLMs) without requiring parameter updates to the target model. Following heuristic-based "Think step by step" approaches, the field has evolved in two main directions: while one group of methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-free way, a concurrent line of research relies on numerical rewards to train a special prompt model, tailored for providing optimal prompts to the target model. In this paper, we introduce the Textual Reward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual feedback into training of the prompt model. Our framework does not require prior dataset collection and is being iteratively improved with the feedback on the generated prompts. When coupled with the capacity of an LLM to internalize the notion of what a "good" prompt is, the high-resolution signal provided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-specific prompts for the problems from the challenging math datasets GSMHard and MATH.

</details>


### [86] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)

*Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, Tongshuang Wu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Checklist Feedback, Language Models, Instruction Following, AI Alignment

**Relevance Score:** 9

**TL;DR:** This paper introduces Reinforcement Learning from Checklist Feedback (RLCF) to enhance language model performance by using instruction-specific criteria for reward computation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of reinforcement learning in language models by employing adaptive, instruction-specific feedback instead of fixed criteria.

**Method:** RLCF extracts checklists from user instructions and evaluates language model responses against these checklists, generating reward scores via AI judges and verifier programs.

**Key Contributions:**

	1. Introduction of RLCF as a novel alignment strategy
	2. Demonstrated effectiveness of checklist-based feedback
	3. Improved performance across multiple benchmarks

**Result:** RLCF outperformed other alignment methods on five benchmarks, achieving significant performance improvements: a 4-point increase on FollowBench, a 6-point increase on InFoBench, and a 3-point rise on Arena-Hard.

**Limitations:** 

**Conclusion:** Checklist feedback is a potent tool for improving instruction-following capabilities in language models.

**Abstract:** Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs.

</details>


### [87] [Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias](https://arxiv.org/abs/2212.10678)

*Yuen Chen, Vethavikashini Chithrra Raghuram, Justus Mattern, Rada Mihalcea, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** bias measurement, generative language models, occupational gender bias, LLMs, bias mitigation

**Relevance Score:** 8

**TL;DR:** This paper presents a causal framework for measuring bias in generative language models, introducing a benchmark called OccuGender to assess occupational gender bias in various LLMs and discussing strategies for bias mitigation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and quantify harmful biases found in the outputs of large language models (LLMs) against different demographics, motivating the need for robust bias measurement and benchmarks.

**Method:** The paper proposes a causal formulation for bias measurement, outlines desiderata for bias benchmarks, and introduces the OccuGender benchmark to evaluate occupational gender bias in state-of-the-art LLMs.

**Key Contributions:**

	1. Introduction of a causal formulation for measuring bias in LLMs.
	2. Development of the OccuGender benchmark for assessing occupational gender bias.
	3. Empirical testing of state-of-the-art LLMs revealing substantial biases.

**Result:** Testing several open-source LLMs like Llama and Mistral on OccuGender revealed significant levels of occupational gender bias within these models.

**Limitations:** 

**Conclusion:** The findings underscore the need for bias mitigation strategies and highlight the generalizability of the proposed causal framework for future bias assessments.

**Abstract:** Generated texts from large language models (LLMs) have been shown to exhibit a variety of harmful, human-like biases against various demographics. These findings motivate research efforts aiming to understand and measure such effects. This paper introduces a causal formulation for bias measurement in generative language models. Based on this theoretical foundation, we outline a list of desiderata for designing robust bias benchmarks. We then propose a benchmark called OccuGender, with a bias-measuring procedure to investigate occupational gender bias. We test several state-of-the-art open-source LLMs on OccuGender, including Llama, Mistral, and their instruction-tuned versions. The results show that these models exhibit substantial occupational gender bias. Lastly, we discuss prompting strategies for bias mitigation and an extension of our causal formulation to illustrate the generalizability of our framework. Our code and data https://github.com/chenyuen0103/gender-bias.

</details>


### [88] [DocTER: Evaluating Document-based Knowledge Editing](https://arxiv.org/abs/2308.09954)

*Suhang Wu, Ante Wang, Minlong Peng, Yujie Lin, Wenbo Li, Mingming Sun, Jinsong Su*

**Main category:** cs.CL

**Keywords:** Knowledge editing, Neural networks, DocTER benchmark, Extract-then-Edit pipeline, Cross-lingual knowledge

**Relevance Score:** 6

**TL;DR:** This paper introduces a novel approach to knowledge editing in neural networks using documents instead of labeled triples, establishing a benchmark and evaluating editing challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge editing techniques by using easily accessible documents for correcting outdated knowledge in neural networks, moving away from reliance on manually labeled factual triples.

**Method:** The authors propose the Extract-then-Edit pipeline, which extracts factual triples from documents before applying conventional knowledge editing techniques. They establish the first evaluation benchmark, DocTER, focusing on document-based editing.

**Key Contributions:**

	1. Introduction of the DocTER benchmark for document-based knowledge editing
	2. Development of the Extract-then-Edit pipeline for knowledge editing
	3. Insights into factors affecting editing performance such as quality of extracted triples and document structure.

**Result:** Experiments indicate that editing with documents presents greater challenges than using traditional triples, with a 10-point gap in success rates for even the top editing methods in document scenarios compared to gold triples.

**Limitations:** The study primarily focuses on document-based editing and may not directly address all aspects of knowledge editing with neural networks, especially those involving labeled triples.

**Conclusion:** The findings reveal significant challenges in document-based knowledge editing, highlighting the need for enhanced methods and analysis of factors affecting performance, paving the way for future research.

**Abstract:** Knowledge editing aims to correct outdated or inaccurate knowledge in neural networks. In this paper, we explore knowledge editing using easily accessible documents instead of manually labeled factual triples employed in earlier research. To advance this field, we establish the first evaluation benchmark, \textit{DocTER}, featuring Documents containing counterfactual knowledge for editing. A comprehensive four-perspective evaluation is introduced: Edit Success, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional triplet-based knowledge editing methods for this task, we develop an Extract-then-Edit pipeline that extracts triples from documents before applying existing methods. Experiments on popular knowledge editing methods demonstrate that editing with documents presents significantly greater challenges than using triples. In document-based scenarios, even the best-performing in-context editing approach still lags behind by 10 points in editing success when compared to using gold triples. This observation also holds for both reasoning and cross-lingual test sets. We further analyze key factors influencing task performance, including the quality of extracted triples, the frequency and position of edited knowledge in documents, various methods for enhancing reasoning, and performance differences across various directions in cross-lingual knowledge editing, which provide valuable insights for future research.

</details>


### [89] [Quantifying the Uniqueness and Divisiveness of Presidential Discourse](https://arxiv.org/abs/2401.01405)

*Karen Zhou, Alexander A. Meitus, Milo Chase, Grace Wang, Anne Mykland, William Howell, Chenhao Tan*

**Main category:** cs.CL

**Keywords:** presidential speech, language models, divisive language, political communication, text analysis

**Relevance Score:** 4

**TL;DR:** This paper analyzes the distinct speech patterns of American presidents, particularly focusing on Donald Trump's unique communication style in comparison to past nominees.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if American presidents have identifiable differences in their speech and whether these differences vary by communication medium.

**Method:** A novel metric of uniqueness based on large language models was introduced, alongside a new lexicon for divisive speech. This framework was applied to various presidential speech corpora.

**Key Contributions:**

	1. Introduction of a uniqueness metric based on large language models
	2. Development of a lexicon for divisive speech
	3. Framework for assessing presidential speech differences

**Result:** Significant evidence that Donald Trump's speech patterns diverge considerably from those of other major party nominees, with a stronger use of distinctive and divisive language.

**Limitations:** 

**Conclusion:** Trump's unique speech patterns are present across different types of communication and are not merely a result of broader changes in presidential communication styles.

**Abstract:** Do American presidents speak discernibly different from each other? If so, in what ways? And are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for assessing the distinctive ways in which presidents speak about their political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Donald Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values appear closer to those of the Democrats. Contributing to these differences is Trump's employment of divisive and antagonistic language, particularly when targeting his political opponents. These differences hold across a variety of measurement strategies, arise on both the campaign trail and in official presidential addresses, and do not appear to be an artifact of secular changes in presidential communications.

</details>


### [90] [VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks](https://arxiv.org/abs/2407.19795)

*Juhwan Choi, Junehyoung Kwon, JungMin Yun, Seunguk Yu, YoungBin Kim*

**Main category:** cs.CL

**Keywords:** domain generalization, vision-language tasks, dataset, LLM, deep learning

**Relevance Score:** 7

**TL;DR:** Introduction of a new dataset VolDoGer for domain generalization in vision-language tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve domain generalizability of deep learning models in vision-language tasks due to a lack of suitable datasets.

**Method:** Proposed VolDoGer dataset is created for image captioning, visual question answering, and visual entailment, using LLM-based data annotation techniques.

**Key Contributions:**

	1. Introduction of the VolDoGer dataset for vision-language tasks.
	2. Utilization of LLM-based annotation methods to reduce reliance on human annotators.
	3. Evaluation of modelsâ€™ domain generalizability using this new dataset.

**Result:** Evaluation of various models, including fine-tuned models and a multimodal large language model, confirms the dataset's effectiveness for assessing domain generalizability.

**Limitations:** The dataset's effectiveness may be limited by the quality of LLM-based annotations and scope of tasks covered.

**Conclusion:** VolDoGer serves as a valuable resource for enhancing the robustness of vision-language models across different domains.

**Abstract:** Domain generalizability is a crucial aspect of a deep learning model since it determines the capability of the model to perform well on data from unseen domains. However, research on the domain generalizability of deep learning models for vision-language tasks remains limited, primarily because of the lack of required datasets. To address these challenges, we propose VolDoGer: Vision-Language Dataset for Domain Generalization, a dedicated dataset designed for domain generalization that addresses three vision-language tasks: image captioning, visual question answering, and visual entailment. We constructed VolDoGer by extending LLM-based data annotation techniques to vision-language tasks, thereby alleviating the burden of recruiting human annotators. We evaluated the domain generalizability of various models, ranging from fine-tuned models to a recent multimodal large language model, through VolDoGer.

</details>


### [91] [Identity-related Speech Suppression in Generative AI Content Moderation](https://arxiv.org/abs/2409.13725)

*Grace Proebsting, Oghenefejiro Isaacs Anigboro, Charlie M. Crawford, DanaÃ© Metaxa, Sorelle A. Friedler*

**Main category:** cs.CL

**Keywords:** Content Moderation, Speech Suppression, Generative AI, Identity Groups, Bias

**Relevance Score:** 8

**TL;DR:** The paper investigates the suppression of identity-related speech by automated content moderation systems, highlighting biases in filtering practices across generative AI and traditional moderation APIs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unjust suppression of speech by automated content moderation systems, particularly related to marginalized identities, and examine its implications on generative AI.

**Method:** The authors define measures of speech suppression and create a benchmark for evaluating how different identity groups are affected by content moderation, using both user-generated and generative AI datasets.

**Key Contributions:**

	1. Introduction of measures of speech suppression for identity groups.
	2. Creation of a benchmark for evaluating speech suppression across different moderation services.
	3. Analysis of suppression reasons linked to stereotypes associated with various identities.

**Result:** The study finds that identity-related speech is more frequently suppressed than other types of speech, with differing reasons for suppression based on identities and stereotypes.

**Limitations:** 

**Conclusion:** The paper calls for increased awareness of the impact of content moderation on the expression of identity-related narratives as generative AI becomes more prevalent in creative domains.

**Abstract:** Automated content moderation has long been used to help identify and filter undesired user-generated content online. But such systems have a history of incorrectly flagging content by and about marginalized identities for removal. Generative AI systems now use such filters to keep undesired generated content from being created by or shown to users. While a lot of focus has been given to making sure such systems do not produce undesired outcomes, considerably less attention has been paid to making sure appropriate text can be generated. From classrooms to Hollywood, as generative AI is increasingly used for creative or expressive text generation, whose stories will these technologies allow to be told, and whose will they suppress?   In this paper, we define and introduce measures of speech suppression, focusing on speech related to different identity groups incorrectly filtered by a range of content moderation APIs. Using both short-form, user-generated datasets traditional in content moderation and longer generative AI-focused data, including two datasets we introduce in this work, we create a benchmark for measurement of speech suppression for nine identity groups. Across one traditional and four generative AI-focused automated content moderation services tested, we find that identity-related speech is more likely to be incorrectly suppressed than other speech. We find that reasons for incorrect flagging behavior vary by identity based on stereotypes and text associations, with, e.g., disability-related content more likely to be flagged for self-harm or health-related reasons while non-Christian content is more likely to be flagged as violent or hateful. As generative AI systems are increasingly used for creative work, we urge further attention to how this may impact the creation of identity-related content.

</details>


### [92] [LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios](https://arxiv.org/abs/2411.07037)

*Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, instruction-following, natural language processing, long-context, LIFEval

**Relevance Score:** 9

**TL;DR:** This paper introduces LIFBench, a dataset for evaluating Large Language Models (LLMs) on instruction-following in long-context scenarios, along with LIFEval, a rubric-based assessment method.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of focus on LLMs' instruction-following capabilities and stability in long-context inputs in existing benchmarks.

**Method:** LIFBench comprises three long-context scenarios with 2,766 generated instructions, evaluated using LIFEval, a rubric-based scoring method.

**Key Contributions:**

	1. Introduction of LIFBench dataset for long-context instruction-following evaluation.
	2. Development of LIFEval for precise automated scoring of LLM responses.
	3. Comprehensive analysis across 20 LLMs on stability and performance metrics.

**Result:** Evaluation conducted on 20 LLMs across six length intervals using the proposed methods revealing insights into instruction-following performance and stability.

**Limitations:** 

**Conclusion:** LIFBench and LIFEval provide robust tools for evaluating LLMs, facilitating future research in LLM development and understanding performance in complex scenarios.

**Abstract:** As Large Language Models (LLMs) evolve in natural language processing (NLP), their ability to stably follow instructions in long-context inputs has become critical for real-world applications. However, existing benchmarks seldom focus on instruction-following in long-context scenarios or stability on different inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed to evaluate LLMs' instruction-following capabilities and stability across long contexts. LIFBench comprises three long-context scenarios and eleven diverse tasks, featuring 2,766 instructions generated through an automated expansion method across three dimensions: length, expression, and variables. For evaluation, we propose LIFEval, a rubric-based assessment method that enables precise, automated scoring of complex LLM responses without reliance on LLM-assisted assessments or human judgment. This method allows for a comprehensive analysis of model performance and stability from multiple perspectives. We conduct detailed experiments on 20 prominent LLMs across six length intervals. Our work contributes LIFBench and LIFEval as robust tools for assessing LLM performance in complex and long-context settings, offering valuable insights to guide future advancements in LLM development.

</details>


### [93] [ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models](https://arxiv.org/abs/2502.15487)

*Martina Miliani, Serena Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci*

**Main category:** cs.CL

**Keywords:** Large Language Models, Causal Reasoning, Temporal Relations, Dataset, Evaluation

**Relevance Score:** 8

**TL;DR:** This paper presents ExpliCa, a new dataset for assessing large language models (LLMs) on explicit causal reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better evaluation methods for LLMs in tasks involving interpretive and inferential reasoning, particularly in causal reasoning scenarios.

**Method:** The authors introduced ExpliCa, which integrates causal and temporal relations expressed through linguistic connectives, and evaluated seven LLMs using prompting and perplexity-based metrics.

**Key Contributions:**

	1. Introduction of the ExpliCa dataset for explicit causal reasoning evaluation.
	2. Demonstration of LLMs' struggles with causal vs. temporal relations.
	3. Insights into the influence of linguistic order on model performance.

**Result:** The evaluation revealed that even leading LLMs did not achieve 0.80 accuracy, often confusing causal and temporal relations, with performance varying based on linguistic ordering.

**Limitations:** 

**Conclusion:** The study highlights significant challenges faced by LLMs in causal reasoning, underscoring the importance of the dataset for future research and benchmarking.

**Abstract:** Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.

</details>


### [94] [How do language models learn facts? Dynamics, curricula and hallucinations](https://arxiv.org/abs/2503.21676)

*Nicolas Zucchet, JÃ¶rg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De*

**Main category:** cs.CL

**Keywords:** language models, learning dynamics, knowledge acquisition, data distribution, fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper examines the learning dynamics of language models during pre-training, revealing three key phases of knowledge acquisition and their implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the dynamics of knowledge acquisition in large language models to improve training effectiveness and knowledge recall.

**Method:** Investigation of learning dynamics on a synthetic factual recall task, analyzing the impact of training data distribution and model behavior during training.

**Key Contributions:**

	1. Identification of three phases in the learning dynamics of language models
	2. Demonstration of the impact of data distribution on learning
	3. Insights into the challenges of knowledge integration through fine-tuning

**Result:** Language models learn in three phases, experience performance plateaus, are affected by data distribution, and face challenges integrating new knowledge without corrupting existing memories.

**Limitations:** The findings are based on synthetic tasks and may not fully represent real-world applications of language models.

**Conclusion:** Data distribution plays a crucial role in the learning dynamics of language models, suggesting new strategies for effective data scheduling in training.

**Abstract:** Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.

</details>


### [95] [Exploiting individual differences to bootstrap communication](https://arxiv.org/abs/2504.05211)

*Richard A. Blythe, Casimir Fisch*

**Main category:** cs.CL

**Keywords:** communication systems, social cognition, modeling

**Relevance Score:** 4

**TL;DR:** The paper presents a model demonstrating how a communication system can emerge from non-communicative behaviors in a population, driven by predictable behavior and shared psychological states, without prior communication mechanisms.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of explaining how communication systems can form without existing signals or meanings, bridging the gap between non-communicative behavior and structured communication.

**Method:** The authors develop a theoretical model that illustrates the emergence of a communication system through individual behavioral differences, focusing on predictable actions and shared intentionality among individuals.

**Key Contributions:**

	1. Development of a model to explain the emergence of communication systems from non-communicative behaviors
	2. Identification of key cognitive capabilities (predictable behavior, shared psychological states) that facilitate communication formation
	3. Reinforcement of theories relating to social cognition and the evolution of language

**Result:** The model indicates that a communication system capable of expressing unlimited meanings can arise from non-communicative interactions, grounded in general capacities for social cognition.

**Limitations:** 

**Conclusion:** The findings support the idea that complex communication systems may evolve from simpler social interactions and cognitive skills, rather than from established communicative frameworks.

**Abstract:** Establishing a communication system is hard because the intended meaning of a signal is unknown to its receiver when first produced, and the signaller also has no idea how that signal will be interpreted. Most theoretical accounts of the emergence of communication systems rely on feedback to reinforce behaviours that have led to successful communication in the past. However, providing such feedback requires already being able to communicate the meaning that was intended or interpreted. Therefore these accounts cannot explain how communication can be bootstrapped from non-communicative behaviours. Here we present a model that shows how a communication system, capable of expressing an unbounded number of meanings, can emerge as a result of individual behavioural differences in a large population without any pre-existing means to determine communicative success. The two key cognitive capabilities responsible for this outcome are behaving predictably in a given situation, and an alignment of psychological states ahead of signal production that derives from shared intentionality. Since both capabilities can exist independently of communication, our results are compatible with theories in which large flexible socially-learned communication systems like language are the product of a general but well-developed capacity for social cognition.

</details>


### [96] [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)

*Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu*

**Main category:** cs.CL

**Keywords:** multi-modal, audio understanding, speech conversation, reinforcement learning, retrieval-augmented generation

**Relevance Score:** 8

**TL;DR:** Step-Audio 2 is a multi-modal large language model for audio understanding and speech conversation, utilizing latent audio encoding and reinforcement learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automatic speech recognition and audio understanding while enabling responsive speech conversations that incorporate emotions and speaking styles.

**Method:** Integrates a latent audio encoder, reasoning-centric reinforcement learning, and a RAG approach while calling external tools for dynamic conversation.

**Key Contributions:**

	1. Development of a multi-modal large language model for audio understanding.
	2. Incorporation of reinforcement learning for reasoning-centric audio analysis.
	3. Enhanced responsiveness to emotional and stylistic variances in speech.

**Result:** Achieves state-of-the-art performance on various benchmarks for audio understanding and conversational AI, enhancing responsiveness and expressiveness.

**Limitations:** 

**Conclusion:** Step-Audio 2's innovative integration of audio token generation and external tools significantly improves conversational capabilities.

**Abstract:** This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.

</details>


### [97] [LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs](https://arxiv.org/abs/2507.16809)

*Da-Chen Lian, Ri-Sheng Huang, Pin-Er Chen, Chunki Lim, You-Kuan Lin, Guan-Yu Tseng, Zi-Cheng Yang, Zhen-Yu Lin, Pin-Cheng Chen, Shu-Kai Hsieh*

**Main category:** cs.CL

**Keywords:** Linguistic Benchmark, Large Language Models, Reasoning Framework, Cross-Cultural Evaluation, Cognitive Plausibility

**Relevance Score:** 8

**TL;DR:** LingBench++ is a benchmark and reasoning framework to evaluate large language models on complex linguistic tasks, emphasizing structured reasoning and cross-cultural insights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of LLMs beyond accuracy, focusing on reasoning and understanding across low-resource languages.

**Method:** Development of a multi-agent architecture that integrates grammatical knowledge retrieval, tool-augmented reasoning, and hypothesis testing.

**Key Contributions:**

	1. Introduces structured reasoning traces for LLM evaluation.
	2. Expands linguistic task coverage across 90+ languages.
	3. Demonstrates the effectiveness of integrating external knowledge in LLMs.

**Result:** Multi-agent models with external knowledge and iterative reasoning showed improved accuracy and interpretability over conventional single-pass methods.

**Limitations:** Statistical tests on the experiments are needed to support strong claims; experiments were only run once.

**Conclusion:** LingBench++ provides a new framework for advancing LLM reasoning with a strong linguistic and cultural basis.

**Abstract:** We propose LingBench++, a linguistically-informed benchmark and reasoning framework designed to evaluate large language models (LLMs) on complex linguistic tasks inspired by the International Linguistics Olympiad (IOL). Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++ provides structured reasoning traces, stepwise evaluation protocols, and rich typological metadata across over 90 low-resource and cross-cultural languages. We further develop a multi-agent architecture integrating grammatical knowledge retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through systematic comparisons of baseline and our proposed agentic models, we demonstrate that models equipped with external knowledge sources and iterative reasoning outperform single-pass approaches in both accuracy and interpretability. LingBench++ offers a comprehensive foundation for advancing linguistically grounded, culturally informed, and cognitively plausible reasoning in LLMs.

</details>
