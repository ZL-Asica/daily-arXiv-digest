# 2025-09-03

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 31]

- [cs.CL](#cs.CL) [Total: 181]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Unlocking Mixed Reality for Medical Education: A See-Through Perspective on Head Anatomy](https://arxiv.org/abs/2509.00352)

*Yuqing Wei, Yupeng Wang, Jiayi Zhao, Yanjun Liu, Huxin Gao, Jiewen Lai*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Anatomy Education, Interactive Learning, Medical Education, 3D Models

**Relevance Score:** 6

**TL;DR:** This paper presents a mixed reality application for head anatomy education that enhances interactivity and immersion for medical learners.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional medical education methods often lack interactivity and struggle to convey complex spatial relationships, necessitating innovative solutions like mixed reality.

**Method:** Development of a mixed reality application that allows users to interact with 3D anatomical structures using hand gestures and controllers, supported by a hierarchical information design and automatic calibration module.

**Key Contributions:**

	1. Introduction of a mixed reality application for anatomy education.
	2. Implementation of intuitive interactions using hand gestures and controllers.
	3. Development of an automatic calibration module for real human interactions.

**Result:** Experiments indicate that the mixed reality system effectively matches virtual anatomical models with real-time scenes, significantly improving the interactive learning experience.

**Limitations:** 

**Conclusion:** The proposed mixed reality tool serves as an innovative method for teaching head anatomy, enhancing the learning experience in medical education.

**Abstract:** Extended reality (XR), encompassing Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), is emerging as a transformative platform for medical education. Traditional methods such as textbooks, physical models, and cadaveric dissections often lack interactivity and fail to convey complex spatial relationships effectively. The emerging MR technology addresses these limitations by providing immersive environments that blend virtual elements with real-world contexts. This study presents an MR application for head anatomy education, enabling learners to intuitively interact with see-through 3D anatomical structures via hand gestures and controllers. Our hierarchical information design supports progressive learning, guiding users from basic anatomical labels to detailed structural insights. Additionally, the system incorporates an automatic calibration module that aligns virtual anatomical models with a real human head, thereby facilitating realistic human-model interactions. Experiments show that the system can effectively match the anatomical model with real-time scenes, thus enhancing the interactivity and immersion of medical education, providing an innovative tool for teaching anatomy.

</details>


### [2] [Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners](https://arxiv.org/abs/2509.00440)

*Ibrahim Al-Hazwani, Ke Er Zhang, Laura Garrison, Jürgen Bernard*

**Main category:** cs.HC

**Keywords:** Data Humanism, Human-Centered Design, Visualization

**Relevance Score:** 5

**TL;DR:** This paper characterizes Data Humanism principles for visualization researchers, bridging the gap between design practice and research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide concrete definitions of Data Humanism principles and create a common language for human-centered visualization.

**Method:** A mixed-methods approach involving a systematic literature review, multimedia analysis, and expert interviews.

**Key Contributions:**

	1. Characterization of Data Humanism principles for visualization researchers.
	2. Concrete definitions to operationalize design choices.
	3. Mapping principles to specific visual design choices.

**Result:** The work offers a characterization of Data Humanism principles and maps these principles to specific visual design choices in a visualization work.

**Limitations:** 

**Conclusion:** The characterization promotes systematic application of Data Humanism in visualization research, facilitating better communication between practice and research.

**Abstract:** Data Humanism is a human-centered design approach that emphasizes the personal, contextual, and imperfect nature of data. Despite its growing influence among practitioners, the 13 principles outlined in Giorgia Lupi's visual manifesto remain loosely defined in research contexts, creating a gap between design practice and systematic application. Through a mixed-methods approach, including a systematic literature review, multimedia analysis, and expert interviews, we present a characterization of Data Humanism principles for visualization researchers. Our characterization provides concrete definitions that maintain interpretive flexibility in operationalizing design choices. We validate our work through direct consultation with Lupi. Moreover, we leverage the characterization to decode a visualization work, mapping Data Humanism principles to specific visual design choices. Our work creates a common language for human-centered visualization, bridging the gap between practice and research for future applications and evaluations.

</details>


### [3] [How to Make Museums More Interactive? Case Study of Artistic Chatbot](https://arxiv.org/abs/2509.00572)

*Filip J. Kucia, Bartosz Grabek, Szymon D. Trochimiak, Anna Wróblewska*

**Main category:** cs.HC

**Keywords:** Conversational agents, Large Language Models, Cultural heritage, Visitor engagement, Chatbot, Informal learning

**Relevance Score:** 9

**TL;DR:** Artistic Chatbot is a voice-responsive RAG-powered system designed to enhance visitor engagement in cultural heritage sites during art exhibitions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential for conversational agents powered by LLMs in physical learning environments like museums and cultural sites, where their use has been largely unexplored.

**Method:** Developed a QA chatbot that responds to spoken questions in Polish, utilizing a domain-specific knowledge base of 226 documents relevant to the exhibition.

**Key Contributions:**

	1. Introduction of a RAG-powered chatbot in a live exhibition context.
	2. Documented insights into user interaction and system architecture.
	3. Identification of practical challenges for chatbot deployment in cultural heritage sites.

**Result:** The chatbot maintained a 60% relevance in responses to unpredictable queries, demonstrating its effectiveness in enhancing interactivity at public cultural sites.

**Limitations:** The chatbot primarily focuses on a specific exhibition context, which may limit its generalizability to other domains.

**Conclusion:** Artistic Chatbot shows promise for improving visitor engagement and learning at cultural exhibitions, despite challenges in deploying such technologies in public spaces.

**Abstract:** Conversational agents powered by Large Language Models (LLMs) are increasingly utilized in educational settings, in particular in individual closed digital environments, yet their potential adoption in the physical learning environments like cultural heritage sites, museums, and art galleries remains relatively unexplored. In this study, we present Artistic Chatbot, a voice-to-voice RAG-powered chat system to support informal learning and enhance visitor engagement during a live art exhibition celebrating the 15th anniversary of the Faculty of Media Art at the Warsaw Academy of Fine Arts, Poland. The question answering (QA) chatbot responded to free-form spoken questions in Polish using the context retrieved from a curated, domain-specific knowledge base consisting of 226 documents provided by the organizers, including faculty information, art magazines, books, and journals. We describe the key aspects of the system architecture and user interaction design, as well as discuss the practical challenges associated with deploying chatbots at public cultural sites. Our findings, based on interaction analysis, demonstrate that chatbots such as Artistic Chatbot effectively maintain responses grounded in exhibition content (60\% of responses directly relevant), even when faced with unpredictable queries outside the target domain, showing their potential for increasing interactivity in public cultural sites.   GitHub project page: https://github.com/cinekucia/artistic-chatbot-cikm2025

</details>


### [4] [Queuing for Civility: Regulating Emotions and Reducing Toxicity in Digital Discourse](https://arxiv.org/abs/2509.00696)

*Akriti Verma, Shama Islam, Valeh Moghaddam, Adnan Anwar*

**Main category:** cs.HC

**Keywords:** online toxicity, emotion regulation, real-time moderation, HCI, social media

**Relevance Score:** 8

**TL;DR:** This paper introduces a graph-based framework for real-time emotion regulation in online conversations to mitigate toxicity and improve user well-being.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the issue of online toxicity, which disrupts digital interactions and well-being, by focusing on real-time emotional dynamics and their influence in conversations.

**Method:** A graph-based framework is developed to identify the need for emotion regulation, accompanied by a comment queuing mechanism introducing delays in comment publishing to allow users to self-reflect and manage emotional responses.

**Key Contributions:**

	1. A novel graph-based framework for identifying emotion regulation needs in conversations.
	2. A comment queuing mechanism that delays the publication of comments to promote self-regulation.
	3. Empirical evidence showing significant reduction in online toxicity and anger spread.

**Result:** Analysis of social media data showed a 12% reduction in toxicity and a 15% decrease in the spread of anger due to the proposed methods, with an average of 4% of comments held for review.

**Limitations:** 

**Conclusion:** Implementing real-time emotion regulation combined with delayed moderation can significantly enhance emotional balance and well-being in online interactions.

**Abstract:** The pervasiveness of online toxicity, including hate speech and trolling, disrupts digital interactions and online well-being. Previous research has mainly focused on post-hoc moderation, overlooking the real-time emotional dynamics of online conversations and the impact of users' emotions on others. This paper presents a graph-based framework to identify the need for emotion regulation within online conversations. This framework promotes self-reflection to manage emotional responses and encourage responsible behaviour in real time. Additionally, a comment queuing mechanism is proposed to address intentional trolls who exploit emotions to inflame conversations. This mechanism introduces a delay in publishing comments, giving users time to self-regulate before further engaging in the conversation and helping maintain emotional balance. Analysis of social media data from Twitter and Reddit demonstrates that the graph-based framework reduced toxicity by 12%, while the comment queuing mechanism decreased the spread of anger by 15%, with only 4% of comments being temporarily held on average. These findings indicate that combining real-time emotion regulation with delayed moderation can significantly improve well-being in online environments.

</details>


### [5] [Why it is worth making an effort with GenAI](https://arxiv.org/abs/2509.00852)

*Yvonne Rogers*

**Main category:** cs.HC

**Keywords:** Generative AI, education, metacognition, AI tools, learning effort

**Relevance Score:** 6

**TL;DR:** The paper discusses the challenge of over-reliance on Generative AI (GenAI) tools for homework among students and proposes ways to encourage deeper learning by requiring more effort in their use.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** There is concern that the use of GenAI tools like ChatGPT may hinder the development of students' writing and critical thinking skills due to ease of use and high-quality output.

**Method:** The paper outlines potential methods for requiring additional effort from students when using GenAI, such as integrating traditional learning techniques and designing tools that promote metacognition.

**Key Contributions:**

	1. Proposes new AI tool designs that require increased effort to use
	2. Suggests combining GenAI with traditional learning methods to facilitate deeper learning
	3. Explores the concept of metacognition in the context of using GenAI for education

**Result:** The proposed approach aims to balance ease of use with the need for effort, potentially leading to improved learning outcomes for students as they engage with hybrid learning methods.

**Limitations:** 

**Conclusion:** By designing AI tools that require more engagement and effort, students may develop better writing and critical thinking skills, thereby enhancing their learning experience.

**Abstract:** Students routinely use ChatGPT and the like now to help them with their homework, such as writing an essay. It takes less effort to complete and is easier to do than by hand. It can even produce as good if not better output than the student's own work. However, there is a growing concern that over-reliance on using GenAI in this way will stifle the development of learning writing and critical thinking skills. How might this trend be reversed? What if students were required to make more effort when using GenAI to do their homework? It might be more challenging, but the additional effort involved could result in them learning more and having a greater sense of achievement. This tension can be viewed as a form of effort paradox; where effort is both viewed as something to be avoided but at the same time is valued. Is it possible to let students learn sometimes with less and other times more effort? Students are already adept at the former but what about the latter? Could we design new kinds of AI tools that deliberately require more effort to use to deepen the learning experience? In this paper, I begin to outline what form these might take, for example, asking students to use a combination of GenAI tools with traditional learning approaches (e.g. note-taking while reading). I also discuss how else to design tools to think with that augments human cognition; where students learn more the skills of metacognition and reflection.

</details>


### [6] [Through the Expert's Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR](https://arxiv.org/abs/2509.00944)

*Clara Sayffaerth, Annika Köhler, Julian Rasch, Albrecht Schmidt, Florian Müller*

**Main category:** cs.HC

**Keywords:** eXtended Reality, Asynchronous Learning, Manual Skills, Knowledge Transfer, AI in Education

**Relevance Score:** 7

**TL;DR:** This paper explores the effectiveness of asynchronous learning using XR and AI for teaching complex manual tasks by analyzing first- and third-person perspectives and gaze visualizations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of transferring complex manual skills across generations necessitates innovative educational technologies beyond traditional methods like videos, particularly those incorporating XR and AI.

**Method:** The study employed an empirical approach with 36 participants to compare the effectiveness of first-person and third-person perspectives alongside gaze visualizations in teaching manual tasks.

**Key Contributions:**

	1. Investigates the role of first- and third-person perspectives in learning manual tasks through XR
	2. Demonstrates superior user preference and efficiency of first-person perspective in XR learning
	3. Provides guidelines for designing future XR educational systems.

**Result:** The findings reveal that the first-person perspective significantly enhances performance in quantitative measures and is preferred by users, demonstrating its advantages for learning in XR environments.

**Limitations:** 

**Conclusion:** The paper concludes with best practices for knowledge presentation and guidelines for the design of future learning systems utilizing asynchronous XR technologies.

**Abstract:** Transferring knowledge across generations is fundamental to human civilization, yet the challenge of passing on complex practical skills persists. Methods without a physically present instructor, such as videos, often fail to explain complex manual tasks, where spatial and social factors are critical. Technologies such as eXtended Reality and Artificial Intelligence hold the potential to retain expert knowledge and facilitate the creation of tailored, contextualized, and asynchronous explanations regardless of time and place. In contrast to videos, the learner's perspective can be different from the recorded perspective in XR. This paper investigates the impact of asynchronous first- and third-person perspectives and gaze visualizations on efficiency, feeling of embodiment, and connectedness during manual tasks. The empirical results of our study (N=36) show that the first-person perspective is better in quantitative measures and preferred by users. We identify best practices for presenting preserved knowledge and provide guidelines for designing future systems.

</details>


### [7] [The State of the Art in Visualization Literacy](https://arxiv.org/abs/2509.01018)

*Matthew Varona, Karen Bonilla, Maryam Hedayati, Alark Joshi, Lane Harrison, Matthew Kay, Carolina Nobre*

**Main category:** cs.HC

**Keywords:** visualization literacy, taxonomy, competency, literature review, education

**Relevance Score:** 5

**TL;DR:** This report surveys literature on visualization literacy, proposing a taxonomy and framework for enhancing skills related to engaging with visualizations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview and operational framework for visualization literacy, addressing the ambiguity of its definition.

**Method:** A survey of current literature organized into a taxonomy of visualization literacy based on competencies and contexts.

**Key Contributions:**

	1. Proposed taxonomy of visualization literacy
	2. Operational framework based on contexts and competencies
	3. Identification of key trends and open challenges in the field.

**Result:** Identification of key trends, competencies, open challenges, and interrelations within five categories: ontology, assessment, mechanisms, populiteracy, and intervention.

**Limitations:** 

**Conclusion:** Advancements within the identified categories inform each other, driving progress in the field of visualization literacy.

**Abstract:** Research in visualization literacy explores the skills required to engage with visualizations. This state-of-the-art report surveys the current literature in visualization literacy to provide a comprehensive overview of the field. We propose a taxonomy of visualization literacy that organizes the field into competency themes and research categories. To address ambiguity surrounding the term ``visualization literacy'', we provide a framework for operationalizing visualization literacy based on application contexts (including domain, scenario, and audience) and relevant competencies, which are categorized under consumption, construction, critique, and connection. Research contributions are organized into five categories: ontology, assessment, mechanisms, populiteracy, and intervention. For each category, we identify key trends, discuss which competencies are addressed, highlight open challenges, and examine how advancements within these areas inform and reinforce each other, driving progress in the field.

</details>


### [8] [Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces](https://arxiv.org/abs/2509.01051)

*Matte Lim, Catherine Yeh, Martin Wattenberg, Fernanda Viégas, Panagiotis Michalatos*

**Main category:** cs.HC

**Keywords:** visualization, dimensionality reduction, temporal data

**Relevance Score:** 6

**TL;DR:** A new visualization technique, Chronotome, combines force-based projection and streaming clustering to explore evolving themes in time-based data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing dimensionality reduction methods struggle to capture semantic changes in real-world datasets over time.

**Method:** The paper introduces a visualization technique that integrates force-based projection with streaming clustering methods to create spatial-temporal maps of embeddings.

**Key Contributions:**

	1. Introduction of Chronotome for visualizing temporal changes in datasets.
	2. Combination of force-based projection with streaming clustering methods.
	3. Real-time interactive exploration of evolving themes in data.

**Result:** Chronotome allows for interactive exploration of evolving themes in real-time across text and image data, demonstrating enhanced understanding of aesthetics and semantics in temporal datasets.

**Limitations:** 

**Conclusion:** Chronotome provides a novel approach for analyzing time-based data, revealing evolving themes effectively.

**Abstract:** Many real-world datasets -- from an artist's body of work to a person's social media history -- exhibit meaningful semantic changes over time that are difficult to capture with existing dimensionality reduction methods. To address this gap, we introduce a visualization technique that combines force-based projection and streaming clustering methods to build a spatial-temporal map of embeddings. Applying this technique, we create Chronotome, a tool for interactively exploring evolving themes in time-based data -- in real time. We demonstrate the utility of our approach through use cases on text and image data, showing how it offers a new lens for understanding the aesthetics and semantics of temporal datasets.

</details>


### [9] [CosinorAge: Unified Python and Web Platform for Biological Age Estimation from Wearable- and Smartwatch-Based Activity Rhythms](https://arxiv.org/abs/2509.01089)

*Jinjoo Shim, Jacob Hunecke, Elgar Fleisch, Filipe Barata*

**Main category:** cs.HC

**Keywords:** biological aging, wearable technology, health informatics

**Relevance Score:** 9

**TL;DR:** CosinorAge is an open-source framework for estimating biological age from wearable data, addressing fragmentation in health data analysis and promoting accessibility for users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To transform self-tracking data from wearable devices into actionable insights regarding biological aging, overcoming the barriers posed by fragmented and proprietary analysis tools.

**Method:** CosinorAge is a Python package that processes raw wearable data, computes features, estimates biological age, and provides trained model parameters derived from large-scale datasets. It also includes a web-based calculator for non-technical users.

**Key Contributions:**

	1. Open-source framework for biological age estimation from wearables
	2. Support for multiple data sources and preprocessing workflows
	3. Web-based interface for non-technical users to access analytical capabilities

**Result:** The framework offers a unified pipeline for analyzing various health metrics, enabling reproducible and transparent health monitoring, and links self-tracking to aging research.

**Limitations:** 

**Conclusion:** CosinorAge facilitates personalized health insights by bridging the gap between digital health technologies and biological aging research, enhancing accessibility and usability for a broader audience.

**Abstract:** Every day, millions of people worldwide track their steps, sleep, and activity rhythms with smartwatches and fitness trackers. These continuously collected data streams present a remarkable opportunity to transform routine self-tracking into meaningful health insights that enable individuals to understand -- and potentially influence -- their biological aging. Yet most tools for analyzing wearable data remain fragmented, proprietary, and inaccessible, creating a major barrier between this vast reservoir of personal health information and its translation into actionable insights on aging. CosinorAge is an open-source framework that estimates biological age from wearable-derived circadian, physical activity, and sleep metrics. It addresses the lack of unified, reproducible pipelines for jointly analyzing rest--activity rhythmicity, physical activity, and sleep, and linking them to health outcomes. The Python package provides an end-to-end workflow from raw data ingestion and preprocessing to feature computation and biological age estimation, supporting multiple input sources across wearables and smartwatch. It also makes available trained model parameters (open weights) derived from large-scale population datasets such as UK Biobank, enabling reproducibility, transparency, and generalizability across studies. Its companion web-based CosinorAge Calculator enables non-technical users to access identical analytical capabilities through an intuitive interface. By combining transparent, reproducible analysis with broad accessibility, CosinorAge advances scalable, personalized health monitoring and bridges digital health technologies with biological aging research.

</details>


### [10] [Unpacking Personal(?!) Health Informatics: An Investigation of Awareness, Understanding, And Leveraged Utility in India](https://arxiv.org/abs/2509.01231)

*Shyama Sastha Krishnamoorthy Srinivasan, Mohan Kumar, Pushpendra Singh*

**Main category:** cs.HC

**Keywords:** Personal Health Informatics, health literacy, usability, digital health, India

**Relevance Score:** 8

**TL;DR:** This study investigates the adoption of Personal Health Informatics (PHI) in India, identifying barriers and proposing design recommendations for a user-centered platform.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To explore barriers to the adoption of Personal Health Informatics in India and to propose solutions for enhancing healthcare delivery through digital tools.

**Method:** A multi-method approach including an awareness survey (n=87) on PHI usage, semi-structured interviews (n=22) for qualitative insights, and usability evaluation of a developed prototype.

**Key Contributions:**

	1. Identification of barriers to PHI adoption specific to the Indian context.
	2. Development of a prototype for a user-controlled PHI platform based on user feedback.
	3. Design recommendations aimed at improving usability and trust in digital health platforms.

**Result:** Barriers to PHI adoption identified include low health literacy, usability issues, and mistrust in digital platforms. Insights led to the design of a user-controlled PHI platform.

**Limitations:** Focused on a specific demographic in India; findings may not generalize to other regions or populations.

**Conclusion:** The study emphasizes the necessity for reliable and user-centric solutions to address the socio-technical challenges faced in PHI adoption in India.

**Abstract:** Personal Health Informatics (PHI), which leverages digital tools and information systems to support health assessment and self-care, holds promise for empowering individuals and transforming healthcare delivery. However, barriers to its adoption remain underexplored in the Indian context. This study investigates PHI adoption among Indian users and stakeholders using a multi-method approach. An awareness survey (n = 87) examined the usage of wearables and general PHI engagement, followed by semi-structured interviews (n = 22) that explored motivations, usage patterns, and health information sources. Qualitative analysis revealed that while PHI is valued for health monitoring and shared/collective care, its adoption is hindered by factors such as low health literacy, usability challenges, and mistrust in digital health platforms. Further stakeholder interviews and co-design workshops informed the development of a Figma-based prototype, which was evaluated for usability. Based on these findings, we offer design recommendations for an integrated, user-controlled PHI platform featuring accessible analytics and verifiable health information. Our insights highlight the socio-technical challenges of PHI adoption in India and underscore the need for reliable, user-centric solutions to support proactive healthcare.

</details>


### [11] [An AI-Based Shopping Assistant System to Support the Visually Impaired](https://arxiv.org/abs/2509.01246)

*Larissa R. de S. Shibata, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata*

**Main category:** cs.HC

**Keywords:** AI-based shopping assistant, accessibility, inclusivity, visual impairments, assistive technologies

**Relevance Score:** 8

**TL;DR:** The paper presents an AI-based shopping assistant prototype that enhances autonomy and inclusivity for visually impaired individuals shopping in supermarkets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by visually impaired individuals in navigating supermarkets and identifying products.

**Method:** The system uses computer vision, speech recognition, text-to-speech synthesis, and indoor navigation technologies for real-time environmental scanning and guidance.

**Key Contributions:**

	1. Integration of multiple technologies into a single user-friendly platform
	2. Real-time auditory guidance and context about surroundings
	3. Focus on enhancing user autonomy and inclusivity in shopping

**Result:** Experiments showed that the assistant effectively guided users and improved their shopping experience in supermarket environments.

**Limitations:** 

**Conclusion:** The development of this AI-driven assistive technology contributes to enhancing accessibility and independence for visually impaired shoppers.

**Abstract:** Shopping plays a significant role in shaping consumer identity and social integration. However, for individuals with visual impairments, navigating in supermarkets and identifying products can be an overwhelming and challenging experience. This paper presents an AI-based shopping assistant prototype designed to enhance the autonomy and inclusivity of visually impaired individuals in supermarket environments. The system integrates multiple technologies, including computer vision, speech recognition, text-to-speech synthesis, and indoor navigation, into a single, user-friendly platform. Using cameras for ArUco marker detection and real-time environmental scanning, the system helps users navigate the store, identify product locations, provide real-time auditory guidance, and gain context about their surroundings. The assistant interacts with the user through voice commands and multimodal feedback, promoting a more dynamic and engaging shopping experience. The system was evaluated through experiments, which demonstrated its ability to guide users effectively and improve their shopping experience. This paper contributes to the development of inclusive AI-driven assistive technologies aimed at enhancing accessibility and user independence for the shopping experience.

</details>


### [12] [MetaRoundWorm: A Virtual Reality Escape Room Game for Learning the Lifecycle and Immune Response to Parasitic Infections](https://arxiv.org/abs/2509.01367)

*Xuanru Cheng, Xian Wang, Chi-lok Tai, Lik-Hang Lee*

**Main category:** cs.HC

**Keywords:** virtual reality, gamification, biomedical education, immune system, parasitic infections

**Relevance Score:** 6

**TL;DR:** MetaRoundWorm is a VR game that gamifies learning about parasitic infections and the immune system, showing improved educational outcomes compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of promoting public health education, particularly regarding understanding the immune system and related parasitic infections.

**Method:** An immersive VR escape room game that simulates the lifecycle of Ascaris lumbricoides and related immunological responses, combining exploration, puzzle-solving, and task-driven experiences.

**Key Contributions:**

	1. Development of a VR educational game for health education
	2. Evidence of improved learning outcomes and user engagement
	3. Integration of serious game mechanics with educational principles

**Result:** MetaRoundWorm significantly improves immediate learning outcomes, cognitive engagement, and emotional experience while ensuring knowledge retention over time compared to traditional interactive slides.

**Limitations:** 

**Conclusion:** Immersive VR gamification is an effective pedagogical tool for conveying complex biomedical concepts and enhancing digital health education.

**Abstract:** Promoting public health is challenging owing to its abstract nature, and individuals may be apprehensive about confronting it. Recently, there has been an increasing interest in using the metaverse and gamification as novel educational techniques to improve learning experiences related to the immune system. Thus, we present MetaRoundWorm, an immersive virtual reality (VR) escape room game designed to enhance the understanding of parasitic infections and host immune responses through interactive, gamified learning. The application simulates the lifecycle of Ascaris lumbricoides and corresponding immunological mechanisms across anatomically accurate environments within the human body. Integrating serious game mechanics with embodied learning principles, MetaRoundWorm offers players a task-driven experience combining exploration, puzzle-solving, and immune system simulation. To evaluate the educational efficacy and user engagement, we conducted a controlled study comparing MetaRoundWorm against a traditional approach, i.e., interactive slides. Results indicate that MetaRoundWorm significantly improves immediate learning outcomes, cognitive engagement, and emotional experience, while maintaining knowledge retention over time. Our findings suggest that immersive VR gamification holds promise as an effective pedagogical tool for communicating complex biomedical concepts and advancing digital health education.

</details>


### [13] [AttenTrack: Mobile User Attention Awareness Based on Context and External Distractions](https://arxiv.org/abs/2509.01414)

*Yutong Lin, Suyuan Liu, Kaiwen Guo, Haohua Du, Chao Liu, Xiang-Yang Li*

**Main category:** cs.HC

**Keywords:** attention management, mobile notifications, attention-aware systems

**Relevance Score:** 7

**TL;DR:** A study proposing AttenTrack, an attention-aware model for mobile notifications that does not rely on personalized data and has strong cold-start capability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To manage limited attention amid information overload in the mobile internet era and improve collaboration and information delivery without relying on wearable devices or personalized data.

**Method:** Field study framework combining subjective and objective data to analyze user response behaviors, contextual information, and attention states, leading to the development of AttenTrack.

**Key Contributions:**

	1. Development of AttenTrack, a lightweight attention awareness model
	2. Creation of a publicly available dataset for attention management research
	3. Analysis of the relationship between context, distractions, and attention states

**Result:** Constructed a fine-grained dataset and proposed a privacy-friendly attention awareness model that works well across different contexts without needing historical data.

**Limitations:** 

**Conclusion:** AttenTrack can effectively manage attention from mobile notifications while being applicable to various attention management tasks, with a dataset to support further research.

**Abstract:** In the mobile internet era, managing limited attention amid information overload is crucial for enhancing collaboration and information delivery. However, current attention-aware systems often depend on wearables or personalized data, limiting their scalability and cross-context adaptability. Inspired by psychological theories, we attempt to treat mobile notifications as naturally occurring external distractions and infer users' attention states based on their response behaviors and contextual information. Our goal is to build an attention-aware model that does not rely on personalized historical data or complex subjective input, while ensuring strong cold-start capability and cross-context adaptability. To this end, We design a field study framework integrating subjective and objective data, closely aligned with real-world external distractions (i.e., mobile notifications). Through field studies, we construct a fine-grained and interpretable dataset centered on the relationship among current context - external distractions - subjective attention. Through our field studies, we conduct an in-depth analysis of the relationships among users' response behaviors, response motivations, contextual information, and attention states. Building on our findings, we propose AttenTrack, a lightweight, privacy-friendly attention awareness model with strong cold-start capability. The model relies solely on non-privacy-sensitive objective data available on mobile devices, and can be applied to a variety of attention management tasks. In addition, we will publicly release the constructed dataset to support future research and advance the field of mobile attention awareness.

</details>


### [14] [Body Ownership Affects the Processing of Sensorimotor Contingencies in Virtual Reality](https://arxiv.org/abs/2509.01420)

*Evan G. Center, Matti Pouke, Alessandro Nardi, Lukas Gehrke, Klaus Gramann, Timo Ojala, Steven M. LaValle*

**Main category:** cs.HC

**Keywords:** virtual reality, presence, EEG, event-related potentials, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper investigates the measurement of presence in virtual reality using EEG, addressing shortcomings of previous research by manipulating presence and validating measures against traditional techniques in a preregistered ERP experiment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the measurement of presence in VR, a key factor in user experience, by leveraging EEG instead of traditional self-reporting methods that can disrupt the virtual experience.

**Method:** A preregistered ERP experiment where participants engage in a target shooting game in VR, with manipulations of presence and embodiment. Event-related potentials are measured in response to specific trial conditions.

**Key Contributions:**

	1. Introduced an EEG-based measurement approach for presence in VR
	2. Validated presence measures against traditional methods
	3. Manipulated presence and embodiment in a controlled VR setting

**Result:** The study found that the N2, P3b, and N400 ERP components are sensitive to manipulations of presence and embodiment, providing a new perspective on how presence can be quantified in VR settings.

**Limitations:** The experiment’s realism may be limited to the designed scenario and may not generalize to all VR experiences.

**Conclusion:** The findings suggest that EEG can effectively measure presence in virtual environments, offering a more reliable method compared to self-reports and enhancing the understanding of presence-related theories.

**Abstract:** Presence in virtual reality (VR), the subjective sense of "being there" in a virtual environment, is notoriously difficult to measure. Electroencephalography (EEG) may offer a promising, unobtrusive means of assessing a user's momentary state of presence. Unlike traditional questionnaires, EEG does not interrupt the experience or rely on users' retrospective self-reports, thereby avoiding interference with the very state it aims to capture. Previous research has attempted to quantify presence in virtual environments using event-related potentials (ERPs). We contend, however, that previous efforts have fallen short of fully realizing this goal, failing to either A) independently manipulate presence, B) validate their measure of presence against traditional techniques, C) adequately separate the constructs of presence and attention, and/or D) implement a realistic and immersive environment and task. We address these shortcomings in a preregistered ERP experiment in which participants play an engaging target shooting game in VR. ERPs are time-locked to the release of a ball from a sling. We induce breaks in presence (BIPs) by freezing the ball's release on a minority of trials. Embodiment is manipulated by allowing manual manipulation of the sling with a realistic avatar in one condition (embodied condition) and passive manipulation with only controllers in another (non-embodied condition). We support our predictions that the N2, the P3b, and the N400, are selectively sensitive towards specific components of these manipulations. The pattern of findings carries significant implications for theories of presence, which have been seldom addressed in previous ERP investigations on this topic.

</details>


### [15] [Dissecting Atomic Facts: Visual Analytics for Improving Fact Annotations in Language Model Evaluation](https://arxiv.org/abs/2509.01460)

*Manuel Schmidt, Daniel A. Keim, Frederik L. Dennig*

**Main category:** cs.HC

**Keywords:** factuality evaluation, large language models, fact extraction, visual analytics, annotation consistency

**Relevance Score:** 8

**TL;DR:** This paper presents a visual analytics approach to improve the consistency of factuality evaluation for large language models by analyzing annotation discrepancies in fact extraction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the high disagreement among annotators in decomposing text into atomic facts, which complicates factuality evaluation of LLM outputs.

**Method:** The methodology involves visualizing semantic alignment, granularity, and referential dependencies to facilitate systematic inspection of extracted facts and support guided revision loops.

**Key Contributions:**

	1. Introduction of a visual analytics concept for fact extraction
	2. Explanation of the impact of annotation inconsistencies in LLM evaluation
	3. Guided revision loops for better consensus among annotators

**Result:** The approach enables a better understanding of inconsistencies in fact extraction and helps establish a more stable foundation for factuality evaluation benchmarks.

**Limitations:** The paper does not provide extensive empirical evaluation of the proposed method and its effectiveness in different contexts.

**Conclusion:** Ultimately, this work aims to enhance the evaluation of large language models by reducing annotation inconsistencies and improving factual accuracy.

**Abstract:** Factuality evaluation of large language model (LLM) outputs requires decomposing text into discrete "atomic" facts. However, existing definitions of atomicity are underspecified, with empirical results showing high disagreement among annotators, both human and model-based, due to unresolved ambiguity in fact decomposition. We present a visual analytics concept to expose and analyze annotation inconsistencies in fact extraction. By visualizing semantic alignment, granularity and referential dependencies, our approach aims to enable systematic inspection of extracted facts and facilitate convergence through guided revision loops, establishing a more stable foundation for factuality evaluation benchmarks and improving LLM evaluation.

</details>


### [16] [Quantifying the Effect of Thermal Illusions in Virtual Reality](https://arxiv.org/abs/2509.01609)

*Yannick Weiss, Marlene Eder, Oguzhan Cesur, Steeven Villa*

**Main category:** cs.HC

**Keywords:** thermal illusions, virtual reality, temperature perception, cross-modal interactions, user studies

**Relevance Score:** 6

**TL;DR:** This paper investigates the effectiveness of thermal illusions in simulating temperature sensations in virtual and extended reality systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how thermal illusions can enhance virtual experiences by simulating temperature sensations without bulky hardware.

**Method:** Two user studies were conducted to order and scale the effects of various visual and auditory cues on perceived temperature, involving 20 and 24 participants respectively.

**Key Contributions:**

	1. Introduction of auditory cues as a potential method for enhancing thermal illusions
	2. Quantification of the efficacy of thermal illusions in temperature perception
	3. Order and scaling of visual and auditory cues based on user studies

**Result:** Thermal illusions produced significant changes in subjective temperature ratings, with auditory cues showing promise as alternatives or complements to visual methods, though effects were small (+-0.5°C).

**Limitations:** The extent of the effect on perceived temperature was small and not always aligned with abstract ratings, indicating limitations in the current approaches.

**Conclusion:** The findings suggest reconsideration of the design and evaluation of thermal illusions due to the modest effects recorded.

**Abstract:** Thermal sensations are central to how we experience the world, yet most virtual and extended reality systems fail to simulate them effectively. While hardware-based thermal displays can provide accurate temperature changes, they are often bulky, power-intensive, and restrict user mobility. Consequently, recent works have explored thermal illusions, perceptual effects that rely on cross-modal interactions, to achieve thermal experiences without physical heating or cooling. While thermal illusions have been shown to consistently alter subjective ratings, the actual extent of their effect on the perceived temperature of interacted objects remains unexplored. To address this, we contribute the findings of two user studies following psychophysical procedures. We first ordered and scaled the effects of a variety of visual and auditory cues (N=20) and subsequently quantified their isolated and combined efficacy in offsetting physical temperature changes (N=24). We found that thermal illusions elicited robust changes in subjective judgments, and auditory cues showed potential as an alternative or complementary approach to established visual techniques. However, the actual effects induced by thermal illusions were relatively small (+-0.5{\deg}C) and did not consistently align with abstract ratings, suggesting a need to reconsider how future thermal illusions or experiences are designed and evaluated.

</details>


### [17] [An Interactive Google Earth Engine Application for Global Multi-Scale Vegetation Analysis Using NDVI Thresholding](https://arxiv.org/abs/2509.01628)

*Md. Moktader Moula, Israt Jahan Shonom, Azharul Islam, Mohammad Mosharraf Hossain*

**Main category:** cs.HC

**Keywords:** vegetation dynamics, NDVI, remote sensing, cloud-based application, Google Earth Engine

**Relevance Score:** 3

**TL;DR:** An interactive cloud-based application for simplified global vegetation analysis using NDVI.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an accessible solution for monitoring vegetation dynamics without complex technical expertise.

**Method:** An application on Google Earth Engine automates NDVI calculations using Sentinel-2 and Landsat imagery, allowing users to create cloud-free vegetation images from a median composite of data.

**Key Contributions:**

	1. User-friendly cloud-based platform for vegetation analysis
	2. Automation of NDVI calculations to reduce complexity and resource needs
	3. High accuracy validation results supporting real-time decision making

**Result:** Validation in two protected areas in Bangladesh showed over 97% agreement with reference data for area estimates.

**Limitations:** 

**Conclusion:** The tool enables non-experts to easily access geospatial analytics for effective vegetation monitoring and policy support.

**Abstract:** Monitoring vegetation dynamics is crucial for addressing global environmental challenges like degradation and deforestation, but traditional remote sensing methods are often complex and resource-intensive. To overcome these barriers, we developed an interactive, cloud-based application on the Google Earth Engine (GEE) platform for few clicks on-demand global vegetation analysis without complex technical knowledge. The application automates the calculation of vegetated areas using the Normalized Difference Vegetation Index (NDVI) derived from Sentinel-2 and Landsat imagery. It utilizes a median composite of images over a selected period to create a single, robust, cloud-free image, minimizing atmospheric noise and other artifacts. It offers a flexible, global multi-scale analytical platform, allowing users to define regions of interest based on administrative boundaries, protected areas, or custom-drawn polygons. The user-friendly interface enables the selection of specific time periods and NDVI thresholds to quantify vegetation cover in real time, eliminating the need for manual and time intensive data handling and processing. A validation of the platform was conducted for two protected areas in Bangladesh which demonstrated high accuracy, with area estimates showing over 97% agreement with published reference data. By simplifying access to powerful geospatial analytics to general people, this tool provides a scalable and practical solution for researchers, land managers, policymakers, and any interested person to monitor vegetation trends, support conservation efforts, to inform decision making in spatial context where policy maker need to use insights in few clicks and inform environmental policy.

</details>


### [18] [EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras](https://arxiv.org/abs/2509.01786)

*Vimal Mollyn, Chris Harrison*

**Main category:** cs.HC

**Keywords:** augmented reality, virtual reality, on-body input, human-computer interaction, touch interfaces

**Relevance Score:** 8

**TL;DR:** Demonstrates accurate skin input using RGB cameras in AR/VR, enhancing touch interface capabilities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing in-air interfaces by enabling high-accuracy on-body input in AR/VR environments.

**Method:** Utilizes an RGB camera to capture skin input without special instrumentation, assessing performance across various conditions.

**Key Contributions:**

	1. Demonstration of bare hands skin input accuracy using RGB cameras
	2. Robustness across lighting conditions and body movements
	3. Rich input metadata including touch force and finger identification

**Result:** Achieves high accuracy in touch input, demonstrating robustness in diverse lighting, skin tones, and motion.

**Limitations:** 

**Conclusion:** The developed pipeline unlocks new possibilities for on-skin interfaces, bolstering the potential for richer human-computer interaction.

**Abstract:** In augmented and virtual reality (AR/VR) experiences, a user's arms and hands can provide a convenient and tactile surface for touch input. Prior work has shown on-body input to have significant speed, accuracy, and ergonomic benefits over in-air interfaces, which are common today. In this work, we demonstrate high accuracy, bare hands (i.e., no special instrumentation of the user) skin input using just an RGB camera, like those already integrated into all modern XR headsets. Our results show this approach can be accurate, and robust across diverse lighting conditions, skin tones, and body motion (e.g., input while walking). Finally, our pipeline also provides rich input metadata including touch force, finger identification, angle of attack, and rotation. We believe these are the requisite technical ingredients to more fully unlock on-skin interfaces that have been well motivated in the HCI literature but have lacked robust and practical methods.

</details>


### [19] [Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore](https://arxiv.org/abs/2509.01845)

*Gabriel Spadon, Oladapo Oyebode, Camilo M. Botero, Tushar Sharma, Floris Goerlandt, Ronald Pelot*

**Main category:** cs.HC

**Keywords:** climate resilience, community engagement, digital archive, interdisciplinary collaboration, coastal monitoring

**Relevance Score:** 4

**TL;DR:** An initiative to enhance climate resilience in Nova Scotia's Eastern Shore through community collaboration and technology integration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address climate change threats faced by rural communities in Nova Scotia and maintain their way of life.

**Method:** A collaborative approach involving expertise from Computer Science, Industrial Engineering, and Coastal Geography, along with community engagement, particularly with elders, to create a living digital archive.

**Key Contributions:**

	1. Development of a living digital archive co-created with the community
	2. Integration of interdisciplinary expertise to tackle climate challenges
	3. Establishment of a replicable collaboration model for community-driven tech solutions

**Result:** The project successfully integrates traditional knowledge and modern technology to strengthen community resilience against climate change.

**Limitations:** 

**Conclusion:** The initiative demonstrates a replicable model for using technology to support and empower traditional communities in climate adaptation.

**Abstract:** This paper presents an overview of a human-centered initiative aimed at strengthening climate resilience along Nova Scotia's Eastern Shore. This region, a collection of rural villages with deep ties to the sea, faces existential threats from climate change that endanger its way of life. Our project moves beyond a purely technical response, weaving together expertise from Computer Science, Industrial Engineering, and Coastal Geography to co-create tools with the community. By integrating generational knowledge of residents, particularly elders, through the Eastern Shore Citizen Science Coastal Monitoring Network, this project aims to collaborate in building a living digital archive. This effort is hosted under Dalhousie University's Transforming Climate Action (TCA) initiative, specifically through its Transformative Adaptations to Social-Ecological Climate Change Trajectories (TranSECT) and TCA Artificial Intelligence (TCA-AI) projects. This work is driven by a collaboration model in which student teams work directly with residents. We present a detailed project timeline and a replicable model for how technology can support traditional communities, enabling them to navigate climate transformation more effectively.

</details>


### [20] [E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI](https://arxiv.org/abs/2509.02100)

*Sharjeel Tahir, Judith Johnson, Jumana Abu-Khalaf, Syed Afaq Ali Shah*

**Main category:** cs.HC

**Keywords:** empathy, AI, therapeutic engagement, verbal-visual incongruence, multimodal dataset

**Relevance Score:** 9

**TL;DR:** The paper introduces E-THER, a novel multimodal dataset for detecting verbal-visual incongruence in empathic AI systems, enabling better training for genuine empathy and therapeutic engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current empathic AI systems struggle to recognize emotional states due to superficial emotion recognition datasets that overlook verbal-visual incongruence.

**Method:** E-THER is a Person-Centered Therapy-based dataset with multidimensional annotations focusing on verbal-visual emotional misalignment in client-counsellor interactions, aimed at enhancing empathic AI training.

**Key Contributions:**

	1. Introduction of the E-THER dataset for verbal-visual incongruence detection
	2. Multi-dimensional annotations grounded in Person-Centered Therapy
	3. Demonstrated improvement in conversational qualities of AI models through incongruence training

**Result:** Empirically, models trained on the E-THER dataset show improved performance in empathic and therapeutic dialogue qualities compared to general-purpose models, as they better sustain engagement and align with therapeutic principles.

**Limitations:** 

**Conclusion:** The findings highlight the importance of verbal-visual congruence in developing empathetic AI systems capable of genuine therapeutic interactions.

**Abstract:** A prevalent shortfall among current empathic AI systems is their inability to recognize when verbal expressions may not fully reflect underlying emotional states. This is because the existing datasets, used for the training of these systems, focus on surface-level emotion recognition without addressing the complex verbal-visual incongruence (mismatch) patterns useful for empathic understanding. In this paper, we present E-THER, the first Person-Centered Therapy-grounded multimodal dataset with multidimensional annotations for verbal-visual incongruence detection, enabling training of AI systems that develop genuine rather than performative empathic capabilities. The annotations included in the dataset are drawn from humanistic approach, i.e., identifying verbal-visual emotional misalignment in client-counsellor interactions - forming a framework for training and evaluating AI on empathy tasks. Additional engagement scores provide behavioral annotations for research applications. Notable gains in empathic and therapeutic conversational qualities are observed in state-of-the-art vision-language models (VLMs), such as IDEFICS and VideoLLAVA, using evaluation metrics grounded in empathic and therapeutic principles. Empirical findings indicate that our incongruence-trained models outperform general-purpose models in critical traits, such as sustaining therapeutic engagement, minimizing artificial or exaggerated linguistic patterns, and maintaining fidelity to PCT theoretical framework.

</details>


### [21] [Shared Control for Game Accessibility: Understanding Current Human Cooperation Practices to Inform the Design of Partial Automation Solutions](https://arxiv.org/abs/2509.02132)

*Dragan Ahmetovic, Matteo Manzoni, Filippo Corti, Sergio Mascetti*

**Main category:** cs.HC

**Keywords:** shared control, gaming accessibility, automation, disabilities, support systems

**Relevance Score:** 8

**TL;DR:** Explores shared control in accessible gaming, highlighting the need for automation of support systems to ease access for players with disabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how shared control technologies are used by individuals with disabilities in gaming, and to address the limitations of human assistance in these scenarios.

**Method:** Interviews conducted with 14 individuals having lived experience of accessible gaming using shared control technologies.

**Key Contributions:**

	1. Insights into current practices in shared control for gaming accessibility
	2. Identification of limitations in human-supported shared control
	3. Proposals for designing automated support systems

**Result:** Shared control is vital for accessing inaccessible games, but it heavily relies on human support, which can be mitigated through automation.

**Limitations:** The study primarily focuses on subjective experiences, which may not generalize to all shared control technologies or gaming contexts.

**Conclusion:** The study suggests that while shared control enhances gaming accessibility, moving toward automated support systems could improve the experience and reduce dependency on human assistance.

**Abstract:** Shared control is a form of video gaming accessibility support that allows players with disabilities to delegate inaccessible controls to another person. Through interviews involving 14 individuals with lived experience of accessible gaming in shared control, we explore the ways in which shared control technologies are adopted in practice, the accessibility challenges they address, and how the support currently provided in shared control can be automated to remove the need for a human assistant. Findings indicate that shared control is essential for enabling access to otherwise inaccessible games, but its reliance on human support is a key limitation. Participants welcomed the idea of automating the support with software agents, while also identifying limitations and design requirements. Accordingly, this work contributes insights into current practices and proposes guidelines for developing automated support systems.

</details>


### [22] [A Theoretical Framework of the Processes of Change in Psychotherapy Delivered by Artificial Agents](https://arxiv.org/abs/2509.02144)

*Arthur Bran Herbener, Malene Flensborg Damholdt*

**Main category:** cs.HC

**Keywords:** artificial agents, psychotherapy, human therapists, credibility gap, HCI

**Relevance Score:** 9

**TL;DR:** This paper presents a theoretical framework analyzing the effectiveness of psychotherapy delivered by artificial agents in comparison to human therapists, focusing on the influence of the therapists' human status on treatment outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the processes of change in psychotherapy delivered by artificial agents, which remains under-researched despite the rise of large language models.

**Method:** The paper offers a conceptual analysis of the inherent active ingredients linked to human therapists in the context of therapy delivered by artificial agents.

**Key Contributions:**

	1. First theoretical framework for psychotherapy processes involving artificial agents
	2. Identification of the genuineness and credibility gaps
	3. Discussion of the unique agentic nature of artificial agents in therapeutic contexts

**Result:** The authors identify the 'genuineness gap' and 'credibility gap' that may arise when replacing human therapists with artificial agents, potentially undermining treatment outcomes.

**Limitations:** 

**Conclusion:** The study proposes avenues for further scientific investigation into leveraging both artificial agents and human therapists effectively, while calling attention to the complex nature of artificial agents.

**Abstract:** The question of whether artificial agents (e.g., chatbots and social robots) can replace human therapists has received notable attention following the recent launch of large language models. However, little is known about the processes of change in psychotherapy delivered by artificial agents. To facilitate hypothesis development and stimulate scientific debate, the present article offers the first theoretical framework of the processes of change in psychotherapy delivered by artificial agents. The theoretical framework rests upon a conceptual analysis of what active ingredients may be inherently linked to the presence of human therapists. We propose that human therapists' ontological status as human beings and sociocultural status as socially sanctioned healthcare professionals play crucial roles in promoting treatment outcomes. In the absence of the ontological and sociocultural status of human therapists, we propose what we coin the genuineness gap and credibility gap can emerge and undermine key processes of change in psychotherapy. Based on these propositions, we propose avenues for scientific investigations and practical applications aimed at leveraging the strengths of artificial agents and human therapists respectively. We also highlight the intricate agentic nature of artificial agents and discuss how this complicates endeavors to establish universally applicable propositions regarding the processes of change in these interventions.

</details>


### [23] [Look: AI at Work! - Analysing Key Aspects of AI-support at the Work Place](https://arxiv.org/abs/2509.02274)

*Stefan Schiffer, Anna Milena Rothermel, Alexander Ferrein, Astrid Rosenthal-von der Pütten*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Workplace Integration, AI Literacy, Trust in AI, Human-AI Interaction

**Relevance Score:** 6

**TL;DR:** Analysis of technological and psychological factors for AI applications in workplaces through twelve case studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of AI in future work contexts and develop recommendations for effective AI application and literacy

**Method:** Analysis of twelve specific application cases of AI at workplaces, focusing on technological areas and psychological factors.

**Key Contributions:**

	1. Recommendations for developing AI applications in workplaces
	2. Identifies key psychological factors influencing AI acceptance
	3. Insights into the importance of high-quality data and human expertise

**Result:** Identification of key areas in AI application development, emphasizing high-quality data and human expertise, along with psychological factors affecting acceptance and trust.

**Limitations:** 

**Conclusion:** The study provides insights into improving AI literacy and developing AI systems that are accepted and trusted by users.

**Abstract:** In this paper we present an analysis of technological and psychological factors of applying artificial intelligence (AI) at the work place. We do so for a number of twelve application cases in the context of a project where AI is integrated at work places and in work systems of the future. From a technological point of view we mainly look at the areas of AI that the applications are concerned with. This allows to formulate recommendations in terms of what to look at in developing an AI application and what to pay attention to with regards to building AI literacy with different stakeholders using the system. This includes the importance of high-quality data for training learning-based systems as well as the integration of human expertise, especially with knowledge-based systems. In terms of the psychological factors we derive research questions to investigate in the development of AI supported work systems and to consider in future work, mainly concerned with topics such as acceptance, openness, and trust in an AI system.

</details>


### [24] [Balaton Borders: Data Ceramics for Ecological Reflection](https://arxiv.org/abs/2509.02284)

*Hajnal Gyeviki, Mihály Minkó, Mary Karyda, Damla Çay*

**Main category:** cs.HC

**Keywords:** ceramics, ecological data, performative dining, environmental awareness, Lake Balaton

**Relevance Score:** 2

**TL;DR:** Balaton Borders transforms ecological data into ceramic tableware to create multisensory dining experiences that highlight human impact on the environment.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To raise awareness and provoke reflection on ecological disruption through the medium of ceramics and dining.

**Method:** Designing ceramic tableware that incorporates ecological data, facilitating performative dining experiences.

**Key Contributions:**

	1. Translates ecological data into a tangible art form (ceramics)
	2. Creates multisensory dining experiences that encourage societal reflection
	3. Addresses issues of ecological disruption through innovative design

**Result:** The tableware engages users in collective reflection on the ecological changes in Lake Balaton, such as reedbed reduction and shoreline modification.

**Limitations:** 

**Conclusion:** By integrating ecological data into dining experiences, the project aims to enhance awareness of environmental issues.

**Abstract:** Balaton Borders translates ecological data from Lake Balaton into ceramic tableware that represents human impact on the landscape, from reedbed reduction to shoreline modification and land erosion. Designed for performative dining, the pieces turn shared meals into multisensory encounters where food and data ceramics spark collective reflection on ecological disruption.

</details>


### [25] [Talking Spell: A Wearable System Enabling Real-Time Anthropomorphic Voice Interaction with Everyday Objects](https://arxiv.org/abs/2509.02367)

*Xuetong Wang, Ching Christie Pang, Pan Hui*

**Main category:** cs.HC

**Keywords:** Virtual Assistants, Human-Computer Interaction, Emotional Connection

**Relevance Score:** 8

**TL;DR:** Introducing Talking Spell, a wearable system that enhances emotional connections by allowing users to give everyday objects anthropomorphic personas through speech and interactions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create emotional bonds between users and everyday objects, reducing feelings of detachment commonly associated with existing virtual assistants.

**Method:** Talking Spell employs a user-centric radiative network, integrating computer vision, large vision-language models, and speech processing to guide users through emotional connection stages.

**Key Contributions:**

	1. User-centric design for emotional connections with objects
	2. Integration of advanced computer vision and large language models
	3. Validation through user study demonstrating effectiveness

**Result:** User study with 12 participants revealed that Talking Spell fosters meaningful interactions and enhances emotional significance with everyday objects across various intentions.

**Limitations:** 

**Conclusion:** Talking Spell provides engaging and personalized experiences, making everyday objects more interactive and emotionally resonant for users.

**Abstract:** Virtual assistants (VAs) have become ubiquitous in daily life, integrated into smartphones and smart devices, sparking interest in AI companions that enhance user experiences and foster emotional connections. However, existing companions are often embedded in specific objects-such as glasses, home assistants, or dolls-requiring users to form emotional bonds with unfamiliar items, which can lead to reduced engagement and feelings of detachment. To address this, we introduce Talking Spell, a wearable system that empowers users to imbue any everyday object with speech and anthropomorphic personas through a user-centric radiative network. Leveraging advanced computer vision (e.g., YOLOv11 for object detection), large vision-language models (e.g., QWEN-VL for persona generation), speech-to-text and text-to-speech technologies, Talking Spell guides users through three stages of emotional connection: acquaintance, familiarization, and bonding. We validated our system through a user study involving 12 participants, utilizing Talking Spell to explore four interaction intentions: entertainment, companionship, utility, and creativity. The results demonstrate its effectiveness in fostering meaningful interactions and emotional significance with everyday objects. Our findings indicate that Talking Spell creates engaging and personalized experiences, as demonstrated through various devices, ranging from accessories to essential wearables.

</details>


### [26] [Octo's Heartland: Supporting Children with Congenital Heart Disease through Digital Health Education](https://arxiv.org/abs/2509.02537)

*Irene Zeng, Neda Barbazi, Ji Youn Shin, Gurumurthy Hiremath, Carlye Anne Lauff*

**Main category:** cs.HC

**Keywords:** Congenital heart disease, Health literacy, Serious games, Participatory design, User testing

**Relevance Score:** 4

**TL;DR:** The paper discusses the development of a digital application for children with congenital heart disease (CHD) aimed at improving health literacy through metaphor-based gameplay.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To support health education for children with congenital heart disease by designing tools that cater to their developmental needs and improve health outcomes.

**Method:** The research utilized participatory design and user testing with 30 children to develop and refine a digital application focused on CHD education through serious game principles.

**Key Contributions:**

	1. Design and refinement of a digital application tailored for children with CHD.
	2. Informed the application of serious game principles to enhance health literacy.
	3. Identified design opportunities for future health education tools.

**Result:** User testing highlighted improvements in usability, engagement, and comprehension, with specific interactive activities redesigned to align with children's understanding of heart health.

**Limitations:** Limited to children with CHD; may not generalize to other health conditions.

**Conclusion:** The study emphasizes the importance of developmentally appropriate design in health education tools and sets the foundation for further testing and refinement for use in home and clinical settings.

**Abstract:** Children with congenital heart disease (CHD) often face challenges that require them to understand complex medical information from an early age in order to support lifelong care and improve health outcomes. However, prior research has rarely included young children in designing and evaluating digital tools to support health education using developmentally appropriate strategies. This study is part of a multi-phase research involving participatory design (PD), user testing, and iterative development. We present the design and refinement of a digital application that introduces basic information about CHD, including heart anatomy and healthy habits, through metaphor-based gameplay. User testing sessions with 30 children informed the redesign of interactive activities aligned with specific health conditions. Findings highlight usability, engagement, and comprehension outcomes and reveal design opportunities for supporting health literacy through serious game (SG) principles. These results inform the next phase, including further testing, refinement, and deployment in home and clinical settings.

</details>


### [27] [CLARE: Cognitive Load Assessment in REaltime with Multimodal Data](https://arxiv.org/abs/2404.17098)

*Anubhav Bhatti, Prithila Angkan, Behnam Behinaein, Zunayed Mahmud, Dirk Rodenburg, Heather Braund, P. James Mclellan, Aaron Ruberto, Geoffery Harrison, Daryl Wilson, Adam Szulewski, Dan Howes, Ali Etemad, Paul Hungler*

**Main category:** cs.HC

**Keywords:** Cognitive Load, Multimodal Dataset, Machine Learning, Human-Computer Interaction, Physiological Data

**Relevance Score:** 8

**TL;DR:** This paper presents the CLARE dataset, which assesses cognitive load in real-time using multimodal physiological and gaze data from 24 participants.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to develop a comprehensive dataset that enables the assessment of cognitive load using multiple modalities, contributing to the understanding of mental load in human-computer interaction tasks.

**Method:** Participants completed a complex mental workload task while physiological data (ECG, EDA, EEG) and gaze data were recorded. Cognitive load was self-reported every 10 seconds, and the dataset includes benchmark classification results from various machine learning and deep learning models using two evaluation schemes.

**Key Contributions:**

	1. Introduction of the CLARE dataset for cognitive load assessment
	2. Use of multiple physiological modalities for enhanced accuracy
	3. Benchmarking various machine learning models on a real-world task

**Result:** The best classification performance in 10-fold evaluation was achieved by a CNN model using ECG, EDA, and Gaze data, while the LOSO evaluation showed the best results with ECG, EDA, and EEG data.

**Limitations:** The dataset is limited to 24 participants, which may affect generalizability, and the focus on specific modalities may overlook other relevant factors influencing cognitive load.

**Conclusion:** The CLARE dataset provides valuable insights into cognitive load assessment and demonstrates effective machine learning methods for classification tasks, enhancing HCI research.

**Abstract:** We present a novel multimodal dataset for Cognitive Load Assessment in REal-time (CLARE). The dataset contains physiological and gaze data from 24 participants with self-reported cognitive load scores as ground-truth labels. The dataset consists of four modalities, namely, Electrocardiography (ECG), Electrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To map diverse levels of mental load on participants during experiments, each participant completed four nine-minutes sessions on a computer-based operator performance and mental workload task (the MATB-II software) with varying levels of complexity in one minute segments. During the experiment, participants reported their cognitive load every 10 seconds. For the dataset, we also provide benchmark binary classification results with machine learning and deep learning models on two different evaluation schemes, namely, 10-fold and leave-one-subject-out (LOSO) cross-validation. Benchmark results show that for 10-fold evaluation, the convolutional neural network (CNN) based deep learning model achieves the best classification performance with ECG, EDA, and Gaze. In contrast, for LOSO, the best performance is achieved by the deep learning model with ECG, EDA, and EEG.

</details>


### [28] [A Survey of AI Reliance](https://arxiv.org/abs/2408.03948)

*Sven Eckhardt, Niklas Kühl, Mateusz Dolata, Gerhard Schwabe*

**Main category:** cs.HC

**Keywords:** AI reliance, sociotechnical systems, research framework

**Relevance Score:** 7

**TL;DR:** The survey presents a sociotechnical perspective on AI reliance, introducing a categorization framework to advance research in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how humans rely on AI systems despite their increasing importance.

**Method:** A comprehensive survey and introduction of a categorization framework for AI reliance within sociotechnical systems.

**Key Contributions:**

	1. Introduction of a novel categorization framework for AI reliance
	2. Comprehensive examination of sociotechnical influences on AI reliance
	3. Discussion of emerging research avenues in the field.

**Result:** Identification of core influences on AI reliance and a morphological box to guide research efforts.

**Limitations:** Limited exploration of real-world applications and case studies on AI reliance.

**Conclusion:** Emerging research avenues are discussed to advance the agenda in AI reliance studies.

**Abstract:** Although artificial intelligence (AI) systems are becoming increasingly indispensable, research into how humans rely on these systems (AI reliance) is lagging behind. To advance this research, this survey presents a novel, comprehensive sociotechnical perspective on AI reliance, essential to fully understand the phenomenon. To address these challenges, the survey introduces a categorization framework resulting in a morphological box, which guides rigorous AI reliance research. Further, the survey identifies the core influences on AI reliance within the components of a sociotechnical system and discusses current limitations alongside emerging future research avenues to form a research agenda.

</details>


### [29] [GLITTER: An AI-assisted Platform for Material-Grounded Asynchronous Discussion in Flipped Learning](https://arxiv.org/abs/2504.14695)

*Weirui Peng, Yinuo Yang, Zheng Zhang, Toby Jia-Jun Li*

**Main category:** cs.HC

**Keywords:** AI-assisted learning, flipped classrooms, asynchronous discussions, student engagement, metacognition

**Relevance Score:** 6

**TL;DR:** GLITTER is an AI-assisted discussion platform that enhances pre-class learning in flipped classrooms by improving engagement and reflection among students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address cognitive challenges faced by students in asynchronous online discussions in flipped classrooms, such as navigation barriers and anxiety.

**Method:** A lab study with 12 subjects was conducted to evaluate GLITTER's effectiveness in supporting engagement and reflection.

**Key Contributions:**

	1. Introduction of GLITTER, an AI-assisted platform for flipped classrooms
	2. Identification of cognitive challenges in asynchronous discussions
	3. Demonstrated improvement in student engagement and preparation for class.

**Result:** Participants using GLITTER showed improved discussion engagement, generated new ideas, and felt more prepared for in-class activities.

**Limitations:** Study is limited to a small sample size (n = 12), which may not generalize to larger populations.

**Conclusion:** GLITTER effectively facilitates knowledge integration and enhances metacognition, making it a valuable tool for flipped classrooms.

**Abstract:** Flipped classrooms promote active learning by having students engage with materials independently before class, allowing in-class time for collaborative problem-solving. During this pre-class phase, asynchronous online discussions help students build knowledge and clarify concepts with peers. However, it remains difficult to engage with temporally dispersed peer contributions, connect discussions with static learning materials, and prepare for in-class sessions based on their self-learning outcome. Our formative study identified cognitive challenges students encounter, including navigation barriers, reflection gaps, and contribution difficulty and anxiety. We present GLITTER, an AI-assisted discussion platform for pre-class learning in flipped classrooms. GLITTER helps students identify posts with shared conceptual dimensions, scaffold knowledge integration through conceptual blending, and enhance metacognition via personalized reflection reports. A lab study within subjects (n = 12) demonstrates that GLITTER improves discussion engagement, sparks new ideas, supports reflection, and increases preparedness for in-class activities.

</details>


### [30] [Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces](https://arxiv.org/abs/2509.01051)

*Matte Lim, Catherine Yeh, Martin Wattenberg, Fernanda Viégas, Panagiotis Michalatos*

**Main category:** cs.HC

**Keywords:** visualization, temporal data, dimensionality reduction, embedding, streaming clustering

**Relevance Score:** 6

**TL;DR:** Chronotome is a visualization technique designed to explore evolving themes in real-time through a spatial-temporal map of embeddings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing dimensionality reduction methods fail to capture meaningful semantic changes in real-world datasets over time.

**Method:** The proposed technique integrates force-based projection with streaming clustering to create an interactive exploration tool.

**Key Contributions:**

	1. Development of Chronotome for real-time exploration of temporal data
	2. Integration of force-based projection with streaming clustering
	3. Novel approach for visualizing evolving themes in multimodal datasets

**Result:** Chronotome successfully visualizes temporal datasets, allowing for an interactive understanding of evolving themes in both text and image data.

**Limitations:** 

**Conclusion:** Chronotome provides a novel perspective on analyzing the aesthetics and semantics of time-based data, enhancing interpretability.

**Abstract:** Many real-world datasets -- from an artist's body of work to a person's social media history -- exhibit meaningful semantic changes over time that are difficult to capture with existing dimensionality reduction methods. To address this gap, we introduce a visualization technique that combines force-based projection and streaming clustering methods to build a spatial-temporal map of embeddings. Applying this technique, we create Chronotome, a tool for interactively exploring evolving themes in time-based data -- in real time. We demonstrate the utility of our approach through use cases on text and image data, showing how it offers a new lens for understanding the aesthetics and semantics of temporal datasets.

</details>


### [31] [E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI](https://arxiv.org/abs/2509.02100)

*Sharjeel Tahir, Judith Johnson, Jumana Abu-Khalaf, Syed Afaq Ali Shah*

**Main category:** cs.HC

**Keywords:** Empathy, Verbal-Visual Incongruence, AI, Therapeutic Engagement, Multimodal Dataset

**Relevance Score:** 9

**TL;DR:** The paper introduces E-THER, a dataset for training empathic AI systems to recognize verbal-visual incongruence in therapeutic contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current empathic AI systems fail to recognize when verbal expressions do not reflect true emotional states due to limitations in existing datasets that ignore verbal-visual mismatches.

**Method:** The authors present E-THER, a multimodal dataset with annotations for verbal-visual incongruence detection, based on client-counsellor interactions, enabling improved training of vision-language models for empathic interactions.

**Key Contributions:**

	1. Introduction of the E-THER dataset for verbal-visual incongruence detection
	2. Demonstration of improved performance in empathic AI using state-of-the-art vision-language models
	3. Framework for training AI systems on empathy tasks based on humanistic approaches.

**Result:** Empirical findings show that incongruence-trained models like IDEFICS and VideoLLAVA significantly improve empathic and therapeutic conversational qualities compared to general-purpose models.

**Limitations:** 

**Conclusion:** The study demonstrates that models trained with the E-THER dataset exhibit enhanced therapeutic engagement and fidelity to Person-Centered Therapy principles.

**Abstract:** A prevalent shortfall among current empathic AI systems is their inability to recognize when verbal expressions may not fully reflect underlying emotional states. This is because the existing datasets, used for the training of these systems, focus on surface-level emotion recognition without addressing the complex verbal-visual incongruence (mismatch) patterns useful for empathic understanding. In this paper, we present E-THER, the first Person-Centered Therapy-grounded multimodal dataset with multidimensional annotations for verbal-visual incongruence detection, enabling training of AI systems that develop genuine rather than performative empathic capabilities. The annotations included in the dataset are drawn from humanistic approach, i.e., identifying verbal-visual emotional misalignment in client-counsellor interactions - forming a framework for training and evaluating AI on empathy tasks. Additional engagement scores provide behavioral annotations for research applications. Notable gains in empathic and therapeutic conversational qualities are observed in state-of-the-art vision-language models (VLMs), such as IDEFICS and VideoLLAVA, using evaluation metrics grounded in empathic and therapeutic principles. Empirical findings indicate that our incongruence-trained models outperform general-purpose models in critical traits, such as sustaining therapeutic engagement, minimizing artificial or exaggerated linguistic patterns, and maintaining fidelity to PCT theoretical framework.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [32] [MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation](https://arxiv.org/abs/2509.00030)

*Marshall Thomas, Edward Fish, Richard Bowden*

**Main category:** cs.CL

**Keywords:** Sign Language Translation, Large Language Models, Machine Learning, Human-Computer Interaction

**Relevance Score:** 3

**TL;DR:** The paper introduces MultiStream-LLM, a modular framework for Sign Language Translation (SLT) that improves recognition of fingerspelling and integrates non-manual cues through specialized predictors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of monolithic end-to-end models in recognizing high-speed fingerspelling and integrating facial cues in Sign Language Translation.

**Method:** The framework uses separate expert networks for continuous signing, fingerspelling, and lipreading, with a lightweight transformer to fuse the outputs before passing to a Large Language Model for final sentence generation.

**Key Contributions:**

	1. Introduction of MultiStream-LLM framework for SLT
	2. Use of modular predictors for specialized tasks
	3. Improved performance metrics on established benchmarks

**Result:** Achieved state-of-the-art results on the How2Sign benchmark with a BLEU-4 score of 23.5 and 73.2% letter accuracy on the ChicagoFSWildPlus dataset.

**Limitations:** 

**Conclusion:** Isolating and solving distinct recognition tasks before fusion enhances performance in sign language translation.

**Abstract:** Despite progress in gloss-free Sign Language Translation (SLT), monolithic end-to-end models consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in Automated Sign Language Translation with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names,places, and technical terms. We introduce MultiStream-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign benchmark with a BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging ChicagoFSWildPlus fingerspelling dataset. These results validate our core hypothesis: by isolating and solving distinct recogni tion tasks before fusion, our multi-expert approach provides a more powerful and effective pathway to robust, high-fidelity sign language translation.

</details>


### [33] [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)

*Teo Susnjak*

**Main category:** cs.CL

**Keywords:** large language models, systematic literature reviews, prompt optimization, evidence synthesis, transparency

**Relevance Score:** 9

**TL;DR:** This paper proposes a structured framework for automating systematic literature reviews (SLRs) using large language models (LLMs), addressing issues of reliability and reproducibility in current methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability and reproducibility of LLM-assisted evidence synthesis in systematic literature reviews (SLRs), which is currently undermined by brittle prompt designs.

**Method:** The research adapts techniques from declarative prompt optimization to create a structured, domain-specific framework for SLRs that includes task declarations, test suites, and automated prompt tuning.

**Key Contributions:**

	1. Introduction of a domain-specific framework for SLR automation using LLMs.
	2. Embedding task declarations and test suites into the literature review process.
	3. Provision of working code examples that facilitate the construction of verifiable LLM pipelines.

**Result:** The proposed framework provides a concrete blueprint with code examples, allowing researchers to create verifiable LLM pipelines that adhere to principles of transparency and rigour in evidence synthesis.

**Limitations:** 

**Conclusion:** This work demonstrates a novel application of prompt optimization to SLR workflows, offering a more robust approach for leveraging LLMs in academic research.

**Abstract:** Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.

</details>


### [34] [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)

*Jian Wu, Sarah Rajtmajer*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Hypothesis, Natural Language Understanding, Machine Learning, Research Methodology

**Relevance Score:** 5

**TL;DR:** This paper reviews and clarifies various definitions of 'hypothesis' in natural language understanding (NLU) tasks, highlighting the need for well-defined hypotheses for machine interpretation.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** There has been increasing focus on training models for hypothesis extraction and understanding in the context of NLU, yet inconsistencies exist in defining what constitutes a hypothesis across different domains and tasks.

**Method:** The authors conducted a literature review to identify and analyze definitions of 'hypothesis' across various NLU applications, documenting the differences and nuances.

**Key Contributions:**

	1. Overview of various definitions of hypothesis in NLU tasks
	2. Identification of inconsistencies in hypothesis definitions
	3. Emphasis on the importance of clear hypothesis structuring for machine learning applications

**Result:** The paper identifies various interpretations of hypotheses, emphasizing the need for clarity and structure in these definitions to enhance the machine interpretability of scholarly records.

**Limitations:** 

**Conclusion:** A well-structured definition of hypotheses is crucial for advancing NLU and ensuring meaningful machine understanding of scholarly work.

**Abstract:** Over the past decades, alongside advancements in natural language processing, significant attention has been paid to training models to automatically extract, understand, test, and generate hypotheses in open and scientific domains. However, interpretations of the term \emph{hypothesis} for various natural language understanding (NLU) tasks have migrated from traditional definitions in the natural, social, and formal sciences. Even within NLU, we observe differences defining hypotheses across literature. In this paper, we overview and delineate various definitions of hypothesis. Especially, we discern the nuances of definitions across recently published NLU tasks. We highlight the importance of well-structured and well-defined hypotheses, particularly as we move toward a machine-interpretable scholarly record.

</details>


### [35] [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)

*Sheldon Yu, Yuxin Xiong, Junda Wu, Xintong Li, Tong Yu, Xiang Chen, Ritwik Sinha, Jingbo Shang, Julian McAuley*

**Main category:** cs.CL

**Keywords:** chain-of-thought prompting, large language models, explainability, Markov chain, latent dynamics

**Relevance Score:** 8

**TL;DR:** This paper introduces a state-aware transition framework for improving the explainability of chain-of-thought prompting in large language models by modeling reasoning steps as structured latent dynamics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the explainability of multi-step reasoning in large language models, which is currently limited due to a lack of focus on high-level semantic roles of reasoning steps.

**Method:** The authors develop a state-aware transition framework that uses spectral analysis of token-level embeddings to cluster reasoning steps into semantically coherent latent states and model their progression using a Markov chain.

**Key Contributions:**

	1. Introduction of the state-aware transition framework for CoT prompting.
	2. Modeling reasoning steps as a Markov chain for improved interpretability.
	3. Enabling various analyses such as semantic role identification and temporal visualization.

**Result:** The proposed framework provides a structured and interpretable view of the reasoning process that enables semantic role identification, temporal pattern visualization, and consistency evaluation.

**Limitations:** The approach may depend on the quality of token-level embeddings and the abstraction method may overlook some nuances of reasoning context.

**Conclusion:** By abstracting CoT trajectories into structured latent dynamics, the framework contributes to better understanding and analyzing the reasoning processes of large language models.

**Abstract:** Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.

</details>


### [36] [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)

*Seiji Maekawa, Hayate Iso, Nikita Bhutani*

**Main category:** cs.CL

**Keywords:** Distinctive Feature Mining, LLM, Benchmark, Statistical Reasoning, Rarity Detection

**Relevance Score:** 7

**TL;DR:** This paper introduces Distinctive Feature Mining (DFM), a task for LLMs focused on identifying rare features across document collections, and presents DiFBench, a benchmark for evaluating this capability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve decision-making processes by enabling models to identify globally distinctive features rather than just retrieving relevant information.

**Method:** The authors propose the Distinctive Feature Mining (DFM) task and the DiFBench framework to evaluate LLMs on their ability to analyze collections of documents and surface statistically rare features.

**Key Contributions:**

	1. Introduction of the DFM task for LLMs.
	2. Development of DiFBench, a benchmark for evaluating distinctive feature mining.
	3. Insights into the performance of LLMs on complex reasoning tasks.

**Result:** A large-scale assessment of ten state-of-the-art LLMs reveals performance gaps between general-purpose and reasoning-enhanced models, highlighting that all models struggle as task complexity increases.

**Limitations:** Most models degrade significantly as task complexity and document count increase, indicating scalability issues.

**Conclusion:** Contemporary LLMs show limitations in statistical reasoning and rarity detection, often misclassifying frequent features as distinctive, indicating a need for further improvement.

**Abstract:** Effective decision-making often relies on identifying what makes each candidate distinctive. While existing benchmarks for LLMs emphasize retrieving or summarizing information relevant to a given query, they do not evaluate a model's ability to identify globally distinctive features across a set of documents. We introduce Distinctive Feature Mining (DFM), a new task that challenges models to analyze a small-to-medium collection (10-40 documents) and surface features that are rare in the global context (e.g., appearing in less than 10% of documents). This setting mirrors real-world scenarios such as candidate selection or product differentiation, where statistical reasoning, not retrieval, is key. To enable systematic evaluation of this capability, we present DiFBench, a configurable benchmark creation framework with controllable parameters such as document set size and distinctiveness thresholds. Using DiFBench, we perform a large-scale assessment of distinctive feature mining across ten state-of-the-art LLMs. Our findings reveal a significant performance gap between general-purpose and reasoning-enhanced models. All models, however, substantially degrade as the task complexity and document count increase. We also find that a common failure mode is misidentifying frequent features as distinctive. These insights reveal core limitations in contemporary LLMs' abilities to perform fine-grained, statistical reasoning and rarity detection.

</details>


### [37] [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)

*Zachary K. Stine, James E. Deitrick*

**Main category:** cs.CL

**Keywords:** semiotics, model semantics, C. S. Peirce, meaning-making, semiotic systems

**Relevance Score:** 3

**TL;DR:** The paper proposes a general theoretical framework for modeling human meaning-making grounded in Peirce's semiotic theory, enabling relational understanding of models as signs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of a general theoretical framework for accurately describing the diverse modeling practices in the analysis of complex semiotic systems.

**Method:** The framework is grounded in C. S. Peirce's semiotic theory and focuses on understanding models relationally, particularly when traditional performance measures are insufficient.

**Key Contributions:**

	1. Proposing a general framework for modeling human meaning-making.
	2. Introducing the concept of models as latent symbol geometries.
	3. Highlighting models' interpretive lenses through relational comparisons.

**Result:** The proposed framework allows for the examination of models as hypotheses about semiotic agencies, enabling insights into their interpretive lenses.

**Limitations:** 

**Conclusion:** This framework can enhance the understanding of modeling practices and guide future research directions in the field of semiotic analysis.

**Abstract:** The proliferation of methods for modeling of human meaning-making constitutes a powerful class of instruments for the analysis of complex semiotic systems. However, the field lacks a general theoretical framework for describing these modeling practices across various model types in an apples-to-apples way. In this paper, we propose such a framework grounded in the semiotic theory of C. S. Peirce. We argue that such models measure latent symbol geometries, which can be understood as hypotheses about the complex of semiotic agencies underlying a symbolic dataset. Further, we argue that in contexts where a model's value cannot be straightforwardly captured by proxy measures of performance, models can instead be understood relationally, so that the particular interpretive lens of a model becomes visible through its contrast with other models. This forms the basis of a theory of model semantics in which models, and the modeling decisions that constitute them, are themselves treated as signs. In addition to proposing the framework, we illustrate its empirical use with a few brief examples and consider foundational questions and future directions enabled by the framework.

</details>


### [38] [The Temporal Game: A New Perspective on Temporal Relation Extraction](https://arxiv.org/abs/2509.00250)

*Hugo Sousa, Ricardo Campos, Alípio Jorge*

**Main category:** cs.CL

**Keywords:** temporal relation extraction, interactive annotation, reinforcement learning

**Relevance Score:** 6

**TL;DR:** A novel approach called Temporal Game for temporal relation extraction using interactive gaming mechanics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the process of temporal relation annotation by making it more interactive and by allowing more fine-grained and flexible annotation.

**Method:** The Temporal Game decomposes interval-level temporal relations into point-wise comparisons between start and end points of entities, utilizing a game format to enhance user engagement and feedback.

**Key Contributions:**

	1. Introduction of an interactive game format for temporal relation extraction
	2. Supports fine-grained annotation through point-wise comparisons
	3. Publicly available demo and open-sourced code for community use

**Result:** The system supports both interval and instant entities and enables reinforcement learning agents to be trained by treating temporal annotation as a sequential decision-making process.

**Limitations:** 

**Conclusion:** The Temporal Game serves as both a research tool for temporal reasoning and an annotation interface for users, fostering community-driven development.

**Abstract:** In this paper we demo the Temporal Game, a novel approach to temporal relation extraction that casts the task as an interactive game. Instead of directly annotating interval-level relations, our approach decomposes them into point-wise comparisons between the start and end points of temporal entities. At each step, players classify a single point relation, and the system applies temporal closure to infer additional relations and enforce consistency. This point-based strategy naturally supports both interval and instant entities, enabling more fine-grained and flexible annotation than any previous approach. The Temporal Game also lays the groundwork for training reinforcement learning agents, by treating temporal annotation as a sequential decision-making task. To showcase this potential, the demo presented in this paper includes a Game mode, in which users annotate texts from the TempEval-3 dataset and receive feedback based on a scoring system, and an Annotation mode, that allows custom documents to be annotated and resulting timeline to be exported. Therefore, this demo serves both as a research tool and an annotation interface. The demo is publicly available at https://temporal-game.inesctec.pt, and the source code is open-sourced to foster further research and community-driven development in temporal reasoning and annotation.

</details>


### [39] [Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval](https://arxiv.org/abs/2509.00276)

*Yuxiang Liu, Tian Wang, Gourab Kundu, Tianyu Cao, Guang Cheng, Zhen Ge, Jianshu Chen, Qingjun Cui, Trishul Chilimbi*

**Main category:** cs.CL

**Keywords:** Reasoning, Text Embedding, Large Language Models, Document Retrieval, NLP

**Relevance Score:** 9

**TL;DR:** This paper introduces Reasoning-Infused Text Embedding (RITE), which incorporates logical reasoning into text embedding using generative LLMs, enhancing retrieval performance on reasoning-intensive tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing transformer-based models in handling complex queries that require reasoning, aiming to improve document retrieval effectiveness.

**Method:** RITE integrates logical reasoning into the text embedding process by generating intermediate reasoning texts in the token space before computing embeddings.

**Key Contributions:**

	1. Development of RITE that combines reasoning with text embeddings.
	2. Demonstration of significant improvements in retrieval performance on reasoning-intensive tasks.
	3. Introduction of a method for generating intermediate reasoning texts in the token space.

**Result:** Experimental results show that RITE significantly enhances zero-shot retrieval performance on the BRIGHT benchmark, performing better across diverse domains compared to existing methods.

**Limitations:** 

**Conclusion:** Incorporating reasoning into the text embedding process effectively addresses challenges in retrieving relevant documents for complex queries.

**Abstract:** Transformer-based models such as BERT and E5 have significantly advanced text embedding by capturing rich contextual representations. However, many complex real-world queries require sophisticated reasoning to retrieve relevant documents beyond surface-level lexical matching, where encoder-only retrievers often fall short. Decoder-only large language models (LLMs), known for their strong reasoning capabilities, offer a promising alternative. Despite this potential, existing LLM-based embedding methods primarily focus on contextual representation and do not fully exploit the reasoning strength of LLMs. To bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple but effective approach that integrates logical reasoning into the text embedding process using generative LLMs. RITE builds upon existing language model embedding techniques by generating intermediate reasoning texts in the token space before computing embeddings, thereby enriching representations with inferential depth. Experimental results on BRIGHT, a reasoning-intensive retrieval benchmark, demonstrate that RITE significantly enhances zero-shot retrieval performance across diverse domains, underscoring the effectiveness of incorporating reasoning into the embedding process.

</details>


### [40] [OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews](https://arxiv.org/abs/2509.00285)

*Mir Tafseer Nayeem, Davood Rafiei*

**Main category:** cs.CL

**Keywords:** opinion highlights, user reviews, RAG, large language models, sentiment analysis

**Relevance Score:** 9

**TL;DR:** OpinioRAG is introduced as a scalable framework for generating personalized opinion highlights from large volumes of user reviews, incorporating LLMs and RAG-based retrieval methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing methods in generating personalized summaries from large user reviews, which often fail to scale or provide generic outputs.

**Method:** The OpinioRAG framework combines retrieval-augmented generation (RAG) with large language models (LLMs) for efficient and tailored summaries. It also proposes new reference-free verification metrics suitable for sentiment-rich contexts.

**Key Contributions:**

	1. Introduction of a training-free, scalable framework for opinion highlights generation.
	2. Development of novel reference-free verification metrics for sentiment-rich domains.
	3. Creation of a large-scale dataset of long-form user reviews and unbiased expert summaries.

**Result:** Extensive experiments revealed key challenges in opinion highlights generation and provided actionable insights for enhancing these systems, demonstrating the efficacy of OpinioRAG.

**Limitations:** 

**Conclusion:** OpinioRAG is positioned as a robust framework capable of generating accurate and structured summaries from large datasets of user reviews, addressing both scalability and personalization needs.

**Abstract:** We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.

</details>


### [41] [Wage Sentiment Indices Derived from Survey Comments via Large Language Models](https://arxiv.org/abs/2509.00290)

*Taihei Sone*

**Main category:** cs.CL

**Keywords:** Wage Sentiment Index, Large Language Models, Economic Policy, Forecasting, Economy Watchers Survey

**Relevance Score:** 7

**TL;DR:** The paper introduces the Wage Sentiment Index (WSI) using Large Language Models to forecast wage dynamics in Japan, proving its efficacy over traditional models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage generative AI for economic text analysis, specifically in forecasting wage dynamics influenced by real-time economic sentiments.

**Method:** The study develops a Wage Sentiment Index (WSI) based on the Economy Watchers Survey, integrating LLMs and a scalable data architecture to include diverse data sources.

**Key Contributions:**

	1. Introduction of the Wage Sentiment Index (WSI) for economic forecasting.
	2. Development of a scalable data architecture for integrating various data sources.
	3. Demonstration of the superior performance of WSI models compared to traditional methods.

**Result:** WSI models driven by LLMs significantly outperform baseline and pretrained models in forecasting wage dynamics, demonstrating their potential for economic policy design.

**Limitations:** 

**Conclusion:** LLM-driven sentiment indices can enhance the timely and effective formulation of economic policies by governments and central banks.

**Abstract:** The emergence of generative Artificial Intelligence (AI) has created new opportunities for economic text analysis. This study proposes a Wage Sentiment Index (WSI) constructed with Large Language Models (LLMs) to forecast wage dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS), a monthly survey conducted by the Cabinet Office of Japan that captures real-time economic assessments from workers in industries highly sensitive to business conditions. The WSI extends the framework of the Price Sentiment Index (PSI) used in prior studies, adapting it specifically to wage related sentiment. To ensure scalability and adaptability, a data architecture is also developed that enables integration of additional sources such as newspapers and social media. Experimental results demonstrate that WSI models based on LLMs significantly outperform both baseline approaches and pretrained models. These findings highlight the potential of LLM-driven sentiment indices to enhance the timeliness and effectiveness of economic policy design by governments and central banks.

</details>


### [42] [Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models](https://arxiv.org/abs/2509.00309)

*Chen Zheng, Yiyuan Ma, Yuan Yang, Deyi Liu, Jing Liu, Zuquan Song, Yuxin Song, Cheng Ren, Hang Zhu, Xin Liu, Yiyuan Ma, Siyuan Qiao, Xun Zhou, Liang Xiang, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** large language models, alignment, reasoning, RLHF, distillation

**Relevance Score:** 8

**TL;DR:** This paper investigates the challenges in training large language models using a third paradigm that combines RLHF and distillation-focused approaches, revealing critical instabilities and proposing a solution called Balanced Actor Initialization (BAI).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for enhanced alignment and reasoning capabilities in large language models has led to exploring effective training paradigms, particularly combining RLHF and distillation methods.

**Method:** The authors propose a two-stage weighted model merging approach called Balanced Actor Initialization (BAI), which merges instruction-following models with distillation-trained models to stabilize training.

**Key Contributions:**

	1. Introduction of Balanced Actor Initialization (BAI) for training language models.
	2. Identification of critical phenomena like Sequence Length Collapse and Reward Hockey Stick Curve in RLHF-distillation training.
	3. Demonstration of improved model performance through balanced merging ratios.

**Result:** BAI resolves issues such as Sequence Length Collapse and the Reward Hockey Stick Curve, enhancing training stability and improving reasoning capabilities.

**Limitations:** 

**Conclusion:** By implementing BAI, the study demonstrates improved stability and reasoning in language models, suggesting a viable method for effective training in the combined RLHF and distillation paradigm.

**Abstract:** The development of alignment and reasoning capabilities in large language models has seen remarkable progress through two paradigms: instruction tuning and reinforcement learning from human feedback (RLHF) alignment paradigm, and distillation-based reasoning fine-tuning paradigm. While both approaches prove effective independently, the third paradigm of applying RLHF to distillation-trained models presents significant challenges. Our investigation reveals two critical phenomena that emerge in this paradigm: Sequence Length Collapse, where language generation dramatically reduces during early RLHF training, and the Reward Hockey Stick Curve, featuring severe reward score drops followed by gradual recovery. These instabilities fundamentally compromise the model's alignment and reasoning capabilities. To address these challenges, we propose Balanced Actor Initialization (BAI), a two-stage weighted model merging approach. BAI first merges instruction-following and distillation-based reasoning fine-tuned models, then further combines this intermediate model with the pretrained model to preserve foundational knowledge. Through comprehensive experiments across diverse benchmarks and detailed analysis of training experiments, we demonstrate that BAI resolves Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables continuous sequence length improvement during training. Additionally, our analysis reveals that balanced merging ratios achieve optimal trade-offs between training stability and reasoning capability preservation. Our work provides the effective solution for stable training in this third paradigm, enabling more capable reasoning models that combine distillation efficiency with RLHF alignment.

</details>


### [43] [GIER: Gap-Driven Self-Refinement for Large Language Models](https://arxiv.org/abs/2509.00325)

*Rinku Dewri*

**Main category:** cs.CL

**Keywords:** large language models, self-reflection, response enhancement, reasoning gaps, iterative refinement

**Relevance Score:** 9

**TL;DR:** GIER is a framework for enhancing LLM outputs through self-reflection and iterative revision based on conceptual quality criteria, showing improvements across various reasoning tasks and models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind GIER is to improve the quality of responses generated by large language models (LLMs) by enabling them to self-reflect and revise outputs according to specific reasoning criteria.

**Method:** GIER utilizes natural language descriptions of reasoning gaps, prompting the model to critique and refine its own outputs iteratively.

**Key Contributions:**

	1. Introduction of a new framework (GIER) for LLM output enhancement.
	2. Demonstrated improvements in rationale quality and reasoning alignment across multiple LLMs.
	3. Provided insights into the self-reflective capabilities of LLMs based on conceptual quality criteria.

**Result:** GIER improves rationale quality, grounding, and reasoning alignment across tasks without degrading task accuracy, as demonstrated on SciFact, PrivacyQA, and e-SNLI using four different LLMs.

**Limitations:** 

**Conclusion:** The framework successfully allows models to interpret abstract reasoning gaps and translate them into concrete improvements in reasoning quality.

**Abstract:** We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general framework for improving large language model (LLM) outputs through self-reflection and revision based on conceptual quality criteria. Unlike prompting strategies that rely on demonstrations, examples, or chain-of-thought templates, GIER utilizes natural language descriptions of reasoning gaps, and prompts a model to iteratively critique and refine its own outputs to better satisfy these criteria. Across three reasoning-intensive tasks (SciFact, PrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and Llama 3.3 70B), GIER improves rationale quality, grounding, and reasoning alignment without degrading task accuracy. Our analysis demonstrates that models can not only interpret abstract conceptual gaps but also translate them into concrete reasoning improvements.

</details>


### [44] [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375)

*Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hierarchical Constraint Satisfaction Problems, Deep Research Tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces InfoSeek, a scalable framework for synthesizing complex Deep Research tasks, which formalizes these tasks as Hierarchical Constraint Satisfaction Problems (HCSPs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks fail to capture the complexity of Deep Research tasks that LLMs are expected to perform, leading to inadequate training and evaluation.

**Method:** InfoSeek employs a dual-agent system to recursively build a Research Tree from large-scale webpages, transforming complex tasks into natural language questions requiring multi-step reasoning.

**Key Contributions:**

	1. Introduction of Hierarchical Constraint Satisfaction Problems (HCSPs) for Deep Research tasks
	2. Development of a dual-agent system for synthesizing complex research questions
	3. Provision of a large dataset and framework enabling advanced optimization strategies

**Result:** Experiments show that models trained on InfoSeek outperform strong baselines, achieving better results on BrowseComp-Plus than larger models and commercial APIs.

**Limitations:** 

**Conclusion:** InfoSeek demonstrates the potential to improve LLM performance in Deep Research tasks by preserving meta-information and supporting advanced optimization strategies.

**Abstract:** Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.

</details>


### [45] [GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction](https://arxiv.org/abs/2509.00388)

*Xuelin Li, Xiangqi Jin, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** Key-Value Cache, Large Language Models, Graph-Based Framework

**Relevance Score:** 9

**TL;DR:** GraphKV introduces a graph-based framework for efficient key-value cache management in large language models, improving token selection by dynamically updating importance scores based on similarity relationships.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the efficiency of KV cache management in large language models, addressing limitations of conventional eviction strategies that rely on static heuristics.

**Method:** GraphKV models tokens as nodes within a graph where edges signify similarity relationships. It employs a decay-signal-propagation mechanism to dynamically update token importance scores during inference.

**Key Contributions:**

	1. Introduction of a graph-based framework for KV cache management
	2. Dynamic updating of token importance scores through signal propagation
	3. Compatibility with existing KV eviction methods

**Result:** GraphKV enhances the adaptive retention of contextually significant tokens in the KV cache, leading to better performance in processing long text sequences.

**Limitations:** 

**Conclusion:** GraphKV can be integrated into existing KV eviction methods like SnapKV and PyramidKV, offering a more efficient and adaptive approach to KV cache management.

**Abstract:** Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes will be released on Github.

</details>


### [46] [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391)

*Yuting Tan, Xuying Li, Zhuo Li, Huizhen Shu, Peikang Hu*

**Main category:** cs.CL

**Keywords:** adversarial prompting, large language models, Greedy Coordinate Gradient, safety-oriented prompts, reasoning tasks

**Relevance Score:** 8

**TL;DR:** This paper evaluates the Greedy Coordinate Gradient (GCG) algorithm and its variant T-GCG in successfully attacking large language models (LLMs).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically assess the effectiveness of gradient-based adversarial prompting methods on LLMs and identify vulnerabilities in their responses to varying types of prompts.

**Method:** The study involves testing GCG and T-GCG algorithms on open-source models (Qwen2.5-0.5B, LLaMA-3.2-1B, GPT-OSS-20B) using safety-oriented and reasoning-intensive prompts, with an analysis of attack success rates (ASR).

**Key Contributions:**

	1. Systematic appraisal of GCG and T-GCG on various LLMs
	2. Discovery of declining attack success rates with model size
	3. Identification of coding-related prompts as more vulnerable than safety prompts

**Result:** Findings indicate that ASR decreases with model size, prefix-based heuristics overestimate effectiveness, and coding prompts are more exploitable than safety prompts. Preliminary T-GCG results show some benefits from simulated annealing but limited under strict evaluation measures.

**Limitations:** Preliminary results indicate T-GCG's benefits may be limited under semantic judgment evaluations.

**Conclusion:** The research underscores limitations in GCG scalability, reveals vulnerabilities in reasoning tasks, and suggests the need for improving adversarial evaluation methods via annealing techniques.

**Abstract:** Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient (GCG) algorithm, has emerged as a powerful method for jailbreaking large language models (LLMs). In this paper, we present a systematic appraisal of GCG and its annealing-augmented variant, T-GCG, across open-source LLMs of varying scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack effectiveness on both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Our study reveals three key findings: (1) attack success rates (ASR) decrease with model size, reflecting the increasing complexity and non-convexity of larger models' loss landscapes; (2) prefix-based heuristics substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide a stricter and more realistic evaluation; and (3) coding-related prompts are significantly more vulnerable than adversarial safety prompts, suggesting that reasoning itself can be exploited as an attack vector. In addition, preliminary results with T-GCG show that simulated annealing can diversify adversarial search and achieve competitive ASR under prefix evaluation, though its benefits under semantic judgment remain limited. Together, these findings highlight the scalability limits of GCG, expose overlooked vulnerabilities in reasoning tasks, and motivate further development of annealing-inspired strategies for more robust adversarial evaluation.

</details>


### [47] [MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature](https://arxiv.org/abs/2509.00414)

*Juraj Vladika, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Health Informatics, Large Language Models

**Relevance Score:** 9

**TL;DR:** MedSEBA is an AI-powered system that synthesizes evidence-based answers to medical questions using Large Language Models and PubMed data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of distinguishing reliable medical information amid vast online content and numerous medical studies published yearly.

**Method:** The system utilizes Large Language Models to generate answers based on dynamically retrieved studies from PubMed, presenting key points, arguments, and visualizing research consensus over time.

**Key Contributions:**

	1. The integration of LLMs with dynamically retrieved medical studies.
	2. The ability to visualize the evolution of research consensus.
	3. User study results showing high usability and perceived trustworthiness.

**Result:** User studies indicated that both medical experts and lay users found MedSEBA usable, helpful, and provided trustworthy answers.

**Limitations:** 

**Conclusion:** MedSEBA is an effective tool for answering everyday health questions and providing insights for advanced medical research.

**Abstract:** In the digital age, people often turn to the Internet in search of medical advice and recommendations. With the increasing volume of online content, it has become difficult to distinguish reliable sources from misleading information. Similarly, millions of medical studies are published every year, making it challenging for researchers to keep track of the latest scientific findings. These evolving studies can reach differing conclusions, which is not reflected in traditional search tools. To address these challenges, we introduce MedSEBA, an interactive AI-powered system for synthesizing evidence-based answers to medical questions. It utilizes the power of Large Language Models to generate coherent and expressive answers, but grounds them in trustworthy medical studies dynamically retrieved from the research database PubMed. The answers consist of key points and arguments, which can be traced back to respective studies. Notably, the platform also provides an overview of the extent to which the most relevant studies support or refute the given medical claim, and a visualization of how the research consensus evolved through time. Our user study revealed that medical experts and lay users find the system usable and helpful, and the provided answers trustworthy and informative. This makes the system well-suited for both everyday health questions and advanced research insights.

</details>


### [48] [The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang](https://arxiv.org/abs/2509.00425)

*Fenghua Liu, Yulong Chen, Yixuan Liu, Zhujun Jin, Solomon Tsai, Ming Zhong*

**Main category:** cs.CL

**Keywords:** Large Language Models, metalinguistic competence, language acquisition, cognitive science, language learning

**Relevance Score:** 9

**TL;DR:** The paper tests Large Language Models' reasoning abilities through a novel language learning framework using Camlang, revealing significant gaps in their metalinguistic competence compared to humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether Large Language Models can demonstrate genuine reasoning capabilities rather than mere pattern matching, particularly in the context of language acquisition.

**Method:** Camlang, a constructed language with explicit resources for grammar and vocabulary, was used to conduct human experiments and evaluate LLM performance in language learning tasks.

**Key Contributions:**

	1. Introduction of Camlang as a novel evaluation paradigm for LLM reasoning.
	2. Demonstration of significant performance differences between LLMs and humans in language learning tasks.
	3. Empirical evidence of the limitations of LLMs in mastering grammar and metalinguistic reasoning.

**Result:** GPT-5 achieved 98% EM accuracy in English but only 47% in Camlang, which is significantly lower than human performance (87%), with the model's success largely attributed to shallow lexical alignment.

**Limitations:** The study is a working progress and may not comprehensively test all aspects of metalinguistic competence in LLMs.

**Conclusion:** Current LLMs exhibit limited metalinguistic awareness and do not achieve systematic grammatical mastery, highlighting essential gaps in their reasoning abilities compared to human learners.

**Abstract:** Large Language Models (LLMs) achieve gold-medal performance across many benchmarks, yet it remains unclear whether such success reflects genuine reasoning or pattern matching. From a cognitive science perspective, an informative test is whether models can master an unfamiliar language through explicit metalinguistic deductive learning, a paradigm where human learners can reliably internalise grammatical systems through metalinguistic reasoning. We address this question with Camlang, a novel constructed language that exhibits naturalistic yet unattested feature combinations. Camlang consists of two explicit resources, a grammar book and a bilingual dictionary, which mirror adult second-language learning via explicit grammar rules and lexical lookup, and enable us to disentangle errors in morpho-syntax, lexical semantics, and sentence-level reasoning. Human experiments show that these resources are sufficient for participants to acquire Camlang and successfully solve Camlang tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang, creating Camlang-CSQA-v0, the first task in a broader suite where solving questions requires applying grammar rules and lexical mappings. Experimental results show that GPT-5 achieves 98\% EM accuracy in English but only 47\% in Camlang, far below human performance at 87\%, while other state-of-the-art reasoning LLMs perform even worse. Human verification further reveals that most model successes stem from shallow lexical alignment while GPT-5 shows emerging metalinguistic awareness to a limited extent but not systematic grammatical mastery as humans. Camlang establishes a cognitively grounded evaluation paradigm that exposes fundamental gaps between current models and human metalinguistic competence.

</details>


### [49] [GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework](https://arxiv.org/abs/2509.00449)

*Xuecheng Zou, Ke Liu, Bingbing Wang, Huafei Deng, Li Zhang, Yu Tang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Heterogeneous graphs, Semantic units

**Relevance Score:** 8

**TL;DR:** GOSU is a framework that enhances graph-based RAG by integrating heterogeneous graphs and hypergraphs, improving retrieval and generation through global disambiguation and semantic unit extraction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of standard RAG in capturing fine-grained relationships and global knowledge by leveraging semantic units and reducing ambiguity.

**Method:** GOSU constructs heterogeneous graphs and hypergraphs, performing global merging of semantic units and introducing hierarchical keyword extraction and semantic unit completion during retrieval and generation.

**Key Contributions:**

	1. Introduction of GOSU framework for RAG
	2. Global disambiguation of semantic units
	3. Hierarchical keyword extraction to refine relationships

**Result:** GOSU demonstrates improved performance over baseline RAG methods across various tasks, particularly in generation quality.

**Limitations:** 

**Conclusion:** GOSU offers a more effective approach for semantic unit extraction and relationship handling in RAG frameworks, which could significantly enhance applications in natural language processing.

**Abstract:** Building upon the standard graph-based Retrieval-Augmented Generation (RAG), the introduction of heterogeneous graphs and hypergraphs aims to enrich retrieval and generation by leveraging the relationships between multiple entities through the concept of semantic units (SUs). But this also raises a key issue: The extraction of high-level SUs limited to local text chunks is prone to ambiguity, complex coupling, and increased retrieval overhead due to the lack of global knowledge or the neglect of fine-grained relationships. To address these issues, we propose GOSU, a semantic unit-centric RAG framework that efficiently performs global disambiguation and utilizes SUs to capture interconnections between different nodes across the global context. In the graph construction phase, GOSU performs global merging on the pre-extracted SUs from local text chunks and guides entity and relationship extraction, reducing the difficulty of coreference resolution while uncovering global semantic objects across text chunks. In the retrieval and generation phase, we introduce hierarchical keyword extraction and semantic unit completion. The former uncovers the fine-grained binary relationships overlooked by the latter, while the latter compensates for the coarse-grained n-ary relationships missing from the former. Evaluation across multiple tasks demonstrates that GOSU outperforms the baseline RAG methods in terms of generation quality.

</details>


### [50] [CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2509.00457)

*Salah Eddine Bekhouche, Abdellah Zakaria Sellam, Hichem Telli, Cosimo Distante, Abdenour Hadid*

**Main category:** cs.CL

**Keywords:** Islamic inheritance law, Arabic text encoding, Attentive Relevance Scoring, on-device inference, machine learning

**Relevance Score:** 4

**TL;DR:** This paper presents a lightweight framework for solving Islamic inheritance law questions using a specialized Arabic text encoder, emphasizing efficiency and on-device inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for precise identification of heirs and calculation of shares in Islamic inheritance law presents challenges for AI applications.

**Method:** The framework utilizes a specialized Arabic text encoder and Attentive Relevance Scoring (ARS) to rank answer options based on semantic relevance, allowing for fast, on-device inference.

**Key Contributions:**

	1. Lightweight framework for solving Islamic inheritance questions
	2. Use of specialized Arabic text encoder for efficient processing
	3. Evaluation of various Arabic encoders compared to large LLMs

**Result:** The MARBERT-based system achieves 69.87% accuracy, demonstrating efficiency and privacy advantages over larger API-based models that reach up to 87.6% accuracy but require more resources.

**Limitations:** 

**Conclusion:** While the MARBERT approach has lower peak performance than the best LLMs, it highlights the trade-off between accuracy and practical deployment in sensitive areas like inheritance law.

**Abstract:** Islamic inheritance law (Ilm al-Mawarith) requires precise identification of heirs and calculation of shares, which poses a challenge for AI. In this paper, we present a lightweight framework for solving multiple-choice inheritance questions using a specialised Arabic text encoder and Attentive Relevance Scoring (ARS). The system ranks answer options according to semantic relevance, and enables fast, on-device inference without generative reasoning. We evaluate Arabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based LLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an accuracy of up to 87.6%, they require more resources and are context-dependent. Our MARBERT-based approach achieves 69.87% accuracy, presenting a compelling case for efficiency, on-device deployability, and privacy. While this is lower than the 87.6% achieved by the best-performing LLM, our work quantifies a critical trade-off between the peak performance of large models and the practical advantages of smaller, specialized systems in high-stakes domains.

</details>


### [51] [TECP: Token-Entropy Conformal Prediction for LLMs](https://arxiv.org/abs/2509.00461)

*Beining Xu*

**Main category:** cs.CL

**Keywords:** uncertainty quantification, language generation, conformal prediction, token entropy, large language models

**Relevance Score:** 9

**TL;DR:** This paper presents Token-Entropy Conformal Prediction (TECP), a framework for uncertainty quantification in open-ended language generation that uses token-level entropy to measure uncertainty and ensure reliable predictions without needing model internals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective uncertainty quantification in language generation, particularly under black-box conditions where internal signals of models are not accessible.

**Method:** TECP leverages token-level entropy as a logit-free uncertainty measure and incorporates it into a split conformal prediction pipeline to construct prediction sets with formal coverage guarantees.

**Key Contributions:**

	1. Introduction of Token-Entropy Conformal Prediction (TECP) for uncertainty quantification.
	2. Direct estimation of epistemic uncertainty using token entropy.
	3. Empirical validation showing superiority over existing methods in various large language models.

**Result:** Empirical evaluations show that TECP outperforms existing self-consistency-based uncertainty quantification methods in terms of reliable coverage and compact prediction sets across multiple large language models.

**Limitations:** 

**Conclusion:** TECP offers a principled, efficient approach for achieving trustworthy language generation in settings where model internals are not available.

**Abstract:** Uncertainty quantification (UQ) for open-ended language generation remains a critical yet underexplored challenge, especially under black-box constraints where internal model signals are inaccessible. In this paper, we introduce Token-Entropy Conformal Prediction (TECP), a novel framework that leverages token-level entropy as a logit-free, reference-free uncertainty measure and integrates it into a split conformal prediction (CP) pipeline to construct prediction sets with formal coverage guarantees. Unlike existing approaches that rely on semantic consistency heuristics or white-box features, TECP directly estimates epistemic uncertainty from the token entropy structure of sampled generations and calibrates uncertainty thresholds via CP quantiles to ensure provable error control. Empirical evaluations across six large language models and two benchmarks (CoQA and TriviaQA) demonstrate that TECP consistently achieves reliable coverage and compact prediction sets, outperforming prior self-consistency-based UQ methods. Our method provides a principled and efficient solution for trustworthy generation in black-box LLM settings.

</details>


### [52] [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)

*Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul*

**Main category:** cs.CL

**Keywords:** large language models, role-playing dialogue agents, prompting strategies, Commonsense Persona-grounded Dialogue Challenge

**Relevance Score:** 9

**TL;DR:** This report explores four prompting methods to enhance tool-augmented LLMs as role-playing dialogue agents, finding that rule-based prompting significantly outperforms other techniques.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve dialogue agents' responses by preventing them from over-speaking and under-acting while utilizing tools effectively.

**Method:** Four prompting approaches were investigated: basic role prompting, human-crafted role prompting, automatic prompt optimization, and rule-based role prompting, with various strategies for performance enhancement.

**Key Contributions:**

	1. Development of the rule-based role prompting approach
	2. Novel techniques: character-card/scene-contract design and function calling enforcement
	3. Open-sourcing effective prompts and tools for broader use

**Result:** The rule-based role prompting (RRP) achieved the highest performance with a score of 0.571, surpassing the zero-shot baseline of 0.519 and demonstrating improved effectiveness in role-playing contexts.

**Limitations:** 

**Conclusion:** The findings highlight the potential of RRP design in enhancing dialogue agents' performance, and the authors are open-sourcing their prompts and tools for community use.

**Abstract:** This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.

</details>


### [53] [ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics](https://arxiv.org/abs/2509.00496)

*Li S. Yifei, Allen Chang, Chaitanya Malaviya, Mark Yatskar*

**Main category:** cs.CL

**Keywords:** LLM evaluation, survey articles, research queries, rubric development, competency analysis

**Relevance Score:** 9

**TL;DR:** ResearchQA is introduced as a benchmark for evaluating LLM systems using queries and rubrics derived from survey articles across various fields.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing evaluation methods that depend heavily on expert annotators, limiting their application across diverse research areas.

**Method:** Created a dataset of 21K queries and 160K rubric items from survey articles in 75 research fields, evaluated by 31 Ph.D. annotators across 8 fields.

**Key Contributions:**

	1. Introduction of ResearchQA for LLM evaluation
	2. Development of a robust dataset from survey articles
	3. Creation of evaluation rubrics tailored for various research fields

**Result:** Achieved 96% of queries supporting Ph.D. information needs and identified competency gaps in 18 LLM systems, with none exceeding 70% on covering rubric items.

**Limitations:** The highest-ranking system addresses less than 11% of citation rubric items and may not fully evaluate all necessary response criteria.

**Conclusion:** ResearchQA facilitates better evaluations of LLM systems across multiple fields and helps identify their performance gaps in addressing complex queries.

**Abstract:** Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is widespread: survey articles synthesize knowledge distributed across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Each rubric, derived jointly with queries from survey sections, lists query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of queries support Ph.D. information needs and 87% of rubric items should be addressed in system responses by a sentence or more. Using our rubrics, we are able to construct an automatic pairwise judge obtaining 74% agreement with expert judgments. We leverage ResearchQA to analyze competency gaps in 18 systems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking agentic system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.

</details>


### [54] [Entropy-based Coarse and Compressed Semantic Speech Representation Learning](https://arxiv.org/abs/2509.00503)

*Jialong Zuo, Guangyan Zhang, Minghui Fang, Shengpeng Ji, Xiaoqi Jiao, Jingyu Li, Yiwen Guo, Zhou Zhao*

**Main category:** cs.CL

**Keywords:** speech representation, semantic modeling, dynamic aggregation

**Relevance Score:** 6

**TL;DR:** The paper proposes an entropy-based dynamic aggregation framework for learning compressed semantic speech representations, improving efficiency in speech modeling.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the redundancy and inefficiency of existing fine-grained tokenization methods in speech representation learning, which hinder downstream training and inference.

**Method:** A speech language model is pre-trained on large-scale unlabeled data using next-token prediction. Predictive entropy is used to determine aggregation boundaries, and a cross-attention module fuses information within segments to create compressed representations.

**Key Contributions:**

	1. Proposed a novel entropy-based dynamic aggregation framework
	2. Demonstrated efficacy on ASR and speech-to-text tasks
	3. Provided a method for adaptive control of representation granularity

**Result:** The experiments show that the compressed representations perform comparably to or better than dense token sequences across various tasks including ASR, speech-to-text translation, and voice conversion.

**Limitations:** 

**Conclusion:** The proposed framework allows for flexible control of granularity and compression ratio, enhancing efficiency in semantic speech representation learning.

**Abstract:** Discrete speech representation learning has recently attracted increasing interest in both acoustic and semantic modeling. Existing approaches typically encode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per second. However, given that speech generally conveys only 2 to 5 words per second, such fine-grained tokenization introduces redundancy and hinders efficiency in downstream training and inference. Moreover, semantic speech representations at this frequency primarily capture phonetic-level information, while semantic understanding may not require such detailed token-level resolution. To address these limitations, we propose an entropy-based dynamic aggregation framework for learning compressed semantic speech representations. A speech language model is first pre-trained via next-token prediction on large-scale unlabeled data to capture frequent token patterns. Predictive entropy is then used to adaptively determine aggregation boundaries, followed by a cross-attention module that fuses information within each segment. By adjusting the entropy threshold, the granularity and compression ratio of the representations can be flexibly controlled. Experiments on ASR, speech-to-text translation, and voice conversion tasks demonstrate that the compressed representations perform on par with or better than dense token sequences, demonstrating the effectiveness of the proposed approach.

</details>


### [55] [Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization](https://arxiv.org/abs/2509.00529)

*Eunjung Cho, Alexander Hoyle, Yoan Hermstrüwer*

**Main category:** cs.CL

**Keywords:** Large Language Models, legal stakeholders, summarization, motivated reasoning, AI in legal practice

**Relevance Score:** 8

**TL;DR:** This paper investigates how LLMs generate summaries based on different legal roles, highlighting potential biases in information framing and the implications for legal practice.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs adapt their summarization strategies according to different legal roles and the consequences of these adaptations in legal settings.

**Method:** An evaluation framework was developed to assess the summarization behavior of LLMs in relation to various legal roles, focusing on legal fact inclusion and favorability towards stakeholders.

**Key Contributions:**

	1. Introduction of a role-based evaluation framework for LLMs in legal contexts
	2. Empirical evidence of biased summarization patterns based on legal roles
	3. Discussion on the implications of role-inference in AI outputs for legal applications.

**Result:** Findings reveal that LLMs demonstrate biased information inclusion patterns based on the specified legal role, even when provided with balancing prompts.

**Limitations:** The study primarily focuses on legal contexts, which may not generalize to other domains; further exploration is needed in broader applications of LLMs.

**Conclusion:** The study emphasizes the necessity for role-aware evaluation of LLM outputs in legal contexts to mitigate issues of biased summarization.

**Abstract:** Large Language Models (LLMs) are increasingly used to generate user-tailored summaries, adapting outputs to specific stakeholders. In legal contexts, this raises important questions about motivated reasoning -- how models strategically frame information to align with a stakeholder's position within the legal system. Building on theories of legal realism and recent trends in legal practice, we investigate how LLMs respond to prompts conditioned on different legal roles (e.g., judges, prosecutors, attorneys) when summarizing judicial decisions. We introduce an evaluation framework grounded in legal fact and reasoning inclusion, also considering favorability towards stakeholders. Our results show that even when prompts include balancing instructions, models exhibit selective inclusion patterns that reflect role-consistent perspectives. These findings raise broader concerns about how similar alignment may emerge as LLMs begin to infer user roles from prior interactions or context, even without explicit role instructions. Our results underscore the need for role-aware evaluation of LLM summarization behavior in high-stakes legal settings.

</details>


### [56] [Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.00544)

*Hanqi Yan, Hainiu Xu, Yulan He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning-Induced Misalignment, safety, alignment, mixture-of-experts

**Relevance Score:** 9

**TL;DR:** The paper investigates the phenomenon of Reasoning-Induced Misalignment in Large Language Models (LLMs) and its implications for safety and alignment with human values.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs, ensuring their alignment with human values is critical, especially as concerns about their safety grow.

**Method:** The research involves analyzing the effects of enhanced reasoning in LLMs, particularly when shifting to 'think-mode' and fine-tuning on benign datasets.

**Key Contributions:**

	1. Identification of Reasoning-Induced Misalignment in LLMs
	2. Insights into the reasoning-safety trade-off
	3. Analysis of model states related to safety guardrails

**Result:** The study finds that LLMs exhibit increased responsiveness to malicious requests when their reasoning capabilities are heightened, especially in dense models.

**Limitations:** The study focuses mainly on density and specific model architectures, which may not generalize to all LLMs.

**Conclusion:** There is a critical need to address the reasoning-safety trade-off in advanced reasoning models to improve their alignment with human values.

**Abstract:** With Large Language Models (LLMs) becoming increasingly widely adopted, concerns regarding their safety and alignment with human values have intensified. Previous studies have shown that fine-tuning LLMs on narrow and malicious datasets induce misaligned behaviors. In this work, we report a more concerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe that LLMs become more responsive to malicious requests when reasoning is strengthened, via switching to "think-mode" or fine-tuning on benign math datasets, with dense models particularly vulnerable. Moreover, we analyze internal model states and find that both attention shifts and specialized experts in mixture-of-experts models help redirect excessive reasoning towards safety guardrails. These findings provide new insights into the emerging reasoning-safety trade-off and underscore the urgency of advancing alignment for advanced reasoning models.

</details>


### [57] [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)

*Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul*

**Main category:** cs.CL

**Keywords:** role-playing dialogue agents, large language models, prompting strategies, Commonsense Persona-grounded Dialogue Challenge, AI dialogue systems

**Relevance Score:** 9

**TL;DR:** This report explores how to enhance tool-augmented LLMs for role-playing dialogue via four prompting methods, identifying the rule-based approach as most effective.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues of dialogue agents generating overly long responses and ineffective tool usage, this research investigates prompting methods for large language models in dialogue settings.

**Method:** The study employs four prompting strategies: basic role prompting, human-crafted role prompting, automatic prompt optimization (APO), and rule-based role prompting (RRP). Each method's effectiveness is evaluated based on the responsiveness and tool usage of dialogue agents.

**Key Contributions:**

	1. Introduction of rule-based role prompting techniques that improve dialogue agent performance.
	2. Development of character-card/scene-contract design to enhance persona adherence.
	3. Open-sourcing of best-performing prompts and APO tool for future research.
	4. Establishment of a new benchmark for role-playing dialogue agent effectiveness.

**Result:** The rule-based role prompting approach outperformed others, achieving an overall score of 0.571, which is a notable improvement over the zero-shot baseline score of 0.519.

**Limitations:** 

**Conclusion:** The findings indicate that systematic RRP design can significantly enhance the capabilities of role-playing dialogue agents compared to more complicated approaches like APO.

**Abstract:** This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.

</details>


### [58] [StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks](https://arxiv.org/abs/2509.00591)

*Lang Xiong, Nishant Bhargava, Wesley Chang, Jianhang Hong, Haihao Liu, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation awareness, prompt rewriting, AI alignment, model safety

**Relevance Score:** 9

**TL;DR:** The paper investigates the phenomenon of 'evaluation awareness' in Large Language Models (LLMs), which affects their performance in test versus deployment contexts. It introduces a methodology for quantifying behavioral shifts and shows how rewriting prompts to mimic deployment contexts can improve safety and honesty in responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the behavioral shifts of LLMs when transitioning from deployment to evaluation contexts, which impacts AI alignment and safety.

**Method:** The authors developed a linear probe to score prompts from 'test-like' to 'deploy-like' and used an LLM rewriting strategy to transform prompts toward a more natural deployment context without altering the original task.

**Key Contributions:**

	1. Quantified the impact of 'evaluation awareness' on LLM behavior.
	2. Introduced a method to transform prompts to reduce test bias.
	3. Demonstrated significant performance improvements in safety and honesty through prompt rewriting.

**Result:** Implementation of the methodology led to a 30% increase in average probe scores, an increase in honest responses by 5.26%, a decrease in deceptive responses by 12.40%, and heightened safety compliance evidenced by a 6.38% increase in refusal rates.

**Limitations:** 

**Conclusion:** The study highlights that evaluation awareness can significantly distort LLM behavior and urges for the development of more realistic evaluation frameworks for assessing models before deployment.

**Abstract:** Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as "evaluation awareness." This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model's true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from "test-like" to "deploy-like" and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten "deploy-like" prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment.

</details>


### [59] [Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling](https://arxiv.org/abs/2509.00605)

*Rishiraj Acharya*

**Main category:** cs.CL

**Keywords:** Gated Associative Memory, sequence modeling, Transformer, machine learning

**Relevance Score:** 7

**TL;DR:** The Gated Associative Memory (GAM) network is proposed as a novel architecture for sequence modeling that achieves linear complexity and outperforms standard Transformer models in speed and validation perplexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational bottleneck of the Transformer architecture, which scales quadratically with sequence length, limiting its efficiency for long context processing.

**Method:** The GAM network introduces two parallel pathways: a causal convolution for local context and a parallel associative memory retrieval for global patterns, dynamically fused via a gating mechanism.

**Key Contributions:**

	1. Introduction of Gated Associative Memory (GAM) as a new sequence modeling architecture
	2. Demonstration of linear time complexity in processing sequences
	3. Rigorous comparative analysis against existing models showing superior performance on benchmark datasets

**Result:** GAM consistently demonstrates faster training speed and achieves superior or competitive validation perplexity compared to both a standard Transformer model and a modern linear-time baseline (Mamba) across different datasets.

**Limitations:** 

**Conclusion:** The GAM network offers a promising and efficient alternative for sequence modeling, effectively combining local and global information.

**Abstract:** The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.

</details>


### [60] [A Multi-Strategy Approach for AI-Generated Text Detection](https://arxiv.org/abs/2509.00623)

*Ali Zain, Sareem Farooqui, Muhammad Rafi*

**Main category:** cs.CL

**Keywords:** AI-generated content, RoBERTa, SVM, ensemble model, content detection

**Relevance Score:** 6

**TL;DR:** The paper discusses three systems for detecting AI-generated content, with a focus on a highly effective RoBERTa-based classifier.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the detection of AI-generated content in news articles and academic abstracts.

**Method:** Three distinct systems are developed: a fine-tuned RoBERTa-base classifier, a classical TF-IDF + SVM classifier, and an innovative ensemble model called Candace that uses features from multiple Llama-3.2 models.

**Key Contributions:**

	1. Introduction of an effective RoBERTa-based classifier for content detection
	2. Development of a novel ensemble model (Candace)
	3. Comparison of classical and modern machine learning approaches for this task.

**Result:** The RoBERTa-based system achieved near-perfect results on both development and test sets.

**Limitations:** 

**Conclusion:** The RoBERTa-based classifier is recommended for AI-generated content detection due to its high performance.

**Abstract:** This paper presents presents three distinct systems developed for the M-DAIGT shared task on detecting AI generated content in news articles and academic abstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2) A classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An Innovative ensemble model named Candace, leveraging probabilistic features extracted from multiple Llama-3.2 models processed by a customTransformer encoder.The RoBERTa-based system emerged as the most performant, achieving near-perfect results on both development and test sets.

</details>


### [61] [Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?](https://arxiv.org/abs/2509.00629)

*Md Tanzib Hosain, Md Kishor Morol*

**Main category:** cs.CL

**Keywords:** language models, competitive programming, ICPC benchmark, algorithmic thinking, inference techniques

**Relevance Score:** 6

**TL;DR:** This study introduces the ICPC benchmark for evaluating language models in competitive programming, achieving improved solve rates with advanced inference techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities of language models in solving complex algorithmic problems in competitive programming, a task that has not received sufficient attention.

**Method:** Development and evaluation of LM inference techniques using the ICPC benchmark consisting of 254 tasks, employing zero-shot prompting and multi-turn self-judging techniques.

**Key Contributions:**

	1. Introduction of the ICPC benchmark with 254 programming tasks.
	2. Development of advanced LM inference techniques.
	3. Open-sourcing of code and data for further research.

**Result:** With improved multi-turn self-judging techniques, the model's pass rate increased from 19.1% to 42.2%, demonstrating significant advancements in model capabilities.

**Limitations:** The study may not fully address all challenges in competitive programming and the applicability of the findings to other domains is yet to be explored.

**Conclusion:** The study highlights the potential for language models to excel in algorithmic reasoning tasks and offers insights into remaining challenges, while providing open-source resources.

**Abstract:** Among the hardest tasks for humans are those found in competitive programming where problems require sophisticated algorithmic thinking, puzzle solving, and the creation of effective code. As a domain to assess language models (LMs), it has not received enough attention, though. This study presents the ICPC benchmark, which consists of 254 international collegiate programming contest (ICPC) tasks. Each problem includes official analysis, reference code, and sample, high-quality unit, and hidden tests. We are able to develop and evaluate a variety of LM inference techniques for competitive programming with these resources. With zero-shot chain-of-thought prompting, we find that o1 only achieves a 19.1\% pass@1 solve rate. With our best inference technique, which combines multi-turn self-judge with reflection and retrieval over episodic information, raises this to 42.2\%. Furthermore, we conduct a new human-in-the-loop investigation to gain a deeper understanding of the remaining difficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems that were previously unsolvable by any model or technique with just a few specific instructions. A footstep toward LMs with grounded, imaginative, and algorithmic thinking is provided by our quantitative findings and qualitative research. We open-source our code and data at https://github.com/kraritt/zolve.

</details>


### [62] [Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech](https://arxiv.org/abs/2509.00673)

*Sanjeeevan Selvaganapathy, Mehwish Nasim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hate Speech Detection, Safety Alignment, Fairness, Ideological Bias

**Relevance Score:** 9

**TL;DR:** Study assesses Large Language Models' ability to classify hate speech, comparing uncensored and censored versions. Findings show censored models outperform uncensored ones in accuracy, but both types exhibit significant weaknesses in nuanced language and fairness across demographics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of large language models in detecting hate speech and the impact of safety alignment on their classification accuracy.

**Method:** The study compares the performance of uncensored and censored LLMs on hate speech detection, measuring accuracy, robustness, and bias in classification outcomes.

**Key Contributions:**

	1. Comparison of accuracy between censored and uncensored LLMs in hate speech detection
	2. Identification of failures in understanding nuanced language
	3. Highlighting fairness disparities in model performance across demographic groups

**Result:** Censored models achieved 78.7% accuracy, significantly outperforming uncensored models at 64.1%. However, both types exhibited failures in handling nuanced language and faced fairness issues across different user demographics.

**Limitations:** Models struggle with nuanced expressions like irony and display systemic overconfidence, making self-reported certainty unreliable.

**Conclusion:** The findings underscore that LLMs cannot be relied upon as unbiased arbiters in hate speech detection and advocate for improved auditing frameworks focusing on fairness and predictive accuracy.

**Abstract:** We investigate the efficacy of Large Language Models (LLMs) in detecting implicit and explicit hate speech, examining whether models with minimal safety alignment (uncensored) might provide more objective classification capabilities compared to their heavily-aligned (censored) counterparts. While uncensored models theoretically offer a less constrained perspective free from moral guardrails that could bias classification decisions, our results reveal a surprising trade-off: censored models significantly outperform their uncensored counterparts in both accuracy and robustness, achieving 78.7% versus 64.1% strict accuracy. However, this enhanced performance comes with its own limitation -- the safety alignment acts as a strong ideological anchor, making censored models resistant to persona-based influence, while uncensored models prove highly malleable to ideological framing. Furthermore, we identify critical failures across all models in understanding nuanced language such as irony. We also find alarming fairness disparities in performance across different targeted groups and systemic overconfidence that renders self-reported certainty unreliable. These findings challenge the notion of LLMs as objective arbiters and highlight the need for more sophisticated auditing frameworks that account for fairness, calibration, and ideological consistency.

</details>


### [63] [Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling](https://arxiv.org/abs/2509.00679)

*Junfeng Ran, Guangxiang Zhao, Yuhan Wu, Dawei Zhu, Longyun Wu, Yikai Zhao, Tong Yang, Lin Sun, Xiangzheng Zhang, Sujian Li*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, routing techniques, upcycling

**Relevance Score:** 6

**TL;DR:** This paper presents a novel routing technique called Router Upcycling to improve the performance of Mixture-of-Experts (MoE) models by enhancing routing complexity during upcycling.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient training of Mixture-of-Experts models due to their dynamic resource allocation and performance challenges.

**Method:** Router Upcycling initializes multiple routers from preceding attention heads to collaboratively assign tokens to specialized experts, enhancing routing during the MoE upcycling process.

**Key Contributions:**

	1. Introduction of Router Upcycling technique
	2. Enhanced performance of MoE upcycling models
	3. Achieving state-of-the-art results in experimental evaluations.

**Result:** The proposed method achieved state-of-the-art performance, outperforming existing upcycling baselines.

**Limitations:** 

**Conclusion:** Router Upcycling significantly improves the efficiency and effectiveness of training MoE models by addressing routing challenges.

**Abstract:** The Mixture-of-Experts (MoE) models have gained significant attention in deep learning due to their dynamic resource allocation and superior performance across diverse tasks. However, efficiently training these models remains challenging. The MoE upcycling technique has been proposed to reuse and improve existing model components, thereby minimizing training overhead. Despite this, simple routers, such as linear routers, often struggle with complex routing tasks within MoE upcycling. In response, we propose a novel routing technique called Router Upcycling to enhance the performance of MoE upcycling models. Our approach initializes multiple routers from the attention heads of preceding attention layers during upcycling. These routers collaboratively assign tokens to specialized experts in an attention-like manner. Each token is processed into diverse queries and aligned with the experts' features (serving as keys). Experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance, outperforming other upcycling baselines.

</details>


### [64] [Do small language models generate realistic variable-quality fake news headlines?](https://arxiv.org/abs/2509.00680)

*Austin McCutcheon, Chris Brogly*

**Main category:** cs.CL

**Keywords:** Small language models, Fake news generation, Headline detection, Machine learning, Ethical compliance

**Relevance Score:** 7

**TL;DR:** This study evaluates the ability of 14 small language models to generate persuasive fake news headlines and assesses the accuracy of existing detection models on these generated headlines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential for small language models to generate misleading text and to evaluate the effectiveness of machine learning models in detecting such generated headlines.

**Method:** Controlled prompt engineering was used to generate 24,000 news headlines across categories of low and high quality. The generated headlines were then assessed using existing headline quality detectors.

**Key Contributions:**

	1. Evaluation of 14 small language models in generating fake news headlines.
	2. Analysis of the effectiveness of existing deep learning-based headline detection models.
	3. Insights into ethical compliance rates of language models in generating misleading content.

**Result:** Detection models achieved accuracy rates between 35.2% and 63.5%, indicating common misclassification of generated fake news headlines.

**Limitations:** The study did not explore broader implications for societal impact or long-term effects of generated content.

**Conclusion:** The evaluated small language models were generally compliant in generating falsified headlines, with some variations in ethical considerations, and exhibited low detection accuracy against existing classifiers.

**Abstract:** Small language models (SLMs) have the capability for text generation and may potentially be used to generate falsified texts online. This study evaluates 14 SLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and Granite families in generating perceived low and high quality fake news headlines when explicitly prompted, and whether they appear to be similar to real-world news headlines. Using controlled prompt engineering, 24,000 headlines were generated across low-quality and high-quality deceptive categories. Existing machine learning and deep learning-based news headline quality detectors were then applied against these SLM-generated fake news headlines. SLMs demonstrated high compliance rates with minimal ethical resistance, though there were some occasional exceptions. Headline quality detection using established DistilBERT and bagging classifier models showed that quality misclassification was common, with detection accuracies only ranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs generally are compliant in generating falsified headlines, although there are slight variations in ethical restraints, and the generated headlines did not closely resemble existing primarily human-written content on the web, given the low quality classification accuracy.

</details>


### [65] [Text Reinforcement for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.00687)

*Chen Su, Yuanhe Tian, Yan Song, Yongdong Zhang*

**Main category:** cs.CL

**Keywords:** time series forecasting, multimodal input, text reinforcement, reinforcement learning, performance improvement

**Relevance Score:** 7

**TL;DR:** This paper presents a model to enhance multimodal time series forecasting (TSF) by reinforcing textual inputs, using reinforcement learning to improve the quality of generated text, ultimately leading to better forecasting performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of unstable performance in multimodal TSF due to low-quality text inputs, which may not fully capture essential information from historical time series data.

**Method:** The study proposes a text reinforcement model (TeR) that generates enhanced text based on historical data, guided by a reinforcement learning framework that rewards improved performance in TSF tasks.

**Key Contributions:**

	1. Introduction of a text reinforcement model (TeR) for improved multimodal TSF.
	2. Development of a reinforcement learning mechanism to optimize text quality based on TSF performance.
	3. Demonstration of superior results on real-world datasets compared to traditional approaches.

**Result:** Experiments show that the proposed TeR model significantly improves the performance of multimodal TSF on various real-world datasets compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed reinforcement approach to text generation enhances the understanding of time series data, leading to more robust forecasting outcomes.

**Abstract:** Recent studies in time series forecasting (TSF) use multimodal inputs, such as text and historical time series data, to predict future values. These studies mainly focus on developing advanced techniques to integrate textual information with time series data to perform the task and achieve promising results. Meanwhile, these approaches rely on high-quality text and time series inputs, whereas in some cases, the text does not accurately or fully capture the information carried by the historical time series, which leads to unstable performance in multimodal TSF. Therefore, it is necessary to enhance the textual content to improve the performance of multimodal TSF. In this paper, we propose improving multimodal TSF by reinforcing the text modalities. We propose a text reinforcement model (TeR) to generate reinforced text that addresses potential weaknesses in the original text, then apply this reinforced text to support the multimodal TSF model's understanding of the time series, improving TSF performance. To guide the TeR toward producing higher-quality reinforced text, we design a reinforcement learning approach that assigns rewards based on the impact of each reinforced text on the performance of the multimodal TSF model and its relevance to the TSF task. We optimize the TeR accordingly, so as to improve the quality of the generated reinforced text and enhance TSF performance. Extensive experiments on a real-world benchmark dataset covering various domains demonstrate the effectiveness of our approach, which outperforms strong baselines and existing studies on the dataset.

</details>


### [66] [CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders](https://arxiv.org/abs/2509.00691)

*Alex Gulko, Yusen Peng, Sachin Kumar*

**Main category:** cs.CL

**Keywords:** sparse autoencoders, interpretability, benchmarking, contrastive evaluation, LLMs

**Relevance Score:** 8

**TL;DR:** CE-Bench is a contrastive evaluation benchmark for sparse autoencoders that measures interpretability effectively without needing external LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Sparse autoencoders are promising for uncovering interpretable features in large language models, but lack of automated evaluation methods limits their adoption.

**Method:** Introduction of CE-Bench, a contrastive evaluation benchmark built on a curated dataset of contrastive story pairs, along with comprehensive ablation studies.

**Key Contributions:**

	1. Introduction of CE-Bench as a novel evaluation benchmark
	2. Demonstration of CE-Bench's effectiveness through ablation studies
	3. Open-sourcing of implementation and dataset for community use

**Result:** CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks.

**Limitations:** 

**Conclusion:** The official implementation and evaluation dataset are open-sourced under the MIT License, enabling further research and application.

**Abstract:** Probing with sparse autoencoders is a promising approach for uncovering interpretable features in large language models (LLMs). However, the lack of automated evaluation methods has hindered their broader adoption and development. In this work, we introduce CE-Bench, a novel and lightweight contrastive evaluation benchmark for sparse autoencoders, built on a curated dataset of contrastive story pairs. We conduct comprehensive ablation studies to validate the effectiveness of our approach. Our results show that CE-Bench reliably measures the interpretability of sparse autoencoders and aligns well with existing benchmarks, all without requiring an external LLM. The official implementation and evaluation dataset are open-sourced under the MIT License.

</details>


### [67] [Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs](https://arxiv.org/abs/2509.00698)

*Kaiwen Wei, Jinpeng Gao, Jiang Zhong, Yuming Yang, Fengmao Lv, Zhenyang Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Recommendation Systems, User Reviews, Preference Retrieval, Interpretability

**Relevance Score:** 9

**TL;DR:** RevBrowse is a review-driven recommendation framework that enhances LLM-based recommendation systems by effectively integrating user reviews into the reranking process to improve decision-making in recommendation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of utilizing user reviews effectively in LLM-based recommendation systems, particularly regarding the constraints of context windows and the prioritization of relevant reviews.

**Method:** RevBrowse combines a review-driven approach with an LLM-based reranking process and introduces PrefRAG, a retrieval-augmented module that structures user and item representations to retrieve relevant content based on user preferences.

**Key Contributions:**

	1. Introduction of RevBrowse for integrating reviews in LLM-based recommendations
	2. Development of PrefRAG for structured retrieval of preference-relevant content
	3. Demonstration of significant performance improvements on benchmark datasets

**Result:** Experiments on four Amazon review datasets show that RevBrowse consistently outperforms strong baselines, highlighting its generalizability and effectiveness in adapting to dynamic user preferences.

**Limitations:** 

**Conclusion:** RevBrowse not only enhances recommendation accuracy but also provides interpretability by allowing users to see which reviews influence recommendations.

**Abstract:** Large language models (LLMs) have shown strong potential in recommendation tasks due to their strengths in language understanding, reasoning and knowledge integration. These capabilities are especially beneficial for review-based recommendation, which relies on semantically rich user-generated texts to reveal fine-grained user preferences and item attributes. However, effectively incorporating reviews into LLM-based recommendation remains challenging due to (1) inefficient to dynamically utilize user reviews under LLMs' constrained context windows, and (2) lacking effective mechanisms to prioritize reviews most relevant to the user's current decision context. To address these challenges, we propose RevBrowse, a review-driven recommendation framework inspired by the "browse-then-decide" decision process commonly observed in online user behavior. RevBrowse integrates user reviews into the LLM-based reranking process to enhance its ability to distinguish between candidate items. To improve the relevance and efficiency of review usage, we introduce PrefRAG, a retrieval-augmented module that disentangles user and item representations into structured forms and adaptively retrieves preference-relevant content conditioned on the target item. Extensive experiments on four Amazon review datasets demonstrate that RevBrowse achieves consistent and significant improvements over strong baselines, highlighting its generalizability and effectiveness in modeling dynamic user preferences. Furthermore, since the retrieval-augmented process is transparent, RevBrowse offers a certain level of interpretability by making visible which reviews influence the final recommendation.

</details>


### [68] [Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs](https://arxiv.org/abs/2509.00707)

*Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, Jaegul Choo*

**Main category:** cs.CL

**Keywords:** masked diffusion models, reward-weighted sampling, non-autoregressive generation, sequence coherence, token selection

**Relevance Score:** 8

**TL;DR:** Reward-Weighted Sampling (RWS) enhances masked diffusion models (MDMs) by integrating global sequence-level coherence in token selection, leading to improved non-autoregressive generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of standard decoding methods in masked diffusion models, which often yield outputs resembling autoregressive processes.

**Method:** RWS leverages an external reward model at each diffusion step to evaluate and scale token logits for improved selection, promoting coherence in the generated sequence.

**Key Contributions:**

	1. Introduction of Reward-Weighted Sampling (RWS) as a novel decoding strategy for MDMs.
	2. Theoretical justification for rank reversals in token selection due to reward-weighted logit scaling.
	3. Empirical results showing RWS's effectiveness in improving generation quality and coherence.

**Result:** Experiments demonstrate that RWS improves non-autoregressive generation orders and significantly enhances multiple evaluation metrics for MDMs.

**Limitations:** 

**Conclusion:** Integrating global signals through RWS effectively boosts the performance of masked diffusion models and supports non-autoregressive properties.

**Abstract:** Masked diffusion models (MDMs) offer a promising non-autoregressive alternative for large language modeling. Standard decoding methods for MDMs, such as confidence-based sampling, select tokens independently based on individual token confidences at each diffusion step. However, we observe that this independent token selection often results in generation orders resembling sequential autoregressive processes, limiting the advantages of non-autoregressive modeling. To mitigate this pheonomenon, we propose Reward-Weighted Sampling (RWS), a novel decoding strategy that leverages an external reward model to provide a principled global signal during the iterative diffusion process. Specifically, at each diffusion step, RWS evaluates the quality of the entire intermediate sequence and scales token logits accordingly, guiding token selection by integrating global sequence-level coherence. This method selectively increases the confidence of tokens that initially have lower scores, thereby promoting a more non-autoregressive generation order. Furthermore, we provide theoretical justification showing that reward-weighted logit scaling induces beneficial rank reversals in token selection and consistently improves expected reward. Experiments demonstrate that RWS significantly promotes non-autoregressive generation orders, leading to improvements across multiple evaluation metrics. These results highlight the effectiveness of integrating global signals in enhancing both the non-autoregressive properties and overall performance of MDMs.

</details>


### [69] [Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI](https://arxiv.org/abs/2509.00709)

*Elias Ra, Seung Je Kim, Eui-Yeong Seo, Geunju So*

**Main category:** cs.CL

**Keywords:** AI-powered Learning Management System, Generative AI, Conversational AI

**Relevance Score:** 6

**TL;DR:** This study presents a framework for an AI-powered Learning Management System that enhances personalized learning experiences in higher education.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in delivering personalized, scalable, and pedagogically coherent learning experiences in higher education.

**Method:** Utilizes a design-based research (DBR) methodology across five phases: literature review, SWOT analysis, ethical-pedagogical principles development, system design, and instructional strategy formulation.

**Key Contributions:**

	1. Structured framework for AI-LMS design
	2. Integration of generative and conversational AI in education
	3. Ethical-pedagogical principles for learner-centered instruction.

**Result:** The AI-LMS includes modular components such as configurable prompts, adaptive feedback loops, and multi-agent conversation flows, in line with various pedagogical paradigms.

**Limitations:** 

**Conclusion:** The framework offers a practical model for integrating AI in education, emphasizing human-centered design and ethical aspects; further research will test the system in real-world settings.

**Abstract:** Higher education faces growing challenges in delivering personalized, scalable, and pedagogically coherent learning experiences. This study introduces a structured framework for designing an AI-powered Learning Management System (AI-LMS) that integrates generative and conversational AI to support adaptive, interactive, and learner-centered instruction. Using a design-based research (DBR) methodology, the framework unfolds through five phases: literature review, SWOT analysis, development of ethical-pedagogical principles, system design, and instructional strategy formulation. The resulting AI-LMS features modular components -- including configurable prompts, adaptive feedback loops, and multi-agent conversation flows -- aligned with pedagogical paradigms such as behaviorist, constructivist, and connectivist learning theories. By combining AI capabilities with human-centered design and ethical safeguards, this study advances a practical model for AI integration in education. Future research will validate and refine the system through real-world implementation.

</details>


### [70] [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)

*Shreyas Tirumala, Nishant Jain, Danny D. Leybzon, Trent D. Buskirk*

**Main category:** cs.CL

**Keywords:** AI interviewers, Interactive Voice Response, data collection, transformer models, qualitative research

**Relevance Score:** 8

**TL;DR:** The paper reviews the effectiveness of Transformer-based AI interviewers compared to traditional Interactive Voice Response systems in data collection for research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the fit-for-purpose capabilities of AI interviewers in real-time voice-based surveys and their impact on quantitative and qualitative research.

**Method:** The paper evaluates AI interviewers and IVR systems based on their input/output performance and verbal reasoning abilities, supported by field studies.

**Key Contributions:**

	1. Comparison of AI interviewers with IVR systems in research contexts.
	2. Field studies demonstrating AI interviewers' superior capabilities.
	3. Identification of limitations in current AI interviewer technology.

**Result:** AI interviewers surpass IVR systems in both quantitative and qualitative data collection, although they face challenges with transcription errors and emotion detection.

**Limitations:** Real-time transcription errors, limited emotion detection, and inconsistent follow-up quality.

**Conclusion:** AI interviewer technology's utility for qualitative data collection is context-dependent, suggesting careful consideration of its application in research settings.

**Abstract:** Transformer-based Large Language Models (LLMs) have paved the way for "AI interviewers" that can administer voice-based surveys with respondents in real-time. This position paper reviews emerging evidence to understand when such AI interviewing systems are fit for purpose for collecting data within quantitative and qualitative research contexts. We evaluate the capabilities of AI interviewers as well as current Interactive Voice Response (IVR) systems across two dimensions: input/output performance (i.e., speech recognition, answer recording, emotion handling) and verbal reasoning (i.e., ability to probe, clarify, and handle branching logic). Field studies suggest that AI interviewers already exceed IVR capabilities for both quantitative and qualitative data collection, but real-time transcription error rates, limited emotion detection abilities, and uneven follow-up quality indicate that the utility, use and adoption of current AI interviewer technology may be context-dependent for qualitative data collection efforts.

</details>


### [71] [LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA](https://arxiv.org/abs/2509.00731)

*Houji Jin, Negin Ashrafi, Armin Abdollahi, Wei Liu, Jian Wang, Ganyu Gui, Maryam Pishgar, Huanghao Feng*

**Main category:** cs.CL

**Keywords:** AI-generated text detection, Chinese NLP, Fine-tuning methods

**Relevance Score:** 9

**TL;DR:** This study compares various AI models for detecting AI-generated Chinese text, highlighting the superior performance of LoRA-adapted decoder models over traditional encoder models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The demand for accurate detection of AI-generated text in Chinese is increasing, necessitating improved methods that account for linguistic nuances.

**Method:** The study systematically compares encoder-based Transformers (Chinese BERT-large, RoBERTa-wwm-ext-large) and a decoder-only LLM (Qwen2.5-7B fine-tuned with LoRA) using a specific dataset for Chinese AI-generated text detection.

**Key Contributions:**

	1. Systematic comparison of encoder and decoder models for Chinese AI-generated text detection
	2. Introduction of prompt-based masked language modeling for fine-tuning encoder models
	3. Demonstration of superior performance of LoRA-adapted models over traditional methods

**Result:** The LoRA-adapted Qwen2.5-7B model achieved 95.94% test accuracy, outperforming encoder models but presenting challenges related to distribution shifts and semantic understanding in baseline methods.

**Limitations:** Encoder models struggle with distribution shifts and have a tendency to memorize training data.

**Conclusion:** Decoder-based LLMs with parameter-efficient fine-tuning prove to be more robust for detecting AI-generated Chinese text, suggesting future avenues for improving cross-domain robustness.

**Abstract:** The rapid growth of large language models (LLMs) has heightened the demand for accurate detection of AI-generated text, particularly in languages like Chinese, where subtle linguistic nuances pose significant challenges to current methods. In this study, we conduct a systematic comparison of encoder-based Transformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM (Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank Adaptation, LoRA), and a FastText baseline using the publicly available dataset from the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models were fine-tuned using a novel prompt-based masked language modeling approach, while Qwen2.5-7B was adapted for classification with an instruction-format input and a lightweight classification head trained via LoRA. Experiments reveal that although encoder models nearly memorize training data, they suffer significant performance degradation under distribution shifts (RoBERTa: 76.3% test accuracy; BERT: 79.3%). FastText demonstrates surprising lexical robustness (83.5% accuracy) yet lacks deeper semantic understanding. In contrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with balanced precision-recall metrics, indicating superior generalization and resilience to dataset-specific artifacts. These findings underscore the efficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust Chinese AI-generated text detection. Future work will explore next-generation Qwen3 models, distilled variants, and ensemble strategies to enhance cross-domain robustness further.

</details>


### [72] [Decomposing and Revising What Language Models Generate](https://arxiv.org/abs/2509.00765)

*Zhichao Yan, Jiaoyan Chen, Jiapu Wang, Xiaoli Li, Ru Li, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** question answering, large language models, fact decomposition, evidence aggregation, machine learning

**Relevance Score:** 9

**TL;DR:** FIDES is a new framework for improving attribution in question answering with LLMs by enhancing fact decomposition and evidence aggregation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of existing question decomposition-based QA approaches, particularly the issues of irrelevance in generated questions and incomplete fact retrieval.

**Method:** FIDES employs a two-stage faithful decomposition method to break down long-form answers into sub-facts, which are used to retrieve and aggregate evidence snippets from related documents.

**Key Contributions:**

	1. Introduced a novel fact decomposition method that enhances the contextual relevance of sub-facts.
	2. Developed a new metric, $Attr_{auto-P}$, for evaluating evidence precision in question answering.
	3. Achieved superior performance compared to existing state-of-the-art QA methods.

**Result:** FIDES shows a significant performance improvement over state-of-the-art methods, achieving over 14% better average results on six datasets using LLMs like GPT-3.5-turbo, Gemini, and Llama 70B.

**Limitations:** 

**Conclusion:** The proposed framework effectively enhances the relevance and completeness of evidence retrieval in QA tasks.

**Abstract:** Attribution is crucial in question answering (QA) with Large Language Models (LLMs).SOTA question decomposition-based approaches use long form answers to generate questions for retrieving related documents. However, the generated questions are often irrelevant and incomplete, resulting in a loss of facts in retrieval.These approaches also fail to aggregate evidence snippets from different documents and paragraphs. To tackle these problems, we propose a new fact decomposition-based framework called FIDES (\textit{faithful context enhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES uses a contextually enhanced two-stage faithful decomposition method to decompose long form answers into sub-facts, which are then used by a retriever to retrieve related evidence snippets. If the retrieved evidence snippets conflict with the related sub-facts, such sub-facts will be revised accordingly. Finally, the evidence snippets are aggregated according to the original sentences.Extensive evaluation has been conducted with six datasets, with an additionally proposed new metric called $Attr_{auto-P}$ for evaluating the evidence precision. FIDES outperforms the SOTA methods by over 14\% in average with GPT-3.5-turbo, Gemini and Llama 70B series.

</details>


### [73] [LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation](https://arxiv.org/abs/2509.00783)

*Weizhe Shi, Qiqi Wang, Yihong Pan, Qian Liu, Kaiqi Zhao*

**Main category:** cs.CL

**Keywords:** Judicial Opinion Generation, LegalAI, Legal Reasoning, Sentencing Prediction, LegalChainReasoner

**Relevance Score:** 4

**TL;DR:** This paper proposes a new framework for automatically generating judicial opinions, integrating legal reasoning and sentencing predictions to enhance consistency and applicability in judicial settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the consistency and practical deployment of judicial opinion generation, addressing limitations in current isolated approaches to legal reasoning and sentencing prediction.

**Method:** The authors introduce LegalChainReasoner, a framework that uses structured legal chains to guide comprehensive assessments of cases, incorporating factual premises and legal conditions for opinion generation.

**Key Contributions:**

	1. Introduction of the LegalAI task for Judicial Opinion Generation
	2. Development of the LegalChainReasoner framework that integrates structured legal reasoning
	3. Empirical validation showing superior performance over current methods

**Result:** Experiments show that the proposed method significantly outperforms existing baseline models on two real-world Chinese legal case datasets.

**Limitations:** The methods and results are based on datasets limited to Chinese legal cases, which may not generalize to other legal systems.

**Conclusion:** The study demonstrates the feasibility and effectiveness of a unified approach to judicial opinion generation that combines legal reasoning with sentencing predictions.

**Abstract:** A criminal judicial opinion represents the judge's disposition of a case, including the decision rationale and sentencing. Automatically generating such opinions can assist in analyzing sentencing consistency and provide judges with references to similar past cases. However, current research typically approaches this task by dividing it into two isolated subtasks: legal reasoning and sentencing prediction. This separation often leads to inconsistency between the reasoning and predictions, failing to meet real-world judicial requirements. Furthermore, prior studies rely on manually curated knowledge to enhance applicability, yet such methods remain limited in practical deployment. To address these limitations and better align with legal practice, we propose a new LegalAI task: Judicial Opinion Generation, which simultaneously produces both legal reasoning and sentencing decisions. To achieve this, we introduce LegalChainReasoner, a framework that applies structured legal chains to guide the model through comprehensive case assessments. By integrating factual premises, composite legal conditions, and sentencing conclusions, our approach ensures flexible knowledge injection and end-to-end opinion generation. Experiments on two real-world and open-source Chinese legal case datasets demonstrate that our method outperforms baseline models.

</details>


### [74] [CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA](https://arxiv.org/abs/2509.00806)

*Reem Abdel-Salam, Mary Adewunmi, Modinat A. Abayomi*

**Main category:** cs.CL

**Keywords:** Biomedical question answering, Large language models, Fine-tuning, Multi-hop QA, Medical applications

**Relevance Score:** 9

**TL;DR:** This paper discusses the evaluation of large language models (LLMs) for multi-hop biomedical question answering, presenting a methodology and results from the BioCreative IX shared task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for rigorous evaluation of LLMs' performance in complex question answering for biomedical and healthcare applications.

**Method:** Utilizes a supervised fine-tuning strategy with the LLaMA 3 8B model on a curated biomedical question-answer dataset, exploring three experimental setups.

**Key Contributions:**

	1. Development of a two-stage inference pipeline for short-answer extraction
	2. Analysis of multi-hop biomedical question answering
	3. Insights into the performance gaps of LLMs in biomedical applications

**Result:** Models achieved concept-level accuracy scores of up to 0.8, but Exact Match (EM) scores were significantly lower, particularly during testing.

**Limitations:** Models still struggle with generating strictly formatted outputs despite some improvements; challenges remain in alignment with evaluation metrics.

**Conclusion:** The findings reveal gaps between semantic understanding and exact answer evaluation, highlighting the need for further research in output control and post-processing.

**Abstract:** Large language models (LLMs) are increasingly evident for accurate question answering across various domains. However, rigorous evaluation of their performance on complex question-answering (QA) capabilities is essential before deployment in real-world biomedical and healthcare applications. This paper presents our approach to the MedHopQA track of the BioCreative IX shared task, which focuses on multi-hop biomedical question answering involving diseases, genes, and chemicals. We adopt a supervised fine-tuning strategy leveraging LLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled from external sources including BioASQ, MedQuAD, and TREC. Three experimental setups are explored: fine-tuning on combined short and long answers, short answers only, and long answers only. While our models demonstrate strong domain understanding, achieving concept-level accuracy scores of up to 0.8, their Exact Match (EM) scores remain significantly lower, particularly in the test phase. We introduce a two-stage inference pipeline for precise short-answer extraction to mitigate verbosity and improve alignment with evaluation metrics. Despite partial improvements, challenges persist in generating strictly formatted outputs. Our findings highlight the gap between semantic understanding and exact answer evaluation in biomedical LLM applications, motivating further research in output control and post-processing strategies.

</details>


### [75] [TMT: A Simple Way to Translate Topic Models Using Dictionaries](https://arxiv.org/abs/2509.00822)

*Felix Engl, Andreas Henrich*

**Main category:** cs.CL

**Keywords:** topic models, multilingual, topic model translation, Latent Dirichlet Allocation, natural language processing

**Relevance Score:** 5

**TL;DR:** We present Topic Model Translation (TMT), a technique to transfer topic models between languages without requiring metadata or aligned corpora.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The training of topic models in a multilingual context is complicated by the need for sophisticated algorithms and the unavailability of sufficient multilingual data.

**Method:** Introduction of Topic Model Translation (TMT), which allows for the transfer of topic models from one language to another without needing aligned corpora, metadata, or embeddings.

**Key Contributions:**

	1. Introduction of a new technique, Topic Model Translation (TMT) for multilingual topic modeling.
	2. Elimination of the need for aligned corpora, metadata, or embeddings in topic transfer.
	3. Extensive evaluation showing effectiveness in semantically coherent topic translation.

**Result:** TMT was evaluated using both quantitative and qualitative methods, demonstrating it produces semantically coherent and consistent topic translations across languages.

**Limitations:** The effectiveness may vary with different languages and the underlying quality of the original topic model used for translation.

**Conclusion:** TMT offers a robust solution for reusing topic models in languages where resources are limited or unavailable, facilitating multilingual topic modeling.

**Abstract:** The training of topic models for a multilingual environment is a challenging task, requiring the use of sophisticated algorithms, topic-aligned corpora, and manual evaluation. These difficulties are further exacerbated when the developer lacks knowledge of the target language or is working in an environment with limited data, where only small or unusable multilingual corpora are available.   Considering these challenges, we introduce Topic Model Translation (TMT), a novel, robust and transparent technique designed to transfer topic models (e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language to another, without the need for metadata, embeddings, or aligned corpora. TMT enables the reuse of topic models across languages, making it especially suitable for scenarios where large corpora in the target language are unavailable or manual translation is infeasible. Furthermore, we evaluate TMT extensively using both quantitative and qualitative methods, demonstrating that it produces semantically coherent and consistent topic translations.

</details>


### [76] [Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations](https://arxiv.org/abs/2509.00841)

*Michelle Elizabeth, Alicja Kasicka, Natalia Krawczyk, Magalie Ochs, Gwénolé Lecorvé, Justyna Gromada, Lina M. Rojas-Barahona*

**Main category:** cs.CL

**Keywords:** Generative AI, Dialogue Systems, Evaluation Metrics

**Relevance Score:** 6

**TL;DR:** The paper discusses the evaluation of generative AI-based dialogue systems, focusing on predicting dialogue-level scores through small models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** With the increase in generative AI dialogue systems, there is a pressing need to evaluate their effectiveness reliably.

**Method:** Utilized Language Models (LMs) for evaluation through prompting, alongside encoder-based models for classification and regression.

**Key Contributions:**

	1. Demonstration of using small models for dialogue evaluation
	2. Comparison of LM prompting with regression and classification models
	3. Insights on performance variability across different datasets

**Result:** LM prompting showed modest correlation with human judgments but ranked second on the test set, while smaller regression and classification models showed high correlation on the validation set despite performance drops on the test set.

**Limitations:** Performance variations on the test set due to differing score ranges compared to train and validation datasets.

**Conclusion:** While small models can evaluate dialogue systems, their performance may vary depending on the dataset used.

**Abstract:** The growing number of generative AI-based dialogue systems has made their evaluation a crucial challenge. This paper presents our contribution to this important problem through the Dialogue System Technology Challenge (DSTC-12, Track 1), where we developed models to predict dialogue-level, dimension-specific scores. Given the constraint of using relatively small models (i.e. fewer than 13 billion parameters) our work follows two main strategies: employing Language Models (LMs) as evaluators through prompting, and training encoder-based classification and regression models.   Our results show that while LM prompting achieves only modest correlations with human judgments, it still ranks second on the test set, outperformed only by the baseline. The regression and classification models, with significantly fewer parameters, demonstrate high correlation for some dimensions on the validation set. Although their performance decreases on the test set, it is important to note that the test set contains annotations with significantly different score ranges for some of the dimensions with respect to the train and validation sets.

</details>


### [77] [Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings](https://arxiv.org/abs/2509.00842)

*Tengyu Pan, Zhichao Duan, Zhenyu Li, Bowen Dong, Ning Liu, Xiuxing Li, Jianyong Wang*

**Main category:** cs.CL

**Keywords:** text embedding, contrastive learning, hard-negative sampling, large language models, semantic representations

**Relevance Score:** 8

**TL;DR:** This paper introduces a Multi-Granularity Hard-negative synthesis framework using large language models to enhance text embedding models via diverse negative samples and an Anchor Token Aware pooling method for improved accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance text embedding models by improving the generation of negative samples in contrastive learning, which is crucial for differentiating subtle semantic distinctions.

**Method:** The framework uses large language models to generate negative samples of varying similarity levels, employing a coarse-to-fine curriculum learning approach. Additionally, it introduces an Anchor Token Aware pooling method to improve text embedding accuracy.

**Key Contributions:**

	1. Introduction of MGH framework for generating diverse negative samples
	2. Development of ATA pooling method for improved embedding accuracy
	3. Demonstration of state-of-the-art performance on MTEB benchmark

**Result:** The proposed methods achieve state-of-the-art results on the MTEB benchmark, outperforming existing synthesis strategies with both synthetic and public dataset combinations.

**Limitations:** 

**Conclusion:** The MGH synthesis framework and ATA pooling method significantly enhance the performance of text embedding models without complicating the model structure.

**Abstract:** Text embedding models are essential for various natural language processing tasks, enabling the effective encoding of semantic information into dense vector representations. These models are typically optimized using triplets of (query, positive, negative) data pairs for contrastive learning, where the negative samples play a critical role in enhancing the model's ability to discern subtle semantic distinctions. In this work, we introduce a Multi-Granularity Hard-negative (MGH) synthesis framework that leverages large language models (LLMs) to generate diverse negative samples with varying levels of similarity with the query. This approach facilitates a coarse-to-fine curriculum learning strategy during supervised training, allowing the embedding model to progressively learn more nuanced semantic representations. Meanwhile, we propose an Anchor Token Aware (ATA) pooling method that assigns higher weights to anchor tokens based on aggregation patterns observed in LLMs, improving text embedding accuracy without increasing model complexity. Comprehensive experiments on the MTEB benchmark demonstrate that our methods achieve state-of-the-art performance, surpassing existing synthesis strategies both with synthetic data and when combined with public retrieval datasets.

</details>


### [78] [Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations](https://arxiv.org/abs/2509.00849)

*Shaina Raza, Maximus Powers, Partha Pratim Saha, Mahveen Raza, Rizwan Qureshi*

**Main category:** cs.CL

**Keywords:** Text-to-Image, Bias Assessment, Fairness Intervention, Demographic Diversity, Model Evaluation

**Relevance Score:** 8

**TL;DR:** This paper assesses representational societal bias in Text-to-Image (TTI) models through a structured benchmarking study, revealing varying effectiveness across different models in promoting demographic diversity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and mitigate harmful social biases in Text-to-Image models and assess the efficacy of prompting as a fairness intervention.

**Method:** The study uses five models (both closed-source and open-source) to analyze occupational portrayals and compares traditional prompts against fairness-aware controlled prompts, focusing on gender and race annotations.

**Key Contributions:**

	1. Introduction of a pilot benchmark for evaluating bias in TTI models
	2. Demonstrated varying effectiveness of prompting strategies across different models
	3. Released code and data for transparency

**Result:** The results indicate that prompting can affect demographic representation, but the outcomes are model-specific; while some models diversify outputs effectively, others may reinforce stereotypes or show resistance to change.

**Limitations:** Analysis is limited to a specific set of occupational roles and may not generalize across all demographics and contexts.

**Conclusion:** Prompting can shift representations but has notable limitations, suggesting that additional model-specific strategies are necessary for effective bias mitigation.

**Abstract:** Text-to-Image (TTI) models are powerful creative tools but risk amplifying harmful social biases. We frame representational societal bias assessment as an image curation and evaluation task and introduce a pilot benchmark of occupational portrayals spanning five socially salient roles (CEO, Nurse, Software Engineer, Teacher, Athlete). Using five state-of-the-art models: closed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable Diffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against fairness-aware controlled prompts designed to encourage demographic diversity. All outputs are annotated for gender (male, female) and race (Asian, Black, White), enabling structured distributional analysis. Results show that prompting can substantially shift demographic representations, but with highly model-specific effects: some systems diversify effectively, others overcorrect into unrealistic uniformity, and some show little responsiveness. These findings highlight both the promise and the limitations of prompting as a fairness intervention, underscoring the need for complementary model-level strategies. We release all code and data for transparency and reproducibility https://github.com/maximus-powers/img-gen-bias-analysis.

</details>


### [79] [Exploring and Mitigating Fawning Hallucinations in Large Language Models](https://arxiv.org/abs/2509.00869)

*Zixuan Shangguan, Yanjie Dong, Lanjun Wang, Xiaoyi Fan, Victor C. M. Leung, Xiping Hu*

**Main category:** cs.CL

**Keywords:** large language models, fawning hallucinations, contrastive decoding, natural language processing, factuality

**Relevance Score:** 9

**TL;DR:** This paper addresses the issue of fawning hallucinations in large language models (LLMs) and proposes a collaborative contrastive decoding method to mitigate these occurrences and improve factual responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing occurrence of fawning hallucinations in LLMs poses a challenge for the accuracy and reliability of generated content, especially in critical applications.

**Method:** The authors develop a contrastive decoding method, specifically the collaborative contrastive decoding (CCD), to reduce misleading outputs by contrasting response distributions of deceptive and neutral inputs without needing extra training.

**Key Contributions:**

	1. Analysis of fawning hallucinations in LLMs
	2. Development of collaborative contrastive decoding
	3. Demonstration of improved factuality in LLM outputs across tasks

**Result:** The CCD method effectively decreased the prevalence of fawning hallucinations and enhanced the factual correctness of LLM outputs across multiple NLP tasks.

**Limitations:** 

**Conclusion:** By introducing CCD, the research provides a viable approach to improve the precision of responses from LLMs, thus making them more trustworthy in applications.

**Abstract:** Large language models (LLMs) have demonstrated exceptional proficiency in language understanding. However, when LLMs align their outputs with deceptive and/or misleading prompts, the generated responses could deviate from the de facto information. Such observations are known as fawning hallucinations, where the model prioritizes alignment with the input's implied perspective over accuracy and truthfulness. In this work, we analyze fawning hallucinations in various natural language processing tasks and tailor the so-termed contrastive decoding method for fawning-hallucination mitigation. Specifically, we design two paradigms to generate corresponding deceptive and/or misleading inputs for the consistent fawning hallucinations induction. Then, we propose the collaborative contrastive decoding (CCD) to handle the fawning hallucinations across different tasks in LLMs. By contrasting the deviation in output distribution between induced and transformed neutral inputs, the proposed CCD can reduce reliance on deceptive and/or misleading information without requiring additional training. Extensive experiments demonstrate that the proposed CCD can effectively mitigate fawning hallucinations and improve the factuality of the generated responses over various tasks.

</details>


### [80] [EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes](https://arxiv.org/abs/2509.00877)

*Yuqin Dai, Guoqing Wang, Yuan Wang, Kairan Dou, Kaichen Zhou, Zhanwei Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Can Yi, Changhua Meng, Yuchen Zhou, Yongliang Shen, Shuai Lu*

**Main category:** cs.CL

**Keywords:** Large Language Models, retrieval-augmented generation, open-domain question answering, evidence quality, human-like reasoning

**Relevance Score:** 8

**TL;DR:** EviNote-RAG enhances open-domain QA by introducing a retrieve--note--answer pipeline that composes supportive evidence notes to improve reasoning and reduce noise.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in conventional retrieve--then--answer paradigms, specifically low signal-to-noise ratios in evidence and error accumulation in multi-hop reasoning.

**Method:** The proposed framework, EviNote-RAG, replaces direct reasoning over raw retrievals with a structured pipeline that creates Supportive-Evidence Notes (SENs) and employs an Evidence Quality Reward (EQR) for robust reasoning.

**Key Contributions:**

	1. Introduction of Supportive-Evidence Notes (SENs) for concise information capture
	2. Implementation of Evidence Quality Reward (EQR) to guide reasoning
	3. Demonstrated state-of-the-art performance with significant F1 improvements in various benchmarks.

**Result:** EviNote-RAG demonstrates significant improvements in accuracy, generalization, and stability over strong baselines, achieving state-of-the-art results across several QA benchmarks with notable F1 score gains.

**Limitations:** 

**Conclusion:** The introduction of SENs and EQR leads to more faithful reasoning and improved performance in QA tasks while managing noise more effectively.

**Abstract:** Large Language Models (LLMs) empowered with retrieval mechanisms have achieved strong progress in open-domain question answering (QA). Yet, the conventional retrieve--then--answer paradigm often suffers from two key limitations: (1) low signal-to-noise ratio in retrieved evidence, where useful information is buried under irrelevant content, and (2) error accumulation in multi-hop reasoning when incomplete or noisy passages are involved. To address these challenges, we present EviNote-RAG, an agentic RAG framework that introduces a structured retrieve--note--answer pipeline. Instead of directly reasoning over raw retrievals, the model is trained to compose Supportive-Evidence Notes (SENs), concise, human-like notes that preserve only answer-relevant information, highlight uncertainty, and explicitly state when no useful evidence exists. This distillation process is further reinforced by the Evidence Quality Reward (EQR), an entailment-based signal that evaluates whether SENs logically support the final answer. Together, SENs and EQR guide the model toward faithful and robust reasoning, while reducing the impact of noise. Experiments on in-domain and out-of-domain QA benchmarks show that EviNote-RAG consistently outperforms strong baselines in accuracy, generalization, and training stability. In particular, it achieves state-of-the-art results while enhancing robustness and efficiency, yielding relative F1 gains of 20\% on HotpotQA (+0.093), 40\% on Bamboogle (+0.151), and 91\% on 2Wiki (+0.256) via denser rewards and reduced verbosity.

</details>


### [81] [SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset](https://arxiv.org/abs/2509.00893)

*Răzvan-Alexandru Smădu, Andreea Iuga, Dumitru-Clementin Cercel, Florin Pop*

**Main category:** cs.CL

**Keywords:** satire detection, large language models, machine learning, natural language processing, dataset

**Relevance Score:** 4

**TL;DR:** This paper introduces a dataset for detecting satire in Romanian news articles and evaluates various language models for this task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the confusion between satire and factual reporting in news articles, particularly in the context of Romanian satire detection.

**Method:** Development of the SeLeRoSa dataset comprising 13,873 annotated sentences and evaluation of various language models, including LLMs, in both zero-shot and fine-tuning scenarios.

**Key Contributions:**

	1. Introduction of the SeLeRoSa dataset for Romanian satire detection
	2. Evaluation of LLMs and transformer-based models on satire detection
	3. Identification of limitations in current model performances

**Result:** Evaluation revealed limitations of current models in accurately detecting satire at the sentence level.

**Limitations:** Current models performed inadequately in detecting satire, indicating a need for improved techniques.

**Conclusion:** The study highlights the necessity for further research in enhancing model performance for satire detection.

**Abstract:** Satire, irony, and sarcasm are techniques typically used to express humor and critique, rather than deceive; however, they can occasionally be mistaken for factual reporting, akin to fake news. These techniques can be applied at a more granular level, allowing satirical information to be incorporated into news articles. In this paper, we introduce the first sentence-level dataset for Romanian satire detection for news articles, called SeLeRoSa. The dataset comprises 13,873 manually annotated sentences spanning various domains, including social issues, IT, science, and movies. With the rise and recent progress of large language models (LLMs) in the natural language processing literature, LLMs have demonstrated enhanced capabilities to tackle various tasks in zero-shot settings. We evaluate multiple baseline models based on LLMs in both zero-shot and fine-tuning settings, as well as baseline transformer-based models. Our findings reveal the current limitations of these models in the sentence-level satire detection task, paving the way for new research directions.

</details>


### [82] [Supervised In-Context Fine-Tuning for Generative Sequence Labeling](https://arxiv.org/abs/2509.00921)

*David Dukić, Goran Glavaš, Jan Šnajder*

**Main category:** cs.CL

**Keywords:** sequence labeling, causal LLMs, supervised generative SL, in-context learning, fine-tuning

**Relevance Score:** 8

**TL;DR:** The paper proposes supervised in-context fine-tuning (SIFT) for sequence labeling tasks, demonstrating that causal LLMs can outperform traditional encoder models through a new approach to generative SL.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of causal LLMs in sequence labeling tasks and address the limitations of existing encoder models, especially in the context of supervised generative SL.

**Method:** The paper introduces SIFT, which combines in-context learning (ICL) from demonstrations with supervised fine-tuning to treat SL tasks as constrained response generation.

**Key Contributions:**

	1. Introduction of supervised in-context fine-tuning (SIFT) for generative SL tasks.
	2. Empirical demonstration of SIFT's superiority over existing methods.
	3. Insights into the impact of long context and instruction removal on SL performance.

**Result:** SIFT shows significant performance improvements over both in-context learning and traditional decoder-as-encoder fine-tuning approaches across several sequence labeling tasks.

**Limitations:** Performance can be hindered by long context lengths, which can be mitigated by modifying instructions.

**Conclusion:** The study demonstrates that removing unnecessary instructions can improve performance in generative SL and emphasizes the effectiveness of a response-based formulation.

**Abstract:** Sequence labeling (SL) tasks, where labels are assigned to tokens, are abundant in NLP (e.g., named entity recognition and aspect-based sentiment analysis). Owing to the intuition that they require bidirectional context, SL tasks are commonly tackled with encoder-only models. Recent work also shows that removing the causal mask in fine-tuning enables decoder-based LLMs to become effective token classifiers. Less work, however, focused on (supervised) generative SL, a more natural setting for causal LLMs. Due to their rapid scaling, causal LLMs applied to SL are expected to outperform encoders, whose own development has stagnated. In this work, we propose supervised in-context fine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained response generation, natural to LLMs, combining (1) in-context learning (ICL) from demonstrations with (2) supervised fine-tuning. SIFT considerably outperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of standard SL tasks. We further find that although long context hinders the performance of generative SL in both ICL and SIFT, this deficiency can be mitigated by removing the instruction, as instructions are shown to be largely unnecessary for achieving strong SL performance with SIFT. Our findings highlight strengths and limitations of SL with LLMs, underscoring the importance of a response-based generative task formulation for effective SL performance.

</details>


### [83] [MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework](https://arxiv.org/abs/2509.00934)

*Md Shahidul Salim, Lian Fu, Arav Adikesh Ramakrishnan, Zonghai Yao, Hong Yu*

**Main category:** cs.CL

**Keywords:** medical translation, large language models, domain-specific knowledge

**Relevance Score:** 9

**TL;DR:** MedCOD improves English-to-Spanish medical translation by integrating domain-specific knowledge into LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved medical translation quality between English and Spanish, focusing on incorporating structured medical knowledge into LLMs.

**Method:** The study employs a hybrid framework (MedCOD) that leverages the Unified Medical Language System (UMLS) alongside LLMs to enhance prompting and fine-tuning, tested on a parallel corpus of medical articles.

**Key Contributions:**

	1. Introduction of MedCOD for medical translation
	2. Integration of UMLS knowledge into LLMs
	3. Demonstrated performance improvements across multiple LLMs

**Result:** MedCOD significantly enhances translation quality across various LLMs, with the Phi-4 model achieving a BLEU score of 44.23, outperforming baseline models such as GPT-4o.

**Limitations:** 

**Conclusion:** Integrating structured medical knowledge improves LLM performance in translation tasks, as confirmed by experimental results and ablation studies.

**Abstract:** We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed to improve English-to-Spanish medical translation by integrating domain-specific structured knowledge into large language models (LLMs). MedCOD integrates domain-specific knowledge from both the Unified Medical Language System (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance structured prompting and fine-tuning. We constructed a parallel corpus of 2,999 English-Spanish MedlinePlus articles and a 100-sentence test set annotated with structured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B, Qwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that incorporated multilingual variants, medical synonyms, and UMLS-derived definitions, combined with LoRA-based fine-tuning. Experimental results demonstrate that MedCOD significantly improves translation quality across all models. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23, chrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o and GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model adaptation independently contribute to performance gains, with their combination yielding the highest improvements. These findings highlight the potential of structured knowledge integration to enhance LLMs for medical translation tasks.

</details>


### [84] [Structure and Destructure: Dual Forces in the Making of Knowledge Engines](https://arxiv.org/abs/2509.00949)

*Yihong Chen*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Knowledge Engines, Large Language Models, Knowledge Graphs, Model Adaptability

**Relevance Score:** 8

**TL;DR:** The thesis connects structured and unstructured paradigms in natural language processing to propose a framework for developing adaptable knowledge engines.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between structured symbolic interactions (e.g., knowledge graphs) and unstructured data-driven models (e.g., large language models).

**Method:** Explores the interplay between structure and destructure in knowledge engineering, utilizing periodic embedding resets for enhanced model plasticity.

**Key Contributions:**

	1. Establishes conceptual connections between structure and unstructured paradigms in NLP.
	2. Introduces the dual concepts of structure and destructure in knowledge engine development.
	3. Proposes a framework for creating adaptable and controllable intelligent systems.

**Result:** Establishes a conceptual framework that promotes the creation of intelligent systems that are transparent, controllable, and adaptable.

**Limitations:** 

**Conclusion:** The proposed connections between the two paradigms offer a new recipe for developing robust knowledge engines.

**Abstract:** The making of knowledge engines in natural language processing has been shaped by two seemingly distinct paradigms: one grounded in structure, the other driven by massively available unstructured data. The structured paradigm leverages predefined symbolic interactions, such as knowledge graphs, as priors and designs models to capture them. In contrast, the unstructured paradigm centers on scaling transformer architectures with increasingly vast data and model sizes, as seen in modern large language models. Despite their divergence, this thesis seeks to establish conceptual connections bridging these paradigms. Two complementary forces, structure and destructure, emerge across both paradigms: structure organizes seen symbolic interactions, while destructure, through periodic embedding resets, improves model plasticity and generalization to unseen scenarios. These connections form a new recipe for developing general knowledge engines that can support transparent, controllable, and adaptable intelligent systems.

</details>


### [85] [RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning](https://arxiv.org/abs/2509.00974)

*Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Feng Liu, Fang-Ming Hung*

**Main category:** cs.CL

**Keywords:** medical question answering, reinforcement learning, large language models

**Relevance Score:** 9

**TL;DR:** Proposes a novel framework, Ranked Preference Reinforcement Optimization (RPRO), to improve reasoning accuracy in medical question answering by combining reinforcement learning with preference-driven reasoning refinement.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need for advanced reasoning in medical question answering that integrates domain knowledge with logical inference and addresses the factual inaccuracy of existing LLMs.

**Method:** RPRO introduces task-adaptive reasoning templates and a probabilistic evaluation mechanism, using groupwise ranking optimization based on the Bradley-Terry model and KL-divergence regularization for stable training.

**Key Contributions:**

	1. Introduction of Ranked Preference Reinforcement Optimization (RPRO) framework.
	2. Use of task-adaptive reasoning templates for better output quality.
	3. Groupwise ranking optimization method that enhances clinical workflow alignment.

**Result:** Experiments show that RPRO achieves consistent improvements on PubMedQA and MedQA-USMLE compared to strong baseline models, with a 1.1B parameter model outperforming larger 7B-13B models.

**Limitations:** 

**Conclusion:** The study demonstrates that combining preference optimization with quality-driven refinement can create more reliable, clinically grounded medical LLMs.

**Abstract:** Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley-Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B-13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs.

</details>


### [86] [A Computational Method for Measuring "Open Codes" in Qualitative Analysis](https://arxiv.org/abs/2411.12142)

*John Chen, Alexandros Lotsos, Sihan Cheng, Caiyi Wang, Lexie Zhao, Jessica Hullman, Bruce Sherin, Uri Wilensky, Michael Horn*

**Main category:** cs.CL

**Keywords:** inductive coding, Generative AI, qualitative analysis, LLM, human-AI collaboration

**Relevance Score:** 7

**TL;DR:** This paper presents a computational method for measuring inductive coding results achieved by humans and Generative AI, addressing the challenges of qualitative analysis in social sciences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the methodological rigor in inductive coding, especially as researchers adopt Generative AI, while addressing the limitations of traditional evaluation methods.

**Method:** The method merges individual codebooks using an LLM-enriched algorithm to create a unified result, measuring contributions with four metrics: Coverage, Overlap, Novelty, and Divergence.

**Key Contributions:**

	1. Development of a computational method for inductive coding results
	2. Introduction of four novel metrics for evaluating coding contributions
	3. Validation of the metrics' robustness across multiple LLMs

**Result:** The experiments reveal the merging algorithm's impact and validate the stability of the metrics, highlighting their effectiveness in diagnosing relevant coding issues.

**Limitations:** 

**Conclusion:** The proposed method offers a robust framework for ensuring quality in qualitative analysis involving human and AI collaboration.

**Abstract:** Qualitative analysis is critical to understanding human datasets in many social science disciplines. A central method in this process is inductive coding, where researchers identify and interpret codes directly from the datasets themselves. Yet, this exploratory approach poses challenges for meeting methodological expectations (such as ``depth'' and ``variation''), especially as researchers increasingly adopt Generative AI (GAI) for support. Ground-truth-based metrics are insufficient because they contradict the exploratory nature of inductive coding, while manual evaluation can be labor-intensive. This paper presents a theory-informed computational method for measuring inductive coding results from humans and GAI. Our method first merges individual codebooks using an LLM-enriched algorithm. It measures each coder's contribution against the merged result using four novel metrics: Coverage, Overlap, Novelty, and Divergence. Through two experiments on a human-coded online conversation dataset, we 1) reveal the merging algorithm's impact on metrics; 2) validate the metrics' stability and robustness across multiple runs and different LLMs; and 3) showcase the metrics' ability to diagnose coding issues, such as excessive or irrelevant (hallucinated) codes. Our work provides a reliable pathway for ensuring methodological rigor in human-AI qualitative analysis.

</details>


### [87] [Performance Analysis of Supervised Machine Learning Algorithms for Text Classification](https://arxiv.org/abs/2509.00983)

*Sadia Zaman Mishu, S M Rafiuddin*

**Main category:** cs.CL

**Keywords:** text classification, supervised machine learning, Artificial Neural Networks

**Relevance Score:** 4

**TL;DR:** This paper discusses the growing demand for text classification in various fields and evaluates different supervised machine learning techniques on labeled datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The demand for effective text classification is increasing in sectors like web searching, data mining, and recommendation systems, necessitating thorough exploration of machine learning techniques.

**Method:** The paper utilizes several standard supervised machine learning classifiers, including an Artificial Neural Network model with Back Propagation Network, to classify labeled text documents and measures their accuracy.

**Key Contributions:**

	1. Demonstration of supervised machine learning techniques for text classification
	2. Comparison of different classifiers on various datasets
	3. Establishment of a platform for analyzing supervised text classification models

**Result:** Experimental analysis indicates the performance of different classifiers on real data, highlighting their respective accuracy levels in text classification tasks.

**Limitations:** 

**Conclusion:** The study exemplifies the effectiveness of using multiple classification models for text classification and provides insights into which classifiers deliver higher accuracy.

**Abstract:** The demand for text classification is growing significantly in web searching, data mining, web ranking, recommendation systems, and so many other fields of information and technology. This paper illustrates the text classification process on different datasets using some standard supervised machine learning techniques. Text documents can be classified through various kinds of classifiers. Labeled text documents are used to classify the text in supervised classifications. This paper applies these classifiers on different kinds of labeled documents and measures the accuracy of the classifiers. An Artificial Neural Network (ANN) model using Back Propagation Network (BPN) is used with several other models to create an independent platform for labeled and supervised text classification process. An existing benchmark approach is used to analyze the performance of classification using labeled documents. Experimental analysis on real data reveals which model works well in terms of classification accuracy.

</details>


### [88] [Ranking of Bangla Word Graph using Graph-based Ranking Algorithms](https://arxiv.org/abs/2509.01011)

*S M Rafiuddin*

**Main category:** cs.CL

**Keywords:** Bangla words, word graph, ranking algorithms, graph-based methods, information retrieval

**Relevance Score:** 3

**TL;DR:** The paper investigates ranking Bangla words using graph-based algorithms by constructing word graphs to analyze relationships among words.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a method for ranking Bangla words due to the absence of a standard word database and to enhance information retrieval and text summarization.

**Method:** The study utilizes the Indian Language POS-tag Corpora to create word graphs and applies various graph-based ranking algorithms after preprocessing the data.

**Key Contributions:**

	1. Introduction of a word graph approach for Bangla word ranking
	2. Comparison of different graph-based ranking algorithms
	3. Use of Indian Language POS-tag Corpora for Bangla words

**Result:** The paper shows that the experimental results reveal the accuracy of the ranking algorithms using the F1 measure on real data.

**Limitations:** Dependence on the availability of a standard Bangla word database; performance limited to the algorithms evaluated in the study.

**Conclusion:** The proposed method demonstrates a systematic approach for calculating the ranking of Bangla words, useful for text summarization and information retrieval.

**Abstract:** Ranking words is an important way to summarize a text or to retrieve information. A word graph is a way to represent the words of a sentence or a text as the vertices of a graph and to show the relationship among the words. It is also useful to determine the relative importance of a word among the words in the word-graph. In this research, the ranking of Bangla words are calculated, representing Bangla words from a text in a word graph using various graph based ranking algorithms. There is a lack of a standard Bangla word database. In this research, the Indian Language POS-tag Corpora is used, which has a rich collection of Bangla words in the form of sentences with their parts of speech tags. For applying a word graph to various graph based ranking algorithms, several standard procedures are applied. The preprocessing steps are done in every word graph and then applied to graph based ranking algorithms to make a comparison among these algorithms. This paper illustrate the entire procedure of calculating the ranking of Bangla words, including the construction of the word graph from text. Experimental result analysis on real data reveals the accuracy of each ranking algorithm in terms of F1 measure.

</details>


### [89] [We Politely Insist: Your LLM Must Learn the Persian Art of Taarof](https://arxiv.org/abs/2509.01035)

*Nikta Gohari Sadr, Sahar Heidariasl, Karine Megerdoomian, Laleh Seyyed-Kalantari, Ali Emami*

**Main category:** cs.CL

**Keywords:** large language models, taarof, cultural competence, benchmarking, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces TaarofBench, a benchmark for evaluating large language models' understanding of the Persian cultural norm of taarof, highlighting significant gaps in their cultural competence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in navigating culturally specific communication norms, specifically focusing on the Persian social norm of taarof.

**Method:** The study evaluates five frontier large language models using TaarofBench, which comprises 450 role-play scenarios covering 12 social interaction topics, along with supervised fine-tuning and Direct Preference Optimization for model improvement.

**Key Contributions:**

	1. Introduction of TaarofBench as a novel evaluation benchmark for LLMs regarding cultural competence.
	2. Demonstration of significant gaps in LLM performance in understanding Persian taarof.
	3. Implementation of supervised fine-tuning methods leading to notable improvements in adherence to cultural norms.

**Result:** The evaluation reveals accuracy rates 40-48% lower than native speakers in culturally appropriate contexts and significant performance improvements with tailored prompts and optimization techniques.

**Limitations:** Focus on a specific cultural norm (taarof) may not generalize to other cultural contexts; limited sample size of participants in the human study.

**Conclusion:** The research demonstrates the importance of developing culturally aware language models and provides a foundational benchmark for enhancing their performance in diverse social contexts.

**Abstract:** Large language models (LLMs) struggle to navigate culturally specific communication norms, limiting their effectiveness in global contexts. We focus on Persian taarof, a social norm in Iranian interactions, which is a sophisticated system of ritual politeness that emphasizes deference, modesty, and indirectness, yet remains absent from existing cultural benchmarks. We introduce TaarofBench, the first benchmark for evaluating LLM understanding of taarof, comprising 450 role-play scenarios covering 12 common social interaction topics, validated by native speakers. Our evaluation of five frontier LLMs reveals substantial gaps in cultural competence, with accuracy rates 40-48% below native speakers when taarof is culturally appropriate. Performance varies between interaction topics, improves with Persian-language prompts, and exhibits gender-based asymmetries. We also show that responses rated "polite" by standard metrics often violate taarof norms, indicating the limitations of Western politeness frameworks. Through supervised fine-tuning and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in model alignment with cultural expectations. Our human study with 33 participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers) forms baselines in varying degrees of familiarity with Persian norms. This work lays the foundation for developing diverse and culturally aware LLMs, enabling applications that better navigate complex social interactions.

</details>


### [90] [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)

*Xiaoying Song, Anirban Saha Anik, Eduardo Blanco, Vanessa Frias-Martinez, Lingzi Hong*

**Main category:** cs.CL

**Keywords:** Crisis communication, Language models, Style consistency, Human-computer interaction, Natural language generation

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach to ensure consistent response styles in automated crisis communications driven by language models, focusing on trust-building with affected populations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the urgent need for effective communication in crisis situations and highlights the overlooked importance of stylistic consistency in maintaining trust with crisis-affected individuals.

**Method:** The proposed methodology includes a two-stage process: first assessing the style of candidate responses and then optimizing and integrating them through a fusion process to ensure consistency.

**Key Contributions:**

	1. Introduction of a novel metric for evaluating style consistency
	2. Development of a fusion-based generation approach
	3. Demonstration of improved response quality and stylistic uniformity across datasets

**Result:** Experimental results show that the proposed method outperforms baseline approaches in both the quality of responses and the uniformity of stylistic elements across generated answers.

**Limitations:** 

**Conclusion:** The study concludes that maintaining stylistic consistency in automated responses can enhance trust and communication effectiveness in crisis scenarios.

**Abstract:** In response to the urgent need for effective communication with crisis-affected populations, automated responses driven by language models have been proposed to assist in crisis communications. A critical yet often overlooked factor is the consistency of response style, which could affect the trust of affected individuals in responders. Despite its importance, few studies have explored methods for maintaining stylistic consistency across generated responses. To address this gap, we propose a novel metric for evaluating style consistency and introduce a fusion-based generation approach grounded in this metric. Our method employs a two-stage process: it first assesses the style of candidate responses and then optimizes and integrates them at the instance level through a fusion process. This enables the generation of high-quality responses while significantly reducing stylistic variation between instances. Experimental results across multiple datasets demonstrate that our approach consistently outperforms baselines in both response quality and stylistic uniformity.

</details>


### [91] [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)

*Xiaoying Song, Anirban Saha Anik, Dibakar Barua, Pengcheng Luo, Junhua Ding, Lingzi Hong*

**Main category:** cs.CL

**Keywords:** health misinformation, counterspeech, health literacy, retrieval-augmented generation, reinforcement learning

**Relevance Score:** 9

**TL;DR:** The paper introduces a Controlled-Literacy framework for generating tailored counterspeech to health misinformation, adapting to different audience health literacy levels using retrieval-augmented generation and reinforcement learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Health misinformation poses a major risk to public health, and effective counterspeech is needed to mitigate this threat.

**Method:** The Controlled-Literacy framework utilizes retrieval-augmented generation (RAG) combined with reinforcement learning (RL) to tailor counterspeech based on specified health literacy levels, integrating knowledge retrieval and a novel reward function that includes user preferences and readability.

**Key Contributions:**

	1. Introduction of the Controlled-Literacy framework for tailored counterspeech
	2. Integration of retrieval-augmented generation and reinforcement learning
	3. Development of a reward function that focuses on user preferences and readability

**Result:** Experiment results demonstrate that Controlled-Literacy generates more accessible and user-preferred counterspeech compared to existing baselines.

**Limitations:** 

**Conclusion:** This research enhances public health communication by ensuring that counterspeech to health misinformation is more equitable and comprehensible, tailored to varying health literacy levels.

**Abstract:** Health misinformation spreading online poses a significant threat to public health. Researchers have explored methods for automatically generating counterspeech to health misinformation as a mitigation strategy. Existing approaches often produce uniform responses, ignoring that the health literacy level of the audience could affect the accessibility and effectiveness of counterspeech. We propose a Controlled-Literacy framework using retrieval-augmented generation (RAG) with reinforcement learning (RL) to generate tailored counterspeech adapted to different health literacy levels. In particular, we retrieve knowledge aligned with specific health literacy levels, enabling accessible and factual information to support generation. We design a reward function incorporating subjective user preferences and objective readability-based rewards to optimize counterspeech to the target health literacy level. Experiment results show that Controlled-Literacy outperforms baselines by generating more accessible and user-preferred counterspeech. This research contributes to more equitable and impactful public health communication by improving the accessibility and comprehension of counterspeech to health misinformation.

</details>


### [92] [Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation](https://arxiv.org/abs/2509.01081)

*Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutez Al-Khatib, Mohammed Ghaly*

**Main category:** cs.CL

**Keywords:** Large Language Models, Islamic inheritance law, legal reasoning, domain adaptation, error analysis

**Relevance Score:** 4

**TL;DR:** Evaluation of LLMs in Islamic inheritance law shows variable accuracy; top models surpass 90%, others struggle below 50%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLMs' reasoning capabilities in Islamic inheritance law and identify performance disparities among them.

**Method:** Seven LLMs were tested using a benchmark of 1,000 multiple-choice questions related to Islamic inheritance scenarios.

**Key Contributions:**

	1. Benchmarking of LLMs in a specific legal context
	2. Identification of reasoning gaps in handling inheritance law
	3. Error analysis revealing misunderstanding patterns among models.

**Result:** Top models (o3 and Gemini 2.5) scored above 90% accuracy, while others (ALLaM, Fanar, LLaMA, Mistral) were below 50% accuracy.

**Limitations:** Limited to Islamic inheritance law; findings may not generalize to other legal domains.

**Conclusion:** Performance gaps highlight challenges in structured legal reasoning; improvements and adaptations are necessary for better performance in Islamic legal contexts.

**Abstract:** This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: https://github.com/bouchekif/inheritance_evaluation

</details>


### [93] [A Paradigm Gap in Urdu](https://arxiv.org/abs/2509.01084)

*Farah Adeeba, Rajesh Bhatt*

**Main category:** cs.CL

**Keywords:** Urdu, Hindi, Grammatical Shift, Morphosyntax, Perfective Form

**Relevance Score:** 2

**TL;DR:** The paper explores a grammatical gap in modern Urdu and Hindi regarding the perfective form of the -ya: kar construction, which was once common in 19th century literature but is now considered ungrammatical.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the diachronic shift in the grammatical status of perfective forms in Urdu and Hindi.

**Method:** The study employs historical text analysis, a large-scale corpus study, and subjective evaluation tasks with native speakers to assess acceptability.

**Key Contributions:**

	1. Identification of a grammatical gap in modern Urdu and Hindi
	2. Historical analysis linking 19th century literature with contemporary usage
	3. Psycholinguistic evidence from native speaker evaluations

**Result:** The research confirms the absence of perfective forms in modern usage, with speakers judging them as unnatural. It identifies a morphosyntactic conflict as the cause of this grammatical gap.

**Limitations:** 

**Conclusion:** The perfective form became unstable due to the conflict between the construction's requirements and transitive perfective case assignment, leading to its replacement in modern grammar.

**Abstract:** In this paper, we document a paradigm gap in the combinatorial possibilities of verbs and aspect in Urdu: the perfective form of the -ya: kar construction (e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and Hindi, despite being freely attested in 19th century literature. We investigate this diachronic shift through historical text analysis, a large-scale corpus study which confirms the stark absence of perfective forms and subjective evaluation tasks with native speakers, who judge perfective examples as highly unnatural. We argue that this gap arose from a fundamental morphosyntactic conflict: the construction's requirement for a nominative subject and an invariant participle clashes with the core grammatical rule that transitive perfective assign ergative case. This conflict rendered the perfective form unstable, and its functional replacement by other constructions allowed the gap to become entrenched in the modern grammar.

</details>


### [94] [Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation](https://arxiv.org/abs/2509.01088)

*Jinwen Chen, Hainan Zhang, Liang Pang, Yongxin Tong, Haibo Zhou, Yuan Zhan, Wei Lin, Zhiming Zheng*

**Main category:** cs.CL

**Keywords:** parametric RAG, distillation, privacy-preserving reasoning, cross-document QA, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces DistilledPRAG, a privacy-preserving knowledge-distilled approach to improve the efficiency and generalization of retrieval-augmented generation (RAG) systems without exposing raw document content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns in current RAG systems that require uploading plaintext documents to the cloud, risking data leakage.

**Method:** DistilledPRAG enhances document reasoning by synthesizing QA pairs from documents while using a parameter generator to translate masked plaintext documents into LoRA format, maintaining RAG structure.

**Key Contributions:**

	1. Introduction of DistilledPRAG as a knowledge-distilled parametric RAG model.
	2. Enhancement of cross-document reasoning through synthesized QA pairs.
	3. Demonstration of improved accuracy and generalization on OOD data compared to baseline models.

**Result:** DistilledPRAG outperforms existing methods in accuracy and demonstrates improved generalization to out-of-distribution (OOD) input data across four QA datasets.

**Limitations:** The study does not address potential scalability issues with the parameter generator for very large document sets.

**Conclusion:** The proposed model effectively balances privacy and performance, achieving efficient parameterization while retaining the core advantages of standard RAG systems.

**Abstract:** The current RAG system requires uploading plaintext documents to the cloud, risking private data leakage. Parametric RAG (PRAG) addresses this by encoding documents as LoRA within LLMs, enabling reasoning without exposing raw content. However, it still faces two issues: (1) PRAG demands synthesizing QA pairs and fine-tuning LLM for each individual document to create its corresponding LoRA, leading to unacceptable inference latency. (2) The performance of PRAG relies solely on synthetic QA data, lacking internal alignment with standard RAG, resulting in poor generalization on out-of-distribution(OOD) inputs. Therefore, achieving high-efficiency parameterization while maintaining RAG-level performance remains a critical challenge for privacy-preserving reasoning. In this paper, we propose DistilledPRAG, a generalizable knowledge-distilled parametric RAG model aligned with standard RAG in document structure and parameter activation. We first synthesize QA pairs from single and multi-documents to enhance cross-document reasoning. Then, we mask the plaintext documents with a special token and translate them to LoRA via a parameter generator, maintaining the standard RAG document structure. Finally, guided by synthetic QA data, we train the parameter generator to match standard RAG's hidden states and output logits, enabling RAG-style reasoning without original documents. Experiments on four QA datasets show that DistilledPRAG outperforms baselines in accuracy and generalizes well on OOD data.

</details>


### [95] [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)

*Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Decoding Framework, Efficiency, Long-Context Inputs

**Relevance Score:** 9

**TL;DR:** REFRAG is an efficient decoding framework designed to reduce latency in retrieval-augmented generation (RAG) applications by optimizing long-context input processing, achieving significant speedups without sacrificing performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-off between knowledge enrichment and system efficiency in LLMs when processing long-context inputs in RAG applications.

**Method:** REFRAG utilizes a compress-sense-expand approach to eliminate unnecessary computations during decoding, capitalizing on the sparsity structure of concatenated retrieval passages, which are generally low in semantic similarity.

**Key Contributions:**

	1. Introduction of the REFRAG framework for efficient decoding in RAG applications
	2. Realization of significant speedup in time-to-first-token without accuracy loss
	3. Extension of the context size of LLMs for improved performance on long-context inputs

**Result:** REFRAG achieved a 30.85 times acceleration in time-to-first-token and a 3.75 improvement over previous work, extending the context size of LLMs by 16 without loss in perplexity.

**Limitations:** 

**Conclusion:** The experimental validation of REFRAG demonstrates substantial speedup across diverse long-context tasks with maintained accuracy, proving its utility in enhancing LLM performance in RAG scenarios.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.

</details>


### [96] [Natural Context Drift Undermines the Natural Language Understanding of Large Language Models](https://arxiv.org/abs/2509.01093)

*Yulong Wu, Viktor Schlegel, Riza Batista-Navarro*

**Main category:** cs.CL

**Keywords:** Large Language Models, Question Answering, Text Evolution, Semantic Similarity, Natural Language Understanding

**Relevance Score:** 9

**TL;DR:** The study investigates how natural evolution of text passages affects the performance of generative Large Language Models in question answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of naturally evolved, human-edited text on the performance of Large Language Models in question answering.

**Method:** A framework was proposed to curate variants of reading passages and evaluate LLM performance across various semantic similarity scores.

**Key Contributions:**

	1. Development of a framework for evaluating text evolution in QA benchmarks
	2. Insight into the impact of text variation on LLM performance
	3. Quantitative analysis of the relationship between text similarity and model accuracy

**Result:** LLM performance declines significantly as the text diverges from pretraining data, with accuracy on BoolQ dropping by over 30% across similarity bins, indicating the challenge posed by text evolution to LLM understanding.

**Limitations:** Limitations of the study include potential biases in the selection of QA datasets and LLMs used for evaluation.

**Conclusion:** Natural text evolution significantly challenges the language understanding capabilities of LLMs, affecting their performance in question answering tasks.

**Abstract:** How does the natural evolution of context paragraphs affect question answering in generative Large Language Models (LLMs)? To investigate this, we propose a framework for curating naturally evolved, human-edited variants of reading passages from contemporary QA benchmarks and for analyzing LLM performance across a range of semantic similarity scores, which quantify how closely each variant aligns with content seen during pretraining. Using this framework, we evaluate six QA datasets and eight LLMs with publicly available training data. Our experiments reveal that LLM performance declines as reading passages naturally diverge from the versions encountered during pretraining-even when the question and all necessary information remains present at inference time. For instance, average model accuracy on BoolQ drops by over 30% from the highest to lowest similarity bins, with slopes exceeding 70 across several LLMs. These findings suggest that natural text evolution poses a significant challenge to the language understanding capabilities of LLMs.

</details>


### [97] [Dream-Coder 7B: An Open Diffusion Language Model for Code](https://arxiv.org/abs/2509.01142)

*Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, Lingpeng Kong*

**Main category:** cs.CL

**Keywords:** code generation, diffusion model, language model, reinforcement learning, open-source

**Relevance Score:** 9

**TL;DR:** Dream-Coder 7B is an open-source diffusion model for code generation that can adaptively change its decoding strategy based on the coding task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a language model that improves code generation capabilities beyond traditional autoregressive methods by utilizing a discrete diffusion framework for any-order generation.

**Method:** The model adapts a pretrained autoregressive checkpoint for discrete diffusion using a continuous-time weighted cross-entropy objective. Fine-tuning involves supervised methods to address padding issues and reinforcement learning guided by high-quality prompts.

**Key Contributions:**

	1. Introduction of Dream-Coder 7B, an open-source model for code generation.
	2. Demonstration of emergent any-order generation capabilities in a diffusion model.
	3. Release of checkpoints and training resources for reproducibility.

**Result:** Dream-Coder 7B Instruct achieves a 21.4% pass@1 score on LiveCodeBench and performs well on other benchmarks like HumanEval and BigCodeBench.

**Limitations:** 

**Conclusion:** The paper introduces an innovative method for code generation through a new model architecture and release of all supporting materials to aid further research.

**Abstract:** We present Dream-Coder 7B, an open-source discrete diffusion language model for code generation that exhibits emergent any-order generation capabilities. Unlike traditional autoregressive (AR) models that decode strictly left-to-right, Dream-Coder 7B adaptively determines its decoding strategy based on the coding task: sketch-first generation for complex algorithms, left-to-right generation for straightforward completions, and interleaved reasoning generation for code understanding tasks. We adapt a pretrained AR checkpoint to a discrete diffusion frameworks with a continuous-time weighted cross-entropy objective. Our post-training recipe comprises (i) supervised fine-tuning, where we mitigate padding pathologies via random truncation and a padding penalty to improve sample efficiency and stabilize generation; and (ii) reinforcement learning with verifiable rewards over a curated high-quality prompt set drawn from open-source datasets, using a tailored reinforcement learning recipe for diffusion language models. The resulting Dream-Coder 7B Instruct attains 21.4\% pass@1 on LiveCodeBench (2410--2505) and demonstrates competitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We release Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training recipes, preprocessing pipelines, and inference code to facilitate reproducibility and further research.

</details>


### [98] [Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective](https://arxiv.org/abs/2509.01147)

*Zhihao Zhang, Sophia Yat Mei Lee, Dong Zhang, Shoushan Li, Guodong Zhou*

**Main category:** cs.CL

**Keywords:** cross-lingual NER, zero-shot learning, non-Latin scripts, large language models, entity alignment

**Relevance Score:** 7

**TL;DR:** This paper introduces an entity-aligned translation approach for improving cross-lingual named entity recognition in non-Latin script languages using large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing zero-shot CL-NER approaches that perform poorly for non-Latin script languages due to structural differences.

**Method:** The proposed EAT approach employs a dual-translation strategy to align entities between non-Latin script languages and English while fine-tuning large language models on multilingual Wikipedia data.

**Key Contributions:**

	1. Proposes an entity-aligned translation method for CL-NER in non-Latin scripts
	2. Implements a dual-translation strategy leveraging LLMs
	3. Fine-tunes LLMs using multilingual Wikipedia for better entity alignment

**Result:** The approach enhances entity recognition performance in non-Latin script languages by providing better alignment with English entities.

**Limitations:** Performance improvement remains context-dependent and may not generalize across all non-Latin languages.

**Conclusion:** The paper concludes that employing LLMs with a dual-translation strategy significantly improves entity alignment and recognition in cross-lingual contexts.

**Abstract:** Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge from high-resource languages to low-resource languages. However, existing zero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language (LSL), where shared linguistic features facilitate effective knowledge transfer. In contrast, for non-Latin script language (NSL), such as Chinese and Japanese, performance often degrades due to deep structural differences. To address these challenges, we propose an entity-aligned translation (EAT) approach. Leveraging large language models (LLMs), EAT employs a dual-translation strategy to align entities between NSL and English. In addition, we fine-tune LLMs using multilingual Wikipedia data to enhance the entity alignment from source to target languages.

</details>


### [99] [Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA](https://arxiv.org/abs/2509.01158)

*Xuemei Tang, Chengxi Yan, Jinghang Gu, Chu-Ren Huang*

**Main category:** cs.CL

**Keywords:** Chinese information extraction, multi-task learning, LoRA, Mixture-of-Experts, task-aware routing

**Relevance Score:** 4

**TL;DR:** Tea-MOELoRA is a multi-task framework for Chinese information extraction that uses low-rank parameter efficiency and a router mechanism to combine expertise from different tasks and eras effectively.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective information extraction from diverse Chinese documents across classical and modern eras, while avoiding performance interference in multi-task learning.

**Method:** Introduction of Tea-MOELoRA framework, which uses Mixture-of-Experts (MoE) combined with low-rank adaptation (LoRA) to specialize multiple experts for different tasks and era contexts, alongside a task-era-aware routing mechanism.

**Key Contributions:**

	1. Introduction of the Tea-MOELoRA framework for multi-task learning in information extraction.
	2. Combination of LoRA with MoE for parameter efficiency and task specialization.
	3. Utilization of a dynamic router mechanism to enhance expert performance based on task and era.

**Result:** Tea-MOELoRA outperforms both single-task and joint LoRA baselines in extracting information from heterogeneous tasks and temporal domains.

**Limitations:** 

**Conclusion:** Tea-MOELoRA demonstrates improved performance in Chinese IE by effectively utilizing task and temporal knowledge through specialized experts and dynamic routing.

**Abstract:** Chinese information extraction (IE) involves multiple tasks across diverse temporal domains, including Classical and Modern documents. Fine-tuning a single model on heterogeneous tasks and across different eras may lead to interference and reduced performance. Therefore, in this paper, we propose Tea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with a Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in different IE tasks and eras, while a task-era-aware router mechanism dynamically allocates expert contributions. Experiments show that Tea-MOELoRA outperforms both single-task and joint LoRA baselines, demonstrating its ability to leverage task and temporal knowledge effectively.

</details>


### [100] [Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning](https://arxiv.org/abs/2509.01166)

*Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Large Language Models, Reasoning, Hierarchical Knowledge Alignment, Instruction Tuning

**Relevance Score:** 9

**TL;DR:** This paper introduces SAT, a framework that enhances large language models for knowledge graph completion by addressing inconsistencies in representation spaces and optimizing instruction tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge graph completion by leveraging the reasoning capabilities of large language models while overcoming existing challenges in representation and task-specific instructions.

**Method:** The authors propose a framework called SAT that utilizes hierarchical knowledge alignment and structural instruction tuning for effective knowledge graph completion.

**Key Contributions:**

	1. Development of the SAT framework for KGC
	2. Introduction of hierarchical knowledge alignment
	3. Proposal of structural instruction tuning for unified graph instruction

**Result:** SAT significantly outperforms state-of-the-art methods in knowledge graph completion tasks, particularly in link prediction, with performance improvements between 8.7% and 29.8%.

**Limitations:** 

**Conclusion:** The results indicate that SAT provides a more efficient and effective means to enhance large language models for knowledge graph completion tasks.

**Abstract:** Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%.

</details>


### [101] [Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation](https://arxiv.org/abs/2509.01185)

*Seganrasan Subramanian, Abhigya Verma*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long-context processing, Data generation framework

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework for synthetic long-context data generation for large language models (LLMs) to enhance their ability to process long textual inputs, addressing the lack of diverse and verifiable datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in high-quality long-context datasets for training and evaluating large language models, which hampers their reasoning capabilities over long texts.

**Method:** A modular framework is developed for generating synthetic long-context data through prompt-based interactions with LLMs, supporting various training and alignment objectives.

**Key Contributions:**

	1. Introduction of a modular and extensible framework for long-context data generation.
	2. Support for multiple training and alignment objectives such as SFT and DPO.
	3. Creation of metadata-enriched outputs to enhance dataset verifiability.

**Result:** The framework produces four core types of long-context data, including conversational dialogues and reasoning examples, enabling scalable and controllable dataset creation for enhancing LLM capabilities.

**Limitations:** 

**Conclusion:** The proposed approach facilitates advancements in long-context handling for LLMs by enabling purpose-aligned dataset creation, thus improving their practical applications.

**Abstract:** The ability of large language models (LLMs) to process and reason over long textual inputs is critical for a wide range of real-world applications. However, progress in this area is significantly constrained by the absence of high-quality, diverse, and verifiable long-context datasets suitable for both training and evaluation. This work introduces a modular, extensible framework for synthetic long-context data generation via prompt-based interaction with LLMs. The framework supports multiple training and alignment objectives, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). It encompasses four core generation paradigms: multi-turn conversational dialogues, document-grounded input-output pairs, verifiable instruction-response tasks, and long-context reasoning examples. Through templated prompting, a model-agnostic architecture, and metadata-enriched outputs, the proposed approach facilitates scalable, controllable, and purpose-aligned dataset creation for advancing long-context capabilities in LLMs.

</details>


### [102] [Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)

*Luxi He, Nimra Nadeem, Michel Liao, Howard Chen, Danqi Chen, Mariano-Florentino Cuéllar, Peter Henderson*

**Main category:** cs.CL

**Keywords:** AI alignment, interpretive ambiguity, legal theory, rule refinement, judgment consistency

**Relevance Score:** 8

**TL;DR:** The paper addresses interpretive ambiguity in AI alignment processes, proposing a framework inspired by legal systems to enhance consistency in rule application.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** AI systems face challenges due to interpretive ambiguity stemming from natural language governance, similar to legal systems.

**Method:** The authors create a computational framework that includes a rule refinement pipeline and prompt-based interpretive constraints, analogous to mechanisms in legal systems.

**Key Contributions:**

	1. Proposes a new computational framework to address interpretive ambiguity in AI.
	2. Applies legal theory to AI alignment processes.
	3. Demonstrates improved consistency in AI decision-making through experimental evaluation.

**Result:** The proposed framework was evaluated on a subset of the WildChat dataset, demonstrating significant improvement in judgment consistency among interpreters.

**Limitations:** 

**Conclusion:** This framework is a crucial step toward managing interpretive ambiguity, contributing to the development of more reliable AI systems.

**Abstract:** AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity arises both from how these principles are written and how they are applied. But while legal systems use institutional safeguards to manage such ambiguity, such as transparent appellate review policing interpretive constraints, AI alignment pipelines offer no comparable protections. Different interpretations of the same rule can lead to inconsistent or unstable model behavior. Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity at both the rule creation and rule application steps. We then propose a computational framework that mirrors two legal mechanisms: (1) a rule refinement pipeline that minimizes interpretive disagreement by revising ambiguous rules (analogous to agency rulemaking or iterative legislative action), and (2) prompt-based interpretive constraints that reduce inconsistency in rule application (analogous to legal canons that guide judicial discretion). We evaluate our framework on a 5,000-scenario subset of the WildChat dataset and show that both interventions significantly improve judgment consistency across a panel of reasonable interpreters. Our approach offers a first step toward systematically managing interpretive ambiguity, an essential step for building more robust, law-following AI systems.

</details>


### [103] [Efficient Large Language Models with Zero-Shot Adjustable Acceleration](https://arxiv.org/abs/2509.01190)

*Sajjad Kachuee, Mohammad Sharifkhani*

**Main category:** cs.CL

**Keywords:** Large Language Models, Zero-Shot Learning, Inference Optimization, Computational Efficiency, Text Generation

**Relevance Score:** 8

**TL;DR:** This paper presents a novel method, Zero-Shot Adjustable Acceleration, for optimizing the use of hardware during inference without additional fine-tuning, achieving significant speed improvements for Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of computational efficiency and performance when using Large Language Models in real-world applications.

**Method:** The paper introduces a training and inference method that dynamically adjusts hardware usage during inference, optimizing acceleration.

**Key Contributions:**

	1. Introduction of Zero-Shot Adjustable Acceleration as a novel method for LLMs.
	2. Demonstration of a significant speedup in inference tasks without additional fine-tuning.
	3. Evaluation across multiple classification and text generation tasks showing the effectiveness of the approach.

**Result:** The proposed method allows for a wide range of acceleration in a zero-shot manner, achieving up to an 11x speedup compared to baseline performance in classification and text generation tasks.

**Limitations:** 

**Conclusion:** Zero-Shot Adjustable Acceleration provides an effective solution for optimizing LLM performance during inference without the need for extra fine-tuning, enhancing computational resource utilization.

**Abstract:** Using Large Language Models (LLMs) in real-world applications presents significant challenges, particularly in balancing computational efficiency and performance. Optimizing acceleration after the fine-tuning phase and during inference is crucial for building an efficient architecture. This paper introduces Zero-Shot Adjustable Acceleration, a novel training and inference method that dynamically adjusts hardware usage during inference without requiring additional fine-tuning. The proposed approach is applied to newly developed models and evaluated across multiple classification and text generation tasks. Experimental results demonstrate that the method enables a wide range of acceleration in a zero-shot manner and achieves up to a 11x speedup compared to the baseline.

</details>


### [104] [SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation](https://arxiv.org/abs/2509.01200)

*Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian*

**Main category:** cs.CL

**Keywords:** Simultaneous Speech Translation, Natural Language Processing, Machine Learning

**Relevance Score:** 4

**TL;DR:** Introducing SimulMEGA, an unsupervised policy learning framework for simultaneous speech translation that optimizes translation quality and latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve real-time cross-lingual communication by addressing limitations in translation quality, latency, and semantic coherence in existing simultaneous speech translation systems.

**Method:** SimulMEGA utilizes a Mixture-of-Experts refiner combined with prefix-based training for implicit learning of effective read and write decisions, integrated with standard transformer architectures.

**Key Contributions:**

	1. Introduction of an unsupervised policy learning framework for speech translation
	2. Combines prefix-based training with Mixture-of-Experts refining
	3. Demonstrates effectiveness across multiple language pairs and tasks

**Result:** SimulMEGA outperforms the Seamless baseline with a 500M parameter speech-to-text model, achieving low BLEU degradation with fast average lag across six language pairs.

**Limitations:** 

**Conclusion:** The framework shows promise in enhancing simultaneous speech translation and streaming text-to-speech tasks, achieving superior tradeoffs between latency and quality.

**Abstract:** Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.

</details>


### [105] [Mitigating Catastrophic Forgetting in Continual Learning through Model Growth](https://arxiv.org/abs/2509.01213)

*Ege Süalp, Mina Rezaei*

**Main category:** cs.CL

**Keywords:** catastrophic forgetting, continual learning, large language models, model growth, transformer stacking

**Relevance Score:** 9

**TL;DR:** This paper investigates model growth strategies in continual learning to mitigate catastrophic forgetting in large language models, showing that growth-based models retain prior capabilities better, although some degradation still occurs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address catastrophic forgetting in continual learning, particularly for large language models, which struggle to retain performance on previous tasks when fine-tuned on new ones.

**Method:** The study evaluates growth-based pretraining using transformer stacking to improve retention of domain knowledge and comprehension across fine-tuning tasks.

**Key Contributions:**

	1. Investigates the efficacy of model growth strategies in mitigating catastrophic forgetting in LLMs.
	2. Evaluates the performance differences between growth-based models and standard models.
	3. Finds trade-offs in retention abilities and bias in models during continual learning.

**Result:** Growth-based models (Stack LLM) show improvements in retaining domain knowledge and reduced degradation in reading comprehension compared to baseline LLM models; however, some loss of reasoning abilities and persistence of biases are noted.

**Limitations:** The study highlights that, despite improvements, reasoning and reading comprehension degrade over time, indicating limitations within the growth-based approach.

**Conclusion:** While growth-based pretraining offers modest improvements in resisting catastrophic forgetting, trade-offs in reasoning abilities and social bias handling still exist.

**Abstract:** Catastrophic forgetting is a significant challenge in continual learning, in which a model loses prior knowledge when it is fine-tuned on new tasks. This problem is particularly critical for large language models (LLMs) undergoing continual learning, as retaining performance across diverse domains is important for their general utility. In this paper, we explore model growth, a promising strategy that leverages smaller models to expedite and structure the training of larger ones for mitigating the catastrophic forgetting problem. Although growth-based pretraining, particularly via transformer stacking, has shown promise in accelerating convergence, its impact on forgetting remains under-explored. Therefore, we evaluate whether growth-based models can retain previously learned capabilities more effectively across a sequence of fine-tuning tasks involving domain knowledge, reasoning, reading comprehension, and bias. Our findings show that both models -- one trained with growth (Stack LLM) and one without (LLM) -- exhibit improvements in domain knowledge. However, reasoning and reading comprehension degrade over time, indicating signs of catastrophic forgetting. Stack LLM consistently shows less degradation, especially in reading comprehension, suggesting enhanced retention capabilities. Interestingly, in bias evaluation, the baseline LLM becomes progressively more neutral with continued fine-tuning, while Stack LLM maintains a steady bias ratio around 60--61\%. These results indicate that growth-based pretraining may deliver modest improvements in resisting catastrophic forgetting, though trade-offs remain in handling social biases.

</details>


### [106] [DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression](https://arxiv.org/abs/2509.01221)

*Wei Huang, Huang Wei, Yinggui Wang*

**Main category:** cs.CL

**Keywords:** large language models, data filtering, model compression

**Relevance Score:** 9

**TL;DR:** The paper presents DaMoC, a framework for efficiently selecting and fine-tuning LLMs for domain-specific tasks, significantly reducing training time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Selecting the best open-source LLM for specific domain tasks is challenging and time-consuming.

**Method:** Introduces a systematic data filtering categorization and employs a model compression approach to optimize LLM training.

**Key Contributions:**

	1. Systematic categorization of data filtering methodologies for LLMs.
	2. Introduction of a sparse merging paradigm to enhance model efficiency.
	3. Demonstration of substantial training time reduction across diverse datasets.

**Result:** DaMoC allows for optimal LLM selection while achieving approximately a 20-fold reduction in training time across various datasets.

**Limitations:** 

**Conclusion:** The proposed framework supports effective and efficient LLM fine-tuning, vital for domain-specific applications.

**Abstract:** Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&A, financial Q&A, general Q&A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.

</details>


### [107] [Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors](https://arxiv.org/abs/2509.01236)

*Hao Yang, Zhiyu Yang, Yunjie Zhang, Shanyi Zhu, Lin Yang*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought reasoning, in-context learning, pretrained priors, prompt engineering, large language models

**Relevance Score:** 8

**TL;DR:** This paper investigates the mechanisms of Chain-of-Thought reasoning in models, analyzing its reliance on pretrained priors versus in-context learning, and effects of prompt engineering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unclear underlying mechanisms of Chain-of-Thought reasoning that enhance model inference capabilities.

**Method:** The paper performs a lexical-level analysis of rationales, introduces noisy exemplars to study their impact on model decision-making, and explores the effects of prompt engineering on reasoning processes.

**Key Contributions:**

	1. Introduced fine-grained analysis of reasoning structures in Chain-of-Thought processing.
	2. Identified the balance between pretrained priors and in-context signals under various prompting conditions.
	3. Showed that long prompts may enhance reasoning depth and task performance.

**Result:** The experiments demonstrate that models learn reasoning structures and patterns, exhibit dependence on pretrained priors, and show shifts in decision-making based on the quality of exemplars and prompts.

**Limitations:** The reliance on pretrained knowledge may limit adaptability to novel tasks.

**Conclusion:** Effective long prompting can enhance model reasoning ability, resulting in improved performance on downstream tasks.

**Abstract:** Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing model inference capabilities. Despite growing interest in Chain-of-Thought reasoning, its underlying mechanisms remain unclear. This paper explores the working mechanisms of Chain-of-Thought reasoning from the perspective of the dual relationship between in-context learning and pretrained priors. We first conduct a fine-grained lexical-level analysis of rationales to examine the model's reasoning behavior. Then, by incrementally introducing noisy exemplars, we examine how the model balances pretrained priors against erroneous in-context information. Finally, we investigate whether prompt engineering can induce slow thinking in large language models. Our extensive experiments reveal three key findings: (1) The model not only quickly learns the reasoning structure at the lexical level but also grasps deeper logical reasoning patterns, yet it heavily relies on pretrained priors. (2) Providing sufficient exemplars shifts the model's decision-making from pretrained priors to in-context signals, while misleading prompts introduce instability. (3) Long Chain-of-Thought prompting can induce the model to generate longer reasoning chains, thereby improving its performance on downstream tasks.

</details>


### [108] [Annotation and modeling of emotions in a textual corpus: an evaluative approach](https://arxiv.org/abs/2509.01260)

*Jonas Noblet*

**Main category:** cs.CL

**Keywords:** emotion, textual analysis, language models, annotation, evaluative framework

**Relevance Score:** 7

**TL;DR:** The paper explores the evaluation of emotion in text using annotated data, revealing that language models can capture emotional nuances despite variability in human annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding of emotional expressions in textual data, which is underexplored with traditional methods.

**Method:** Analysis of an industrial corpus annotated for emotion using a novel evaluative framework, followed by training language models on this data.

**Key Contributions:**

	1. Introduced a new theoretical framework for emotion evaluation in text.
	2. Demonstrated the feasibility of modeling human emotion labeling processes with language models.
	3. Highlighted the relationship between linguistic features and emotional variability in annotations.

**Result:** Language models successfully modeled the labeling process of emotions and identified underlying linguistic features driving annotation variability.

**Limitations:** The study is based on a specific industrial corpus, which may limit generalizability.

**Conclusion:** The findings suggest that emotional situations can be distinguished by language models based on evaluative criteria, despite annotation disagreements.

**Abstract:** Emotion is a crucial phenomenon in the functioning of human beings in society. However, it remains a widely open subject, particularly in its textual manifestations. This paper examines an industrial corpus manually annotated following an evaluative approach to emotion. This theoretical framework, which is currently underutilized, offers a different perspective that complements traditional approaches. Noting that the annotations we collected exhibit significant disagreement, we hypothesized that they nonetheless follow stable statistical trends. Using language models trained on these annotations, we demonstrate that it is possible to model the labeling process and that variability is driven by underlying linguistic features. Conversely, our results indicate that language models seem capable of distinguishing emotional situations based on evaluative criteria.

</details>


### [109] [Culture is Everywhere: A Call for Intentionally Cultural Evaluation](https://arxiv.org/abs/2509.01301)

*Juhyun Oh, Inha Cha, Michael Saxon, Hyunseung Lim, Shaily Bhatt, Alice Oh*

**Main category:** cs.CL

**Keywords:** Cultural alignment, Language models, Evaluation methodologies, Human-Computer Interaction, Inclusive research

**Relevance Score:** 7

**TL;DR:** This paper critiques the current methods for evaluating cultural alignment in large language models, advocating for a more inclusive and comprehensive approach to cultural evaluation in NLP.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current trivia-centered evaluation methods for cultural alignment in LLMs are inadequate for capturing the complexities of culture.

**Method:** The paper proposes an 'intentionally cultural evaluation' approach that examines the cultural assumptions in all evaluation aspects and emphasizes researcher positionality.

**Key Contributions:**

	1. Introduction of the concept of intentionally cultural evaluation in NLP.
	2. Emphasis on the need for inclusivity via researcher positionality.
	3. Call for participatory design methodologies in evaluation processes.

**Result:** By promoting a comprehensive understanding of cultural considerations, the paper aims to foster more inclusive and effective NLP research practices.

**Limitations:** 

**Conclusion:** The authors argue for the necessity of engaging communities and utilizing participatory methodologies to improve evaluation design in NLP.

**Abstract:** The prevailing ``trivia-centered paradigm'' for evaluating the cultural alignment of large language models (LLMs) is increasingly inadequate as these models become more advanced and widely deployed. Existing approaches typically reduce culture to static facts or values, testing models via multiple-choice or short-answer questions that treat culture as isolated trivia. Such methods neglect the pluralistic and interactive realities of culture, and overlook how cultural assumptions permeate even ostensibly ``neutral'' evaluation settings. In this position paper, we argue for \textbf{intentionally cultural evaluation}: an approach that systematically examines the cultural assumptions embedded in all aspects of evaluation, not just in explicitly cultural tasks. We systematically characterize the what, how, and circumstances by which culturally contingent considerations arise in evaluation, and emphasize the importance of researcher positionality for fostering inclusive, culturally aligned NLP research. Finally, we discuss implications and future directions for moving beyond current benchmarking practices, discovering important applications that we don't know exist, and involving communities in evaluation design through HCI-inspired participatory methodologies.

</details>


### [110] [TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering](https://arxiv.org/abs/2509.01312)

*Sishi Xiong, Ziyang He, Zhongjiang He, Yu Zhao, Changzai Pan, Jie Zhang, Zhenhe Wu, Shuangyong Song, Yongxiang Li*

**Main category:** cs.CL

**Keywords:** large language models, table question answering, machine learning, framework, health informatics

**Relevance Score:** 8

**TL;DR:** TableZoomer is a novel LLM-powered framework that enhances performance in table question answering by using structured table schemas, a query-aware zooming mechanism, and a Program-of-Thoughts strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges of LLMs in industrial applications, such as structural heterogeneity and complex reasoning in table question answering.

**Method:** Introduces three innovations: (1) uses structured table schemas to reduce complexity, (2) employs query-aware zooming for better localization, and (3) applies a Program-of-Thoughts strategy to convert queries to executable code.

**Key Contributions:**

	1. Structured table schema to reduce complexity
	2. Query-aware table zooming mechanism for efficiency
	3. Program-of-Thoughts strategy to reduce numerical hallucination

**Result:** TableZoomer shows accuracy improvements of 19.34% and 25% over conventional methods on large-scale DataBench and small-scale TableBench datasets, respectively.

**Limitations:** 

**Conclusion:** The framework enhances usability while significantly improving performance and scalability in table question answering tasks.

**Abstract:** While large language models (LLMs) have shown promise in the table question answering (TQA) task through prompt engineering, they face challenges in industrial applications, including structural heterogeneity, difficulties in target data localization, and bottlenecks in complex reasoning. To address these limitations, this paper presents TableZoomer, a novel LLM-powered, programming-based agent framework. It introduces three key innovations: (1) replacing the original fully verbalized table with structured table schema to bridge the semantic gap and reduce computational complexity; (2) a query-aware table zooming mechanism that dynamically generates sub-table schema through column selection and entity linking, significantly improving target localization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that transforms queries into executable code to mitigate numerical hallucination. Additionally, we integrate the reasoning workflow with the ReAct paradigm to enable iterative reasoning. Extensive experiments demonstrate that our framework maintains the usability advantages while substantially enhancing performance and scalability across tables of varying scales. When implemented with the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of 19.34% and 25% over conventional PoT methods on the large-scale DataBench dataset and the small-scale Fact Checking task of TableBench dataset, respectively.

</details>


### [111] [Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization](https://arxiv.org/abs/2509.01314)

*Anum Afzal, Mehul Kumawat, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Domain adaptation, Low-resource domains, Text summarization

**Relevance Score:** 8

**TL;DR:** The paper explores the use of parameter-efficient fine-tuning techniques (PEFTs) to improve Large Language Models' (LLMs) adaptability to low-resource domains, demonstrating effective domain adaptation strategies for text summarization tasks using Llama-3 models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of adapting LLMs to low-resource domains where labeled data is scarce and traditional fine-tuning is computationally expensive.

**Method:** Leveraging parameter-efficient fine-tuning techniques (PEFTs) on high-resource datasets to enhance performance in low-resource domains, while evaluating linguistic commonalities through benchmarks with 14 datasets across different sectors.

**Key Contributions:**

	1. Introduction of parameter-efficient fine-tuning techniques for LLMs
	2. Evaluation of within-domain versus cross-domain adapters
	3. Identification of linguistic commonalities that aid adaptation in low-resource environments

**Result:** Within-Domain Adapters significantly outperform Few-Shot methods and a larger Llama-3 model in low-resource settings, showcasing promising adaptability through the strategic use of domain adapters.

**Limitations:** 

**Conclusion:** The study concludes that leveraging intrinsic linguistic similarities and using a combination of different domain adapters can lead to better performance in low-resource applications.

**Abstract:** Large Language Models (LLMs), being generic task solvers, are versatile. However, despite the vast amount of data they are trained on, there are speculations about their adaptation capabilities to a new domain. Additionally, the simple fine-tuning of the model to incorporate knowledge of a new domain is computationally expensive and time-consuming. This becomes more challenging when the domain in question is also low-resource, and labeled data is unavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on high-resource datasets to address these challenges to improve performance on unseen low-resource domains. Throughout our experiments, we evaluate whether intrinsic linguistic commonalities between datasets can be leveraged for efficient domain adaptation. We benchmark six PEFTs with \texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific, Medical, Legal, and News domains for a Text Summarization task. Our experiments show that for low-resource domains, inference using Within-Domain Adapters can achieve better performance than Few-Shot as well as a much larger \texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain Adapters, we explore the concept of using Cross-Domain Adapters as well as the strategic combinations of adapters to leverage intrinsic language similarities across domains, facilitating better adaptability and performance in low-resource settings.

</details>


### [112] [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)

*Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhua Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, agentic intelligence, language model, computational efficiency

**Relevance Score:** 8

**TL;DR:** LongCat-Flash is a 560-billion-parameter Mixture-of-Experts language model that focuses on computational efficiency and agentic capabilities, achieving high performance in inference and task execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a scalable and efficient solution for language model training and performance, addressing the increasing demand for computational resources while enhancing agentic intelligence.

**Method:** The paper introduces Zero-computation Experts for dynamic resource allocation and Shortcut-connected MoE to improve computation-communication overlap, with robust training techniques combined for model stability.

**Key Contributions:**

	1. Introduction of Zero-computation Experts for dynamic allocation
	2. Development of Shortcut-connected MoE for improved efficiency
	3. Open-sourcing the LongCat-Flash model checkpoint

**Result:** LongCat-Flash trains on over 20 trillion tokens in 30 days, achieving over 100 tokens per second inference speed while maintaining low costs per output token, with strong performance on agentic tasks in evaluations against leading models.

**Limitations:** 

**Conclusion:** LongCat-Flash represents a significant advancement in the use of large language models for agentic applications, providing open-source resources for further research.

**Abstract:** We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.   LongCat Chat: https://longcat.ai   Hugging Face: https://huggingface.co/meituan-longcat   GitHub: https://github.com/meituan-longcat

</details>


### [113] [KoBLEX: Open Legal Question Answering with Multi-hop Reasoning](https://arxiv.org/abs/2509.01324)

*Jihyung Lee, Daehui Kim, Seonjeong Hwang, Hyounghun Kim, Gary Lee*

**Main category:** cs.CL

**Keywords:** Legal QA, Large Language Models, Multi-hop reasoning, Benchmarking, Legal fidelity evaluation

**Relevance Score:** 7

**TL;DR:** The paper introduces KoBLEX, a benchmark for evaluating provision-grounded legal QA, and the ParSeR method that improves multi-hop reasoning in legal questions via LLM-generated provisions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks fail to assess open-ended, provision-grounded QA in legal contexts, necessitating a new approach to evaluate LLMs in law.

**Method:** The proposed method, ParSeR, employs LLM-generated parametric provisions to guide the retrieval of answers, utilizing a three-stage sequential retrieval process for complex legal reasoning.

**Key Contributions:**

	1. Introduction of KoBLEX, a benchmark for legal QA evaluation
	2. Development of ParSeR for improved multi-hop reasoning
	3. Proposal of LF-Eval as a metric for assessing legal answer fidelity

**Result:** ParSeR achieves significant improvements in legal QA performance, outperforming baselines with +37.91 higher F1 and +30.81 higher LF-Eval scores across multiple LLMs.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of ParSeR in delivering consistent performance in legal reasoning and offers a novel metric, LF-Eval, for evaluating answer fidelity.

**Abstract:** Large Language Models (LLM) have achieved remarkable performances in general domains and are now extending into the expert domain of law. Several benchmarks have been proposed to evaluate LLMs' legal capabilities. However, these benchmarks fail to evaluate open-ended and provision-grounded Question Answering (QA). To address this, we introduce a Korean Benchmark for Legal EXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop legal reasoning. KoBLEX includes 226 scenario-based QA instances and their supporting provisions, created using a hybrid LLM-human expert pipeline. We also propose a method called Parametric provision-guided Selection Retrieval (ParSeR), which uses LLM-generated parametric provisions to guide legally grounded and reliable answers. ParSeR facilitates multi-hop reasoning on complex legal questions by generating parametric provisions and employing a three-stage sequential retrieval process. Furthermore, to better evaluate the legal fidelity of the generated answers, we propose Legal Fidelity Evaluation (LF-Eval). LF-Eval is an automatic metric that jointly considers the question, answer, and supporting provisions and shows a high correlation with human judgments. Experimental results show that ParSeR consistently outperforms strong baselines, achieving the best results across multiple LLMs. Notably, compared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1 and +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently delivers consistent performance across reasoning depths, with ablations confirming the effectiveness of ParSeR.

</details>


### [114] [Can Large Language Models Master Complex Card Games?](https://arxiv.org/abs/2509.01328)

*Wei Wang, Fuqing Bie, Junzhe Chen, Dan Zhang, Shiyu Huang, Evgeny Kharlamov, Jie Tang*

**Main category:** cs.CL

**Keywords:** large language models, complex card games, fine-tuning, game AI, learning capabilities

**Relevance Score:** 8

**TL;DR:** This paper explores the potential of large language models (LLMs) to master complex card games by assessing their learning capabilities across eight diverse games.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs can achieve success in complex games similar to AI algorithms like AlphaGo, AlphaZero, and MuZero that excelled in Go and Chess.

**Method:** The study systematically evaluates the learning capabilities of LLMs through supervised fine-tuning on high-quality gameplay data across eight card games.

**Key Contributions:**

	1. Assessment of LLM performance in complex card games
	2. Realization of simultaneous mastery of multiple games
	3. Identification of strategies to mitigate decline in general capabilities

**Result:** LLMs can approach strong game AI performance through fine-tuning, master multiple card games simultaneously, and their general capabilities may decline but can be mitigated with general instruction data.

**Limitations:** 

**Conclusion:** The findings highlight the strong learning ability and versatility of LLMs in mastering complex games, although caution is needed regarding their general capabilities.

**Abstract:** Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can master multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs.

</details>


### [115] [Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic](https://arxiv.org/abs/2509.01363)

*Mohammad Zbeeb, Hasan Abed Al Kader Hammoud, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Transfer Learning, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This work shows how reasoning abilities in large language models can be extracted and transferred between models using a compact task vector.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the computational costs involved in training large language models for complex reasoning tasks.

**Method:** Extract reasoning vectors from two Qwen2.5 models, one fine-tuned with SFT and the other with GRPO, and apply these vectors to instruction-tuned models to enhance performance.

**Key Contributions:**

	1. Extraction and transfer of reasoning abilities across models using compact task vectors.
	2. Demonstrated performance improvements on various reasoning benchmarks through simple arithmetic.
	3. Highlighted the potential of reusing prior computational investments in model training.

**Result:** Applying the reasoning vector consistently improved model performance on several reasoning benchmarks, with notable increases in scores across various tests.

**Limitations:** 

**Conclusion:** The study demonstrates a practical way to enhance models by reusing reasoning capabilities without expensive retraining, thereby recycling computational investments.

**Abstract:** Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: $v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.

</details>


### [116] [WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data](https://arxiv.org/abs/2509.01379)

*Paloma Piot, Diego Sánchez, Javier Parapar*

**Main category:** cs.CL

**Keywords:** hate speech, content moderation, AI chatbot, human oversight, Large Language Models

**Relevance Score:** 8

**TL;DR:** WATCHED is a chatbot that assists content moderators in detecting and explaining hate speech using AI and LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Online harms, particularly hate speech, pose significant risks to user safety and trust in social media, necessitating effective moderation tools.

**Method:** WATCHED combines automated systems with human judgment, utilizing LLMs, a BERT-based classifier, slang databases, and platform guidelines to identify and explain hate speech.

**Key Contributions:**

	1. Development of WATCHED for hate speech moderation
	2. Integration of LLMs with human insights for content analysis
	3. Demonstrated superior performance in identifying hate speech

**Result:** The chatbot achieved a macro F1 score of 0.91 in experimental evaluations, outperforming existing methods.

**Limitations:** 

**Conclusion:** WATCHED enhances collaboration between AI and human moderators to effectively reduce online harms.

**Abstract:** Online harms are a growing problem in digital spaces, putting user safety at risk and reducing trust in social media platforms. One of the most persistent forms of harm is hate speech. To address this, we need tools that combine the speed and scale of automated systems with the judgment and insight of human moderators. These tools should not only find harmful content but also explain their decisions clearly, helping to build trust and understanding. In this paper, we present WATCHED, a chatbot designed to support content moderators in tackling hate speech. The chatbot is built as an Artificial Intelligence Agent system that uses Large Language Models along with several specialised tools. It compares new posts with real examples of hate speech and neutral content, uses a BERT-based classifier to help flag harmful messages, looks up slang and informal language using sources like Urban Dictionary, generates chain-of-thought reasoning, and checks platform guidelines to explain and support its decisions. This combination allows the chatbot not only to detect hate speech but to explain why content is considered harmful, grounded in both precedent and policy. Experimental results show that our proposed method surpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91. Designed for moderators, safety teams, and researchers, the tool helps reduce online harms by supporting collaboration between AI and human oversight.

</details>


### [117] [ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links](https://arxiv.org/abs/2509.01387)

*Serwar Basch, Ilia Kuznetsov, Tom Hope, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** cross-document linking, LLMs, peer review, news, datasets

**Relevance Score:** 8

**TL;DR:** A framework for creating training datasets for cross-document linking, showing significant improvements using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in creating efficient training and evaluation datasets for automated cross-document linking methods.

**Method:** Generate semi-synthetic datasets of interconnected documents followed by automatic evaluation and human evaluation to identify the best-performing linking approaches.

**Key Contributions:**

	1. A domain-agnostic framework for cross-document linking
	2. Generation of semi-synthetic datasets for evaluation
	3. High approval rates for LLM-enhanced retrieval models

**Result:** Utilizing retrieval models combined with LLMs achieved a 78% link approval rate from human raters, improving the precision of traditional methods.

**Limitations:** 

**Conclusion:** The proposed framework facilitates systematic cross-document understanding and contributes novel datasets for various applications.

**Abstract:** Understanding fine-grained relations between documents is crucial for many application domains. However, the study of automated assistance is limited by the lack of efficient methods to create training and evaluation datasets of cross-document links. To address this, we introduce a new domain-agnostic framework for selecting a best-performing approach and annotating cross-document links in a new domain from scratch. We first generate and validate semi-synthetic datasets of interconnected documents. This data is used to perform automatic evaluation, producing a shortlist of best-performing linking approaches. These approaches are then used in an extensive human evaluation study, yielding performance estimates on natural text pairs. We apply our framework in two distinct domains -- peer review and news -- and show that combining retrieval models with LLMs achieves 78\% link approval from human raters, more than doubling the precision of strong retrievers alone. Our framework enables systematic study of cross-document understanding across application scenarios, and the resulting novel datasets lay foundation for numerous cross-document tasks like media framing and peer review. We make the code, data, and annotation protocols openly available.

</details>


### [118] [Analysing the Language of Neural Audio Codecs](https://arxiv.org/abs/2509.01390)

*Joonyong Park, Shinnosuke Takamichi, David M. Chan, Shunsuke Kando, Yuki Saito, Hiroshi Saruwatari*

**Main category:** cs.CL

**Keywords:** neural audio codecs, linguistic properties, speech recognition, quality assessment, semantic preservation

**Relevance Score:** 4

**TL;DR:** This study analyzes the statistical and linguistic properties of neural audio codecs (NACs), focusing on their token-level characteristics and their correlation with speech recognition and quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the linguistic properties of discrete speech tokens produced by neural audio codecs and their impact on speech intelligibility and quality.

**Method:** A comparative analysis of various NAC models was conducted, examining token adherence to linguistic laws and evaluating intelligibility and quality using automatic speech recognition error rates and UTMOS scores.

**Key Contributions:**

	1. Demonstration of linguistic adherence in NAC tokens
	2. Establishment of a correlation between token properties and automatic speech recognition performance
	3. Insights into design for improved generative speech models

**Result:** NAC tokens, particularly 3-grams, demonstrated language-like statistical patterns, and the correlation of these properties with speech recognition performance was established.

**Limitations:** 

**Conclusion:** The findings provide insights into NAC token structures and can guide the development of better generative speech models.

**Abstract:** This study presents a comparative analysis of the statistical and linguistic properties of neural audio codecs (NACs). We investigate discrete speech tokens produced by various NAC models, examining their adherence to linguistic statistical laws such as Zipf's law and Heaps' law, as well as their entropy and redundancy. To assess how these token-level properties relate to semantic and acoustic preservation in synthesized speech, we evaluate intelligibility using error rates of automatic speech recognition, and quality using the UTMOS score. Our results reveal that NAC tokens, particularly 3-grams, exhibit language-like statistical patterns. Moreover, these properties, together with measures of information content, are found to correlate with improved performances in speech recognition and resynthesis tasks. These findings offer insights into the structure of NAC token sequences and inform the design of more effective generative speech models.

</details>


### [119] [LLMs cannot spot math errors, even when allowed to peek into the solution](https://arxiv.org/abs/2509.01395)

*KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar*

**Main category:** cs.CL

**Keywords:** large language models, meta-reasoning, math word problems, error identification, student solutions

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of large language models to identify the first error step in student solutions to math word problems, highlighting their shortcomings and proposing a novel approach to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Although large language models excel at solving math word problems, they struggle with meta-reasoning tasks like identifying errors in student solutions. This study focuses on addressing this limitation.

**Method:** We evaluate state-of-the-art LLMs on two error reasoning datasets, VtG and PRM800K, to assess their performance in locating the first error step in student solutions. We propose a method that generates an intermediate corrected version of the student's solution.

**Key Contributions:**

	1. Introduction of two error reasoning datasets: VtG and PRM800K
	2. Development of a method for generating intermediate corrected solutions to assist LLMs
	3. Demonstration of the limitations of existing LLMs in error identification tasks

**Result:** The experiments reveal that LLMs have significant difficulty in pinpointing the first error in student solutions, even when presented with reference solutions.

**Limitations:** The proposed method may not generalize to all types of math problems or student solutions.

**Conclusion:** Our proposed correction mechanism aligns more closely with original student solutions, which is shown to enhance the LLM's ability to identify errors accurately.

**Abstract:** Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance.

</details>


### [120] [Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.01412)

*Kaviraj Pather, Elena Hadjigeorgiou, Arben Krasniqi, Claire Schmit, Irina Rusu, Marc Pons, Kabir Khan*

**Main category:** cs.CL

**Keywords:** human-in-the-loop, reasoning graph, chain-of-thought, large language models, trust

**Relevance Score:** 9

**TL;DR:** Vis-CoT is a human-in-the-loop framework that creates interactive reasoning graphs from chain-of-thought prompts of LLMs, enabling better accuracy and trust.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The opacity of reasoning in large language models makes verification and debugging challenging in critical applications.

**Method:** Vis-CoT converts linear chain-of-thought text into an interactive reasoning graph that allows users to visualize the reasoning process and intervene as needed.

**Key Contributions:**

	1. Development of an interactive reasoning graph for LLMs
	2. Significant improvement in accuracy with user intervention
	3. Enhanced perceived usability and trust in LLM outputs

**Result:** Vis-CoT improves final-answer accuracy by up to 24 percentage points on GSM8K and StrategyQA benchmarks and enhances user perceptions of usability and trust.

**Limitations:** 

**Conclusion:** Vis-CoT facilitates more reliable and collaborative reasoning by combining LLMs with human oversight.

**Abstract:** Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.

</details>


### [121] [On the Alignment of Large Language Models with Global Human Opinion](https://arxiv.org/abs/2509.01418)

*Yang Liu, Masahiro Kaneko, Chenhui Chu*

**Main category:** cs.CL

**Keywords:** large language models, opinion alignment, World Values Survey, multilingual scenarios, prompt engineering

**Relevance Score:** 9

**TL;DR:** This study investigates the alignment of large language models (LLMs) with human opinions across different countries and historical periods, using a framework based on the World Values Survey.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gaps in existing research regarding the alignment of LLMs with diverse demographic opinions globally, particularly influenced by the language of prompts.

**Method:** An evaluation framework was created based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across various countries and historical periods.

**Key Contributions:**

	1. Development of a new evaluation framework for opinion alignment in LLMs
	2. Comprehensive global analysis of LLM opinion alignment across countries and historical periods
	3. Demonstration of prompt language's role in steering LLMs' alignment with human opinions

**Result:** It was found that LLMs tend to over-align or align appropriately with opinions from only a few countries, while under-aligning with most others. Additionally, prompt language effectively steers LLMs towards aligning with the opinions of the respective country.

**Limitations:** The study may be limited by the specific languages and countries included in the analysis, as well as the historical periods examined.

**Conclusion:** The study represents the first comprehensive investigation into opinion alignment in LLMs across global, linguistic, and temporal dimensions, revealing significant findings about prompt language influence.

**Abstract:** Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at https://github.com/nlply/global-opinion-alignment.

</details>


### [122] [Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal](https://arxiv.org/abs/2509.01455)

*Markus Oehri, Giulia Conti, Kaviraj Pather, Alexandre Rossi, Laia Serra, Adrian Parody, Rogvi Johannesen, Aviaja Petersen, Arben Krasniqi*

**Main category:** cs.CL

**Keywords:** language models, uncertainty calibration, risk-control, trustworthiness, refusal messages

**Relevance Score:** 9

**TL;DR:** UniCR is a framework for calibrating language models' responses by managing uncertainty and enforcing user-specified error budgets through principled refusals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve trustworthiness and decision-making in deployed language models by managing uncertainty effectively.

**Method:** UniCR integrates various uncertainty metrics such as sequence likelihoods and retrieval compatibility to produce a calibrated probability of correctness. It employs a lightweight calibration mechanism with a focus on semantic fidelity for long-form generation.

**Key Contributions:**

	1. Development of a unified framework for uncertainty management in language models.
	2. Introduction of a lightweight calibration head for enhanced response accuracy.
	3. A method that generates meaningful user-facing refusal messages based on evidence assessment.

**Result:** Experiments demonstrated significant enhancements in calibration metrics and risk-coverage trade-offs, outperforming existing methods in several key areas.

**Limitations:** 

**Conclusion:** UniCR provides a robust framework for enhancing the reliability of language model outputs without necessitating base model fine-tuning, making it adaptable to various scenarios under distribution shifts.

**Abstract:** Deployed language models must decide not only what to answer but also when not to answer. We present UniCR, a unified framework that turns heterogeneous uncertainty evidence including sequence likelihoods, self-consistency dispersion, retrieval compatibility, and tool or verifier feedback into a calibrated probability of correctness and then enforces a user-specified error budget via principled refusal. UniCR learns a lightweight calibration head with temperature scaling and proper scoring, supports API-only models through black-box features, and offers distribution-free guarantees using conformal risk control. For long-form generation, we align confidence with semantic fidelity by supervising on atomic factuality scores derived from retrieved evidence, reducing confident hallucinations while preserving coverage. Experiments on short-form QA, code generation with execution tests, and retrieval-augmented long-form QA show consistent improvements in calibration metrics, lower area under the risk-coverage curve, and higher coverage at fixed risk compared to entropy or logit thresholds, post-hoc calibrators, and end-to-end selective baselines. Analyses reveal that evidence contradiction, semantic dispersion, and tool inconsistency are the dominant drivers of abstention, yielding informative user-facing refusal messages. The result is a portable recipe of evidence fusion to calibrated probability to risk-controlled decision that improves trustworthiness without fine-tuning the base model and remains valid under distribution shift.

</details>


### [123] [Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA](https://arxiv.org/abs/2509.01468)

*Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Editing, Multi-hop Question Answering, Reasoning Framework

**Relevance Score:** 9

**TL;DR:** Reason-KE is an innovative framework for knowledge-editing in large language models that enhances QA accuracy while managing distractions effectively.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are static post-training, which complicates integration of new knowledge and necessitates effective knowledge-editing techniques.

**Method:** Reason-KE employs a structured four-stage process: fact acknowledgment, relevance determination, selective application, and final reasoning to process queries and filter out distractions in an efficient single pass.

**Key Contributions:**

	1. Introduction of the Reason-KE framework for knowledge-editing in LLMs.
	2. Structured editing process improving resilience to irrelevant distractors.
	3. Achievement of state-of-the-art accuracy in multi-hop QA tasks.

**Result:** Reason-KE improves Qwen2.5-7B's multi-hop QA accuracy to 90.2%, with a minimal drop of 6.3% under heavy distractions and less than 1% when answers are leaked.

**Limitations:** 

**Conclusion:** The quantitative analysis supports Reason-KE's efficiency and resilience, marking it as a new benchmark for reliable updates in LLM knowledge.

**Abstract:** Large language models (LLMs) encode vast amounts of world knowledge but remain static once trained, making the timely integration of emerging facts prohibitively expensive via full retraining. Knowledge-editing techniques have thus emerged to inject or overwrite specific facts into LLMs, yet they either over-rely on superficial cues or incur complex, iterative pipelines that collapse under noisy, multi-hop conditions. We introduce Reason-KE, an end-to-end reasoning-chain-based editing framework that steers a pretrained LLM through four structured stages-fact acknowledgment, relevance determination, selective application, and final reasoning-to filter distractors in a single pass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates Qwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop under heavy distraction and <1% when answers are leaked. Our quantitative analysis confirms Reason-KE's resilience and efficiency, establishing a new state-of-the-art for reliable LLM knowledge updates.

</details>


### [124] [Do Retrieval Augmented Language Models Know When They Don't Know?](https://arxiv.org/abs/2509.01476)

*Youchao Zhou, Heyan Huang, Yicheng Liu, Rui Dai, Xinglin Wang, Xingchen Zhang, Shumin Shi, Yang Deng*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Language Models, LLMs, Refusal Post-training

**Relevance Score:** 9

**TL;DR:** The paper examines the over-refusal behavior in Retrieval Augmented Language Models and evaluates the impact of refusal post-training methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the problem of hallucinations in LLMs and investigates if RALMs can accurately assess their knowledge limitations.

**Method:** The research analyzes the calibration of RALMs regarding internal and external knowledge states, assesses the impact of refusal post-training, and develops a new refusal method.

**Key Contributions:**

	1. Examines the over-refusal behavior in RALMs
	2. Evaluates the effectiveness of refusal post-training methods
	3. Develops a new refusal method for improving answer quality

**Result:** Findings indicate significant over-refusal behavior in LLMs and show that In-context fine-tuning mitigates this issue, while R-tuning exacerbates it.

**Limitations:** 

**Conclusion:** The study provides insights into improving refusal capabilities in RALMs without compromising answer quality.

**Abstract:** Existing Large Language Models (LLMs) occasionally generate plausible yet factually incorrect responses, known as hallucinations. Researchers are primarily using two approaches to mitigate hallucinations, namely Retrieval Augmented Language Models (RALMs) and refusal post-training. However, current research predominantly emphasizes their individual effectiveness while overlooking the evaluation of the refusal capability of RALMs. In this study, we ask the fundamental question: Do RALMs know when they don't know? Specifically, we ask three questions. First, are RALMs well-calibrated regarding different internal and external knowledge states? We examine the influence of various factors. Contrary to expectations, we find that LLMs exhibit significant \textbf{over-refusal} behavior. Then, how does refusal post-training affect the over-refusal issue? We investigate the Refusal-aware Instruction Tuning and In-Context Fine-tuning methods. Our results show that the over-refusal problem is mitigated by In-context fine-tuning. but magnified by R-tuning. However, we also find that the refusal ability may conflict with the quality of the answer. Finally, we develop a simple yet effective refusal method for refusal post-trained models to improve their overall answer quality in terms of refusal and correct answers. Our study provides a more comprehensive understanding of the influence of important factors on RALM systems.

</details>


### [125] [MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models](https://arxiv.org/abs/2509.01514)

*Andreas Ottem*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Memory Verification, Context Composition

**Relevance Score:** 9

**TL;DR:** MeVe is a modular architecture for improving Retrieval-Augmented Generation (RAG) systems by optimizing the process of information retrieval and context composition.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** RAG systems often fail due to irrelevant or redundant information being included, affecting performance and efficiency.

**Method:** MeVe employs a five-phase modular design: initial retrieval, relevance verification, fallback retrieval, context prioritization, and token budgeting, providing fine control over knowledge availability to LLMs.

**Key Contributions:**

	1. Introduces a five-phase modular design for RAG systems.
	2. Significantly improves context efficiency and relevance in LLM applications.
	3. Provides a reference implementation and evaluation against standard RAG systems.

**Result:** MeVe demonstrates significant improvements in context efficiency, with a 57% reduction in irrelevant information in a QA task on Wikipedia and a 75% reduction on the more complex HotpotQA dataset compared to standard RAG systems.

**Limitations:** 

**Conclusion:** The proposed MeVe architecture enhances the scalability and reliability of LLM applications, promoting better grounding and factual accuracy.

**Abstract:** Retrieval-Augmented Generation (RAG) systems typically face constraints because of their inherent mechanism: a simple top-k semantic search [1]. The approach often leads to the incorporation of irrelevant or redundant information in the context, degrading performance and efficiency [10][11]. This paper presents MeVe, a novel modular architecture intended for Memory Verification and smart context composition. MeVe rethinks the RAG paradigm by proposing a five-phase modular design that distinctly breaks down the retrieval and context composition process into distinct, auditable, and independently tunable phases: initial retrieval, relevance verification, fallback retrieval, context prioritization, and token budgeting. This architecture enables fine-grained control of what knowledge is made available to an LLM, enabling task-dependent filtering and adaptation. We release a reference implementation of MeVe as a proof of concept and evaluate its performance on knowledge-heavy QA tasks over a subset of English Wikipedia [22]. Our results demonstrate that by actively verifying information before composition, MeVe significantly improves context efficiency, achieving a 57% reduction on the Wikipedia dataset and a 75% reduction on the more complex HotpotQA dataset compared to standard RAG implementations [25]. This work provides a framework for more scalable and reliable LLM applications. By refining and distilling contextual information, MeVe offers a path toward better grounding and more accurate factual support [16].

</details>


### [126] [Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community](https://arxiv.org/abs/2509.01529)

*Thomas Compton*

**Main category:** cs.CL

**Keywords:** community unionism, NLP, labor studies, thematic modeling, historical analysis

**Relevance Score:** 0

**TL;DR:** Comparative analysis of community unionism in historical contexts using NLP techniques reveals significant differences in thematic focus between two unions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how community unionism has evolved and how modern NLP techniques can enhance the study of historical labor archives.

**Method:** Comparative analysis using BERTopic for thematic modeling, cTF-IDF weighting, and word frequency analysis.

**Key Contributions:**

	1. Use of modern NLP techniques in historical analysis
	2. Insight into thematic differences in union discourse
	3. Challenging assumptions about community unionism continuity

**Result:** Significant differences were found in thematic focus; Unite Community is more aligned with social justice, whereas the B&S union emphasized traditional union roles.

**Limitations:** 

**Conclusion:** The study challenges assumptions about the continuity of community unionism and highlights divergent models of engagement between different eras.

**Abstract:** This paper presents a comparative analysis of community unionism (CU) in two distinct historical and organizational contexts: the National Boot and Shoe Union (B\&S) in the 1920s and Unite Community in the 2010s--2020s. Using BERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency analysis, the study examines the extent to which each union's discourse aligns with key features of CU -- such as coalition-building, grassroots engagement, and action beyond the workplace. The results reveal significant differences in thematic focus and discursive coherence. While Unite Community demonstrates stronger alignment with outward-facing, social justice-oriented themes, the B\&S corpus emphasizes internal administration, industrial relations, and member services -- reflecting a more traditional, servicing-oriented union model. The analysis also highlights methodological insights, demonstrating how modern NLP techniques can enhance the study of historical labor archives. Ultimately, the findings suggest that while both unions engage with community-related themes, their underlying models of engagement diverge significantly, challenging assumptions about the continuity and universality of community unionism across time and sector.

</details>


### [127] [CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models](https://arxiv.org/abs/2509.01535)

*Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang*

**Main category:** cs.CL

**Keywords:** Causal Knowledge, Large Language Models, Attention Mechanism, Human Priors, Out-of-Distribution

**Relevance Score:** 9

**TL;DR:** This paper presents Causal Attention Tuning (CAT), a method to enhance Large Language Models (LLMs) by incorporating causal knowledge into their attention mechanism, improving their performance in various prediction tasks and robustness in out-of-distribution scenarios.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the predictive performance of LLMs by utilizing causal knowledge instead of relying solely on spurious correlations inherent in large-scale data.

**Method:** Causal Attention Tuning (CAT) introduces fine-grained causal knowledge into the attention mechanism of LLMs through an automated pipeline that generates token-level causal signals and employs a Re-Attention mechanism to prioritize causal relationships during training.

**Key Contributions:**

	1. Introduction of Causal Attention Tuning (CAT) method
	2. Automated pipeline for generating token-level causal signals
	3. Demonstrated improvement in robustness against out-of-distribution scenarios

**Result:** Experimental results demonstrate that CAT significantly enhances the ability of LLMs to leverage causal knowledge for prediction while maintaining robustness in out-of-distribution scenarios, as evidenced by evaluations on the Spurious Token Game (STG) benchmark and various downstream tasks.

**Limitations:** 

**Conclusion:** Causal Attention Tuning (CAT) presents a promising direction for integrating causal knowledge into LLMs, improving predictive accuracy and generalization in challenging scenarios.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. Implementation details can be found at https://github.com/Kairong-Han/CAT.

</details>


### [128] [In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents](https://arxiv.org/abs/2509.01560)

*Seungkyu Lee, Nalim Kim, Yohan Jo*

**Main category:** cs.CL

**Keywords:** tool agents, API graphs, In-N-Out dataset, multi-tool queries, LLM

**Relevance Score:** 9

**TL;DR:** This paper introduces In-N-Out, an expert-annotated dataset of API graphs that enhances LLM-based tool agents' abilities to handle multi-tool queries and API dependencies, improving performance significantly in task execution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLM-based tool agents tackle increasingly complex tasks, they struggle to identify and call the correct APIs in the necessary order, necessitating a better understanding of API documentation and relationships.

**Method:** The paper proposes converting API documentation into a structured API graph that captures API dependencies and facilitates multi-tool queries. It introduces the In-N-Out dataset created from two real-world API benchmarks and their documentation.

**Key Contributions:**

	1. Introduction of the In-N-Out dataset of API graphs
	2. Demonstration of improved performance in API retrieval and multi-tool query generation
	3. Highlighting the importance of API graphs for tool agents

**Result:** Using the In-N-Out dataset significantly boosts performance on both tool retrieval and multi-tool query generation, with improvements nearly doubling that achieved by LLMs relying solely on documentation. Fine-tuned models on In-N-Out close 90% of the performance gap.

**Limitations:** 

**Conclusion:** The results suggest that explicit API graphs can greatly enhance tool agents' abilities, and the In-N-Out dataset serves as a valuable resource for this purpose.

**Abstract:** Tool agents -- LLM-based systems that interact with external APIs -- offer a way to execute real-world tasks. However, as tasks become increasingly complex, these agents struggle to identify and call the correct APIs in the proper order. To tackle this problem, we investigate converting API documentation into a structured API graph that captures API dependencies and leveraging it for multi-tool queries that require compositional API calls. To support this, we introduce In-N-Out, the first expert-annotated dataset of API graphs built from two real-world API benchmarks and their documentation. Using In-N-Out significantly improves performance on both tool retrieval and multi-tool query generation, nearly doubling that of LLMs using documentation alone. Moreover, graphs generated by models fine-tuned on In-N-Out close 90% of this gap, showing that our dataset helps models learn to comprehend API documentation and parameter relationships. Our findings highlight the promise of using explicit API graphs for tool agents and the utility of In-N-Out as a valuable resource. We will release the dataset and code publicly.

</details>


### [129] [Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief](https://arxiv.org/abs/2509.01564)

*Zeguan Xiao, Diyang Dou, Boya Xiong, Yun Chen, Guanhua Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, calibration, self-evaluation, uncertainty, confidence scores

**Relevance Score:** 8

**TL;DR:** EAGLE is a self-evaluation-based calibration method for Large Language Models (LLMs) that improves confidence score accuracy by leveraging internal hidden states.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of overconfidence in LLMs, especially those fine-tuned with Reinforcement Learning from Human Feedback (RLHF), impacting reliability and safety.

**Method:** EAGLE extracts internal beliefs from intermediate layers of LLMs during self-evaluation and aggregates these to compute a refined confidence score based on the expectation of the score distribution.

**Key Contributions:**

	1. Introduction of EAGLE for improved confidence score calibration
	2. Layer-wise examination of uncertainty in LLMs
	3. Analysis of self-evaluation prompt effects on confidence scores

**Result:** EAGLE significantly enhances calibration performance over existing methods across various datasets and LLMs, providing a more accurate reflection of the model's confidence.

**Limitations:** 

**Conclusion:** The proposed method not only improves calibration but also offers insights into uncertainty patterns and the effects of self-evaluation in LLMs.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range.

</details>


### [130] [Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply](https://arxiv.org/abs/2509.01606)

*Vivi Nastase, Paola Merlo*

**Main category:** cs.CL

**Keywords:** sentence embeddings, cosine similarity, linguistic tasks, embedding space, machine learning

**Relevance Score:** 7

**TL;DR:** This paper investigates the relationship between the geometry of sentence embeddings and their performance on various linguistic tasks.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether the closeness in embedding space of sentence embeddings predicts their effectiveness on different linguistic tasks.

**Method:** The authors compute sentence embeddings using three different methods: averaged token embeddings, the embedding of the [CLS] token, and a random token, then analyze the correlation between their distances and their performance on tasks.

**Key Contributions:**

	1. Investigation of various computation methods for sentence embeddings
	2. Analysis of the relationship between embedding geometry and task performance
	3. Insight into how linguistic information is encoded in embeddings

**Result:** The study finds that while cosine similarity captures some commonalities among embeddings, it does not predict their performance on tasks; performance relates more to weighted combinations of embedding dimensions.

**Limitations:** The study mainly focuses on cosine similarity and may not explore other potential metrics for predicting task performance.

**Conclusion:** The geometry of sentence embeddings is not a reliable predictor of their effectiveness on linguistic tasks; instead, information is encoded in complex dimensional combinations.

**Abstract:** Transformer models learn to encode and decode an input text, and produce contextual token embeddings as a side-effect. The mapping from language into the embedding space maps words expressing similar concepts onto points that are close in the space. In practice, the reverse implication is also assumed: words corresponding to close points in this space are similar or related, those that are further are not.   Does closeness in the embedding space extend to shared properties for sentence embeddings? We present an investigation of sentence embeddings and show that the geometry of their embedding space is not predictive of their relative performances on a variety of tasks.   We compute sentence embeddings in three ways: as averaged token embeddings, as the embedding of the special [CLS] token, and as the embedding of a random token from the sentence. We explore whether there is a correlation between the distance between sentence embedding variations and their performance on linguistic tasks, and whether despite their distances, they do encode the same information in the same manner.   The results show that the cosine similarity -- which treats dimensions shallowly -- captures (shallow) commonalities or differences between sentence embeddings, which are not predictive of their performance on specific tasks. Linguistic information is rather encoded in weighted combinations of different dimensions, which are not reflected in the geometry of the sentence embedding space.

</details>


### [131] [Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry](https://arxiv.org/abs/2509.01620)

*Shanshan Wang, Junchao Wu, Fengying Ye, Jingming Yao, Lidia S. Chao, Derek F. Wong*

**Main category:** cs.CL

**Keywords:** AI-generated poetry, large language models, poetry detection, Chinese poetry, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper proposes a benchmark for detecting AI-generated modern Chinese poetry, highlighting the challenges in distinguishing it from human-written poetry.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of AI-generated modern Chinese poetry disrupts the poetry ecosystem, necessitating effective detection methods.

**Method:** A high-quality dataset of 800 human-written and 41,600 AI-generated poems was constructed, and the performance of six detection systems was systematically assessed on this dataset.

**Key Contributions:**

	1. Development of a novel benchmark for detecting AI-generated modern Chinese poetry.
	2. Creation of a large dataset combining human and AI-generated poems.
	3. Assessment of current detection systems' performance, revealing significant shortcomings.

**Result:** Current detectors are inadequate for reliably identifying AI-generated modern Chinese poems, with intrinsic poetic qualities, particularly style, proving hardest to detect.

**Limitations:** The study focuses exclusively on modern Chinese poetry and may not generalize to other poetic forms or languages.

**Conclusion:** The work emphasizes the need for effective benchmarks and lays the groundwork for future AI poetry detection research.

**Abstract:** The rapid development of advanced large language models (LLMs) has made AI-generated text indistinguishable from human-written text. Previous work on detecting AI-generated text has made effective progress, but has not involved modern Chinese poetry. Due to the distinctive characteristics of modern Chinese poetry, it is difficult to identify whether a poem originated from humans or AI. The proliferation of AI-generated modern Chinese poetry has significantly disrupted the poetry ecosystem. Based on the urgency of identifying AI-generated poetry in the real Chinese world, this paper proposes a novel benchmark for detecting LLMs-generated modern Chinese poetry. We first construct a high-quality dataset, which includes both 800 poems written by six professional poets and 41,600 poems generated by four mainstream LLMs. Subsequently, we conduct systematic performance assessments of six detectors on this dataset. Experimental results demonstrate that current detectors cannot be used as reliable tools to detect modern Chinese poems generated by LLMs. The most difficult poetic features to detect are intrinsic qualities, especially style. The detection results verify the effectiveness and necessity of our proposed benchmark. Our work lays a foundation for future detection of AI-generated poetry.

</details>


### [132] [TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring](https://arxiv.org/abs/2509.01640)

*Hind Aljuaid, Areej Alhothali, Ohoud Al-Zamzami, Hussein Assalahi*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Graph Neural Networks, Transformers, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** TransGAT integrates Transformers and GNNs for automated essay scoring, improving prediction accuracy over baseline methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Manual scoring of essays is labor-intensive and inconsistent, leading to a need for a more effective automated solution for essay scoring.

**Method:** TransGAT combines fine-tuned Transformer models with Graph Attention Networks (GAT) in a two-stream prediction approach. One stream produces essay-level predictions, while the other analyzes token embeddings with contextual relationships from syntactic dependencies.

**Key Contributions:**

	1. Introduction of TransGAT as a novel AES approach
	2. Combination of fine-tuned Transformers and GNNs for analytic scoring
	3. Demonstrated superior performance on the ELLIPSE dataset

**Result:** TransGAT outperforms existing models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across multiple analytic scoring dimensions on the ELLIPSE dataset.

**Limitations:** 

**Conclusion:** The study suggests that TransGAT could significantly improve the effectiveness of automated essay scoring systems.

**Abstract:** Essay writing is a critical component of student assessment, yet manual scoring is labor-intensive and inconsistent. Automated Essay Scoring (AES) offers a promising alternative, but current approaches face limitations. Recent studies have incorporated Graph Neural Networks (GNNs) into AES using static word embeddings that fail to capture contextual meaning, especially for polysemous words. Additionally, many methods rely on holistic scoring, overlooking specific writing aspects such as grammar, vocabulary, and cohesion. To address these challenges, this study proposes TransGAT, a novel approach that integrates fine-tuned Transformer models with GNNs for analytic scoring. TransGAT combines the contextual understanding of Transformers with the relational modeling strength of Graph Attention Networks (GAT). It performs two-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa, and DeBERTaV3) with a separate GAT. In each pair, the first stream generates essay-level predictions, while the second applies GAT to Transformer token embeddings, with edges constructed from syntactic dependencies. The model then fuses predictions from both streams to produce the final analytic score. Experiments on the ELLIPSE dataset show that TransGAT outperforms baseline models, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all analytic scoring dimensions. These findings highlight the potential of TransGAT to advance AES systems.

</details>


### [133] [Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions](https://arxiv.org/abs/2509.01654)

*Dominic Plein*

**Main category:** cs.CL

**Keywords:** phonetic similarity, Needleman-Wunsch, Rust, CUDA, clustering

**Relevance Score:** 4

**TL;DR:** A new method for calculating phonetic similarity between words using the Needleman-Wunsch algorithm, implemented in Rust with parallel processing capabilities for large datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and analyze the phonetic structure of languages through phonetic similarity of words.

**Method:** The method employs the Needleman-Wunsch algorithm for phonetic transcription similarity, implemented in Rust and parallelized using CPU and GPU (CUDA).

**Key Contributions:**

	1. Introduction of a phonetic similarity calculation method using the Needleman-Wunsch algorithm
	2. Implementation in Rust with CPU and GPU parallel processing
	3. Construction and analysis of a fully-connected graph for clustering similar words.

**Result:** The GPU implementation significantly improves performance, enabling efficient handling of large datasets; a fully-connected graph is created from the results, facilitating clustering of phonetically similar words.

**Limitations:** 

**Conclusion:** The proposed method is feasible and effective for analyzing phonetic structures and can be adapted for different languages.

**Abstract:** We present a method to calculate the similarity between words based on their phonetic transcription (their pronunciation) using the Needleman-Wunsch algorithm. We implement this algorithm in Rust and parallelize it on both CPU and GPU to handle large datasets efficiently. The GPU implementation leverages CUDA and the cudarc Rust library to achieve significant performance improvements. We validate our approach by constructing a fully-connected graph where nodes represent words and edges have weights according to the similarity between the words. This graph is then analyzed using clustering algorithms to identify groups of phonetically similar words. Our results demonstrate the feasibility and effectiveness of the proposed method in analyzing the phonetic structure of languages. It might be easily expanded to other languages.

</details>


### [134] [Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection](https://arxiv.org/abs/2509.01660)

*Zhengjia Wang, Qiang Sheng, Danding Wang, Beizhe Hu, Juan Cao*

**Main category:** cs.CL

**Keywords:** fake news detection, intent modeling, graph-based learning, semantic analysis, machine learning

**Relevance Score:** 6

**TL;DR:** This paper presents a novel approach for fake news detection by incorporating news intent alongside semantic clues, using a graph-based model to enhance detection accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on semantic clues alone in fake news detection can lead to poor performance as patterns shift. This research aims to integrate news intent to better understand the underlying motives behind news deception.

**Method:** The proposed method, Graph-based Intent-Semantic Joint Modeling (InSide), models deception cues using heterogeneous graph structures to facilitate the interaction between semantic and intent signals.

**Key Contributions:**

	1. Introduction of a graph-based model for integrating news intent and semantics.
	2. Development of a dynamic pathway-based graph alignment strategy for improved message passing.
	3. Demonstration of superior performance over state-of-the-art methods in fake news detection.

**Result:** Extensive experiments show that InSide outperforms existing state-of-the-art fake news detection methods across four benchmark datasets.

**Limitations:** 

**Conclusion:** Incorporating news intent significantly enhances the detection of fake news by addressing the limitations of traditional semantic-only approaches.

**Abstract:** Fake news detection is an important and challenging task for defending online information integrity. Existing state-of-the-art approaches typically extract news semantic clues, such as writing patterns that include emotional words, stylistic features, etc. However, detectors tuned solely to such semantic clues can easily fall into surface detection patterns, which can shift rapidly in dynamic environments, leading to limited performance in the evolving news landscape. To address this issue, this paper investigates a novel perspective by incorporating news intent into fake news detection, bridging intents and semantics together. The core insight is that by considering news intents, one can deeply understand the inherent thoughts behind news deception, rather than the surface patterns within words alone. To achieve this goal, we propose Graph-based Intent-Semantic Joint Modeling (InSide) for fake news detection, which models deception clues from both semantic and intent signals via graph-based joint learning. Specifically, InSide reformulates news semantic and intent signals into heterogeneous graph structures, enabling long-range context interaction through entity guidance and capturing both holistic and implementation-level intent via coarse-to-fine intent modeling. To achieve better alignment between semantics and intents, we further develop a dynamic pathway-based graph alignment strategy for effective message passing and aggregation across these signals by establishing a common space. Extensive experiments on four benchmark datasets demonstrate the superiority of the proposed InSide compared to state-of-the-art methods.

</details>


### [135] [chDzDT: Word-level morphology-aware language model for Algerian social media text](https://arxiv.org/abs/2509.01772)

*Abdelkrime Aries*

**Main category:** cs.CL

**Keywords:** pre-trained language models, Algerian dialect, character-level modeling, morphological analysis, low-resource NLP

**Relevance Score:** 7

**TL;DR:** Introduction of chDzDT, a character-level pre-trained language model for the Algerian dialect, addressing challenges in morphology and code-switching that conventional models face.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The Algerian dialect is under-represented in NLP, hindering effective processing due to its complex characteristics.

**Method:** Developed a character-level PLM trained on isolated words, using diverse multilingual sources including social media and Wikipedia.

**Key Contributions:**

	1. Morphological analysis of Algerian dialect via YouTube comments.
	2. Construction of a multilingual Algerian lexicon dataset.
	3. Development and evaluation of a character-level PLM for morphology-focused tasks.

**Result:** The model performs robustly in encoding morphological patterns and shows effectiveness in downstream NLP tasks despite the dialect's complexities.

**Limitations:** 

**Conclusion:** chDzDT sets a precedent for character-level modeling in low-resource dialects, enhancing NLP adaptability.

**Abstract:** Pre-trained language models (PLMs) have substantially advanced natural language processing by providing context-sensitive text representations. However, the Algerian dialect remains under-represented, with few dedicated models available. Processing this dialect is challenging due to its complex morphology, frequent code-switching, multiple scripts, and strong lexical influences from other languages. These characteristics complicate tokenization and reduce the effectiveness of conventional word- or subword-level approaches.   To address this gap, we introduce chDzDT, a character-level pre-trained language model tailored for Algerian morphology. Unlike conventional PLMs that rely on token sequences, chDzDT is trained on isolated words. This design allows the model to encode morphological patterns robustly, without depending on token boundaries or standardized orthography. The training corpus draws from diverse sources, including YouTube comments, French, English, and Berber Wikipedia, as well as the Tatoeba project. It covers multiple scripts and linguistic varieties, resulting in a substantial pre-training workload.   Our contributions are threefold: (i) a detailed morphological analysis of Algerian dialect using YouTube comments; (ii) the construction of a multilingual Algerian lexicon dataset; and (iii) the development and extensive evaluation of a character-level PLM as a morphology-focused encoder for downstream tasks. The proposed approach demonstrates the potential of character-level modeling for morphologically rich, low-resource dialects and lays a foundation for more inclusive and adaptable NLP systems.

</details>


### [136] [Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs](https://arxiv.org/abs/2509.01790)

*Andong Hua, Kenan Tang, Chenhe Gu, Jindong Gu, Eric Wong, Yao Qin*

**Main category:** cs.CL

**Keywords:** large language models, prompt sensitivity, evaluation methods, LLM-as-a-Judge, semantics

**Relevance Score:** 9

**TL;DR:** This paper investigates the phenomenon of prompt sensitivity in large language models (LLMs), challenging the notion that it is an inherent weakness and suggesting it may result from current evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine whether the reported high prompt sensitivity in LLMs is a fundamental issue or a byproduct of evaluation processes.

**Method:** The authors evaluated 7 LLMs across 6 benchmarks using 12 diverse prompt templates, employing both traditional and LLM-as-a-Judge evaluation methods.

**Key Contributions:**

	1. Challenges the view of inherent prompt sensitivity in LLMs.
	2. Demonstrates the influence of evaluation methods on perceived performance variance.
	3. Suggests LLM-as-a-Judge evaluations improve robustness assessments of models.

**Result:** The research indicates that prompt sensitivity arises largely from heuristics in evaluation methods that fail to account for semantically correct alternative responses, revealing that LLMs exhibit more robustness to prompt variations than previously thought.

**Limitations:** 

**Conclusion:** The findings imply that the prompt sensitivity observed in many studies may be an artifact of evaluation techniques rather than an intrinsic flaw in the LLMs.

**Abstract:** Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e., repeating something written or spoken using different words) leads to significant changes in large language model (LLM) performance, has been widely accepted as a core limitation of LLMs. In this work, we revisit this issue and ask: Is the widely reported high prompt sensitivity truly an inherent weakness of LLMs, or is it largely an artifact of evaluation processes? To answer this question, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family) across 6 benchmarks, including both multiple-choice and open-ended tasks on 12 diverse prompt templates. We find that much of the prompt sensitivity stems from heuristic evaluation methods, including log-likelihood scoring and rigid answer matching, which often overlook semantically correct responses expressed through alternative phrasings, such as synonyms or paraphrases. When we adopt LLM-as-a-Judge evaluations, we observe a substantial reduction in performance variance and a consistently higher correlation in model rankings across prompts. Our findings suggest that modern LLMs are more robust to prompt templates than previously believed, and that prompt sensitivity may be more an artifact of evaluation than a flaw in the models.

</details>


### [137] [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)

*Shreyas Tirumala, Nishant Jain, Danny D. Leybzon, Trent D. Buskirk*

**Main category:** cs.CL

**Keywords:** AI interviewers, IVR systems, data collection, speech recognition, qualitative research

**Relevance Score:** 8

**TL;DR:** This position paper reviews AI interviewers' effectiveness in real-time voice-based surveys compared to traditional IVR systems for research data collection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate when AI interviewing systems are suitable for collecting data in both quantitative and qualitative research contexts.

**Method:** The paper assesses AI interviewers and IVR systems by their performance in speech recognition, answer recording, emotion handling, and verbal reasoning capabilities through field studies.

**Key Contributions:**

	1. Comparison of AI interviewers and IVR systems
	2. Assessment of performance across multiple dimensions
	3. Context-dependent utility for qualitative research

**Result:** Field studies show that AI interviewers surpass IVR systems in performance for both quantitative and qualitative data collection.

**Limitations:** Real-time transcription errors, limited emotion detection, and inconsistent follow-up quality.

**Conclusion:** Despite the advantages, current AI interviewer technology has some limitations, indicating that their utility may depend on specific contexts for qualitative data collection.

**Abstract:** Transformer-based Large Language Models (LLMs) have paved the way for "AI interviewers" that can administer voice-based surveys with respondents in real-time. This position paper reviews emerging evidence to understand when such AI interviewing systems are fit for purpose for collecting data within quantitative and qualitative research contexts. We evaluate the capabilities of AI interviewers as well as current Interactive Voice Response (IVR) systems across two dimensions: input/output performance (i.e., speech recognition, answer recording, emotion handling) and verbal reasoning (i.e., ability to probe, clarify, and handle branching logic). Field studies suggest that AI interviewers already exceed IVR capabilities for both quantitative and qualitative data collection, but real-time transcription error rates, limited emotion detection abilities, and uneven follow-up quality indicate that the utility, use and adoption of current AI interviewer technology may be context-dependent for qualitative data collection efforts.

</details>


### [138] [Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning](https://arxiv.org/abs/2509.01885)

*Zhimeng Luo, Abhibha Gupta, Adam Frisch, Daqing He*

**Main category:** cs.CL

**Keywords:** Electronic Health Records, Large Language Models, Text Generation, Information Extraction, Healthcare AI

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach to extracting OPQRST assessments from Electronic Health Records using Large Language Models, transforming the task from sequence labeling to text generation to enhance interpretability and accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity and unstructured nature of EHR data hinder effective extraction of critical patient information, leading to challenges in clinician decision-making.

**Method:** The authors propose using Large Language Models (LLMs) to reframe the information extraction task from sequence labeling to text generation, incorporating reasoning steps that reflect a clinician's cognitive processes, and modifying NER metrics to include semantic similarity measures.

**Key Contributions:**

	1. Introduction of LLM-based text generation for EHR information extraction
	2. Enhanced interpretability through mimicry of physician reasoning
	3. Modification of NER metrics with semantic similarity measures

**Result:** The proposed method demonstrates improved accuracy and usability in extracting information from EHRs, making it easier for clinicians to utilize these tools and improve patient care outcomes.

**Limitations:** 

**Conclusion:** By leveraging LLMs for EHR data extraction and enhancing evaluation metrics, this approach offers a scalable solution that aligns with clinician needs, ultimately supporting better patient care.

**Abstract:** The extraction of critical patient information from Electronic Health Records (EHRs) poses significant challenges due to the complexity and unstructured nature of the data. Traditional machine learning approaches often fail to capture pertinent details efficiently, making it difficult for clinicians to utilize these tools effectively in patient care. This paper introduces a novel approach to extracting the OPQRST assessment from EHRs by leveraging the capabilities of Large Language Models (LLMs). We propose to reframe the task from sequence labeling to text generation, enabling the models to provide reasoning steps that mimic a physician's cognitive processes. This approach enhances interpretability and adapts to the limited availability of labeled data in healthcare settings. Furthermore, we address the challenge of evaluating the accuracy of machine-generated text in clinical contexts by proposing a modification to traditional Named Entity Recognition (NER) metrics. This includes the integration of semantic similarity measures, such as the BERT Score, to assess the alignment between generated text and the clinical intent of the original records. Our contributions demonstrate a significant advancement in the use of AI in healthcare, offering a scalable solution that improves the accuracy and usability of information extraction from EHRs, thereby aiding clinicians in making more informed decisions and enhancing patient care outcomes.

</details>


### [139] [Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints](https://arxiv.org/abs/2509.01899)

*Zhimeng Luo, Zhendong Wang, Rui Meng, Diyang Xue, Adam Frisch, Daqing He*

**Main category:** cs.CL

**Keywords:** weakly supervised learning, entity extraction, chief complaints, medical text mining, BERT

**Relevance Score:** 8

**TL;DR:** A weakly supervised method to extract and link entities in chief complaints from medical records without human annotation is proposed, demonstrating improved performance over previous approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of varied notations in chief complaint records which hinder standardization for record keeping and text mining across medical institutions.

**Method:** The study employs a split-and-match algorithm to create weak annotations from 1.2 million de-identified chief complaint records, followed by training a BERT-based model to locate entity mentions and link them to a predefined ontology.

**Key Contributions:**

	1. Introduction of a weakly supervised method for entity extraction
	2. Utilization of a large dataset (1.2 million records) for training
	3. Demonstration of improved performance over existing methods without human annotation.

**Result:** The proposed method achieved superior performance compared to prior approaches that required human annotations, demonstrating effectiveness in extracting and linking medical entities.

**Limitations:** 

**Conclusion:** The Weakly Supervised Entity Extraction and Linking method successfully enhances the process of handling chief complaint records by eliminating the need for human annotations while improving accuracy.

**Abstract:** A Chief complaint (CC) is the reason for the medical visit as stated in the patient's own words. It helps medical professionals to quickly understand a patient's situation, and also serves as a short summary for medical text mining. However, chief complaint records often take a variety of entering methods, resulting in a wide variation of medical notations, which makes it difficult to standardize across different medical institutions for record keeping or text mining. In this study, we propose a weakly supervised method to automatically extract and link entities in chief complaints in the absence of human annotation. We first adopt a split-and-match algorithm to produce weak annotations, including entity mention spans and class labels, on 1.2 million real-world de-identified and IRB approved chief complaint records. Then we train a BERT-based model with generated weak labels to locate entity mentions in chief complaint text and link them to a pre-defined ontology. We conducted extensive experiments, and the results showed that our Weakly Supervised Entity Extraction and Linking (\ours) method produced superior performance over previous methods without any human annotation.

</details>


### [140] [DRAssist: Dispute Resolution Assistance using Large Language Models](https://arxiv.org/abs/2509.01962)

*Sachin Pawar, Manoj Apte, Girish K. Palshikar, Basit Ali, Nitin Ramrakhiyani*

**Main category:** cs.CL

**Keywords:** dispute resolution, large language models, legal technology, DRAssist, automobile insurance

**Relevance Score:** 9

**TL;DR:** This paper presents DRAssist, a system that utilizes large language models (LLMs) to assist human judges in resolving disputes from automobile insurance and domain name sectors by summarizing unstructured data and evaluating party arguments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve the efficiency and accuracy of dispute resolution processes by leveraging advanced LLMs to assist judges in legal contexts.

**Method:** DRAssist identifies structural elements of disputes, prompts LLMs to provide structured resolutions at multiple levels, and evaluates performance against baseline measures.

**Key Contributions:**

	1. Introduction of DRAssist system for dispute resolution
	2. Application of LLMs in legal contexts
	3. Evaluation of various prompting strategies for optimal LLM outputs

**Result:** The system was tested with several prompting strategies for LLMs, and the evaluations showed promising results in identifying stronger parties and evaluating arguments.

**Limitations:** The paper may not address broader legal implications of using AI in dispute resolution.

**Conclusion:** DRAssist demonstrates the potential of LLMs in legal resolution settings but requires further investigation for refinement.

**Abstract:** Disputes between two parties occur in almost all domains such as taxation, insurance, banking, healthcare, etc. The disputes are generally resolved in a specific forum (e.g., consumer court) where facts are presented, points of disagreement are discussed, arguments as well as specific demands of the parties are heard, and finally a human judge resolves the dispute by often favouring one of the two parties. In this paper, we explore the use of large language models (LLMs) as assistants for the human judge to resolve such disputes, as part of our DRAssist system. We focus on disputes from two specific domains -- automobile insurance and domain name disputes. DRAssist identifies certain key structural elements (e.g., facts, aspects or disagreement, arguments) of the disputes and summarizes the unstructured dispute descriptions to produce a structured summary for each dispute. We then explore multiple prompting strategies with multiple LLMs for their ability to assist in resolving the disputes in these domains. In DRAssist, these LLMs are prompted to produce the resolution output at three different levels -- (i) identifying an overall stronger party in a dispute, (ii) decide whether each specific demand of each contesting party can be accepted or not, (iii) evaluate whether each argument by each contesting party is strong or weak. We evaluate the performance of LLMs on all these tasks by comparing them with relevant baselines using suitable evaluation metrics.

</details>


### [141] [StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching](https://arxiv.org/abs/2509.02033)

*Chao Xue, Ziyuan Gao*

**Main category:** cs.CL

**Keywords:** Semantic Matching, Graph Neural Networks, Contrastive Learning

**Relevance Score:** 4

**TL;DR:** This paper presents StructCoh, a graph-enhanced contrastive learning framework for text semantic matching that improves semantic distinctions and structural reasoning.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Text semantic matching often struggles with understanding structural relationships and fine-grained semantic distinctions in documents, particularly in legal contexts.

**Method:** The proposed StructCoh uses a dual-graph encoder to create semantic graphs and applies graph isomorphism networks for feature propagation. It employs a hierarchical contrastive objective with both node-level and graph-aware contrastive learning.

**Key Contributions:**

	1. Introduction of a dual-graph encoder for semantic graph construction
	2. Implementation of graph isomorphism networks for structural feature propagation
	3. Development of a hierarchical contrastive learning objective that enhances semantic matching at various levels

**Result:** StructCoh demonstrates significant improvements over state-of-the-art methods, achieving an F1-score of 86.7% in legal statute matching, marking a 6.2% gain.

**Limitations:** 

**Conclusion:** The StructCoh framework successfully enhances the capturing of structural and semantic nuances in text matching tasks, particularly in legal document analysis.

**Abstract:** Text semantic matching requires nuanced understanding of both structural relationships and fine-grained semantic distinctions. While pre-trained language models excel at capturing token-level interactions, they often overlook hierarchical structural patterns and struggle with subtle semantic discrimination. In this paper, we proposed StructCoh, a graph-enhanced contrastive learning framework that synergistically combines structural reasoning with representation space optimization. Our approach features two key innovations: (1) A dual-graph encoder constructs semantic graphs via dependency parsing and topic modeling, then employs graph isomorphism networks to propagate structural features across syntactic dependencies and cross-document concept nodes. (2) A hierarchical contrastive objective enforces consistency at multiple granularities: node-level contrastive regularization preserves core semantic units, while graph-aware contrastive learning aligns inter-document structural semantics through both explicit and implicit negative sampling strategies. Experiments on three legal document matching benchmarks and academic plagiarism detection datasets demonstrate significant improvements over state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score (+6.2% absolute gain) on legal statute matching by effectively identifying argument structure similarities.

</details>


### [142] [DeepSeek performs better than other Large Language Models in Dental Cases](https://arxiv.org/abs/2509.02036)

*Hexian Zhang, Xinyu Yan, Yanqi Yang, Lijian Jin, Ping Yang, Junwen Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Health Informatics, Dentistry, Clinical Reasoning, AI in Healthcare

**Relevance Score:** 9

**TL;DR:** This study evaluates four large language models (LLMs) for their ability to analyze longitudinal dental case narratives, with DeepSeek emerging as the top performer.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of large language models in interpreting longitudinal patient narratives in dentistry and to assess their reasoning abilities in clinical tasks.

**Method:** The study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) using 34 standardized longitudinal periodontal cases, comprising 258 question-answer pairs, assessed through automated metrics and blinded evaluations by licensed dentists.

**Key Contributions:**

	1. Evaluation of LLMs in a specific healthcare domain (dentistry)
	2. Identification of DeepSeek as the top performer among evaluated LLMs
	3. Recommendations for integrating LLMs in medical education and research

**Result:** DeepSeek outperformed other models with a superior faithfulness score and higher expert ratings, demonstrating its effectiveness in analyzing dental case vignettes.

**Limitations:** 

**Conclusion:** DeepSeek is positioned as the leading LLM for dental case analysis, recommending its integration into medical education and research as a supportive tool.

**Abstract:** Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMs' reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent.

</details>


### [143] [NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task](https://arxiv.org/abs/2509.02038)

*Bashar Talafha, Hawau Olamide Toyin, Peter Sullivan, AbdelRahim Elmadany, Abdurrahman Juma, Amirbek Djanibekov, Chiyu Zhang, Hamad Alshehhi, Hanan Aldarmaki, Mustafa Jarrar, Nizar Habash, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** Arabic speech processing, dialect identification, speech recognition, diacritic restoration, NADI

**Relevance Score:** 3

**TL;DR:** This paper presents the results of the NADI 2025 Shared Task on Arabic speech dialect processing, detailing submissions across three subtasks and highlighting challenges in the field.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance research in Arabic dialect processing by evaluating various methods through a competitive shared task.

**Method:** The study involved 44 teams competing across three subtasks: dialect identification, speech recognition, and diacritic restoration, with performance evaluated based on accuracy and error rates.

**Key Contributions:**

	1. Presentation of benchmark results for Arabic dialect processing
	2. Analysis of diverse methodologies applied by competing teams
	3. Identification of key challenges in dialect recognition and restoration

**Result:** The best systems achieved 79.8% accuracy in dialect identification, with varying WER/CER results across speech recognition and diacritic restoration subtasks.

**Limitations:** The study primarily focuses on Arabic dialects, which may not be applicable to other language dialects or languages.

**Conclusion:** The results reveal significant challenges in Arabic dialect processing, indicating the need for further improvements and future task iterations.

**Abstract:** We present the findings of the sixth Nuanced Arabic Dialect Identification (NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spoken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 "five teams{\ae}, 47 submissions for Subtask 2 "six teams", and 19 submissions for Subtask 3 "two teams". The best-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20 WER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These results highlight the ongoing challenges of Arabic dialect speech processing, particularly in dialect identification, recognition, and diacritic restoration. We also summarize the methods adopted by participating teams and briefly outline directions for future editions of NADI.

</details>


### [144] [Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation](https://arxiv.org/abs/2509.02040)

*Guangzeng Han, Weisi Liu, Xiaolei Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Synthetic Data Generation, Genetic Algorithms, NLP, Active Learning

**Relevance Score:** 9

**TL;DR:** Genetic Prompt uses genetic algorithms with LLMs to improve synthetic data generation quality and diversity for NLP tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality and diversity of synthetic data generated by Large Language Models (LLMs).

**Method:** The framework treats semantic text attributes as gene sequences and employs genetic algorithms to perform crossover and mutation operations, supplemented by an active learning scheme for parent selection.

**Key Contributions:**

	1. Introduction of Genetic Prompt framework combining genetic algorithms with LLMs
	2. Demonstrated improvement in synthetic data quality and diversity
	3. Effective in boosting downstream model performance, especially for class-imbalanced data.

**Result:** Genetic Prompt significantly outperforms state-of-the-art baselines across various NLP tasks and shows robust performance with different generator model sizes. Additionally, combining synthetic data with original training datasets enhances model performance, especially in class-imbalanced scenarios.

**Limitations:** 

**Conclusion:** Genetic Prompt is validated as an effective method for generating high-quality synthetic data applicable to various NLP tasks.

**Abstract:** Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.

</details>


### [145] [How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis](https://arxiv.org/abs/2509.02075)

*Elisabetta Rocchetti, Alfio Ferrara*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction-tuning, Length control, Cumulative Weighted Attribution, Text generation

**Relevance Score:** 7

**TL;DR:** This study investigates the impact of instruction-tuning on length-controlled text generation in LLMs, revealing it enhances performance, particularly in later model layers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of generating text that adheres to explicit length constraints in LLMs.

**Method:** We analyze length-controlled text generation performance in LLMs using Cumulative Weighted Attribution in both English and Italian.

**Key Contributions:**

	1. Demonstrates the effectiveness of instruction-tuning in length control for LLMs.
	2. Compares performance in English and Italian, highlighting linguistic differences.
	3. Introduces Cumulative Weighted Attribution for analyzing model contributions.

**Result:** Instruction-tuning improves length control significantly, with positive contributions from attention heads in deeper layers for English, while final-layer MLPs play a crucial role in Italian.

**Limitations:** The study primarily focuses on two languages, which may limit the generalizability of the findings.

**Conclusion:** Instruction-tuning reconfigures model layers to enhance task adherence, with adjustments based on linguistic context.

**Abstract:** Adhering to explicit length constraints, such as generating text with a precise word count, remains a significant challenge for Large Language Models (LLMs). This study aims at investigating the differences between foundation models and their instruction-tuned counterparts, on length-controlled text generation in English and Italian. We analyze both performance and internal component contributions using Cumulative Weighted Attribution, a metric derived from Direct Logit Attribution. Our findings reveal that instruction-tuning substantially improves length control, primarily by specializing components in deeper model layers. Specifically, attention heads in later layers of IT models show increasingly positive contributions, particularly in English. In Italian, while attention contributions are more attenuated, final-layer MLPs exhibit a stronger positive role, suggesting a compensatory mechanism. These results indicate that instruction-tuning reconfigures later layers for task adherence, with component-level strategies potentially adapting to linguistic context.

</details>


### [146] [Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization](https://arxiv.org/abs/2509.02093)

*Juhyeon Lee, Wonduk Seo, Hyunjin An, Seunghyun Lee, Yi Bu*

**Main category:** cs.CL

**Keywords:** prompt optimization, contrastive reasoning, large language models, NLP

**Relevance Score:** 9

**TL;DR:** This paper introduces Contrastive Reasoning Prompt Optimization (CRPO), which utilizes LLMs' reasoning capabilities to optimize prompts by comparing high and low quality examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of prompts for Large Language Models (LLMs) by leveraging their inherent reasoning capability through contrasting examples.

**Method:** The CRPO framework retrieves reference prompts from the HelpSteer2 dataset and employs two paradigms: tiered contrastive reasoning and multi-metric contrastive reasoning to refine prompt generation.

**Key Contributions:**

	1. Introduction of CRPO for prompt optimization
	2. Utilization of contrasting examples for learning
	3. Demonstrated significant performance improvements over baseline methods

**Result:** CRPO significantly outperforms existing baseline methods on the HelpSteer2 benchmark for prompt optimization.

**Limitations:** 

**Conclusion:** The contrastive, retrieval-augmented reasoning approach promises advancements in automatic prompt optimization for LLMs, leading to more robust and interpretable results.

**Abstract:** Automatic prompt optimization has recently emerged as a strategy for improving the quality of prompts used in Large Language Models (LLMs), with the goal of generating more accurate and useful responses. However, most prior work focuses on direct prompt refinement or model fine-tuning, overlooking the potential of leveraging LLMs' inherent reasoning capability to learn from contrasting examples. In this paper, we present Contrastive Reasoning Prompt Optimization (CRPO), a novel framework that formulates prompt optimization as a retrieval augmented reasoning process. Our approach retrieves top k reference prompts from the HelpSteer2 dataset, an open-source collection annotated for helpfulness, correctness, coherence, complexity, and verbosity, and constructs two complementary optimization paradigms: (1) tiered contrastive reasoning, where the LLM compares high, medium, and low quality prompts to refine its own generation through reflective reasoning, and (2) multi-metric contrastive reasoning, where the LLM analyzes the best prompts along each evaluation dimension and integrates their strengths into an optimized prompt. By explicitly contrasting high and low quality exemplars, CRPO enables the model to deduce why certain prompts succeed while others fail, thereby achieving more robust and interpretable optimization. Experimental results on the HelpSteer2 benchmark demonstrate that CRPO significantly outperforms baselines. Our findings highlight the promise of contrastive, retrieval-augmented reasoning for advancing automatic prompt optimization.

</details>


### [147] [JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer](https://arxiv.org/abs/2509.02097)

*Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang, Shengjie Ma, Yinghan Shen, Yuanzhuo Wang*

**Main category:** cs.CL

**Keywords:** large language models, evaluation framework, knowledge-target adaptive, dynamic evaluation, interactive testing

**Relevance Score:** 9

**TL;DR:** JudgeAgent is a dynamic evaluation framework for large language models (LLMs) that addresses key challenges in LLM evaluation methods by employing an interviewer-style evaluation paradigm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of LLMs, which faces issues like limited interaction, difficulty control, and verification of results.

**Method:** JudgeAgent utilizes a knowledge-target adaptive approach with benchmark grading, interactive testing, and feedback mechanisms to enhance evaluation.

**Key Contributions:**

	1. Introduction of an interviewer-style evaluation paradigm for LLMs
	2. Comprehensive evaluation framework combining various assessment methods
	3. Demonstration of enhanced accuracy and effectiveness in evaluation results

**Result:** The framework provides more accurate and effective evaluation results through extended testing with adaptive difficulty adjustment.

**Limitations:** 

**Conclusion:** JudgeAgent demonstrates a novel and effective evaluation methodology that surpasses traditional evaluation paradigms through extensive experiments.

**Abstract:** Evaluating the capabilities of large language models (LLMs) is an essential step to ensure the successful application of LLMs across various domains. The current evaluation of LLMs is based on a paradigm that involves querying them with predefined question sets and assessing their outputs. This paradigm offers controllable processes and simplicity, but faces challenges such as limited interaction with targets, insufficient difficulty control, and difficulties in verifying the validity of evaluation results, making it hard to precisely determine the knowledge and capability boundaries of target models. To address these challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic evaluation framework based on a new interviewer-style evaluation paradigm. JudgeAgent employs a comprehensive evaluation approach consisting of benchmark grading, interactive extension, and evaluation feedback. It utilizes knowledge-driven data synthesis and target-adaptive difficulty adjustment methods to conduct extended testing, providing accurate and effective evaluation results. We also introduce a novel insight into validating evaluation methods, demonstrating the effectiveness of JudgeAgent and its dynamic evaluation paradigm through extensive experiments.

</details>


### [148] [CMRAG: Co-modality-based document retrieval and visual question answering](https://arxiv.org/abs/2509.02123)

*Wang Chen, Guanqiang Qi, Weikang Li, Yang Li*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Co-modality, Visual Question Answering

**Relevance Score:** 8

**TL;DR:** This paper introduces Co-modality-based Retrieval-Augmented Generation (CMRAG) to improve question answering on multimodal documents by effectively integrating text and image information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing retrieval-augmented generation methods struggle with multimodal documents, limiting the effective use of both text and images in question answering tasks.

**Method:** The proposed CMRAG first performs structured parsing to obtain co-modality representations and utilizes cross-modal retrieval to combine text and image evidence when responding to user queries.

**Key Contributions:**

	1. Introduction of co-modality representations for improved document processing.
	2. Development of a unified framework for text and image retrieval.
	3. Significant performance enhancements in visual question-answering tasks.

**Result:** Experiments indicate that CMRAG significantly outperforms traditional pure-vision-based retrieval-augmented generation methods in visual document question answering tasks.

**Limitations:** 

**Conclusion:** Integrating co-modality information into the RAG framework improves the performance and effectiveness of complex document visual question-answering systems.

**Abstract:** Retrieval-Augmented Generation (RAG) has become a core paradigm in document question answering tasks. However, existing methods have limitations when dealing with multimodal documents: one category of methods relies on layout analysis and text extraction, which can only utilize explicit text information and struggle to capture images or unstructured content; the other category treats document segmentation as visual input and directly passes it to visual language models (VLMs) for processing, yet it ignores the semantic advantages of text, leading to suboptimal generation results. This paper proposes co-modality-based RAG (CMRAG), which can simultaneously leverage text and images for efficient retrieval and generation. Specifically, we first perform structured parsing on documents to obtain co-modality representations of text segments and image regions. Subsequently, in response to user queries, we retrieve candidate evidence from text and image channels, respectively, and aggregate the results at the cross-modal retrieval level. Finally, we prompt the VLM to generate the final response based on the co-modality retrieval results. Experiments demonstrate that our method significantly outperforms pure-vision-based RAG in visual document question answering tasks. The findings of this paper show that integrating co-modality information into the RAG framework in a unified manner is an effective approach to improving the performance of complex document visual question-answering (VQA) systems.

</details>


### [149] [AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models](https://arxiv.org/abs/2509.02133)

*Snehasis Mukhopadhyay, Aryan Kasat, Shivam Dubey, Rahul Karthikeyan, Dhruv Sood, Vinija Jain, Aman Chadha, Amitava Das*

**Main category:** cs.CL

**Keywords:** Large Language Models, caste bias, communal bias, fairness, India

**Relevance Score:** 9

**TL;DR:** This paper proposes AMBEDKAR, a fairness framework for Large Language Models (LLMs) in the Indian context, addressing sociocultural biases related to caste and religion with a novel Constitution-Aware Decoding Layer.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate harmful societal biases present in LLMs, especially regarding caste and religion in India, where existing strategies are often Western-centric.

**Method:** The AMBEDKAR framework introduces a Constitution-Aware Decoding Layer that operates during inference time to reduce biases, using a speculative decoding algorithm without changing the model internally.

**Key Contributions:**

	1. Introduced AMBEDKAR for mitigating caste and communal bias in LLMs.
	2. Developed Constitution-Aware Decoding Layer for real-time fairness adjustments.
	3. Proposed fairness-by-speculation paradigm enhancing bias robustness without model internals alteration.

**Result:** The approach resulted in an absolute reduction of bias by up to 26.41% compared to baseline models.

**Limitations:** 

**Conclusion:** The fairness-by-speculation paradigm allows for enhanced LLM outputs that respect the principles of the AI Constitution of India without the need for retraining on biased data.

**Abstract:** Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/

</details>


### [150] [Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages](https://arxiv.org/abs/2509.02160)

*David Demitri Africa, Suchir Salhan, Yuval Weiss, Paula Buttery, Richard Diehl Martinez*

**Main category:** cs.CL

**Keywords:** named-entity recognition, low-resource languages, meta-learning, zero-shot, multilingual, language models

**Relevance Score:** 8

**TL;DR:** The paper explores the effectiveness of small decoder language models for zero-shot named-entity recognition (NER) in low-resource languages, utilizing model-agnostic meta-learning (MAML) to enhance adaptation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Named-entity recognition (NER) in low-resource languages often relies on large multilingual language models, which may be impractical due to resource constraints. This research investigates the potential of smaller models and novel training techniques for effective zero-shot adaptation to new languages.

**Method:** The study replaces part of the autoregressive objective of small decoder language models with first-order model-agnostic meta-learning (MAML) to improve their performance in NER tasks for low-resource languages.

**Key Contributions:**

	1. Introduction of MAML for small decoder LMs to enhance zero-shot NER
	2. Demonstrated effectiveness in Tagalog and Cebuano, highlighting the model's adaptability
	3. Reduction in convergence time for model training.

**Result:** MAML improves zero-shot micro-F1 scores by 2-6 percentage points during head-only tuning and by 1-3 percentage points after full tuning. Additionally, it reduces convergence time by up to 8%, with the most significant gains seen in the recognition of single-token person entities in Tagalog.

**Limitations:** The approach may still be limited by the inherent challenges of low-resource language training data and model generalization.

**Conclusion:** Utilizing MAML allows smaller language models to perform more effectively in zero-shot settings for NER tasks, suggesting a viable alternative to larger models in low-resource environments.

**Abstract:** Named-entity recognition (NER) in low-resource languages is usually tackled by finetuning very large multilingual LMs, an option that is often infeasible in memory- or latency-constrained settings. We ask whether small decoder LMs can be pretrained so that they adapt quickly and transfer zero-shot to languages unseen during pretraining. To this end we replace part of the autoregressive objective with first-order model-agnostic meta-learning (MAML). Tagalog and Cebuano are typologically similar yet structurally different in their actor/non-actor voice systems, and hence serve as a challenging test-bed. Across four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp under head-only tuning and 1-3 pp after full tuning, while cutting convergence time by up to 8%. Gains are largest for single-token person entities that co-occur with Tagalog case particles si/ni, highlighting the importance of surface anchors.

</details>


### [151] [Avoidance Decoding for Diverse Multi-Branch Story Generation](https://arxiv.org/abs/2509.02170)

*Kyeongman Park, Nakyeong Yang, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** Large Language Models, Avoidance Decoding, story generation

**Relevance Score:** 8

**TL;DR:** This paper introduces Avoidance Decoding, a novel strategy that enhances output diversity in Large Language Models by penalizing similarity to prior generated texts, achieving significantly reduced repetition and improved creativity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models tend to produce repetitive outputs, especially in creative tasks like story generation, limiting their effectiveness in generating diverse narratives.

**Method:** The proposed Avoidance Decoding modifies token logits by applying penalties based on two measures of similarity—concept-level in early stages and narrative-level as the story progresses—to encourage diversity in generated outputs.

**Key Contributions:**

	1. Introduction of Avoidance Decoding for LLMs
	2. Improvement of output diversity by up to 2.6 times
	3. Reduction of redundancy in story generation by 30%

**Result:** The method achieves up to 2.6 times higher output diversity and a 30% reduction in repetition compared to baseline models, demonstrating a substantial improvement in the creative diversity of generated stories.

**Limitations:** 

**Conclusion:** Avoidance Decoding not only diversifies outputs but also enhances the model's use of its intrinsic creativity by activating a wider range of neurons during generation.

**Abstract:** Large Language Models (LLMs) often generate repetitive and monotonous outputs, especially in tasks like story generation, due to limited creative diversity when given the same input prompt. To address this challenge, we propose a novel decoding strategy, Avoidance Decoding, that modifies token logits by penalizing similarity to previously generated outputs, thereby encouraging more diverse multi-branch stories. This penalty adaptively balances two similarity measures: (1) Concept-level Similarity Penalty, which is prioritized in early stages to diversify initial story concepts, and (2) Narrative-level Similarity Penalty, which is increasingly emphasized later to ensure natural yet diverse plot development. Notably, our method achieves up to 2.6 times higher output diversity and reduces repetition by an average of 30% compared to strong baselines, while effectively mitigating text degeneration. Furthermore, we reveal that our method activates a broader range of neurons, demonstrating that it leverages the model's intrinsic creativity.

</details>


### [152] [FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain](https://arxiv.org/abs/2509.02198)

*Anum Afzal, Juraj Vladika, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fact-checking, Medical domain, FActBench, NLI

**Relevance Score:** 9

**TL;DR:** The paper presents a comprehensive benchmark for evaluating LLMs in the medical domain, focusing on factuality and fact-checking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges that Large Language Models face in specialized domains, particularly the medical field, where factual accuracy is crucial.

**Method:** The paper introduces FActBench, a fact-checking benchmark that evaluates LLMs on four generation tasks using Chain-of-Thought (CoT) Prompting and Natural Language Inference (NLI) techniques.

**Key Contributions:**

	1. Introduction of FActBench for evaluating LLMs in the medical domain
	2. Application of two state-of-the-art fact-checking techniques (CoT and NLI)
	3. Demonstration of correlation between automated fact-checking scores and expert evaluations.

**Result:** Experiments demonstrate that the fact-checking scores derived from Unanimous Voting of both CoT and NLI techniques align closely with evaluations by domain experts.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of reliable fact-checking methods for enhancing the performance of LLMs in specialized fields, particularly health-related domains.

**Abstract:** Large Language Models tend to struggle when dealing with specialized domains. While all aspects of evaluation hold importance, factuality is the most critical one. Similarly, reliable fact-checking tools and data sources are essential for hallucination mitigation. We address these issues by providing a comprehensive Fact-checking Benchmark FActBench covering four generation tasks and six state-of-the-art Large Language Models (LLMs) for the Medical domain. We use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT) Prompting and Natural Language Inference (NLI). Our experiments show that the fact-checking scores acquired through the Unanimous Voting of both techniques correlate best with Domain Expert Evaluation.

</details>


### [153] [Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?](https://arxiv.org/abs/2509.02225)

*Jaime Collado-Montañez, L. Alfonso Ureña-López, Arturo Montejo-Ráez*

**Main category:** cs.CL

**Keywords:** Fundamental Language Model, modular language modeling, linguistic competence, factual retrieval, NLP efficiency

**Relevance Score:** 9

**TL;DR:** This paper introduces the Fundamental Language Model (FLM) paradigm, advocating for smaller, linguistically competent models that leverage external factual retrieval tools to address limitations of large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models, including hallucinations, biases, privacy concerns, and high computational costs, by proposing a modular approach to language modeling.

**Method:** The paper empirically evaluates models ranging from 135M to 32B parameters, focusing on their linguistic competence and factual knowledge (both internal and external).

**Key Contributions:**

	1. Introduction of the Fundamental Language Model (FLM) paradigm
	2. Empirical evaluation of model performance across different parameter sizes
	3. Support for a modular approach in language model architecture

**Result:** The study finds that while both linguistic competence and factual knowledge improve with scale, internal factual knowledge scales significantly faster than linguistic competence, indicating that larger models are more tied to memorization.

**Limitations:** 

**Conclusion:** The FLM paradigm promotes a shift toward compact, linguistically proficient models that enhance efficiency and interpretable NLP solutions by integrating external factual retrieval.

**Abstract:** Large Language Models offer impressive language capabilities but suffer from well-known limitations, including hallucinations, biases, privacy concerns, and high computational costs. These issues are largely driven by the combination of linguistic competence and factual memorization within a single monolithic model. This paper introduces and empirically supports the Fundamental Language Model (FLM) paradigm, which advocates for smaller, linguistically competent models that offload factual retrieval to external tools. We evaluate models ranging from 135M to 32B parameters across three dimensions: linguistic competence, external factual knowledge, and internal factual knowledge. Our findings reveal that while both linguistic competence and factual knowledge improve with scale, internal factual knowledge grows significantly faster, suggesting that model size is more closely tied to memorization than to core language ability. These results support a modular approach to language modeling, where compact, linguistically proficient models serve as the foundation for tool-augmented systems. The FLM paradigm offers a path toward more efficient, interpretable, and sustainable NLP solutions.

</details>


### [154] [LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue](https://arxiv.org/abs/2509.02292)

*Katharine Kowalyshyn, Matthias Scheutz*

**Main category:** cs.CL

**Keywords:** large language models, team dialogues, shared mental models, discrepancy detection, evaluation framework

**Relevance Score:** 9

**TL;DR:** This paper presents a two-step framework utilizing large language models to annotate team dialogues and detect discrepancies in shared mental models among team members.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of large language models in understanding human mindsets and identifying discrepancies in team dialogue.

**Method:** The framework involves LLMs generating annotations from task-oriented dialogues and a secondary LLM comparing these annotations to gold-standard labels to detect divergences.

**Key Contributions:**

	1. Development of a framework for using LLMs in team dialogue analysis
	2. Creation of a dataset with human and LLM annotations
	3. Establishment of an evaluation framework for SMM coherence

**Result:** A dataset of human and LLM annotations, a reproducible evaluation framework for assessing shared mental model coherence, and an empirical evaluation of LLM-based discrepancy detection were produced.

**Limitations:** LLMs struggle with tasks requiring spatial reasoning and prosodic cue interpretation.

**Conclusion:** LLMs show coherence in straightforward annotation tasks but struggle with spatial reasoning and disambiguating prosodic cues.

**Abstract:** What if large language models could not only infer human mindsets but also expose every blind spot in team dialogue such as discrepancies in the team members' joint understanding? We present a novel, two-step framework that leverages large language models (LLMs) both as human-style annotators of team dialogues to track the team's shared mental models (SMMs) and as automated discrepancy detectors among individuals' mental states. In the first step, an LLM generates annotations by identifying SMM elements within task-oriented dialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a secondary LLM compares these LLM-derived annotations and human annotations against gold-standard labels to detect and characterize divergences. We define an SMM coherence evaluation framework for this use case and apply it to six CReST dialogues, ultimately producing: (1) a dataset of human and LLM annotations; (2) a reproducible evaluation framework for SMM coherence; and (3) an empirical assessment of LLM-based discrepancy detection. Our results reveal that, although LLMs exhibit apparent coherence on straightforward natural-language annotation tasks, they systematically err in scenarios requiring spatial reasoning or disambiguation of prosodic cues.

</details>


### [155] [DCPO: Dynamic Clipping Policy Optimization](https://arxiv.org/abs/2509.02333)

*Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Dynamic Clipping, Token Exploration, Reward Standardization

**Relevance Score:** 6

**TL;DR:** The paper introduces Dynamic Clipping Policy Optimization (DCPO) to enhance reinforcement learning from verifiable rewards in large language models, overcoming issues of zero gradients and inefficient gradient updates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reinforcement learning methods like GRPO face challenges with zero gradients and ineffective updates due to clipping bounds and reward standardization, necessitating improvements for better model performance.

**Method:** Proposed a dynamic clipping strategy that adjusts clipping bounds based on token-specific prior probabilities and a smooth advantage standardization technique for more effective reinforcement learning in language models.

**Key Contributions:**

	1. Introduction of a dynamic clipping strategy for token-specific prior probabilities
	2. Development of a smooth advantage standardization technique
	3. Achieved state-of-the-art performance surpassing existing methods on several benchmarks

**Result:** DCPO achieved state-of-the-art results on four benchmarks with significant performance improvements over GRPO and DAPO, such as an Avg@1 of 46.7 on the AIME24 benchmark and a 28% increase in nonzero advantages.

**Limitations:** 

**Conclusion:** Dynamic Clipping Policy Optimization demonstrates superior efficiency and effectiveness in applying reinforcement learning to large language models, ultimately leveraging generated data better than traditional methods.

**Abstract:** Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.

</details>


### [156] [Implicit Reasoning in Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2509.02350)

*Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying*

**Main category:** cs.CL

**Keywords:** Large Language Models, Implicit Reasoning, Execution Paradigms, Evaluation Metrics, Computer Science

**Relevance Score:** 8

**TL;DR:** This paper surveys implicit reasoning in large language models (LLMs), introducing a taxonomy of execution paradigms for internal computation.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how reasoning unfolds internally within LLMs, beyond explicit chain-of-thought prompting.

**Method:** The paper categorizes existing methods into three execution paradigms: latent optimization, signal-guided control, and layer-recurrent execution; it reviews evidence supporting implicit reasoning.

**Key Contributions:**

	1. Introduces a taxonomy of execution paradigms for LLM internal computation
	2. Examines structural, behavioral, and representation evidence of implicit reasoning
	3. Provides a comprehensive overview of evaluation metrics and benchmarks for assessing implicit reasoning

**Result:** The survey reveals the advantages of implicit reasoning in LLMs, including lower generation costs and faster inference.

**Limitations:** 

**Conclusion:** A structured overview of evaluation metrics for implicit reasoning is provided, alongside a continuously updated project resource for ongoing research.

**Abstract:** Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning.We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.

</details>


### [157] [Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models](https://arxiv.org/abs/2509.02363)

*Gaurav Negi, Atul Kr. Ojha, Omnia Zayed, Paul Buitelaar*

**Main category:** cs.CL

**Keywords:** temporal opinion knowledge base, large language models, opinion mining, sentiment analysis, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** The paper presents a method to create a scalable temporal opinion knowledge base using LLMs for automated annotation, enhancing opinion analysis for various applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in temporal opinion analysis due to the lack of fine-grained annotations, which limits the potential of current methodologies.

**Method:** The proposed approach integrates established opinion mining formulations into an LLM annotation pipeline, allowing for structured opinion extraction with defined data models based on sentiment and opinion mining literature.

**Key Contributions:**

	1. Development of a scalable LLM-based annotation pipeline for opinion mining
	2. Definition of three data models for structured opinion representation
	3. Quantitative evaluation using human-annotated samples to ensure accuracy

**Result:** The evaluation shows successful structured representation of time-aligned opinions and high inter-annotator agreement, demonstrating the effectiveness of using LLMs for fine-grained annotation.

**Limitations:** The study may require further validation across various text domains and the performance of different LLMs needs thorough exploration.

**Conclusion:** The knowledge base created is valuable for applications such as RAG, temporal question answering, and timeline summarization, facilitating better opinion analysis.

**Abstract:** We propose a scalable method for constructing a temporal opinion knowledge base with large language models (LLMs) as automated annotators. Despite the demonstrated utility of time-series opinion analysis of text for downstream applications such as forecasting and trend analysis, existing methodologies underexploit this potential due to the absence of temporally grounded fine-grained annotations. Our approach addresses this gap by integrating well-established opinion mining formulations into a declarative LLM annotation pipeline, enabling structured opinion extraction without manual prompt engineering. We define three data models grounded in sentiment and opinion mining literature, serving as schemas for structured representation. We perform rigorous quantitative evaluation of our pipeline using human-annotated test samples. We carry out the final annotations using two separate LLMs, and inter-annotator agreement is computed label-wise across the fine-grained opinion dimensions, analogous to human annotation protocols. The resulting knowledge base encapsulates time-aligned, structured opinions and is compatible with applications in Retrieval-Augmented Generation (RAG), temporal question answering, and timeline summarisation.

</details>


### [158] [An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction](https://arxiv.org/abs/2509.02446)

*Ali Hamdi, Malak Mohamed, Rokaia Emad, Khaled Shaban*

**Main category:** cs.CL

**Keywords:** Social Telehealth, Disease Classification, Large Language Models, Arabic Medical Text, Ensemble Learning

**Relevance Score:** 8

**TL;DR:** The study evaluates Arabic medical text preprocessing methods and employs fine-tuned transformer models for disease classification in social telehealth data, achieving an accuracy of 80.56%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage online medical data for disease classification through improved text processing methods and models in the context of Arabic social telehealth.

**Method:** The study explores three Arabic medical text preprocessing methods (summarization, refinement, NER) and applies fine-tuned transformer models (CAMeLBERT, AraBERT, AsafayaBERT) followed by a majority voting ensemble to enhance prediction accuracy.

**Key Contributions:**

	1. First integration of LLM-based preprocessing with Arabic transformer models for disease classification
	2. Utilization of a majority voting ensemble for improved accuracy
	3. Evaluation of multiple preprocessing methods in the Arabic medical text domain

**Result:** Achieved a classification accuracy of 80.56%, demonstrating the effectiveness of combining various text preprocessing techniques and model predictions.

**Limitations:** 

**Conclusion:** Integrating LLM-based preprocessing with transformer models and ensemble learning significantly improves disease classification efforts in Arabic social telehealth data.

**Abstract:** Social telehealth has made remarkable progress in healthcare by allowing patients to post symptoms and participate in medical consultations remotely. Users frequently post symptoms on social media and online health platforms, creating a huge repository of medical data that can be leveraged for disease classification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along with transformer-based models like BERT, have demonstrated strong capabilities in processing complex medical text. In this study, we evaluate three Arabic medical text preprocessing methods such as summarization, refinement, and Named Entity Recognition (NER) before applying fine-tuned Arabic transformer models (CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a majority voting ensemble that combines predictions from original and preprocessed text representations. This approach achieved the best classification accuracy of 80.56%, thus showing its effectiveness in leveraging various text representations and model predictions to improve the understanding of medical texts. To the best of our knowledge, this is the first work that integrates LLM-based preprocessing with fine-tuned Arabic transformer models and ensemble learning for disease classification in Arabic social telehealth data.

</details>


### [159] [EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling](https://arxiv.org/abs/2509.02450)

*Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel*

**Main category:** cs.CL

**Keywords:** Personality Detection, Emotion Awareness, Self-Supervised Learning, Multi-Task Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents EmoPerso, a self-supervised framework that enhances personality detection from text through emotion-aware modelling and multi-task learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in high-quality personality labeling and the independence assumption between emotion and personality in existing methods.

**Method:** EmoPerso uses generative mechanisms for data augmentation, extracts pseudo-labeled emotion features, and employs multi-task learning with a cross-attention module to optimize personality prediction based on emotional representations.

**Key Contributions:**

	1. Introduction of the EmoPerso framework for emotion-aware personality detection
	2. Use of generative mechanisms for synthetic data augmentation
	3. A self-taught strategy to enhance relational reasoning in personality detection

**Result:** EmoPerso outperforms state-of-the-art models in personality detection on benchmark datasets, showing improved accuracy by incorporating emotion-aware modeling.

**Limitations:** 

**Conclusion:** The framework effectively captures the interactions between emotion and personality traits, leading to better performance in personality detection tasks.

**Abstract:** Personality detection from text is commonly performed by analysing users' social media posts. However, existing methods heavily rely on large-scale annotated datasets, making it challenging to obtain high-quality personality labels. Moreover, most studies treat emotion and personality as independent variables, overlooking their interactions. In this paper, we propose a novel self-supervised framework, EmoPerso, which improves personality detection through emotion-aware modelling. EmoPerso first leverages generative mechanisms for synthetic data augmentation and rich representation learning. It then extracts pseudo-labeled emotion features and jointly optimizes them with personality prediction via multi-task learning. A cross-attention module is employed to capture fine-grained interactions between personality traits and the inferred emotional representations. To further refine relational reasoning, EmoPerso adopts a self-taught strategy to enhance the model's reasoning capabilities iteratively. Extensive experiments on two benchmark datasets demonstrate that EmoPerso surpasses state-of-the-art models. The source code is available at https://github.com/slz0925/EmoPerso.

</details>


### [160] [Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions](https://arxiv.org/abs/2509.02452)

*Seyedali Mohammadi, Bhaskara Hanuma Vedula, Hemank Lamba, Edward Raff, Ponnurangam Kumaraguru, Francis Ferraro, Manas Gaur*

**Main category:** cs.CL

**Keywords:** LLMs, explanation benchmarks, external definitions, machine learning, task-solving

**Relevance Score:** 8

**TL;DR:** The paper investigates whether large language models (LLMs) incorporate external definitions or rely on their internal knowledge by conducting experiments on various benchmark datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs utilize external definitions in their task-solving processes and the impact on accuracy and explainability.

**Method:** Controlled experiments were conducted on multiple explanation benchmark datasets, testing different types of definition conditions such as expert-curated and LLM-generated definitions.

**Key Contributions:**

	1. Provides empirical insights into LLMs' reliance on external definitions versus internalized knowledge.
	2. Reveals the differential benefits of explicit definitions for general versus domain-specific tasks.
	3. Calls for deeper research into LLMs' processing of external knowledge.

**Result:** The study found that explicit definitions can enhance performance but are not consistently integrated into LLM's processes; models tend to prefer internal representations for general tasks, while domain-specific tasks show more benefit from explicit definitions.

**Limitations:** The integration of definitions into LLMs' processes varies and is not guaranteed; further studies are needed to explore additional factors influencing this reliance.

**Conclusion:** These findings highlight the importance of understanding how LLMs process both external and internal knowledge in their operations.

**Abstract:** Do LLMs genuinely incorporate external definitions, or do they primarily rely on their parametric knowledge? To address these questions, we conduct controlled experiments across multiple explanation benchmark datasets (general and domain-specific) and label definition conditions, including expert-curated, LLM-generated, perturbed, and swapped definitions. Our results reveal that while explicit label definitions can enhance accuracy and explainability, their integration into an LLM's task-solving processes is neither guaranteed nor consistent, suggesting reliance on internalized representations in many cases. Models often default to their internal representations, particularly in general tasks, whereas domain-specific tasks benefit more from explicit definitions. These findings underscore the need for a deeper understanding of how LLMs process external knowledge alongside their pre-existing capabilities.

</details>


### [161] [SpecEval: Evaluating Model Adherence to Behavior Specifications](https://arxiv.org/abs/2509.02464)

*Ahmed Ahmed, Kevin Klyman, Yi Zeng, Sanmi Koyejo, Percy Liang*

**Main category:** cs.CL

**Keywords:** foundation models, behavioral guidelines, model auditing

**Relevance Score:** 7

**TL;DR:** This paper introduces an automated framework for auditing foundation models to evaluate their adherence to developer-defined behavioral guidelines, revealing significant compliance gaps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for systematic assessment of foundation models' compliance with behavioral guidelines set by their developers.

**Method:** An automated framework was developed that parses behavioral statements, generates targeted prompts, and employs models to judge adherence, focusing on consistency between provider specifications and model outputs.

**Key Contributions:**

	1. Introduction of an automated auditing framework for models
	2. Empirical findings of compliance gaps in model adherence
	3. Focus on three-way consistency for evaluation.

**Result:** The framework was applied to 16 models from six developers, uncovering up to 20 percent compliance gaps between developers' guidelines and model outputs.

**Limitations:** The study may not cover all models and guidelines, limiting its generalizability.

**Conclusion:** There are notable inconsistencies in how foundation models follow the behavioral guidelines set by their developers, indicating a need for improved compliance mechanisms.

**Abstract:** Companies that develop foundation models publish behavioral guidelines they pledge their models will follow, but it remains unclear if models actually do so. While providers such as OpenAI, Anthropic, and Google have published detailed specifications describing both desired safety constraints and qualitative traits for their models, there has been no systematic audit of adherence to these guidelines. We introduce an automated framework that audits models against their providers specifications by parsing behavioral statements, generating targeted prompts, and using models to judge adherence. Our central focus is on three way consistency between a provider specification, its model outputs, and its own models as judges; an extension of prior two way generator validator consistency. This establishes a necessary baseline: at minimum, a foundation model should consistently satisfy the developer behavioral specifications when judged by the developer evaluator models. We apply our framework to 16 models from six developers across more than 100 behavioral statements, finding systematic inconsistencies including compliance gaps of up to 20 percent across providers.

</details>


### [162] [GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning](https://arxiv.org/abs/2509.02492)

*Chenglong Wang, Yongyu Mu, Hang Zhou, Yifu Huo, Ziming Zhu, Jiali Zeng, Murun Yang, Bei Li, Tong Xiao, Xiaoyang Hao, Chunliang Zhang, Fandong Meng, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** reward modeling, self-training, generative models, human feedback, machine learning

**Relevance Score:** 7

**TL;DR:** Introducing GRAM-R$^2$, a generative reward model that leverages self-training on unlabeled data for reward reasoning, outperforming traditional models in various applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of developing effective reward models dependent on large-scale labeled preference data.

**Method:** A self-training approach that utilizes unlabeled data to elicit reasoning within reward models, resulting in GRAM-R$^2$.

**Key Contributions:**

	1. Introduced a self-training approach for reward modeling using unlabeled data.
	2. Developed GRAM-R$^2$, a generative reward model that delivers preference labels with rationales.
	3. Demonstrated superior performance in downstream tasks compared to existing models.

**Result:** GRAM-R$^2$ shows improved performance in response ranking and task-specific tuning, surpassing both discriminative and generative baselines.

**Limitations:** 

**Conclusion:** GRAM-R$^2$ can serve as a foundation model for reward reasoning, applicable to a variety of tasks with minimal fine-tuning required.

**Abstract:** Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.

</details>


### [163] [MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds](https://arxiv.org/abs/2509.02499)

*Junxi Wu, Jinpeng Wang, Zheng Liu, Bin Chen, Dongjian Hu, Hao Wu, Shu-Tao Xiu*

**Main category:** cs.CL

**Keywords:** AI-generated text detection, stylistics modeling, uncertainty quantification

**Relevance Score:** 7

**TL;DR:** This paper presents the Mixture of Stylistic Experts (MoSEs) framework for enhancing AI-generated text detection by incorporating stylistics-aware uncertainty quantification for better performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address public concerns regarding the misuse of AI-generated text and improve detection performance, particularly in low-resource settings.

**Method:** The MoSEs framework includes a Stylistics Reference Repository (SRR) for activating relevant reference data, a Stylistics-Aware Router (SAR) for processing inputs, and a Conditional Threshold Estimator (CTE) that models linguistic and semantic features to dynamically set detection thresholds.

**Key Contributions:**

	1. Introduction of the MoSEs framework for AI text detection.
	2. Integration of stylistics-aware uncertainty quantification.
	3. Demonstrated significant performance improvements in low-resource environments.

**Result:** The MoSEs framework achieved an average performance improvement of 11.34% in text detection against baseline models and a notable 39.15% improvement in low-resource scenarios.

**Limitations:** 

**Conclusion:** MoSEs provides a robust solution for detecting AI-generated text by incorporating stylistic elements, which enhances reliability and trustworthiness in detection systems.

**Abstract:** The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.

</details>


### [164] [L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages](https://arxiv.org/abs/2509.02503)

*Nishant Tanksale, Tanmay Kokate, Darshan Gohad, Sarvadnyaa Barate, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** NLP, low-resource languages, sentence transformers, headline identification, Indic languages

**Relevance Score:** 6

**TL;DR:** This paper introduces L3Cube-IndicHeadline-ID, a dataset for headline identification in ten low-resource Indic languages, benchmarked using sentence transformers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of high-quality benchmarks for semantic evaluation in low-resource Indic languages in NLP.

**Method:** The study introduces a dataset featuring 20,000 news articles per language, each article paired with four types of headlines. Performance of several sentence transformers is evaluated using cosine similarity metrics.

**Key Contributions:**

	1. Introduction of a comprehensive dataset for low-resource Indic languages
	2. Benchmarking of various sentence transformers on this dataset
	3. Demonstration of the dataset's versatility for different NLP tasks

**Result:** Multilingual models perform consistently well across the dataset, while language-specific models exhibit varying effectiveness, demonstrating the challenges and opportunities in low-resource language scenarios.

**Limitations:** 

**Conclusion:** The L3Cube-IndicHeadline-ID dataset is a valuable resource for improving semantic understanding in multiple NLP tasks, including Retrieval-Augmented Generation and question answering.

**Abstract:** Semantic evaluation in low-resource languages remains a major challenge in NLP. While sentence transformers have shown strong performance in high-resource settings, their effectiveness in Indic languages is underexplored due to a lack of high-quality benchmarks. To bridge this gap, we introduce L3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten low-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada, Malayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000 news articles paired with four headline variants: the original, a semantically similar version, a lexically similar version, and an unrelated one, designed to test fine-grained semantic understanding. The task requires selecting the correct headline from the options using article-headline similarity. We benchmark several sentence transformers, including multilingual and language-specific models, using cosine similarity. Results show that multilingual models consistently perform well, while language-specific models vary in effectiveness. Given the rising use of similarity models in Retrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a valuable resource for evaluating and improving semantic understanding in such applications. Additionally, the dataset can be repurposed for multiple-choice question answering, headline classification, or other task-specific evaluations of LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared publicly at https://github.com/l3cube-pune/indic-nlp

</details>


### [165] [The Forgotten Code: Validating a Century-Old Translation System with AI](https://arxiv.org/abs/2509.02506)

*Jean-Marie Le Ray*

**Main category:** cs.CL

**Keywords:** machine translation, rule-based, AI, computational linguistics, historical translations

**Relevance Score:** 4

**TL;DR:** This paper explores the revival of a historical rule-based mechanical translation system by Federico Pucci, demonstrating its relevance through modern AI translation methods.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To illustrate the historical significance of Federico Pucci's mechanical translation system and its potential applications in contemporary AI-driven translation.

**Method:** AIs are used to retranslate text excerpts translated by Pucci in 1931, following his original methodology, comparing results from both periods.

**Key Contributions:**

	1. Validation of Pucci's mechanical translation system using modern AI methods.
	2. Demonstration of low error rates in translations done 94 years apart.
	3. Promotion of Pucci's work as a noteworthy contribution to the history of machine translation.

**Result:** The AI translations of the texts showed a low average difference with only minor variations, validating Pucci's system.

**Limitations:** 

**Conclusion:** Pucci's translation methods were successfully revived, indicating their continuing relevance and suggesting further experimentation in modern contexts.

**Abstract:** A pioneering rule-based mechanical translation system (precursor of modern RBMTs) was first presented in December 1929 by its inventor, Federico Pucci, who later published the full method in a book titled "Il traduttore meccanico ed il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria lingua: Parte I", in Salerno (Italy), in 1931. This study illustrates how AI breathes new life into the system of international keys and ideograms devised by Pucci to translate from/into any Romance language (at least as a first step). The methodology involves having the AIs retranslate, following Pucci's method, the two text excerpts originally translated in 1931 and clearly documented in his publication: a passage from Dante's La Vita Nuova, translated from Italian into French, and a passage from Voltaire's Zadig, translated from French into Italian. The result is notable: the two texts, translated 94 years apart using the same method--by Pucci in 1931 and by AIs in 2025--show a low average difference, with only minor variations observed. With Pucci's system thus validated, it became feasible to have the AIs reproduce the excerpts in English, Spanish, and German according to his method. The results were consistent, and Pucci--via Artificial Intelligence--was tasked with translating more modern and technical texts, thereby reviving, nearly a century later, an invention that had remained almost entirely unknown and never applied beyond its creator, now brought to wider attention and opened to possible experimentation. Such a demonstration would not only affirm Pucci's historical status but also place him among the precursors and intellectual contributors to machine translation, whose work merits examination alongside figures such as Troyanskij, Booth, and Weaver, with possible consequences for how the history of the field is understood.

</details>


### [166] [Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation](https://arxiv.org/abs/2509.02510)

*Erfan Baghaei Potraghloo, Seyedarmin Azizi, Souvik Kundu, Massoud Pedram*

**Main category:** cs.CL

**Keywords:** Large language models, Text generation, Sampling techniques, Creativity, Coherence

**Relevance Score:** 9

**TL;DR:** This paper introduces top-H decoding, a novel approach to improve text generation by balancing creativity and coherence in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing truncated sampling techniques in generating diverse and coherent text.

**Method:** Theoretical foundation establishes the relationship between creativity and coherence via an entropy-constrained minimum divergence problem, leading to the development of the top-H decoding algorithm as a solution to this NP-hard problem.

**Key Contributions:**

	1. Introduction of top-H decoding for balancing creativity and coherence
	2. Theoretical formulation connecting entropy constraints with text generation
	3. Demonstrated significant performance improvements over existing methods

**Result:** Top-H decoding outperforms min-p sampling by up to 25.63% on creative writing benchmarks and maintains robustness on question-answering datasets.

**Limitations:** 

**Conclusion:** Top-H decoding advances state-of-the-art techniques in open-ended text generation and can be easily integrated into creative writing applications.

**Abstract:** Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at https://github.com/ErfanBaghaei/Top-H-Decoding.

</details>


### [167] [Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition](https://arxiv.org/abs/2509.02514)

*Mayur Shirke, Amey Shembade, Pavan Thorat, Madhushri Wagh, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, code-mixed text, Hindi-English, Hinglish, large language models

**Relevance Score:** 6

**TL;DR:** This study evaluates Named Entity Recognition (NER) in code-mixed Hindi-English (Hinglish) using various models and provides insights on their effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Named Entity Recognition (NER) in code-mixed text presents unique challenges, which necessitates an evaluation of specialized versus generalized models.

**Method:** A comparative analysis of code-mixed fine-tuned models (HingBERT, HingMBERT, HingRoBERTa) and non-code-mixed multilingual models (BERT Base, IndicBERT, RoBERTa, MuRIL), along with zero-shot evaluations of Google Gemini.

**Key Contributions:**

	1. Comparative evaluation of code-mixed and non-code-mixed models for NER in Hinglish.
	2. Demonstration of the effectiveness of domain-specific pretrained models for NER tasks.
	3. Insights into the performance of zero-shot generative LLMs for NER.

**Result:** Code-mixed models outperform non-code-mixed models, with HingRoBERTa and HingBERT showing the best performance; Google Gemini demonstrates competitive performance in zero-shot settings.

**Limitations:** The study focuses on a specific dataset and may not generalize across all code-mixed text scenarios.

**Conclusion:** Specialized pretraining in code-mixed models leads to superior performance for NER tasks in code-mixed texts compared to generalized models.

**Abstract:** Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English (Hinglish), presents unique challenges due to informal structure, transliteration, and frequent language switching. This study conducts a comparative evaluation of code-mixed fine-tuned models and non-code-mixed multilingual models, along with zero-shot generative large language models (LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained on code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained on non-code-mixed multilingual data). We also assess the performance of Google Gemini in a zero-shot setting using a modified version of the dataset with NER tags removed. All models are tested on a benchmark Hinglish NER dataset using Precision, Recall, and F1-score. Results show that code-mixed models, particularly HingRoBERTa and HingBERT-based fine-tuned models, outperform others - including closed-source LLMs like Google Gemini - due to domain-specific pretraining. Non-code-mixed models perform reasonably but show limited adaptability. Notably, Google Gemini exhibits competitive zero-shot performance, underlining the generalization strength of modern LLMs. This study provides key insights into the effectiveness of specialized versus generalized models for code-mixed NER tasks.

</details>


### [168] [Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR](https://arxiv.org/abs/2509.02522)

*Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Verifiable Rewards, Large Language Models, Supervised Learning, Reasoning Tasks

**Relevance Score:** 9

**TL;DR:** A novel RLVR framework called PACS improves LLM reasoning tasks by reformulating RLVR as a supervised learning problem, resulting in more stable training.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the stability and performance of large language models (LLMs) using Reinforcement Learning with Verifiable Rewards (RLVR) for reasoning tasks.

**Method:** PACS reformulates the RLVR problem into a supervised learning task by treating the outcome reward as a predictable label, optimizing a score function with cross-entropy loss while coupling actor and critic roles.

**Key Contributions:**

	1. Introduces PACS, a novel framework for RLVR that stabilizes training.
	2. Reformulates RLVR as a supervised learning task for better performance.
	3. Demonstrates superior reasoning performance on mathematical tasks compared to existing methods.

**Result:** PACS outperforms traditional RLVR baselines like PPO and GRPO, achieving a 59.78% pass rate on AIME 2025, significantly better than its predecessors by up to 14.36 points.

**Limitations:** 

**Conclusion:** PACS provides a robust framework for integrating verifiable rewards into LLMs, enhancing their reasoning capabilities post-training, and is accessible as open source.

**Abstract:** Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\textbf{PACS}$, a novel RLVR framework that achieves im$\textbf{P}$licit $\textbf{A}$ctor $\textbf{C}$ritic coupling via a $\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.

</details>


### [169] [Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices](https://arxiv.org/abs/2509.02523)

*Evan King, Adam Sabra, Manjunath Kudlur, James Wang, Pete Warden*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, underrepresented languages, monolingual models

**Relevance Score:** 7

**TL;DR:** This paper introduces the Flavors of Moonshine, a suite of automatic speech recognition models for underrepresented languages, demonstrating that monolingual models can outperform multilingual counterparts for small model sizes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the prevailing assumption that multilingual ASR models are superior to monolingual models for small parameter sizes, particularly for underrepresented languages.

**Method:** Training monolingual ASR systems on a balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data, while keeping the model size to 27M parameters.

**Key Contributions:**

	1. Introduction of monolingual models that outperform larger multilingual counterparts.
	2. Significant reduction in error rates for small ASR models.
	3. Release of new ASR models for multiple underrepresented languages.

**Result:** The monolingual models achieve, on average, 48% lower error rates than the Whisper Tiny model and outperform larger Whisper models in most cases.

**Limitations:** 

**Conclusion:** The results advance the state of the art for small ASR models, enabling accurate on-device recognition for languages that lack sufficient support.

**Abstract:** We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license.

</details>


### [170] [Jointly Reinforcing Diversity and Quality in Language Model Generations](https://arxiv.org/abs/2509.02534)

*Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang*

**Main category:** cs.CL

**Keywords:** Diversity, Reinforcement Learning, Large Language Models, Semantic Quality, Exploratory Tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces Diversity-Aware Reinforcement Learning (DARLING), a framework that balances response quality and semantic diversity in Large Language Models (LMs).

**Read time:** 29 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of reduced diversity in responses from Large Language Models during post-training, which limits their effectiveness in creative and exploratory tasks.

**Method:** DARLING employs a learned partition function to measure diversity and combines this with quality rewards during reinforcement learning to optimize model outputs.

**Key Contributions:**

	1. Introduction of Diversity-Aware Reinforcement Learning (DARLING) framework
	2. Demonstrated generalization of DARLING across multiple model families and tasks
	3. Achieved higher quality and novelty in Large Language Model outputs compared to traditional methods

**Result:** Experiments show that DARLING consistently outperforms quality-only RL baselines across both non-verifiable (instruction following, creative writing) and verifiable tasks (competition math), yielding higher quality and more novel outputs.

**Limitations:** 

**Conclusion:** Optimizing for diversity enhances exploration in online reinforcement learning, resulting in higher-quality responses from models.

**Abstract:** Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.

</details>


### [171] [PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture](https://arxiv.org/abs/2509.02550)

*Fakhraddin Alwajih, Abdellah El Mekki, Hamdy Mubarak, Majd Hawasly, Abubakr Mohamed, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Competence, Arabic Culture, Islamic Culture, Fine-Tuning

**Relevance Score:** 7

**TL;DR:** PalmX 2025 establishes a benchmarking task for evaluating the cultural competence of LLMs regarding Arabic and Islamic cultures, revealing significant performance improvements through fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural understanding gap in LLMs, particularly regarding Arabic and Islamic cultures, which are often under-represented in pre-training data.

**Method:** The benchmarking task consists of two subtasks involving multiple-choice questions in Modern Standard Arabic, focusing on General Arabic Culture and General Islamic Culture.

**Key Contributions:**

	1. Introduction of the PalmX 2025 cultural competence benchmarking task for LLMs.
	2. Demonstrated significant performance enhancements via tailored fine-tuning strategies.
	3. Findings highlight disparities in LLMs' competency across different cultural contexts.

**Result:** Task-specific fine-tuning led to notable performance increases, with top accuracy scores reaching 72.15% on cultural questions and 84.22% on Islamic knowledge.

**Limitations:** 

**Conclusion:** Fine-tuning is essential for improving LLM performance in culturally specific domains, and the effectiveness of techniques such as parameter-efficient fine-tuning and data augmentation varies by context.

**Abstract:** Large Language Models (LLMs) inherently reflect the vast data distributions they encounter during their pre-training phase. As this data is predominantly sourced from the web, there is a high chance it will be skewed towards high-resourced languages and cultures, such as those of the West. Consequently, LLMs often exhibit a diminished understanding of certain communities, a gap that is particularly evident in their knowledge of Arabic and Islamic cultures. This issue becomes even more pronounced with increasingly under-represented topics. To address this critical challenge, we introduce PalmX 2025, the first shared task designed to benchmark the cultural competence of LLMs in these specific domains. The task is composed of two subtasks featuring multiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General Arabic Culture and General Islamic Culture. These subtasks cover a wide range of topics, including traditions, food, history, religious practices, and language expressions from across 22 Arab countries. The initiative drew considerable interest, with 26 teams registering for Subtask 1 and 19 for Subtask 2, culminating in nine and six valid submissions, respectively. Our findings reveal that task-specific fine-tuning substantially boosts performance over baseline models. The top-performing systems achieved an accuracy of 72.15% on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine-tuning emerged as the predominant and most effective approach among participants, while the utility of data augmentation was found to be domain-dependent.

</details>


### [172] [Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation](https://arxiv.org/abs/2210.14275)

*Wenchuan Mu*

**Main category:** cs.CL

**Keywords:** similarity computation, regression models, language units

**Relevance Score:** 5

**TL;DR:** This thesis presents a framework for refining language unit similarity computation through regression models, focusing on precision in sensitive areas like legal and medical fields while improving interpretability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of capturing similarities in human language units necessitates new approaches, especially in critical domains where precision is vital.

**Method:** A progressively refined similarity computation model is developed that combines attack testing with adversarial training to improve similarity measures.

**Key Contributions:**

	1. Development of a regression model for refined similarity computation.
	2. Integration of attack testing with adversarial training.
	3. Improvement of interpretability in similarity measures.

**Result:** The regression model shows state-of-the-art performance in handling edge cases in similarity computation.

**Limitations:** 

**Conclusion:** The proposed model not only enhances the accuracy of similarity calculations but also offers clearer interpretations, addressing previous limitations in the field.

**Abstract:** Capturing the similarities between human language units is crucial for explaining how humans associate different objects, and therefore its computation has received extensive attention, research, and applications. With the ever-increasing amount of information around us, calculating similarity becomes increasingly complex, especially in many cases, such as legal or medical affairs, measuring similarity requires extra care and precision, as small acts within a language unit can have significant real-world effects. My research goal in this thesis is to develop regression models that account for similarities between language units in a more refined way.   Computation of similarity has come a long way, but approaches to debugging the measures are often based on continually fitting human judgment values. To this end, my goal is to develop an algorithm that precisely catches loopholes in a similarity calculation. Furthermore, most methods have vague definitions of the similarities they compute and are often difficult to interpret. The proposed framework addresses both shortcomings. It constantly improves the model through catching different loopholes. In addition, every refinement of the model provides a reasonable explanation. The regression model introduced in this thesis is called progressively refined similarity computation, which combines attack testing with adversarial training. The similarity regression model of this thesis achieves state-of-the-art performance in handling edge cases.

</details>


### [173] [Rule-Guided Joint Embedding Learning over Knowledge Graphs](https://arxiv.org/abs/2401.02968)

*Qisong Li, Ji Lin, Sijia Wei, Neng Liu*

**Main category:** cs.CL

**Keywords:** knowledge graph embedding, graph convolutional network, contextual information

**Relevance Score:** 4

**TL;DR:** This paper presents a model that integrates contextual and textual information into knowledge graph embeddings using graph convolutional networks, enhancing embedding effectiveness.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge graph embedding by incorporating rich contextual and textual information, which existing models have underutilized.

**Method:** The authors propose a novel model that utilizes a graph convolutional network to integrate contextual and textual signals, introducing metrics for confidence and relatedness to weigh this information during embedding learning.

**Key Contributions:**

	1. Integration of contextual and textual signals into embeddings
	2. Introduction of confidence and relatedness metrics for contextual weighting
	3. Demonstrated effectiveness on benchmark datasets

**Result:** The model shows consistent improvements in embedding effectiveness over strong baselines in experiments on benchmark datasets.

**Limitations:** 

**Conclusion:** The proposed approach enhances the precision of knowledge graph embeddings by better utilizing contextual and textual signals.

**Abstract:** Recent studies on knowledge graph embedding focus on mapping entities and relations into low-dimensional vector spaces. While most existing models primarily exploit structural information, knowledge graphs also contain rich contextual and textual information that can enhance embedding effectiveness. In this work, we propose a novel model that integrates both contextual and textual signals into entity and relation embeddings through a graph convolutional network. To better utilize context, we introduce two metrics: confidence, computed via a rule-based method, and relatedness, derived from textual representations. These metrics enable more precise weighting of contextual information during embedding learning. Extensive experiments on two widely used benchmark datasets demonstrate the effectiveness of our approach, showing consistent improvements over strong baselines.

</details>


### [174] [Semantic Parsing for Question Answering over Knowledge Graphs](https://arxiv.org/abs/2401.06772)

*Sijia Wei, Wenwen Zhang, Qisong Li, Jiang Zhao*

**Main category:** cs.CL

**Keywords:** knowledge graphs, question answering, semantic parsing

**Relevance Score:** 8

**TL;DR:** Novel method for question answering over knowledge graphs using graph-to-segment mapping and semantic parsing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of natural language questions in knowledge graphs by addressing implicit entities and complex constraints.

**Method:** A framework that combines rule-based and neural methods, using an encoder-decoder neural network and a graph neural network to map questions into semantic segments and enrich representations.

**Key Contributions:**

	1. Introduces a novel graph-to-segment mapping method for question answering.
	2. Combines rule-based and neural techniques for improved semantic parsing.
	3. Employs a graph neural network for enriched question representations.

**Result:** Experimental evaluations show superior performance of the proposed model in semantic parsing compared to existing methods on benchmark datasets.

**Limitations:** 

**Conclusion:** The integrated approach significantly enhances the accuracy of question answering over knowledge graphs.

**Abstract:** In this paper, we propose a novel method for question answering over knowledge graphs based on graph-to-segment mapping, designed to improve the understanding of natural language questions. Our approach is grounded in semantic parsing, a key technique for interpreting question utterances. The main challenges arise from handling implicit entities and relations, as well as complex constraints such as temporal conditions, ordinality, and aggregation within the context of a knowledge graph. To address these issues, our framework integrates both rule-based and neural methods to parse and construct accurate, comprehensive semantic segment sequences. These sequences are then assembled into semantic query graphs, providing precise representations of question utterances. We formulate question semantic parsing as a sequence generation task, employing an encoder-decoder neural network to map natural language questions into semantic segments. Furthermore, to enhance the identification of implicit entities and relations, we incorporate a graph neural network that leverages knowledge graph context to enrich question representations. Experimental evaluations on two benchmark datasets demonstrate the effectiveness and superior performance of our model in semantic parsing for knowledge graph question answering.

</details>


### [175] [Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports](https://arxiv.org/abs/2401.12989)

*Adriano Belisario, Scott A. Hale, Luc Rocher*

**Main category:** cs.CL

**Keywords:** gun violence, language models, social media, human rights, BERT

**Relevance Score:** 6

**TL;DR:** This paper evaluates a BERT-based model for monitoring gun violence through social media data, showing its effectiveness in aiding human rights organizations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Gun violence is a critical human rights issue, and there is a need for reliable data to improve public policy and emergency responses.

**Method:** The study employed a fine-tuned BERT-based model trained on Twitter texts to identify gun violence reports from Portuguese texts, which was integrated into a web application for live testing.

**Key Contributions:**

	1. A fine-tuned BERT model to detect gun violence reports in Portuguese texts.
	2. Implementation of the model in a web application for real-time analyses.
	3. Qualitative and quantitative evaluations confirming the model's utility for analysts.

**Result:** The model improved analysts' efficiency in identifying gun violence incidents on social media and enhanced their interaction with users reporting such events.

**Limitations:** 

**Conclusion:** Human-centered interventions using language models can effectively support the efforts of human rights organizations in monitoring gun violence.

**Abstract:** Gun violence is a pressing human rights issue that affects nearly every dimension of the social fabric, from healthcare and education to psychology and the economy. Reliable data on firearm events is paramount to developing more effective public policy and emergency responses. However, the lack of comprehensive databases and the risks of in-person surveys prevent human rights organizations from collecting needed data in most countries. Here, we partner with a Brazilian human rights organization to conduct a systematic evaluation of language models to assist with monitoring real-world firearm events from social media data. We propose a fine-tuned BERT-based model trained on Twitter (now X) texts to distinguish gun violence reports from ordinary Portuguese texts. We then incorporate our model into a web application and test it in a live intervention. We study and interview Brazilian analysts who continuously check social media texts to identify new gun violence events. Qualitative assessments show that our solution helped all analysts use their time more efficiently and expanded their search capacities. Quantitative assessments show that the use of our model was associated with analysts having further interactions with online users reporting gun violence. Our findings suggest that human-centered interventions using language models can help support the work of human rights organizations.

</details>


### [176] [Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard](https://arxiv.org/abs/2402.14533)

*Ariel Rosenfeld, Teddy Lazebnik*

**Main category:** cs.CL

**Keywords:** Large Language Models, linguistic analysis, text attribution

**Relevance Score:** 9

**TL;DR:** The paper investigates whether Large Language Models (LLMs) exhibit distinctive linguistic styles, comparing textual outputs from GPT-3.5, GPT-4, and Bard across various dimensions of linguistic analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLMs have unique linguistic styles similar to human authors and to explore implications of these findings.

**Method:** A comprehensive linguistic analysis was conducted, comparing vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by GPT-3.5, GPT-4, and Bard.

**Key Contributions:**

	1. Identification of unique linguistic styles in LLM-generated texts.
	2. Demonstration of high accuracy in text attribution to specific LLMs.
	3. Exploration of implications for understanding LLM capabilities and ethical considerations.

**Result:** The analysis revealed significant linguistic variations among the LLMs, allowing for attribution of generated texts to their respective LLM origins with 88% accuracy using a classification model.

**Limitations:** 

**Conclusion:** The linguistic differences identified suggest that LLMs not only generate high-quality texts but do so with distinct styles, which has both theoretical and practical implications.

**Abstract:** Large Language Models (LLMs) are capable of generating text that is similar to or surpasses human quality. However, it is unclear whether LLMs tend to exhibit distinctive linguistic styles akin to how human authors do. Through a comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech (POS) distribution, dependency distribution, and sentiment of texts generated by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse inputs. The results point to significant linguistic variations which, in turn, enable us to attribute a given text to its LLM origin with a favorable 88\% accuracy using a simple off-the-shelf classification model. Theoretical and practical implications of this intriguing finding are discussed.

</details>


### [177] [Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations](https://arxiv.org/abs/2404.07851)

*Dayeon Ki, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Machine Translation, Large Language Models, Quality Metrics, Post-editing, Fine-tuning

**Relevance Score:** 8

**TL;DR:** This study explores enhancing Machine Translation (MT) by guiding Large Language Models (LLMs) to post-edit MT using feedback from Multidimensional Quality Metric (MQM) annotations, demonstrating improvements in translation quality across several language pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in NLP, MT still relies on supervised systems; this paper investigates integrating LLMs to improve MT quality.

**Method:** The authors employed prompting strategies with LLaMA-2 models and fine-tuning to enhance LLMs' ability to post-edit MT based on quality feedback derived from MQM annotations.

**Key Contributions:**

	1. Integration of LLMs in MT post-editing using feedback from MQM annotations
	2. Demonstration of improved translation quality across multiple language pairs
	3. Evaluation of fine-tuning strategies in enhancing LLM performance in MT tasks

**Result:** Experiments show that LLMs directed to post-edit MT yield improved scores in TER, BLEU, and COMET, with fine-tuning leading to better integration of fine-grained feedback, although the specific benefits of detailed feedback were ambiguous.

**Limitations:** 

**Conclusion:** LLMs can be effectively utilized for post-editing MT when guided by external quality feedback, enhancing translation quality as evidenced by various evaluation metrics.

**Abstract:** Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.

</details>


### [178] [Why Not Transform Chat Large Language Models to Non-English?](https://arxiv.org/abs/2405.13923)

*Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, Shujian Huang*

**Main category:** cs.CL

**Keywords:** large language models, knowledge distillation, multilingual models

**Relevance Score:** 7

**TL;DR:** This paper presents TransLLM, a framework for transforming English-centric chat LLMs into non-English models while addressing issues of knowledge retention and capability transfer.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of non-English training data limits the development of non-English large language models, necessitating effective methods for transforming existing English LLMs.

**Method:** TransLLM divides the transformation process into common sub-tasks using a translation chain-of-thought approach, and employs low-rank adaptation and recovery knowledge distillation to maintain original knowledge while enabling the transfer of advanced capabilities.

**Key Contributions:**

	1. Introduction of the TransLLM framework for non-English LLM development.
	2. Utilization of the translation chain-of-thought to bridge language barriers in LLM transfer.
	3. Methods to maintain original knowledge in transformed LLMs through low-rank adaptation and recovery KD.

**Result:** TransLLM outperforms strong baselines and ChatGPT in transforming LLaMA-2-chat-7B to Thai, excelling in multi-turn scenario benchmarks and safety benchmarks without needing additional safety data.

**Limitations:** 

**Conclusion:** TransLLM effectively transforms LLMs into non-English versions while preserving their advanced abilities and mitigating the risk of knowledge loss.

**Abstract:** The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. Code is available at https://github.com/hy5468/TransLLM.

</details>


### [179] [Intrinsic Test of Unlearning Using Parametric Knowledge Traces](https://arxiv.org/abs/2406.11614)

*Yihuai Hong, Lei Yu, Haiqin Yang, Shauli Ravfogel, Mor Geva*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, concept vectors, evaluation methodology, adversarial manipulation

**Relevance Score:** 9

**TL;DR:** This paper proposes a new methodology for evaluating unlearning in large language models (LLMs) by analyzing parameter changes, introducing a benchmark dataset called ConceptVectors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of current unlearning evaluations that rely solely on behavioral tests, which may leave residual knowledge exploitable by adversaries.

**Method:** The proposed methodology utilizes vocabulary projections to examine parametric knowledge traces related to unlearned concepts in LLMs, identifying and localizing 'concept vectors'.

**Key Contributions:**

	1. Introduction of a new evaluation methodology for unlearning in LLMs.
	2. Release of ConceptVectors benchmark dataset for assessing parametric knowledge traces in LLMs.
	3. Demonstration of limitations in traditional unlearning evaluations based on behavior.

**Result:** Evaluation on the generated ConceptVectors dataset reveals that existing unlearning methods have limited effectiveness on concept vectors and primarily suppress them during model inference, while direct ablation of these vectors effectively removes knowledge and reduces model vulnerability to adversarial attacks.

**Limitations:** The study focuses on only two open-source LLMs, limiting the generalizability of results to other models.

**Conclusion:** The study calls for a paradigm shift in unlearning evaluation practices by incorporating parameter-based assessments alongside behavioral tests, emphasizing the necessity for robust methodologies in addressing unlearning challenges.

**Abstract:** The task of "unlearning" certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize "concept vectors" - parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.

</details>


### [180] [MEGen: Generative Backdoor into Large Language Models via Model Editing](https://arxiv.org/abs/2408.10722)

*Jiyang Qiu, Xinbei Ma, Zhuosheng Zhang, Hai Zhao, Yun Li, Qianren Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Generative Backdoors, Safety Risks, NLP, MEGen

**Relevance Score:** 8

**TL;DR:** The paper discusses the discovery of generative backdoors in large language models (LLMs) and proposes a method to inject these backdoors, which can compromise safety by outputting harmful information during normal tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety concerns regarding the use of backdoored LLMs, particularly in generative applications where traditional methods fall short.

**Method:** The authors propose a novel editing-based generative backdoor named MEGen, which allows for backdoor injections that influence generative tasks more broadly than traditional yes-or-no discriminative tasks.

**Key Contributions:**

	1. Introduction of MEGen as a method for injecting generative backdoors into LLMs.
	2. Demonstration of backdoored LLMs' ability to produce controlled harmful outputs during generative tasks.
	3. Highlighting safety implications of generative capabilities in LLMs.

**Result:** Experiments demonstrate that MEGen achieves a high attack success rate with minimal adjustments to local parameters and few-shot samples, enabling the backdoored model to output dangerous information while performing downstream tasks.

**Limitations:** The study is primarily focused on the design and effectiveness of the backdoor, with limited exploration of countermeasures or mitigation strategies.

**Conclusion:** The work illustrates the potential safety risks of generative backdoors in LLMs, emphasizing the need for attention to these capabilities in their applications.

**Abstract:** Large language models (LLMs) have exhibited remarkable versatility and adaptability, while their widespread adoption across various applications also raises critical safety concerns. This paper focuses on the impact of backdoored LLMs. Traditional backdoor injection methods are primarily limited to yes-or-no discriminative tasks, leading users to underestimate the potential risks of backdoored LLMs. Given the inherently generative nature of LLMs, this paper reveals that a generative backdoor injected into LLMs can expose the true safety risks in their applications. We propose an editing-based generative backdoor, named MEGen, aiming to expand the backdoor to generative tasks in a unified format of any text-to any text, leading to natural generations with a specific intention. Experiments show that MEGen achieves a high attack success rate by adjusting only a small set of local parameters with few-shot samples. Notably, we show that the backdoored model, when triggered, can freely output pre-set dangerous information while completing downstream tasks. Our work highlights that MEGen enables backdoors in LLMs to exhibit generative capabilities, causing potential safety risks by altering the generative style. The code is available at https://github.com/MonoQ-hub/MEGen.

</details>


### [181] [On the Diagram of Thought](https://arxiv.org/abs/2409.10038)

*Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Diagram of Thought, Reasoning Framework

**Relevance Score:** 8

**TL;DR:** This paper introduces the Diagram of Thought (DoT), a framework for enhancing LLM reasoning through structured, multi-step processes, grounded in category theory.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of Large Language Models in handling complex problems that require structured reasoning.

**Method:** The framework allows LLMs to create a dynamic mental map of reasoning, proposing various thoughts, critiquing steps, and synthesizing insights.

**Key Contributions:**

	1. Introduction of the Diagram of Thought framework for LLMs
	2. Grounding in category theory for logical consistency
	3. Enhanced transparency and auditability of LLM reasoning processes.

**Result:** The DoT framework results in a more efficient and transparent reasoning process, producing an auditable trace of the model's thinking.

**Limitations:** 

**Conclusion:** By grounding the DoT in category theory, the model achieves logical and robust information synthesis, enabling improved reasoning capabilities.

**Abstract:** Large Language Models (LLMs) excel at many tasks but often falter on complex problems that require structured, multi-step reasoning. We introduce the Diagram of Thought (DoT), a new framework that enables a single LLM to build and navigate a mental map of its reasoning. Instead of thinking in a straight line, the model constructs a dynamic diagram of ideas, where it can propose different lines of thought, critique its own steps, and synthesize validated insights into a final conclusion. This entire process is self-contained within the model, making it highly efficient by avoiding the complex external controllers or search algorithms required by other methods. To ensure the reliability of this process, we ground DoT in a rigorous mathematical framework from category theory. This foundation guarantees that the way the model combines information is logical, consistent, and robust, regardless of the order in which ideas were explored. The result is a more powerful and transparent reasoning process that produces a fully auditable, step-by-step trace of the LLM's thinking, bridging the gap between fluent language and formal reasoning.

</details>


### [182] [Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint](https://arxiv.org/abs/2409.15664)

*Dayeon Ki, Cheonbok Park, Hyunjoong Kim*

**Main category:** cs.CL

**Keywords:** cross-lingual embeddings, semantic leakage, multilingual models, ORACLE, disentangled representation

**Relevance Score:** 6

**TL;DR:** This paper introduces ORACLE, a novel training objective to address semantic leakage in multi-lingual sentence embeddings, enhancing cross-lingual data alignment.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective parallel data mining relies heavily on accurate alignment of contextual representations in cross-lingual sentence embeddings, which current methods struggle with due to semantic leakage.

**Method:** The paper proposes ORthogonAlity Constraint LEarning (ORACLE), which enforces orthogonality between semantic and language embeddings through intra-class clustering and inter-class separation techniques.

**Key Contributions:**

	1. Introduction of the concept of semantic leakage in multilingual embeddings
	2. Development of the ORACLE objective for better alignment
	3. Demonstration of effectiveness through specific experimental tasks

**Result:** Experimental results show that ORACLE significantly reduces semantic leakage and improves semantic alignment in the embedding space for both cross-lingual retrieval and semantic textual similarity tasks.

**Limitations:** The effectiveness of ORACLE may vary across different languages and contexts; further testing is needed.

**Conclusion:** ORACLE represents a substantial improvement in disentangling semantic and language information in sentence embeddings, facilitating better performance in multilingual applications.

**Abstract:** Accurately aligning contextual representations in cross-lingual sentence embeddings is key for effective parallel data mining. A common strategy for achieving this alignment involves disentangling semantics and language in sentence embeddings derived from multilingual pre-trained models. However, we discover that current disentangled representation learning methods suffer from semantic leakage - a term we introduce to describe when a substantial amount of language-specific information is unintentionally leaked into semantic representations. This hinders the effective disentanglement of semantic and language representations, making it difficult to retrieve embeddings that distinctively represent the meaning of the sentence. To address this challenge, we propose a novel training objective, ORthogonAlity Constraint LEarning (ORACLE), tailored to enforce orthogonality between semantic and language embeddings. ORACLE builds upon two components: intra-class clustering and inter-class separation. Through experiments on cross-lingual retrieval and semantic textual similarity tasks, we demonstrate that training with the ORACLE objective effectively reduces semantic leakage and enhances semantic alignment within the embedding space.

</details>


### [183] [Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI](https://arxiv.org/abs/2410.12341)

*Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo*

**Main category:** cs.CL

**Keywords:** model collapse, generative AI, training data, surplexity, autophagy

**Relevance Score:** 8

**TL;DR:** This paper introduces new measures to characterize model collapse in generative AI models and proposes a surprisingness-based mitigation strategy that does not require distinguishing between human- and AI-generated data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in understanding model collapse caused by retraining generative AI models on their outputs and the reliance on human-authored content for mitigation strategies.

**Method:** The authors introduce measures that characterize model collapse using next-token probability distributions from models, rather than relying on characteristics of the generated text. They propose filtering training items by high surprise (surplexity) to mitigate collapse.

**Key Contributions:**

	1. Introduced measures to characterize model collapse based on next-token probability distributions.
	2. Proposed a surprise-based mitigation strategy that does not require distinguishing data types.
	3. Demonstrated effectiveness of the new strategy in several experiments.

**Result:** Experiments show that the proposed surprise-based filtering is as effective as human-data baselines in mitigating model collapse and more effective in dealing with distributional skewedness.

**Limitations:** 

**Conclusion:** The study provides a new understanding of model collapse and suggests more robust methods for training generative AI systems in an environment filled with synthetic data.

**Abstract:** As synthetic content increasingly infiltrates the web, generative AI models may be retrained on their own outputs: a process termed "autophagy". This leads to model collapse: a progressive loss of performance and diversity across generations. Recent studies have examined the emergence of model collapse across various generative AI models and data types, and have proposed mitigation strategies that rely on incorporating human-authored content. However, current characterizations of model collapse remain limited, and existing mitigation methods assume reliable knowledge of whether training data is human-authored or AI-generated. In this paper, we address these gaps by introducing new measures that characterise collapse directly from a model's next-token probability distributions, rather than from properties of AI-generated text. Using these measures, we show that the degree of collapse depends on the complexity of the initial training set, as well as on the extent of autophagy. Our experiments prompt a new suggestion: that model collapse occurs when a model trains on data that does not "surprise" it. We express this hypothesis in terms of the well-known Free Energy Principle in cognitive science. Building on this insight, we propose a practical mitigation strategy: filtering training items by high surplexity, maximising the surprise of the model. Unlike existing methods, this approach does not require distinguishing between human- and AI-generated data. Experiments across datasets and models demonstrate that our strategy is at least as effective as human-data baselines, and even more effective in reducing distributional skewedness. Our results provide a richer understanding of model collapse and point toward more resilient approaches for training generative AI systems in environments increasingly saturated with synthetic data.

</details>


### [184] [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/abs/2410.13258)

*Xiangci Li, Jessica Ouyang*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Knowledge selection, Natural language generation

**Relevance Score:** 9

**TL;DR:** This paper analyzes how knowledge selection affects generation performance in retrieval-augmented generation (RAG) systems, revealing its varying significance based on the generator model and task complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the influence of knowledge selection on the performance of RAG systems, particularly its role compared to knowledge retrieval in natural language generation.

**Method:** Empirical analysis through controlled experiments involving different retrieval and selection conditions, mixing gold and distractor knowledge to evaluate their impact on generation performance.

**Key Contributions:**

	1. Empirical analysis of knowledge selection in RAG systems
	2. Identification of conditions under which knowledge selection is vital
	3. Demonstration of varying impact of knowledge recall and F1 scores based on task complexity

**Result:** The findings show that the generator model's strength and task complexity significantly impact the importance of knowledge selection; improving knowledge recall is crucial for high-performance scenarios, while knowledge F1 score is essential for weaker models and ambiguous tasks.

**Limitations:** 

**Conclusion:** Knowledge selection is critical in specific contexts, particularly involving weaker generator models and complex tasks, suggesting that enhancing knowledge recall can improve outcomes in RAG systems.

**Abstract:** Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection, a.k.a. reranking or filtering, remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.

</details>


### [185] [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](https://arxiv.org/abs/2410.18798)

*Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, visual reasoning, data synthesis

**Relevance Score:** 7

**TL;DR:** Proposes a method for synthesizing training data for multimodal large language models focused on visual reasoning through a dataset called ReachQA.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To solve complex chart Q&A tasks using MLLMs with enhanced visual reasoning capabilities while addressing the challenges of collecting and annotating training data.

**Method:** The Code-as-Intermediary Translation (CIT) method translates visual chart representations into textual representations to generate high-quality Q&A pairs for training.

**Key Contributions:**

	1. Introduction of Code-as-Intermediary Translation (CIT) for data synthesis
	2. Creation of the ReachQA dataset with extensive visual reasoning Q&A pairs
	3. Demonstration of enhanced MLLM performance on both chart and general reasoning tasks

**Result:** ReachQA dataset includes 3k reasoning-intensive charts and 20k Q&A pairs, improving MLLMs' performance on chart tasks and general reasoning benchmarks.

**Limitations:** 

**Conclusion:** Fine-tuning models with the ReachQA dataset results in significant performance improvements for both specialized and general reasoning tasks.

**Abstract:** Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs), including recognizing key information from visual inputs and conducting reasoning over it. While fine-tuning MLLMs for reasoning is critical, collecting and annotating charts and questions is expensive, hard to scale, and often results in low-quality annotations. To address this, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling language models to understand cross-modal information and generate reasoning chains accordingly. In this way, we can employ text-based synthesizing techniques to expand chart-plotting code and generate high-quality Q&A pairs for training models. This produces ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities of MLLMs. Experiments show that models fine-tuned with ReachQA not only perform well on chart-related tasks but also show performance gains on general reasoning benchmarks. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA.

</details>


### [186] [A Computational Method for Measuring "Open Codes" in Qualitative Analysis](https://arxiv.org/abs/2411.12142)

*John Chen, Alexandros Lotsos, Sihan Cheng, Caiyi Wang, Lexie Zhao, Jessica Hullman, Bruce Sherin, Uri Wilensky, Michael Horn*

**Main category:** cs.CL

**Keywords:** Qualitative Analysis, Inductive Coding, Generative AI, Human-AI Interaction, Methodological Rigor

**Relevance Score:** 7

**TL;DR:** This paper presents a novel computational method to assess the results of inductive coding when combining human and Generative AI contributions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced in qualitative analysis when using inductive coding, particularly with Generative AI, by developing a reliable measurement method.

**Method:** The proposed method merges individual codebooks via an LLM-enhanced algorithm and measures contributions using four metrics: Coverage, Overlap, Novelty, and Divergence.

**Key Contributions:**

	1. Development of a new method for assessing inductive coding
	2. Introduction of four novel metrics for evaluation
	3. Demonstration of diagnostic capabilities for coding issues

**Result:** Experiments reveal the impact of merging algorithms on metric outcomes, demonstrate the stability and robustness of the metrics, and illustrate their diagnostic capabilities for identifying issues in coding.

**Limitations:** 

**Conclusion:** The framework offers a reliable means to uphold methodological rigor in qualitative analysis that involves both human and AI inputs.

**Abstract:** Qualitative analysis is critical to understanding human datasets in many social science disciplines. A central method in this process is inductive coding, where researchers identify and interpret codes directly from the datasets themselves. Yet, this exploratory approach poses challenges for meeting methodological expectations (such as ``depth'' and ``variation''), especially as researchers increasingly adopt Generative AI (GAI) for support. Ground-truth-based metrics are insufficient because they contradict the exploratory nature of inductive coding, while manual evaluation can be labor-intensive. This paper presents a theory-informed computational method for measuring inductive coding results from humans and GAI. Our method first merges individual codebooks using an LLM-enriched algorithm. It measures each coder's contribution against the merged result using four novel metrics: Coverage, Overlap, Novelty, and Divergence. Through two experiments on a human-coded online conversation dataset, we 1) reveal the merging algorithm's impact on metrics; 2) validate the metrics' stability and robustness across multiple runs and different LLMs; and 3) showcase the metrics' ability to diagnose coding issues, such as excessive or irrelevant (hallucinated) codes. Our work provides a reliable pathway for ensuring methodological rigor in human-AI qualitative analysis.

</details>


### [187] [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)

*Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, Graham Neubig*

**Main category:** cs.CL

**Keywords:** synthetic data, language models, data generation, benchmark, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces AgoraBench, a benchmark for evaluating language models' data generation capabilities in a systematic manner.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to understand and compare the ability of language models to generate high-quality synthetic data, especially as this ability becomes increasingly important for their application in various tasks.

**Method:** The authors propose AgoraBench, which standardizes settings and metrics to evaluate language models as synthetic data generators. They synthesize 1.26 million training instances using 6 different LMs and subsequently train 99 student models to analyze performance.

**Key Contributions:**

	1. Introduction of AgoraBench for systematic evaluation of LMs' data generation abilities
	2. Comparison of different language models revealing their unique strengths
	3. Insight into the relationship between data generation capability and problem-solving ability of LMs.

**Result:** The study reveals distinct strengths among different language models: GPT-4o excels at creating new problems, while Claude-3.5-Sonnet is superior at enhancing existing ones. Additionally, the correlation between data generation ability and problem-solving capability is weak, suggesting that intrinsic features like response quality and instruction difficulty are better indicators of data quality.

**Limitations:** 

**Conclusion:** Strategic choices in output format and model selection are crucial for optimizing the effectiveness of data generation, highlighting the multifaceted nature of language models' capabilities.

**Abstract:** Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.

</details>


### [188] [Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research](https://arxiv.org/abs/2412.04497)

*Tianyang Zhong, Zhenyuan Yang, Zhengliang Liu, Ruidong Zhang, Yiheng Liu, Haiyang Sun, Yi Pan, Yiwei Li, Yifan Zhou, Hanqi Jiang, Junhao Chen, Tianming Liu*

**Main category:** cs.CL

**Keywords:** low-resource languages, large language models, cultural preservation, linguistic diversity, ethical considerations

**Relevance Score:** 4

**TL;DR:** This paper evaluates the application of LLMs in researching low-resource languages, focusing on linguistic, historical, and cultural studies while addressing challenges like data accessibility and cultural sensitivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the significance of low-resource languages as repositories of human history and the impact of LLMs in their preservation and study.

**Method:** Systematic evaluation of LLM applications in low-resource language research, including analysis of technical frameworks, methodologies, and ethical considerations.

**Key Contributions:**

	1. Systematic analysis of LLM applications in low-resource languages
	2. Identification of key challenges in utilizing LLMs for linguistic and cultural research
	3. Emphasis on interdisciplinary approaches for research advancement

**Result:** Identification of key challenges such as data accessibility, model adaptability, and the need for cultural sensitivity in the application of LLMs.

**Limitations:** 

**Conclusion:** Interdisciplinary collaboration and customized models are essential for advancing low-resource language research and preserving global linguistic diversity.

**Abstract:** Low-resource languages serve as invaluable repositories of human history, embodying cultural evolution and intellectual diversity. Despite their significance, these languages face critical challenges, including data scarcity and technological limitations, which hinder their comprehensive study and preservation. Recent advancements in large language models (LLMs) offer transformative opportunities for addressing these challenges, enabling innovative methodologies in linguistic, historical, and cultural research. This study systematically evaluates the applications of LLMs in low-resource language research, encompassing linguistic variation, historical documentation, cultural expressions, and literary analysis. By analyzing technical frameworks, current methodologies, and ethical considerations, this paper identifies key challenges such as data accessibility, model adaptability, and cultural sensitivity. Given the cultural, historical, and linguistic richness inherent in low-resource languages, this work emphasizes interdisciplinary collaboration and the development of customized models as promising avenues for advancing research in this domain. By underscoring the potential of integrating artificial intelligence with the humanities to preserve and study humanity's linguistic and cultural heritage, this study fosters global efforts towards safeguarding intellectual diversity.

</details>


### [189] [Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction](https://arxiv.org/abs/2412.09318)

*Jing Liu, Abdellah Fourtassi*

**Main category:** cs.CL

**Keywords:** LLMs, child-caregiver interactions, benchmarking, language simulation, discourse patterns

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of LLMs to simulate child-caregiver interactions, revealing strengths in approximating dialogue but limitations in diversity and discursive patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how effectively LLMs can replicate the distinctive language features found in child-caregiver interactions and to inform the development of benchmarks for child-oriented applications.

**Method:** The researchers employed both static and interactive benchmarking methods to evaluate the performance of state-of-the-art LLMs like Llama 3 and GPT-4o in simulating child-caregiver dialogues.

**Key Contributions:**

	1. Introduction of static and interactive benchmarking for LLMs in child-caregiver dialogue simulation.
	2. Identification of specific limitations in existing LLM performance regarding discourse patterns and linguistic diversity.
	3. Aiming to inform the development of comprehensive benchmarks for evaluating LLMs in child-oriented interactions.

**Result:** LLMs can match dialogues at the word and utterance level, but they struggle with the discursive patterns and diversity present in human interactions, indicating significant areas for improvement.

**Limitations:** The study indicates that LLMs exaggerate alignment in dialogues and do not achieve the diversity of human interactions.

**Conclusion:** While LLMs show promise in generating child-caregiver dialogues, their current limitations suggest a need for further developments to create robust benchmarks for assessing LLM performance in child-oriented applications.

**Abstract:** LLMs can generate human-like dialogues, yet their ability to simulate early child-adult interactions remains largely unexplored. In this paper, we examined how effectively LLMs can capture the distinctive features of child-caregiver language in interaction, using both static and interactive benchmarking methods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can approximate child-caregiver dialogues at the word and utterance level, but they struggle to reproduce the child and caregiver's discursive patterns, exaggerate alignment, and fail to reach the level of diversity shown by humans. The broader goal of this work is to initiate the development of a comprehensive benchmark for LLMs in child-oriented applications.

</details>


### [190] [Truthful Text Sanitization Guided by Inference Attacks](https://arxiv.org/abs/2412.12928)

*Ildikó Pilán, Benet Manzanares-Salor, David Sánchez, Pierre Lison*

**Main category:** cs.CL

**Keywords:** text sanitization, personal information, large language models, privacy, utility

**Relevance Score:** 8

**TL;DR:** This paper presents a novel text sanitization method using LLMs to replace personally identifiable information while balancing privacy and utility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of text sanitization, which requires both preventing the disclosure of personal information and preserving document utility.

**Method:** The method involves a two-stage process that uses instruction-tuned LLMs to find and rank truth-preserving replacement candidates for PII, followed by evaluating their effectiveness against privacy attacks.

**Key Contributions:**

	1. Introduction of a two-stage text sanitization process using LLMs
	2. Development of novel metrics for evaluating privacy and utility
	3. Demonstration of improved performance over existing methods in preserving truthfulness of text.

**Result:** The proposed method achieves improved utility with only a marginal increase in re-identification risk compared to conventional suppression methods, and it is more truth-preserving than existing approaches like Microsoft Presidio.

**Limitations:** 

**Conclusion:** This novel approach is effective in balancing privacy protection and utility, offering a practical solution for text sanitization.

**Abstract:** Text sanitization aims to rewrite parts of a document to prevent disclosure of personal information. The central challenge of text sanitization is to strike a balance between privacy protection (avoiding the leakage of personal information) and utility preservation (retaining as much as possible of the document's original content). To this end, we introduce a novel text sanitization method based on generalizations, that is, broader but still informative terms that subsume the semantic content of the original text spans. The approach relies on the use of instruction-tuned large language models (LLMs) and is divided into two stages. Given a document including text spans expressing personally identifiable information (PII), the LLM is first applied to obtain truth-preserving replacement candidates for each text span and rank those according to their abstraction level. Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM. Finally, the system selects the most informative replacement candidate shown to be resistant to those attacks. This two-stage process produces replacements that effectively balance privacy and utility.   We also present novel metrics to evaluate these two aspects without needing to manually annotate documents. Results on the Text Anonymization Benchmark show that the proposed approach, implemented with Mistral 7B Instruct, leads to enhanced utility, with only a marginal (< 1 p.p.) increase in re-identification risk compared to fully suppressing the original spans. Furthermore, our approach is shown to be more truth-preserving than existing methods such as Microsoft Presidio's synthetic replacements.

</details>


### [191] [Acquisition of Recursive Possessives and Recursive Locatives in Mandarin](https://arxiv.org/abs/2412.16556)

*Chenxi Fu, Xiaoyi Wang, Zaijiang Man, Caimei Yang*

**Main category:** cs.CL

**Keywords:** language acquisition, recursion, Mandarin-speaking children, cognitive development, structural complexity

**Relevance Score:** 2

**TL;DR:** This study investigates how Mandarin-speaking children acquire recursive possessives and locatives, revealing that adult-like understanding is reached by age 6.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the developmental process of acquiring recursive language structures in children, particularly focusing on Mandarin.

**Method:** The research compares comprehension of two-level recursive structures among children aged 3 to 7 using a picture-based question-answering task.

**Key Contributions:**

	1. Insights into the age at which children reach proficiency in recursive structures
	2. Comparison of recursive possessives versus locatives
	3. Implications for understanding cognitive foundations of language development

**Result:** Children achieve adult-like proficiency in two-level recursion by age 6, with notable differences in the acquisition of recursive possessives and locatives.

**Limitations:** 

**Conclusion:** The findings highlight the importance of structural complexity and cognitive factors in the language acquisition process, emphasizing recursion’s role in child development.

**Abstract:** As recursion has been underlying any linguistic work for the last 60 years, the acquisition of recursive structures by children during language learning has become a focal point of inquiry. This study delves into the developmental trajectory of Mandarin-speaking children's acquisition of recursive possessives and locatives, assessing the impact of structural diversity on language acquisition. The research contrasts the comprehension of two-level recursive structures among children aged 3 to 7 years, employing answering question while seeing a picture task to elicit responses. The findings indicate that children do not attain adult-like proficiency in two-level recursion until the age of 6, and there exists a notable asymmetry in the acquisition of recursive possessives versus locatives. These results underscore the primacy of structural complexity and cognitive factors in the acquisition process, enhancing our comprehension of the cognitive foundations of language development and the pivotal role of recursion in child language acquisition.

</details>


### [192] [Improving Low-Resource Machine Translation via Cross-Linguistic Transfer from Typologically Similar High-Resource Languages](https://arxiv.org/abs/2501.00045)

*Saughmon Boujkian*

**Main category:** cs.CL

**Keywords:** Transfer learning, Machine translation, Low-resource languages, Hyperparameter tuning, Linguistic similarity

**Relevance Score:** 6

**TL;DR:** The study investigates the effectiveness of transfer learning for low-resource machine translation by fine-tuning models from high-resource languages using limited data from target languages. It confirms the applicability of transfer learning across diverse language families and offers insights into optimal hyperparameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how linguistic similarity impacts the transfer learning process in low-resource machine translation and to reduce the need for extensive training data.

**Method:** Experiments conducted on five language pairs across different families to analyze the impact of transfer learning and hyperparameter variations on translation quality.

**Key Contributions:**

	1. Demonstrated the effectiveness of transfer learning across diverse language families for low-resource translation.
	2. Provided empirical evidence supporting linguistic similarity in transfer learning adaptations.
	3. Identified optimal hyperparameter configurations for enhanced translation performance.

**Result:** Transfer learning improved translation quality across all language pairs, demonstrating its effectiveness even among typologically diverse languages. Key hyperparameters influenced the stability and performance of training.

**Limitations:** 

**Conclusion:** Transfer learning can be employed effectively in low-resource settings, providing a practical framework for building machine translation systems with reduced tuning efforts.

**Abstract:** This study examines the cross-linguistic effectiveness of transfer learning for low-resource machine translation by fine-tuning models initially trained on typologically similar high-resource languages, using limited data from the target low-resource language. We hypothesize that linguistic similarity enables efficient adaptation, reducing the need for extensive training data. To test this, we conduct experiments on five typologically diverse language pairs spanning distinct families: Semitic (Modern Standard Arabic to Levantine Arabic), Bantu (Hausa to Zulu), Romance (Spanish to Catalan), Slavic (Slovak to Macedonian), and a language isolate (Eastern Armenian to Western Armenian). Results show that transfer learning consistently improves translation quality across all pairs, confirming its applicability beyond closely related languages. As a secondary analysis, we vary key hyperparameters learning rate, batch size, number of epochs, and weight decay to ensure results are not dependent on a single configuration. We find that moderate batch sizes (e.g., 32) are often optimal for similar pairs, smaller sizes benefit less similar pairs, and excessively high learning rates can destabilize training. These findings provide empirical evidence for the generalizability of transfer learning across language families and offer practical guidance for building machine translation systems in low-resource settings with minimal tuning effort.

</details>


### [193] [Echoes in AI: Quantifying lack of plot diversity in LLM outputs](https://arxiv.org/abs/2501.00273)

*Weijia Xu, Nebojsa Jojic, Sudha Rao, Chris Brockett, Bill Dolan*

**Main category:** cs.CL

**Keywords:** large language models, story generation, creativity, Sui Generis score, plot uniqueness

**Relevance Score:** 9

**TL;DR:** The paper investigates the diversity of story ideas generated by state-of-the-art LLMs, using a new metric called the Sui Generis score to analyze uniqueness in plots across different generations.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs, it is important to understand their ability to foster creativity by generating diverse story ideas.

**Method:** Two LLMs, GPT-4 and LLaMA-3, were evaluated for story generation, using a new metric, the Sui Generis score, to quantify the uniqueness of plot elements across 100 stories.

**Key Contributions:**

	1. Introduction of the Sui Generis score for measuring plot uniqueness.
	2. Evaluation of GPT-4 and LLaMA-3 in story generation and their repetitive use of plot elements.
	3. Correlation between Sui Generis scores and human judgment of surprise in stories.

**Result:** LLM-generated stories often have repeated plot elements indicating a lack of diversity, while human-written stories show unique plots that are not echoed in LLM outputs.

**Limitations:** 

**Conclusion:** While LLMs can generate stories, they often lack the diversity seen in human creativity, as measured by the Sui Generis score, which also correlates with human assessments of surprise.

**Abstract:** With rapid advances in large language models (LLMs), there has been an increasing application of LLMs in creative content ideation and generation. A critical question emerges: can current LLMs provide ideas that are diverse enough to truly bolster collective creativity? We examine two state-of-the-art LLMs, GPT-4 and LLaMA-3, on story generation and discover that LLM-generated stories often consist of plot elements that are echoed across a number of generations. To quantify this phenomenon, we introduce the Sui Generis score, an automatic metric that measures the uniqueness of a plot element among alternative storylines generated using the same prompt under an LLM. Evaluating on 100 short stories, we find that LLM-generated stories often contain combinations of idiosyncratic plot elements echoed frequently across generations and across different LLMs, while plots from the original human-written stories are rarely recreated or even echoed in pieces. Moreover, our human evaluation shows that the ranking of Sui Generis scores among story segments correlates moderately with human judgment of surprise level, even though score computation is completely automatic without relying on human judgment.

</details>


### [194] [Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/abs/2502.03275)

*DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, latent tokens, reasoning, VQ-VAE, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a hybrid representation for reasoning processes in LLMs, using latent discrete tokens to reduce input length and computational resource consumption.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with lengthy inputs for reasoning tasks, which consume significant resources; thus, there is a need for more efficient representations.

**Method:** We propose a hybrid representation that abstracts initial reasoning steps with latent discrete tokens generated by VQ-VAE and train the model using a mixture of latent and text tokens.

**Key Contributions:**

	1. Introduction of a hybrid reasoning representation using latent tokens
	2. Demonstrated efficiency in training and adaptability to new latent tokens
	3. Benchmark performance improvements over existing methods

**Result:** The method consistently outperforms baseline approaches in benchmarks related to logical and mathematical reasoning problems.

**Limitations:** 

**Conclusion:** The hybrid approach enables LLMs to efficiently handle reasoning tasks while reducing input length and resource usage, and shows promise in various reasoning scenarios.

**Abstract:** Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.

</details>


### [195] [Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2502.10708)

*Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, domain-specific knowledge, knowledge injection, natural language processing, health informatics

**Relevance Score:** 9

**TL;DR:** This survey categorizes methods to enhance Large Language Models (LLMs) with domain-specific knowledge into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of general-purpose LLMs in domain-specific applications such as healthcare and chemistry by integrating specialized knowledge.

**Method:** The survey categorizes enhancement methods for LLMs into four approaches and discusses their mechanisms, trade-offs, and effectiveness in specialized tasks.

**Key Contributions:**

	1. Comprehensive overview of methods to enhance LLMs with domain knowledge.
	2. Categorization of methods into four key approaches with discussion on their trade-offs.
	3. Evaluation of domain-specific LLMs against general-purpose LLMs and summarization of datasets.

**Result:** The methods enable LLMs to better tackle specialized tasks by incorporating domain expertise, and the paper compares their advantages, disadvantages, and evaluates performance against general LLMs.

**Limitations:** 

**Conclusion:** The paper highlights the challenges and opportunities in enhancing LLMs for domain-specific applications and maintains a resource repository for ongoing research.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.

</details>


### [196] [LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning](https://arxiv.org/abs/2502.11176)

*Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See*

**Main category:** cs.CL

**Keywords:** large language models, logical inference, analogical reasoning, in-context learning, hypothesis verification

**Relevance Score:** 9

**TL;DR:** The paper explores logical inference in large language models (LLMs) through a controlled evaluation environment for analogical reasoning, analyzing different inference pipelines and their impact on reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how to optimally leverage logical inference paradigms is critical for advancing the reasoning capabilities of LLMs.

**Method:** The paper introduces a controlled evaluation environment parameterized by modality, difficulty, and task format, analyzing inductive, abductive, and deductive inference pipelines.

**Key Contributions:**

	1. Development of a controlled evaluation environment for analogical reasoning
	2. Comparative analysis of inference dynamics across modalities and task formats
	3. Investigation of advanced paradigms for scaling logical inference in LLM reasoning

**Result:** The findings demonstrate that the comparative dynamics of different inference types generalize to broader in-context learning tasks, with advanced paradigms showing potential to enhance reasoning.

**Limitations:** 

**Conclusion:** This exploratory study lays a foundation for future research aimed at improving LLM reasoning through systematic logical inference strategies.

**Abstract:** Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies. Resources are available at https://github.com/HKUST-KnowComp/LogiDynamics.

</details>


### [197] [InsBank: Evolving Instruction Subset for Ongoing Alignment](https://arxiv.org/abs/2502.11419)

*Jiayi Shi, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Huan Ren, Yao Hu, Kan Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Tuning, Machine Learning, Diversity Score, Data Selection

**Relevance Score:** 8

**TL;DR:** The paper presents Instruction Bank (InsBank) and the Progressive Instruction Bank Evolution (PIBE) framework for effectively updating instruction data for large language models (LLMs) to enhance alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ongoing alignment of LLMs through the quality and diversity of instruction data rather than sheer quantity, and to explore how to evolve selected subsets of instruction data over time.

**Method:** Introduces InsBank as a continuously updated repository for instruction data and proposes the PIBE framework, which employs a gradual data selection strategy using a representation-based diversity score for efficient evolution of InsBank.

**Key Contributions:**

	1. Introduction of Instruction Bank (InsBank) for evolving instruction data
	2. Development of Progressive Instruction Bank Evolution (PIBE) for efficient data selection
	3. Utilization of a representation-based diversity score for comprehensive evaluation of instruction data

**Result:** Experimental results show that PIBE significantly outperforms baselines in evolving InsBank and can extract subsets tailored to specific budgets.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates improved effectiveness and adaptability in maintaining a valuable repository of instruction data for LLMs.

**Abstract:** Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMs' ongoing alignment, we introduce Instruction Bank (\textbf{InsBank}), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (\textbf{PIBE}), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability.

</details>


### [198] [Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh](https://arxiv.org/abs/2502.13647)

*Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto*

**Main category:** cs.CL

**Keywords:** instruction tuning, low-resource languages, LLM-assisted data generation, Kazakhstan, governance

**Relevance Score:** 6

**TL;DR:** This paper presents a large-scale instruction-following dataset for low-resource languages, specifically for Kazakhstan, and explores fine-tuning LLMs for improved performance in governance-related topics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve instruction tuning for low-resource languages by providing a substantial dataset in the context of Kazakhstan's institutional and cultural knowledge.

**Method:** The authors developed a dataset of 10,600 samples using LLM-assisted data generation and manually verified each entry. They compared the performance of various models (Qwen, Falcon, Gemma) after fine-tuning on the dataset.

**Key Contributions:**

	1. Introduction of a large instruction-following dataset for Kazakhstan
	2. Demonstration of LLM-assisted data generation
	3. Fine-tuning effects on multiple models showing performance improvements

**Result:** Fine-tuning LLMs on this dataset resulted in consistent performance improvements in both multiple-choice and generative tasks, indicating successful application of instruction tuning in low-resource settings.

**Limitations:** 

**Conclusion:** The dataset opens up new possibilities for instruction tuning in low-resource languages and enhances the understanding of governance and cultural topics by LLMs.

**Abstract:** Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.

</details>


### [199] [Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs](https://arxiv.org/abs/2502.16534)

*Jonathan Rystrøm, Hannah Rose Kirk, Scott Hale*

**Main category:** cs.CL

**Keywords:** Large Language Models, cultural alignment, language capability, multilingualism, bias

**Relevance Score:** 7

**TL;DR:** This paper examines cultural alignment in LLMs across languages, revealing that self-consistency is a better predictor of multicultural alignment than language capabilities alone.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of US-centric bias in Large Language Models (LLMs) and the need for appropriate cultural representations across different languages.

**Method:** A novel methodology is proposed that compares LLM-generated responses with population-level opinion data from the World Value Survey using a linear mixed-effects regression framework.

**Key Contributions:**

	1. Introduction of a methodology for assessing cultural alignment in LLMs
	2. Comparison of Google's Gemma models and OpenAI's turbo-series regarding cultural bias
	3. Highlighting self-consistency as a key factor for multicultural alignment

**Result:** The study finds that while Gemma models show a positive correlation between language capability and cultural alignment, OpenAI's models do not. Self-consistency emerges as a stronger predictor of multicultural alignment than multilingual capabilities.

**Limitations:** 

**Conclusion:** Achieving meaningful cultural alignment in LLMs necessitates focused efforts beyond just enhancing language capabilities.

**Abstract:** Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.

</details>


### [200] [Automatic Input Rewriting Improves Translation with Large Language Models](https://arxiv.org/abs/2502.16682)

*Dayeon Ki, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** machine translation, input rewriting, LLMs, text simplification, quality estimation

**Relevance Score:** 8

**TL;DR:** This paper explores using LLMs for input rewriting to enhance machine translation efficacy, revealing that text simplification is the most effective strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates whether rewriting inputs through LLMs can improve machine translation, as improved input quality is believed to enhance translation output.

**Method:** An empirical study was conducted evaluating 21 input rewriting methods across 3 open-weight LLMs for English-to-MT translations in 6 target languages.

**Key Contributions:**

	1. Demonstration of 21 effective input rewriting methods for MT.
	2. Identification of text simplification as the top-performing strategy.
	3. Showcasing the utility of LLMs for pre-editing inputs to improve translations.

**Result:** Text simplification emerged as the most effective rewrite strategy, with enhancements possible through quality estimation to assess translatability. Human evaluations confirmed that simplified rewrites retained original meaning effectively.

**Limitations:** 

**Conclusion:** LLM-assisted input rewriting shows promise as a method to enhance machine translation outcomes.

**Abstract:** Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.

</details>


### [201] [A Causal Lens for Evaluating Faithfulness Metrics](https://arxiv.org/abs/2502.18848)

*Kerem Zaman, Shashank Srivastava*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Interpretability, Faithfulness Metrics, Natural Language Explanations, Causal Diagnosticity

**Relevance Score:** 9

**TL;DR:** The paper proposes Causal Diagnosticity, a framework for evaluating faithfulness metrics of natural language explanations from LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the fidelity of natural language explanations from LLMs, addressing the limitations in existing faithfulness metrics.

**Method:** Causal Diagnosticity framework evaluates faithfulness metrics using model-editing methods to create explanations of varying faithfulness.

**Key Contributions:**

	1. Introduction of Causal Diagnosticity framework for evaluating faithfulness metrics
	2. Empirical evaluation of various faithfulness metrics across different tasks
	3. Insights on the effectiveness of continuous vs. binary metrics and the performance of Filler Tokens.

**Result:** The study reveals that diagnostic performance of faithfulness metrics varies by task and model, with continuous metrics generally being more effective; Filler Tokens achieve the best performance overall.

**Limitations:** Focused primarily on a specific set of tasks and metrics; results may not generalize across all LLMs or applications.

**Conclusion:** The research underscores the necessity for improved robustness in faithfulness metrics used for LLM explanations.

**Abstract:** Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.

</details>


### [202] [Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents](https://arxiv.org/abs/2503.12225)

*Rinku Dewri*

**Main category:** cs.CL

**Keywords:** large language models, privacy policy, data practices, automated compliance, accuracy issues

**Relevance Score:** 7

**TL;DR:** Explores gaps in using LLMs for interpreting complex privacy policies, highlighting issues in accuracy and completeness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and inaccuracies that LLMs may produce when simplifying data practices from privacy policies.

**Method:** Analyzing various interpretations generated by an LLM against established accuracy and completeness benchmarks.

**Key Contributions:**

	1. Identification of accuracy gaps in LLM interpretations of privacy policies
	2. Discussion on clarity and representation issues in LLM outputs
	3. Call for further research to improve LLM applications in privacy management

**Result:** Identified specific gaps in interpretation concerning accuracy, completeness, clarity, and representation.

**Limitations:** 

**Conclusion:** There is a need for ongoing research to harness the full capabilities of LLMs in improving privacy management and compliance.

**Abstract:** This article explores the gaps that can manifest when using a large language model (LLM) to obtain simplified interpretations of data practices from a complex privacy policy. We exemplify these gaps to showcase issues in accuracy, completeness, clarity and representation, while advocating for continued research to realize an LLM's true potential in revolutionizing privacy management through personal assistants and automated compliance checking.

</details>


### [203] [UniBERT: Adversarial Training for Language-Universal Representations](https://arxiv.org/abs/2503.12608)

*Andrei-Marius Avram, Marian Lupaşcu, Dumitru-Clementin Cercel, Ionuţ Mironică, Ştefan Trăuşan-Matu*

**Main category:** cs.CL

**Keywords:** multilingual language model, adversarial training, knowledge distillation, NLP, cross-lingual processing

**Relevance Score:** 8

**TL;DR:** UniBERT is a compact multilingual language model that utilizes masked language modeling, adversarial training, and knowledge distillation, achieving improved performance on various NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more efficient multilingual language model that reduces computational demands while preserving performance in natural language processing tasks.

**Method:** UniBERT employs a novel training framework integrating masked language modeling, adversarial training, and knowledge distillation, pre-trained on a Wikipedia corpus in 107 languages.

**Key Contributions:**

	1. Development of UniBERT model
	2. Successful integration of adversarial training and knowledge distillation
	3. Significant improvements in cross-lingual generalization

**Result:** UniBERT demonstrates an average relative improvement of 7.72% over traditional baselines across four NLP tasks, with statistical significance confirmed (p-value = 0.0181).

**Limitations:** 

**Conclusion:** Combining adversarial training and knowledge distillation can build scalable and robust language models, enhancing multilingual and cross-lingual NLP capabilities.

**Abstract:** This paper presents UniBERT, a compact multilingual language model that uses an innovative training framework that integrates three components: masked language modeling, adversarial training, and knowledge distillation. Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages, UniBERT is designed to reduce the computational demands of large-scale models while maintaining competitive performance across various natural language processing tasks. Comprehensive evaluations on four tasks - named entity recognition, natural language inference, question answering, and semantic textual similarity - demonstrate that our multilingual training strategy enhanced by an adversarial objective significantly improves cross-lingual generalization. Specifically, UniBERT models show an average relative improvement of 7.72% over traditional baselines, which achieved an average relative improvement of only 1.17%, and statistical analysis confirms the significance of these gains (p-value = 0.0181). This work highlights the benefits of combining adversarial training and knowledge distillation to build scalable and robust language models, thus advancing the field of multilingual and cross-lingual natural language processing.

</details>


### [204] [Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](https://arxiv.org/abs/2503.18172)

*Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu*

**Main category:** cs.CL

**Keywords:** MLLM, chart comprehension, misleading visualizations, benchmarking, responsible communication

**Relevance Score:** 8

**TL;DR:** The paper introduces the Misleading ChartQA benchmark dataset to evaluate multimodal large language models (MLLMs) in detecting and interpreting misleading chart visualizations, highlighting the need for responsible visual communication in AI systems.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Misleading visualizations can distort perception and lead to incorrect conclusions, posing risks to public understanding and safety in AI-driven data communication.

**Method:** The authors created the Misleading ChartQA benchmark, a dataset with 3,026 examples across 21 misleader types and 10 chart types, including standardized chart code, CSV data, multiple-choice questions, and labeled explanations. They benchmarked 24 state-of-the-art MLLMs and developed a novel reasoning pipeline.

**Key Contributions:**

	1. Introduction of the Misleading ChartQA benchmark for MLLMs
	2. Benchmarking of 24 state-of-the-art MLLMs
	3. Development of a novel region-aware reasoning pipeline to enhance model accuracy.

**Result:** The study found significant variation in the performance of MLLMs across different misleader types and chart formats, illustrating the challenges in recognizing misleading charts.

**Limitations:** Limited to evaluating MLLMs; does not address the broader implications of visual perception in general AI systems.

**Conclusion:** This work establishes a benchmark for future research and development of MLLMs that can better interpret misleading visualizations, contributing to more trustworthy and responsible AI systems.

**Abstract:** Misleading visualizations, which manipulate chart representations to support specific claims, can distort perception and lead to incorrect conclusions. Despite decades of research, they remain a widespread issue-posing risks to public understanding and raising safety concerns for AI systems involved in data-driven communication. While recent multimodal large language models (MLLMs) show strong chart comprehension abilities, their capacity to detect and interpret misleading charts remains unexplored. We introduce Misleading ChartQA benchmark, a large-scale multimodal dataset designed to evaluate MLLMs on misleading chart reasoning. It contains 3,026 curated examples spanning 21 misleader types and 10 chart types, each with standardized chart code, CSV data, multiple-choice questions, and labeled explanations, validated through iterative MLLM checks and exhausted expert human review. We benchmark 24 state-of-the-art MLLMs, analyze their performance across misleader types and chart formats, and propose a novel region-aware reasoning pipeline that enhances model accuracy. Our work lays the foundation for developing MLLMs that are robust, trustworthy, and aligned with the demands of responsible visual communication.

</details>


### [205] [CoRanking: Collaborative Ranking with Small and Large Ranking Agents](https://arxiv.org/abs/2503.23427)

*Wenhan Liu, Xinyu Ma, Yutao Zhu, Lixin Su, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Collaborative Ranking, Passage Ranking, Efficiency, Reinforcement Learning

**Relevance Score:** 8

**TL;DR:** CoRanking is a collaborative ranking framework that combines small and large ranking models for efficient passage ranking using large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the efficiency challenges posed by LLMs' reliance on large-scale parameters and the sliding window process during ranking.

**Method:** The method utilizes a small reranker to pre-rank candidate passages, bringing relevant ones to the top (e.g., top-20), followed by a listwise reranker powered by an LLM only on these top passages. A passage order adjuster trained via reinforcement learning is also introduced to optimize order alignment with LLM preferences.

**Key Contributions:**

	1. Introduction of CoRanking framework that combines small and large models for ranking.
	2. Leveraging a passage order adjuster trained with reinforcement learning.
	3. Demonstrated significant improvements in ranking efficiency and effectiveness on multiple IR benchmarks.

**Result:** CoRanking significantly improves ranking efficiency by reducing ranking latency by about 70% while achieving better effectiveness than using only the LLM listwise reranker.

**Limitations:** The paper does not address potential drawbacks of using reinforcement learning for passage reordering and may require further validation across diverse datasets.

**Conclusion:** The proposed framework not only enhances efficiency in ranking but also addresses the positional biases inherent in LLM rerankers.

**Abstract:** Large Language Models (LLMs) have demonstrated superior listwise ranking performance. However, their superior performance often relies on large-scale parameters (\eg, GPT-4) and a repetitive sliding window process, which introduces significant efficiency challenges. In this paper, we propose \textbf{CoRanking}, a novel collaborative ranking framework that combines small and large ranking models for efficient and effective ranking. CoRanking first employs a small-size reranker to pre-rank all the candidate passages, bringing relevant ones to the top part of the list (\eg, top-20). Then, the LLM listwise reranker is applied to only rerank these top-ranked passages instead of the whole list, substantially enhancing overall ranking efficiency. Although more efficient, previous studies have revealed that the LLM listwise reranker have significant positional biases on the order of input passages. Directly feed the top-ranked passages from small reranker may result in the sub-optimal performance of LLM listwise reranker. To alleviate this problem, we introduce a passage order adjuster trained via reinforcement learning, which reorders the top passages from the small reranker to align with the LLM's preferences of passage order. Extensive experiments on three IR benchmarks demonstrate that CoRanking significantly improves efficiency (reducing ranking latency by about 70\%) while achieving even better effectiveness compared to using only the LLM listwise reranker.

</details>


### [206] [AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset](https://arxiv.org/abs/2504.03612)

*Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Preference Learning, Large Language Models, Dataset Optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces AIR, a component-wise analysis framework for optimizing preference learning datasets for large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the alignment of LLMs with human values by improving the quality of datasets used in preference learning.

**Method:** The authors developed the AIR framework to isolate and systematically optimize the three key components of preference datasets: Annotations, Instructions, and Response Pairs, while evaluating their combined effects.

**Key Contributions:**

	1. Introduction of the AIR framework for preference dataset optimization.
	2. Identification of key principles for improving dataset quality in LLM training.
	3. Evidence of significant performance gains with optimized components under controlled datasets.

**Result:** The AIR framework demonstrates that applying principles like annotation simplicity and response pair quality can lead to an average performance improvement of +5.3 over baseline methods using only 14k high-quality pairs.

**Limitations:** 

**Conclusion:** The study advocates for a shift in preference dataset design towards a more structured, component-aware optimization approach rather than just scaling existing datasets.

**Abstract:** Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \textbf{A}nnotations, \textbf{I}nstructions, and \textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.

</details>


### [207] [RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models](https://arxiv.org/abs/2504.11381)

*Juan Diego Rodriguez, Wenxuan Ding, Katrin Erk, Greg Durrett*

**Main category:** cs.CL

**Keywords:** large language models, generator-validator gap, RankAlign, training method, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper addresses the inconsistency of large language models (LLMs) in reporting information by introducing a generator-validator gap. It proposes a novel training method, RankAlign, to close this gap and improve model reliability.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unreliability and inconsistency in LLMs' responses, particularly when prompts are altered, and to enhance the correlation between generated answers and their verification.

**Method:** The paper defines a generator-validator gap based on score correlation of generated answers across various tasks. It proposes RankAlign, a ranking-based training method to reduce this gap.

**Key Contributions:**

	1. Definition of a stringent generator-validator gap
	2. Introduction of the RankAlign ranking-based training method
	3. Demonstrated efficacy of RankAlign across multiple tasks

**Result:** The proposed RankAlign method significantly closes the generator-validator gap across several tasks, including question answering and lexical semantics, and shows generalization to out-of-domain tasks.

**Limitations:** 

**Conclusion:** RankAlign effectively improves the reliability of language models by aligning the outputs of generators better with their validations, thus minimizing inconsistencies in responses.

**Abstract:** Although large language models (LLMs) have become more capable and accurate across many tasks, some fundamental sources of unreliability remain in their behavior. One key limitation is their inconsistency at reporting the same information when prompts are changed. In this paper, we consider the discrepancy between a model's generated answer and their own verification of that answer, the generator-validator gap. We define this gap in a more stringent way than prior work: we expect correlation of scores from a generator and a validator over the entire set of candidate answers, i.e., candidate completions that could possibly arise during ordinary language use without breaking Gricean norms. We show that according to this measure, a large gap exists in various settings, including question answering, lexical semantics tasks, and next-word prediction. We then propose RankAlign, a ranking-based training method, and show that it significantly closes the gap, surpassing all baseline methods. Moreover, this approach generalizes well to out-of-domain tasks and lexical items.

</details>


### [208] [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)

*Dayeon Ki, Kevin Duh, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Machine Translation, Quality Estimation, AskQE, Question Generation, MT Error Detection

**Relevance Score:** 6

**TL;DR:** AskQE is a framework that generates questions to assess the quality of machine translations for non-French speakers, enabling them to decide on sharing the translations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Monolingual English speakers need a reliable way to evaluate the quality of automatic translations in languages they do not understand.

**Method:** The framework generates questions and provides answers about machine translation errors using a dataset of contrastive synthetic MT errors and incorporates models like LLaMA-3 70B for optimization.

**Key Contributions:**

	1. Introduction of the AskQE framework for MT error detection.
	2. Application of LLaMA-3 70B for question generation and evaluation.
	3. Demonstrated higher accuracy in quality assessment on the BioMQM dataset.

**Result:** AskQE shows improved accuracy and correlation with human evaluations compared to existing quality estimation techniques on the BioMQM dataset.

**Limitations:** The system is primarily optimized for the COVID-19 domain and may not generalize to other contexts without further adaptation.

**Conclusion:** The AskQE framework successfully enhances the ability of users to assess MT outputs, making it a valuable tool for non-expert users.

**Abstract:** How can a monolingual English speaker determine whether an automatic translation in French is good enough to be shared? Existing MT error detection and quality estimation (QE) techniques do not address this practical scenario. We introduce AskQE, a question generation and answering framework designed to detect critical MT errors and provide actionable feedback, helping users decide whether to accept or reject MT outputs even without the knowledge of the target language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the COVID-19 domain, we explore design choices for AskQE and develop an optimized version relying on LLaMA-3 70B and entailed facts to guide question generation. We evaluate the resulting system on the BioMQM dataset of naturally occurring MT errors, where AskQE has higher Kendall's Tau correlation and decision accuracy with human ratings compared to other QE metrics.

</details>


### [209] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)

*Takuma Udagawa, Yang Zhao, Hiroshi Kanayama, Bishwaranjan Bhattacharjee*

**Main category:** cs.CL

**Keywords:** social bias, large language models, annotation pipeline, language polarity, diverse demographics

**Relevance Score:** 9

**TL;DR:** The paper presents an annotation pipeline for investigating social biases in pretraining corpora of large language models (LLMs), specifically targeting biases in Web-crawled texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of undesirable social biases in pretraining data for large language models (LLMs) that can be perpetuated or amplified by these models.

**Method:** The proposed annotation pipeline includes steps for protected attribute detection to identify diverse demographics and a classification phase to analyze the language polarity towards these attributes.

**Key Contributions:**

	1. Introduction of a novel annotation pipeline for bias detection
	2. Application of protected attribute detection and regard classification
	3. Empirical analysis of biases in the Common Crawl dataset

**Result:** The experiments demonstrate the effectiveness of the bias analysis and mitigation measures applied to the Common Crawl dataset, which is utilized as a representative pretraining corpus.

**Limitations:** The study may not cover all forms of bias present in other datasets and models.

**Conclusion:** The study highlights the importance of understanding and mitigating social biases in LLM training data to improve fairness in language model outputs.

**Abstract:** Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.

</details>


### [210] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)

*Christopher Ormerod*

**Main category:** cs.CL

**Keywords:** automated essay scoring, large language models, feedback-oriented annotations, spelling correction, argumentative components

**Relevance Score:** 7

**TL;DR:** This study explores enhancing automated essay scoring (AES) by integrating feedback-oriented annotations identifying errors and argumentative components, improving performance using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of automated essay scoring systems, especially for educational assessments.

**Method:** Feedback-driven annotations for spelling/grammatical errors and argumentative components were integrated into the AES pipeline, validated using the PERSUADE corpus.

**Key Contributions:**

	1. Integration of feedback-oriented annotations in automated essay scoring
	2. Utilization of generative language models for corrections
	3. Demonstration of performance improvement with encoder-based LLMs

**Result:** The study showed improved scoring accuracy when feedback-oriented annotations were utilized in conjunction with LLMs for essay scoring.

**Limitations:** 

**Conclusion:** Incorporating feedback annotations into scoring processes can significantly enhance the effectiveness of automated essay scoring systems.

**Abstract:** This study illustrates how incorporating feedback-oriented annotations into the scoring pipeline can enhance the accuracy of automated essay scoring (AES). This approach is demonstrated with the Persuasive Essays for Rating, Selecting, and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We integrate two types of feedback-driven annotations: those that identify spelling and grammatical errors, and those that highlight argumentative components. To illustrate how this method could be applied in real-world scenarios, we employ two LLMs to generate annotations -- a generative language model used for spell correction and an encoder-based token-classifier trained to identify and mark argumentative elements. By incorporating annotations into the scoring process, we demonstrate improvements in performance using encoder-based large language models fine-tuned as classifiers.

</details>


### [211] [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)

*Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister*

**Main category:** cs.CL

**Keywords:** long-context LLMs, in-context learning, demonstration selection

**Relevance Score:** 9

**TL;DR:** This paper proposes two strategies for demonstration selection in many-shot in-context learning (ICL) for large language models (LLMs), enhancing performance while minimizing computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of many-shot ICL in large language models without incurring high inference costs and leveraging cached computations.

**Method:** The paper introduces two methods: one combines a few similar demonstrations with a larger set of random demonstrations; the other replaces random demonstrations with those selected via k-means clustering based on test sample representations.

**Key Contributions:**

	1. Introduction of two novel strategies for demonstration selection in many-shot ICL
	2. Demonstration selection based on similarity and k-means clustering
	3. Reduction of inference costs by up to an order of magnitude

**Result:** Experimental results show that these demonstration selection strategies consistently outperform random selection and match or surpass the best existing methods, while significantly reducing inference costs.

**Limitations:** 

**Conclusion:** The findings suggest that optimizing demonstration selection can lead to substantial improvements in efficiency and performance in many-shot ICL scenarios.

**Abstract:** Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.

</details>


### [212] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)

*Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu*

**Main category:** cs.CL

**Keywords:** Large Language Models, peer review, multimodal, benchmark, automated systems

**Relevance Score:** 8

**TL;DR:** This paper introduces MMReview, a comprehensive benchmark designed to evaluate the ability of LLMs and MLLMs in generating peer review comments across multiple disciplines and modalities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of a unified evaluation benchmark for LLMs in the context of academic peer review, especially with multimodal content, creates difficulties in assessing their effectiveness.

**Method:** The authors propose the MMReview benchmark, which includes 240 papers from 17 disciplines with expert-written review comments, and design 13 tasks to evaluate LLM and MLLM performance in review generation and alignment with human preferences.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark (MMReview) for evaluating LLMs in peer review tasks.
	2. Inclusion of multimodal content and expert comments across diverse research domains.
	3. Designing evaluation tasks that encompass various aspects of review generation performance.

**Result:** Extensive experiments on 16 open-source and 5 closed-source models showcase the thorough evaluation capabilities of the MMReview benchmark.

**Limitations:** 

**Conclusion:** MMReview serves as a foundational step towards the development of standardized automated peer review systems in academia.

**Abstract:** With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.

</details>
