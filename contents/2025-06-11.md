# 2025-06-11

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 113]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [AffectMachine-Pop: A controllable expert system for real-time pop music generation](https://arxiv.org/abs/2506.08200)

*Kat R. Agres, Adyasha Dash, Phoebe Chua, Stefan K. Ehrlich*

**Main category:** cs.HC

**Keywords:** affective music generation, AI, emotion self-regulation

**Relevance Score:** 4

**TL;DR:** AffectMachine-Pop generates retro-pop music based on user emotions, enhancing flexibility and user control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing AI music systems that are not directly controllable, making them less adaptive to individual users' emotional needs.

**Method:** The paper presents AffectMachine-Pop, which generates music according to specific arousal and valence values from either predetermined data or real-time user emotions, validated through a listening study.

**Key Contributions:**

	1. Introduction of a controllable AI music generation system
	2. Validation of music generation through user listening studies
	3. Application for emotion self-regulation in biofeedback systems

**Result:** The listening study confirmed that AffectMachine-Pop effectively generates music that meets target arousal and valence levels as intended.

**Limitations:** 

**Conclusion:** AffectMachine-Pop can be used as an interactive tool for music generation based on user emotions or integrated into bio/neurofeedback systems for emotion regulation.

**Abstract:** Music is a powerful medium for influencing listeners' emotional states, and this capacity has driven a surge of research interest in AI-based affective music generation in recent years. Many existing systems, however, are a black box which are not directly controllable, thus making these systems less flexible and adaptive to users. We present \textit{AffectMachine-Pop}, an expert system capable of generating retro-pop music according to arousal and valence values, which can either be pre-determined or based on a listener's real-time emotion states. To validate the efficacy of the system, we conducted a listening study demonstrating that AffectMachine-Pop is capable of generating affective music at target levels of arousal and valence. The system is tailored for use either as a tool for generating interactive affective music based on user input, or for incorporation into biofeedback or neurofeedback systems to assist users with emotion self-regulation.

</details>


### [2] [Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling](https://arxiv.org/abs/2506.08294)

*Ruanqianqian Huang, Ayana Monroe, Peli de Halleux, Sorin Lerner, Nikolaj Bj√∏rner*

**Main category:** cs.HC

**Keywords:** logic modeling, educational environments, constraint-satisfaction problems, Z3Guide, student feedback

**Relevance Score:** 4

**TL;DR:** This paper discusses designing educational environments for teaching logic modeling in constraint-satisfaction problems, presenting guidelines and an implementation called Z3Guide that received positive feedback from students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources for teaching logic modeling and improve educational environments for both teachers and students.

**Method:** The authors conducted need-finding interviews with teachers and iterated designs based on their feedback, resulting in ten design guidelines which were implemented in a tool called Z3Guide.

**Key Contributions:**

	1. Curated 10 design guidelines for teaching logic modeling
	2. Developed Z3Guide, an open-source tool for learning logic modeling
	3. Gathered and analyzed student feedback for enhancing educational tools

**Result:** Z3Guide was used in a workshop with over 100 students, who provided positive feedback on its effectiveness in supporting the learning of logic modeling.

**Limitations:** The study primarily focuses on one educational setting and may need further validation in diverse contexts.

**Conclusion:** The findings suggest that the designed educational environment meets the needs of students and teachers, while identifying areas for future improvement.

**Abstract:** Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from budgeting for grocery shopping to verifying software behavior. Logic modeling helps solve CSPs programmatically using SMT solvers. Despite its importance in many Computer Science disciplines, resources for teaching and learning logic modeling are scarce and scattered, and challenges remain in designing educational environments for logic modeling that are accessible and meet the needs of teachers and students. This paper explores how to design such an environment and probes the impact of the design on the learning experience. From a need-finding interview study and a design iteration with teachers of logic modeling, we curated 10 design guidelines spanning three main requirements: providing easy access, supporting various educational modalities, and allowing extensions for customized pedagogical needs. We implemented nine guidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a logic modeling learning workshop with more than 100 students, we gathered positive feedback on its support for learning and identified opportunities for future improvements.

</details>


### [3] [EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation](https://arxiv.org/abs/2506.08303)

*Thomas M. Kwok, Hilary HY Cheng, Wai Tuck Chow*

**Main category:** cs.HC

**Keywords:** wearable technology, haptic feedback, telerehabilitation

**Relevance Score:** 8

**TL;DR:** HJ-Pal is a wearable haptic device that provides muscle activation feedback for remote palpation in telerehabilitation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve remote palpation techniques for small muscle assessment in telerehabilitation.

**Method:** Development of a lightweight device utilizing EMG-driven honeycomb jamming for kinesthetic feedback.

**Key Contributions:**

	1. Introduction of HJ-Pal as a novel wearable haptic device
	2. EMG-driven honeycomb jamming mechanism for feedback
	3. Enhancement of telerehabilitation practices.

**Result:** HJ-Pal effectively transmits muscle activation feedback to users, enhancing their ability to perform remote palpation.

**Limitations:** 

**Conclusion:** HJ-Pal has significant potential applications in telerehabilitation, providing a novel approach to muscle assessment.

**Abstract:** In this work, we introduce HJ-Pal, a lightweight wearable haptic device that leverages EMG-driven honeycomb jamming to render muscle activation as kinesthetic feedback, enabling remote palpation for small muscle assessment in telerehabilitation.

</details>


### [4] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)

*Kazuki Kawamura, Jun Rekimoto*

**Main category:** cs.HC

**Keywords:** AI illustration, HCI, learning environment, image generation, large-language models

**Relevance Score:** 6

**TL;DR:** SakugaFlow integrates a four-stage pipeline for AI image generation with a large-language-model tutor to enhance learning for novice artists by providing feedback and allowing non-linear revisions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current AI illustration tools lack transparency in the creative process of human artists, failing to support novice artists effectively.

**Method:** The paper presents SakugaFlow, a pipeline that pairs diffusion-based image generation with real-time feedback mechanisms in a four-stage process, allowing for interactive learning and revisions.

**Key Contributions:**

	1. Development of a four-stage generative pipeline for AI-assisted art creation
	2. Integration of real-time feedback from a large-language model
	3. Creation of a scaffolded environment for artistic skills improvement

**Result:** SakugaFlow facilitates a creative learning environment where novices can understand artistic principles and receive iterative feedback, leading to improved artistic skills.

**Limitations:** 

**Conclusion:** By revealing the generative process and incorporating dialogue, SakugaFlow transforms image generation into an educational tool that enhances understanding of art fundamentals.

**Abstract:** While current AI illustration tools can generate high-quality images from text prompts, they rarely reveal the step-by-step procedure that human artists follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based image generation with a large-language-model tutor. At each stage, novices receive real-time feedback on anatomy, perspective, and composition, revise any step non-linearly, and branch alternative versions. By exposing intermediate outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box generator into a scaffolded learning environment that supports both creative exploration and skills acquisition.

</details>


### [5] [Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education](https://arxiv.org/abs/2506.08467)

*Prakash Shukla, Suchismita Naik, Ike Obi, Jessica Backus, Nancy Rasche, Paul Parson*

**Main category:** cs.HC

**Keywords:** AI citation, HCI education, UX design, student projects, pedagogical strategies

**Relevance Score:** 8

**TL;DR:** Examines AI citation practices in HCI education based on UX design projects by undergraduate students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address unresolved challenges in citation and documentation of AI-generated content in HCI education.

**Method:** Qualitative analysis of 35 team projects and 175 student reflections on AI usage and citation in UX design.

**Key Contributions:**

	1. Identifies inconsistencies in AI citation practices among students
	2. Raises questions about traditional citation frameworks in HCI education
	3. Proposes alternative citation strategies to support meaningful AI collaboration

**Result:** Identified varied citation practices, revealing gaps in existing citation frameworks and highlighting issues of authorship and pedagogical transparency.

**Limitations:** 

**Conclusion:** Proposes rethinking AI citation as a reflective practice and suggests strategies like AI contribution statements to enhance student-AI collaboration.

**Abstract:** The growing integration of AI tools in student design projects presents an unresolved challenge in HCI education: how should AI-generated content be cited and documented? Traditional citation frameworks -- grounded in credibility, retrievability, and authorship -- struggle to accommodate the dynamic and ephemeral nature of AI outputs. In this paper, we examine how undergraduate students in a UX design course approached AI usage and citation when given the freedom to integrate generative tools into their design process. Through qualitative analysis of 35 team projects and reflections from 175 students, we identify varied citation practices ranging from formal attribution to indirect or absent acknowledgment. These inconsistencies reveal gaps in existing frameworks and raise questions about authorship, assessment, and pedagogical transparency. We argue for rethinking AI citation as a reflective and pedagogical practice; one that supports metacognitive engagement by prompting students to critically evaluate how and why they used AI throughout the design process. We propose alternative strategies -- such as AI contribution statements and process-aware citation models that better align with the iterative and reflective nature of design education. This work invites educators to reconsider how citation practices can support meaningful student--AI collaboration.

</details>


### [6] [Guidelines for Gaze-based Neural Preliminary Diagnosis](https://arxiv.org/abs/2506.08517)

*Mayar Elfares, Salma Younis, Pascal Reisert, Ralf K√ºsters, Tobias Renner, Andreas Bulling*

**Main category:** cs.HC

**Keywords:** neural disorders, eye tracking, gaze-based diagnosis, cognitive processing, objective insights

**Relevance Score:** 7

**TL;DR:** This paper discusses the use of eye tracking as a method for preliminary diagnosis of neural disorders by analyzing eye movements to gain insights into brain function and cognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of traditional neural diagnosis methods, which are often cumbersome and subjective, advocating for the use of eye tracking as a more objective alternative.

**Method:** The authors conduct a systematisation of current knowledge and guidelines regarding gaze-based studies, aiming to standardize protocols for their application in neural diagnostics.

**Key Contributions:**

	1. Systematisation of existing knowledge on gaze-based neural diagnosis.
	2. Proposed guidelines for standardizing gaze-based study protocols.
	3. Identification of key findings that can be helpful in advancing the use of eye tracking for neural diagnostics.

**Result:** The study identifies key findings from previous gaze-based research and proposes standardized guidelines to enhance their usage in preliminary neural diagnosis.

**Limitations:** 

**Conclusion:** The authors conclude that with further research and standardization, gaze-based methods can serve as valuable tools for the preliminary diagnosis of neural disorders.

**Abstract:** Neural disorders refer to any condition affecting the nervous system and that influence how individuals perceive and interact with the world. Traditional neural diagnoses rely on cumbersome, time-consuming, or subjective methods, such as clinical interviews, behavioural observations, or medical imaging. Eye tracking is an attractive alternative because analysing eye movements, such as fixations and saccades, can provide more objective insights into brain function and cognitive processing by capturing non-verbal and unconscious responses. Despite its potential, existing gaze-based studies presented seemingly contradictory findings. They are dispersed across diverse fields, requiring further research to standardise protocols and expand their application, particularly as a preliminary indicator of neural processes for differential diagnosis. Therefore, this paper outlines the main agreed-upon findings and provides a systematisation of knowledge and key guidelines towards advancing gaze-based neural preliminary diagnosis.

</details>


### [7] [Exploring the Convergence of HCI and Evolving Technologies in Information Systems](https://arxiv.org/abs/2506.08549)

*Rajan Das Gupta, Ashikur Rahman, Md Imrul Hasan Showmick, Md. Yeasin Rahat, Md. Jakir Hossen*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Interface Design, Agile Methodologies

**Relevance Score:** 8

**TL;DR:** This study reviews HCI interface design methods, revealing they often do not meet the needs of modern mobile and cloud-based technologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the pressing need for adaptable interfaces for diverse user groups in the context of modern technology-driven information systems.

**Method:** The study reviewed 50 recent papers on HCI interface design to assess their effectiveness in current technological contexts.

**Key Contributions:**

	1. Review of 50 recent papers on HCI interface design
	2. Identification of gaps in current HCI methods for mobile and cloud technologies
	3. Proposed integration of agile methodologies with human-centered principles

**Result:** Findings indicate that most HCI design methods are outdated, primarily based on old desktop models, and insufficient for mobile and location-based services.

**Limitations:** The dependence on older desktop models in current design guidelines limits their applicability in modern contexts.

**Conclusion:** The study advocates for integrating agile methodologies with human-centered design principles to enhance interface design in line with emerging technologies.

**Abstract:** Modern technology driven information systems are part of our daily lives. However, this deep integration poses new challenges to the human computer interaction (HCI) professionals. With the rapid growth of mobile and cloud computing and the Internet of Things (IoT), the demand for HCI specialists to design user-friendly and adaptable interfaces has never been more pressing. Especially for diverse user groups such as children, the elderly and people with disabilities who need interfaces tailored to their needs regardless of time and location. This study reviewed 50 recent papers on HCI interface design for modern information systems. The goal is to see how well these methods address the demands of current technology. The findings show that most HCI design methods are still based on old desktop models and do not support mobile users and location-based services well. Most existing interface design guidelines do not align with the flexibility and dynamism of emerging technologies. The goal of this study is to improve interface design by combining agile methodologies with human-centered design principles. Future studies should also incorporate both qualitative and quantitative approaches, particularly in the context of cloud-based technologies and organizational information systems. This approach aims to bridge the gap between current interface design practices and the changing technological landscape.

</details>


### [8] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)

*Alvaro Becerra, Daniel Andres, Pablo Villegas, Roberto Daza, Ruth Cobos*

**Main category:** cs.HC

**Keywords:** MOSAIC-F, multimodal feedback, student learning, personalized feedback, AI

**Relevance Score:** 7

**TL;DR:** MOSAIC-F is a multimodal feedback framework that combines human assessments and AI to provide personalized feedback on student learning activities, tested for oral presentation skills.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve personalized feedback mechanisms in student learning by integrating various multimodal data and AI analytics.

**Method:** The framework involves peer and professor assessments, collection of multimodal data during activities, AI generation of personalized feedback, and student self-assessment.

**Key Contributions:**

	1. Development of a multimodal feedback framework
	2. Integration of various data sources and AI for feedback generation
	3. Application in oral presentation skill improvement

**Result:** MOSAIC-F generates accurate and actionable feedback by synthesizing qualitative evaluations with multimodal insights such as posture and cognitive load.

**Limitations:** 

**Conclusion:** The integration of AI with traditional assessment techniques enhances the personalization and effectiveness of feedback provided to students.

**Abstract:** In this article, we present a novel multimodal feedback framework called MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI), and Collaborative assessments for generating personalized feedback on student learning activities. This framework consists of four key steps. First, peers and professors' assessments are conducted through standardized rubrics (that include both quantitative and qualitative evaluations). Second, multimodal data are collected during learning activities, including video recordings, audio capture, gaze tracking, physiological signals (heart rate, motion data), and behavioral interactions. Third, personalized feedback is generated using AI, synthesizing human-based evaluations and data-based multimodal insights such as posture, speech patterns, stress levels, and cognitive load, among others. Finally, students review their own performance through video recordings and engage in self-assessment and feedback visualization, comparing their own evaluations with peers and professors' assessments, class averages, and AI-generated recommendations. By combining human-based and data-based evaluation techniques, this framework enables more accurate, personalized and actionable feedback. We tested MOSAIC-F in the context of improving oral presentation skills.

</details>


### [9] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)

*Hyeon Jeon, Jeongin Park, Sungbok Shin, Jinwook Seo*

**Main category:** cs.HC

**Keywords:** t-SNE, UMAP, visual analytics, dimensionality reduction, misuse

**Relevance Score:** 5

**TL;DR:** This paper investigates the misuse of t-SNE and UMAP in visual analytics, identifying the reasons behind it, and proposes ways to improve their appropriate use.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing misuse of t-SNE and UMAP in visual analytics, which often leads to misleading interpretations of data.

**Method:** A literature review of 114 papers was conducted to identify the prevalence of misuse, followed by an interview study to explore the motivations of practitioners.

**Key Contributions:**

	1. Comprehensive literature review on the misuse of t-SNE and UMAP.
	2. Insights from interviews with practitioners about their motivations for using these techniques.
	3. Proposed actions to prevent misuse and improve the understanding of dimensionality reduction methods.

**Result:** The study found that misuse of t-SNE and UMAP results from limited discourse on their proper use and highlights the need for better guidelines.

**Limitations:** The study is limited to t-SNE and UMAP, and the findings may not generalize to other dimensionality reduction techniques.

**Conclusion:** The paper concludes with recommendations for promoting reasonable use of dimensionality reduction techniques in visual analytics.

**Abstract:** Misuses of t-SNE and UMAP in visual analytics have become increasingly common. For example, although t-SNE and UMAP projections often do not faithfully reflect true distances between clusters, practitioners frequently use them to investigate inter-cluster relationships. In this paper, we bring this issue to the surface and comprehensively investigate why such misuse occurs and how to prevent it. We conduct a literature review of 114 papers to verify the prevalence of the misuse and analyze the reasonings behind it. We then execute an interview study to uncover practitioners' implicit motivations for using these techniques -- rationales often undisclosed in the literature. Our findings indicate that misuse of t-SNE and UMAP primarily stems from limited discourse on their appropriate use in visual analytics. We conclude by proposing future directions and concrete action items to promote more reasonable use of DR.

</details>


### [10] [Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration](https://arxiv.org/abs/2506.08805)

*Stina Klein, Pooja Prajod, Katharina Weitz, Matteo Lavit Nicora, Dimitra Tsovaltzi, Elisabeth Andr√©*

**Main category:** cs.HC

**Keywords:** collaborative robots, human-robot collaboration, avatars, worker engagement, industrial settings

**Relevance Score:** 8

**TL;DR:** Explores how avatars can enhance human-robot collaboration in industrial settings to improve worker engagement and well-being.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses concerns regarding worker well-being due to decreased social interactions when integrating collaborative robots (cobots).

**Method:** Conducted a focus group study with employees from a German manufacturing company utilizing cobots and included a scripted HRC demo.

**Key Contributions:**

	1. Identifies potential roles of avatars in HRC
	2. Highlights improvements needed in avatar behavior
	3. Discusses practical considerations for deploying avatars in industry

**Result:** Gained insights into avatar roles, improvements in behavior, and practical considerations for their deployment in industrial environments.

**Limitations:** Study's limitations restrict generalizability of findings.

**Conclusion:** Highlights the need for personalized communication and task assistance from avatars in HRC, acknowledging limitations in generalizability of the findings.

**Abstract:** The integration of collaborative robots (cobots) in industrial settings raises concerns about worker well-being, particularly due to reduced social interactions. Avatars - designed to facilitate worker interactions and engagement - are promising solutions to enhance the human-robot collaboration (HRC) experience. However, real-world perspectives on avatar-supported HRC remain unexplored. To address this gap, we conducted a focus group study with employees from a German manufacturing company that uses cobots. Before the discussion, participants engaged with a scripted, industry-like HRC demo in a lab setting. This qualitative approach provided valuable insights into the avatar's potential roles, improvements to its behavior, and practical considerations for deploying them in industrial workcells. Our findings also emphasize the importance of personalized communication and task assistance. Although our study's limitations restrict its generalizability, it serves as an initial step in recognizing the potential of adaptive, context-aware avatar interactions in real-world industrial environments.

</details>


### [11] [From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags](https://arxiv.org/abs/2506.08881)

*Nicolas Grelier, Johannes Pfau, Nicolas Mathieu, St√©phane Kaufmann*

**Main category:** cs.HC

**Keywords:** video games, trend analysis, data visualization, Steam tags, market trends

**Relevance Score:** 2

**TL;DR:** This paper analyzes the evolution of video game trends using data-driven methods, categorizing trends into fads, fashions, and classics, and providing visualizations and insights.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To support the understanding of video game trends amidst a fast-changing industry and to help stakeholders make informed decisions.

**Method:** Data-driven analysis of Steam tag evolutions, validated by industrial experts, leading to insights and visualizations of trend categories and durations.

**Key Contributions:**

	1. Categorization of video game trends into fads, fashions, and classics
	2. Establishment of the average trend duration in video games
	3. Provision of visual data interpretations for stakeholders

**Result:** Identified that trends can be categorized as short-lived fads, contemporary fashions, or stable classics, with an average surge duration of about four years.

**Limitations:** 

**Conclusion:** The research provides valuable insights and visualizations that enhance the understanding of shifting trends in the video game industry.

**Abstract:** The video game industry deals with a fast-paced, competitive and almost unpredictable market. Trends of genres, settings and modalities change on a perpetual basis, studios are often one big hit or miss away from surviving or perishing, and hitting the pulse of the time has become one of the greatest challenges for industrials, investors and other stakeholders. In this work, we aim to support the understanding of video game trends over time based on data-driven analysis, visualization and interpretation of Steam tag evolutions. We confirm underlying groundwork that trends can be categorized in short-lived fads, contemporary fashions, or stable classics, and derived that the surge of a trend averages at about four years in the realm of video games. After using industrial experts to validate our findings, we deliver visualizations, insights and an open approach of deciphering shifts in video game trends.

</details>


### [12] [Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams](https://arxiv.org/abs/2506.08892)

*Tauhid Tanjim, Jonathan St. George, Kevin Ching, Hee Rin Lee, Angelique Taylor*

**Main category:** cs.HC

**Keywords:** human-robot interaction, multimodal communication, team collaboration, robotic assistance, medical training

**Relevance Score:** 9

**TL;DR:** This study investigates how multimodal robot communication affects workload and human perception in team settings, particularly in medical training scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the knowledge gap in how robots can communicate effectively with human teams in time-sensitive environments, enhancing collaboration and coordination.

**Method:** An experimental in-lab study was conducted to explore team collaboration using a robotic crash cart (RCC) that provided verbal and non-verbal cues in a medical training scenario.

**Key Contributions:**

	1. Investigating the impact of multimodal robot communication on team workload
	2. Demonstrating the effectiveness of verbal and visual cues in robotic assistance
	3. Providing insights for future HRI research in time-sensitive environments

**Result:** Findings indicate that verbal cues for object search tasks and visual cues for task reminders significantly reduce team workload and increase perceived ease of use and usefulness compared to a robot that gives no feedback.

**Limitations:** 

**Conclusion:** The study highlights the importance of multimodal interaction in human-robot teams and calls for more research on integrating collaborative robots in critical environments like hospitals and manufacturing.

**Abstract:** The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to investigate how multimodal robot communication in action teams affects workload and human perception of robots. We explore team collaboration in a medical training scenario where a robotic crash cart (RCC) provides verbal and non-verbal cues to help users remember to perform iterative tasks and search for supplies. Our findings show that verbal cues for object search tasks and visual cues for task reminders reduce team workload and increase perceived ease of use and perceived usefulness more effectively than a robot with no feedback. Our work contributes to multimodal interaction research in the HRI field, highlighting the need for more human-robot teaming research to understand best practices for integrating collaborative robots in time-sensitive environments such as in hospitals, search and rescue, and manufacturing applications.

</details>


### [13] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)

*Petar Jaku≈°, Hrvoje D≈æapo*

**Main category:** cs.HC

**Keywords:** keyword spotting, NXP MCXN947, Neural Processing Unit, CNN, Quantization Aware Training

**Relevance Score:** 6

**TL;DR:** This paper describes a real-time keyword spotting system for resource-constrained devices using a microcontroller with an integrated Neural Processing Unit, achieving high accuracy and efficiency.

**Read time:** 4 min

<details>
  <summary>Details</summary>

**Motivation:** To enable real-time voice interaction on low-power, resource-constrained devices, enhancing user experience in embedded systems.

**Method:** The system employs MFCC feature extraction combined with a CNN classifier, optimized through Quantization Aware Training to balance model size and accuracy.

**Key Contributions:**

	1. Development of a keyword spotting system for low-power devices
	2. Use of Quantization Aware Training to optimize model size
	3. Demonstration of significant speedup in inference time on NPU

**Result:** Achieved a 59x speedup in inference time on the NPU with a model size of 30.58 KB and an accuracy of 97.06%.

**Limitations:** 

**Conclusion:** The research illustrates the potential for efficient, low-power voice interfaces in embedded environments, paving the way for improved HCI solutions.

**Abstract:** This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimental results demonstrate a 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms.

</details>


### [14] [Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants](https://arxiv.org/abs/2503.16586)

*Yash Vekaria, Aurelio Loris Canino, Jonathan Levitsky, Alex Ciechonski, Patricia Callejo, Anna Maria Mandalari, Zubair Shafiq*

**Main category:** cs.HC

**Keywords:** Generative AI, browser assistants, data privacy, user profiling, personalization

**Relevance Score:** 8

**TL;DR:** This paper investigates the privacy implications of generative AI browser assistants, focusing on their data collection and profiling practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how generative AI browser assistants operate and address privacy concerns stemming from their data collection practices.

**Method:** The study employs network traffic analysis and a novel prompting framework to audit the tracking, profiling, and personalization practices of popular GenAI browser extensions.

**Key Contributions:**

	1. Analyzed the data collection practices of popular GenAI browser assistants
	2. Demonstrated the reliance on server-side APIs over local processing
	3. Highlighted the risks associated with user profiling and data sharing

**Result:** The investigation reveals that these browser assistants heavily rely on server-side APIs for data processing, collect extensive user data (including sensitive information), and use this data to create user profiles for personalization.

**Limitations:** The study focuses on only the top ten GenAI browser extensions, and the findings may not represent all browser assistants.

**Conclusion:** The findings indicate that generative AI browser assistants can collect and share personal and sensitive information with minimal safeguards, raising significant privacy concerns.

**Abstract:** Generative AI (GenAI) browser assistants integrate powerful capabilities of GenAI in web browsers to provide rich experiences such as question answering, content summarization, and agentic navigation. These assistants, available today as browser extensions, can not only track detailed browsing activity such as search and click data, but can also autonomously perform tasks such as filling forms, raising significant privacy concerns. It is crucial to understand the design and operation of GenAI browser extensions, including how they collect, store, process, and share user data. To this end, we study their ability to profile users and personalize their responses based on explicit or inferred demographic attributes and interests of users. We perform network traffic analysis and use a novel prompting framework to audit tracking, profiling, and personalization by the ten most popular GenAI browser assistant extensions. We find that instead of relying on local in-browser models, these assistants largely depend on server-side APIs, which can be auto-invoked without explicit user interaction. When invoked, they collect and share webpage content, often the full HTML DOM and sometimes even the user's form inputs, with their first-party servers. Some assistants also share identifiers and user prompts with third-party trackers such as Google Analytics. The collection and sharing continues even if a webpage contains sensitive information such as health or personal information such as name or SSN entered in a web form. We find that several GenAI browser assistants infer demographic attributes such as age, gender, income, and interests and use this profile--which carries across browsing contexts--to personalize responses. In summary, our work shows that GenAI browser assistants can and do collect personal and sensitive information for profiling and personalization with little to no safeguards.

</details>


### [15] [Self-driving technologies need the help of the public: A narrative review of the evidence](https://arxiv.org/abs/2505.23472)

*Jonathan Smith, Siddartha Khastgir*

**Main category:** cs.HC

**Keywords:** self-driving technology, public trust, engagement, safety, education

**Relevance Score:** 7

**TL;DR:** This paper reviews public trust and engagement issues related to self-driving technology, proposing a model for education to enhance acceptance and trust.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the public's perceptions and expectations regarding self-driving technology, which impact its acceptance and success.

**Method:** A narrative review approach is employed to analyze the existing evidence on trust, engagement, and acceptance of safety-critical technologies.

**Key Contributions:**

	1. Review of evidence on trust and safety in self-driving technology
	2. Proposal of a scalable education model for public engagement
	3. Insights into public expectations and expert misunderstandings regarding self-driving systems

**Result:** The findings indicate that a mismatch between public perception and expectations can lead to misuse or disuse of self-driving technologies. Engagement programs that align information with public interests can foster greater trust and active participation.

**Limitations:** The paper mainly focuses on the United Kingdom context and may not generalize to other regions.

**Conclusion:** A scalable model for self-driving technology education is proposed to improve public engagement and trust.

**Abstract:** If public trust is lost in a new technology early in its life cycle it can take much more time for the benefits of that technology to be realised. Eventually tens-of-millions of people will collectively have the power to determine self-driving technology success of failure driven by their perception of risk, data handling, safety, governance, accountability, benefits to their life and more. This paper reviews the evidence on safety critical technology covering trust, engagement, and acceptance. The paper takes a narrative review approach concluding with a scalable model for self-driving technology education and engagement. The paper find that if a mismatch between the publics perception and expectations about self driving systems emerge it can lead to misuse, disuse, or abuse of the system. Furthermore we find from the evidence that industrial experts often misunderstand what matters to the public, users, and stakeholders. However we find that engagement programmes that develop approaches to defining the right information at the right time, in the right format orientated around what matters to the public creates the potential for ever more sophisticated conversations, greater trust, and moving the public into a progressive more active role of critique and advocacy. This work has been undertaken as part of the Partners for Automated Vehicle Education (PAVE) United Kingdom programme.

</details>


### [16] [Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants](https://arxiv.org/abs/2503.16586)

*Yash Vekaria, Aurelio Loris Canino, Jonathan Levitsky, Alex Ciechonski, Patricia Callejo, Anna Maria Mandalari, Zubair Shafiq*

**Main category:** cs.HC

**Keywords:** Generative AI, browser assistants, user profiling, privacy concerns, network analysis

**Relevance Score:** 8

**TL;DR:** Study on privacy implications of Generative AI (GenAI) browser assistants that profile users with little safeguards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the design and operation of GenAI browser extensions and their impact on user privacy.

**Method:** Network traffic analysis and a novel prompting framework to audit popular GenAI browser assistants.

**Key Contributions:**

	1. Analysis of data collection methods in GenAI browser assistants
	2. Revelation of profiling based on demographic attributes
	3. Identification of privacy concerns related to sensitive information sharing

**Result:** GenAI assistants rely on server-side APIs for data collection, often sharing sensitive information without user consent and profiling users based on inferred attributes.

**Limitations:** 

**Conclusion:** GenAI browser assistants can collect and use personal information for profiling with inadequate privacy protections.

**Abstract:** Generative AI (GenAI) browser assistants integrate powerful capabilities of GenAI in web browsers to provide rich experiences such as question answering, content summarization, and agentic navigation. These assistants, available today as browser extensions, can not only track detailed browsing activity such as search and click data, but can also autonomously perform tasks such as filling forms, raising significant privacy concerns. It is crucial to understand the design and operation of GenAI browser extensions, including how they collect, store, process, and share user data. To this end, we study their ability to profile users and personalize their responses based on explicit or inferred demographic attributes and interests of users. We perform network traffic analysis and use a novel prompting framework to audit tracking, profiling, and personalization by the ten most popular GenAI browser assistant extensions. We find that instead of relying on local in-browser models, these assistants largely depend on server-side APIs, which can be auto-invoked without explicit user interaction. When invoked, they collect and share webpage content, often the full HTML DOM and sometimes even the user's form inputs, with their first-party servers. Some assistants also share identifiers and user prompts with third-party trackers such as Google Analytics. The collection and sharing continues even if a webpage contains sensitive information such as health or personal information such as name or SSN entered in a web form. We find that several GenAI browser assistants infer demographic attributes such as age, gender, income, and interests and use this profile--which carries across browsing contexts--to personalize responses. In summary, our work shows that GenAI browser assistants can and do collect personal and sensitive information for profiling and personalization with little to no safeguards.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)

*Toyin Aguda, Erik Wilson, Allan Anzagira, Simerjot Kaur, Charese Smiley*

**Main category:** cs.CL

**Keywords:** large language models, relation extraction, conservative bias, Hobson's choice, semantic similarity

**Relevance Score:** 8

**TL;DR:** This paper investigates the conservative bias of large language models in relation extraction tasks, highlighting significant information loss due to opting for No_Relation labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the conservative bias of LLMs in relation extraction and the associated trade-offs between information loss and incorrect relation assignments.

**Method:** The paper systematically evaluates LLM behavior across various prompts and datasets, introducing Hobson's choice to illustrate preference for safe labels. It uses SBERT and LLM prompts to assess semantic similarity.

**Key Contributions:**

	1. Introduces the concept of Hobson's choice in LLM outputs.
	2. Quantifies the frequency of conservative bias compared to hallucination in LLMs.
	3. Evaluates the semantic similarity of conservative outputs versus more creative labels.

**Result:** The findings indicate that conservative bias occurs twice as frequently as hallucination in LLM outputs, leading to considerable information loss.

**Limitations:** The study may not account for all types of prompts or datasets that could influence LLM behavior.

**Conclusion:** The analysis demonstrates the need for improved understanding and handling of LLM biases in relation extraction scenarios.

**Abstract:** Large language models (LLMs) exhibit pronounced conservative bias in relation extraction tasks, frequently defaulting to No_Relation label when an appropriate option is unavailable. While this behavior helps prevent incorrect relation assignments, our analysis reveals that it also leads to significant information loss when reasoning is not explicitly included in the output. We systematically evaluate this trade-off across multiple prompts, datasets, and relation types, introducing the concept of Hobson's choice to capture scenarios where models opt for safe but uninformative labels over hallucinated ones. Our findings suggest that conservative bias occurs twice as often as hallucination. To quantify this effect, we use SBERT and LLM prompts to capture the semantic similarity between conservative bias behaviors in constrained prompts and labels generated from semi-constrained and open-ended prompts.

</details>


### [18] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)

*Jacob Dineen, Aswin RRV, Qin Liu, Zhikun Xu, Xiao Ye, Ming Shen, Zhaonan Li, Shijie Lu, Chitta Baral, Muhao Chen, Ben Zhou*

**Main category:** cs.CL

**Keywords:** large language models, reward decomposition, alignment, interpretability, AI safety

**Relevance Score:** 8

**TL;DR:** QA-LIGN is a novel approach for aligning large language models with explicit principles that enhances interpretability and adaptability without sacrificing performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure safe and reliable AI systems, it is essential to align large language models with principles such as helpfulness, honesty, and harmlessness.

**Method:** QA-LIGN employs an automatic symbolic reward decomposition that formulates principle-specific evaluation questions, leading to separate reward components for each alignment principle.

**Key Contributions:**

	1. Introduction of QA-LIGN for principle-specific reward decomposition.
	2. Improvement in transparency and adaptability during model alignment.
	3. Compatibility as a drop-in replacement for existing reward models.

**Result:** Experiments show that QA-LIGN provides greater transparency and adaptability in alignment compared to traditional methods, achieving performance on par with or better than existing baselines.

**Limitations:** 

**Conclusion:** QA-LIGN represents an advancement toward more interpretable and controllable alignment of language models without compromising end-task performance.

**Abstract:** Alignment of large language models with explicit principles (such as helpfulness, honesty, and harmlessness) is crucial for ensuring safe and reliable AI systems. However, standard reward-based alignment methods typically collapse diverse feedback into a single scalar reward, entangling multiple objectives into one opaque training signal, which hinders interpretability. In this work, we introduce QA-LIGN, an automatic symbolic reward decomposition approach that preserves the structure of each constitutional principle within the reward mechanism. Instead of training a black-box reward model that outputs a monolithic score, QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, making it a drop-in reward model replacement. Experiments aligning an uncensored large language model with a set of constitutional principles demonstrate that QA-LIGN offers greater transparency and adaptability in the alignment process. At the same time, our approach achieves performance on par with or better than a DPO baseline. Overall, these results represent a step toward more interpretable and controllable alignment of language models, achieved without sacrificing end-task performance.

</details>


### [19] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)

*Zefang Liu, Yinzhu Quan*

**Main category:** cs.CL

**Keywords:** autonomous agents, economic tasks, benchmark, multimodal, large language models

**Relevance Score:** 4

**TL;DR:** EconWebArena is a benchmark for evaluating autonomous agents on complex economic tasks in web environments, emphasizing grounded data and multimodal understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate autonomous agents on realistic economic tasks using multimodal inputs and interactions with authoritative web data.

**Method:** The benchmark involves 360 tasks curated from 82 websites, prompting various LLMs to generate candidates followed by rigorous human curation for clarity and reliability.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for economic tasks on web platforms.
	2. Emphasis on the fidelity to authoritative data sources in economic reasoning.
	3. Evaluation of state-of-the-art multimodal LLMs on these tasks.

**Result:** Performance gaps were identified in various LLMs concerning grounding, navigation, and multimodal understanding, highlighting challenges in economic web intelligence.

**Limitations:** The benchmark does not cover all possible economic domains and may require further expansion for broader applicability.

**Conclusion:** EconWebArena serves as a critical testbed for assessing and improving economic web intelligence in LLMs.

**Abstract:** We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence.

</details>


### [20] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)

*Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov*

**Main category:** cs.CL

**Keywords:** hate speech detection, multilingual dataset, transformer models, social media, natural language processing

**Relevance Score:** 4

**TL;DR:** This paper presents a trilingual dataset for hate speech detection in English, Urdu, and Spanish and a robust framework leveraging attention layers and large language models for improved multilingual detection performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research and tools for hate speech detection in Urdu and improve overall multilingual hate speech detection.

**Method:** The paper introduces a trilingual dataset of 10,193 tweets and utilizes attention layers with transformer models for feature extraction, alongside traditional methods like TF-IDF for non-transformer models.

**Key Contributions:**

	1. Trilingual dataset for hate speech detection in English, Urdu, and Spanish.
	2. Integration of attention layers with transformer models for improved feature extraction.
	3. Benchmarking with state-of-the-art and traditional models, showcasing performance improvements.

**Result:** The framework achieves strong performance with macro F1 scores of 0.87 for English, 0.85 for Spanish, and 0.81 for Urdu using state-of-the-art models, showing significant improvements over SVM baselines.

**Limitations:** 

**Conclusion:** The study contributes a valuable dataset and establishes a framework that enhances multilingual hate speech detection, promoting safer digital environments.

**Abstract:** Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.

</details>


### [21] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)

*Lijing Zhu, Qizhen Lan, Qing Tian, Wenbo Sun, Li Yang, Lu Xia, Yixin Xie, Xi Xiao, Tiehang Duan, Cui Tao, Shuteng Niu*

**Main category:** cs.CL

**Keywords:** Continual Learning, Knowledge Graph, Machine Learning, Graph Embedding, Task-driven Methods

**Relevance Score:** 4

**TL;DR:** ETT-CKGE is a novel method for Continual Knowledge Graph Embedding that improves efficiency and scalability by using learnable tokens for task-relevant knowledge transfer, avoiding costly graph traversals.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for Continual Knowledge Graph Embedding face challenges with efficiency and scalability due to suboptimal knowledge preservation and expensive computations.

**Method:** ETT-CKGE employs task-driven tokens for knowledge transfer, enabling consistent guidance across snapshots and significantly reducing the need for complex graph traversal and node scoring.

**Key Contributions:**

	1. Introduction of learnable task-driven tokens for efficient knowledge transfer
	2. Reduction of computational overhead through simple matrix operations
	3. Consistent guidance across multiple snapshots for better performance

**Result:** ETT-CKGE demonstrates superior or competitive predictive performance across six benchmark datasets, while enhancing training efficiency and scalability.

**Limitations:** 

**Conclusion:** The proposed method offers a practical solution for applying continual learning in knowledge graph settings without heavy computational requirements.

**Abstract:** Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge while preserving past information. However, existing methods struggle with efficiency and scalability due to two key limitations: (1) suboptimal knowledge preservation between snapshots caused by manually designed node/relation importance scores that ignore graph dependencies relevant to the downstream task, and (2) computationally expensive graph traversal for node/relation importance calculation, leading to slow training and high memory overhead. To address these limitations, we introduce ETT-CKGE (Efficient, Task-driven, Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE method that leverages efficient task-driven tokens for efficient and effective knowledge transfer between snapshots. Our method introduces a set of learnable tokens that directly capture task-relevant signals, eliminating the need for explicit node scoring or traversal. These tokens serve as consistent and reusable guidance across snapshots, enabling efficient token-masked embedding alignment between snapshots. Importantly, knowledge transfer is achieved through simple matrix operations, significantly reducing training time and memory usage. Extensive experiments across six benchmark datasets demonstrate that ETT-CKGE consistently achieves superior or competitive predictive performance, while substantially improving training efficiency and scalability compared to state-of-the-art CKGE methods. The code is available at: https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [22] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)

*Gerardo Aleman Manzanarez, Nora de la Cruz Arana, Jorge Garcia Flores, Yobany Garcia Medina, Raul Monroy, Nathalie Pernelle*

**Main category:** cs.CL

**Keywords:** AI-generated microfictions, literary evaluation, GrAImes, aesthetic quality, assessment framework

**Relevance Score:** 5

**TL;DR:** The paper presents GrAImes, an evaluation protocol for assessing the literary merit of AI-generated microfictions, focusing on various aesthetic and thematic criteria.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously evaluate AI-generated narratives for literary merit, addressing the lack of frameworks for assessing aesthetic qualities.

**Method:** The paper develops an evaluation protocol grounded in literary theory, specifically designed to assess AI-generated microfiction.

**Key Contributions:**

	1. Introduction of GrAImes for literary evaluation of AI-generated works
	2. Focus on aesthetic qualities in narrative assessments
	3. Validation of the framework through expert feedback

**Result:** The validation of GrAImes was performed through assessments by literature experts and enthusiasts, demonstrating its applicability in evaluating literary qualities.

**Limitations:** 

**Conclusion:** GrAImes provides an objective framework for evaluating the literary value of automated microfictions, aiding researchers in the field.

**Abstract:** Automated story writing has been a subject of study for over 60 years. Large language models can generate narratively consistent and linguistically coherent short fiction texts. Despite these advancements, rigorous assessment of such outputs for literary merit - especially concerning aesthetic qualities - has received scant attention. In this paper, we address the challenge of evaluating AI-generated microfictions and argue that this task requires consideration of literary criteria across various aspects of the text, such as thematic coherence, textual clarity, interpretive depth, and aesthetic quality. To facilitate this, we present GrAImes: an evaluation protocol grounded in literary theory, specifically drawing from a literary perspective, to offer an objective framework for assessing AI-generated microfiction. Furthermore, we report the results of our validation of the evaluation protocol, as answered by both literature experts and literary enthusiasts. This protocol will serve as a foundation for evaluating automatically generated microfictions and assessing their literary value.

</details>


### [23] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)

*Li Weigang, Pedro Carvalho Brom*

**Main category:** cs.CL

**Keywords:** back-translation, large language models, multilingual terminology, semantic alignment, AI

**Relevance Score:** 8

**TL;DR:** Proposes LLM-BT, a back-translation framework using large language models for automated multilingual terminology standardization and verification.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing speed of English technical term growth necessitates a more efficient method of multilingual consistency due to limitations of expert-driven approaches.

**Method:** Employing back-translation from English to an intermediate language and back to English to validate term-level consistency, along with a multi-path verification workflow integrating both serial and parallel routes.

**Key Contributions:**

	1. Term-Level Consistency Validation
	2. Multi-Path Verification Workflow
	3. Back-Translation as Semantic Embedding

**Result:** Achieved over 90% exact or semantic matches in term consistency, with strong cross-lingual robustness indicated by BLEU scores above 0.45 and 100% accuracy for Portuguese.

**Limitations:** 

**Conclusion:** LLM-BT empowers human-AI collaboration in terminology governance and enhances multilingual standardization processes across various fields.

**Abstract:** The rapid growth of English technical terms challenges traditional expert-driven standardization, especially in fast-evolving fields like AI and quantum computing. Manual methods struggle to ensure multilingual consistency. We propose \textbf{LLM-BT}, a back-translation framework powered by large language models (LLMs) to automate terminology verification and standardization via cross-lingual semantic alignment. Our contributions are: \textbf{(1) Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate language $\rightarrow$ English back-translation, LLM-BT achieves high term consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize'' pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw $\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese $\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%). \textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as dynamic semantic embedding, revealing latent meaning trajectories. Unlike static embeddings, LLM-BT provides transparent path-based embeddings shaped by model evolution. LLM-BT transforms back-translation into an active engine for multilingual terminology standardization, enabling human--AI collaboration: machines ensure semantic fidelity, humans guide cultural interpretation. This infrastructure supports terminology governance across scientific and technological fields worldwide.

</details>


### [24] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)

*Chupei Wang, Jiaqiu Vince Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information Retrieval, Proactive Interference, Working Memory, Prompt Engineering

**Relevance Score:** 8

**TL;DR:** This paper investigates how intra-context interference affects information retrieval in Large Language Models (LLMs), highlighting a fundamental constraint linked to working memory capacity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the effects of intra-context interference on LLMs' retrieval capabilities, which are increasingly viewed as tied to generation rather than just lookup.

**Method:** The authors adapt the proactive interference paradigm from cognitive science to evaluate LLM retrieval performance by streaming semantically related updates and assessing the impact of interference on accuracy.

**Key Contributions:**

	1. Introduces the PI-LLM evaluation framework for studying interference in LLMs
	2. Demonstrates the log-linear decline of retrieval accuracy due to interference
	3. Highlights the limitations of current prompt engineering methods in mitigating interference.

**Result:** Retrieval accuracy in LLMs declines log-linearly as interference accumulates, suggesting significant challenges in disentangling interference when manipulating information.

**Limitations:** The study indicates that mitigation strategies such as prompt engineering are only partially effective in improving retrieval accuracy.

**Conclusion:** The paper concludes that LLMs face a working memory bottleneck that necessitates new strategies to enhance their ability to suppress irrelevant information during retrieval.

**Abstract:** Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.

</details>


### [25] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)

*Samra Zafar, Shaheer Minhas, Syed Ali Hassan Zaidi, Arfa Naeem, Zahra Ali*

**Main category:** cs.CL

**Keywords:** Large Language Models, Writing Process, Educational Technology, Feedback, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** The paper investigates how writing process data can enhance feedback from large language models for student writing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the relevance and personalization of feedback provided by large language models (LLMs) used in student writing.

**Method:** Developed a digital writing tool that captures keystroke logs and periodic snapshots of student essays, utilized by twenty students who wrote timed essays.

**Key Contributions:**

	1. Introduced a novel digital writing tool for capturing the writing process.
	2. Demonstrated that process-aware feedback is preferred by students.
	3. Found correlation between types of edits made and writing quality metrics.

**Result:** Students preferred LLM feedback that was informed by their writing process, and it correlated with better scores in coherence and elaboration when they made significant edits.

**Limitations:** Initial results from a small sample size and limited contexts for feedback evaluation.

**Conclusion:** Enhancing LLMs with awareness of the writing process can make feedback feel more meaningful and supportive for learners.

**Abstract:** Large language models(LLMs) like Gemini are becoming common tools for supporting student writing. But most of their feedback is based only on the final essay missing important context about how that text was written. In this paper, we explore whether using writing process data, collected through keystroke logging and periodic snapshots, can help LLMs give feedback that better reflects how learners think and revise while writing. We built a digital writing tool that captures both what students type and how their essays evolve over time. Twenty students used this tool to write timed essays, which were then evaluated in two ways: (i) LLM generated feedback using both the final essay and the full writing trace, and (ii) After the task, students completed surveys about how useful and relatable they found the feedback. Early results show that learners preferred the process-aware LLM feedback, finding it more in tune with their own thinking. We also found that certain types of edits, like adding new content or reorganizing paragraphs, aligned closely with higher scores in areas like coherence and elaboration. Our findings suggest that making LLMs more aware of the writing process can lead to feedback that feels more meaningful, personal, and supportive.

</details>


### [26] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)

*Yu-Ang Lee, Guan-Ting Yi, Mei-Yi Liu, Jui-Chao Lu, Guan-Bo Yang, Yun-Nung Chen*

**Main category:** cs.CL

**Keywords:** compound AI systems, optimization, natural language feedback, machine learning, AI workflows

**Relevance Score:** 8

**TL;DR:** The paper reviews recent advancements in optimizing compound AI systems, focusing on both traditional and novel techniques, especially natural language feedback methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in optimizing complex AI workflows as they integrate more components and become more sophisticated.

**Method:** Systematic review of existing optimization methods for compound AI systems, classified along key dimensions, including numerical and language-based techniques.

**Key Contributions:**

	1. Systematic classification of optimization methods for compound AI systems.
	2. Identification of the role of natural language feedback in optimizing these systems.
	3. Overview of open research challenges and future research directions.

**Result:** A formalized understanding of compound AI system optimization, identification of existing methods, and highlighting of open research challenges.

**Limitations:** 

**Conclusion:** There is a need for new approaches to optimization, particularly in non-differentiable systems, as the field continues to evolve.

**Abstract:** Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [27] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)

*Shashidhar Reddy Javaji, Yupeng Cao, Haohang Li, Yangyang Yu, Nikhil Muralidhar, Zining Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Claim-Evidence Extraction, Scientific Comprehension, Benchmarking

**Relevance Score:** 9

**TL;DR:** The study presents CLAIM-BENCH, a benchmark for evaluating LLMs' ability in scientific claim-evidence extraction and validation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate LLMs' understanding of complex research relationships, particularly in scientific claim-evidence processing.

**Method:** Systematic comparison of three claim-evidence extraction approaches across six diverse LLMs using over 300 claim-evidence pairs.

**Key Contributions:**

	1. Introduction of CLAIM-BENCH as a benchmark for LLM evaluation
	2. In-depth analysis of model-specific strengths and weaknesses
	3. Demonstration of improved performance through strategic prompting approaches

**Result:** Closed-source models like GPT-4 and Claude outperformed open-source models in precision and recall, revealing significant comprehension limitations in LLMs.

**Limitations:** Limited to claim-evidence extraction and validation tasks; increased computational costs with proposed approaches.

**Conclusion:** CLAIM-BENCH establishes a new standard for assessing LLMs' scientific reasoning capabilities, suggesting methods to enhance their performance in claim-evidence tasks.

**Abstract:** Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.

</details>


### [28] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)

*Wanjing Anya Ma, Michael Flor, Zuowei Wang*

**Main category:** cs.CL

**Keywords:** reading comprehension, inference making, GPT-4o, educational applications, question generation

**Relevance Score:** 7

**TL;DR:** This paper presents a taxonomy of inference types for reading comprehension and experiments using GPT-4o to generate diagnostic reading comprehension questions, analyzing their quality and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reading comprehension instruction and interventions for students by using AI-generated diagnostic questions.

**Method:** The study analyzes a diagnostic reading comprehension item bank, creates a taxonomy of inference types, and conducts experiments with GPT-4o for generating questions through few-shot prompting.

**Key Contributions:**

	1. Introduction of a taxonomy for inference types in reading comprehension
	2. Demonstration of GPT-4o's ability to generate high-quality diagnostic questions
	3. Evaluation framework for assessing the quality and accuracy of generated questions

**Result:** GPT-4o produced 93.8% good-quality questions for grades 3-12; however, only 42.6% matched the intended inference types accurately.

**Limitations:** Only 42.6% of generated questions accurately matched the targeted inference type, indicating room for improvement.

**Conclusion:** Combining AI-generated questions with human judgment holds potential for enhancing diagnostic reading comprehension assessments.

**Abstract:** Inference making is an essential but complex skill in reading comprehension (RC). Some inferences require resolving references across sentences, and some rely on using prior knowledge to fill in the detail that is not explicitly written in the text. Diagnostic RC questions can help educators provide more effective and targeted reading instruction and interventions for school-age students. We introduce a taxonomy of inference types for RC and use it to analyze the distribution of items within a diagnostic RC item bank. Next, we present experiments using GPT-4o to generate bridging-inference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. Generated items were evaluated on three aspects: overall item quality, appropriate inference type, and LLM reasoning, achieving high inter-rater agreements above 0.90. Our results show that GPT-4o produced 93.8% good-quality questions suitable for operational use in grade 3-12 contexts; however, only 42.6% of the generated questions accurately matched the targeted inference type. We conclude that combining automatic item generation with human judgment offers a promising path toward scalable, high-quality diagnostic RC assessments.

</details>


### [29] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)

*Matteo Cargnelutti, Catherine Brobston, John Hess, Jack Cushman, Kristi Mukk, Aristana Scourtas, Kyle Courtney, Greg Leppert, Amanda Watson, Martha Whitehead, Jonathan Zittrain*

**Main category:** cs.CL

**Keywords:** large language models, public domain books, dataset stewardship, historic texts, OCR extraction

**Relevance Score:** 7

**TL;DR:** The paper presents Institutional Books 1.0, a dataset of public domain books derived from Harvard Library's digitization efforts via the Google Books project. It emphasizes the importance of high-quality training data for large language models (LLMs) and discusses the dataset's accessibility and usability for both humans and machines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of high-quality publicly available datasets for training large language models and to promote sustainable practices in dataset stewardship.

**Method:** The authors extracted and processed historical texts from Harvard Library's collection of digitally scanned books, generating an extensively documented dataset that includes both OCR-extracted text and various metadata.

**Key Contributions:**

	1. Introduction of Institutional Books 1.0, a large public domain dataset for LLM training.
	2. Extensive documentation and processing of historical texts from Harvard Library.
	3. Highlighting the need for high-quality training data in LLM development.

**Result:** The project resulted in a dataset containing metadata and OCR-extracted text from 983,004 public domain volumes totaling approximately 242 billion tokens, significantly expanding accessible training data for LLMs.

**Limitations:** 

**Conclusion:** Making this historical collection readily accessible enhances opportunities for meaningful use in machine learning applications and aids the advancement of human-computer interaction.

**Abstract:** Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.

</details>


### [30] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)

*Flavio D'Intino, Hans-Peter Hutter*

**Main category:** cs.CL

**Keywords:** Swiss German, speech-to-text, low-resource languages, OpenAI Whisper, SRB-300

**Relevance Score:** 4

**TL;DR:** The paper introduces the SRB-300 dataset, a 300-hour annotated speech corpus for Swiss German, and demonstrates improved speech-to-text performance using OpenAI Whisper models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Swiss German is a low-resource language with many dialects and no standardized written form, complicating transcription efforts and STT model performance.

**Method:** The authors collected real-world audio recordings from 39 Swiss German radio and TV stations, then fine-tuned multiple OpenAI Whisper models on the new SRB-300 dataset.

**Key Contributions:**

	1. Introduction of the SRB-300 dataset
	2. Fine-tuning results showing notable STT improvements
	3. Focus on real-world spontaneous speech capture

**Result:** Fine-tuning resulted in significant improvements in word error rate (WER) by 19-33% and BLEU scores by 8-40%. The optimal model achieved a WER of 17.1% and a BLEU score of 74.8.

**Limitations:** 

**Conclusion:** This work advances the effectiveness of STT systems for Swiss German and other low-resource languages in realistic scenarios.

**Abstract:** Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.   This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.   We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts.

</details>


### [31] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)

*Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, Tianyi Zhou*

**Main category:** cs.CL

**Keywords:** reasoning, self-reflection, NoWait, multimodal, efficiency

**Relevance Score:** 6

**TL;DR:** NoWait is a method that disables explicit self-reflection in reasoning models to enhance efficiency without sacrificing utility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of excessive verbosity and redundancy in reasoning outputs that hinder efficiency in large reasoning models.

**Method:** The study proposes NoWait, which suppresses explicit self-reflection tokens like 'Wait' and 'Hmm' during inference, streamlining the reasoning process.

**Key Contributions:**

	1. Introduction of NoWait for efficient reasoning
	2. Demonstrated significant reduction in reasoning trajectory length
	3. Maintained model utility across diverse benchmarks

**Result:** NoWait reduces chain-of-thought trajectory length by 27%-51% in five R1-style model series while maintaining model utility across multiple benchmarks in textual, visual, and video reasoning tasks.

**Limitations:** 

**Conclusion:** NoWait presents an effective, plug-and-play solution for enhancing multimodal reasoning efficiency without compromising performance.

**Abstract:** Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.

</details>


### [32] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)

*Yuxuan Zhou, Xien Liu, Chenwei Yan, Chen Ning, Xiao Zhang, Boxun Li, Xiangling Fu, Shijin Wang, Guoping Hu, Yu Wang, Ji Wu*

**Main category:** cs.CL

**Keywords:** large language models, medical applications, cognitive levels, evaluation framework, AI in health

**Relevance Score:** 9

**TL;DR:** This study proposes a framework for evaluating large language models (LLMs) in the medical domain based on cognitive levels, revealing a decline in performance as cognitive complexity increases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of LLMs across different cognitive levels in the medical domain, enhancing their application in real-world scenarios.

**Method:** A multi-cognitive-level evaluation framework was developed, incorporating existing medical datasets and defining tasks for three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving.

**Key Contributions:**

	1. Development of a multi-cognitive-level evaluation framework for LLMs in medicine
	2. Systematic evaluation of six prominent LLM families
	3. Insights into performance trends across cognitive levels

**Result:** Performance evaluation of state-of-the-art general and medical LLMs shows a significant decline in performance with increasing cognitive complexity, indicating model size is more crucial at higher levels.

**Limitations:** The study focuses primarily on performance metrics and does not address broader implications of LLMs in diverse medical scenarios.

**Conclusion:** There is a necessity to improve LLMs' performance in medical applications at higher cognitive levels, suggesting directions for future research.

**Abstract:** Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.

</details>


### [33] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)

*Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu*

**Main category:** cs.CL

**Keywords:** text embeddings, implicit semantics, natural language processing, linguistic theory, modeling

**Relevance Score:** 7

**TL;DR:** The paper argues for a shift in text embedding research towards implicit semantics, emphasizing the need for deeper semantic understanding in NLP models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current text embedding models are largely focused on surface meaning and fail to capture the implicit semantics that are essential for understanding human language.

**Method:** The paper reviews existing text embedding models and highlights their limitations in capturing implicit semantics through a pilot study comparing performance on implicit tasks.

**Key Contributions:**

	1. Highlights the inadequacy of existing models in capturing implicit semantics
	2. Provides evidence from a pilot study demonstrating performance gaps
	3. Calls for a paradigm shift in text embedding research toward implicit semantic understanding

**Result:** The pilot study indicates that state-of-the-art embedding models only marginally outperform basic baselines on tasks involving implicit semantics, revealing significant gaps in their understanding.

**Limitations:** The findings are based on a pilot study, which may limit the generalizability of the results and recommendations.

**Conclusion:** The paper calls for the embedding research community to focus on more diverse training data, meaningful benchmarks, and a core objective of modeling implicit meaning.

**Abstract:** This position paper argues that the text embedding research community should move beyond surface meaning and embrace implicit semantics as a central modeling goal. Text embedding models have become foundational in modern NLP, powering a wide range of applications and drawing increasing research attention. Yet, much of this progress remains narrowly focused on surface-level semantics. In contrast, linguistic theory emphasizes that meaning is often implicit, shaped by pragmatics, speaker intent, and sociocultural context. Current embedding models are typically trained on data that lacks such depth and evaluated on benchmarks that reward the capture of surface meaning. As a result, they struggle with tasks requiring interpretive reasoning, speaker stance, or social meaning. Our pilot study highlights this gap, showing that even state-of-the-art models perform only marginally better than simplistic baselines on implicit semantics tasks. To address this, we call for a paradigm shift: embedding research should prioritize more diverse and linguistically grounded training data, design benchmarks that evaluate deeper semantic understanding, and explicitly frame implicit meaning as a core modeling objective, better aligning embeddings with real-world language complexity.

</details>


### [34] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)

*Li-Ming Zhan, Bo Liu, Zexin Lu, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu*

**Main category:** cs.CL

**Keywords:** large language models, inference-time steering, behavioral relevance, vector-quantized autoencoder, transformers

**Relevance Score:** 8

**TL;DR:** This paper presents a method for identifying behavior-relevant attention heads in large language models to improve inference-time steering accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance inference-time steering of LLMs by identifying internal modules responsible for specific behaviors, as current methods are ineffective.

**Method:** A causal-attribution framework is introduced that uses a vector-quantized autoencoder to analyze attention heads' activations, separating the latent space into behavior-relevant and behavior-irrelevant parts.

**Key Contributions:**

	1. Development of a causal-attribution framework for attention head analysis in LLMs.
	2. Introduction of a vector-quantized autoencoder for behavioral relevance assessment.
	3. Demonstration of improved performance on truthfulness-steering tasks across various LLMs.

**Result:** The approach improves accuracy in steering large language models, particularly in truthfulness tasks, and shows strong zero-shot generalization across different domains.

**Limitations:** The method's effectiveness may vary across different model architectures and behaviors evaluated.

**Conclusion:** The proposed method enables better selection and weighting of attention heads in LLMs for more reliable behavior steering during inference.

**Abstract:** Inference-time steering aims to alter the response characteristics of large language models (LLMs) without modifying their underlying parameters. A critical step in this process is the identification of internal modules within LLMs that are associated with the target behavior. However, current approaches to module selection often depend on superficial cues or ad-hoc heuristics, which can result in suboptimal or unintended outcomes. In this work, we propose a principled causal-attribution framework for identifying behavior-relevant attention heads in transformers. For each head, we train a vector-quantized autoencoder (VQ-AE) on its attention activations, partitioning the latent space into behavior-relevant and behavior-irrelevant subspaces, each quantized with a shared learnable codebook. We assess the behavioral relevance of each head by quantifying the separability of VQ-AE encodings for behavior-aligned versus behavior-violating responses using a binary classification metric. This yields a behavioral relevance score that reflects each head discriminative capacity with respect to the target behavior, guiding both selection and importance weighting. Experiments on seven LLMs from two model families and five behavioral steering datasets demonstrate that our method enables more accurate inference-time interventions, achieving superior performance on the truthfulness-steering task. Furthermore, the heads selected by our approach exhibit strong zero-shot generalization in cross-domain truthfulness-steering scenarios.

</details>


### [35] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)

*Jash Rajesh Parekh, Pengcheng Jiang, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Causal-Chain RAG, Retrieval-Augmented Generation, Large Language Models, causal inference, information retrieval

**Relevance Score:** 9

**TL;DR:** CC-RAG is a novel approach that enhances Retrieval-Augmented Generation by integrating causal structure into the inference process, leading to improved accuracy and interpretability in LLM outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with understanding cause and effect, especially in specialized domains that require deeper reasoning. Standard Retrieval-Augmented Generation lacks the capability to model true causal relationships.

**Method:** CC-RAG integrates zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline, constructing a Directed Acyclic Graph (DAG) of causal triples and employing forward/backward chaining for answer generation.

**Key Contributions:**

	1. Introduction of Causal-Chain RAG (CC-RAG) for structured multi-hop inference.
	2. Integration of causal dependency modeling in standard RAG pipelines.
	3. Demonstrated improved performance in real-world applications compared to existing methods.

**Result:** CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity across two domains: Bitcoin price fluctuations and Gaucher disease.

**Limitations:** 

**Conclusion:** Explicitly modeling causal structures allows LLMs to produce more accurate and interpretable results, particularly in domains where flat retrieval techniques are insufficient.

**Abstract:** Understanding cause and effect relationships remains a formidable challenge for Large Language Models (LLMs), particularly in specialized domains where reasoning requires more than surface-level correlations. Retrieval-Augmented Generation (RAG) improves factual accuracy, but standard RAG pipelines treat evidence as flat context, lacking the structure required to model true causal dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that integrates zero-shot triple extraction and theme-aware graph chaining into the RAG pipeline, enabling structured multi-hop inference. Given a domain specific corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation, effect> triples and uses forward/backward chaining to guide structured answer generation. Experiments on two real-world domains: Bitcoin price fluctuations and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot LLMs in chain similarity, information density, and lexical diversity. Both LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results demonstrate that explicitly modeling causal structure enables LLMs to generate more accurate and interpretable responses, especially in specialized domains where flat retrieval fails.

</details>


### [36] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)

*Zikai Xiao, Ziyang Wang, Wen Ma, Yan Zhang, Wei Shen, Yan Wang, Luqi Gong, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long Context, Positional Contrastive Decoding, Attention Mechanisms, Performance Degradation

**Relevance Score:** 9

**TL;DR:** Positional Contrastive Decoding (PCD) addresses long-context performance issues in Large Language Models without the need for training, significantly improving attention score degradation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with performance degradation in long contexts, and existing solutions are costly and underexplored.

**Method:** The paper proposes PCD, which contrasts logits from long-aware and local-aware attention mechanisms to enhance focus on long-context performance without retraining.

**Key Contributions:**

	1. Introduces Positional Contrastive Decoding (PCD) as a training-free method for long-context performance improvement.
	2. Identifies the Posterior Salience Attenuation (PSA) phenomenon linked to attention score degradation.
	3. Demonstrates state-of-the-art results on long-context benchmarks.

**Result:** PCD alleviates attention score degradation and achieves state-of-the-art results on long-context benchmarks.

**Limitations:** 

**Conclusion:** PCD offers a novel training-free method to improve LLM performance on long-context tasks, showcasing significant effectiveness.

**Abstract:** While Large Language Models (LLMs) support long contexts, they struggle with performance degradation within the context window. Current solutions incur prohibitive training costs, leaving statistical behaviors and cost-effective approaches underexplored. From the decoding perspective, we identify the Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio correlates with long-text performance degradation. Notably, despite the attenuation, gold tokens still occupy high-ranking positions in the decoding space. Motivated by it, we propose the training-free Positional Contrastive Decoding (PCD) that contrasts the logits derived from long-aware attention with those from designed local-aware attention, enabling the model to focus on the gains introduced by large-scale short-to-long training. Through the analysis of long-term decay simulation, we demonstrate that PCD effectively alleviates attention score degradation. Experimental results show that PCD achieves state-of-the-art performance on long-context benchmarks.

</details>


### [37] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)

*Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, draft models, approximate inference, KV cache dropping, memory efficiency

**Relevance Score:** 9

**TL;DR:** A novel framework using draft models to optimize inference in long-context LLMs by accurately predicting token and KV pair importance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve the efficiency of inference in long-context LLMs due to high computational and memory demands.

**Method:** Introduces SpecKV for assessing KV pair importance and SpecPC for identifying unimportant prompt tokens using draft models.

**Key Contributions:**

	1. First approach leveraging draft models for LLM inference acceleration
	2. Development of SpecKV and SpecPC
	3. Empirical validations showing superior performance over existing methods

**Result:** Experimental results indicate improved accuracy over existing methods, with benefits in memory usage, latency, and throughput.

**Limitations:** 

**Conclusion:** The proposed framework enhances inference acceleration in LLMs while maintaining performance metrics; represents a novel application of draft models.

**Abstract:** Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [38] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)

*Tao Zou, Xinghua Zhang, Haiyang Yu, Minzheng Wang, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Multi-Task Instruction Following, SegPO, AI Optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces EIFBENCH, a new benchmark for evaluating large language models in multi-task environments, alongside the SegPO algorithm for optimizing task execution.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for large language models focus on single tasks, failing to capture the complexity of real-world scenarios that require accurate multi-task execution.

**Method:** The paper presents the Extremely Complex Instruction Following Benchmark (EIFBENCH) designed to assess LLMs on multi-task scenarios with various constraints, alongside the Segment Policy Optimization (SegPO) algorithm.

**Key Contributions:**

	1. Introduction of the Extremely Complex Instruction Following Benchmark (EIFBENCH)
	2. Development of the Segment Policy Optimization (SegPO) algorithm
	3. Identification of performance discrepancies in existing LLMs with complex instructions

**Result:** Evaluations on EIFBENCH reveal significant performance gaps in existing LLMs when faced with complex instructions, highlighting the need for further optimization.

**Limitations:** 

**Conclusion:** Ongoing optimization of LLMs is essential to effectively handle the intricacies involved in multi-task workflows as presented in real-world applications.

**Abstract:** With the development and widespread application of large language models (LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands higher capabilities to address complex user needs, often requiring precise workflow execution which involves the accurate understanding of multiple tasks. However, existing benchmarks focusing on single-task environments with limited constraints lack the complexity required to fully reflect real-world scenarios. To bridge this gap, we present the Extremely Complex Instruction Following Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that enable comprehensive assessment across diverse task types concurrently, but also integrates a variety of constraints, replicating complex operational environments. Furthermore, we propose the Segment Policy Optimization (SegPO) algorithm to enhance the LLM's ability to accurately fulfill multi-task workflow. Evaluations on EIFBENCH have unveiled considerable performance discrepancies in existing LLMs when challenged with these extremely complex instructions. This finding underscores the necessity for ongoing optimization to navigate the intricate challenges posed by LLM applications.

</details>


### [39] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)

*Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani*

**Main category:** cs.CL

**Keywords:** large language models, low-resource languages, evaluation benchmark, multimodal tasks, speech and text

**Relevance Score:** 6

**TL;DR:** This paper introduces mSTEB, a benchmark for evaluating LLM performance across tasks and languages, addressing the gap in evaluations for low-resource languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a standardized evaluation benchmark for Low-Resource Languages in the context of Large Language Models (LLMs), as current evaluations are primarily focused on high-resource languages.

**Method:** The authors introduced mSTEB, a new benchmark that evaluates tasks including language identification, text classification, question answering, and translation across speech and text modalities, using leading LLMs for testing.

**Key Contributions:**

	1. Introduction of mSTEB benchmark for evaluating LLMs
	2. Comprehensive evaluation across multiple tasks and modalities
	3. Highlighting the performance gap for low-resource languages

**Result:** Evaluation of LLMs such as Gemini 2.0 Flash and GPT-4o reveals significant performance disparities between high-resource and low-resource languages, particularly affecting languages spoken in Africa and the Americas/Oceania.

**Limitations:** 

**Conclusion:** The findings highlight the necessity for more investment in LLM development to improve functionality for underrepresented, low-resource languages.

**Abstract:** Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage.

</details>


### [40] [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/abs/2411.15129)

*Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell*

**Main category:** cs.CL

**Keywords:** ChatGPT, language analysis, bullshit, natural language processing, empirical study

**Relevance Score:** 8

**TL;DR:** The paper analyzes the language characteristics of ChatGPT in comparison to academic publications and critiques of language by Orwell and Graeber, finding a statistically significant relationship to themes of 'bullshit.'

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the distinctive character of language generated by ChatGPT and its implications in the context of natural language use as critiqued by previous thinkers.

**Method:** An empirical study comparing the language features of 1,000 scientific publications with text generated by ChatGPT, alongside hypothesis testing of linguistic patterns and themes.

**Key Contributions:**

	1. Identifies linguistic features of ChatGPT related to academic language and critique of political speech.
	2. Establishes a statistical model linking AI language to historical critiques of language and social dysfunction.
	3. Expands understanding of how LLM language may reflect or distort human communication norms.

**Result:** The study shows that certain language features identified in ChatGPT correspond significantly with concepts of 'bullshit' as outlined by Frankfurt and reflected in the critiques of political and workplace language by Orwell and Graeber.

**Limitations:** The analysis is limited to specific texts and contexts; broader implications for varied language settings are yet to be explored.

**Conclusion:** Statistical analysis demonstrates a reliable connection between the language used by LLMs like ChatGPT and human-defined constructs of bullshit, prompting further inquiry into the implications of AI-generated language.

**Abstract:** What can we learn about language from studying how it is used by ChatGPT and other large language model (LLM)-based chatbots? In this paper, we analyse the distinctive character of language generated by ChatGPT, in relation to questions raised by natural language processing pioneer, and student of Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based chatbots produce "slop," or even "bullshit," in the sense of Frankfurt's popular monograph On Bullshit, we conduct an empirical study to contrast the language of 1,000 scientific publications with typical text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of political speech, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language.

</details>


### [41] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)

*Weiya Li, Junjie Chen, Bei Li, Boyang Liu, Zichen Wen, Nuanqiao Shan, Xiaoqian Liu, Anping Liu, Huajie Liu, Youyan Wang, Wujiuge Yin, Hu Song, Bing Huang, Zhiyuan Xia, Jialiang Chen, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** Machine Translation, Cognitive Processes, Multi-Agent Systems, Natural Language Processing, Large Language Models

**Relevance Score:** 8

**TL;DR:** TACTIC is a cognitively informed multi-agent framework that enhances machine translation by mimicking human cognitive strategies through collaborative agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing multi-agent translation frameworks that neglect cognitive insights from human translators, leading to suboptimal translation quality.

**Method:** TACTIC employs six distinct agents that simulate human cognitive processes in translation, including drafting, refinement, evaluation, scoring, context reasoning, and knowledge gathering.

**Key Contributions:**

	1. Cognitively informed multi-agent framework for translation
	2. Detailed simulation of human cognitive translation processes
	3. Demonstrated superior performance compared to existing models

**Result:** Experimental results demonstrate that TACTIC achieves state-of-the-art translation performance, consistently surpassing both GPT-4.1 and DeepSeek-R1 across various language pairs.

**Limitations:** 

**Conclusion:** TACTIC effectively leverages LLMs by incorporating cognitive strategies, leading to significant improvements in translation quality.

**Abstract:** Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC.

</details>


### [42] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)

*Ziyang Ma, Qingyue Yuan, Zhenglin Wang, Deyu Zhou*

**Main category:** cs.CL

**Keywords:** Meta-cognition, Large Language Models, Error detection, AutoMeco, MIRA

**Relevance Score:** 8

**TL;DR:** This paper examines the meta-cognitive abilities of Large Language Models (LLMs) and proposes a framework for evaluating them.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Previous research has largely overlooked the meta-cognitive capabilities of LLMs, focusing mostly on cognitive error detection. Understanding their self-awareness regarding errors is essential for improving their reliability.

**Method:** The paper introduces AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking existing evaluation methods. Additionally, the Markovian Intrinsic Reward Adjustment strategy (MIRA) is proposed to enhance the meta-cognition evaluation processes without requiring additional training.

**Key Contributions:**

	1. Introduction of AutoMeco framework for meta-cognition evaluation of LLMs
	2. Development of MIRA strategy to enhance existing evaluation lenses
	3. Empirical results validating the effectiveness of the proposed methods on various datasets

**Result:** Experimental results demonstrate the effectiveness of AutoMeco and reveal that LLMs' meta-cognition can be more accurately assessed using MIRA, particularly when benchmarked against Best-of-N verification from established methods.

**Limitations:** The study primarily focuses on mathematical reasoning datasets, which may not generalize to all types of reasoning tasks. Further exploration is needed in more diverse contexts.

**Conclusion:** The findings indicate that by improving evaluation techniques with AutoMeco and MIRA, the assessment of LLM meta-cognition can be significantly enhanced, ultimately contributing to the reliability of these models.

**Abstract:** Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA.

</details>


### [43] [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/abs/2502.15226)

*Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** User Experience, Language Models, Human-Computer Interaction, LLM Evaluation, CLUE

**Relevance Score:** 8

**TL;DR:** The paper presents CLUE, an LLM-powered interviewer that gathers user insights immediately after interactions with LLMs, showcasing user opinions through a large-scale study.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate current LLMs from a user perspective and gather real-time insights on user experiences.

**Method:** Conducted a study involving thousands of users who interacted with LLMs followed by interviews facilitated by CLUE to collect opinions.

**Key Contributions:**

	1. Introduction of an LLM-powered interview system (CLUE) for user experience evaluation.
	2. Large-scale study with real users to collect insights on LLMs.
	3. Findings on user opinions regarding reasoning processes and multi-modality in LLMs.

**Result:** Users expressed varied opinions on LLMs, highlighting concerns such as the reasoning process and the need for information freshness and multi-modality.

**Limitations:** 

**Conclusion:** CLUE effectively captures nuanced user opinions about LLMs, providing valuable insights into user experiences.

**Abstract:** Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer.

</details>


### [44] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)

*Jiaxiang Liu, Boxuan Xing, Chenhao Yuan, Chenxiang Zhang, Di Wu, Xiusheng Huang, Haida Yu, Chuhan Lang, Pengfei Cao, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** large language models, interpretability, knowledge mechanisms, open-source, machine learning

**Relevance Score:** 9

**TL;DR:** The paper introduces Know-MRI, an open-source tool for analyzing knowledge mechanisms in large language models (LLMs) that supports diverse input formats and interpretation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the interpretability of large language models and address the limitations of current interpretation methods that restrict their practical applications.

**Method:** Developed an extensible core module for Know-MRI that matches various input data with corresponding interpretation methods and consolidates the outputs.

**Key Contributions:**

	1. Introduction of the Know-MRI tool for LLM interpretation
	2. Extensible core module for matching inputs with interpretation methods
	3. Open-source availability for community use

**Result:** Know-MRI allows users to choose interpretation methods tailored to their input data, enhancing the comprehensiveness of model diagnosis across multiple perspectives.

**Limitations:** 

**Conclusion:** The tool facilitates a better understanding of LLMs' internal knowledge mechanisms and is readily accessible for users.

**Abstract:** As large language models (LLMs) continue to advance, there is a growing urgency to enhance the interpretability of their internal knowledge mechanisms. Consequently, many interpretation methods have emerged, aiming to unravel the knowledge mechanisms of LLMs from various perspectives. However, current interpretation methods differ in input data formats and interpreting outputs. The tools integrating these methods are only capable of supporting tasks with specific inputs, significantly constraining their practical applications. To address these challenges, we present an open-source Knowledge Mechanisms Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms within LLMs systematically. Specifically, we have developed an extensible core module that can automatically match different input data with interpretation methods and consolidate the interpreting outputs. It enables users to freely choose appropriate interpretation methods based on the inputs, making it easier to comprehensively diagnose the model's internal knowledge mechanisms from multiple perspectives. Our code is available at https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on https://youtu.be/NVWZABJ43Bs.

</details>


### [45] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)

*Ziqi. Liu, Ziyang. Zhou, Mingxuan. Hu*

**Main category:** cs.CL

**Keywords:** irony detection, large language models, multi-agent system, interpretability, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper presents CAF-I, a multi-agent LLM system for improved irony detection, addressing existing limitations and achieving state-of-the-art performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by existing LLM methods in irony detection, including single-perspective limitations and lack of interpretability.

**Method:** The Collaborative Agent Framework for Irony (CAF-I) uses specialized agents for different aspects‚ÄîContext, Semantics, and Rhetoric‚Äîto perform multidimensional analysis and collaborative optimization, culminating in a Decision Agent to consolidate insights.

**Key Contributions:**

	1. Introduction of the Collaborative Agent Framework for Irony (CAF-I)
	2. Use of specialized agents for multidimensional analysis
	3. Achievement of state-of-the-art performance with improved interpretability

**Result:** CAF-I achieves state-of-the-art performance on irony detection benchmarks, with an average Macro-F1 score of 76.31, representing a 4.98 improvement over the previous baseline.

**Limitations:** 

**Conclusion:** CAF-I effectively simulates human-like analysis for better accuracy and interpretability in irony detection.

**Abstract:** Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability.

</details>


### [46] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)

*Hern√°n Maina, Nicol√°s Wolovick, Luciana Benotti*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain Adaptation, Numerical Precision, Data Parallelization, Energy Efficiency

**Relevance Score:** 8

**TL;DR:** This paper evaluates the impact of numerical precision and data parallelization on training speed and model accuracy for Large Language Models, aiming to improve domain adaptation in low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and accessibility issues associated with training Large Language Models (LLMs) in diverse cultural and value contexts.

**Method:** The study examines different numerical precisions and data parallelization strategies during the training of LLMs, measuring their effects on training speed and model accuracy.

**Key Contributions:**

	1. Analysis of numerical precision and its impact on training performance
	2. Evaluation of data parallelization strategies for LLMs
	3. Recommendations for low-resource environments to facilitate domain adaptation

**Result:** The evaluation shows that adjusting numerical precisions and employing data parallelization can significantly enhance training speed while maintaining model accuracy, making domain adaptation more feasible in low-resource environments.

**Limitations:** 

**Conclusion:** The findings suggest that energy efficiency and model accessibility can be improved for LLM training, benefitting settings with constrained hardware and energy resources.

**Abstract:** Training Large Language Models (LLMs) is costly in terms of energy, hardware, and annotated data, often resulting in a positionality rooted in predominant cultures and values (Santy et al., 2023). Domain adaptation has emerged as a promising strategy to better align models with diverse cultural and value contexts (Hershcovich et al., 2022), but its computational cost remains a significant barrier, particularly for research groups lacking access to large-scale infrastructure. In this paper, we evaluate how the use of different numerical precisions and data parallelization strategies impacts both training speed (as a proxy to energy and hardware consumption) and model accuracy, with the goal of facilitating domain adaptation in low-resource environments. Our findings are relevant to any setting where energy efficiency, accessibility, or limited hardware availability are key concerns.

</details>


### [47] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)

*Jiujun He, Huazhen Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, pruning, machine learning, PCA, linear calibration

**Relevance Score:** 9

**TL;DR:** A pruning framework for LLMs called Olica is proposed, which eliminates the need for retraining by using PCA and linear calibration methods to maintain accuracy while compressing models efficiently.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing structured pruning methods for LLMs require extensive resources for retraining, making them costly and impractical.

**Method:** The Olica framework utilizes principal component analysis (PCA) on matrix products in multi-head attention layers to compress LLMs without retraining and introduces a linear calibration method for feed-forward network layers to correct errors.

**Key Contributions:**

	1. Introduction of the Olica framework for LLM pruning without retraining.
	2. Reduction of PCA complexity in multi-head attention.
	3. Development of a linear calibration method for feed-forward network error correction.

**Result:** Olica significantly reduces the complexity of PCA and mitigates error accumulation in pruned layers, leading to superior performance across multiple benchmarks without requiring retraining.

**Limitations:** 

**Conclusion:** Olica is efficient in terms of data usage, GPU memory, and running time, while achieving competitive performance.

**Abstract:** Most existing structured pruning methods for Large Language Models (LLMs) require substantial computational and data resources for retraining to reestablish the corrupted correlations, making them prohibitively expensive. To address this, we propose a pruning framework for LLMs called Orthogonal decomposition and Linear Calibration (Olica), which eliminates the need for retraining. A key observation is that the multi-head attention (MHA) layer depends on two types of matrix products. By treating these matrix products as unified entities and applying principal component analysis (PCA), we extract the most important information to compress LLMs without sacrificing accuracy or disrupting their original structure. Consequently, retraining becomes unnecessary. A fast decomposition method is devised, reducing the complexity of PCA by a factor of the square of the number of attention heads. Additionally, to mitigate error accumulation problem caused by pruning the feed-forward network (FFN) layer, we introduce a linear calibration method to reconstruct the residual errors of pruned layers using low-rank matrices. By leveraging singular value decomposition (SVD) on the solution of the least-squares problem, these matrices are obtained without requiring retraining. Extensive experiments show that the proposed Olica is efficient in terms of data usage, GPU memory, and running time, while delivering superior performance across multiple benchmarks.

</details>


### [48] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)

*Prarabdh Shukla, Wei Yin Chong, Yash Patel, Brennan Schaffner, Danish Pruthi, Arjun Bhagoji*

**Main category:** cs.CL

**Keywords:** Twitch, content moderation, AutoMod, auditing, hateful content

**Relevance Score:** 8

**TL;DR:** This paper audits Twitch's automated moderation tool, AutoMod, highlighting its ineffectiveness in flagging hateful content while blocking benign messages, showing significant context comprehension issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of Twitch's automated moderation tool, AutoMod, in the context of real-time engagement and content moderation challenges.

**Method:** Conducted an audit using test accounts, sending over 107,000 comments from 4 datasets via Twitch's APIs to evaluate AutoMod's performance against hateful content.

**Key Contributions:**

	1. Conducted a comprehensive audit of AutoMod's effectiveness.
	2. Revealed high rates of hateful content bypassing moderation.
	3. Demonstrated AutoMod's failure to understand context in content moderation.

**Result:** Found that up to 94% of hateful messages bypass moderation; AutoMod relies heavily on slurs, with context-sensitive phrases blocking 89.5% of benign examples.

**Limitations:** Focuses solely on Twitch's AutoMod and may not generalize to other platforms‚Äô moderation systems.

**Conclusion:** There are significant deficiencies in AutoMod's capabilities, particularly in understanding context, highlighting the need for improvements in automated moderation systems.

**Abstract:** To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch's automated moderation tool ($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch's APIs to send over $107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$. Contextual addition of slurs to these messages results in $100\%$ removal, revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$ blocks up to $89.5\%$ of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in $\texttt{AutoMod}$'s capabilities and underscores the importance for such systems to understand context effectively.

</details>


### [49] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)

*Fengjun Pan, Anh Tuan Luu, Xiaobao Wu*

**Main category:** cs.CL

**Keywords:** harmful meme detection, U-CoT+, framework, explainability, large language models

**Relevance Score:** 8

**TL;DR:** U-CoT+ is a novel framework for harmful meme detection that uses a meme-to-text pipeline for efficient and explainable classification with LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of resource efficiency, flexibility, and explainability in current meme detection approaches that hinder their deployment in content moderation.

**Method:** The framework features a high-fidelity meme-to-text pipeline that converts memes into textual descriptions, combined with human-crafted guidelines for zero-shot reasoning.

**Key Contributions:**

	1. Development of the high-fidelity meme-to-text pipeline
	2. Decoupling meme interpretation from classification
	3. Incorporation of human-crafted guidelines for effective reasoning

**Result:** Extensive experiments on seven benchmark datasets demonstrate the framework's effectiveness in harmful meme detection using small-scale LLMs with high explainability and low resource requirements.

**Limitations:** 

**Conclusion:** U-CoT+ allows for adaptation to various harmfulness detection criteria, improving the deployment of meme detection systems across different contexts.

**Abstract:** Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [50] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)

*Chihiro Taguchi, Seiji Maekawa, Nikita Bhutani*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Long-context language models, Open-domain question answering

**Relevance Score:** 9

**TL;DR:** The paper introduces Adaptive-$k$ retrieval, a method that adaptively selects the number of passages to retrieve based on similarity scores, improving efficiency and accuracy in open-domain question answering (QA).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of fixed retrieval sizes in open-domain QA, which can either waste tokens or miss key evidence.

**Method:** The Adaptive-$k$ retrieval method selects the number of passages based on the distribution of similarity scores between the query and candidate passages without requiring model fine-tuning or additional LLM inferences.

**Key Contributions:**

	1. Introduction of Adaptive-$k$ retrieval method
	2. Significant token efficiency compared to fixed-$k$
	3. Improved accuracy in QA tasks without requiring model retraining

**Result:** On factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens and still retrieving 70% of relevant passages.

**Limitations:** 

**Conclusion:** Dynamically adjusting the context size using Adaptive-$k$ improves QA accuracy across multiple long-context language models and embedding models.

**Abstract:** Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain question answering (QA). However, optimal external context to retrieve remains an open problem: fixing the retrieval size risks either wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA, where the optimal context size is both unknown and variable. We present Adaptive-$k$ retrieval, a simple and effective single-pass method that adaptively selects the number of passages based on the distribution of the similarity scores between the query and the candidate passages. It does not require model fine-tuning, extra LLM inferences or changes to existing retriever-reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens than full-context input, yet still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA.

</details>


### [51] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)

*Huixuan Zhang, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** text-to-image generation, evaluation frameworks, image-text alignment

**Relevance Score:** 6

**TL;DR:** This paper critiques existing evaluation methods for text-to-image models and proposes recommendations for improving them.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings in current evaluation frameworks for image-text alignment in text-to-image generation.

**Method:** The authors identify critical properties for reliable evaluation and assess mainstream frameworks against these properties using various metrics and models.

**Key Contributions:**

	1. Identification of key aspects for reliable evaluation frameworks
	2. Empirical assessment of current evaluation methods
	3. Recommendations for improving evaluation of image-text alignment

**Result:** It is demonstrated that existing evaluations fail to fully satisfy the proposed properties, highlighting the need for improvements.

**Limitations:** The paper does not propose new metrics but rather focuses on improving existing evaluation frameworks.

**Conclusion:** New recommendations for enhancing image-text alignment evaluations are provided based on empirical findings.

**Abstract:** Text-to-image models often struggle to generate images that precisely match textual prompts. Prior research has extensively studied the evaluation of image-text alignment in text-to-image generation. However, existing evaluations primarily focus on agreement with human assessments, neglecting other critical properties of a trustworthy evaluation framework. In this work, we first identify two key aspects that a reliable evaluation should address. We then empirically demonstrate that current mainstream evaluation frameworks fail to fully satisfy these properties across a diverse range of metrics and models. Finally, we propose recommendations for improving image-text alignment evaluation.

</details>


### [52] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)

*Sumanth Manduru, Carlotta Domeniconi*

**Main category:** cs.CL

**Keywords:** Small Language Models, ethical risks, fairness, utility, bias

**Relevance Score:** 8

**TL;DR:** The paper audits the ethical risks and performance of Small Language Models (SLMs) with 0.5 to 5 billion parameters, revealing critical insights on utility, fairness, and the impact of model architecture and quantization on bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the ethical risks associated with the rapid adoption of Small Language Models (SLMs) for resource-constrained deployments.

**Method:** A large-scale audit was conducted on nine instruction-tuned SLMs from various families, using the BBQ benchmark to evaluate utility and fairness through zero-shot prompting.

**Key Contributions:**

	1. First large-scale audit of instruction-tuned SLMs
	2. Identification of trade-offs in utility, fairness, and bias by architecture
	3. Practical guidance for deploying SLMs ethically

**Result:** The evaluation found that Phi models show high F1 scores with minimal bias, while Qwen 2.5 models exhibit vacuous neutrality, and LLaMA 3.2 shows significant stereotypical bias.

**Limitations:** Limited to analysis of specific model families and their architectures; does not cover all possible SLMs and configurations.

**Conclusion:** The study offers insights that advocate for the responsible deployment of SLMs, emphasizing the balance between efficiency and fairness, especially for small enterprises.

**Abstract:** The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments.

</details>


### [53] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)

*Ashutosh Dwivedi, Siddhant Shivdutt Singh, Ashutosh Modi*

**Main category:** cs.CL

**Keywords:** cultural sensitivity, LLMs, etiquettes, bias measurement, evaluation tasks

**Relevance Score:** 8

**TL;DR:** This paper presents EtiCor++, a corpus created to evaluate large language models (LLMs) on their understanding of cultural etiquettes and associated biases across various regions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of resources for evaluating LLMs regarding their comprehension and bias related to cultural etiquettes, which are crucial for accurate human-computer interactions.

**Method:** The paper introduces EtiCor++, a worldwide corpus of etiquettes, and outlines tasks and metrics for assessing LLMs' knowledge and biases in this domain.

**Key Contributions:**

	1. Introduction of EtiCor++, a new corpus for evaluating LLMs on cultural etiquettes.
	2. Development of tasks designed to assess etiquettes understanding in LLMs.
	3. Introduction of metrics to measure bias in LLMs regarding cultural backgrounds.

**Result:** Extensive experiments reveal a significant inherent bias in LLMs toward certain regions' etiquettes, indicating the need for improved sensitivity in AI models.

**Limitations:** The study is primarily resource-focused and does not propose comprehensive solutions for identified biases in LLMs.

**Conclusion:** The findings emphasize the importance of culturally sensitive AI and provide a foundation for future research and evaluation of LLM biases related to etiquettes.

**Abstract:** In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.

</details>


### [54] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)

*Xiao Wei, Xiaobao Wang, Ning Zhuang, Chenyang Wang, Longbiao Wang, Jianwu dang*

**Main category:** cs.CL

**Keywords:** intent detection, Generalized Intent Discovery, domain adaptation, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a consistency-driven prototype-prompting framework for Generalized Intent Discovery (GID) that effectively integrates old and new knowledge, improving intent detection from natural language inputs, especially for out-of-domain intents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance intent detection capability, particularly under conditions where supervised methods are hampered by a lack of labeled in-domain data and struggle with out-of-domain intents.

**Method:** The proposed approach utilizes a prototype-prompting framework to leverage existing knowledge from external sources and employs a hierarchical consistency constraint to learn new knowledge from target domains.

**Key Contributions:**

	1. Consistency-driven prototype-prompting framework for GID
	2. Integration of old knowledge through prototype prompting
	3. Hierarchical consistency constraints for new knowledge learning

**Result:** The experiments conducted show that the proposed method significantly outperforms existing baseline methods in intent detection, achieving state-of-the-art results.

**Limitations:** 

**Conclusion:** The findings of the study strongly validate the effectiveness and generalization of the proposed framework in GID, lay the groundwork for future improvements in intent detection methodologies.

**Abstract:** Intent detection aims to identify user intents from natural language inputs, where supervised methods rely heavily on labeled in-domain (IND) data and struggle with out-of-domain (OOD) intents, limiting their practical applicability. Generalized Intent Discovery (GID) addresses this by leveraging unlabeled OOD data to discover new intents without additional annotation. However, existing methods focus solely on clustering unsupervised data while neglecting domain adaptation. Therefore, we propose a consistency-driven prototype-prompting framework for GID from the perspective of integrating old and new knowledge, which includes a prototype-prompting framework for transferring old knowledge from external sources, and a hierarchical consistency constraint for learning new knowledge from target domains. We conducted extensive experiments and the results show that our method significantly outperforms all baseline methods, achieving state-of-the-art results, which strongly demonstrates the effectiveness and generalization of our methods. Our source code is publicly available at https://github.com/smileix/cpp.

</details>


### [55] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)

*Arie Cattan, Alon Jacovi, Ori Ram, Jonathan Herzig, Roee Aharoni, Sasha Goldshtein, Eran Ofek, Idan Szpektor, Avi Caciularu*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, knowledge conflict, large language models, benchmark, conflict resolution

**Relevance Score:** 9

**TL;DR:** This paper introduces a taxonomy of knowledge conflict types in Retrieval Augmented Generation (RAG) and presents a benchmark called CONFLICTS to evaluate how LLMs handle these conflicts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of conflicts in information retrieved by large language models (LLMs) and improve their response quality.

**Method:** The authors propose a novel taxonomy of knowledge conflict types specific to RAG and create CONFLICTS, a benchmark with expert annotations for evaluating model responses to these conflicts.

**Key Contributions:**

	1. Introduction of a novel taxonomy of knowledge conflict types in RAG.
	2. Development of the CONFLICTS benchmark for evaluating conflict resolution in LLMs.
	3. Analysis of LLM performance in resolving information conflicts and strategies for improvement.

**Result:** Experiments demonstrate that LLMs frequently fail to resolve conflicts effectively, although prompting them to consider potential conflicts can enhance response quality.

**Limitations:** The study highlights that while improvements in response quality were observed, LLMs still face substantial challenges in consistently resolving knowledge conflicts.

**Conclusion:** There is significant potential for future research to advance how LLMs manage conflicting information from different sources in RAG settings.

**Abstract:** Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.

</details>


### [56] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)

*Divyaksh Shukla, Ritesh Baviskar, Dwijesh Gohil, Aniket Tiwari, Atul Shree, Ashutosh Modi*

**Main category:** cs.CL

**Keywords:** discourse parsing, code-mixed corpus, multi-domain, human-computer interaction, NLU

**Relevance Score:** 8

**TL;DR:** This paper introduces CoMuMDR, a code-mixed multi-modal corpus for discourse parsing in conversations, highlighting challenges faced by state-of-the-art models.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Current discourse parsing datasets are limited to single-domain, written English dialogues, necessitating a new resource that encompasses multi-domain and code-mixed conversations.

**Method:** The paper presents the CoMuMDR corpus, which contains audio and transcribed text in Hindi and English, annotated with nine discourse relations, and evaluates various state-of-the-art baseline models on it.

**Key Contributions:**

	1. Introduction of CoMuMDR corpus for discourse parsing
	2. Inclusion of audio and transcribed text in multiple languages
	3. Highlighting limitations of existing state-of-the-art models in multi-domain settings.

**Result:** State-of-the-art models show poor performance on the CoMuMDR corpus, underscoring the challenges posed by its multi-domain and code-mixed nature.

**Limitations:** Existing SoTA models struggle with performance, indicating a gap in addressing the complexities of code-mixed discourse parsing.

**Conclusion:** There is a critical need for the development of better models tailored for discourse parsing in realistic, code-mixed, multi-domain settings.

**Abstract:** Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.

</details>


### [57] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)

*Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong, Kunpeng Liu, Yanjie Fu*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning, post-training, embedding refinement, contrastive learning

**Relevance Score:** 8

**TL;DR:** The paper proposes a framework for refining latent reasoning in Large Language Models (LLMs) during post-training, addressing challenges with updating reasoning embeddings effectively.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of Language Models beyond traditional Chain-of-Thought prompting, which is limited by token overhead and fixed reasoning paths.

**Method:** A lightweight post-training framework that incorporates contrastive reasoning feedback and residual embedding refinement to enhance reasoning embeddings and ensure stable convergence.

**Key Contributions:**

	1. Development of a post-training framework for LLMs
	2. Introduction of contrastive reasoning feedback
	3. Implementation of residual embedding refinement

**Result:** The proposed framework yields a 5% accuracy improvement on the MathQA benchmark without requiring additional training.

**Limitations:** The extent of improvement may vary across different tasks and reasoning benchmarks.

**Conclusion:** The results demonstrate that the framework successfully refines reasoning trajectories, leading to more accurate problem-solving in LLMs.

**Abstract:** Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.

</details>


### [58] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)

*Tuukka T√∂r√∂, Antti Suni, Juraj ≈†imko*

**Main category:** cs.CL

**Keywords:** machine learning, linguistic relationships, speech embeddings, language variation, sociolinguistics

**Relevance Score:** 3

**TL;DR:** This study utilizes machine learning methods, specifically speech embeddings, to analyze linguistic relationships among 106 languages, showing potential for scalable approaches in language variation studies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate linguistic relationships globally using diverse features influenced by sociolinguistic factors, leveraging recent machine learning advancements.

**Method:** Embeddings from the XLS-R self-supervised language model were used, employing linear discriminant analysis (LDA) to cluster and compare language embeddings with traditional linguistic measures.

**Key Contributions:**

	1. Introduction of speech embeddings for linguistic analysis.
	2. Alignment of ML-derived distances with traditional linguistic measures.
	3. Addressing methodological considerations for studying low-resource languages.

**Result:** The embedding-based distances align closely with traditional genealogical, lexical, and geographical measures, capturing typological patterns effectively.

**Limitations:** Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods.

**Conclusion:** The study opens new avenues for analyzing low-resource languages and integrating sociolinguistic variation, enhancing our understanding of linguistic diversity.

**Abstract:** Investigating linguistic relationships on a global scale requires analyzing diverse features such as syntax, phonology and prosody, which evolve at varying rates influenced by internal diversification, language contact, and sociolinguistic factors. Recent advances in machine learning (ML) offer complementary alternatives to traditional historical and typological approaches. Instead of relying on expert labor in analyzing specific linguistic features, these new methods enable the exploration of linguistic variation through embeddings derived directly from speech, opening new avenues for large-scale, data-driven analyses.   This study employs embeddings from the fine-tuned XLS-R self-supervised language identification model voxlingua107-xls-r-300m-wav2vec, to analyze relationships between 106 world languages based on speech recordings. Using linear discriminant analysis (LDA), language embeddings are clustered and compared with genealogical, lexical, and geographical distances. The results demonstrate that embedding-based distances align closely with traditional measures, effectively capturing both global and local typological patterns. Challenges in visualizing relationships, particularly with hierarchical clustering and network-based methods, highlight the dynamic nature of language change.   The findings show potential for scalable analyses of language variation based on speech embeddings, providing new perspectives on relationships among languages. By addressing methodological considerations such as corpus size and latent space dimensionality, this approach opens avenues for studying low-resource languages and bridging macro- and micro-level linguistic variation. Future work aims to extend these methods to underrepresented languages and integrate sociolinguistic variation for a more comprehensive understanding of linguistic diversity.

</details>


### [59] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)

*Yahan Li, Jifan Yao, John Bosco S. Bunyi, Adam C. Frank, Angel Hwang, Ruishan Liu*

**Main category:** cs.CL

**Keywords:** large language models, mental health, benchmarking, counseling, adversarial testing

**Relevance Score:** 9

**TL;DR:** CounselBench is a benchmark for evaluating large language models in mental health counseling, revealing both efficacy and safety concerns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of LLMs in real-world counseling scenarios and improve their safety in mental health applications.

**Method:** Development of CounselBench, comprising two components: CounselBench-EVAL for expert evaluations and CounselBench-Adv for adversarial questioning, involving real patient questions and expert ratings.

**Key Contributions:**

	1. Creation of a large-scale LLM benchmarking framework for mental health counseling
	2. Expert evaluation of LLM responses against human therapists
	3. Identification of model-specific failure patterns through adversarial testing

**Result:** LLMs often outperform online human therapists in perceived quality but are flagged for safety issues, highlighting a gap in model evaluation regarding unauthorized medical advice and oversight.

**Limitations:** The study indicates that LLM judges may overrate model responses and overlook safety issues identified by human experts.

**Conclusion:** CounselBench provides a structured approach to enhancing LLM safety and effectiveness in mental health contexts, identifying specific failure patterns.

**Abstract:** Large language models (LLMs) are increasingly proposed for use in mental health support, yet their behavior in realistic counseling scenarios remains largely untested. We introduce CounselBench, a large-scale benchmark developed with 100 mental health professionals to evaluate and stress-test LLMs in single-turn counseling. The first component, CounselBench-EVAL, contains 2,000 expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human therapists to real patient questions. Each response is rated along six clinically grounded dimensions, with written rationales and span-level annotations. We find that LLMs often outperform online human therapists in perceived quality, but experts frequently flag their outputs for safety concerns such as unauthorized medical advice. Follow-up experiments show that LLM judges consistently overrate model responses and overlook safety issues identified by human experts. To probe failure modes more directly, we construct CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling questions designed to trigger specific model issues. Evaluation across 2,880 responses from eight LLMs reveals consistent, model-specific failure patterns. Together, CounselBench establishes a clinically grounded framework for benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [60] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)

*Liyan Xu, Zhenlin Su, Mo Yu, Jiangnan Li, Fandong Meng, Jie Zhou*

**Main category:** cs.CL

**Keywords:** text encoders, fine-grained entities, dense retrieval

**Relevance Score:** 7

**TL;DR:** The paper addresses limitations in text encoders for fine-grained entity recognition, proposing a new dataset for evaluation and enhancing encoder performance through data generation strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the inability of text encoders to effectively recognize fine-grained entities/events, which hampers dense retrieval systems.

**Method:** Introduces the CapRetrieval dataset for evaluating encoder performance on fine-grained matching and proposes data generation strategies to finetune the encoders.

**Key Contributions:**

	1. Introduction of the CapRetrieval dataset for fine-grained evaluation
	2. Data generation strategies for finetuning text encoders
	3. Identification of the granularity dilemma in embeddings

**Result:** Finetuning encoders with the proposed strategies yields the best performance on the CapRetrieval dataset, revealing a granularity dilemma in embedding performance.

**Limitations:** 

**Conclusion:** The work highlights the challenges in fine-grained semantic alignment for embeddings and offers a publicly available dataset and models to further research in this area.

**Abstract:** This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [61] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)

*Shuzhou Yuan, Ercong Nie, Mario Tawfelis, Helmut Schmid, Hinrich Sch√ºtze, Michael F√§rber*

**Main category:** cs.CL

**Keywords:** Hate Speech Detection, Large Language Models, MBTI, Persona Prompts, Fairness

**Relevance Score:** 8

**TL;DR:** This paper investigates how personality traits, particularly MBTI dimensions, influence hate speech detection in LLMs and identifies significant persona-driven variation in model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of personality traits on LLM performance in hate speech detection, an area that has been largely unexplored despite the subjective nature of the task.

**Method:** The study uses MBTI-based persona prompts to evaluate outputs from four open-source LLMs across three hate speech datasets, supplemented by a human annotation survey to confirm the influence of personality traits.

**Key Contributions:**

	1. First comprehensive study on personality traits' impact on LLMs in hate speech detection
	2. Identification of significant persona-driven variation in model outputs
	3. Recommendations for improving LLM-based annotation workflows for fairness.

**Result:** The analysis shows that MBTI dimensions significantly affect labeling behavior and model outputs, revealing inconsistencies with ground truth and biases related to different personas.

**Limitations:** The study may not cover all personality dimensions beyond MBTI, and the findings are specific to the hate speech task.

**Conclusion:** The results underscore the importance of carefully defining persona prompts in LLM-based workflows to ensure fairness and alignment with human values in hate speech classification.

**Abstract:** Hate speech detection is a socially sensitive and inherently subjective task, with judgments often varying based on personal traits. While prior work has examined how socio-demographic factors influence annotation, the impact of personality traits on Large Language Models (LLMs) remains largely unexplored. In this paper, we present the first comprehensive study on the role of persona prompts in hate speech classification, focusing on MBTI-based traits. A human annotation survey confirms that MBTI dimensions significantly affect labeling behavior. Extending this to LLMs, we prompt four open-source models with MBTI personas and evaluate their outputs across three hate speech datasets. Our analysis uncovers substantial persona-driven variation, including inconsistencies with ground truth, inter-persona disagreement, and logit-level biases. These findings highlight the need to carefully define persona prompts in LLM-based annotation workflows, with implications for fairness and alignment with human values.

</details>


### [62] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)

*Minhae Oh, Jeonghye Kim, Nakyung Lee, Donggeon Seo, Taeuk Kim, Jungwoo Lee*

**Main category:** cs.CL

**Keywords:** scientific reasoning, retrieval-augmented framework, logical relevance

**Relevance Score:** 6

**TL;DR:** RAISE is a retrieval-augmented framework designed for effective scientific reasoning, enhancing the retrieval of domain-specific, logically relevant documents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of scientific reasoning which include handling long-chain reasoning processes and adapting to new findings.

**Method:** RAISE operates in three steps: problem decomposition, logical query generation, and logical retrieval.

**Key Contributions:**

	1. Introduction of a three-step framework for scientific reasoning
	2. Demonstration of improved performance on reasoning benchmarks
	3. Ability to retrieve logically relevant documents in addition to domain similarity

**Result:** RAISE consistently outperforms other baselines on scientific reasoning benchmarks by retrieving documents that are not only similar in domain knowledge but also logically relevant.

**Limitations:** 

**Conclusion:** RAISE improves the process of scientific reasoning by effectively integrating retrieval techniques that focus on logical relevance.

**Abstract:** Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant.

</details>


### [63] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)

*Son The Nguyen, Theja Tulabandhula*

**Main category:** cs.CL

**Keywords:** Large Language Models, Decoding Strategies, Optimization Framework, Metaheuristics, Human Preference Alignment

**Relevance Score:** 9

**TL;DR:** Introducing MEMETRON, a task-agnostic framework for optimizing LLM decoding using hybrid metaheuristic algorithms to improve response quality without retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Enhance control over LLM inference-time behavior, which is currently limited by heuristic decoding strategies that lack optimization for specific tasks.

**Method:** MEMETRON formulates LLM decoding as a discrete black-box optimization problem, utilizing hybrid algorithms (GENETRON and ANNETRON) informed by reward models and LLM context.

**Key Contributions:**

	1. Introduction of a task-agnostic optimization framework for LLM decoding
	2. Use of metaheuristic algorithms for improved response quality
	3. Evaluation demonstrates superior performance in human preference alignment over existing methods.

**Result:** The MEMETRON framework significantly outperforms standard decoding methods in human preference alignment tasks.

**Limitations:** 

**Conclusion:** MEMETRON offers a modular solution for improving LLM alignment without the need for model retraining, applicable across various tasks with minimal setup.

**Abstract:** Large language models (LLMs) are increasingly used for both open-ended and structured tasks, yet their inference-time behavior is still largely dictated by heuristic decoding strategies such as greedy search, sampling, or reranking. These methods provide limited control and do not explicitly optimize for task-specific objectives. We introduce MEMETRON, a task-agnostic framework that formulates LLM decoding as a discrete black-box optimization problem. MEMETRON leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the response space, guided by reward models and contextual operations performed by the LLM itself. This approach enables efficient discovery of high-reward responses without requiring model retraining or gradient access. The framework is modular and generalizes across diverse tasks, requiring only a reward function and lightweight prompt templates. We evaluate our framework on the critical human preference alignment task and demonstrate that it significantly outperforms standard decoding and reranking methods, highlighting its potential to improve alignment without model retraining.

</details>


### [64] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)

*Mingyu Zheng, Zhifan Feng, Jia Wang, Lanrui Wang, Zheng Lin, Yang Hao, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Data Synthesis, Table Instruction Tuning, LLM, Machine Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Introduction of TableDreamer, a data synthesis framework for improving table instruction tuning by addressing data diversity and utilization of LLM weaknesses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recent LLM-based data synthesis methods for table instruction tuning struggle with data diversity and inefficient data utilization due to existing weaknesses in LLMs.

**Method:** TableDreamer synthesizes diverse tables and instructions as seed data, then iteratively refines the input space using identified weaknesses to create final training data for LLM fine-tuning.

**Key Contributions:**

	1. Novel data synthesis framework for table instruction tuning
	2. Weakness-guided iterative exploration of training data
	3. Demonstrated significant accuracy improvements using smaller synthetic datasets

**Result:** The proposed framework improves the average accuracy of Llama3.1-8B-instruct by 11.62% on 10 tabular benchmarks, surpassing existing data synthesis methods that use larger datasets.

**Limitations:** 

**Conclusion:** TableDreamer effectively enhances table instruction tuning data generation, demonstrating superior performance in accuracy while utilizing a smaller training set.

**Abstract:** Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [65] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)

*Oumaima El Khettari, Solen Quiniou, Samuel Chaffron*

**Main category:** cs.CL

**Keywords:** relation extraction, large language models, intestinal microbiome, generative methods, biomedical domain

**Relevance Score:** 6

**TL;DR:** A generative relation extraction pipeline using LLMs for studying the intestinal microbiome shows promise, but BERT-based methods currently outperform it.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve relation extraction in the low-resource biomedical domain of the intestinal microbiome.

**Method:** The approach uses summarization with large language models (LLMs) to refine context before relation extraction via instruction-tuned generation.

**Key Contributions:**

	1. Introduction of a generative RE pipeline tailored for the intestinal microbiome
	2. Demonstration of LLM summarization improving RE performance
	3. Insights on the limitations of generative models in comparison to BERT-based approaches.

**Result:** Preliminary results indicate that summarization enhances generative relation extraction performance by reducing noise and guiding the model, yet BERT-based methods still outperform generative models.

**Limitations:** The BERT-based approaches still provide better performance than the generative models tested.

**Conclusion:** This work highlights the potential of generative methods for low-resource domains despite the current superiority of traditional models like BERT.

**Abstract:** We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting.

</details>


### [66] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)

*Yang Liu, Jiaqi Li, Zilong Zheng*

**Main category:** cs.CL

**Keywords:** Rule-based reasoning, Reinforcement learning, Dynamic sampling, Machine learning, Generalization

**Relevance Score:** 6

**TL;DR:** RuleReasoner improves rule-based reasoning in small reasoning models through dynamic sampling and reinforcement learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There are significant challenges in rule-based reasoning due to variations in rule formats and complexities in real-world applications. This paper investigates whether small reasoning models can effectively learn and generalize rule-based reasoning across various tasks and domains.

**Method:** Introducing RuleReasoner, a method that utilizes dynamic sampling based on historical rewards to optimize training for rule-based reasoning tasks, allowing for domain augmentation and flexible online learning schedules.

**Key Contributions:**

	1. Development of RuleReasoner for rule-based reasoning in small reasoning models
	2. Implementation of a domain-aware dynamic sampling approach
	3. Demonstration of superior performance on ID and OOD tasks compared to existing LRM methods

**Result:** Empirical evaluations show RuleReasoner outperforms large reasoning models on benchmark tasks, achieving improvements of 4.1% on in-distribution tasks and 10.4% on out-of-distribution tasks compared to existing methods.

**Limitations:** 

**Conclusion:** RuleReasoner demonstrates effective rule-based reasoning capabilities in small reasoning models with higher computational efficiency and robust generalization across diverse domains.

**Abstract:** Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1% average points on eight ID tasks and $\Delta$10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.

</details>


### [67] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)

*Soham Poddar, Paramita Koley, Janardan Misra, Sanjay Podder, Navveen Balani, Niloy Ganguly, Saptarshi Ghosh*

**Main category:** cs.CL

**Keywords:** Large Language Models, energy efficiency, output compression, prompt engineering, inference optimization

**Relevance Score:** 9

**TL;DR:** This paper addresses energy efficiency in LLMs by proposing prompt-engineering strategies to reduce output length, resulting in significant energy savings without sacrificing response quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The energy consumed by LLM inference processes is significant, making energy-efficient methods crucial; however, output compression techniques are underexplored.

**Method:** Benchmarking 12 decoder-only LLMs across 5 datasets to assess response quality and exploring prompt-engineering strategies for length reduction and information control.

**Key Contributions:**

	1. Benchmarking of LLMs for output length assessment
	2. Identification of information redundancy in LLM responses
	3. Proposed prompt-engineering strategies for effective length reduction

**Result:** The study finds that LLMs frequently produce unnecessarily long responses and demonstrates that prompts can reduce response lengths and achieve 25-60% energy optimization while maintaining response quality.

**Limitations:** Limited exploration of techniques beyond prompt engineering and potential trade-offs in certain scenarios.

**Conclusion:** By implementing specific prompt-engineering strategies, we can significantly reduce the energy consumption of LLMs during inference without compromising the quality of their outputs.

**Abstract:** A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\% by reducing the response length while preserving the quality of LLM responses.

</details>


### [68] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)

*Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert*

**Main category:** cs.CL

**Keywords:** scientific fact-checking, visualization, multimodal language models, knowledge graphs, ClimateViz

**Relevance Score:** 4

**TL;DR:** Introduction of ClimateViz, a benchmark for scientific fact-checking using visualizations and its evaluation of multimodal language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in scientific fact-checking that has largely overlooked charts, which are essential for conveying quantitative evidence.

**Method:** Evaluation of state-of-the-art multimodal language models on a dataset of claims linked to scientific charts, assessing their ability in zero-shot and few-shot settings.

**Key Contributions:**

	1. Introduction of the ClimateViz benchmark for scientific fact-checking with visualizations.
	2. Structured knowledge graph explanations accompanying visualizations to enhance interpretability.
	3. Release of a large-scale dataset for further research in chart-based reasoning.

**Result:** Models achieved only 76.2 to 77.8 percent accuracy in label-only settings, significantly lower than human performance, indicating challenges in chart-based reasoning.

**Limitations:** Current models show limited capability in chart-based reasoning and require further improvements to match human performance.

**Conclusion:** Current language models struggle with interpreting scientific charts, but explanation-augmented outputs show promise in improving performance.

**Abstract:** Scientific fact-checking has mostly focused on text and tables, overlooking scientific charts, which are key for presenting quantitative evidence and statistical reasoning. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking using expert-curated scientific charts. ClimateViz contains 49,862 claims linked to 2,896 visualizations, each labeled as support, refute, or not enough information. To improve interpretability, each example includes structured knowledge graph explanations covering trends, comparisons, and causal relations. We evaluate state-of-the-art multimodal language models, including both proprietary and open-source systems, in zero-shot and few-shot settings. Results show that current models struggle with chart-based reasoning: even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to 77.8 percent accuracy in label-only settings, far below human performance (89.3 and 92.7 percent). Explanation-augmented outputs improve performance in some models. We released our dataset and code alongside the paper.

</details>


### [69] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)

*Hee Suk Yoon, Eunseop Yoon, Mark A. Hasegawa-Johnson, Sungwoong Kim, Chang D. Yoo*

**Main category:** cs.CL

**Keywords:** Large Language Models, preference learning, alignment, direct preference optimization, KL divergence

**Relevance Score:** 8

**TL;DR:** ConfPO is a novel method for preference learning in LLMs that optimizes preference-critical tokens based on the training policy's confidence, improving alignment quality without auxiliary models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the alignment quality of LLMs while avoiding the pitfalls of overoptimization and relying on additional computational resources.

**Method:** ConfPO identifies and optimizes preference-critical tokens based on the confidence from the training policy, avoiding uniform adjustments across all tokens.

**Key Contributions:**

	1. Introduces a method that optimizes only preference-critical tokens based on policy confidence.
	2. Demonstrates superior performance compared to existing DAAs without additional computational burden.
	3. Addresses scalability and reliability issues found in token-level methods relying on external models.

**Result:** Experimental results indicate that ConfPO consistently outperforms previous uniform Direct Alignment Algorithms (DAAs) like DPO on various challenging benchmarks.

**Limitations:** 

**Conclusion:** ConfPO provides a more efficient and targeted method for preference learning in LLMs, offering better alignment with no additional computational costs.

**Abstract:** We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.

</details>


### [70] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)

*Fariz Ikhwantri, Dusica Marijan*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, compliance detection, assurance cases, multi-hop reasoning, large language models

**Relevance Score:** 6

**TL;DR:** Proposes a compliance detection approach, EXCLAIM, using NLI to automate regulatory compliance through multi-hop reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure complex systems meet regulations by effectively validating assurance cases within a claim-argument-evidence framework, overcoming challenges in legal and technical texts.

**Method:** Developed a Natural Language Inference (NLI) approach, EXCLAIM, that formulates the assurance case structure as a multi-hop inference for explainable compliance detection, generating examples using large language models.

**Key Contributions:**

	1. Introduced EXCLAIM for compliance detection via NLI.
	2. Used LLMs for generating assurance cases to address data limitations.
	3. Developed metrics for evaluating coverage and structural consistency of assurance cases.

**Result:** Demonstrated effectiveness of generated assurance cases focusing on GDPR requirements, utilizing metrics for coverage and structural consistency.

**Limitations:** Limited number of assurance cases and challenges in the underlying legal and technical text complexities still persist.

**Conclusion:** NLI-based methods have significant potential in automating the regulatory compliance process, improving efficiency and clarity in assurance case validation.

**Abstract:** Ensuring complex systems meet regulations typically requires checking the validity of assurance cases through a claim-argument-evidence framework. Some challenges in this process include the complicated nature of legal and technical texts, the need for model explanations, and limited access to assurance case data. We propose a compliance detection approach based on Natural Language Inference (NLI): EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the claim-argument-evidence structure of an assurance case as a multi-hop inference for explainable and traceable compliance detection. We address the limited number of assurance cases by generating them using large language models (LLMs). We introduce metrics that measure the coverage and structural consistency. We demonstrate the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task as a case study. Our results highlight the potential of NLI-based approaches in automating the regulatory compliance process.

</details>


### [71] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)

*Mehedi Hasan Bijoy, Dejan Porjazovski, Tam√°s Gr√≥sz, Mikko Kurimo*

**Main category:** cs.CL

**Keywords:** Speech Emotion Recognition, knowledge distillation, multilingual, human-computer interaction, Wav2Vec2.0

**Relevance Score:** 7

**TL;DR:** A novel method for multilingual Speech Emotion Recognition (SER) using knowledge distillation from multiple teacher models, achieving state-of-the-art results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multilingual Speech Emotion Recognition (SER) and enhance human-computer interaction.

**Method:** Introduce a language-aware multi-teacher knowledge distillation method that uses Wav2Vec2.0-based monolingual teacher models to train a single multilingual student model.

**Key Contributions:**

	1. Development of a language-aware multi-teacher knowledge distillation method for SER.
	2. Creation of a high-performing multilingual SER model using Wav2Vec2.0.
	3. Demonstration of improved performance across multiple languages in emotion recognition.

**Result:** The student model achieved a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing existing baselines.

**Limitations:** Challenges remain in accurately recognizing anger and happiness emotions in the multilingual context.

**Conclusion:** The proposed SER model significantly improves recall for sad and neutral emotions but still struggles with anger and happiness recognition.

**Abstract:** Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness.

</details>


### [72] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)

*Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe*

**Main category:** cs.CL

**Keywords:** large language models, numerical question answering, financial documents, critic agents, calculator agents

**Relevance Score:** 8

**TL;DR:** The paper explores the limitations of traditional critic agents for numerical question answering in financial documents when oracle labels are unavailable and introduces a new critic agent and a calculator agent that improve performance and safety.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by large language models in numerical question answering for financial documents involving tabular and textual data, particularly in the absence of oracle labels.

**Method:** The authors examined the performance of traditional critic agents and proposed improvements with a new critic agent and a calculator agent, supported by experimental evaluations.

**Key Contributions:**

	1. Introduction of an improved critic agent for numerical question answering without oracle labels
	2. Development of a calculator agent that outperforms existing methods
	3. Analysis of inter-agent interactions and their impact on performance

**Result:** Experiments demonstrated that the new critic and calculator agents significantly outperform the previous program-of-thought state-of-the-art approach, even in the absence of oracle labels.

**Limitations:** The paper primarily focuses on financial documents and may not generalize well to other domains.

**Conclusion:** The proposed agents effectively enhance performance in numerical questioning tasks and offer a safer alternative to existing methods by improving inter-agent interactions.

**Abstract:** Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.

</details>


### [73] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)

*Dror Kris Markus, Fabrizio Gilardi, Daria Stetsenko*

**Main category:** cs.CL

**Keywords:** AI research, societal concerns, interdisciplinary collaboration, ethical values, AI governance

**Relevance Score:** 8

**TL;DR:** This study analyzes AI-related papers to assess the integration of ethical values and societal concerns in research, finding a shift towards computer science-only teams contributing to societal outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how ethical values and societal concerns are incorporated into AI research amidst the intensifying calls for ethical AI development.

**Method:** Analyzed over 100,000 AI-related papers from ArXiv published between 2014 and 2024, using a classifier to identify societal content in the research.

**Key Contributions:**

	1. Development of a classifier for measuring societal concerns in AI papers
	2. Demonstration of a shift in production of societally-oriented AI research towards computer science-only teams
	3. Insights on the implications for AI safety and governance related to societal integration

**Result:** Interdisciplinary teams are still more likely to produce societally-oriented research, but computer science-only teams account for an increasing share, incorporating a variety of societal concerns.

**Limitations:** 

**Conclusion:** The findings question assumptions about the drivers of societal AI research and emphasize the need for interdisciplinary inputs in a field increasingly attuned to societal demands.

**Abstract:** As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is often championed as a key pathway for fostering such engagement. Yet it remains unclear whether interdisciplinary research teams are actually leading this shift in practice. This study analyzes over 100,000 AI-related papers published on ArXiv between 2014 and 2024 to examine how ethical values and societal concerns are integrated into technical AI research. We develop a classifier to identify societal content and measure the extent to which research papers express these considerations. We find a striking shift: while interdisciplinary teams remain more likely to produce societally-oriented research, computer science-only teams now account for a growing share of the field's overall societal output. These teams are increasingly integrating societal concerns into their papers and tackling a wide range of domains - from fairness and safety to healthcare and misinformation. These findings challenge common assumptions about the drivers of societal AI and raise important questions. First, what are the implications for emerging understandings of AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams? Second, for scholars in the social sciences and humanities: in a technical field increasingly responsive to societal demands, what distinctive perspectives can we still offer to help shape the future of AI?

</details>


### [74] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)

*Muhammad Anwar, Mishca de Costa, Issam Hammad, Daniel Lau*

**Main category:** cs.CL

**Keywords:** Large Language Model, nuclear applications, Transformer architecture, text generation, cybersecurity

**Relevance Score:** 0

**TL;DR:** The paper presents a domain-specific Large Language Model for nuclear applications, trained on the Essential CANDU textbook, demonstrating initial successes in text generation while highlighting the need for improvements in dataset richness and model fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a Large Language Model tailored for the nuclear domain, ensuring compliance with cybersecurity and data confidentiality standards.

**Method:** A compact Transformer-based architecture is utilized, trained on the Essential CANDU textbook on a single GPU, focusing on specialized nuclear vocabulary.

**Key Contributions:**

	1. Introduction of a domain-specific LLM for nuclear applications.
	2. Demonstration of compact Transformer-based architecture for sensitive domains.
	3. Identification of challenges and future directions for model improvement.

**Result:** The model shows initial promise in generating relevant text for nuclear applications, although some outputs lack syntactic coherence.

**Limitations:** The model's output sometimes lacks syntactic coherence and is based on a limited dataset, requiring richer corpora and better preprocessing.

**Conclusion:** While the model demonstrates feasibility for in-house LLM solutions in nuclear applications, it requires enhancements in dataset size, preprocessing, and instruction fine-tuning for better domain accuracy.

**Abstract:** This paper introduces a domain-specific Large Language Model for nuclear applications, built from the publicly accessible Essential CANDU textbook. Drawing on a compact Transformer-based architecture, the model is trained on a single GPU to protect the sensitive data inherent in nuclear operations. Despite relying on a relatively small dataset, it shows encouraging signs of capturing specialized nuclear vocabulary, though the generated text sometimes lacks syntactic coherence. By focusing exclusively on nuclear content, this approach demonstrates the feasibility of in-house LLM solutions that align with rigorous cybersecurity and data confidentiality standards. Early successes in text generation underscore the model's utility for specialized tasks, while also revealing the need for richer corpora, more sophisticated preprocessing, and instruction fine-tuning to enhance domain accuracy. Future directions include extending the dataset to cover diverse nuclear subtopics, refining tokenization to reduce noise, and systematically evaluating the model's readiness for real-world applications in nuclear domain.

</details>


### [75] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)

*Muhammad Anwar, Daniel Lau, Mishca de Costa, Issam Hammad*

**Main category:** cs.CL

**Keywords:** nuclear industry, synthetic data, large language models, unstructured text, information retrieval

**Relevance Score:** 4

**TL;DR:** Exploration of synthetic data generation to create structured Q&A pairs from unstructured text in the nuclear industry, enabling the use of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address data scarcity and privacy concerns in the nuclear industry by transforming unstructured text into usable Q&A pairs for LLM applications.

**Method:** The paper discusses leveraging LLMs to analyze text, extract key information, generate relevant questions, and evaluate synthetic dataset quality.

**Key Contributions:**

	1. Demonstrates the application of LLMs for synthetic data generation in a specialized domain.
	2. Addresses privacy and data scarcity challenges through innovative data transformation.
	3. Enhances decision-making processes in the nuclear industry by providing structured information.

**Result:** Synthetic data generation allows for the creation of structured question-answer pairs that enhance the performance and usability of LLMs in the nuclear sector.

**Limitations:** 

**Conclusion:** By unlocking the potential of LLMs through synthetic data, the nuclear industry can achieve better information retrieval and decision-making capabilities.

**Abstract:** The nuclear industry possesses a wealth of valuable information locked away in unstructured text data. This data, however, is not readily usable for advanced Large Language Model (LLM) applications that require clean, structured question-answer pairs for tasks like model training, fine-tuning, and evaluation. This paper explores how synthetic data generation can bridge this gap, enabling the development of robust LLMs for the nuclear domain. We discuss the challenges of data scarcity and privacy concerns inherent in the nuclear industry and how synthetic data provides a solution by transforming existing text data into usable Q&A pairs. This approach leverages LLMs to analyze text, extract key information, generate relevant questions, and evaluate the quality of the resulting synthetic dataset. By unlocking the potential of LLMs in the nuclear industry, synthetic data can pave the way for improved information retrieval, enhanced knowledge sharing, and more informed decision-making in this critical sector.

</details>


### [76] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)

*Pradyoth Hegde, Santosh Kesiraju, Jan ≈†vec, ≈†imon Sedl√°ƒçek, Bolaji Yusuf, Old≈ôich Plchot, Deepak K T, Jan ƒåernock√Ω*

**Main category:** cs.CL

**Keywords:** in-context learning, dialogue state tracking, LLMs, demonstration selection, MultiWoZ2.4

**Relevance Score:** 7

**TL;DR:** This study investigates the use of in-context learning for dialogue state tracking, examining factors influencing its effectiveness with LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of in-context learning in dialogue state tracking and understand factors affecting performance.

**Method:** A sentence embedding based k-nearest neighbour method is used to select suitable demonstrations for in-context learning, structured as input to LLMs along with test samples.

**Key Contributions:**

	1. Investigates in-context learning in dialogue state tracking
	2. Analyzes demonstration selection and prompt context impact
	3. Provides insights on LLM performance with DST.

**Result:** The study demonstrates insights on the performance of in-context learning abilities of LLMs in dialogue state tracking using various models on the MultiWoZ2.4 dataset.

**Limitations:** 

**Conclusion:** The findings highlight key factors influencing the effectiveness of in-context learning for dialogue state tracking in LLMs.

**Abstract:** This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [77] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)

*Mishca de Costa, Muhammad Anwar, Dave Mercier, Mark Randall, Issam Hammad*

**Main category:** cs.CL

**Keywords:** NL-to-SQL, large language models, nuclear power plants, data retrieval, operational safety

**Relevance Score:** 4

**TL;DR:** This work proposes a function-calling LLM approach for improving the reliability of NL-to-SQL queries in nuclear power plant data retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy and transparency of querying operational data from nuclear power plants due to the critical decisions supported by such data.

**Method:** Leveraging function-calling large language models instead of direct NL-to-SQL translations, creating a library of pre-approved functions to encapsulate validated SQL logic.

**Key Contributions:**

	1. Introduction of a function-calling approach to NL-to-SQL queries
	2. Validation of SQL logic through expert-reviewed functions
	3. Performance comparison showing improvements over direct NL-to-SQL methods

**Result:** Demonstrated improvements in accuracy and maintainability compared to traditional NL-to-SQL methods.

**Limitations:** Initial development and maintenance of the function library may require substantial upfront effort.

**Conclusion:** The proposed hybrid approach strikes a balance between user accessibility and operational safety for critical data retrieval processes.

**Abstract:** Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems.

</details>


### [78] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)

*Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Arabic Language, DeepSeek, Reasoning

**Relevance Score:** 7

**TL;DR:** This paper benchmarks reasoning-focused LLMs, particularly DeepSeek models, on fifteen Arabic NLP tasks, revealing significant improvements in classification and inference tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the performance of LLMs on Arabic data, which presents unique challenges such as rich morphology and diverse dialects, while focusing on their reasoning capabilities.

**Method:** The study conducts a benchmarking analysis of multiple reasoning-focused LLMs, including strategies like zero-shot, few-shot, and fine-tuning across fifteen Arabic NLP tasks.

**Key Contributions:**

	1. Introduces a comprehensive benchmark for Arabic NLP tasks using reasoning-focused LLMs.
	2. Demonstrates significant performance gains through in-context examples and fine-tuning strategies.
	3. Provides insights into the effectiveness of DeepSeek models compared to baseline LLMs.

**Result:** DeepSeek architectures show superior performance to a GPT o4-mini baseline, with significant F1 score improvements in various tasks, particularly in sentiment analysis and paraphrase detection.

**Limitations:** 

**Conclusion:** The findings suggest that targeted strategies can vastly improve the effectiveness of LLMs in Arabic NLP tasks, with fine-tuning proving especially beneficial.

**Abstract:** Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at https://anonymous.4open.science/r/AraReasoner41299

</details>


### [79] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)

*Francisco Vargas, Alejandro Gonz√°lez Coene, Gaston Escalante, Exequiel Lob√≥n, Manuel Pulido*

**Main category:** cs.CL

**Keywords:** traffic accidents, entity extraction, legal documents, large language models, fine-tuning

**Relevance Score:** 6

**TL;DR:** This paper proposes a two-step procedure for extracting traffic accident information from legal documents using segment vectorization and large language models (LLMs).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Extracting relevant entities from legal documents is crucial for quantifying insurance costs but presents significant challenges due to complex reasoning within court decisions.

**Method:** A two-step methodology involving document segmentation to identify relevant segments followed by entity extraction using classic regular expressions and LLMs (fine-tuned LLaMA models and GPT-4 Turbo).

**Key Contributions:**

	1. Introduction of a two-step procedure for legal document analysis.
	2. Comparative analysis of regular expressions vs. segment vectorization for entity extraction.
	3. Demonstration of superior performance of fine-tuned LLMs over traditional methods.

**Result:** LLaMA-2 70B with fine-tuning achieved the highest entity extraction accuracy of 79.4%, while GPT-4 Turbo outperformed all at 86.1%.

**Limitations:** Potential for hallucinations in LLM outputs and the need for extensive fine-tuning.

**Conclusion:** The proposed methodology significantly improves accuracy in entity extraction from legal documents compared to classic methods, with fine-tuning of LLMs reducing hallucinations.

**Abstract:** The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [80] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)

*Flavio D'Intino, Hans-Peter Hutter*

**Main category:** cs.CL

**Keywords:** Swiss German, speech-to-text, dataset, low-resource languages, OpenAI Whisper

**Relevance Score:** 4

**TL;DR:** The paper presents the SRB-300 dataset, a 300-hour annotated speech corpus for Swiss German, addressing limitations in current speech-to-text models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve speech-to-text models for Swiss German, a low-resource language with diverse dialects and no standardized written form.

**Method:** Introduced the SRB-300 dataset with real-world audio recordings and fine-tuned OpenAI Whisper models on this dataset.

**Key Contributions:**

	1. Introduced the SRB-300 dataset with long-audio recordings.
	2. Demonstrated improvements in STT model performance for spontaneous speech.
	3. Addressed the challenges posed by dialectal variation in Swiss German.

**Result:** Achieved significant improvements in word error rate (WER) and BLEU scores, indicating enhanced model performance in real-world speech recognition tasks.

**Limitations:** 

**Conclusion:** The SRB-300 dataset is pivotal for effective speech-to-text systems for Swiss German and can benefit other low-resource languages.

**Abstract:** Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.   This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.   We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts.

</details>


### [81] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)

*Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, Amitava Das*

**Main category:** cs.CL

**Keywords:** adversarial attacks, large language models, alignment, latent space, robustness

**Relevance Score:** 8

**TL;DR:** The paper exposes vulnerabilities in LLMs due to adversarial prompts that exploit latent camouflage, introducing the ALKALI benchmark and GRACE framework to enhance LLM adversarial robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Adversarial threats against LLMs are growing faster than existing defenses can keep up with, revealing critical weaknesses in alignment strategies.

**Method:** The paper introduces ALKALI, a benchmark with 9,000 adversarial prompts spanning various categories, and proposes GRACE, an alignment framework using latent space regularization to enhance defenses against adversarial attacks.

**Key Contributions:**

	1. Introduction of the ALKALI benchmark for adversarial prompts.
	2. Development of the GRACE framework for enhancing LLM robustness.
	3. Proposal of the AVQI metric for assessing latent alignment failure.

**Result:** The evaluation of 21 LLMs shows high Attack Success Rates (ASRs) due to latent camouflage; GRACE achieves up to 39% reduction in ASR, indicating its effectiveness in improving model robustness.

**Limitations:** The framework may require further validation across more diverse models and threat scenarios.

**Conclusion:** Implementing GRACE can enhance LLM safety through geometric representation awareness, providing a novel approach to address challenges posed by adversarial threats in NLP.

**Abstract:** Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [82] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)

*Hiba Khey, Amine Lakhder, Salma Rouichi, Imane El Ghabi, Kamal Hejjaoui, Younes En-nahli, Fahd Kalloubi, Moez Amri*

**Main category:** cs.CL

**Keywords:** Plant science, Natural language processing, Language model, Entity recognition, Domain adaptation

**Relevance Score:** 2

**TL;DR:** PlantBert is an open-source language model designed for extracting structured knowledge from plant stress-response literature, particularly focused on lentil (Lens culinaris).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for domain-adapted tools in plant science due to the shortcomings of current language models in handling relevant literature.

**Method:** PlantBert is built on the DeBERTa architecture, fine-tuned on a curated corpus of expert-annotated abstracts, integrating transformer-based modeling with rule-enhanced post-processing and ontology-grounded normalization.

**Key Contributions:**

	1. Introduction of PlantBert, a specialized model for plant science.
	2. Demonstration of effective domain adaptation in a low-resource field.
	3. Public release of the model to enhance transparency and innovation.

**Result:** PlantBert shows strong generalization capabilities across entity types and demonstrates effective domain adaptation in low-resource scientific areas.

**Limitations:** 

**Conclusion:** The model provides a scalable framework for high-resolution entity recognition and encourages cross-disciplinary collaboration in plant genomics.

**Abstract:** The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantBert, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantBert is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantBert to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantBert exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields. By providing a scalable and reproducible framework for high-resolution entity recognition, PlantBert bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.

</details>


### [83] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)

*Elias Horner, Cristinel Mateis, Guido Governatori, Agata Ciabattoni*

**Main category:** cs.CL

**Keywords:** legal text analysis, large language models, Defeasible Deontic Logic

**Relevance Score:** 5

**TL;DR:** A novel approach to automated semantic analysis of legal texts using LLMs for formal representation in Defeasible Deontic Logic.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the analysis and formalization of legal texts through the application of large language models.

**Method:** A structured pipeline that segments normative language into atomic snippets, extracts deontic rules, and evaluates them for coherence using multiple LLM configurations and prompt engineering techniques.

**Key Contributions:**

	1. Development of a structured pipeline for semantic analysis of legal texts
	2. Evaluation of various LLM configurations and prompt strategies
	3. Demonstration of alignment between machine-generated and expert-crafted representations

**Result:** Empirical results show promising alignment between machine-generated and expert-crafted formalizations, indicating effective contributions of LLMs in legal informatics.

**Limitations:** 

**Conclusion:** LLMs, especially with effective prompting, can enhance the scalability of legal informatics through improved analysis of legal norms.

**Abstract:** We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics.

</details>


### [84] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)

*Antonios Dimakis, John Pavlopoulos, Antonios Anastasopoulos*

**Main category:** cs.CL

**Keywords:** natural language understanding, dialect normalization, large language models, low-resource languages, targeted few-shot prompting

**Relevance Score:** 7

**TL;DR:** This paper presents a novel dialect-to-standard normalization method for low-resource languages using a combination of rule-based transformations and large language models (LLMs), applied on Greek dialects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by natural language understanding systems with low-resource languages and dialects, particularly in the context of enabling standard language tools to work effectively with dialectal text.

**Method:** The proposed normalization method integrates rule-based, linguistically-informed transformations alongside large language models (LLMs) employing targeted few-shot prompting, without the need for parallel data.

**Key Contributions:**

	1. Introduction of a novel normalization method for dialectal text
	2. Combination of rule-based transformations with LLMs
	3. Insights into semantic understanding beyond superficial linguistic features

**Result:** The outputs of the normalization method were evaluated using human annotators, revealing that prior studies on Greek proverbs relied on superficial linguistic features while enabling new semantic insights through the suggested methodology.

**Limitations:** The study is limited to Greek dialects and may not generalize to other low-resource languages without further adaptation.

**Conclusion:** The study demonstrates the effectiveness of combining rule-based techniques with LLMs for dialect normalization, providing deeper understanding of dialectal content.

**Abstract:** Natural language understanding systems struggle with low-resource languages, including many dialects of high-resource ones. Dialect-to-standard normalization attempts to tackle this issue by transforming dialectal text so that it can be used by standard-language tools downstream. In this study, we tackle this task by introducing a new normalization method that combines rule-based linguistically informed transformations and large language models (LLMs) with targeted few-shot prompting, without requiring any parallel data. We implement our method for Greek dialects and apply it on a dataset of regional proverbs, evaluating the outputs using human annotators. We then use this dataset to conduct downstream experiments, finding that previous results regarding these proverbs relied solely on superficial linguistic information, including orthographic artifacts, while new observations can still be made through the remaining semantics.

</details>


### [85] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)

*Zeyu Leo Liu, Greg Durrett, Eunsol Choi*

**Main category:** cs.CL

**Keywords:** knowledge propagation, large language models, hypernetwork, multi-hop questions, gradient modification

**Relevance Score:** 9

**TL;DR:** PropMEND is a hypernetwork-based method that enhances knowledge propagation in LLMs, enabling them to answer multi-hop questions using injected knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current knowledge editing techniques in LLMs allow for knowledge injection but fail to propagate that knowledge for reasoning, especially in multi-hop questions.

**Method:** PropMEND employs a hypernetwork to meta-learn modifications to gradients of language modeling losses, facilitating the propagation of injected knowledge.

**Key Contributions:**

	1. Introduction of a hypernetwork-based approach for knowledge propagation in LLMs
	2. Meta-learning gradient modifications to facilitate reasoning with injected knowledge
	3. Development of a new dataset, Controlled RippleEdit, for evaluating knowledge propagation

**Result:** The approach shows nearly 2x accuracy improvement on the RippleEdit dataset for multi-hop questions, demonstrating effective knowledge propagation.

**Limitations:** Performance gap for unseen entity-relation pairs suggests the need for improvement in knowledge propagation.

**Conclusion:** While PropMEND outperforms existing methods, the performance gap for unseen relations indicates that further work is needed in knowledge propagation.

**Abstract:** Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.

</details>


### [86] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)

*Andrew Shin*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, reinforcement learning, memory optimization, democratization of AI

**Relevance Score:** 8

**TL;DR:** This paper demonstrates how a single average gaming GPU can train a competitive mathematical reasoning model using reinforcement learning and memory optimization, challenging existing paradigms of high computational resource requirements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To show that it is possible to train effective mathematical reasoning models without the need for expensive, high-end hardware clusters, thereby democratizing access to AI research.

**Method:** The authors integrate reinforcement learning and memory optimization techniques to train a 1.5B parameter model on an RTX 3080 Ti GPU.

**Key Contributions:**

	1. Training a 1.5B parameter model on a single gaming GPU
	2. Integration of reinforcement learning and memory optimization techniques
	3. Achievement of competitive performance with reduced computational resources.

**Result:** The model achieves comparable or better performance on mathematical reasoning benchmarks than much larger models, demonstrating efficacy in resource-constrained environments.

**Limitations:** 

**Conclusion:** The findings suggest that high-performance AI research can be accessible without massive infrastructure investments.

**Abstract:** While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.

</details>


### [87] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)

*Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, Jinsong Su*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, knowledge conflict, faithfulness, self-thinking process

**Relevance Score:** 9

**TL;DR:** Proposes FaithfulRAG, a framework to resolve knowledge conflicts in large language models by modeling discrepancies between parametric knowledge and retrieved context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with unfaithfulness in outputs, especially when there are conflicts between retrieved context and parametric knowledge.

**Method:** FaithfulRAG identifies fact-level conflicts and employs a self-thinking process for LLMs to reason about and integrate these conflicting facts before generating responses.

**Key Contributions:**

	1. Introduction of FaithfulRAG framework for knowledge conflict resolution
	2. Identification of conflicting knowledge at the fact level
	3. Self-thinking process for reasoning about conflicting facts

**Result:** Extensive experiments show that FaithfulRAG outperforms existing state-of-the-art methods in maintaining faithfulness while leveraging both retrieved context and parametric knowledge.

**Limitations:** 

**Conclusion:** The proposed framework addresses the limitations of existing approaches by enabling LLMs to effectively manage conflicting information, enhancing response reliability.

**Abstract:** Large language models (LLMs) augmented with retrieval systems have demonstrated significant potential in handling knowledge-intensive tasks. However, these models often struggle with unfaithfulness issues, generating outputs that either ignore the retrieved context or inconsistently blend it with the LLM`s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model`s parametric knowledge. While existing faithful RAG approaches enforce strict context adherence through well-designed prompts or modified decoding strategies, our analysis reveals a critical limitation: they achieve faithfulness by forcibly suppressing the model`s parametric knowledge, which undermines the model`s internal knowledge structure and increases the risk of misinterpreting the context. To this end, this paper proposes FaithfulRAG, a novel framework that resolves knowledge conflicts by explicitly modeling discrepancies between the model`s parametric knowledge and retrieved context. Specifically, FaithfulRAG identifies conflicting knowledge at the fact level and designs a self-thinking process, allowing LLMs to reason about and integrate conflicting facts before generating responses. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. The code is available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [88] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)

*Clara Lachenmaier, Judith Sieker, Sina Zarrie√ü*

**Main category:** cs.CL

**Keywords:** large language models, conversational grounding, misinformation, political discourse, active grounding

**Relevance Score:** 9

**TL;DR:** The paper explores how large language models (LLMs) handle conversational grounding, particularly in the political domain, emphasizing challenges in correcting misinformation and engaging users effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs maintain common ground in conversations, especially concerning political misinformation and grounding failures.

**Method:** The authors evaluate LLMs' responses to direct and loaded questions related to political facts, assessing their ability to correct misinformation and discern their knowledge and biases.

**Key Contributions:**

	1. Analysis of LLM responses to loaded questions in political contexts
	2. Evaluation of LLMs' grounding abilities related to misinformation
	3. Insights into LLM biases and knowledge limitations in conversation

**Result:** Results indicated that LLMs struggle with grounding and often fail to correct false beliefs, highlighting their limitations in addressing misinformation in political discussions.

**Limitations:** The study is constrained by the specific political domain examined and the potential influence of model architecture on results.

**Conclusion:** The findings raise important questions about the effectiveness of LLMs in managing misinformation and engaging users in political discourse.

**Abstract:** Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.

</details>


### [89] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)

*Marek Kadlƒç√≠k, Michal ≈†tef√°nik, Timothee Mickus, Michal Spiegel, Josef Kucha≈ô*

**Main category:** cs.CL

**Keywords:** pretrained language models, numeric values, probing technique, arithmetic errors, embeddings

**Relevance Score:** 7

**TL;DR:** This paper introduces a new probing technique that accurately decodes numeric values from pretrained language models, addressing arithmetic errors linked to numeric embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and mitigate arithmetic errors in pretrained language models, which are mainly attributed to their unreliable numeric representations.

**Method:** A novel probing technique is proposed that decodes numeric values from input embeddings, demonstrating high accuracy across various open-source language models.

**Key Contributions:**

	1. Introduction of a novel probing technique for decoding numeric values from embeddings
	2. Demonstration of high accuracy in numeric representation by language models
	3. Insights into reducing arithmetic errors through embedding alignment

**Result:** The proposed probing method achieves near-perfect accuracy in decoding numeric values and reveals that alignment of numeric embeddings can reduce LM's arithmetic errors significantly.

**Limitations:** 

**Conclusion:** The accuracy of the embedding's representation of numeric values impacts the language models' arithmetic performance, suggesting implications for improving numeric accuracy in LMs.

**Abstract:** Pretrained language models (LMs) are prone to arithmetic errors. Existing work showed limited success in probing numeric values from models' representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities. However, we observe that previous probing methods are inadequate for the emergent structure of learned number embeddings with sinusoidal patterns.   In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs. This proves that after the sole pre-training, LMs represent numbers with remarkable precision. Finally, we find that the embeddings' preciseness judged by our probe's accuracy explains a large portion of LM's errors in elementary arithmetic, and show that aligning the embeddings with the pattern discovered by our probe can mitigate these errors.

</details>


### [90] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)

*Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, Zhuosheng Zhang*

**Main category:** cs.CL

**Keywords:** Autonomous agents, Mobile tasks, Large language models, Task execution, Benchmarking

**Relevance Score:** 8

**TL;DR:** This paper presents UI-NEXUS, a benchmark for evaluating autonomous agents on compositional tasks in mobile environments, and introduces AGENT-NEXUS, a scheduling system that enhances task execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in evaluating mobile agents on compositional tasks which are crucial for real-world applications.

**Method:** The UI-NEXUS benchmark evaluates agents on three categories of compositional tasks within 20 local app environments and 30 online apps, featuring 100 interactive task templates. AGENT-NEXUS dynamically decomposes long tasks into atomic subtasks for improved efficiency.

**Key Contributions:**

	1. Introduction of the UI-NEXUS benchmark for evaluating compositional task performance of mobile agents.
	2. Development of AGENT-NEXUS, an efficient scheduling system for breaking down long tasks into manageable subtasks.
	3. Demonstrated significant improvement in task success rates for existing agents when using the AGENT-NEXUS system.

**Result:** Experiments show that existing agents face challenges with performance and efficiency, often failing to execute tasks effectively, but AGENT-NEXUS improves success rates by 24% to 40% for compositional operations.

**Limitations:** The generalization of findings to all types of mobile agents may be limited, and further testing in diverse environments is warranted.

**Conclusion:** AGENT-NEXUS provides a practical solution to enhance the performance of mobile agents in compositional task execution while maintaining efficiency.

**Abstract:** Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks -- such as shot-chain execution tasks and single-screen grounding tasks -- while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at https://ui-nexus.github.io.

</details>


### [91] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)

*Satu Hopponen, Tomi Kinnunen, Alexandre Nikolaev, Rosa Gonz√°lez Hautam√§ki, Lauri Tavi, Einar Meister*

**Main category:** cs.CL

**Keywords:** Bilingual Corpus, Electromagnetic Articulography, Speaker Verification, Language Variability, Phonetic Analysis

**Relevance Score:** 4

**TL;DR:** Introduction of FROST-EMA corpus for bilingual speech research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To study language variability in bilingual speakers from phonetic and technological perspectives.

**Method:** Introduction of a new corpus consisting of recordings from bilingual speakers producing speech in L1, L2, and imitated L2, with two case studies to illustrate findings.

**Key Contributions:**

	1. Creation of a bilingual speech corpus (FROST-EMA)
	2. Demonstrated case studies on speaker verification and articulatory patterns
	3. Insights on language variability for HCI and ML applications.

**Result:** The first case study assesses how L2 and imitated L2 impact automatic speaker verification, while the second analyzes articulatory patterns across L1, L2, and fake accent.

**Limitations:** 

**Conclusion:** The FROST-EMA corpus provides valuable insights into speech variability and can enhance speaker verification systems.

**Abstract:** We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers, who produced speech in their native language (L1), second language (L2), and imitated L2 (fake foreign accent). The new corpus enables research into language variability from phonetic and technological points of view. Accordingly, we include two preliminary case studies to demonstrate both perspectives. The first case study explores the impact of L2 and imitated L2 on the performance of an automatic speaker verification system, while the second illustrates the articulatory patterns of one speaker in L1, L2, and a fake accent.

</details>


### [92] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)

*Yuejiao Wang, Xianmin Gong, Xixin Wu, Patrick Wong, Hoi-lam Helene Fung, Man Wai Mak, Helen Meng*

**Main category:** cs.CL

**Keywords:** neurocognitive disorder, fMRI, machine learning, cognitive decline, aging

**Relevance Score:** 9

**TL;DR:** This paper introduces a naturalistic language-related fMRI task that leverages machine learning to detect cognitive decline in aging populations, showing promising results with an average AUC of 0.86.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for early detection of neurocognitive disorders (NCD) among the aging population to enable timely intervention.

**Method:** A novel fMRI task was created, and its effectiveness was evaluated using machine learning classification models on data from 97 non-demented older adults.

**Key Contributions:**

	1. Development of a naturalistic fMRI task for cognitive assessment
	2. High classification accuracy for cognitive status using machine learning
	3. Identification of key brain regions involved in language processing related to cognitive decline

**Result:** The classification models achieved an average area under the curve of 0.86 in distinguishing between NORMAL and DECLINE cognitive statuses based on fMRI features and demographics.

**Limitations:** 

**Conclusion:** The study indicates the potential of the language-related fMRI task in early detection of cognitive decline and neurocognitive disorders in older adults.

**Abstract:** Early detection is crucial for timely intervention aimed at preventing and slowing the progression of neurocognitive disorder (NCD), a common and significant health problem among the aging population. Recent evidence has suggested that language-related functional magnetic resonance imaging (fMRI) may be a promising approach for detecting cognitive decline and early NCD. In this paper, we proposed a novel, naturalistic language-related fMRI task for this purpose. We examined the effectiveness of this task among 97 non-demented Chinese older adults from Hong Kong. The results showed that machine-learning classification models based on fMRI features extracted from the task and demographics (age, gender, and education year) achieved an average area under the curve of 0.86 when classifying participants' cognitive status (labeled as NORMAL vs DECLINE based on their scores on a standard neurcognitive test). Feature localization revealed that the fMRI features most frequently selected by the data-driven approach came primarily from brain regions associated with language processing, such as the superior temporal gyrus, middle temporal gyrus, and right cerebellum. The study demonstrated the potential of the naturalistic language-related fMRI task for early detection of aging-related cognitive decline and NCD.

</details>


### [93] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)

*Theo Zhang, Madurya Suresh, Anne S. Warlaumont, Kasia Hitczenko, Alejandrina Cristia, Margaret Cychosz*

**Main category:** cs.CL

**Keywords:** child speech, speech technology, transformer models, vocalization classification, SpeechMaturity

**Relevance Score:** 6

**TL;DR:** This paper presents a novel dataset, SpeechMaturity, addressing child vocalization classification using advanced transformer models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve speech technology systems' performance on child speech, which is hindered by limited training data and the unique challenges posed by children's vocalizations.

**Method:** The paper introduces the SpeechMaturity dataset comprising 242,004 labeled child vocalizations from diverse linguistic backgrounds, and applies transformer models to classify these vocalizations into categories such as cry, laughter, and speech maturity levels.

**Key Contributions:**

	1. Introduction of the SpeechMaturity dataset with 242,004 labeled vocalizations
	2. Demonstration of robust transformer model performance on child speech classification
	3. Establishment of human-comparable accuracy in distinguishing vocalization types.

**Result:** Models trained on the SpeechMaturity dataset significantly outperform previous state-of-the-art models, achieving human-comparable classification accuracy across various settings.

**Limitations:** 

**Conclusion:** The SpeechMaturity dataset offers a substantial improvement in classifying child vocalizations, paving the way for advancements in speech technology for children.

**Abstract:** Speech technology systems struggle with many downstream tasks for child speech due to small training corpora and the difficulties that child speech pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer models to address a fundamental classification task: identifying child vocalizations. Unlike previous corpora, our dataset captures maximally ecologically-valid child vocalizations across an unprecedented sample, comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu, Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004 labeled vocalizations, magnitudes larger than previous work. Models were trained to distinguish between cry, laughter, mature (consonant+vowel), and immature speech (just consonant or vowel). Models trained on the dataset outperform state-of-the-art models trained on previous datasets, achieved classification accuracy comparable to humans, and were robust across rural and urban settings.

</details>


### [94] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)

*Lei Zhang, Jiaxi Yang, Min Yang, Jian Yang, Mouxiang Chen, Jiajun Zhang, Zeyu Cui, Binyuan Hui, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Test-Driven Development, data synthesis, software engineering

**Relevance Score:** 4

**TL;DR:** Introducing SWE-Flow, a framework for automated data synthesis based on Test-Driven Development (TDD) that infers development steps from unit tests.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generation of software engineering data, moving away from human-submitted issues and towards automation-driven processes based on unit tests.

**Method:** Construction of a Runtime Dependency Graph (RDG) to capture function interactions, enabling structured development schedules and task generation for TDD.

**Key Contributions:**

	1. Automatic inference of development steps from unit tests
	2. Creation of SWE-Flow-Eval benchmark from real-world projects
	3. Doubling the efficiency of coding tasks in a TDD environment

**Result:** Generated 16,061 training and 2,020 test instances from real-world projects, leading to the SWE-Flow-Eval benchmark; showed performance improvements with fine-tuning on the dataset.

**Limitations:** 

**Conclusion:** SWE-Flow successfully automates TDD tasks and improves performance in coding through its innovative framework, with resources available for further research.

**Abstract:** We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [95] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)

*Hakyung Sung, Gyu-Ho Shin, Chanyoung Lee, You Kyung Sung, Boo Kyung Jung*

**Main category:** cs.CL

**Keywords:** Universal Dependencies, L2 Korean, morphosyntactic analysis, XPOS, UPOS

**Relevance Score:** 4

**TL;DR:** The study presents a semi-automated framework for improving morphosyntactic analysis in L2 Korean through XPOS-UPOS alignment and new sentence annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy and consistency of morphosyntactic analysis for second-language Korean by addressing limitations in existing datasets and tools.

**Method:** Developed a semi-automated framework to align morphosyntactic constructions with UPOS categories and annotated a new set of sentences from argumentative essays to enrich the corpus.

**Key Contributions:**

	1. Introduction of a semi-automated framework for aligning morphosyntactic constructions
	2. Expansion of the L2-Korean corpus with new sentence annotations
	3. Demonstration of improved model performance through empirical evaluation

**Result:** Aligned datasets show improved consistency across annotation layers and higher accuracy in morphosyntactic tagging and dependency parsing, especially with limited data.

**Limitations:** 

**Conclusion:** The study confirms that XPOS-UPOS alignments significantly benefit the morphosyntactic models, paving the way for better analysis in the realm of L2 Korean.

**Abstract:** The present study extends recent work on Universal Dependencies annotations for second-language (L2) Korean by introducing a semi-automated framework that identifies morphosyntactic constructions from XPOS sequences and aligns those constructions with corresponding UPOS categories. We also broaden the existing L2-Korean corpus by annotating 2,998 new sentences from argumentative essays. To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean morphosyntactic analysis models on datasets both with and without these alignments, using two NLP toolkits. Our results indicate that the aligned dataset not only improves consistency across annotation layers but also enhances morphosyntactic tagging and dependency-parsing accuracy, particularly in cases of limited annotated data.

</details>


### [96] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)

*Jianing Qi, Xi Ye, Hao Tang, Zhigang Zhu, Eunsol Choi*

**Main category:** cs.CL

**Keywords:** sample set aggregation, large language models, reinforcement learning, answer accuracy, reasoning tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces the Sample Set Aggregator (SSA), a compact LLM that enhances the accuracy of answers by aggregating multiple samples with reinforcement learning techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models at test-time by utilizing multiple sample answers for better accuracy.

**Method:** The authors trained the Sample Set Aggregator (SSA) LLM on concatenated sequences of multiple answer samples, optimizing it for answer accuracy using reinforcement learning.

**Key Contributions:**

	1. Introduction of the Sample Set Aggregator (SSA) for LLM answer aggregation.
	2. Optimization of answer accuracy through reinforcement learning on multiple samples.
	3. Demonstration of improved performance across various reasoning tasks and models.

**Result:** SSA demonstrated superior performance on multiple reasoning datasets compared to existing test-time scaling methods such as reward model-based re-ranking.

**Limitations:** 

**Conclusion:** The SSA can efficiently aggregate outputs from various black box models and shows strong generalization across different model families and tasks.

**Abstract:** Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on multiple reasoning datasets show that SSA outperforms other test-time scaling methods such as reward model-based re-ranking. Our approach also shows a promising generalization ability, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently.

</details>


### [97] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)

*Hakyung Sung, Karla Csuros, Min-Chang Sung*

**Main category:** cs.CL

**Keywords:** proofreading, second language, LLM, human-Computer interaction, lexical features

**Relevance Score:** 8

**TL;DR:** The study evaluates the effectiveness of human and LLM proofreading on second language writings, revealing enhanced intelligibility and consistency across multiple LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how human and LLM proofreading can improve the intelligibility of second language writings.

**Method:** The study involved comparing the proofreading effects of human annotators and three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on identical second language texts, focusing on lexical and syntactic features.

**Key Contributions:**

	1. Demonstrated effectiveness of LLMs in proofreading second language texts
	2. Identified differences in approaches between human and LLM proofreading
	3. Provided insights into the consistency of outcomes across multiple LLMs

**Result:** Both human and LLM proofreading improved bigram lexical features, leading to better coherence, but LLMs displayed a more generative approach with diverse vocabulary and enhanced sentence structures.

**Limitations:** 

**Conclusion:** LLM proofreading provides significant enhancements in lexical and syntactic features with high consistency across models, benefiting second language writing.

**Abstract:** This study examines the lexical and syntactic interventions of human and LLM proofreading aimed at improving overall intelligibility in identical second language writings, and evaluates the consistency of outcomes across three LLMs (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and LLM proofreading enhance bigram lexical features, which may contribute to better coherence and contextual connectedness between adjacent words. However, LLM proofreading exhibits a more generative approach, extensively reworking vocabulary and sentence structures, such as employing more diverse and sophisticated vocabulary and incorporating a greater number of adjective modifiers in noun phrases. The proofreading outcomes are highly consistent in major lexical and syntactic features across the three models.

</details>


### [98] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)

*Haozhen Zhang, Tao Feng, Jiaxuan You*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, multi-LLM routing, performance-cost optimization, QA benchmarks

**Relevance Score:** 9

**TL;DR:** Router-R1 is a reinforcement learning framework for multi-LLM routing and aggregation, optimizing performance-cost tradeoffs for complex tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The current limitations of single-round, one-to-one LLM routers hinder their ability to address complex tasks that require the strengths of multiple LLMs.

**Method:** Router-R1 formulates multi-LLM routing as a sequential decision process by integrating both 'think' and 'route' actions, supported by a rule-based reward system for optimizing outcomes and costs.

**Key Contributions:**

	1. Introduces a reinforcement learning-based framework for multi-LLM routing.
	2. Incorporates internal deliberation with dynamic model invocation.
	3. Optimizes performance and cost trade-offs through a novel reward system.

**Result:** Router-R1 demonstrates superior performance on seven general and multi-hop QA benchmarks compared to several strong baselines, showing robust generalization and effective cost management.

**Limitations:** 

**Conclusion:** Router-R1 effectively combines the reasoning capabilities of LLMs with strategic routing decisions, paving the way for improved multi-LLM applications in AI.

**Abstract:** The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.

</details>


### [99] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)

*Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov*

**Main category:** cs.CL

**Keywords:** Vision-Language models, multi-modal processing, data representation, visual inputs, text inputs

**Relevance Score:** 8

**TL;DR:** This paper examines the performance gap between Vision-Language models (VLMs) on visual inputs compared to text inputs, identifying differences in their computational circuits and proposing a method to improve visual data representation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the accuracy gap between Vision-Language models when processing visual versus textual inputs.

**Method:** The study analyzes task-specific computational sub-graphs in VLMs across different modalities and introduces a method to augment visual data representations from later layers into earlier ones.

**Key Contributions:**

	1. Proposed a method to improve visual data representation in VLMs
	2. Identified and compared task-specific computational circuits in different modalities
	3. Highlighted the misalignment of image data representations with textual representations in initial processing layers

**Result:** The proposed intervention reduces the performance gap between visual and text modalities by closing a third of the accuracy deficit, as evidenced by experiments across multiple tasks and models.

**Limitations:** 

**Conclusion:** This research reveals significant insights into VLMs' multi-modal performance and offers a simple, training-free solution to enhance visual input processing.

**Abstract:** Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task on text (e.g., counting words in a text). We investigate this accuracy gap by identifying and comparing the \textit{circuits} - the task-specific computational sub-graphs - in different modalities. We show that while circuits are largely disjoint between modalities, they implement relatively similar functionalities: the differences lie primarily in processing modality-specific data positions (an image or a text sequence). Zooming in on the image data representations, we observe they become aligned with the higher-performing analogous textual representations only towards later layers, too late in processing to effectively influence subsequent positions. To overcome this, we patch the representations of visual data tokens from later layers back into earlier layers. In experiments with multiple tasks and models, this simple intervention closes a third of the performance gap between the modalities, on average. Our analysis sheds light on the multi-modal performance gap in VLMs and suggests a training-free approach for reducing it.

</details>


### [100] [A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement](https://arxiv.org/abs/2206.05446)

*Effi Levi, Shaul R. Shenhav*

**Main category:** cs.CL

**Keywords:** annotation, inter-annotator disagreement, decomposition, narrative analysis, research methodology

**Relevance Score:** 4

**TL;DR:** A method is proposed for decomposing annotations into separate levels to analyze inter-annotators disagreement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand and analyze inter-annotators disagreement by decomposing existing annotations into levels.

**Method:** The paper presents two strategies: a theoretically-driven approach based on prior knowledge and an exploration-based approach that computes possible decompositions inductively.

**Key Contributions:**

	1. Novel method for annotation decomposition
	2. Two distinct strategies for achieving decomposition
	3. Application demonstrated on narrative analysis dataset

**Result:** Application of the method on a narrative analysis dataset demonstrated its potential in testing hypotheses about annotation disagreements and revealing underlying structures.

**Limitations:** The scalability of the approach and its application to different types of annotations are not extensively tested.

**Conclusion:** The approach can be extended for more generalized use and applied to various purposes beyond the presented case.

**Abstract:** We propose a novel method to conceptually decompose an existing annotation into separate levels, allowing the analysis of inter-annotators disagreement in each level separately. We suggest two distinct strategies in order to actualize this approach: a theoretically-driven one, in which the researcher defines a decomposition based on prior knowledge of the annotation task, and an exploration-based one, in which many possible decompositions are inductively computed and presented to the researcher for interpretation and evaluation. Utilizing a recently constructed dataset for narrative analysis as our use-case, we apply each of the two strategies to demonstrate the potential of our approach in testing hypotheses regarding the sources of annotation disagreements, as well as revealing latent structures and relations within the annotation task. We conclude by suggesting how to extend and generalize our approach, as well as use it for other purposes.

</details>


### [101] [A Survey on Long Text Modeling with Transformers](https://arxiv.org/abs/2302.14502)

*Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao*

**Main category:** cs.CL

**Keywords:** long text modeling, Transformer models, NLP, context length, applications

**Relevance Score:** 7

**TL;DR:** Overview of recent advances in long text modeling using Transformer models, addressing challenges and applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop effective modeling methods for processing and analyzing long texts in NLP due to their complex semantics and characteristics.

**Method:** Provide an overview of long text modeling, discuss processing methods and improved Transformer architectures to extend maximum context length, and adapt Transformer models for special characteristics of long texts.

**Key Contributions:**

	1. Definition of long text modeling
	2. Review of methods to extend context length in Transformer models
	3. Identification of applications for long text modeling

**Result:** Identified key methods for effectively processing long texts and adapting Transformer architectures, with applications discussed.

**Limitations:** 

**Conclusion:** The paper concludes with suggestions for future research directions in long text modeling.

**Abstract:** Modeling long texts has been an essential technique in the field of natural language processing (NLP). With the ever-growing number of long documents, it is important to develop effective modeling methods that can process and analyze such texts. However, long texts pose important research challenges for existing text models, with more complex semantics and special characteristics. In this paper, we provide an overview of the recent advances on long texts modeling based on Transformer models. Firstly, we introduce the formal definition of long text modeling. Then, as the core content, we discuss how to process long input to satisfy the length limitation and design improved Transformer architectures to effectively extend the maximum context length. Following this, we discuss how to adapt Transformer models to capture the special characteristics of long texts. Finally, we describe four typical applications involving long text modeling and conclude this paper with a discussion of future directions. Our survey intends to provide researchers with a synthesis and pointer to related work on long text modeling.

</details>


### [102] [Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study](https://arxiv.org/abs/2310.16937)

*Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca Buratti, Lav Varshney*

**Main category:** cs.CL

**Keywords:** large language models, transfer learning, low-resource programming languages, software engineering, code generation

**Relevance Score:** 7

**TL;DR:** This paper explores enhancing large language models (LLMs) performance on low-resource programming languages using transfer learning from high-resource languages, finding significant improvements through cross-lingual transfer.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of data for low-resource programming languages that hampers LLM performance and increases software maintenance costs.

**Method:** An empirical study assessing transferability across 10 to 41 programming languages and five key tasks, including code generation and error detection, supplemented by a performance prediction model.

**Key Contributions:**

	1. Demonstrated effectiveness of cross-lingual transfer over zero-shot learning for LLMs on low-resource languages.
	2. Developed a performance prediction model for choosing source languages based on various linguistic features.
	3. Provided insights into the features that influence transfer performance across different programming languages.

**Result:** Cross-lingual transfer significantly outperforms zero-shot learning, with variation in effectiveness depending on source and target languages.

**Limitations:** The results may vary for unexamined languages or tasks not included in the study.

**Conclusion:** The study provides practical guidance for data acquisition and model training, showing that careful selection of source languages can enhance transfer performance.

**Abstract:** Large language models (LLMs) have achieved state-of-the-art performance in various software engineering tasks, including error detection, clone detection, and code translation, primarily leveraging high-resource programming languages like Python and Java. However, many critical languages, such as COBOL, as well as emerging languages, such as Rust and Swift, remain low-resource due to limited openly available code. This scarcity hampers the training and effectiveness of LLMs for these languages, increasing software maintenance costs and stifling innovation. Addressing this gap, we investigate the potential of transfer learning to enhance LLM performance on low-resource programming languages by leveraging data from high-resource counterparts. Our extensive empirical study evaluates transferability across 10 to 41 programming languages and five key tasks: code generation, clone detection, code repair, solution domain classification, and error detection. Additionally, we develop a performance prediction model to guess the best source languages for a given target and task, and analyze the features that influence transfer performance. We further replicate a representative subset of experiments with a larger model to test the generalizability of our conclusions to contemporary large-scale LLMs. Our findings demonstrate that cross-lingual transfer significantly outperforms zero-shot learning, with effectiveness varying based on both source and target languages. Furthermore, our model reliably predicts successful transfer sources by considering linguistic and dataset-specific features, offering practical guidance for data acquisition and model training. This work contributes to the development of LLM-driven tools for low-resource programming languages and provides insights into the characteristics that facilitate transfer across language pairs.

</details>


### [103] [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856)

*Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, V√§in√∂ Hatanp√§√§, Peter Sarlin, Sampo Pyysalo*

**Main category:** cs.CL

**Keywords:** multilinguality, language models, Finnish, machine learning, open source

**Relevance Score:** 8

**TL;DR:** This paper introduces Poro 34B, a multilingual model that improves upon monolingual models for Finnish through the inclusion of multiple languages in its training dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more training data for languages with limited resources, like Finnish, can be fulfilled by employing multilingual training strategies instead of focusing solely on individual large languages.

**Method:** Poro 34B is a 34 billion parameter model trained on 1 trillion tokens across Finnish, English, and programming languages, demonstrating a multilingual training approach.

**Key Contributions:**

	1. Introduction of Poro 34B model with multilingual training
	2. Demonstration of improved capabilities for Finnish through multilingualism
	3. Release of model parameters and training data under open licenses

**Result:** Poro 34B significantly improves the capabilities for Finnish, excels in translation tasks, and performs competitively in English and programming languages.

**Limitations:** 

**Conclusion:** Multilingual training can effectively overcome data scarcity for less-resourced languages, leading to better model performance compared to monolingual approaches.

**Abstract:** The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing: when the lack of training data is a constraint for effectively training larger models for a target language, augmenting the dataset with other languages can offer a way to improve over the capabilities of monolingual models for that language. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that substantially advances over the capabilities of existing models for Finnish and excels in translation, while also achieving competitive performance in its class for English and programming languages. We release the model parameters, scripts, and data under open licenses at https://huggingface.co/LumiOpen/Poro-34B.

</details>


### [104] [P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts](https://arxiv.org/abs/2406.12548)

*Yuhao Dan, Jie Zhou, Qin Chen, Junfeng Tian, Liang He*

**Main category:** cs.CL

**Keywords:** Personalized LLMs, Big Five personality traits, Mixture of experts, Personality Specialization Loss, OCEAN-Chat

**Relevance Score:** 9

**TL;DR:** This paper introduces P-React, a personalized large language model that incorporates Big Five personality traits to enhance the realism and psychological grounding of AI systems in applications like emotional support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to improve the anthropomorphism and decision-making reflection in personalized large language models by addressing the underlying personality traits that shape behavior, which have been overlooked in existing research.

**Method:** The authors propose a mixture of experts (MoE)-based model called P-React, which integrates a Personality Specialization Loss (PSL) to better capture individual trait expressions of users.

**Key Contributions:**

	1. Introduction of P-React, a novel MoE-based personalized LLM.
	2. Integration of Personality Specialization Loss (PSL) for better trait expression.
	3. Creation of the OCEAN-Chat dataset for training LLMs with personality traits.

**Result:** Experiments show that P-React effectively maintains a consistent and authentic representation of personality traits in responses, outperforming existing models that do not account for these traits.

**Limitations:** The model's effectiveness may vary across different personality dimensions and cultural contexts, which requires further investigation.

**Conclusion:** The proposed P-React model and the OCEAN-Chat dataset together provide a significant step towards creating more nuanced AI systems that can better simulate human-like personality expressions.

**Abstract:** Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality.

</details>


### [105] [SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs](https://arxiv.org/abs/2406.19593)

*Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard*

**Main category:** cs.CL

**Keywords:** Multimodal retrieval, Visual question answering, Synthetic datasets, Context-augmented generation, Knowledge-based VQA

**Relevance Score:** 9

**TL;DR:** Introduction of SK-VQA, a large-scale synthetic multimodal dataset for knowledge-based visual question answering, enhancing multimodal LLMs with context-augmented generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of multimodal LLMs in context-augmented generation tasks, particularly in knowledge-based visual question answering.

**Method:** Development of the SK-VQA dataset containing over 2 million visual question-answer pairs and context documents, followed by human evaluations and extensive experiments on model training.

**Key Contributions:**

	1. Introduction of a synthetic dataset (SK-VQA) with 2 million+ visual question-answer pairs
	2. Demonstration of high-quality and contextual relevance through human evaluations
	3. Proven effectiveness in training models for improved generalization in context-aware tasks

**Result:** Models trained on SK-VQA showcased improved performance in context-aware VQA and multimodal retrieval augmented generation settings, confirming the dataset's value as a benchmark and training resource.

**Limitations:** 

**Conclusion:** SK-VQA enhances the capabilities of MLLMs in context-augmented scenarios and is available for public use, promising better generalization in VQA tasks.

**Abstract:** Multimodal retrieval augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where external knowledge is needed to answer a question. However, existing multimodal LLMs (MLLMs) are not designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training MLLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SK-VQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with context documents containing information necessary to determine the final answer. Compared to previous datasets, SK-VQA contains 11x more unique questions, exhibits greater domain diversity, and covers a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SK-VQA serves both as a challenging KB-VQA benchmark and as an effective training resource for adapting MLLMs to context-augmented generation. Our results further indicate that models trained on SK-VQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings. SK-VQA is publicly available via Hugging Face Hub.

</details>


### [106] [High-Throughput Phenotyping of Clinical Text Using Large Language Models](https://arxiv.org/abs/2408.01214)

*Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi, Michael D. Carrithers*

**Main category:** cs.CL

**Keywords:** High-throughput phenotyping, Large language models, Clinical summaries, Precision medicine, Health informatics

**Relevance Score:** 9

**TL;DR:** This study evaluates the use of large language models (LLMs) for automating phenotyping of clinical summaries to enhance precision medicine.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve precision medicine by automating the mapping of patient signs to standardized ontology concepts using high-throughput phenotyping.

**Method:** Performance comparison of GPT-4 and GPT-3.5-Turbo on clinical summaries from the OMIM database, focusing on their ability to identify, categorize, and normalize signs.

**Key Contributions:**

	1. Comparison of GPT-4 with GPT-3.5-Turbo in clinical summary phenotyping
	2. Demonstration of high concordance with manual annotators
	3. Insight into the potential dominance of LLMs in automating clinical text phenotyping

**Result:** GPT-4 significantly outperforms GPT-3.5-Turbo in identifying, categorizing, and normalizing clinical signs, achieving high agreement with manual annotations.

**Limitations:** Some limitations noted in sign normalization processes.

**Conclusion:** The extensive pre-training of GPT-4 enables superior performance in high-throughput phenotyping tasks without needing manually annotated training data.

**Abstract:** High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automating high-throughput phenotyping of clinical text.

</details>


### [107] [Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR](https://arxiv.org/abs/2409.08797)

*Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu*

**Main category:** cs.CL

**Keywords:** self-supervised learning, speech recognition, Zipformer-Transducer, acoustic context, Gigaspeech

**Relevance Score:** 6

**TL;DR:** This paper explores the use of self-supervised learning (SSL) discrete speech features in ASR systems, demonstrating significant improvements in word error rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this work is to enhance automatic speech recognition (ASR) systems by leveraging SSL discrete speech representations to improve context modeling.

**Method:** The paper employs discrete speech features from WavLM models as additional acoustic context features in Zipformer-Transducer ASR systems, testing their efficacy against traditional Fbank features on the Gigaspeech corpus.

**Key Contributions:**

	1. Demonstrated effectiveness of SSL discrete speech features for ASR.
	2. Achieved statistically significant WER reductions compared to baseline models.
	3. Provided open-source implementation for further research.

**Result:** The study shows that using discrete token features for modeling both cross-utterance and internal contexts significantly reduces the word error rate (WER) by 0.32% to 0.41% absolute, achieving the lowest published WER of 11.15% and 11.14% on respective datasets.

**Limitations:** 

**Conclusion:** The findings indicate that SSL-based discrete speech features improve ASR system performance, providing a promising direction for future work in the domain and are available for public use.

**Abstract:** Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\_ASR.

</details>


### [108] [Guidelines for Fine-grained Sentence-level Arabic Readability Annotation](https://arxiv.org/abs/2410.08674)

*Nizar Habash, Hanada Taha-Thomure, Khalid N. Elmadani, Zeina Zeino, Abdallah Abushmaes*

**Main category:** cs.CL

**Keywords:** Arabic readability, corpus, annotation guidelines, Taha/Arabi21, educational resource

**Relevance Score:** 4

**TL;DR:** This paper details the annotation guidelines for the Balanced Arabic Readability Evaluation Corpus (BAREC), a resource for assessing sentence-level readability in Arabic, validated by educators.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To create a large-scale resource for fine-grained readability assessment in Arabic, addressing the lack of such corpora.

**Method:** The guidelines were developed using the Taha/Arabi21 framework and refined through iterative training with native Arabic-speaking educators, leading to high inter-annotator agreement.

**Key Contributions:**

	1. Creation of a large-scale, publicly available Arabic readability corpus
	2. Refinement of annotation guidelines through collaboration with educators
	3. Benchmarking of automatic readability models across multiple levels

**Result:** The corpus features 69,441 sentences labeled across 19 readability levels, with substantial inter-annotator agreement at 81.8% and benchmarking of automatic readability models across various classification levels.

**Limitations:** 

**Conclusion:** The BAREC corpus and its annotation guidelines provide a standardized resource for Arabic readability assessment, benefiting educational and research applications.

**Abstract:** This paper presents the annotation guidelines of the Balanced Arabic Readability Evaluation Corpus (BAREC), a large-scale resource for fine-grained sentence-level readability assessment in Arabic. BAREC includes 69,441 sentences (1M+ words) labeled across 19 levels, from kindergarten to postgraduate. Based on the Taha/Arabi21 framework, the guidelines were refined through iterative training with native Arabic-speaking educators. We highlight key linguistic, pedagogical, and cognitive factors in determining readability and report high inter-annotator agreement: Quadratic Weighted Kappa 81.8% (substantial/excellent agreement) in the last annotation phase. We also benchmark automatic readability models across multiple classification granularities (19-, 7-, 5-, and 3-level). The corpus and guidelines are publicly available.

</details>


### [109] [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://arxiv.org/abs/2410.19317)

*Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** fairness, large language models, multi-turn dialogue, bias, benchmark

**Relevance Score:** 9

**TL;DR:** This paper introduces a benchmark for assessing fairness in large language models (LLMs) during multi-turn dialogue, addressing bias issues more effectively than existing single-turn benchmarks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns about fairness in LLMs leading to bias and discrimination in real-world applications necessitate a comprehensive evaluation framework for multi-turn dialogue scenarios.

**Method:** The authors propose FairMT-Bench, a benchmark that includes a task taxonomy over three stages related to LLM fairness, and develop a multi-turn dialogue dataset called FairMT-10K for evaluation.

**Key Contributions:**

	1. Introduction of FairMT-Bench for multi-turn dialogue fairness assessment
	2. Creation of FairMT-10K and FairMT-1K datasets for evaluating LLMs
	3. Empirical evidence showcasing bias in LLMs during multi-turn dialogues

**Result:** Current LLMs show increased likelihood of generating biased responses in multi-turn dialogues, with performance variations across tasks and models; a new dataset, FairMT-1K, is introduced for testing.

**Limitations:** 

**Conclusion:** The study highlights the need for improved fairness in LLMs and suggests that FairMT-1K should be adopted for future fairness assessments in multi-turn dialogues.

**Abstract:** The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, \textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, \texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on \texttt{FairMT-10K} reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, \texttt{FairMT-1K}, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of \texttt{FairMT-1K} in such efforts.

</details>


### [110] [SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script](https://arxiv.org/abs/2410.20682)

*Eunwon Kim, Chanho Park, Buru Chang*

**Main category:** cs.CL

**Keywords:** long-term dialogue, shared memories, dataset, dialogue framework, AI

**Relevance Score:** 6

**TL;DR:** The study introduces the SHARE dataset for enhancing long-term dialogue by leveraging shared memories, along with the EPISODE framework that manages these memories in conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To engage long-term dialogue using shared memories between individuals, which strengthen their relationships.

**Method:** The study constructs the SHARE dataset from movie scripts and develops the EPISODE framework to utilize shared experiences, supported by experiments demonstrating improved dialogue engagement.

**Key Contributions:**

	1. Introduction of the SHARE dataset constructed from movie scripts
	2. Development of the EPISODE framework for managing shared memories
	3. Demonstrated effectiveness of shared memories in enhancing dialogue engagement

**Result:** Experiments show that dialogues incorporating shared memories are more engaging and sustainable, and that EPISODE effectively manages these memories.

**Limitations:** 

**Conclusion:** The SHARE dataset and EPISODE framework provide valuable tools for enhancing long-term dialogue systems by incorporating shared memories.

**Abstract:** Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at https://github.com/e1kim/SHARE.

</details>


### [111] [Length-Induced Embedding Collapse in PLM-based Models](https://arxiv.org/abs/2410.24200)

*Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu*

**Main category:** cs.CL

**Keywords:** text embeddings, length collapse, self-attention, PLM models, TempScale

**Relevance Score:** 8

**TL;DR:** This paper introduces Length Collapse, a phenomenon where PLM-based text embeddings cluster for longer texts, causing performance declines in NLP tasks. It proposes TempScale to mitigate this issue, leading to performance improvements in embeddings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance degradation of PLM-based models on longer texts, specifically investigating a phenomenon known as Length Collapse.

**Method:** The paper presents a theoretical analysis of the self-attention mechanism in PLM-based models as a low-pass filter, introducing the method TempScale to narrow filtering gaps between text lengths.

**Key Contributions:**

	1. Introduction of Length Collapse phenomenon
	2. Theoretical analysis of self-attention as a low-pass filter
	3. Development of TempScale to address Length Collapse.

**Result:** The proposed TempScale method improves performance by 0.94% on MTEB and 1.10% on LongEmbed, which focuses on long-context retrieval tasks.

**Limitations:** 

**Conclusion:** TempScale effectively mitigates the Length Collapse phenomenon, leading to more consistent embeddings for texts of varying lengths, and thereby improving model performance on downstream tasks.

**Abstract:** Text embeddings from PLM-based models enable a wide range of applications, yet their performance often degrades on longer texts. In this paper, we introduce a phenomenon we call Length Collapse, where embeddings of longer texts tend to cluster together. This clustering results in a distributional inconsistency between the embeddings of short and long texts. We further investigate how these differences contribute to the performance decline observed with longer texts across various downstream tasks. Through a rigorous theoretical analysis of the self-attention mechanism, which acts as a low-pass filter in PLM-based models, we demonstrate that as text length increases, the strength of low-pass filtering intensifies, causing embeddings to retain more low-frequency components. As a result, input token features become more similar, leading to clustering and ultimately the collapse of embeddings for longer texts. To address this issue, we propose a simple method, TempScale, which mitigates the Length Collapse phenomenon. By narrowing the gap in low-pass filtering rates between long and short texts, TempScale ensures more consistent embeddings across different text lengths. This approach leads to performance improvements of 0.94% on MTEB and 1.10% on LongEmbed, which focuses specifically on long-context retrieval, providing strong evidence for the validity of our analysis. The source code is available at https://github.com/Yuqi-Zhou/Length_Collapse.

</details>


### [112] [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/abs/2411.15129)

*Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell*

**Main category:** cs.CL

**Keywords:** ChatGPT, bullshit, language model, natural language processing, social dysfunction

**Relevance Score:** 8

**TL;DR:** This paper examines the language generated by ChatGPT, analyzing its similarities to descriptors of 'bullshit' in human discourse, through empirical comparison with scientific publications.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the language features of ChatGPT in relation to criticisms of its quality and its comparison to human-generated text in contexts of social dysfunction.

**Method:** Empirical analysis contrasting the language of 1,000 scientific publications with text generated by ChatGPT, using statistical modeling and hypothesis-testing methods.

**Key Contributions:**

	1. Empirical analysis of language generated by ChatGPT
	2. Comparison to human-produced 'bullshit' in political and workplace contexts
	3. Statistical modeling linking language features to social dysfunction

**Result:** The study finds a reliable statistical correlation between the characteristics of language produced by ChatGPT and the functions of 'bullshit' as described in political speech and workplace contexts.

**Limitations:** Focus is primarily on empirical characteristics without delving into deeper implications of language quality.

**Conclusion:** LLM-generated language can exhibit features similar to those found in problematic human communication, raising questions about linguistic quality and meaning.

**Abstract:** What can we learn about language from studying how it is used by ChatGPT and other large language model (LLM)-based chatbots? In this paper, we analyse the distinctive character of language generated by ChatGPT, in relation to questions raised by natural language processing pioneer, and student of Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based chatbots produce "slop," or even "bullshit," in the sense of Frankfurt's popular monograph On Bullshit, we conduct an empirical study to contrast the language of 1,000 scientific publications with typical text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of political speech, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language.

</details>


### [113] [From Language Models over Tokens to Language Models over Characters](https://arxiv.org/abs/2412.03719)

*Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J. O'Donnell, Ryan Cotterell*

**Main category:** cs.CL

**Keywords:** language models, tokenization, character-level models, compression, approximation

**Relevance Score:** 8

**TL;DR:** This paper proposes algorithms to convert token-level language models to character-level models, improving the processing and prompt specification for applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by programmers using token-level language models due to the requirements of tokenization, which is sensitive to prompt specification.

**Method:** The paper presents both exact and approximate algorithms for converting token-level language models to character-level models, and benchmarks their runtime and approximation quality.

**Key Contributions:**

	1. Algorithms for converting token-level to character-level language models
	2. Benchmarking performance across multiple language models
	3. Achieving higher compression rates for language models

**Result:** The methods maintain accuracy in approximating character-level distributions and demonstrate significant improvements in compression rates across four language models tested.

**Limitations:** 

**Conclusion:** The proposed conversion algorithms enable better performance and efficiency for applications using language models by optimizing character-level handling.

**Abstract:** Modern language models are internally -- and mathematically -- distributions over $\it{token}$ strings rather than $\it{character}$ strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. Across four publicly available language models, we find that -- even with a small computation budget -- our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved.

</details>


### [114] [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569)

*Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, Asaf Yehudai*

**Main category:** cs.CL

**Keywords:** Generative AI, LLM judges, System ranking, Bias assessment, Human comparison

**Relevance Score:** 8

**TL;DR:** This paper presents a large-scale study on evaluating LLM judges for system ranking in generative AI, addressing biases that affect system-level rankings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid advancements in generative AI necessitate effective evaluation mechanisms for various models and configurations, particularly due to the multitude of LLMs available.

**Method:** The study conducts large-scale assessments of LLM judges by aggregating judgment scores from multiple system outputs to generate rankings, which are then compared with human-based rankings.

**Key Contributions:**

	1. First large-scale study of LLM judges as system rankers
	2. Characterization of judge behavior, including biases and decisiveness
	3. Comparison of LLM-based rankings with human rankings

**Result:** The assessment reveals biases in LLM judges towards certain systems, affecting their ranking capabilities, and provides insights into judge decisiveness.

**Limitations:** The study may not encompass all types of biases or judge behaviors across different generative AI models.

**Conclusion:** The findings highlight the importance of validating LLM judges in system-ranking contexts and understanding their behavior to improve evaluation quality.

**Abstract:** Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.

</details>


### [115] [Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence](https://arxiv.org/abs/2412.13949)

*Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng Jia, Ming Tang, Tat-Seng Chua, Jinqiao Wang*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, multimodal reasoning, hallucination, attention mechanisms, visual context

**Relevance Score:** 8

**TL;DR:** This paper addresses hallucinations in Large Vision-Language Models (LVLMs) by investigating the internal mechanisms involving multi-head attention, introducing metrics and methods to enhance attention focused on visual information while reducing reliance on language patterns.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucination issues in LVLMs, which undermine their accuracy and reliability, by exploring the underlying mechanisms rather than just the symptoms at the generation stage.

**Method:** The study introduces Vision-aware Head Divergence (VHD) as a metric to evaluate attention head sensitivity to visual context, and proposes Vision-aware Head Reinforcement (VHR) to enhance the performance of vision-aware attention heads without requiring additional training.

**Key Contributions:**

	1. Introduction of Vision-aware Head Divergence (VHD) for evaluating attention heads' visual sensitivity
	2. Proposal of Vision-aware Head Reinforcement (VHR) as a training-free method to reduce hallucinations
	3. Extensive experimental results demonstrating improved performance over state-of-the-art methods.

**Result:** The proposed method, VHR, shows superior performance in reducing hallucinations compared to existing approaches while maintaining efficiency with minimal time overhead.

**Limitations:** 

**Conclusion:** Enhancing the role of vision-aware attention heads helps mitigate hallucination in LVLMs, leading to more reliable outputs without compromising efficiency.

**Abstract:** Large vision-language models (LVLMs) have made substantial progress in integrating large language models (LLMs) with visual inputs, enabling advanced multimodal reasoning. Despite their success, a persistent challenge is hallucination-where generated text fails to accurately reflect visual content-undermining both accuracy and reliability. Existing methods focus on alignment training or decoding refinements but primarily address symptoms at the generation stage without probing the underlying causes. In this work, we investigate the internal mechanisms driving hallucination in LVLMs, with an emphasis on the multi-head attention module. Specifically, we introduce Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of attention head outputs to visual context. Based on this, our findings reveal the presence of vision-aware attention heads that are more attuned to visual information; however, the model's overreliance on its prior language patterns is closely related to hallucinations. Building on these insights, we propose Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate hallucination by enhancing the role of vision-aware attention heads. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches in mitigating hallucinations, while maintaining high efficiency with negligible additional time overhead.

</details>


### [116] [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)

*Paul Youssef, Zhixue Zhao, Daniel Braun, J√∂rg Schl√∂tterer, Christin Seifert*

**Main category:** cs.CL

**Keywords:** Large Language Models, knowledge editing, safety risks, AI ecosystem, malicious use cases

**Relevance Score:** 9

**TL;DR:** The paper discusses the safety risks associated with knowledge editing methods (KEs) in Large Language Models (LLMs), highlighting the potential for malicious use and vulnerabilities in the AI ecosystem.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs hold vast amounts of factual content, ensuring their reliability and safety has become critical, especially as outdated information can lead to misinformation through knowledge editing.

**Method:** The paper examines the characteristics of KEs that make them appealing to malicious actors, analyzes specific use cases of malicious activities enabled by KEs, and identifies vulnerabilities within the AI ecosystem.

**Key Contributions:**

	1. Highlighting the safety risks of knowledge editing methods in LLMs.
	2. Identifying specific malicious use cases of KEs.
	3. Calling for research on tamper-resistant AI models.

**Result:** The analysis shows that KEs are computationally inexpensive and easily adaptable for malicious purposes, posing risks to the integrity of LLMs and the AI ecosystem.

**Limitations:** The paper does not provide experimental data to support the claims, focusing instead on theoretical implications.

**Conclusion:** The paper urges the AI community to focus on developing tamper-resistant models and engaging in active protection of the AI ecosystem against knowledge editing risks.

**Abstract:** Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.

</details>


### [117] [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/abs/2502.03699)

*Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik*

**Main category:** cs.CL

**Keywords:** Large Language Models, LLM alignment, Information Retrieval, Reinforcement Learning, direct optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces LarPO, a novel method for aligning Large Language Models (LLMs) using direct optimization principles from Information Retrieval (IR).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in aligning LLMs for trustworthy and ethical behavior, particularly issues like misinformation and bias.

**Method:** The authors propose a framework that connects LLM alignment with IR methodologies, specifically using a retriever-reranker paradigm.

**Key Contributions:**

	1. Introduction of LarPO, a novel alignment method for LLMs.
	2. Integration of Information Retrieval principles for LLM alignment.
	3. Demonstrated significant improvements in evaluation metrics.

**Result:** LarPO demonstrates significant improvements in alignment quality, achieving a 38.9% and 13.7% enhancement on two evaluation benchmarks.

**Limitations:** 

**Conclusion:** The integration of IR principles into LLM alignment offers a simpler and effective alternative, paving the way for future research in this area.

**Abstract:** Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.

</details>


### [118] [In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs](https://arxiv.org/abs/2502.04390)

*Simone Clemente, Zied Ben Houidi, Alexis Huet, Dario Rossi, Giulio Franzese, Pietro Michiardi*

**Main category:** cs.CL

**Keywords:** Large Language Models, catastrophic interference, neural networks, contradictory information, selective plasticity

**Relevance Score:** 9

**TL;DR:** The paper investigates the catastrophic interference in Large Language Models when updating facts with contradictory information, leading to significant loss of unrelated knowledge. It explores selective plasticity in neural networks and proposes potential architectural changes to mitigate these issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the fundamental limitations of Large Language Models in handling contradictory information and to investigate methods to prevent catastrophic knowledge overwrites.

**Method:** The researchers conducted systematic empirical investigations on various model scales (GPT-2 to GPT-J-6B) to study the effects of contradictory updates and implemented targeted network updates to distinguish between frequently used and rarely used neurons.

**Key Contributions:**

	1. Identified catastrophic interference in LLMs due to contradictory information updates.
	2. Evaluated the impact of neuron usage frequency on knowledge retention during updates.
	3. Proposed a detection mechanism for contradictory information with high accuracy.

**Result:** The study found that contradictory updates lead to catastrophic interference that destroys up to 80% of unrelated knowledge, regardless of the targeted updating strategy. However, sparing frequently-used neurons significantly improves knowledge retention for non-contradictory updates.

**Limitations:** The approach may not generalize to all types of neural network architectures or domains beyond language models.

**Conclusion:** The findings indicate a fundamental limitation in how neural networks handle contradictions, suggesting the need for new architectures that can resist contradictions similar to human cognition.

**Abstract:** Through systematic empirical investigation, we uncover a fundamental and concerning property of Large Language Models: while they can safely learn facts that don't contradict their knowledge, attempting to update facts with contradictory information triggers catastrophic corruption of unrelated knowledge. Unlike humans, who naturally resist contradictory information, these models indiscriminately accept contradictions, leading to devastating interference, destroying up to 80% of unrelated knowledge even when learning as few as 10-100 contradicting facts. To understand whether this interference could be mitigated through selective plasticity, we experiment with targeted network updates, distinguishing between previously used (stubborn) and rarely used (plastic) neurons. We uncover another asymmetry: while sparing frequently-used neurons significantly improves retention of existing knowledge for non-contradictory updates (98% vs 93% with standard updates), contradictory updates trigger catastrophic interference regardless of targeting strategy. This effect which persists across tested model scales (GPT-2 to GPT-J-6B), suggests a fundamental limitation in how neural networks handle contradictions. Finally, we demonstrate that contradictory information can be reliably detected (95%+ accuracy) using simple model features, offering a potential protective mechanism. These findings motivate new architectures that can, like humans, naturally resist contradictions rather than allowing destructive overwrites.

</details>


### [119] [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/abs/2502.05202)

*Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Gaurav Jain, Oren Pereg, Moshe Wasserblat, David Harel*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, inference acceleration

**Relevance Score:** 9

**TL;DR:** This paper introduces novel speculative decoding methods that efficiently infer large language models without requiring shared vocabularies between drafter and target models, enabling faster processing while preserving output quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to accelerate inference in large language models (LLMs) while keeping results accurate and efficient.

**Method:** Introduction of three new speculative decoding methods that allow different vocabularies for drafter and target models, preserving the target distribution and requiring no additional training.

**Key Contributions:**

	1. Three new speculative decoding methods eliminating shared vocabulary constraints
	2. Lossless methods that work with existing models without retraining
	3. Significant speed enhancements on inference tasks

**Result:** The proposed methods achieve speedups of up to 2.8 times over standard autoregressive decoding on various tasks including summarization and programming.

**Limitations:** 

**Conclusion:** These methods enhance the application of speculative decoding in practice by broadening the range of models that can be utilized without retraining.

**Abstract:** Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.

</details>


### [120] [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/abs/2502.12658)

*Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen*

**Main category:** cs.CL

**Keywords:** Privacy, Large Language Models, Personal Identifiable Information, Data Scrubbing, Security vulnerabilities

**Relevance Score:** 8

**TL;DR:** This paper introduces R.R., a novel attack method to reconstruct personally identifiable information (PII) from masked training data in large language models (LLMs).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the significant privacy risks posed by LLMs, particularly the challenge of reconstructing specific PII from scrubbed training data, which is a gap in current privacy attack methodologies.

**Method:** The proposed method, R.R. (Recollect and Rank), involves a two-step process: first, using a prompt paradigm for recollecting masked text, and second, scoring and ranking the identified PII candidates based on a new criterion.

**Key Contributions:**

	1. Introduction of R.R., a two-step privacy attack for PII reconstruction
	2. Development of a novel scoring criterion for PII candidates
	3. Demonstration of superior performance in PII identification over existing methods

**Result:** Experiments on three popular PII datasets show that R.R. outperforms existing baselines in identifying PII, demonstrating the vulnerability of LLMs even when training data is scrubbed.

**Limitations:** 

**Conclusion:** R.R. effectively highlights the risks of PII leakage in LLMs and raises awareness about privacy concerns in language models, with the authors providing their code and datasets for public use.

**Abstract:** Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLMs' training data remains challenging. In this paper, we propose R.R. (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the R.R. achieves better PII identification performance than baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release our code and datasets at GitHub.

</details>


### [121] [Retrieval-augmented systems can be dangerous medical communicators](https://arxiv.org/abs/2502.14898)

*Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal*

**Main category:** cs.CL

**Keywords:** Generative AI, Health Information, Retrieval-Augmented Generation, Patient Misconceptions, Communication Pragmatics

**Relevance Score:** 8

**TL;DR:** This paper examines the limitations of generative AI in providing accurate health information, arguing that despite the use of techniques like retrieval-augmented generation, AI responses can mislead patients by decontextualizing facts and perpetuating misconceptions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misleading nature of AI-generated health information despite accurate sourcing, particularly in high-stakes medical contexts where patient understanding is critical.

**Method:** Conducted a large-scale query analysis on disputed diagnoses and procedure safety, combining quantitative and qualitative evidence to assess the effectiveness of current generative AI systems.

**Key Contributions:**

	1. Critiques the reliability of generative AI in conveying health information accurately.
	2. Presents quantitative and qualitative evidence demonstrating the misleading nature of AI responses.
	3. Offers actionable recommendations to enhance AI communication strategies in health.

**Result:** Findings highlight that current AI systems often fail to provide context, omit critical sources, and reinforce patient misconceptions despite generating text without hallucinations.

**Limitations:** The study may not cover all areas of AI-generated content in health and focuses primarily on specific query examples.

**Conclusion:** Recommendations are proposed for improving AI communication in health contexts, emphasizing the need for better source comprehension and consideration of communication pragmatics.

**Abstract:** Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain.

</details>


### [122] [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/abs/2502.15226)

*Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** LLM, user experience, interviews, automation, user feedback

**Relevance Score:** 9

**TL;DR:** The paper introduces CLUE, an LLM-powered interviewer that collects user experience insights post-interaction with LLMs, based on a large study.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To gauge real user opinions on current large language models (LLMs) through automated user interviews.

**Method:** The study involved thousands of users interacting with LLMs followed by interviews conducted by CLUE to capture their opinions.

**Key Contributions:**

	1. Introduction of CLUE, an automated user interview tool for LLMs.
	2. Large-scale user study capturing real-time feedback on LLM interactions.
	3. Insights into user opinions regarding LLM reasoning and information freshness.

**Result:** CLUE successfully gathered user insights, revealing polarized views on LLM reasoning processes and user demands for updated and multimodal information.

**Limitations:** The study may have biases based on user selection and limited to specific LLMs tested.

**Conclusion:** The findings demonstrate the effectiveness of CLUE in capturing nuanced user feedback on LLMs, highlighting key areas of user concern.

**Abstract:** Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer.

</details>


### [123] [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/abs/2502.20122)

*Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought reasoning, fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper proposes a fine-tuning method for large language models to achieve more concise reasoning, reducing the number of output tokens by 30% while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The intent is to reduce the redundant tokens generated during chain-of-thought reasoning in LLMs, which incur unnecessary inference costs.

**Method:** The authors propose simple fine-tuning methods utilizing self-generated concise reasoning paths derived from best-of-N sampling and few-shot conditioning in a task-specific manner.

**Key Contributions:**

	1. Introduction of fine-tuning methods for concise reasoning
	2. Demonstration of 30% output token reduction
	3. Robust application across a variety of LLMs and tasks

**Result:** The method results in a 30% reduction in output tokens on average across five model families tested on GSM8K and MATH, while preserving average accuracy.

**Limitations:** 

**Conclusion:** The self-training approach effectively demonstrates that LLMs can reason more concisely, capitalizing on their stochasticity and in-context learning capabilities to improve efficiency.

**Abstract:** Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning

</details>


### [124] [Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference](https://arxiv.org/abs/2503.04793)

*Wenjie Qiu, Yi-Chen Li, Xuqin Zhang, Tianyi Zhang, Yihang Zhang, Zongzhang Zhang, Yang Yu*

**Main category:** cs.CL

**Keywords:** reward models, language models, human preferences, reinforcement learning, attention mechanism

**Relevance Score:** 9

**TL;DR:** This paper introduces an intermediate-grained reward model for aligning language models with human preferences by assigning scores to sentences rather than entire responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of aligning LLMs with human preferences and address the limitations of existing coarse and token-level reward models.

**Method:** The authors propose a method that assigns scores to sentences in a response, using differential operations on sentence start and end positions, and an attention mechanism to aggregate these scores into a single score for the response.

**Key Contributions:**

	1. Introduction of an intermediate-grained reward model that scores sentences.
	2. Development of a novel attention mechanism to aggregate sentence scores into response-level scores.
	3. Demonstration of improved performance on standard benchmarks for reward modeling and alignment evaluation.

**Result:** The proposed intermediate-grained reward model outperforms traditional response-level models by 2.7% on the RewardBench benchmark and surpasses all baselines on the AlpacaEval alignment evaluation.

**Limitations:** 

**Conclusion:** Segmenting responses into sentences and assigning scores at this level provides a more effective way to model rewards compared to existing methods, contributing positively to the alignment of LLMs with human preferences.

**Abstract:** Learning reward models from human preference datasets and subsequently optimizing language models via reinforcement learning has emerged as a fundamental paradigm for aligning LLMs with human preferences. The performance of the reward model plays a crucial role in the effectiveness of alignment. Previous reward models operate at a coarse-grained level, requiring the generation of a complete response to obtain a reward value. The sparse reward may present challenges for downstream reinforcement learning. While recent efforts have attempted to learn token-level reward models, the lack of explicit semantic information makes it difficult to model the credit of every individual token. In this paper, we propose assigning scores to every sentence, introducing an intermediate-grained reward model. By segmenting the complete response into sentences and applying differential operations to reward output at the start and end positions of each sentence, we can effectively model the rewards of sentences. Moreover, a novel attention mechanism is introduced to aggregate the scores of all sentences into a response-level score, which allows it to be trained using the Bradley-Terry model. On common benchmarks, our method outperforms the response-level reward model by 2.7% on RewardBench (for reward modeling evaluation) and surpasses all baselines on AlpacaEval (for alignment evaluation).

</details>


### [125] [Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection](https://arxiv.org/abs/2503.17739)

*Chatrine Qwaider, Bashar Alhafni, Kirill Chirkunov, Nizar Habash, Ted Briscoe*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Arabic language, Large Language Models, Transformer models, Dataset generation

**Relevance Score:** 8

**TL;DR:** This paper presents a method for generating synthetic Arabic essays to enhance Automated Essay Scoring (AES) systems using Large Language Models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of annotated essay datasets hampers the development of Arabic AES systems, leading to a need for synthetic data generation.

**Method:** Utilizing Large Language Models (LLMs) and Transformer models, the authors generate essays across CEFR proficiency levels and implement two error injection approaches.

**Key Contributions:**

	1. Development of a synthetic Arabic essay dataset with injected errors
	2. Comparison of two error injection methods
	3. Implementation of a BERT-based AES system for Arabic calibrated to CEFR levels

**Result:** The experiments conducted demonstrate that the synthetic dataset significantly improves the performance of a BERT-based Arabic AES system calibrated to CEFR levels.

**Limitations:** The paper may have limitations regarding the generalizability of the synthetic essays to real-world writing.

**Conclusion:** The study successfully creates a dataset of 3,040 annotated essays, showcasing the potential of synthetic data in advancing Arabic language assessment tools.

**Abstract:** Automated Essay Scoring (AES) plays a crucial role in assessing language learners' writing quality, reducing grading workload, and providing real-time feedback. The lack of annotated essay datasets inhibits the development of Arabic AES systems. This paper leverages Large Language Models (LLMs) and Transformer models to generate synthetic Arabic essays for AES. We prompt an LLM to generate essays across the Common European Framework of Reference (CEFR) proficiency levels and introduce and compare two approaches to error injection. We create a dataset of 3,040 annotated essays with errors injected using our two methods. Additionally, we develop a BERT-based Arabic AES system calibrated to CEFR levels. Our experimental results demonstrate the effectiveness of our synthetic dataset in improving Arabic AES performance. We make our code and data publicly available.

</details>


### [126] [Summarizing Speech: A Comprehensive Survey](https://arxiv.org/abs/2504.08024)

*Fabian Retkowski, Maike Z√ºfle, Andreas Sudmann, Dinah Pfau, Shinji Watanabe, Jan Niehues, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** speech summarization, dataset evaluation, multilingual datasets, fine-tuned models, context handling

**Relevance Score:** 7

**TL;DR:** This survey on speech summarization outlines its importance, reviews existing datasets and evaluation methods, and highlights advancements toward more sophisticated models while addressing ongoing challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of speech summarization due to its rising significance in managing spoken content, while clarifying its loose definitions and intersection with various research areas.

**Method:** The paper surveys current techniques in speech summarization, including existing datasets and evaluation protocols, and synthesizes recent developments and shifts towards advanced models.

**Key Contributions:**

	1. Comprehensive overview of speech summarization techniques
	2. Assessment of current datasets and evaluation methods
	3. Identification of ongoing challenges and future directions

**Result:** The survey identifies a need for improved evaluation benchmarks, multilingual datasets, and solutions for managing long-contexts in speech summarization.

**Limitations:** 

**Conclusion:** The paper concludes that despite advances, several challenges persist in the effective evaluation and application of speech summarization technologies.

**Abstract:** Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization remains loosely defined. The field intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation protocols, which are crucial for assessing the quality of summarization approaches, but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions. In doing so, we surface the ongoing challenges, such as the need for realistic evaluation benchmarks, multilingual datasets, and long-context handling.

</details>


### [127] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)

*Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, Ivan Oseledets*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Self-Confidence, Post-Training, Unlabelled Supervision

**Relevance Score:** 9

**TL;DR:** RLSC improves LLM performance using self-generated confidence as reward signals without needing external labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance alignment of large language models with task goals post-training without relying on costly human annotations or external reward models.

**Method:** The proposed method, Reinforcement Learning via Self-Confidence (RLSC), utilizes the model's own confidence as reward signals during post-training to guide learning.

**Key Contributions:**

	1. Introduces a novel self-confidence based reward mechanism for LLMs
	2. Demonstrates significant performance improvements on multiple math benchmarks
	3. Eliminates the need for costly human annotations in training

**Result:** RLSC exhibits significant accuracy improvements: +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23 with limited training data.

**Limitations:** 

**Conclusion:** RLSC serves as a simple and scalable post-training method for inference models, needing only a handful of samples and unlabelled data to improve performance.

**Abstract:** Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.

</details>


### [128] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)

*LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Medical AI, Data Curation

**Relevance Score:** 9

**TL;DR:** This paper presents Lingshu, a medical-specialized Multimodal Large Language Model, addressing current limitations in medical AI applications through enhanced data curation, training methods, and evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to overcome the gaps in medical applications of MLLMs, specifically addressing issues like limited medical knowledge coverage and reasoning capabilities.

**Method:** The authors propose a comprehensive data curation process that gathers rich medical knowledge from imaging and extensive texts, and builds a specialized model (Lingshu) that incorporates this data through multi-stage training and reinforcement learning.

**Key Contributions:**

	1. Introduction of Lingshu, a medical-specialized MLLM
	2. Proposed comprehensive medical data curation procedure
	3. Development of MedEvalKit for standardized model evaluation

**Result:** Lingshu shows significant improvements in performance across three core medical tasks compared to existing multimodal models, indicating effective enhancement in task-solving capabilities and reasoning.

**Limitations:** Potential challenges in data quality and the generalizability of the model in diverse medical scenarios.

**Conclusion:** The development of Lingshu and the associated MedEvalKit provides a step forward in the effective application of MLLMs in the medical domain, demonstrating the efficacy of the proposed methodologies.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...

</details>


### [129] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)

*Prarabdh Shukla, Wei Yin Chong, Yash Patel, Brennan Schaffner, Danish Pruthi, Arjun Bhagoji*

**Main category:** cs.CL

**Keywords:** content moderation, Twitch, AutoMod, hateful content, HCI

**Relevance Score:** 7

**TL;DR:** An audit of Twitch's AutoMod reveals its inability to effectively moderate hateful content, with up to 94% of such messages bypassing the system and excessive blocking of benign messages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of automated content moderation systems, particularly in real-time engagement contexts on platforms like Twitch.

**Method:** The study involved creating test accounts and simulating live chat interactions using Twitch's APIs to send over 107,000 comments, measuring AutoMod's accuracy in flagging hateful content.

**Key Contributions:**

	1. Detailed audit of Twitch's AutoMod effectiveness
	2. Identification of context-related limitations in moderation
	3. Quantitative analysis of flagged versus unflagged content

**Result:** The audit found that a significant portion (up to 94%) of hateful messages bypassed moderation, while contextually added slurs resulted in 100% removal, indicating a reliance on slur signals. Up to 89.5% of benign content was incorrectly blocked.

**Limitations:** The study only focuses on Twitch's AutoMod and may not generalize to other platforms' moderation systems.

**Conclusion:** The findings highlight substantial gaps in AutoMod's moderation capabilities and the necessity for such systems to better understand message context.

**Abstract:** To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch's automated moderation tool ($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch's APIs to send over $107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$. Contextual addition of slurs to these messages results in $100\%$ removal, revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$ blocks up to $89.5\%$ of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in $\texttt{AutoMod}$'s capabilities and underscores the importance for such systems to understand context effectively.

</details>
