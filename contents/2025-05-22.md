# 2025-05-22

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 210]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Designing Semantically-Resonant Abstract Patterns for Data Visualization](https://arxiv.org/abs/2505.14816)

*Zihan Lu, Tingying He, Jiayi Hong, Lijie Yao, Tobias Isenberg*

**Main category:** cs.HC

**Keywords:** semantically-resonant patterns, design methodology, pattern design

**Relevance Score:** 4

**TL;DR:** The paper presents a design methodology for creating semantically-resonant abstract patterns to assist the general public in pattern design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in the systematic framework for developing semantically-resonant abstract patterns, which improve comprehension and speed in chart reading through intuitive design.

**Method:** Conducted workshops with design experts to develop the methodology and evaluated it with non-design participants through additional workshops.

**Key Contributions:**

	1. Development of a structured design methodology for abstract patterns
	2. Workshops with design experts and non-design participants to validate the methodology
	3. Demonstration of effectiveness in enhancing pattern design accessibility

**Result:** The design methodology effectively aids non-design participants in creating semantically-resonant patterns for various concepts.

**Limitations:** 

**Conclusion:** The proposed methodology makes it easier for the general public to design intuitive patterns that resonate semantically with specific concepts.

**Abstract:** We present a structured design methodology for creating semantically-resonant abstract patterns, making the pattern design process accessible to the general public. Semantically-resonant patterns are those that intuitively evoke the concept they represent within a specific set (e.g., in a vegetable concept set, small dots for olives and large dots for tomatoes), analogous to the concept of semantically-resonant colors (e.g., using olive green for olives and red for tomatoes). Previous research has shown that semantically-resonant colors can improve chart reading speed, and designers have made attempts to integrate semantic cues into abstract pattern designs. However, a systematic framework for developing such patterns was lacking. To bridge this gap, we conducted a series of workshops with design experts, resulting in a design methodology that summarizes the methodology for designing semantically-resonant abstract patterns. We evaluated our design methodology through another series of workshops with non-design participants. The results indicate that our proposed design methodology effectively supports the general public in designing semantically-resonant abstract patterns for both abstract and concrete concepts.

</details>


### [2] [Looking for an out: Affordances, uncertainty and collision avoidance behavior of human drivers](https://arxiv.org/abs/2505.14842)

*Leif Johnson, Johan Engström, Aravinda Srinivasan, Ibrahim Özturk, Gustav Markkula*

**Main category:** cs.HC

**Keywords:** collision avoidance, driving simulator, affordances, human behavior, traffic safety

**Relevance Score:** 4

**TL;DR:** The study explores human evasive maneuvers in collision avoidance scenarios using a driving simulator, emphasizing the impact of scenario kinematics and uncertainty on decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of human evasive maneuver decisions and execution in collision avoidance situations, which is critical for enhancing traffic safety and developing advanced driver assistance systems.

**Method:** Conducted a driving simulator study exposing participants to three variants of opposite direction lateral incursion scenarios to analyze collision avoidance behavior.

**Key Contributions:**

	1. Identified key factors influencing human collision avoidance behavior
	2. Introduced the concept of affordances for understanding escape paths
	3. Provided a framework for operationalizing collision avoidance in uncertain scenarios

**Result:** Participants' evasive maneuvers and collision outcomes were significantly influenced by the scenario's kinematics and the uncertainty regarding the other vehicle's trajectory.

**Limitations:** Does not account for all real-world driving conditions; limited sample size in the study.

**Conclusion:** Results can inform computational models of collision avoidance behavior and highlight the relevance of understanding affordances in constructing escape paths.

**Abstract:** Understanding collision avoidance behavior is of key importance in traffic safety research and for designing and evaluating advanced driver assistance systems and autonomous vehicles. While existing experimental work has primarily focused on response timing in traffic conflicts, the goal of the present study was to gain a better understanding of human evasive maneuver decisions and execution in collision avoidance scenarios. To this end, we designed a driving simulator study where participants were exposed to one of three surprising opposite direction lateral incursion (ODLI) scenario variants. The results demonstrated that both the participants' collision avoidance behavior patterns and the collision outcome was strongly determined by the scenario kinematics and, more specifically, by the uncertainty associated with the oncoming vehicle's future trajectory. We discuss pitfalls related to hindsight bias when judging the quality of evasive maneuvers in uncertain situations and suggest that the availability of escape paths in collision avoidance scenarios can be usefully understood based on the notion of affordances, and further demonstrate how such affordances can be operationalized in terms of reachable sets. We conclude by discussing how these results can be used to inform computational models of collision avoidance behavior.

</details>


### [3] [The Pin of Shame: Examining Content Creators' Adoption of Pinning Inappropriate Comments as a Moderation Strategy](https://arxiv.org/abs/2505.14844)

*Yunhee Shim, Shagun Jhaver*

**Main category:** cs.HC

**Keywords:** HCI, content moderation, public shaming, social media, norm enforcement

**Relevance Score:** 5

**TL;DR:** This study investigates the use of the 'Pin of Shame' tactic by content creators on social media, where negative comments are purposely pinned to provoke audience responses and enforce community norms.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how content creators utilize the 'Pin of Shame' moderation technique to handle norm-violating comments and understand the implications of its use.

**Method:** Interviews with 20 content creators who have pinned negative comments to analyze their motivations and the outcomes of using this strategy.

**Key Contributions:**

	1. Insights into the motivations behind the 'Pin of Shame' tactic
	2. Comparison of the effectiveness of public shaming versus other moderation strategies
	3. Recommendations for designing tools for addressing content-based harm in social media

**Result:** The study finds that the 'Pin of Shame' serves multiple purposes, including punishment of inappropriate commenters, fostering emotional accountability, and shaping audience negotiation of community norms, all while contributing to creators' impression management.

**Limitations:** The study is based on a limited sample size of 20 interviews, which may not be representative of the broader population of content creators.

**Conclusion:** The findings highlight both the potential benefits and risks associated with public shaming as a tool for enforcing norms, providing insights for user-centered design in HCI.

**Abstract:** Many social media platforms allow content creators to pin user comments in response to their content. Once pinned, a comment remains fixed at the top of the comments section, regardless of subsequent activity or the selected sorting order. The "Pin of Shame" refers to an innovative re-purposing of this feature, where creators intentionally pin norm-violating comments to spotlight them and prompt shaming responses from their audiences. This study explores how creators adopt this emerging moderation tactic, examining their motivations, its outcomes, and how it compares-procedurally and in effect-to other content moderation strategies. Through interviews with 20 content creators who had pinned negative comments on their posts, we find that the Pin of Shame is used to punish and educate inappropriate commenters, elicit emotional accountability, provoke audience negotiation of community norms, and support creators' impression management goals. Our findings shed light on the benefits, precarities, and risks of using public shaming as a tool for norm enforcement. We contribute to HCI research by informing the design of user-centered tools for addressing content-based harm.

</details>


### [4] [Voice to Vision: Enhancing Civic Decision-Making through Co-Designed Data Infrastructure](https://arxiv.org/abs/2505.14853)

*Maggie Hughes, Cassandra Overney, Ashima Kamra, Jasmin Tepale, Elizabeth Hamby, Mahmood Jasim, Deb Roy*

**Main category:** cs.HC

**Keywords:** Civic decision-making, Participatory design, Transparency, Community engagement, Sociotechnical systems

**Relevance Score:** 4

**TL;DR:** Voice to Vision is a sociotechnical system designed to enhance trust and transparency in civic decision-making by connecting community feedback to planning outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant erosion of trust and transparency in civic decision-making, with community members feeling their feedback is often ignored.

**Method:** We developed a sociotechnical system called Voice to Vision through a five-month iterative design process involving 21 stakeholders, followed by field evaluations with 24 participants.

**Key Contributions:**

	1. A sociotechnical system that bridges community input with planning outputs
	2. Insights into participatory design for civic engagement
	3. Empirical findings on promoting shared understanding among stakeholders

**Result:** The system helps planners compile diverse inputs effectively while allowing community members to see their participation reflected, identify patterns in feedback, and understand decision-making rigor.

**Limitations:** 

**Conclusion:** The study provides insights on participatory design in civic contexts, proposing a complete sociotechnical system for enhancing transparency in decision-making processes.

**Abstract:** Trust and transparency in civic decision-making processes, like neighborhood planning, are eroding as community members frequently report sending feedback "into a void" without understanding how, or whether, their input influences outcomes. To address this gap, we introduce Voice to Vision, a sociotechnical system that bridges community voices and planning outputs through a structured yet flexible data infrastructure and complementary interfaces for both community members and planners. Through a five-month iterative design process with 21 stakeholders and subsequent field evaluation involving 24 participants, we examine how this system facilitates shared understanding across the civic ecosystem. Our findings reveal that while planners value systematic sensemaking tools that find connections across diverse inputs, community members prioritize seeing themselves reflected in the process, discovering patterns within feedback, and observing the rigor behind decisions, while emphasizing the importance of actionable outcomes. We contribute insights into participatory design for civic contexts, a complete sociotechnical system with an interoperable data structure for civic decision-making, and empirical findings that inform how digital platforms can promote shared understanding among elected or appointed officials, planners, and community members by enhancing transparency and legitimacy.

</details>


### [5] [Unremarkable to Remarkable AI Agent: Exploring Boundaries of Agent Intervention for Adults With and Without Cognitive Impairment](https://arxiv.org/abs/2505.14872)

*Mai Lee Chang, Samantha Reig, Alicia, Lee, Anna Huang, Hugo Simão, Nara Han, Neeta M Khanuja, Abdullah Ubed Mohammad Ali, Rebekah Martinez, John Zimmerman, Jodi Forlizzi, Aaron Steinfeld*

**Main category:** cs.HC

**Keywords:** AI agents, older adults, cognitive decline, HCI, social boundaries

**Relevance Score:** 8

**TL;DR:** This study explores the potential of AI agents to support older adults, especially those facing cognitive decline, while revealing social boundaries influencing their acceptance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** There is an increasing need for support systems for older adults to age in place, particularly as cognitive decline becomes more prevalent.

**Method:** Conducted a speed dating with storyboards study to investigate the social boundaries affecting acceptance of AI agents by older adults and caregivers.

**Key Contributions:**

	1. Identified social boundaries preventing acceptance of AI agents by older adults
	2. Outlined design opportunities for making AI agents more acceptable and effective
	3. Articulated directions for future research in HCI and AI for aging populations

**Result:** Healthy older adults fear that accepting AI agents might lead to dementia, yet desire immediate access to personalized agents if cognitive decline occurs. Those in early cognitive decline seek agents to assist them and advocate for their needs.

**Limitations:** 

**Conclusion:** The findings highlight the need for careful design of AI agents that transition from being unremarkable to remarkable and address the fears of older adults.

**Abstract:** As the population of older adults increases, there is a growing need for support for them to age in place. This is exacerbated by the growing number of individuals struggling with cognitive decline and shrinking number of youth who provide care for them. Artificially intelligent agents could provide cognitive support to older adults experiencing memory problems, and they could help informal caregivers with coordination tasks. To better understand this possible future, we conducted a speed dating with storyboards study to reveal invisible social boundaries that might keep older adults and their caregivers from accepting and using agents. We found that healthy older adults worry that accepting agents into their homes might increase their chances of developing dementia. At the same time, they want immediate access to agents that know them well if they should experience cognitive decline. Older adults in the early stages of cognitive decline expressed a desire for agents that can ease the burden they saw themselves becoming for their caregivers. They also speculated that an agent who really knew them well might be an effective advocate for their needs when they were less able to advocate for themselves. That is, the agent may need to transition from being unremarkable to remarkable. Based on these findings, we present design opportunities and considerations for agents and articulate directions of future research.

</details>


### [6] [Towards a Working Definition of Designing Generative User Interfaces](https://arxiv.org/abs/2505.15049)

*Kyungho Lee*

**Main category:** cs.HC

**Keywords:** Generative UI, Human-Computer Interaction, AI-driven design, collaborative workflows, design ethics

**Relevance Score:** 8

**TL;DR:** This study defines Generative UI, exploring its role in AI-driven design workflows through qualitative methods including literature review and expert interviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a clear definition of Generative UI and its implications for interface design in the context of AI collaboration.

**Method:** Multi-method qualitative approach, including a systematic literature review of 127 publications, expert interviews with 18 participants, and analyses of 12 case studies.

**Key Contributions:**

	1. Establishment of a working definition of Generative UI.
	2. Identification of emerging design models such as hybrid creation and AI-assisted refinement.
	3. Insights into ethical challenges and evaluation criteria in Generative UI.

**Result:** Identified five core themes of Generative UI and proposed emerging design models and strategies for effective implementation.

**Limitations:** 

**Conclusion:** The study contributes to theoretical discourse and practical applications in generative UI design, advocating for ethical and effective design practices in HCI.

**Abstract:** Generative UI is transforming interface design by facilitating AI-driven collaborative workflows between designers and computational systems. This study establishes a working definition of Generative UI through a multi-method qualitative approach, integrating insights from a systematic literature review of 127 publications, expert interviews with 18 participants, and analyses of 12 case studies. Our findings identify five core themes that position Generative UI as an iterative and co-creative process. We highlight emerging design models, including hybrid creation, curation-based workflows, and AI-assisted refinement strategies. Additionally, we examine ethical challenges, evaluation criteria, and interaction models that shape the field. By proposing a conceptual foundation, this study advances both theoretical discourse and practical implementation, guiding future HCI research toward responsible and effective generative UI design practices.

</details>


### [7] [Development of Digital Twin Environment through Integration of Commercial Metaverse Platform and IoT Sensors of Smart Building](https://arxiv.org/abs/2505.15089)

*Yusuke Masubuchi, Takefumi Hiraki, Yuichi Hiroi, Masanori Ibara, Kazuki Matsutani, Megumi Zaizen, Junya Morita*

**Main category:** cs.HC

**Keywords:** digital twin, metaverse, IoT, smart buildings, real-time collaboration

**Relevance Score:** 6

**TL;DR:** This paper presents a digital twin environment that integrates smart buildings with a metaverse platform to enhance real-time, multi-user collaboration.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing digital twin solutions in supporting real-time collaboration in smart cities and architectural environments.

**Method:** The proposed system integrates Kajima Corp.'s smart building with the metaverse platform Cluster through a standardized IoT sensor platform, real-time data relay system, and environmental data visualization framework.

**Key Contributions:**

	1. Integration of IoT sensors in a metaverse platform
	2. Real-time data relay for improved collaboration
	3. Quantitative performance metrics in large-scale environments

**Result:** Quantitative measurements demonstrate the system's feasibility for real-world applications in large architectural spaces, enabling innovative collaborative experiences.

**Limitations:** 

**Conclusion:** The framework advances the development of interactive environments that allow collaboration beyond spatial limitations.

**Abstract:** The digital transformation of smart cities and workplaces requires effective integration of physical and cyber spaces, yet existing digital twin solutions remain limited in supporting real-time, multi-user collaboration. While metaverse platforms enable shared virtual experiences, they have not supported comprehensive integration of IoT sensors on physical spaces, especially for large-scale smart architectural environments. This paper presents a digital twin environment that integrates Kajima Corp.'s smart building facility "The GEAR" in Singapore with a commercial metaverse platform Cluster. Our system consists of three key components: a standardized IoT sensor platform, a real-time data relay system, and an environmental data visualization framework. Quantitative end-to-end latency measurements confirm the feasibility of our approach for real-world applications in large architectural spaces. The proposed framework enables new forms of collaboration that transcend spatial constraints, advancing the development of next-generation interactive environments.

</details>


### [8] [AI Solutionism and Digital Self-Tracking with Wearables](https://arxiv.org/abs/2505.15162)

*Hannah R. Nolasco, Andrew Vargo, Koichi Kise*

**Main category:** cs.HC

**Keywords:** self-tracking, automation, user agency, Oura Ring, health decision-making

**Relevance Score:** 8

**TL;DR:** This position paper examines the impacts of automation in self-tracking technologies, particularly focusing on the Oura Ring, and suggests potential remedies for user agency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the neglect of user agency in automated self-tracking systems and explore the implications of using artificial intelligence.

**Method:** The authors relate their findings and observations regarding the Oura Ring, a sleep tracking wearable, as a case study to analyze the effects of automation on user decision-making.

**Key Contributions:**

	1. Analysis of automation's impact on user agency in self-tracking technologies
	2. Case study on the Oura Ring and its implications for health decision-making
	3. Recommendations for redesigning automated systems to support user reflection and independence

**Result:** The paper highlights how automation can diminish user agency and independent reflection in self-tracking applications.

**Limitations:** 

**Conclusion:** It calls for a reconsideration of how self-tracking interventions are designed to ensure they enhance, rather than hinder, user decision-making and agency.

**Abstract:** Self-tracking technologies and wearables automate the process of data collection and insight generation with the support of artificial intelligence systems, with many emerging studies exploring ways to evolve these features further through large-language models (LLMs). This is done with the intent to reduce capture burden and the cognitive stress of health-based decision making, but studies neglect to consider how automation has stymied the agency and independent reflection of users of self-tracking interventions. In this position paper, we explore the consequences of automation in self-tracking by relating it to our experiences with investigating the Oura Ring, a sleep wearable, and navigate potential remedies.

</details>


### [9] [MHANet: Multi-scale Hybrid Attention Network for Auditory Attention Detection](https://arxiv.org/abs/2505.15364)

*Lu Li, Cunhang Fan, Hongyu Zhang, Jingjing Zhang, Xiaoke Yang, Jian Zhou, Zhao Lv*

**Main category:** cs.HC

**Keywords:** Auditory Attention Detection, Multi-Scale Hybrid Attention, Electroencephalography, Spatiotemporal Convolution, Machine Learning

**Relevance Score:** 6

**TL;DR:** A novel multi-scale hybrid attention network (MHANet) for auditory attention detection (AAD) is proposed, leveraging EEG signals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance auditory attention detection in multi-talker environments by incorporating multi-scale contextual information within EEG signals.

**Method:** MHANet features a multi-scale hybrid attention (MHA) module combining channel and global attention mechanisms, along with a spatiotemporal convolution (STC) module for aggregating representations.

**Key Contributions:**

	1. Introduction of a multi-scale hybrid attention mechanism for EEG processing.
	2. Efficient aggregation of spatiotemporal representations via convolution.
	3. State-of-the-art results with reduced model complexity.

**Result:** MHANet achieves state-of-the-art performance with fewer trainable parameters across three datasets, demonstrating effective long-short range spatiotemporal dependency capture.

**Limitations:** 

**Conclusion:** The proposed approach significantly improves AAD performance while being more efficient in terms of model parameters compared to existing methods.

**Abstract:** Auditory attention detection (AAD) aims to detect the target speaker in a multi-talker environment from brain signals, such as electroencephalography (EEG), which has made great progress. However, most AAD methods solely utilize attention mechanisms sequentially and overlook valuable multi-scale contextual information within EEG signals, limiting their ability to capture long-short range spatiotemporal dependencies simultaneously. To address these issues, this paper proposes a multi-scale hybrid attention network (MHANet) for AAD, which consists of the multi-scale hybrid attention (MHA) module and the spatiotemporal convolution (STC) module. Specifically, MHA combines channel attention and multi-scale temporal and global attention mechanisms. This effectively extracts multi-scale temporal patterns within EEG signals and captures long-short range spatiotemporal dependencies simultaneously. To further improve the performance of AAD, STC utilizes temporal and spatial convolutions to aggregate expressive spatiotemporal representations. Experimental results show that the proposed MHANet achieves state-of-the-art performance with fewer trainable parameters across three datasets, 3 times lower than that of the most advanced model. Code is available at: https://github.com/fchest/MHANet.

</details>


### [10] [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)

*Stefan Pasch*

**Main category:** cs.HC

**Keywords:** Large Language Models, Content Moderation, Automated Evaluation

**Relevance Score:** 8

**TL;DR:** This paper investigates how automated models evaluate refusal responses from large language models (LLMs) compared to human users, highlighting a divergence in the assessment of ethical versus technical refusals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models in sensitive applications, understanding how these models handle ethically sensitive prompts is crucial for responsible AI practices and content moderation.

**Method:** The study analyzes response data from Chatbot Arena and compares judgments from two AI evaluators (GPT-4o and Llama 3 70B) regarding different refusal types.

**Key Contributions:**

	1. Analysis of how LLMs refuse sensitive prompts
	2. Comparison of human and AI evaluator responses
	3. Identification of moderation bias in model assessments

**Result:** The findings reveal that LLM-as-a-Judge systems favor ethical refusals more than human users, identifying a moderation bias that could impact the design of automated evaluation systems.

**Limitations:** 

**Conclusion:** The identified moderation bias poses important questions about transparency and value alignment in automated evaluations of AI systems.

**Abstract:** As large language models (LLMs) are increasingly deployed in high-stakes settings, their ability to refuse ethically sensitive prompts-such as those involving hate speech or illegal activities-has become central to content moderation and responsible AI practices. While refusal responses can be viewed as evidence of ethical alignment and safety-conscious behavior, recent research suggests that users may perceive them negatively. At the same time, automated assessments of model outputs are playing a growing role in both evaluation and training. In particular, LLM-as-a-Judge frameworks-in which one model is used to evaluate the output of another-are now widely adopted to guide benchmarking and fine-tuning. This paper examines whether such model-based evaluators assess refusal responses differently than human users. Drawing on data from Chatbot Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how different types of refusals are rated. We distinguish ethical refusals, which explicitly cite safety or normative concerns (e.g., "I can't help with that because it may be harmful"), and technical refusals, which reflect system limitations (e.g., "I can't answer because I lack real-time data"). We find that LLM-as-a-Judge systems evaluate ethical refusals significantly more favorably than human users, a divergence not observed for technical refusals. We refer to this divergence as a moderation bias-a systematic tendency for model-based evaluators to reward refusal behaviors more than human users do. This raises broader questions about transparency, value alignment, and the normative assumptions embedded in automated evaluation systems.

</details>


### [11] [Stress Bytes: Decoding the Associations between Internet Use and Perceived Stress](https://arxiv.org/abs/2505.15377)

*Mohammad Belal, Nguyen Luong, Talayeh Aledavood, Juhi Kulshrestha*

**Main category:** cs.HC

**Keywords:** internet use, stress, well-being, longitudinal study, behavioral markers

**Relevance Score:** 5

**TL;DR:** The paper examines the relationship between internet use and stress through a longitudinal study, revealing contextual associations that could inform tools for self-monitoring online behaviors to improve well-being.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the complex relationship between internet use and well-being, particularly focusing on stress as a factor that influences mental health.

**Method:** A longitudinal multimodal study involving 1490 German internet users, combining web browsing data, sociodemographics, and monthly stress measures over seven months.

**Key Contributions:**

	1. Identification of contextual associations between internet use and stress.
	2. Development of a conceptual framework for studying internet use.
	3. Potential to inform tools to self-moderate online behaviors.

**Result:** The analysis found that social media, entertainment, online shopping, and gaming were positively associated with stress, while productivity, news, and adult content use were negatively associated.

**Limitations:** 

**Conclusion:** The findings suggest that behavioral markers of internet use can support the development of individualized tools for enhancing online well-being and alleviating stress.

**Abstract:** In today's digital era, internet plays a pervasive role in our lives, influencing everyday activities such as communication, work, and leisure. This online engagement intertwines with offline experiences, shaping individuals' overall well-being. Despite its significance, existing research often falls short in capturing the relationship between internet use and well-being, relying primarily on isolated studies and self-reported data. One of the major contributors to deteriorated well-being - both physical and mental - is stress. While some research has examined the relationship between internet use and stress, both positive and negative associations have been reported. Our primary goal in this work is to identify the associations between an individual's internet use and their stress. For achieving our goal, we conducted a longitudinal multimodal study that spanned seven months. We combined fine-grained URL-level web browsing traces of 1490 German internet users with their sociodemographics and monthly measures of stress. Further, we developed a conceptual framework that allows us to simultaneously explore different contextual dimensions, including how, where, when, and by whom the internet is used. Our analysis revealed several associations between internet use and stress that vary by context. Social media, entertainment, online shopping, and gaming were positively associated with stress, while productivity, news, and adult content use were negatively associated. In the future, the behavioral markers we identified can pave the way for designing individualized tools for people to self-monitor and self-moderate their online behaviors to enhance their well-being, reducing the burden on already overburdened mental health services.

</details>


### [12] [What Is Serendipity? An Interview Study to Conceptualize Experienced Serendipity in Recommender Systems](https://arxiv.org/abs/2505.15440)

*Brett Binst, Lien Michiels, Annelien Smets*

**Main category:** cs.HC

**Keywords:** serendipity, recommender systems, user experience, grounded theory, framework

**Relevance Score:** 7

**TL;DR:** This paper conceptualizes user experience of serendipity in recommender systems through interviews and proposes a unified framework highlighting key components necessary for serendipitous experiences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the conceptual ambiguity of serendipity in recommender systems, which has led to inconsistent operationalizations across studies, making synthesis and comparison difficult.

**Method:** Interviews with 17 participants were conducted and analyzed using the grounded theory paradigm to understand user experiences of serendipity.

**Key Contributions:**

	1. Conceptualization of experienced serendipity in recommender systems
	2. Identification of necessary components for serendipitous experiences
	3. Development of a framework unifying previous definitions of serendipity

**Result:** The study conceptualizes experienced serendipity as a user experience defined by three necessary components: fortuitous, refreshing, and enriching, and identifies various conditions under which these can be satisfied.

**Limitations:** 

**Conclusion:** This work helps unify previous definitions, offers insights into users' experiences, and can guide the design of recommender systems to promote serendipity while laying groundwork for standardized operationalization.

**Abstract:** Serendipity has been associated with numerous benefits in the context of recommender systems, e.g., increased user satisfaction and consumption of long-tail items. Despite this, serendipity in the context of recommender systems has thus far remained conceptually ambiguous. This conceptual ambiguity has led to inconsistent operationalizations between studies, making it difficult to compare and synthesize findings. In this paper, we conceptualize the user's experience of serendipity. To this effect, we interviewed 17 participants and analyzed the data following the grounded theory paradigm. Based on these interviews, we conceptualize experienced serendipity as "a user experience in which a user unintentionally encounters content that feels fortuitous, refreshing, and enriching". We find that all three components -- fortuitous, refreshing and enriching -- are necessary and together are sufficient to classify a user's experience as serendipitous. However, these components can be satisfied through a variety of conditions. Our conceptualization unifies previous definitions of serendipity within a single framework, resolving inconsistencies by identifying distinct flavors of serendipity. It highlights underexposed flavors, offering new insights into how users experience serendipity in the context of recommender systems. By clarifying the components and conditions of experienced serendipity in recommender systems, this work can guide the design of recommender systems that stimulate experienced serendipity in their users, and lays the groundwork for developing a standardized operationalization of experienced serendipity in its many flavors, enabling more consistent and comparable evaluations.

</details>


### [13] [Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use](https://arxiv.org/abs/2505.15596)

*Xinyi Lu, Aditya Mahesh, Zejia Shen, Mitchell Dudley, Larissa Sano, Xu Wang*

**Main category:** cs.HC

**Keywords:** AI-generated feedback, Teaching Assistants, Economics class, Human-Computer Interaction, Feedback quality

**Relevance Score:** 8

**TL;DR:** This project investigates AI-generated feedback for human instructors, focusing on teaching assistants' perspectives on its quality and utility in an Economics class context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and quality of feedback provided by human instructors, particularly teaching assistants, in grading student essays.

**Method:** An LLM-powered feedback engine was developed to generate feedback based on grading rubrics for Economics essays. Think-aloud studies were conducted with 5 TAs who evaluated and compared AI-generated feedback with their own handwritten comments during grading.

**Key Contributions:**

	1. Introduction of an LLM-powered feedback engine for essay grading
	2. Insights into TAs' perceptions of AI feedback quality
	3. Methods to improve AI feedback through decomposed tasks and detailed rubrics

**Result:** TAs found that AI-generated feedback could expedite grading, enhance consistency, and improve feedback quality, particularly when detailed rubrics were provided for better AI performance.

**Limitations:** 

**Conclusion:** AI feedback suggestions can be valuable tools for TAs, promoting efficiency and quality in grading when structured appropriately.

**Abstract:** This project examines the prospect of using AI-generated feedback as suggestions to expedite and enhance human instructors' feedback provision. In particular, we focus on understanding the teaching assistants' perspectives on the quality of AI-generated feedback and how they may or may not utilize AI feedback in their own workflows. We situate our work in a foundational college Economics class, which has frequent short essay assignments. We developed an LLM-powered feedback engine that generates feedback on students' essays based on grading rubrics used by the teaching assistants (TAs). To ensure that TAs can meaningfully critique and engage with the AI feedback, we had them complete their regular grading jobs. For a randomly selected set of essays that they had graded, we used our feedback engine to generate feedback and displayed the feedback as in-text comments in a Word document. We then performed think-aloud studies with 5 TAs over 20 1-hour sessions to have them evaluate the AI feedback, contrast the AI feedback with their handwritten feedback, and share how they envision using the AI feedback if they were offered as suggestions. The study highlights the importance of providing detailed rubrics for AI to generate high-quality feedback for knowledge-intensive essays. TAs considered that using AI feedback as suggestions during their grading could expedite grading, enhance consistency, and improve overall feedback quality. We discuss the importance of decomposing the feedback generation task into steps and presenting intermediate results, in order for TAs to use the AI feedback.

</details>


### [14] [Exploring the Innovation Opportunities for Pre-trained Models](https://arxiv.org/abs/2505.15790)

*Minjung Park, Jodi Forlizzi, John Zimmerman*

**Main category:** cs.HC

**Keywords:** pre-trained models, AI innovation, HCI, interaction design, opportunity domains

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of pre-trained models on AI innovation, particularly focusing on applications developed by HCI researchers to identify successful use cases for these models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand where pre-trained models can succeed in meeting user needs and support AI innovation amidst the hype surrounding them.

**Method:** The authors employed an artifact analysis approach to categorize applications based on their capabilities, opportunity domains, data types, and interaction design patterns.

**Key Contributions:**

	1. Categorization of pre-trained model applications based on capabilities, data types, and interaction design patterns.
	2. Identification of opportunity domains for innovation in AI using pre-trained models.
	3. Analysis of ethical considerations in HCI applications of AI.

**Result:** The study identified several opportunity domains for innovation with pre-trained models, showcasing technical capabilities and real user needs while navigating ethical challenges.

**Limitations:** 

**Conclusion:** By analyzing HCI research applications, the paper reveals insights on the practical use of pre-trained models and highlights areas ripe for further innovation in AI applications.

**Abstract:** Innovators transform the world by understanding where services are successfully meeting customers' needs and then using this knowledge to identify failsafe opportunities for innovation. Pre-trained models have changed the AI innovation landscape, making it faster and easier to create new AI products and services. Understanding where pre-trained models are successful is critical for supporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained models makes it hard to know where AI can really be successful. To address this, we investigated pre-trained model applications developed by HCI researchers as a proxy for commercially successful applications. The research applications demonstrate technical capabilities, address real user needs, and avoid ethical challenges. Using an artifact analysis approach, we categorized capabilities, opportunity domains, data types, and emerging interaction design patterns, uncovering some of the opportunity space for innovation with pre-trained models.

</details>


### [15] [TinyClick: Single-Turn Agent for Empowering GUI Automation](https://arxiv.org/abs/2410.11871)

*Pawel Pawlowski, Krystian Zawistowski, Wojciech Lapacz, Adam Wiacek, Marcin Skorupa, Sebastien Postansque, Jakub Hoscilowicz*

**Main category:** cs.HC

**Keywords:** UI agent, Vision-Language Model, multi-task training, data augmentation, computer resources

**Relevance Score:** 8

**TL;DR:** A UI agent using the Vision-Language Model Florence-2-Base effectively identifies UI elements from user commands with low computational requirements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to create a more inclusive and sustainable approach to researching UI agents by reducing the need for expensive compute resources and manually annotated data.

**Method:** The authors employed a vision-specific multi-task training approach along with MLLM-based data augmentation to enhance the model's performance.

**Key Contributions:**

	1. Demonstration of strong performance in identifying UI elements with a lightweight model.
	2. Introduction of effective multi-task training and data augmentation techniques.
	3. Highlighting the potential for more sustainable research practices in UI agent development.

**Result:** The UI agent showed strong performance on Screenspot and OmniAct annotations while being lightweight at 0.27B parameters and achieving minimal latency.

**Limitations:** 

**Conclusion:** The findings indicate that effective UI interaction can be achieved with reduced compute budgets, enabling broader research opportunities in the field of UI agents.

**Abstract:** We present an UI agent for user interface (UI) interaction tasks, using Vision-Language Model Florence-2-Base. The agent's primary task is identifying the screen coordinates of the UI element corresponding to the user's command. It demonstrates very strong performance on Screenspot and OmniAct annotations, while maintaining a very small size of 0.27B parameters and minimal latency. Moreover, training needs small compute budget of 56 GPU-hours (worth about 40 USD). Relevant improvement comes from vision-specific multi-task training and MLLM-based data augmentation. We hope that decreased needs for expensive compute resources and manually annotated data will allow to facilitate more inclusive and sustainable research of UI agents.

</details>


### [16] [Perceptions of Blind Adults on Non-Visual Mobile Text Entry](https://arxiv.org/abs/2410.22324)

*Dylan Gaines, Keith Vertanen*

**Main category:** cs.HC

**Keywords:** mobile text input, blind, non-visual interface, human-computer interaction, assistive technology

**Relevance Score:** 7

**TL;DR:** This paper explores the challenges faced by blind individuals in mobile text input and suggests improvements based on participant interviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the challenges blind individuals face with text input on mobile devices without physical keys and to propose interface improvements.

**Method:** In-depth interviews with 12 blind adults about their experiences and difficulties in mobile text input, followed by discussions on an experimental non-visual text input method.

**Key Contributions:**

	1. Insights into the experiences of blind individuals using mobile text input
	2. Identification of major challenges in current mobile text input methods
	3. Proposition of five future directions for improving non-visual text input methods

**Result:** Identified key challenges including poor dictation accuracy, difficulties in noisy environments, and error correction issues. Majority prefer manual typing over word predictions.

**Limitations:** 

**Conclusion:** Improvements are needed in dictation technology, audio feedback, error correction, and the introduction of new text input methods that are easier to learn.

**Abstract:** Text input on mobile devices without physical keys can be challenging for people who are blind or low-vision. We interview 12 blind adults about their experiences with current mobile text input to provide insights into what sorts of interface improvements may be the most beneficial. We identify three primary themes that were experiences or opinions shared by participants: the poor accuracy of dictation, difficulty entering text in noisy environments, and difficulty correcting errors in entered text. We also discuss an experimental non-visual text input method with each participant to solicit opinions on the method and probe their willingness to learn a novel method. We find that the largest concern was the time required to learn a new technique. We find that the majority of our participants do not use word predictions while typing but instead find it faster to finish typing words manually. Finally, we distill five future directions for non-visual text input: improved dictation, less reliance on or improved audio feedback, improved error correction, reducing the barrier to entry for new methods, and more fluid non-visual word predictions.

</details>


### [17] [Identifying the Desired Word Suggestion in Simultaneous Audio](https://arxiv.org/abs/2501.10568)

*Dylan Gaines, Keith Vertanen*

**Main category:** cs.HC

**Keywords:** human-computer interaction, auditory suggestion, predictive keyboard, user accuracy, assistive technology

**Relevance Score:** 8

**TL;DR:** This paper presents a method for enhancing word suggestions in non-visual text input using simultaneous voices, showing how slight delays can improve user accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of improving user interactions with predictive keyboards by exploring auditory suggestions for non-visual input.

**Method:** Two perceptual studies were conducted to assess user detection of spoken word suggestions under different voice presentation conditions, with a focus on simultaneous versus sequential playback.

**Key Contributions:**

	1. Demonstration of simultaneous voice presentations for text input suggestions
	2. Evaluation of the impact of delays on user accuracy
	3. Finding that simultaneous playback can be faster without sacrificing accuracy

**Result:** Findings indicate that simultaneous presentation of two voices can be as effective as sequential playback when a slight delay is introduced, allowing for faster suggestions without significant accuracy loss.

**Limitations:** The study focuses on specific word sets and may not generalize to all types of text input.

**Conclusion:** The introduction of a brief delay between simultaneous word presentations can enhance the efficiency of word suggestion systems in predictive keyboards, facilitating quicker text input.

**Abstract:** We explore a method for presenting word suggestions for non-visual text input using simultaneous voices. We conduct two perceptual studies and investigate the impact of different presentations of voices on a user's ability to detect which voice, if any, spoke their desired word. Our sets of words simulated the word suggestions of a predictive keyboard during real-world text input. We find that when voices are simultaneous, user accuracy decreases significantly with each added word suggestion. However, adding a slight 0.15 s delay between the start of each subsequent word allows two simultaneous words to be presented with no significant decrease in accuracy compared to presenting two words sequentially (84% simultaneous versus 86% sequential). This allows two word suggestions to be presented to the user 32% faster than sequential playback without decreasing accuracy.

</details>


### [18] [OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable Attitude and Behavior Change](https://arxiv.org/abs/2502.02863)

*Pat Pataranutaporn, Alexander Doudkin, Pattie Maes*

**Main category:** cs.HC

**Keywords:** Marine Ecosystems, Large Language Models, Conversational AI, Environmental Education, Behavioral Change

**Relevance Score:** 4

**TL;DR:** OceanChat is an interactive system using AI to promote environmental behavior through dialogue with animated marine creatures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional environmental education struggles to translate awareness into sustained behavioral change, necessitating new approaches to engage and instill long-term environmental practices.

**Method:** A between-subjects experiment with 900 participants comparing three conditions: Static Scientific Information, Static Character Narrative, and Conversational Character Narrative, which enabled real-time dialogue with AI characters.

**Key Contributions:**

	1. Introduced OceanChat as an innovative approach to environmental education using AI.
	2. Demonstrated the effectiveness of interactive narratives in promoting behavioral change.
	3. Provided design principles for creating engaging AI characters in sustainability education.

**Result:** Conversational Character Narrative significantly increased behavioral intentions and sustainable choices compared to static approaches; the beluga whale character elicited stronger emotional engagement.

**Limitations:** The paper notes limited impact on deeper measures like climate policy support and the complexity of shifting entrenched beliefs.

**Conclusion:** OceanChat illustrates how emotionally resonant, context-aware AI narratives can facilitate environmental behavior change, though challenges remain in addressing deeper beliefs.

**Abstract:** Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures -- specifically a beluga whale, a jellyfish, and a seahorse -- designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change.

</details>


### [19] [Influence of prior and task generated emotions on XAI explanation retention and understanding](https://arxiv.org/abs/2505.10427)

*Birte Richter, Christian Schütze, Anna Aksonova, Britta Wrede*

**Main category:** cs.HC

**Keywords:** human-computer interaction, decision support systems, explanation of AI, emotions and decision-making, feature relevance

**Relevance Score:** 7

**TL;DR:** This study investigates how emotions influence the explanation process in decision support systems (DSS), particularly focusing on understanding and retention of feature relevance after inducing different emotions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the role of social factors, particularly emotions, in the explanation process of decision support systems, addressing a gap in existing research.

**Method:** Participants were induced with happiness or fear before interacting with a DSS. User characteristics were assessed via a questionnaire, and emotional reactions were measured using heart rate variability, facial expressions, and self-reports while retaining and understanding explanations of feature relevance.

**Key Contributions:**

	1. Investigated the influence of prior emotions on understanding feature relevance in DSS explanations.
	2. Demonstrated confirmation bias induced by emotions in processing explanations.
	3. Provided insights into individual emotional reactions affecting decision-making tasks.

**Result:** Findings suggest prior emotions affect understanding of certain features, revealing that unrelated emotions may lead to confirmation bias and emotional arousal can affect perception of feature importance.

**Limitations:** Focused on specific emotions (happiness and fear), which may not generalize to other emotional states; the sample size and diversity of participants may also limit generalizability.

**Conclusion:** Prior emotions can significantly influence how users process and understand explanations in decision support systems, highlighting the need to consider emotional factors in the explanation of AI results.

**Abstract:** The explanation of AI results and how they are received by users is an increasingly active research field. However, there is a surprising lack of knowledge about how social factors such as emotions affect the process of explanation by a decision support system (DSS). While previous research has shown effects of emotions on DSS supported decision-making, it remains unknown in how far emotions affect cognitive processing during an explanation. In this study, we, therefore, investigated the influence of prior emotions and task-related arousal on the retention and understanding of explained feature relevance. To investigate the influence of prior emotions, we induced happiness and fear prior to the decision support interaction. Before emotion induction, user characteristics to assess their risk type were collected via a questionnaire. To identify emotional reactions to the explanations of the relevance of different features, we observed heart rate variability (HRV), facial expressions, and self-reported emotions of the explainee while observing and listening to the explanation and assessed their retention of the features as well as their influence on the outcome of the decision task. Results indicate that (1) task-unrelated prior emotions do not affected the ratantion but may affect the understanding of the relevance of certain features in the sense of an emotion-induced confirmation bias, (2) certain features related to personal attitudes yielded arousal in individual participants, (3) this arousal affected the understanding of these variables.

</details>


### [20] [Emotion-sensitive Explanation Model](https://arxiv.org/abs/2505.10454)

*Christian Schütze, Birte Richter, Britta Wrede*

**Main category:** cs.HC

**Keywords:** Explainable AI, emotions, user understanding, decision-making, adaptive systems

**Relevance Score:** 7

**TL;DR:** This paper proposes a three-stage model for emotion-sensitive explanations in Explainable AI (XAI), addressing how emotional factors affect user understanding and decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the impact of emotional factors on the understanding and processing of explanations in XAI, which has largely focused on rational users.

**Method:** The paper presents a three-stage model consisting of emotional or epistemic arousal, understanding, and agreement for developing emotion-sensitive XAI systems.

**Key Contributions:**

	1. Introduction of a three-stage model for emotion-sensitive explanations
	2. Emphasis on the role of emotions in understanding AI explanations
	3. Framework for adaptive XAI systems based on user emotions

**Result:** The proposed model aims to improve user-centered decision-making by dynamically adapting explanation strategies to the user's emotional state.

**Limitations:** 

**Conclusion:** Integrating emotional considerations into XAI can enhance user experience and support better decision-making outcomes.

**Abstract:** Explainable AI (XAI) research has traditionally focused on rational users, aiming to improve understanding and reduce cognitive biases. However, emotional factors play a critical role in how explanations are perceived and processed. Prior work shows that prior and task-generated emotions can negatively impact the understanding of explanation. Building on these insights, we propose a three-stage model for emotion-sensitive explanation grounding: (1) emotional or epistemic arousal, (2) understanding, and (3) agreement. This model provides a conceptual basis for developing XAI systems that dynamically adapt explanation strategies to users emotional states, ultimately supporting more effective and user-centered decision-making.

</details>


### [21] [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)

*Stefan Pasch*

**Main category:** cs.HC

**Keywords:** large language models, content moderation, ethics, automated evaluation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper explores the differences in how automated evaluators (LLM-as-a-Judge frameworks) and human users assess refusal responses by large language models (LLMs), highlighting a moderation bias favoring ethical refusals over technical ones.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are used in sensitive contexts, understanding how refusal responses are perceived is crucial for content moderation and AI ethics.

**Method:** The study compares evaluations of refusal responses by two AI judges (GPT-4o and Llama 3 70B) against human judgments, focusing on ethical and technical refusals.

**Key Contributions:**

	1. Identification of moderation bias in LLM evaluations.
	2. Comparison of ethical and technical refusal assessments between AI judges and human users.
	3. Insights into the implications of automated evaluators in AI ethics and content moderation.

**Result:** Automated evaluators rate ethical refusals more favorably than human users, indicating a systematic moderation bias.

**Limitations:** 

**Conclusion:** The divergence in evaluation between LLMs and humans raises important questions about transparency and value alignment in automated assessment systems.

**Abstract:** As large language models (LLMs) are increasingly deployed in high-stakes settings, their ability to refuse ethically sensitive prompts-such as those involving hate speech or illegal activities-has become central to content moderation and responsible AI practices. While refusal responses can be viewed as evidence of ethical alignment and safety-conscious behavior, recent research suggests that users may perceive them negatively. At the same time, automated assessments of model outputs are playing a growing role in both evaluation and training. In particular, LLM-as-a-Judge frameworks-in which one model is used to evaluate the output of another-are now widely adopted to guide benchmarking and fine-tuning. This paper examines whether such model-based evaluators assess refusal responses differently than human users. Drawing on data from Chatbot Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how different types of refusals are rated. We distinguish ethical refusals, which explicitly cite safety or normative concerns (e.g., "I can't help with that because it may be harmful"), and technical refusals, which reflect system limitations (e.g., "I can't answer because I lack real-time data"). We find that LLM-as-a-Judge systems evaluate ethical refusals significantly more favorably than human users, a divergence not observed for technical refusals. We refer to this divergence as a moderation bias-a systematic tendency for model-based evaluators to reward refusal behaviors more than human users do. This raises broader questions about transparency, value alignment, and the normative assumptions embedded in automated evaluation systems.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Addressing the Challenges of Planning Language Generation](https://arxiv.org/abs/2505.14763)

*Prabhu Prakash Kagitha, Andrew Zhu, Li Zhang*

**Main category:** cs.CL

**Keywords:** LLMs, PDDL, planning, symbolic solvers, open-source models

**Relevance Score:** 8

**TL;DR:** This paper evaluates different methods for using open-source LLMs to generate PDDL for planning tasks, finding that certain inference-time scaling approaches significantly improve performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of open-source LLMs in generating formal planning languages and to evaluate their effectiveness in plan derivation compared to closed-source models.

**Method:** Eight different pipelines for generating PDDL were designed and evaluated using open-source models under 50 billion parameters, assessing various approaches to understand their impact on performance.

**Key Contributions:**

	1. Evaluation of 8 different PDDL generation pipelines with open-source models
	2. Identification of ineffective methods (high-resource wrappers, constrained decoding)
	3. Demonstration of significant performance improvements through feedback revision methods

**Result:** The study found that high-resource language wrappers and constrained decoding negatively impacted performance, while inference-time scaling methods, particularly revision with solver feedback, greatly enhanced outcome effectiveness, more than doubling performance.

**Limitations:** 

**Conclusion:** Open-source LLMs can be effectively utilized for planning language generation if appropriate methods such as feedback revision are adopted.

**Abstract:** Using LLMs to generate formal planning languages such as PDDL that invokes symbolic solvers to deterministically derive plans has been shown to outperform generating plans directly. While this success has been limited to closed-sourced models or particular LLM pipelines, we design and evaluate 8 different PDDL generation pipelines with open-source models under 50 billion parameters previously shown to be incapable of this task. We find that intuitive approaches such as using a high-resource language wrapper or constrained decoding with grammar decrease performance, yet inference-time scaling approaches such as revision with feedback from the solver and plan validator more than double the performance.

</details>


### [23] [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)

*Richard Khoury, Maxence Verhaverbeke, Julie A. Gramaccia*

**Main category:** cs.CL

**Keywords:** 5W1H, news articles, automated extraction, French language, NLP

**Relevance Score:** 4

**TL;DR:** This paper presents an automated pipeline for extracting 5W1H information from French news articles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve clarity and systematic presentation of news events by automating the extraction of essential information that supports various tasks like summarization and news aggregation.

**Method:** The authors designed an automated extraction pipeline specifically for French news articles, which leverages a newly created corpus of 250 articles annotated with 5W1H information.

**Key Contributions:**

	1. First automated extraction pipeline for 5W1H from French news articles.
	2. Creation of a 250-article corpus with marked 5W1H answers for evaluation.
	3. Performance demonstration against GPT-4o.

**Result:** The evaluation shows that the proposed pipeline achieves performance comparable to the large language model GPT-4o in extracting 5W1H information.

**Limitations:** 

**Conclusion:** Automating the extraction of 5W1H questions can facilitate better information processing in the field of journalism and related applications.

**Abstract:** The 5W1H questions -- who, what, when, where, why and how -- are commonly used in journalism to ensure that an article describes events clearly and systematically. Answering them is a crucial prerequisites for tasks such as summarization, clustering, and news aggregation. In this paper, we design the first automated extraction pipeline to get 5W1H information from French news articles. To evaluate the performance of our algo- rithm, we also create a corpus of 250 Quebec news articles with 5W1H answers marked by four human annotators. Our results demonstrate that our pipeline performs as well in this task as the large language model GPT-4o.

</details>


### [24] [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)

*Tingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, Yu Cheng*

**Main category:** cs.CL

**Keywords:** instruction-following, mathematical reasoning, large language models, benchmark, reasoning performance

**Relevance Score:** 8

**TL;DR:** The paper presents MathIF, a benchmark for evaluating instruction-following in mathematical reasoning tasks, highlighting a tension between reasoning capacity and instruction adherence in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the inadequacy of large language models (LLMs) in adhering to natural language instructions despite their reasoning capabilities.

**Method:** Introduction of the MathIF benchmark to empirically assess instruction-following in models on mathematical reasoning tasks.

**Key Contributions:**

	1. Introduction of the MathIF benchmark for instruction-following tasks
	2. Identification of the trade-off between reasoning effectiveness and instruction adherence
	3. Demonstration of simple interventions to recover instruction adherence at the cost of reasoning performance.

**Result:** The analysis shows that increased reasoning capacity often leads to poorer adherence to instructions, especially with longer generation outputs.

**Limitations:** 

**Conclusion:** Models need to be more instruction-aware in their training to balance reasoning performance with adherence to user directives.

**Abstract:** Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.

</details>


### [25] [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)

*Mingyang Wang, Lukas Lange, Heike Adel, Yunpu Ma, Jannik Strötgen, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** language models, multilingual reasoning, language mixing, RLMs, chain-of-thought

**Relevance Score:** 7

**TL;DR:** This paper studies language mixing in reasoning language models, revealing its impact on performance and suggesting methods for optimization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how language mixing affects the performance of reasoning language models and exploring ways to optimize their multilingual capabilities.

**Method:** A systematic analysis of language mixing in RLMs across 15 languages, evaluating its patterns, impacts, and causes under varying task difficulties and subject areas.

**Key Contributions:**

	1. First systematic study of language mixing in RLMs
	2. Analysis of how task complexity and subject areas affect language mixing
	3. Demonstration that constrained reasoning in certain languages improves accuracy.

**Result:** Results indicate that both the complexity of tasks and the language used for reasoning affect performance, with specific scripts improving accuracy significantly.

**Limitations:** 

**Conclusion:** The research highlights the significance of script choice in RLMs and offers insights into optimizing multilingual reasoning for better performance and interpretability.

**Abstract:** Reasoning language models (RLMs) excel at complex tasks by leveraging a chain-of-thought process to generate structured intermediate steps. However, language mixing, i.e., reasoning steps containing tokens from languages other than the prompt, has been observed in their outputs and shown to affect performance, though its impact remains debated. We present the first systematic study of language mixing in RLMs, examining its patterns, impact, and internal causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and show how all three factors influence language mixing. Moreover, we demonstrate that the choice of reasoning language significantly affects performance: forcing models to reason in Latin or Han scripts via constrained decoding notably improves accuracy. Finally, we show that the script composition of reasoning traces closely aligns with that of the model's internal representations, indicating that language mixing reflects latent processing preferences in RLMs. Our findings provide actionable insights for optimizing multilingual reasoning and open new directions for controlling reasoning languages to build more interpretable and adaptable RLMs.

</details>


### [26] [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)

*Leon Lin, Jun Zheng, Haidong Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, storytelling, benchmark, narrative generation, evaluation

**Relevance Score:** 8

**TL;DR:** WebNovelBench is a new benchmark designed to evaluate long-form storytelling by Large Language Models using a dataset of Chinese web novels.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in evaluating the storytelling capabilities of LLMs, which have been inadequately assessed by existing benchmarks.

**Method:** Introduced a dataset of over 4,000 Chinese web novels and a framework that evaluates narrative quality across eight dimensions using an LLM-as-Judge approach with scores aggregated through PCA.

**Key Contributions:**

	1. Introduction of WebNovelBench for long-form storytelling evaluation
	2. Assessment of LLMs using a novel multi-faceted framework
	3. Comprehensive analysis of 24 state-of-the-art LLMs

**Result:** WebNovelBench effectively differentiates between human-written masterpieces and LLM-generated content, demonstrating its capability to rank 24 state-of-the-art LLMs based on their storytelling abilities.

**Limitations:** 

**Conclusion:** This benchmark offers a scalable, replicable, and data-driven methodology for advancing LLM-driven narrative generation.

**Abstract:** Robustly evaluating the long-form storytelling capabilities of Large Language Models (LLMs) remains a significant challenge, as existing benchmarks often lack the necessary scale, diversity, or objective measures. To address this, we introduce WebNovelBench, a novel benchmark specifically designed for evaluating long-form novel generation. WebNovelBench leverages a large-scale dataset of over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story generation task. We propose a multi-faceted framework encompassing eight narrative quality dimensions, assessed automatically via an LLM-as-Judge approach. Scores are aggregated using Principal Component Analysis and mapped to a percentile rank against human-authored works. Our experiments demonstrate that WebNovelBench effectively differentiates between human-written masterpieces, popular web novels, and LLM-generated content. We provide a comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling abilities and offering insights for future development. This benchmark provides a scalable, replicable, and data-driven methodology for assessing and advancing LLM-driven narrative generation.

</details>


### [27] [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)

*Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Felicia Körner, Ercong Nie, Barbara Plank, François Yvon, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** Large Language Models, factual recall, crosslingual consistency, pretraining, multilingual

**Relevance Score:** 9

**TL;DR:** This paper investigates how factual recall and crosslingual consistency of Large Language Models evolve during pretraining, using OLMo-7B as a case study.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the development of factual recall and crosslingual consistency in Large Language Models during pretraining, which has been largely neglected in prior studies.

**Method:** The study traces the evolution of factual recall and crosslingual consistency over time for the OLMo-7B model, analyzing the effects of fact frequency in the pretraining corpus on accuracy and consistency.

**Key Contributions:**

	1. Provides insights into the evolution of multilingual factual knowledge in LLMs during pretraining.
	2. Identifies frequency-driven learning and crosslingual transfer as key mechanisms of factual recall.
	3. Releases code and data for further research, promoting transparency and collaboration.

**Result:** Both accuracy and consistency improve over time for most languages, primarily driven by the frequency of facts in the pretraining corpus. Some low-frequency facts in non-English languages can still be recalled correctly, benefiting from crosslingual transfer.

**Limitations:** The study focuses primarily on OLMo-7B and may not be generalizable to all LLMs.

**Conclusion:** The research identifies two pathways of multilingual factual knowledge acquisition: frequency-driven learning and crosslingual transfer, the latter being limited in scale and specific to certain relation types.

**Abstract:** Large Language Models (LLMs) are capable of recalling multilingual factual knowledge present in their pretraining data. However, most studies evaluate only the final model, leaving the development of factual recall and crosslingual consistency throughout pretraining largely unexplored. In this work, we trace how factual recall and crosslingual consistency evolve during pretraining, focusing on OLMo-7B as a case study. We find that both accuracy and consistency improve over time for most languages. We show that this improvement is primarily driven by the fact frequency in the pretraining corpus: more frequent facts are more likely to be recalled correctly, regardless of language. Yet, some low-frequency facts in non-English languages can still be correctly recalled. Our analysis reveals that these instances largely benefit from crosslingual transfer of their English counterparts -- an effect that emerges predominantly in the early stages of pretraining. We pinpoint two distinct pathways through which multilingual factual knowledge acquisition occurs: (1) frequency-driven learning, which is dominant and language-agnostic, and (2) crosslingual transfer, which is limited in scale and typically constrained to relation types involving named entities. We release our code and data to facilitate further research at https://github.com/cisnlp/multilingual-fact-tracing.

</details>


### [28] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)

*Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao*

**Main category:** cs.CL

**Keywords:** LLM, autoregressive generation, Bayesian estimation

**Relevance Score:** 9

**TL;DR:** Proposes Mixture of Inputs (MoI) to enhance LLM autoregressive generation by blending token outputs with distribution information.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Standard autoregressive methods discard rich distribution information after generating tokens, limiting the model's internal representation.

**Method:** Introduces a Bayesian estimation approach that uses the token distribution as prior and the sampled token as observation to construct a continued input representation.

**Key Contributions:**

	1. Introduction of Mixture of Inputs (MoI) method
	2. Utilization of Bayesian estimation for token distribution
	3. Demonstrated performance improvements on various complex tasks

**Result:** MoI significantly enhances model performance in tasks such as mathematical reasoning, code generation, and PhD-level question answering with multiple models.

**Limitations:** 

**Conclusion:** MoI improves text quality and reasoning abilities in LLMs without additional training or significant computational costs.

**Abstract:** In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.

</details>


### [29] [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)

*Wonje Jeung, Sangyeon Yoon, Albert No*

**Main category:** cs.CL

**Keywords:** Machine Unlearning, Large Language Models, Evaluation Framework, Mixed Prompt Unlearning, Forget and Retain Queries

**Relevance Score:** 9

**TL;DR:** The paper introduces SEPS, an evaluation framework for measuring a model's capability to forget and retain information in the context of mixed queries, addressing shortcomings in existing unlearning methods and proposing a new Mixed Prompt unlearning strategy.

**Read time:** 32 min

<details>
  <summary>Details</summary>

**Motivation:** To improve machine unlearning by assessing the performance of models under mixed query conditions, where forget and retain queries coexist, which existing metrics fail to evaluate.

**Method:** The authors developed the SEPS framework for evaluation and proposed the Mixed Prompt (MP) unlearning strategy that combines forget and retain queries into a single training objective.

**Key Contributions:**

	1. Introduction of the SEPS evaluation framework for mixed query assessment in machine unlearning.
	2. Identification of key failure modes in existing unlearning methods.
	3. Development of the Mixed Prompt (MP) unlearning strategy that integrates forget and retain queries.

**Result:** The proposed approach significantly enhances the unlearning process, showing increased effectiveness and robustness in handling prompts with multiple mixed queries compared to traditional methods.

**Limitations:** 

**Conclusion:** Mixed Prompt unlearning addresses the limitations of prior techniques, significantly improving the capability of models to selectively unlearn information while retaining essential knowledge.

**Abstract:** Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial.   We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.

</details>


### [30] [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)

*Wang Jiaqi, Wang bo, Guo fa, Cheng cheng, Yang li*

**Main category:** cs.CL

**Keywords:** Large Language Models, personality traits, human-AI interaction, Distributed Personality Framework, responsible AI

**Relevance Score:** 8

**TL;DR:** This study investigates personality-like traits in Large Language Models (LLMs) through empirical studies, proposing the Distributed Personality Framework based on findings of variability and input sensitivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs exhibit personality-like traits and how these traits compare to human personality using conventional assessment tools.

**Method:** Three empirical studies examining test-retest stability, cross-variant consistency, and personality retention during role-playing were conducted.

**Key Contributions:**

	1. Proposed the Distributed Personality Framework for LLMs
	2. Found that LLMs' personality traits are dynamic and input-driven
	3. Provided insights for constructing LLM-specific personality frameworks

**Result:** LLMs showed higher variability, sensitivity to input and prompt settings, and low internal consistency compared to humans.

**Limitations:** LLMs lack long-term stability and exhibit varying responses based on input.

**Conclusion:** LLMs express fluid personality patterns shaped by external factors, informing responsible AI development and aiding in human-AI interactions.

**Abstract:** Large Language Models (LLMs) have demonstrated human-like capabilities in language comprehension and generation, becoming active participants in social and cognitive domains. This study investigates whether LLMs exhibit personality-like traits and how these traits compare with human personality, focusing on the applicability of conventional personality assessment tools. A behavior-based approach was used across three empirical studies. Study 1 examined test-retest stability and found that LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability. Based on this, we propose the Distributed Personality Framework, conceptualizing LLM traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency in personality measures and found LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans. Study 3 explored personality retention during role-playing, showing LLM traits are shaped by prompt and parameter settings. These findings suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction. This work contributes to responsible AI development and extends the boundaries of personality psychology in the age of intelligent systems.

</details>


### [31] [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)

*Xi Wang, Jiaqian Hu, Safinah Ali*

**Main category:** cs.CL

**Keywords:** Multi-Agent System, Translation Quality, Multidimensional Quality Metrics

**Relevance Score:** 7

**TL;DR:** MAATS is a Multi Agent Automated Translation System that improves translation quality using a specialized agent approach based on the Multidimensional Quality Metrics framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance translation accuracy and contextual fidelity using a multi-agent approach rather than traditional single-agent methods.

**Method:** MAATS employs multiple AI agents, each targeting different aspects of translation quality as defined by the MQM framework, followed by a synthesis agent for iterative refinement.

**Key Contributions:**

	1. Introduced a multi-agent approach to translation quality
	2. Utilized the MQM framework for error detection
	3. Demonstrated significant improvements in translation across diverse language pairs

**Result:** MAATS outperforms zero-shot and single-agent systems in various language pairs, achieving statistically significant improvements in both automatic metrics and human assessments.

**Limitations:** 

**Conclusion:** Aligning agent roles with MQM dimensions improves the translation process, bridging the gap between LLMs and human workflows by focusing on deeper semantic accuracy.

**Abstract:** We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction.   Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity.

</details>


### [32] [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)

*Drishya Karki, Michiel Kamphuis, Angelecia Frey*

**Main category:** cs.CL

**Keywords:** math reasoning, language models, benchmark, machine learning, natural language processing

**Relevance Score:** 4

**TL;DR:** EasyMath is a benchmark for math reasoning in small language models, covering various arithmetic and algebra topics; its accuracy increases with model size and training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a benchmark for evaluating practical math reasoning capabilities in small language models across various topics without delving into specialist areas.

**Method:** Tested 23 language models ranging from 14M to 4B parameters using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting.

**Key Contributions:**

	1. Introduction of the EasyMath benchmark for small language models
	2. Demonstrated trends in model performance related to size and training
	3. Insights into reasoning strategies like chain-of-thought in improving accuracy

**Result:** Model accuracy improves with size and training, chain-of-thought reasoning offers modest gains, and consistency enhances as model scale increases.

**Limitations:** Focuses on non-specialist topics and may not cover all areas of mathematical reasoning.

**Conclusion:** Smaller language models demonstrate improved math reasoning with appropriate scaling and training, highlighting the importance of model size and reasoning strategies.

**Abstract:** EasyMath is a compact benchmark for practical math reasoning in small language models. It covers thirteen categories, from basic arithmetic and order of operations to word problems, algebraic expressions, edge cases, and omits specialist topics. We tested 23 models (14M to 4B parameters) using exact, numerical, and symbolic checks on free-form answers in a zero-shot setting. Accuracy rises with size and training, chain-of-thought adds modest gains, and consistency improves at scale.

</details>


### [33] [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)

*Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang*

**Main category:** cs.CL

**Keywords:** large language models, tensor compression, sparse augmented tensor networks

**Relevance Score:** 9

**TL;DR:** The paper presents a novel framework called sparse augmented tensor networks (Saten) for compressing large language models (LLMs) on resource-constrained devices, improving accuracy and efficiency in fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Efficient deployment of large language models is critical for resource-constrained environments, leading to the need for effective compression techniques.

**Method:** The study proposes the Saten framework, utilizing low-rank tensorization to enhance the performance of pre-trained language models during fine-tuning.

**Key Contributions:**

	1. Introduction of sparse augmented tensor networks (Saten) for LLM compression
	2. Demonstrated enhancements in both accuracy and efficiency during fine-tuning
	3. State-of-the-art performance in tensorized language models

**Result:** Saten achieves state-of-the-art performance in terms of accuracy and compression efficiency in tensorized language models.

**Limitations:** 

**Conclusion:** The proposed method offers a promising solution for compressing large language models to make them more deployable on limited-resource devices.

**Abstract:** The efficient implementation of large language models (LLMs) is crucial for deployment on resource-constrained devices. Low-rank tensor compression techniques, such as tensor-train (TT) networks, have been widely studied for over-parameterized neural networks. However, their applications to compress pre-trained large language models (LLMs) for downstream tasks (post-training) remains challenging due to the high-rank nature of pre-trained LLMs and the lack of access to pretraining data. In this study, we investigate low-rank tensorized LLMs during fine-tuning and propose sparse augmented tensor networks (Saten) to enhance their performance. The proposed Saten framework enables full model compression. Experimental results demonstrate that Saten enhances both accuracy and compression efficiency in tensorized language models, achieving state-of-the-art performance.

</details>


### [34] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)

*Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, dysarthric speech, voice conversion, multilingual ASR, speech synthesis

**Relevance Score:** 7

**TL;DR:** This paper addresses the challenges in automatic speech recognition for dysarthric speech by using voice conversion to create dysarthric-like non-English speech for improving ASR performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of data for dysarthric speech recognition in non-English languages presents a significant challenge.

**Method:** A voice conversion model is fine-tuned on English dysarthric speech to produce non-English dysarthric-like speech, which is then used to enhance a multilingual ASR model, Massively Multilingual Speech (MMS).

**Key Contributions:**

	1. Fine-tuning voice conversion for creating dysarthric-like speech
	2. Demonstrating significant improvements in ASR performance using generated speech
	3. Objective and subjective validation of dysarthric characteristics in generated data

**Result:** The approach using voice conversion significantly improves dysarthric speech recognition compared to traditional techniques and the baseline MMS model.

**Limitations:** The study focuses on specific non-English languages and may not generalize across all dysarthric speech variations.

**Conclusion:** The study successfully demonstrates that the generated speech simulates dysarthric characteristics and enhances ASR models for various languages.

**Abstract:** Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics.

</details>


### [35] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)

*Wenqing Wu, Haixu Xi, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** peer review, machine learning, NLP, text analysis, confidence scores

**Relevance Score:** 4

**TL;DR:** This paper analyzes the consistency between reviewer confidence scores and textual reviews in AI conference peer reviews using deep learning and NLP techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of peer review processes in academia by providing a fine-grained analysis of the consistency between reviewer confidence scores and written reviews.

**Method:** The study employs deep learning techniques to detect hedge sentences and various aspects in peer reviews, analyzing factors such as report length, frequency of hedge words and sentences, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests are conducted to assess the effect of confidence scores on paper outcomes.

**Key Contributions:**

	1. Fine-grained analysis of text-score consistency in peer reviews
	2. Use of deep learning techniques to analyze reviewer confidence and textual reviews
	3. Evidence showing correlation between reviewer confidence scores and paper rejection rates

**Result:** The results indicate a high level of text-score consistency across words, sentences, and aspects. Regression analysis shows that higher reviewer confidence scores correlate with paper rejections, which supports the validity of expert assessments in peer review.

**Limitations:** 

**Conclusion:** The findings offer evidence for the fairness of peer review processes and suggest that confidence scores play a significant role in the evaluation outcomes for submitted papers.

**Abstract:** Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.

</details>


### [36] [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)

*Chris Sypherd, Sergei Petrov, Sonny George, Vaishak Belle*

**Main category:** cs.CL

**Keywords:** large language models, prompting strategies, efficiency, token usage, performance

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for evaluating the efficiency of prompting strategies in large language models, emphasizing the balance between performance and token usage.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the growing importance of not only evaluating task performance of prompting strategies in large language models but also considering the efficiency of these strategies in terms of token usage.

**Method:** The authors propose the Big-$O_{tok}$ framework to theoretically describe token usage growth and introduce Token Cost as an empirical measure to evaluate tokens per performance across different prompting strategies.

**Key Contributions:**

	1. Introduction of the Big-$O_{tok}$ theoretical framework for token usage
	2. Proposal of Token Cost as a metric for efficiency
	3. Empirical validation of efficiency in prompting strategies for LLMs

**Result:** The analyses reveal that increased token usage leads to significantly diminishing returns in performance, reinforcing the need for efficiency-focused evaluations.

**Limitations:** The study may not cover all potential prompting strategies and the generalizability to all tasks could be limited.

**Conclusion:** The study successfully validates the Big-$O_{tok}$ framework and calls for efficiency-aware methods in assessing prompting strategies for practical applications.

**Abstract:** In recent years, large language models have demonstrated remarkable performance across diverse tasks. However, their task effectiveness is heavily dependent on the prompting strategy used to elicit output, which can vary widely in both performance and token usage. While task performance is often used to determine prompting strategy success, we argue that efficiency--balancing performance and token usage--can be a more practical metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a theoretical framework for describing the token usage growth of prompting strategies, and analyze Token Cost, an empirical measure of tokens per performance. We apply these to several common prompting strategies and find that increased token usage leads to drastically diminishing performance returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need for efficiency-aware evaluations.

</details>


### [37] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)

*Ian Steenstra, Timothy W. Bickmore*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI psychotherapists, mental healthcare, risk assessment, conversational AI

**Relevance Score:** 9

**TL;DR:** Introducing a novel risk taxonomy for evaluating conversational AI psychotherapists to improve safety and effectiveness in mental healthcare.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of LLMs and virtual agents as psychotherapists raises concerns about user harm and the need for standardized evaluation methodologies to assess these risks.

**Method:** Developed through a process including literature review, expert interviews, and alignment with clinical criteria, resulting in a structured risk taxonomy for conversational AI in psychotherapy.

**Key Contributions:**

	1. Novel risk taxonomy for AI psychotherapists
	2. Integration of expert insights and clinical criteria
	3. Use cases demonstrating effective risk monitoring

**Result:** The taxonomy provides a systematic approach to identify and assess potential user harms, facilitating better monitoring in both human-AI and automated counseling scenarios.

**Limitations:** 

**Conclusion:** The proposed taxonomy is a foundational step towards safer innovation in AI-driven mental health support, enhancing evaluation practices.

**Abstract:** The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.

</details>


### [38] [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)

*Danqing Wang, Zhuorui Ye, Xinran Zhao, Fei Fang, Lei Li*

**Main category:** cs.CL

**Keywords:** debate framework, argumentation, Rehearsal Tree, Debate Flow Tree, competitive debate

**Relevance Score:** 6

**TL;DR:** TreeDebater is a novel debate framework that utilizes two tree structures to enhance competitive debate performance, addressing time constraints and the dynamic nature of debates.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Winning competitive debates requires advanced reasoning and argumentation skills, compounded by time constraints and the interactive nature of debates.

**Method:** TreeDebater employs two structured trees: the Rehearsal Tree, which predicts attacks and defenses, and the Debate Flow Tree, which tracks ongoing debate actions. It optimizes time allocation for actions and incorporates feedback from a simulated audience to refine discourse.

**Key Contributions:**

	1. Introduction of the Rehearsal Tree and Debate Flow Tree structures
	2. Improved time allocation strategies in debates
	3. Human evaluation confirms superior performance over state-of-the-art systems

**Result:** TreeDebater demonstrates superior performance over existing multi-agent debate systems by effectively limiting time to essential debate actions.

**Limitations:** 

**Conclusion:** TreeDebater not only outperforms current methodologies but also mirrors the strategic approaches of expert human debaters in competitive settings.

**Abstract:** Winning competitive debates requires sophisticated reasoning and argument skills. There are unique challenges in the competitive debate: (1) The time constraints force debaters to make strategic choices about which points to pursue rather than covering all possible arguments; (2) The persuasiveness of the debate relies on the back-and-forth interaction between arguments, which a single final game status cannot evaluate. To address these challenges, we propose TreeDebater, a novel debate framework that excels in competitive debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the strength of the claim, while the Debate Flow Tree tracks the debate status to identify the active actions. TreeDebater allocates its time budget among candidate actions and uses the speech time controller and feedback from the simulated audience to revise its statement. The human evaluation on both the stage-level and the debate-level comparison shows that our TreeDebater outperforms the state-of-the-art multi-agent debate system. Further investigation shows that TreeDebater shows better strategies in limiting time to important debate actions, aligning with the strategies of human debate experts.

</details>


### [39] [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)

*Nathan Roll, Calbert Graham, Yuka Tatsumi, Kim Tien Nguyen, Meghan Sumner, Dan Jurafsky*

**Main category:** cs.CL

**Keywords:** spoken language models, in-context learning, automatic speech recognition, language adaptation, low-resource languages

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for in-context learning in spoken language models, showing significant reductions in word error rates with minimal example utterances.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates whether adaptation benefits seen in human listeners also apply to spoken language models, particularly in enhancing automatic speech recognition (ASR).

**Method:** A scalable framework for in-context learning (ICL) is introduced, utilizing interleaved task prompts and audio-text pairs during inference.

**Key Contributions:**

	1. Introduced a novel framework for ICL in spoken language models.
	2. Demonstrated significant reduction in word error rates with minimal data during inference.
	3. Revealed performance similarities to human listeners in ASR tasks.

**Result:** The proposed ICL adaptation scheme reduces word error rates by an average of 19.7% with just 12 example utterances, particularly improving performance in low-resource language varieties.

**Limitations:** Some varieties still present significant gaps in performance compared to human listeners, indicating the need for further model refinement.

**Conclusion:** While the adaptation strategy aligns well with human flexibility, gaps remain for certain speaker varieties, indicating areas for model improvement.

**Abstract:** Human listeners readily adjust to unfamiliar speakers and language varieties through exposure, but do these adaptation benefits extend to state-of-the-art spoken language models? We introduce a scalable framework that allows for in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts and audio-text pairs, and find that as few as 12 example utterances (~50 seconds) at inference time reduce word error rates by a relative 19.7% (1.2 pp.) on average across diverse English corpora. These improvements are most pronounced in low-resource varieties, when the context and target speaker match, and when more examples are provided--though scaling our procedure yields diminishing marginal returns to context length. Overall, we find that our novel ICL adaptation scheme (1) reveals a similar performance profile to human listeners, and (2) demonstrates consistent improvements to automatic speech recognition (ASR) robustness across diverse speakers and language backgrounds. While adaptation succeeds broadly, significant gaps remain for certain varieties, revealing where current models still fall short of human flexibility. We release our prompts and code on GitHub.

</details>


### [40] [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)

*Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, state dynamics, next-state prediction, attention heads, HCI

**Relevance Score:** 8

**TL;DR:** This paper evaluates the ability of Large Language Models (LLMs) to model state transition dynamics across various tasks, revealing challenges in internal state tracking as complexity increases.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand the limitations of LLMs in modeling deterministic state dynamics, which is crucial for tasks that involve state tracking.

**Method:** The authors assess LLMs' next-state prediction accuracy across three domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, which are formalized as finite-state systems.

**Key Contributions:**

	1. Evaluation of LLMs on state dynamics across multiple tasks
	2. Identification of specific attention heads responsible for state information propagation
	3. Findings suggest LLMs rely on distributed interactions for state tracking rather than explicit computation.

**Result:** Results show that next-state prediction accuracy declines significantly with larger state spaces and sparse transitions; GPT-2 XL achieved around 70% accuracy in simpler conditions but fell below 30% in more complex scenarios.

**Limitations:** The analysis is focused on deterministic state dynamics in limited domains and may not generalize to all LLM applications.

**Conclusion:** The research indicates that effective state tracking by LLMs may derive from the interactions of next-token heads rather than through coherent symbolic reasoning, pointing to weaknesses in joint state-action processing.

**Abstract:** Large Language Models (LLMs) are increasingly used in tasks requiring internal state tracking, yet their ability to model state transition dynamics remains poorly understood. We evaluate how well LLMs capture deterministic state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and Complex Text Games, each formalizable as a finite-state system. Across tasks, we find that next-state prediction accuracy degrades with increasing state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in low-complexity settings but drops below 30% when the number of boxes or states exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50% accuracy when the number of states is > 10 and transitions are < 30. Through activation patching, we identify attention heads responsible for propagating state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10, 11, 12, and 14. While these heads successfully move relevant state features, action information is not reliably routed to the final token, indicating weak joint state-action reasoning. Our results suggest that state tracking in LLMs emerges from distributed interactions of next-token heads rather than explicit symbolic computation.

</details>


### [41] [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)

*Xiaoyan Bai, Ike Peng, Aditya Singh, Chenhao Tan*

**Main category:** cs.CL

**Keywords:** concept incongruence, large language models, role-playing, model behavior, abstention metrics

**Relevance Score:** 8

**TL;DR:** This paper introduces and analyzes the phenomenon of concept incongruence in large language models, focusing on role-playing scenarios and proposing metrics to evaluate model behavior under such discrepancies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To define and analyze how large language models handle concept incongruence, particularly in role-playing contexts, and to identify behavioral shortcomings in these situations.

**Method:** The authors propose three metrics (abstention rate, conditional accuracy, answer rate) to quantify model behavior under concept incongruence, specifically focusing on scenarios involving temporal boundaries and role 'death'. They conduct probing experiments to explore causes of model behavior changes.

**Key Contributions:**

	1. Introduction of the concept incongruence phenomenon in language models
	2. Development of three behavioral metrics to evaluate model responses under incongruence
	3. Insights into improving model consistency in abstention and answers despite incongruences.

**Result:** The study finds that models do not abstain adequately after 'death' in role-playing, leading to significant drops in accuracy when compared to settings without role-playing.

**Limitations:** The study focuses exclusively on role-playing scenarios and may not generalize to other contexts of concept incongruence.

**Conclusion:** The findings reveal that concept incongruence results in unexpected behaviors in models, with implications for future improvements in how models handle these discrepancies.

**Abstract:** Consider this prompt "Draw a unicorn with two horns". Should large language models (LLMs) recognize that a unicorn has only one horn by definition and ask users for clarifications, or proceed to generate something anyway? We introduce concept incongruence to capture such phenomena where concept boundaries clash with each other, either in user prompts or in model representations, often leading to under-specified or mis-specified behaviors. In this work, we take the first step towards defining and analyzing model behavior under concept incongruence. Focusing on temporal boundaries in the Role-Play setting, we propose three behavioral metrics--abstention rate, conditional accuracy, and answer rate--to quantify model behavior under incongruence due to the role's death. We show that models fail to abstain after death and suffer from an accuracy drop compared to the Non-Role-Play setting. Through probing experiments, we identify two main causes: (i) unreliable encoding of the "death" state across different years, leading to unsatisfactory abstention behavior, and (ii) role playing causes shifts in the model's temporal representations, resulting in accuracy drops. We leverage these insights to improve consistency in the model's abstention and answer behaviors. Our findings suggest that concept incongruence leads to unexpected model behaviors and point to future directions on improving model behavior under concept incongruence.

</details>


### [42] [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)

*Ye Yuan, Haolun Wu, Hao Zhou, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang*

**Main category:** cs.CL

**Keywords:** Telecommunications, Entity Extraction, 6G Networks, AI-Native Architectures, Information Extraction

**Relevance Score:** 4

**TL;DR:** This paper presents TeleSEE, a novel information extraction technique for the telecom sector that enhances structured entity extraction for 6G networks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enable better understanding of telecom terminologies through AI models by transforming fragmented telecom knowledge into structured formats.

**Method:** TeleSEE utilizes a token-efficient representation method for predicting entity types and attributes and employs a hierarchical parallel decoding approach to improve the encoder-decoder architecture.

**Key Contributions:**

	1. Introduction of the TeleSEE technique for entity extraction in telecom
	2. Creation of the 6GTech dataset from extensive 6G-related publications
	3. Higher accuracy and improved processing speed of entity extraction tasks.

**Result:** TeleSEE achieves higher prediction accuracy and a 5 to 9 times increase in processing speed compared to baseline techniques.

**Limitations:** 

**Conclusion:** The proposed TeleSEE method demonstrates significant improvements in entity extraction within telecom context while offering an evaluative dataset called 6GTech.

**Abstract:** Knowledge understanding is a foundational part of envisioned 6G networks to advance network intelligence and AI-native network architectures. In this paradigm, information extraction plays a pivotal role in transforming fragmented telecom knowledge into well-structured formats, empowering diverse AI models to better understand network terminologies. This work proposes a novel language model-based information extraction technique, aiming to extract structured entities from the telecom context. The proposed telecom structured entity extraction (TeleSEE) technique applies a token-efficient representation method to predict entity types and attribute keys, aiming to save the number of output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a hierarchical parallel decoding method, improving the standard encoder-decoder architecture by integrating additional prompting and decoding strategies into entity extraction tasks. In addition, to better evaluate the performance of the proposed technique in the telecom domain, we further designed a dataset named 6GTech, including 2390 sentences and 23747 words from more than 100 6G-related technical publications. Finally, the experiment shows that the proposed TeleSEE method achieves higher accuracy than other baseline techniques, and also presents 5 to 9 times higher sample processing speed.

</details>


### [43] [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)

*Zhiwei Liu, Paul Thompson, Jiaqi Rong, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conspiracy Theory Detection, Sentiment Analysis

**Relevance Score:** 8

**TL;DR:** This paper presents ConDID-v2, a dataset for conspiracy theory detection, and ConspEmoLLM-v2, an enhanced detection model capable of identifying disguised conspiracy theories by analyzing sentiment-transformed tweets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve the detection of conspiracy theories, particularly those disguised by LLMs to reduce negative sentiment, which can evade conventional detection methods.

**Method:** An augmented version of the ConDID conspiracy detection dataset (ConDID-v2) was developed, which includes LLM-rewritten conspiracy tweets. ConspEmoLLM-v2 was trained using this dataset and evaluated against previous models.

**Key Contributions:**

	1. Development of the ConDID-v2 dataset with human and LLM-verified content.
	2. Introduction of ConspEmoLLM-v2, which shows improved performance over previous models.
	3. Demonstration of how LLM manipulation of sentiment affects conspiracy theory detection.

**Result:** ConspEmoLLM-v2 retains or exceeds performance on original human-authored content and significantly outperforms multiple baselines on sentiment-transformed tweets in ConDID-v2.

**Limitations:** 

**Conclusion:** The findings suggest that enhancing detection models with LLM-generated text leads to better identification of disguised conspiracy theories.

**Abstract:** Despite the many benefits of large language models (LLMs), they can also cause harm, e.g., through automatic generation of misinformation, including conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories by altering characteristic textual features, e.g., by transforming their typically strong negative emotions into a more positive tone. Although several studies have proposed automated conspiracy theory detection methods, they are usually trained using human-authored text, whose features can vary from LLM-generated text. Furthermore, several conspiracy detection models, including the previously proposed ConspEmoLLM, rely heavily on the typical emotional features of human-authored conspiracy content. As such, intentionally disguised content may evade detection. To combat such issues, we firstly developed an augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which supplements human-authored conspiracy tweets with versions rewritten by an LLM to reduce the negativity of their original sentiment. The quality of the rewritten tweets was verified by combining human and LLM-based assessment. We subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or exceeds the performance of ConspEmoLLM on the original human-authored content in ConDID, and considerably outperforms both ConspEmoLLM and several other baselines when applied to sentiment-transformed tweets in ConDID-v2. The project will be available at https://github.com/lzw108/ConspEmoLLM.

</details>


### [44] [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)

*Fadel M. Megahed, Ying-Ju Chen, L. Allision Jones-Farmer, Younghwa Lee, Jiawei Brooke Wang, Inez M. Zwetsloot*

**Main category:** cs.CL

**Keywords:** large language models, binary text classification, reliability assessment, sentiment analysis, financial news

**Relevance Score:** 9

**TL;DR:** This study presents a framework for evaluating the consistency in large language model binary text classification, demonstrating methodologies for reliability assessment and applying it to financial news sentiment classification across multiple LLMs.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for reliable assessment methods for consistency in large language model binary text classification, adapting principles from psychometrics.

**Method:** The study establishes sample size requirements, develops metrics for invalid responses, and evaluates intra- and inter-rater reliability across various LLMs in a case study of financial news sentiment classification.

**Key Contributions:**

	1. Introduction of a reliability assessment framework for LLMs
	2. Evaluation of various LLMs in binary classification tasks
	3. Demonstration of high consistency and performance metrics in financial news sentiment classification

**Result:** Models exhibited high intra-rater consistency with 90-98% agreement and performed strongly against validation labels, while prediction of actual market movements showed all models performed at chance.

**Limitations:** The models were found to perform at chance in predicting actual market movements, which may reflect task constraints.

**Conclusion:** The proposed framework effectively guides LLM selection and optimizes resources for classification tasks by assessing reliability and consistency in performance.

**Abstract:** This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.

</details>


### [45] [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)

*Sil Hamilton, Rebecca M. M. Hicke, Matthew Wilkens, David Mimno*

**Main category:** cs.CL

**Keywords:** large language models, long-context evaluation, TLDM benchmark, narrative understanding, plot summary

**Relevance Score:** 9

**TL;DR:** The paper introduces the Too Long, Didn't Model (TLDM) benchmark, revealing limitations in LLMs' understanding of complex narratives beyond 64k tokens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of large language models (LLMs) in understanding long and complex narratives, which are prevalent in novels.

**Method:** The authors released the TLDM benchmark to test LLMs on their ability to summarize plots, configure storyworlds, and understand elapsed narrative time.

**Key Contributions:**

	1. Introduction of the TLDM benchmark for LLM evaluation
	2. Demonstration of LLM limitations in understanding long narratives
	3. Provision of reference code and data for future model development

**Result:** Seven frontier LLMs were tested and none maintained stable understanding beyond 64k tokens, indicating significant limitations in long-context scenarios.

**Limitations:** The benchmark focuses on narrative structures and may not encompass all complexity types present in other text forms.

**Conclusion:** The findings suggest that model evaluations should move beyond simple benchmarks to better assess performance in complex contexts; the TLDM benchmark is provided for further research.

**Abstract:** Although the context length of large language models (LLMs) has increased to millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack approaches has proven difficult. We argue that novels provide a case study of subtle, complicated structure and long-range semantic dependencies often over 128k tokens in length. Inspired by work on computational novel analysis, we release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's ability to report plot summary, storyworld configuration, and elapsed narrative time. We find that none of seven tested frontier LLMs retain stable understanding beyond 64k tokens. Our results suggest language model developers must look beyond "lost in the middle" benchmarks when evaluating model performance in complex long-context scenarios. To aid in further development we release the TLDM benchmark together with reference code and data.

</details>


### [46] [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)

*Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, Danielle S. Bitterman*

**Main category:** cs.CL

**Keywords:** large language models, clinical decision support, MedBrowseComp, multi-hop reasoning, medical information synthesis

**Relevance Score:** 9

**TL;DR:** MedBrowseComp is a benchmark for evaluating large language models' ability to synthesize multi-hop medical facts from domain-specific knowledge bases.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for reliable decision-support systems in clinical practice that can integrate various knowledge sources under accuracy constraints.

**Method:** The authors developed MedBrowseComp, a benchmark featuring over 1,000 human-curated clinical questions that require synthesis of fragmented medical information.

**Key Contributions:**

	1. Introduction of a novel benchmark (MedBrowseComp) for evaluating clinical reasoning in LLMs.
	2. More than 1,000 curated clinical questions designed to mimic real-world decision-making scenarios.
	3. Identification of critical performance gaps in current LLM applications for clinical decision support.

**Result:** The benchmark revealed significant performance shortfalls in existing large language models, with accuracy as low as ten percent in clinical scenarios.

**Limitations:** The benchmark primarily focuses on multi-hop reasoning and may not cover all aspects of clinical decision-making.

**Conclusion:** MedBrowseComp serves as a testbed for improving LLM capabilities in medical contexts and highlights the ongoing need for advancements in this area.

**Abstract:** Large language models (LLMs) are increasingly envisioned as decision-support tools in clinical practice, yet safe clinical reasoning demands integrating heterogeneous knowledge bases -- trials, primary studies, regulatory documents, and cost data -- under strict accuracy constraints. Existing evaluations often rely on synthetic prompts, reduce the task to single-hop factoid queries, or conflate reasoning with open-ended generation, leaving their real-world utility unclear. To close this gap, we present MedBrowseComp, the first benchmark that systematically tests an agent's ability to reliably retrieve and synthesize multi-hop medical facts from live, domain-specific knowledge bases. MedBrowseComp contains more than 1,000 human-curated questions that mirror clinical scenarios where practitioners must reconcile fragmented or conflicting information to reach an up-to-date conclusion. Applying MedBrowseComp to frontier agentic systems reveals performance shortfalls as low as ten percent, exposing a critical gap between current LLM capabilities and the rigor demanded in clinical settings. MedBrowseComp therefore offers a clear testbed for reliable medical information seeking and sets concrete goals for future model and toolchain upgrades. You can visit our project page at: https://moreirap12.github.io/mbc-browse-app/

</details>


### [47] [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)

*Prashanth Vijayaraghavan, Soroush Vosoughi, Lamogha Chizor, Raya Horesh, Rogerio Abreu de Paula, Ehsan Degan, Vandana Mukherjee*

**Main category:** cs.CL

**Keywords:** caste bias, large language models, natural language processing, DECASTE, bias evaluation

**Relevance Score:** 8

**TL;DR:** This paper proposes DECASTE, a framework to detect caste biases in large language models (LLMs), revealing systematic reinforcement of biases against marginalized caste groups in India.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding and assessing caste-based biases in LLMs, particularly towards marginalized groups in India like Dalits and Shudras.

**Method:** DECASTE evaluates caste fairness across four dimensions (socio-cultural, economic, educational, political) using customized prompting strategies to benchmark various state-of-the-art LLMs.

**Key Contributions:**

	1. Introduction of DECASTE framework for assessing caste biases in LLMs
	2. Quantitative evidence of caste biases in major LLMs
	3. Identification of disparities in model outputs based on caste affiliation.

**Result:** The analysis shows that LLMs systematically reinforce caste biases, with significant disparities in outputs favoring dominant caste groups over oppressed ones.

**Limitations:** The study primarily focuses on Indian caste biases and may not be directly applicable to other cultural contexts without adaptation.

**Conclusion:** There is a critical need for comprehensive methodologies to evaluate and mitigate caste biases in LLMs before deployment in real-world scenarios.

**Abstract:** Recent advancements in large language models (LLMs) have revolutionized natural language processing (NLP) and expanded their applications across diverse domains. However, despite their impressive capabilities, LLMs have been shown to reflect and perpetuate harmful societal biases, including those based on ethnicity, gender, and religion. A critical and underexplored issue is the reinforcement of caste-based biases, particularly towards India's marginalized caste groups such as Dalits and Shudras. In this paper, we address this gap by proposing DECASTE, a novel, multi-dimensional framework designed to detect and assess both implicit and explicit caste biases in LLMs. Our approach evaluates caste fairness across four dimensions: socio-cultural, economic, educational, and political, using a range of customized prompting strategies. By benchmarking several state-of-the-art LLMs, we reveal that these models systematically reinforce caste biases, with significant disparities observed in the treatment of oppressed versus dominant caste groups. For example, bias scores are notably elevated when comparing Dalits and Shudras with dominant caste groups, reflecting societal prejudices that persist in model outputs. These results expose the subtle yet pervasive caste biases in LLMs and emphasize the need for more comprehensive and inclusive bias evaluation methodologies that assess the potential risks of deploying such models in real-world contexts.

</details>


### [48] [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)

*Haoyi Qiu, Kung-Hsiang Huang, Ruichen Zheng, Jiao Sun, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** vision-language models, cultural safety, multimodal evaluation, artificial intelligence, cultural awareness

**Relevance Score:** 7

**TL;DR:** This paper introduces CROSS, a benchmark for assessing the cultural safety reasoning capabilities of large vision-language models (LVLMs) through a framework called CROSS-Eval, revealing significant gaps in cultural safety and proposing enhancement strategies to improve performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored issue of cultural appropriateness in responses generated by LVLMs deployed in global applications, focusing on preventing symbolic harm and violations of cultural norms.

**Method:** The paper introduces CROSS, a benchmark with 1,284 multilingual visually grounded queries and a framework (CROSS-Eval) that evaluates four dimensions: cultural awareness, norm education, compliance, and helpfulness. It evaluates 21 LVLMs and proposes enhancement strategies for improving performance.

**Key Contributions:**

	1. Introduction of CROSS benchmark for cultural safety assessment in LVLMs
	2. Development of CROSS-Eval framework for measuring cultural dimensions
	3. Proposed enhancement strategies for improving cultural awareness and compliance in LVLMs

**Result:** The evaluation reveals significant gaps in cultural safety with the best-performing model achieving only 61.79% awareness and 37.73% compliance. Enhancement strategies developed improve cultural awareness by +60.14% and compliance by +55.2% without significant loss in general multimodal capabilities.

**Limitations:** The study relies on existing models and may not generalize to all LVLMs; cultural contexts are complex and continuously evolving.

**Conclusion:** The findings highlight the necessity for improved cultural safety reasoning in LVLMs and demonstrate that enhancing reasoning capacity can lead to better cultural alignment, though it does not fully resolve the existing issues.

**Abstract:** Large vision-language models (LVLMs) are increasingly deployed in globally distributed applications, such as tourism assistants, yet their ability to produce culturally appropriate responses remains underexplored. Existing multimodal safety benchmarks primarily focus on physical safety and overlook violations rooted in cultural norms, which can result in symbolic harm. To address this gap, we introduce CROSS, a benchmark designed to assess the cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284 multilingual visually grounded queries from 16 countries, three everyday domains, and 14 languages, where cultural norm violations emerge only when images are interpreted in context. We propose CROSS-Eval, an intercultural theory-based framework that measures four key dimensions: cultural awareness, norm education, compliance, and helpfulness. Using this framework, we evaluate 21 leading LVLMs, including mixture-of-experts models and reasoning models. Results reveal significant cultural safety gaps: the best-performing model achieves only 61.79% in awareness and 37.73% in compliance. While some open-source models reach GPT-4o-level performance, they still fall notably short of proprietary models. Our results further show that increasing reasoning capacity improves cultural alignment but does not fully resolve the issue. To improve model performance, we develop two enhancement strategies: supervised fine-tuning with culturally grounded, open-ended data and preference tuning with contrastive response pairs that highlight safe versus unsafe behaviors. These methods substantially improve GPT-4o's cultural awareness (+60.14%) and compliance (+55.2%), while preserving general multimodal capabilities with minimal performance reduction on general multimodal understanding benchmarks.

</details>


### [49] [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)

*Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang*

**Main category:** cs.CL

**Keywords:** human-model cooperation, large language models, natural language processing, taxonomy, NLP challenges

**Relevance Score:** 9

**TL;DR:** This paper provides a comprehensive review of human-model cooperation in NLP, introducing a new taxonomy and exploring principles, formalizations, and challenges while discussing potential future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To review the concept of human-model cooperation as LLMs evolve into autonomous agents and to outline existing challenges and opportunities in NLP.

**Method:** A thorough literature review is conducted to analyze the principles and formalizations of human-model cooperation, alongside the introduction of a new taxonomy for existing approaches.

**Key Contributions:**

	1. Introduction of a new taxonomy for human-model cooperation in NLP
	2. Identification of open challenges in the research area
	3. Discussion of potential frontier areas for future exploration

**Result:** The paper presents a unified view of human-model cooperation, identifying key areas of progress in NLP tasks and laying out challenges and opportunities for future research.

**Limitations:** 

**Conclusion:** This work serves as a foundational resource for understanding human-model cooperation and encourages further advancements in the field of NLP.

**Abstract:** With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.

</details>


### [50] [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)

*Adarsh Singh, Kushal Raj Bhandari, Jianxi Gao, Soham Dan, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Table Question Answering, Sparse Retrieval, Dense Retrieval, Large Language Models, Information Retrieval

**Relevance Score:** 4

**TL;DR:** We introduce CRAFT, a novel cascaded retrieval approach for Table Question Answering that combines sparse and dense models for improved performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing dense retrieval models are costly and require retraining on new datasets, limiting their adaptability for Table Question Answering tasks.

**Method:** CRAFT uses a two-step retrieval process: first a sparse retrieval model to filter candidate tables, followed by dense models and neural re-rankers for better accuracy.

**Key Contributions:**

	1. Introduction of a cascaded retrieval model for TQA
	2. Improved retrieval performance over conventional methods
	3. Utilization of Gemini Flash 1.5 for enhanced table representations

**Result:** CRAFT outperforms state-of-the-art sparse, dense, and hybrid retrieval models in retrieving relevant tables for question answering.

**Limitations:** 

**Conclusion:** End-to-end evaluations with Large Language Models show that CRAFT significantly improves the effectiveness of Table Question Answering tasks.

**Abstract:** Table Question Answering (TQA) involves retrieving relevant tables from a large corpus to answer natural language queries. Traditional dense retrieval models, such as DTR and ColBERT, not only incur high computational costs for large-scale retrieval tasks but also require retraining or fine-tuning on new datasets, limiting their adaptability to evolving domains and knowledge. In this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that first uses a sparse retrieval model to filter a subset of candidate tables before applying more computationally expensive dense models and neural re-rankers. Our approach achieves better retrieval performance than state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further enhance table representations by generating table descriptions and titles using Gemini Flash 1.5. End-to-end TQA results using various Large Language Models (LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate $\textbf{CRAFT}$ effectiveness.

</details>


### [51] [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)

*Ishika Agarwal, Nimet Beyza Bozdag, Dilek Hakkani-Tür*

**Main category:** cs.CL

**Keywords:** code-switching, language models, cultural knowledge, language-specific knowledge, machine learning

**Relevance Score:** 8

**TL;DR:** This paper explores the concept of Language Specific Knowledge (LSK) in language models, examining how code-switching between languages can enhance reasoning and performance, particularly in culturally specific contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research investigates whether language models can possess knowledge in different languages and whether reasoning performance can be improved by changing the language used, reflecting the idea that people code-switch based on comfort with language regarding specific topics.

**Method:** A methodology called LSKExtractor is designed to benchmark the language-specific knowledge within language models, allowing for the exploitation of this knowledge during inference based on culturally specific datasets.

**Key Contributions:**

	1. Introduction of the term Language Specific Knowledge (LSK)
	2. Development of the LSKExtractor methodology
	3. Demonstration of performance improvements in language models through culturally specific reasoning

**Result:** The study shows that language models can achieve an average relative improvement of 10% in accuracy when employing chain-of-thought reasoning in languages other than English, including low-resource languages.

**Limitations:** 

**Conclusion:** The findings support the development of inclusive language models that consider cultural and linguistic contexts, and highlight the importance of language-specific knowledge in enhancing model performance.

**Abstract:** Code-switching is a common phenomenon of alternating between different languages in the same utterance, thought, or conversation. We posit that humans code-switch because they feel more comfortable talking about certain topics and domains in one language than another. With the rise of knowledge-intensive language models, we ask ourselves the next, natural question: Could models hold more knowledge on some topics in some language X? More importantly, could we improve reasoning by changing the language that reasoning is performed in? We coin the term Language Specific Knowledge (LSK) to represent this phenomenon. As ethnic cultures tend to develop alongside different languages, we employ culture-specific datasets (that contain knowledge about cultural and social behavioral norms). We find that language models can perform better when using chain-of-thought reasoning in some languages other than English, sometimes even better in low-resource languages. Paired with previous works showing that semantic similarity does not equate to representational similarity, we hypothesize that culturally specific texts occur more abundantly in corresponding languages, enabling specific knowledge to occur only in specific "expert" languages. Motivated by our initial results, we design a simple methodology called LSKExtractor to benchmark the language-specific knowledge present in a language model and, then, exploit it during inference. We show our results on various models and datasets, showing an average relative improvement of 10% in accuracy. Our research contributes to the open-source development of language models that are inclusive and more aligned with the cultural and linguistic contexts in which they are deployed.

</details>


### [52] [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)

*Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu*

**Main category:** cs.CL

**Keywords:** Information Extraction, Large Language Models, Natural Language Processing, On-device Computing, Schema Caching

**Relevance Score:** 8

**TL;DR:** Proposes an on-device information extraction method called Dual-LoRA with Incremental Schema Caching (DLISC) to enhance schema identification and extraction efficiency using large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective information extraction on resource-constrained devices using large language models, addressing hallucinations, limited context length, and high latency.

**Method:** A two-stage approach combining an Identification LoRA module for schema retrieval and an Extraction LoRA module for information extraction, with Incremental Schema Caching to optimize redundancy.

**Key Contributions:**

	1. Introduction of Dual-LoRA framework for schema-aware extraction
	2. Incremental Schema Caching to reduce redundant computations
	3. Demonstrated effectiveness on various extraction datasets

**Result:** Significant improvements in both effectiveness and efficiency of information extraction tasks across multiple datasets were observed.

**Limitations:** 

**Conclusion:** DLISC successfully enhances schema-aware information extraction for on-device LLMs, making the process more viable despite computational constraints.

**Abstract:** Information extraction (IE) plays a crucial role in natural language processing (NLP) by converting unstructured text into structured knowledge. Deploying computationally intensive large language models (LLMs) on resource-constrained devices for information extraction is challenging, particularly due to issues like hallucinations, limited context length, and high latency-especially when handling diverse extraction schemas. To address these challenges, we propose a two-stage information extraction approach adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching (DLISC), which enhances both schema identification and schema-aware extraction in terms of effectiveness and efficiency. In particular, DLISC adopts an Identification LoRA module for retrieving the most relevant schemas to a given query, and an Extraction LoRA module for performing information extraction based on the previously selected schemas. To accelerate extraction inference, Incremental Schema Caching is incorporated to reduce redundant computation, substantially improving efficiency. Extensive experiments across multiple information extraction datasets demonstrate notable improvements in both effectiveness and efficiency.

</details>


### [53] [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)

*Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

**Main category:** cs.CL

**Keywords:** multi-agent systems, self-supervised learning, large language models, dynamic configuration, adaptability

**Relevance Score:** 8

**TL;DR:** SELF-MAS is a self-supervised framework for dynamic multi-agent system design using Large Language Models, outperforming manual and automatic methods without needing a validation set.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of current multi-agent systems that rely on manually designed roles and static communication protocols, which do not adapt well to new tasks.

**Method:** SELF-MAS uses a self-supervised, inference-time only approach to automatically generate, evaluate, and refine multi-agent system configurations based on problem instances, without requiring a validation set.

**Key Contributions:**

	1. Introduction of a self-supervised framework for MAS design
	2. Dynamic agent composition and problem decomposition
	3. Demonstrated superior performance compared to existing MAS approaches

**Result:** SELF-MAS demonstrated a 7.44% average accuracy improvement over the strongest baseline while maintaining cost-efficiency in various benchmarks, including math and software engineering.

**Limitations:** The experiments are limited to defined benchmarks and may not generalize to all real-world scenarios.

**Conclusion:** The study highlights the effectiveness of meta-level self-supervised design in creating adaptive multi-agent systems that align better with the capabilities of LLMs.

**Abstract:** Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation-set for tuning and yield static MAS designs lacking adaptability during inference. We introduce SELF-MAS, the first self-supervised, inference-time only framework for automatic MAS design. SELF-MAS employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM back-bones of varying sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-supervised design for creating effective and adaptive MAS.

</details>


### [54] [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)

*Chengwei Wei, Bin Wang, Jung-jae Kim, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** large language models, spoken math, speech-based models, math problem solving, spoken input

**Relevance Score:** 9

**TL;DR:** This paper introduces Spoken Math Question Answering (Spoken-MQA), a benchmark for evaluating the mathematical reasoning ability of speech-based models and highlights their limitations in tackling spoken mathematical problems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the mathematical reasoning capabilities of speech models, which have been largely underexplored, especially in the context of spoken input for complex problems.

**Method:** The paper presents a new benchmark called Spoken-MQA that includes various math problems posed in natural spoken language, designed to assess both cascade ASR + LLM models and end-to-end speech LLMs.

**Key Contributions:**

	1. Introduction of Spoken Math Question Answering (Spoken-MQA) benchmark
	2. Comprehensive evaluation of speech-based models on mathematical reasoning tasks
	3. Insights into the limitations of current LLMs in interpreting spoken math expressions

**Result:** Experiments show that while some speech LLMs excel at contextual reasoning with basic arithmetic, they struggle with direct arithmetic problems and face challenges in interpreting verbal mathematical expressions; furthermore, their mathematical knowledge reasoning abilities are significantly reduced.

**Limitations:** The study only explores a specific subset of mathematical reasoning tasks and the performance of selected speech models, leaving broader implications unaddressed.

**Conclusion:** The results underscore the need for improvements in speech LLMs, particularly in their approach to verbalized mathematical expressions and in handling more complex reasoning tasks.

**Abstract:** Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs) have led to strong reasoning ability across a wide range of tasks. However, their ability to perform mathematical reasoning from spoken input remains underexplored. Prior studies on speech modality have mostly focused on factual speech understanding or simple audio reasoning tasks, providing limited insight into logical step-by-step reasoning, such as that required for mathematical problem solving. To address this gap, we introduce Spoken Math Question Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical reasoning capabilities of speech-based models, including both cascade models (ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of math problems, including pure arithmetic, single-step and multi-step contextual reasoning, and knowledge-oriented reasoning problems, all presented in unambiguous natural spoken language. Through extensive experiments, we find that: (1) while some speech LLMs perform competitively on contextual reasoning tasks involving basic arithmetic, they still struggle with direct arithmetic problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical expressions written in LaTex and have difficulty interpreting verbalized mathematical expressions; and (3) mathematical knowledge reasoning abilities are significantly degraded in current speech LLMs.

</details>


### [55] [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)

*Furong Jia, David Sontag, Monica Agrawal*

**Main category:** cs.CL

**Keywords:** large language models, clinical natural language processing, EHR, machine learning, data quality

**Relevance Score:** 9

**TL;DR:** This paper investigates how open-source large language models learn from general corpora regarding clinical terminology and unsupported medical claims.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance of large language models on clinical NLP tasks, especially their grasp of clinical jargon and handling of unsupported medical claims.

**Method:** The paper evaluates LLMs on a new dataset called MedLingo, analyzing the correlation between the frequency of clinical jargon in pretraining corpora and model performance, as well as the types of sources from which the data is derived.

**Key Contributions:**

	1. Introduced a dataset (MedLingo) for evaluating LLMs on clinical jargon understanding.
	2. Analyzed the relationship between pretraining corpora and model outputs related to clinical terminology and unsupported claims.
	3. Classified the sources of clinical jargon and unsupported medical claims in training data.

**Result:** A correlation was found between the frequency of clinical terminology in pretraining corpora and the performance of LLMs; however, many frequently used clinical terms in notes are underrepresented in training data. Additionally, many sources of clinical jargon and unsupported claims are identified, highlighting issues with data quality.

**Limitations:** The findings may not generalize to all LLMs or datasets, and the evaluation is limited to specific open-source models.

**Conclusion:** Improvements in the datasets used to train LLMs are necessary to enhance their understanding of clinical terminology and to mitigate the risk of propagating unsupported medical claims.

**Abstract:** Large language models (LLMs) have performed well across various clinical natural language processing tasks, despite not being directly trained on electronic health record (EHR) data. In this work, we examine how popular open-source LLMs learn clinical information from large mined corpora through two crucial but understudied lenses: (1) their interpretation of clinical jargon, a foundational ability for understanding real-world clinical notes, and (2) their responses to unsupported medical claims. For both use cases, we investigate the frequency of relevant clinical information in their corresponding pretraining corpora, the relationship between pretraining data composition and model outputs, and the sources underlying this data. To isolate clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo. Unsurprisingly, we find that the frequency of clinical jargon mentions across major pretraining corpora correlates with model performance. However, jargon frequently appearing in clinical notes often rarely appears in pretraining corpora, revealing a mismatch between available data and real-world usage. Similarly, we find that a non-negligible portion of documents support disputed claims that can then be parroted by models. Finally, we classified and analyzed the types of online sources in which clinical jargon and unsupported medical claims appear, with implications for future dataset composition.

</details>


### [56] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)

*Wenqing Wu, Haixu Xi, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** peer review, confidence scores, text analysis, deep learning, NLP

**Relevance Score:** 4

**TL;DR:** The paper evaluates the consistency of reviewer confidence scores in AI conference peer reviews by analyzing text elements using deep learning and NLP techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of research quality in academia by analyzing the consistency of peer review scores with textual content in reviews.

**Method:** Utilizes deep learning to analyze text elements such as hedge sentences and aspects, and examines various metrics (report length, hedge frequency, etc.) to assess alignment between text and scores.

**Key Contributions:**

	1. Fine-grained analysis of text-score consistency at multiple levels
	2. Correlation tests demonstrating the influence of confidence scores on paper outcomes
	3. Use of deep learning techniques to enhance peer review evaluation

**Result:** Findings indicate a high consistency between reviewer scores and text analysis, with significant correlations showing that higher confidence scores are associated with higher rejection rates of papers.

**Limitations:** 

**Conclusion:** The results support the validity of expert assessments in peer review, highlighting the importance of reviewer confidence in the evaluation process.

**Abstract:** Peer review is vital in academia for evaluating research quality. Top AI conferences use reviewer confidence scores to ensure review reliability, but existing studies lack fine-grained analysis of text-score consistency, potentially missing key details. This work assesses consistency at word, sentence, and aspect levels using deep learning and NLP conference review data. We employ deep learning to detect hedge sentences and aspects, then analyze report length, hedge word/sentence frequency, aspect mentions, and sentiment to evaluate text-score alignment. Correlation, significance, and regression tests examine confidence scores' impact on paper outcomes. Results show high text-score consistency across all levels, with regression revealing higher confidence scores correlate with paper rejection, validating expert assessments and peer review fairness.

</details>


### [57] [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)

*Haiyan Zhao, Xuansheng Wu, Fan Yang, Bo Shen, Ninghao Liu, Mengnan Du*

**Main category:** cs.CL

**Keywords:** Sparse Autoencoders, Large Language Models, Steering Mechanisms

**Relevance Score:** 8

**TL;DR:** The paper introduces Sparse Autoencoder-Denoised Concept Vectors (SDCV) to enhance the robustness of steering large language models by filtering out noise in hidden representations.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for deriving linear concept vectors from LLM hidden representations are affected by noise from diverse data, hindering steering robustness.

**Method:** The proposed SDCV uses Sparse Autoencoders to filter out irrelevant features from LLM hidden representations, which is then applied to techniques like linear probing and difference-in-means.

**Key Contributions:**

	1. Introduction of Sparse Autoencoder-Denoised Concept Vectors (SDCV)
	2. Demonstration of improved steering success rates using SDCV
	3. Validation of noise hypothesis through counterfactual experiments and feature visualizations.

**Result:** Applying SDCV improves steering success rates for linear probing and difference-in-means when used with LLMs.

**Limitations:** 

**Conclusion:** The study confirms that filtering out noisy features enhances the effectiveness of steering mechanisms for LLMs.

**Abstract:** Linear Concept Vectors have proven effective for steering large language models (LLMs). While existing approaches like linear probing and difference-in-means derive these vectors from LLM hidden representations, diverse data introduces noises (i.e., irrelevant features) that challenge steering robustness. To address this, we propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy features from hidden representations. When applied to linear probing and difference-in-means, our method improves their steering success rates. We validate our noise hypothesis through counterfactual experiments and feature visualizations.

</details>


### [58] [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)

*Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, Chen Zhao*

**Main category:** cs.CL

**Keywords:** language models, text embeddings, bidirectional attention, document retrieval, reasoning-intensive tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces diffusion language models for text embeddings, addressing limitations of LLM embeddings by leveraging bidirectional attention, leading to significant improvements in document retrieval tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLM embeddings have surpassed BERT and T5 in text embedding tasks but are limited by their unidirectional attention, which is misaligned with the bidirectional nature of these tasks.

**Method:** The authors propose a diffusion language embedding model that is inherently bidirectional and systematically study its performance across various retrieval tasks.

**Key Contributions:**

	1. Introduction of diffusion language models for text embeddings
	2. First systematic study of diffusion language embedding models
	3. Demonstrated significant performance improvements over LLM-based embeddings in various retrieval tasks

**Result:** The proposed model outperforms LLM embeddings by 20% in long-document retrieval, 8% in reasoning-intensive retrieval, and 2% in instruction-following retrieval, achieving competitive performance on traditional text embedding benchmarks.

**Limitations:** 

**Conclusion:** Bidirectional attention is essential for effectively encoding global context in long and complex text, showing the advantages of diffusion language models for text embeddings.

**Abstract:** Large language model (LLM)-based embedding models, benefiting from large scale pre-training and post-training, have begun to surpass BERT and T5-based models on general-purpose text embedding tasks such as document retrieval. However, a fundamental limitation of LLM embeddings lies in the unidirectional attention used during autoregressive pre-training, which misaligns with the bidirectional nature of text embedding tasks. To this end, We propose adopting diffusion language models for text embeddings, motivated by their inherent bidirectional architecture and recent success in matching or surpassing LLMs especially on reasoning tasks. We present the first systematic study of the diffusion language embedding model, which outperforms the LLM-based embedding model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval, 2% on instruction-following retrieval, and achieve competitive performance on traditional text embedding benchmarks. Our analysis verifies that bidirectional attention is crucial for encoding global context in long and complex text.

</details>


### [59] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)

*Yifan Wu, Lutao Yan, Leixian Shen, Yinan Mei, Jiannan Wang, Yuyu Luo*

**Main category:** cs.CL

**Keywords:** Multi-modal Large Language Models, Chart understanding, Datasets

**Relevance Score:** 8

**TL;DR:** The paper presents ChartCards, a framework for generating structured metadata for multi-task chart understanding, and introduces the MetaChart dataset for fine-tuning models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of high data collection and training costs associated with fine-tuning Multi-modal Large Language Models for chart understanding tasks.

**Method:** ChartCards synthesizes various chart-related information into structured metadata, enhancing support for tasks like text-to-chart retrieval and chart summarization. The MetaChart dataset is created to facilitate fine-tuning.

**Key Contributions:**

	1. Introduction of the ChartCards framework for generating chart metadata.
	2. Creation of the MetaChart dataset containing extensive chart-related data for model fine-tuning.
	3. Demonstration of improved performance across various chart understanding tasks using fine-tuned models.

**Result:** Fine-tuning six models on the MetaChart dataset showed an average performance improvement of 5%, with significant gains in tasks like text-to-chart retrieval and chart-to-table conversion.

**Limitations:** The framework's effectiveness may depend on the quality of the synthesized metadata and its generalizability across different domains or chart types.

**Conclusion:** The proposed approach demonstrates that structured metadata can significantly enhance model performance on chart understanding tasks while reducing the reliance on extensive high-quality datasets.

**Abstract:** The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.

</details>


### [60] [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)

*Gaurav Kumar, Debajyoti Mazumder, Ayush Garg, Jasabanta Patro*

**Main category:** cs.CL

**Keywords:** fact-checking, language models, entailment, justifications, evidence

**Relevance Score:** 7

**TL;DR:** This paper presents a novel approach to automated fact-checking using language models that generate support and refute justifications. The proposed method shows significant improvements over existing baseline methods across various datasets.

**Read time:** 44 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in automated fact-checking, particularly the complexities arising from contradictory evidence and the limitations of current approaches based on language models.

**Method:** The authors developed a method that leverages entailment and generative capabilities of language models to produce justifications for claims, systematically comparing different prompting and fine-tuning strategies.

**Key Contributions:**

	1. Development of a new approach for automated fact-checking using language models
	2. Systematic comparison of prompting and fine-tuning strategies
	3. Sharing of code repository for reproducibility of results

**Result:** The proposed method achieved significant improvements in macro-F1 scores, outperforming baselines by up to 44.26% on the RAW-FC dataset using entailed justifications.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of using justifications for training language models in fact-checking tasks and provides a structured comparison that is currently lacking in the literature.

**Abstract:** Automated fact-checking is a crucial task in this digital age. To verify a claim, current approaches majorly follow one of two strategies i.e. (i) relying on embedded knowledge of language models, and (ii) fine-tuning them with evidence pieces. While the former can make systems to hallucinate, the later have not been very successful till date. The primary reason behind this is that fact verification is a complex process. Language models have to parse through multiple pieces of evidence before making a prediction. Further, the evidence pieces often contradict each other. This makes the reasoning process even more complex. We proposed a simple yet effective approach where we relied on entailment and the generative ability of language models to produce ''supporting'' and ''refuting'' justifications (for the truthfulness of a claim). We trained language models based on these justifications and achieved superior results. Apart from that, we did a systematic comparison of different prompting and fine-tuning strategies, as it is currently lacking in the literature. Some of our observations are: (i) training language models with raw evidence sentences registered an improvement up to 8.20% in macro-F1, over the best performing baseline for the RAW-FC dataset, (ii) similarly, training language models with prompted claim-evidence understanding (TBE-2) registered an improvement (with a margin up to 16.39%) over the baselines for the same dataset, (iii) training language models with entailed justifications (TBE-3) outperformed the baselines by a huge margin (up to 28.57% and 44.26% for LIAR-RAW and RAW-FC, respectively). We have shared our code repository to reproduce the results.

</details>


### [61] [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)

*Feiyang Cai, Jiahui Bai, Tao Tang, Joshua Luo, Tianyu Zhu, Ling Liu, Feng Luo*

**Main category:** cs.CL

**Keywords:** molecular structures, AI evaluation, cheminformatics, molecular generation, language interface

**Relevance Score:** 3

**TL;DR:** MolLangBench is a benchmark for evaluating AI models in recognizing, editing, and generating molecular structures, revealing significant performance limitations in current systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The benchmark addresses the need for precise AI interactions in chemical tasks, including recognition, editing, and generation of molecular structures.

**Method:** MolLangBench was developed using automated cheminformatics tools for recognition tasks and expert validation for editing and generation tasks.

**Key Contributions:**

	1. Introduction of MolLangBench for molecule-language tasks
	2. Evaluation of AI model performances in chemistry
	3. Highlighting limitations in current molecular generation capabilities

**Result:** State-of-the-art models showed limited accuracy: 79.2% in recognition, 78.5% in editing, and only 29.0% in generation tasks, highlighting inadequacies in current AI approaches.

**Limitations:** Limited to three primary tasks (recognition, editing, generation) and may not encompass all aspects of molecular manipulation.

**Conclusion:** The findings underscore the need for improved AI systems for chemical applications, as current technologies struggle with basic molecular tasks.

**Abstract:** Precise recognition, editing, and generation of molecules are essential prerequisites for both chemists and AI systems tackling various chemical tasks. We present MolLangBench, a comprehensive benchmark designed to evaluate fundamental molecule-language interface tasks: language-prompted molecular structure recognition, editing, and generation. To ensure high-quality, unambiguous, and deterministic outputs, we construct the recognition tasks using automated cheminformatics tools, and curate editing and generation tasks through rigorous expert annotation and validation. MolLangBench supports the evaluation of models that interface language with different molecular representations, including linear strings, molecular images, and molecular graphs. Evaluations of state-of-the-art models reveal significant limitations: the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition and editing tasks, which are intuitively simple for humans, and performs even worse on the generation task, reaching only $29.0\%$ accuracy. These results highlight the shortcomings of current AI systems in handling even preliminary molecular recognition and manipulation tasks. We hope MolLangBench will catalyze further research toward more effective and reliable AI systems for chemical applications.

</details>


### [62] [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)

*Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Item Response Theory, Benchmarking, Model Evaluation, Human Preference

**Relevance Score:** 8

**TL;DR:** The paper critiques current LLM benchmarks and introduces PSN-IRT framework for improved measurement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There are inconsistencies in LLM benchmarks that fail to accurately reflect model capabilities, raising concerns about their effectiveness.

**Method:** The authors propose a new framework called Pseudo-Siamese Network for Item Response Theory (PSN-IRT), enhancing measurement of item characteristics and model abilities.

**Key Contributions:**

	1. Introduction of Pseudo-Siamese Network for Item Response Theory (PSN-IRT) framework
	2. Identification of major flaws in current LLM benchmarks
	3. Demonstration of effective smaller benchmark construction while improving measurement quality

**Result:** The PSN-IRT framework reveals significant shortcomings in existing benchmarks and shows that smaller benchmarks can be created while still aligning better with human preferences.

**Limitations:** 

**Conclusion:** Utilizing PSN-IRT leads to more reliable evaluations of model performance through better benchmarks.

**Abstract:** The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining main-stream prominent LLM benchmarks using results from diverse models. We first propose a new framework for accurate and reliable estimations of item characteristics and model abilities. Specifically, we propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive analysis which reveals significant and varied shortcomings in the measurement quality of current benchmarks. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.

</details>


### [63] [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)

*Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Knowledge Graph, Biomedical QA, Associative Thinking

**Relevance Score:** 9

**TL;DR:** Self-GIVE is a framework that enhances large language models (LLMs) using reinforcement learning to improve knowledge retrieval and reasoning in biomedical QA tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current methods of knowledge extrapolation in LLMs that rely on associative reasoning, which are inefficient and not generalizable.

**Method:** Self-GIVE employs a retrieve-RL framework that automatically facilitates associative thinking by extracting structured information and entity sets.

**Key Contributions:**

	1. Introduction of Self-GIVE for improved performance in biomedical QA tasks
	2. Significant reduction in token usage while maintaining high accuracy
	3. Demonstration of effective associative reasoning in smaller LLMs

**Result:** After fine-tuning, Self-GIVE improved performance in challenging biomedical QA tasks significantly, enhancing 3B and 7B models by up to 28.5%-71.4% and 78.6%-90.5%, respectively, while reducing token usage by more than 90%.

**Limitations:** 

**Conclusion:** Self-GIVE allows smaller LLMs to perform comparably to larger models like GPT-3.5 turbo, indicating its effectiveness in scalable structured retrieval and reasoning tasks.

**Abstract:** When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate "hormones helping mental disorders" with "melatonin being a hormone and insomnia a mental disorder" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and $\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.

</details>


### [64] [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)

*Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, Preslav Nakov*

**Main category:** cs.CL

**Keywords:** large language models, Urdu, fact-checking, NLP, machine learning

**Relevance Score:** 9

**TL;DR:** UrduFactCheck is the first fact-checking framework designed for Urdu, addressing the need for reliable outputs from LLMs in low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in LLM usage has raised concerns about their output reliability, particularly for low-resource languages such as Urdu, which lacks dedicated fact-checking solutions.

**Method:** UrduFactCheck employs a dynamic, multi-strategy evidence retrieval pipeline, combining monolingual and translation-based methods to create a robust fact-checking system.

**Key Contributions:**

	1. Introduction of the first fact-checking framework for Urdu
	2. Development of two new annotated benchmarks: UrduFactBench and UrduFactQA
	3. Demonstration of superior performance compared to baseline models in Urdu factuality challenges.

**Result:** Experiments reveal that UrduFactCheck, especially its translation-augmented variants, outperforms existing baselines and open-source solutions in multiple metrics.

**Limitations:** 

**Conclusion:** UrduFactCheck represents a significant advancement in addressing fact-checking in Urdu and provides open-source resources for further research.

**Abstract:** The rapid use of large language models (LLMs) has raised critical concerns regarding the factual reliability of their outputs, especially in low-resource languages such as Urdu. Existing automated fact-checking solutions overwhelmingly focus on English, leaving a significant gap for the 200+ million Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first comprehensive, modular fact-checking framework specifically tailored for Urdu. Our system features a dynamic, multi-strategy evidence retrieval pipeline that combines monolingual and translation-based approaches to address the scarcity of high-quality Urdu evidence. We curate and release two new hand-annotated benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating LLM factuality. Extensive experiments demonstrate that UrduFactCheck, particularly its translation-augmented variants, consistently outperforms baselines and open-source alternatives on multiple metrics. We further benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in Urdu, highlighting persistent gaps between proprietary and open-source models. UrduFactCheck's code and datasets are open-sourced and publicly available at https://github.com/mbzuai-nlp/UrduFactCheck.

</details>


### [65] [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)

*Suhas BN, Yash Mahajan, Dominik Mattioli, Andrew M. Sherrill, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah*

**Main category:** cs.CL

**Keywords:** language models, empathy, PTSD, HCI, dialogue systems

**Relevance Score:** 9

**TL;DR:** The paper introduces TIDE, a dataset for assessing small language models' ability to engage in empathetic dialogue for PTSD, revealing fine-tuning improves perceived empathy with varying results across scenarios and demographics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate if small language models can engage in trauma-informed dialogue for individuals with PTSD.

**Method:** Introducing TIDE, a dataset of 10,000 dialogues based on PTSD personas, using a three-factor empathy model and evaluating eight small language models pre- and post-fine-tuning against a frontier model.

**Key Contributions:**

	1. Introduction of the TIDE dataset for PTSD DIALOGUES
	2. Evaluation of small language models' empathetic engagement
	3. Insights into demographic preferences in dialogue response

**Result:** Fine-tuning generally enhances perceived empathy but varies by scenario and user demographics; smaller models may encounter an empathy ceiling.

**Limitations:** Reliance on automatic metrics and context-specific empathetic dialogue design.

**Conclusion:** The findings highlight the importance of context in AI dialogue systems and suggest a foundation for Safe empathetic AI in mental health applications.

**Abstract:** Can small language models with 0.5B to 5B parameters meaningfully engage in trauma-informed, empathetic dialogue for individuals with PTSD? We address this question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning 500 diverse PTSD client personas and grounded in a three-factor empathy model: emotion recognition, distress normalization, and supportive reflection. All scenarios and reference responses were reviewed for realism and trauma sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight small language models before and after fine-tuning, comparing their outputs to a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and automatic metrics show that fine-tuning generally improves perceived empathy, but gains are highly scenario- and user-dependent, with smaller models facing an empathy ceiling. Demographic analysis shows older adults value distress validation and graduate-educated users prefer nuanced replies, while gender effects are minimal. We highlight the limitations of automatic metrics and the need for context- and user-aware system design. Our findings, along with the planned release of TIDE, provide a foundation for building safe, resource-efficient, and ethically sound empathetic AI to supplement, not replace, clinical mental health care.

</details>


### [66] [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)

*Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, bandit algorithms, domain adaptation, low-resource languages, model selection

**Relevance Score:** 4

**TL;DR:** This paper explores bandit-based algorithms for selecting optimal Neural Machine Translation models for low-resource languages in domain adaptation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Neural Machine Translation systems struggle with low-resource languages and domain adaptation due to limited training data and poor model generalization.

**Method:** The paper investigates strategies for model selection using bandit algorithms such as Upper Confidence Bound, Linear UCB, Neural Linear Bandit, and Thompson Sampling.

**Key Contributions:**

	1. Introduction of bandit-based model selection for NMT in low-resource settings
	2. Evaluation of the approach across multiple African languages
	3. Demonstration of effectiveness in absence of target data

**Result:** The proposed method effectively enables optimal model selection with high confidence and is evaluated across three African languages, demonstrating robustness in various scenarios.

**Limitations:** 

**Conclusion:** The approach is effective for both scenarios with and without target data, improving performance in translating low-resource languages.

**Abstract:** Neural Machine Translation (NMT) systems face significant challenges when working with low-resource languages, particularly in domain adaptation tasks. These difficulties arise due to limited training data and suboptimal model generalization, As a result, selecting an optimal model for translation is crucial for achieving strong performance on in-domain data, particularly in scenarios where fine-tuning is not feasible or practical. In this paper, we investigate strategies for selecting the most suitable NMT model for a given domain using bandit-based algorithms, including Upper Confidence Bound, Linear UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively addresses the resource constraints by facilitating optimal model selection with high confidence. We evaluate the approach across three African languages and domains, demonstrating its robustness and effectiveness in both scenarios where target data is available and where it is absent.

</details>


### [67] [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)

*Chen Huang, Junkai Luo, Xinzuo Wang, Wenqiang Lei, Jiancheng Lv*

**Main category:** cs.CL

**Keywords:** Large Language Models, Buzzwords, User-Generated Content, Definition Generation, Chinese Social Media

**Relevance Score:** 8

**TL;DR:** This paper studies the generation of definitions for internet buzzwords from user-generated content using large language models (LLMs), introducing a new dataset and a novel method for improved accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capability of LLMs in generating definitions for Chinese internet buzzwords based on massive user-generated content from social media.

**Method:** Introduces CHEER, a dataset of Chinese internet buzzwords, and proposes RESS, a method to guide LLMs in generating accurate definitions.

**Key Contributions:**

	1. Introduction of the CHEER dataset for Chinese internet buzzwords.
	2. Development of the RESS method to enhance LLM comprehension.
	3. Benchmarking various definition generation methods against RESS.

**Result:** The benchmark demonstrates RESS's effectiveness and highlights challenges such as over-reliance on prior exposure and the need for high-quality UGC.

**Limitations:** The study reveals shared challenges in LLMs, including the over-reliance on prior exposure and difficulties in identifying quality UGC.

**Conclusion:** The paper contributes a dataset and methodology that pave the way for better LLM-based definition generation, with implications for future research.

**Abstract:** The massive user-generated content (UGC) available in Chinese social media is giving rise to the possibility of studying internet buzzwords. In this paper, we study if large language models (LLMs) can generate accurate definitions for these buzzwords based on UGC as examples. Our work serves a threefold contribution. First, we introduce CHEER, the first dataset of Chinese internet buzzwords, each annotated with a definition and relevant UGC. Second, we propose a novel method, called RESS, to effectively steer the comprehending process of LLMs to produce more accurate buzzword definitions, mirroring the skills of human language learning. Third, with CHEER, we benchmark the strengths and weaknesses of various off-the-shelf definition generation methods and our RESS. Our benchmark demonstrates the effectiveness of RESS while revealing crucial shared challenges: over-reliance on prior exposure, underdeveloped inferential abilities, and difficulty identifying high-quality UGC to facilitate comprehension. We believe our work lays the groundwork for future advancements in LLM-based definition generation. Our dataset and code are available at https://github.com/SCUNLP/Buzzword.

</details>


### [68] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)

*Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, Furong Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Domain Imbalance, Human Feedback, Policy Optimization

**Relevance Score:** 9

**TL;DR:** The paper introduces DISCO, a novel extension to GRPO, enhancing language model training by addressing domain imbalance and fairness in multi-domain settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness and fairness of LLM training methods that rely on human feedback, especially in scenarios with imbalanced data distributions.

**Method:** DISCO extends GRPO using domain-aware and difficulty-aware reward scaling to better handle underrepresented domains and prioritize uncertain prompts during training.

**Key Contributions:**

	1. Introduction of Domain-Informed Self-Consistency Policy Optimization (DISCO)
	2. Implementation of domain-aware reward scaling
	3. Introduction of difficulty-aware reward scaling to identify uncertain prompts

**Result:** DISCO improves generalization and outperforms existing GRPO variants by 5% on Qwen3 models, achieving state-of-the-art results on multi-domain alignment tasks.

**Limitations:** 

**Conclusion:** The proposed method promotes equitable learning across various domains, addressing limitations seen in traditional GRPO approaches.

**Abstract:** Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks.

</details>


### [69] [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)

*Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara*

**Main category:** cs.CL

**Keywords:** multimodal large language models, cross-lingual consistency, cultural knowledge

**Relevance Score:** 8

**TL;DR:** This paper introduces two benchmarks for assessing cross-lingual consistency in multimodal large language models: KnowRecall for factual knowledge and VisRecall for visual memory.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of consistent performance across languages in MLLMs, particularly in the context of cultural knowledge integration.

**Method:** Two benchmarks are introduced: KnowRecall for evaluating visual question answering consistency in 15 languages, and VisRecall for assessing visual memory consistency in 9 languages without images.

**Key Contributions:**

	1. Introduction of KnowRecall benchmark for cross-lingual factual knowledge consistency
	2. Development of VisRecall benchmark for visual memory assessment in multiple languages
	3. Empirical evidence of current MLLM struggles with cross-lingual consistency

**Result:** Experimental results show that state-of-the-art MLLMs struggle with cross-lingual consistency, indicating a gap in current models regarding multilingual and culturally aware applications.

**Limitations:** 

**Conclusion:** There is a need for more robust approaches in MLLMs to achieve true multilinguality and cultural awareness with consistent performance across languages.

**Abstract:** The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.

</details>


### [70] [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)

*Zhiyu Shen, Jiyuan Liu, Yunhe Pang, Yanghui Rao*

**Main category:** cs.CL

**Keywords:** multi-hop question answering, dataset synthesis, natural language processing

**Relevance Score:** 7

**TL;DR:** Introduction of HopWeaver, an automated framework for synthesizing multi-hop questions from unstructured text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of creating extensive and high-quality multi-hop question answering datasets, which are hindered by expensive manual annotation and limitations of current synthesis methods.

**Method:** HopWeaver synthesizes two types of multi-hop questions (bridge and comparison) by identifying complementary documents across corpora and constructing reasoning paths that connect information within multiple documents.

**Key Contributions:**

	1. First automatic framework for synthesizing multi-hop questions without human intervention
	2. Synthesis of bridge and comparison questions by identifying complementary documents
	3. Public availability of code for further research and application

**Result:** Empirical evaluations show that the synthesized multi-hop questions from HopWeaver achieve quality comparable or superior to human-annotated datasets while significantly reducing the cost of annotation.

**Limitations:** 

**Conclusion:** HopWeaver provides a valuable tool for the development of multi-hop question answering datasets, especially in specialized domains with limited annotated resources.

**Abstract:** Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's capability to integrate information from diverse sources. However, creating extensive and high-quality MHQA datasets is challenging: (i) manual annotation is expensive, and (ii) current synthesis methods often produce simplistic questions or require extensive manual guidance. This paper introduces HopWeaver, the first automatic framework synthesizing authentic multi-hop questions from unstructured text corpora without human intervention. HopWeaver synthesizes two types of multi-hop questions (bridge and comparison) using an innovative approach that identifies complementary documents across corpora. Its coherent pipeline constructs authentic reasoning paths that integrate information across multiple documents, ensuring synthesized questions necessitate authentic multi-hop reasoning. We further present a comprehensive system for evaluating synthesized multi-hop questions. Empirical evaluations demonstrate that the synthesized questions achieve comparable or superior quality to human-annotated datasets at a lower cost. Our approach is valuable for developing MHQA datasets in specialized domains with scarce annotated resources. The code for HopWeaver is publicly available.

</details>


### [71] [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)

*Sona Elza Simon, Preethi Jyothi*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, sparse fine-tuning, low-resource languages, singular value decomposition, NLP

**Relevance Score:** 8

**TL;DR:** DeFT-X enhances cross-lingual transfer by improving composable sparse fine-tuning using denoised weight matrices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in cross-lingual transfer from high-resource to low-resource languages using large language models.

**Method:** DeFT-X introduces denoising of weight matrices of pretrained models before applying magnitude pruning through singular value decomposition.

**Key Contributions:**

	1. Introduction of DeFT-X for denoising weight matrices before pruning
	2. Evaluation on low-resource languages
	3. Demonstration of performance superiority over baseline methods

**Result:** DeFT-X shows performance on par with or better than previous sparse fine-tuning methods on sentiment classification and natural language inference tasks in extremely low-resource languages.

**Limitations:** 

**Conclusion:** The proposed DeFT-X method yields more robust sparse fine-tuned vectors, enhancing the effectiveness of cross-lingual transfer in NLP tasks.

**Abstract:** Effective cross-lingual transfer remains a critical challenge in scaling the benefits of large language models from high-resource to low-resource languages. Towards this goal, prior studies have explored many approaches to combine task knowledge from task-specific data in a (high-resource) source language and language knowledge from unlabeled text in a (low-resource) target language. One notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual transfer that learns task-specific and language-specific sparse masks to select a subset of the pretrained model's parameters that are further fine-tuned. These sparse fine-tuned vectors (SFTs) are subsequently composed with the pretrained model to facilitate zero-shot cross-lingual transfer to a task in a target language, using only task-specific data from a source language. These sparse masks for SFTs were identified using a simple magnitude-based pruning. In our work, we introduce DeFT-X, a novel composable SFT approach that denoises the weight matrices of a pretrained model before magnitude pruning using singular value decomposition, thus yielding more robust SFTs. We evaluate DeFT-X on a diverse set of extremely low-resource languages for sentiment classification (NusaX) and natural language inference (AmericasNLI) and demonstrate that it performs at par or outperforms SFT and other prominent cross-lingual transfer baselines.

</details>


### [72] [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)

*Jing Yu, Yuqi Tang, Kehua Feng, Mingyang Rao, Lei Liang, Zhiqiang Zhang, Mengshu Sun, Wen Zhang, Qiang Zhang, Keyan Ding, Huajun Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, benchmark dataset, scientific context, evaluation, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents SciCUEval, a benchmark dataset for evaluating the performance of Large Language Models (LLMs) in scientific contexts, addressing current gaps in assessment methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for LLMs focus on general domains, lacking depth in evaluating their abilities in complex scientific domains.

**Method:** The authors developed SciCUEval, which includes ten domain-specific sub-datasets across various scientific disciplines. It evaluates LLM capabilities in four key competencies through diverse question formats.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark dataset for evaluating LLMs in scientific contexts.
	2. Systematic assessment of LLM capabilities in relevant information identification and context-aware inference.
	3. Insights generated from evaluations to guide future LLM improvements in scientific domains.

**Result:** Extensive evaluations showed how state-of-the-art LLMs perform on the SciCUEval benchmark, revealing specific strengths and weaknesses in understanding scientific contexts.

**Limitations:** Limited to the specific scientific domains included in the dataset; performance may vary with unseen domains.

**Conclusion:** These insights will help inform the future design and development of LLMs tailored for scientific applications.

**Abstract:** Large Language Models (LLMs) have shown impressive capabilities in contextual understanding and reasoning. However, evaluating their performance across diverse scientific domains remains underexplored, as existing benchmarks primarily focus on general domains and fail to capture the intricate complexity of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive benchmark dataset tailored to assess the scientific context understanding capability of LLMs. It comprises ten domain-specific sub-datasets spanning biology, chemistry, physics, biomedicine, and materials science, integrating diverse data modalities including structured tables, knowledge graphs, and unstructured texts. SciCUEval systematically evaluates four core competencies: Relevant information identification, Information-absence detection, Multi-source information integration, and Context-aware inference, through a variety of question formats. We conduct extensive evaluations of state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their strengths and limitations in scientific context understanding, and offering valuable insights for the future development of scientific-domain LLMs.

</details>


### [73] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)

*Ishmanbir Singh, Dipankar Srirag, Aditya Joshi*

**Main category:** cs.CL

**Keywords:** sarcasm detection, Pragmatic Metacognitive Prompting, sentiment analysis, NLP, explainable AI

**Relevance Score:** 7

**TL;DR:** This study addresses the challenge of sarcasm detection in sentiment analysis by using Pragmatic Metacognitive Prompting (PMP), particularly for Australian and Indian English, and benchmarks it against standard English.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Sarcasm's incongruity poses a significant challenge for sentiment analysis, especially when it relates to specific geographical contexts. The research seeks to improve explainable sarcasm detection through PMP.

**Method:** The authors enhance an existing dataset (BESSTIE) for Australian and Indian English with sarcasm explanations and evaluate their approach using two open-weight LLMs (GEMMA and LLAMA), comparing it against alternative prompting strategies.

**Key Contributions:**

	1. Introduction of Pragmatic Metacognitive Prompting (PMP) for sarcasm detection.
	2. Creation of a benchmark dataset with sarcasm explanations for Australian and Indian English.
	3. Demonstration of significant performance improvements in sarcasm detection using PMP against alternative strategies.

**Result:** The proposed method achieves statistically significant performance improvements in detecting explainable sarcasm across various tasks and datasets compared to standard prompting techniques.

**Limitations:** 

**Conclusion:** Utilizing PMP effectively enhances the generation of sarcasm explanations for different varieties of English, contributing to better sentiment analysis in specific contexts.

**Abstract:** Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.

</details>


### [74] [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)

*Aryaman Arora, Neil Rathi, Nikil Roashan Selvam, Róbert Csórdas, Dan Jurafsky, Christopher Potts*

**Main category:** cs.CL

**Keywords:** state space models, Associative Recall, Transformers, mechanistic evaluations, language modeling

**Relevance Score:** 7

**TL;DR:** This paper investigates the performance of state space models (SSMs) in Associative Recall tasks and introduces the Associative Treecall (ATR) task to evaluate the underlying mechanisms in architectures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why different architectures perform variably in recalling information from context, focusing on state space models and their comparison with Transformers.

**Method:** Experiments on Associative Recall (AR) tasks and introduction of the Associative Treecall (ATR) task, applied mechanical evaluations via causal interventions to determine the learning mechanisms of different architectures.

**Key Contributions:**

	1. Introduced Associative Treecall (ATR) for exploring architectural performance
	2. Demonstrated varying performance of state space models in recall tasks
	3. Highlighted the role of architectural mechanisms in model performance

**Result:** Transformers and Based SSM models excel in AR tasks, while others, like H3 and Hyena, fail. The Mamba model succeeds due to a short convolution component. ATR results mirrored those of AR, reinforcing findings about model mechanisms.

**Limitations:** 

**Conclusion:** The study highlights the necessity of mechanistic evaluations in language models, revealing that similar accuracy can mask substantial architectural differences.

**Abstract:** State space models (SSMs) for language modelling promise an efficient and performant alternative to quadratic-attention Transformers, yet show variable performance on recalling basic information from the context. While performance on synthetic tasks like Associative Recall (AR) can point to this deficiency, behavioural metrics provide little information as to why--on a mechanistic level--certain architectures fail and others succeed. To address this, we conduct experiments on AR and find that only Transformers and Based SSM models fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3, Hyena) fail. We then use causal interventions to explain why. We find that Transformers and Based learn to store key-value associations in-context using induction heads. By contrast, the SSMs compute these associations only at the last state, with only Mamba succeeding because of its short convolution component. To extend and deepen these findings, we introduce Associative Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR introduces language-like hierarchical structure into the AR setting. We find that all architectures learn the same mechanism as they did for AR, and the same three models succeed at the task. These results reveal that architectures with similar accuracy may still have substantive differences, motivating the adoption of mechanistic evaluations.

</details>


### [75] [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)

*Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-hop question answering, reinforcement learning

**Relevance Score:** 9

**TL;DR:** StepSearch is a framework designed for search LLMs, improving multi-hop QA performance through detailed intermediate rewards and token-level supervision.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing models struggle with complex, multi-hop question-answering tasks due to sparse global rewards. This paper introduces a new framework to overcome these challenges.

**Method:** The StepSearch framework utilizes a step-wise proximal policy optimization method, incorporating richer intermediate rewards and token-level supervision to optimize the search process.

**Key Contributions:**

	1. Introduction of StepSearch framework for LLMs in multi-hop QA
	2. Development of a fine-grained question-answering dataset
	3. Demonstrated significant improvements over existing RL baselines

**Result:** On standard multi-hop QA benchmarks, StepSearch significantly outperforms existing global-reward baselines, achieving notable absolute improvements for both 3B and 7B parameter models with a small training dataset.

**Limitations:** 

**Conclusion:** Fine-grained, stepwise supervision is effective in optimizing deep search LLMs for improved performance in multi-hop question-answering tasks.

**Abstract:** Efficient multi-hop reasoning requires Large Language Models (LLMs) based agents to acquire high-value external knowledge iteratively. Previous work has explored reinforcement learning (RL) to train LLMs to perform search-based document retrieval, achieving notable improvements in QA performance, but underperform on complex, multi-hop QA resulting from the sparse rewards from global signal only. To address this gap in existing research, we introduce StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method. It consists of richer and more detailed intermediate search rewards and token-level process supervision based on information gain and redundancy penalties to better guide each search step. We constructed a fine-grained question-answering dataset containing sub-question-level search trajectories based on open source datasets through a set of data pipeline method. On standard multi-hop QA benchmarks, it significantly outperforms global-reward baselines, achieving 11.2% and 4.2% absolute improvements for 3B and 7B models over various search with RL baselines using only 19k training data, demonstrating the effectiveness of fine-grained, stepwise supervision in optimizing deep search LLMs. Our implementation is publicly available at https://github.com/zxh20001117/StepSearch.

</details>


### [76] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)

*Ian Steenstra, Timothy W. Bickmore*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI psychotherapists, mental healthcare, risk evaluation, user safety

**Relevance Score:** 9

**TL;DR:** Proposes a risk taxonomy for evaluating conversational AI psychotherapists to improve mental healthcare access while addressing user harm.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of standardized evaluation methodologies for Large Language Models (LLMs) acting as psychotherapists and their associated risks.

**Method:** Developed through iterative reviews of psychotherapy risk literature, expert interviews, and alignment with clinical criteria and assessment tools.

**Key Contributions:**

	1. Introduction of a risk taxonomy for evaluating AI psychotherapists
	2. Addressing the nuanced risks in therapeutic interactions
	3. Providing a structured approach for identifying user harms

**Result:** Introduced a structured risk taxonomy that helps in identifying and assessing potential user/patient harms during AI therapy sessions.

**Limitations:** 

**Conclusion:** This taxonomy represents a foundational step towards safer innovations in AI-driven mental health support, emphasizing the need for systematic evaluation.

**Abstract:** The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.

</details>


### [77] [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)

*Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** table reasoning, Row-of-Thought, large language models, machine learning, data acquisition

**Relevance Score:** 7

**TL;DR:** The paper introduces Row-of-Thought (RoT), an iterative table reasoning approach that enhances performance and reduces hallucinations compared to existing methods like Long Chain-of-Thought.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and reliability of table reasoning tasks while mitigating issues related to hallucinations in large language models.

**Method:** RoT utilizes an iterative row-wise traversal of tables, promoting reflection and refinement without the need for extensive training.

**Key Contributions:**

	1. Introduction of Row-of-Thought (RoT) method
	2. Demonstrated enhancement in performance over Long Chain-of-Thought
	3. Training-free approach that reduces hallucination effects

**Result:** RoT outperforms reasoning large language models by 4.3% on average and achieves state-of-the-art results on specific benchmarks with reduced reasoning tokens.

**Limitations:** 

**Conclusion:** RoT proves to be a more efficient and effective approach for table reasoning tasks compared to traditional Long Chain-of-Thought methods.

**Abstract:** The table reasoning task, crucial for efficient data acquisition, aims to answer questions based on the given table. Recently, reasoning large language models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance reasoning capabilities, leading to brilliant performance on table reasoning. However, Long CoT suffers from high cost for training and exhibits low reliability due to table content hallucinations. Therefore, we propose Row-of-Thought (RoT), which performs iteratively row-wise table traversal, allowing for reasoning extension and reflection-based refinement at each traversal. Scaling reasoning length by row-wise traversal and leveraging reflection capabilities of LLMs, RoT is training-free. The sequential traversal encourages greater attention to the table, thus reducing hallucinations. Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions and TableBench with comparable models, proving its effectiveness. Also, RoT outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.

</details>


### [78] [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)

*Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Search Agents, Empirical Studies, Real-World Applications

**Relevance Score:** 8

**TL;DR:** This paper investigates the design of reinforcement learning (RL) based LLM search agents, focusing on reward formulation, LLM characteristics, and search engine role, providing actionable insights for real-world applications.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the growing utilization of RL in training LLMs for enhanced reasoning in solving real-world problems, particularly in developing effective search agents.

**Method:** Comprehensive empirical studies were conducted to understand the impacts of reward formulation, LLM characteristics, and search engine interactions on RL-based search agents.

**Key Contributions:**

	1. Identified effective reward strategies for RL-based LLM training.
	2. Analyzed the influence of LLM characteristics on RL outcomes.
	3. Demonstrated the critical role of search engines in RL dynamics.

**Result:** The findings indicate that format rewards enhance performance, LLM scaling and initialization significantly affect RL outcomes, and the search engine choice greatly influences training and inference dynamics.

**Limitations:** 

**Conclusion:** The paper outlines essential guidelines for the successful design and deployment of LLM-based search agents, emphasizing the importance of the investigated factors.

**Abstract:** Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [79] [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)

*Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, Han Wang, Can Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multimodal Reasoning, Adaptive Reasoning, Efficiency, VQA

**Relevance Score:** 9

**TL;DR:** This paper introduces Certainty-based Adaptive Reasoning (CAR), a framework that optimizes reasoning in LLMs by dynamically switching between short and long answers based on model perplexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve efficiency and accuracy in LLM reasoning tasks by addressing the drawbacks of excessive reliance on chain-of-thought reasoning.

**Method:** CAR first generates a short answer, evaluates its perplexity, and triggers longer reasoning only when the model shows low confidence.

**Key Contributions:**

	1. Introduction of the CAR framework for adaptive reasoning in LLMs
	2. Empirical validation through benchmarks showing performance improvements over existing methods
	3. Demonstration of efficiency gains by reducing unnecessary long-form reasoning

**Result:** Experiments indicate that CAR outperforms traditional short-answer and long-form approaches across various multimodal VQA/KIE benchmarks and text reasoning datasets.

**Limitations:** Further exploration is needed for edge cases where CAR might not perform optimally compared to other methods.

**Conclusion:** CAR effectively balances accuracy and efficiency in LLM reasoning, demonstrating that not all tasks benefit from prolonged reasoning.

**Abstract:** Recent advancements in reasoning have significantly enhanced the capabilities of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) across diverse tasks. However, excessive reliance on chain-of-thought (CoT) reasoning can impair model performance and brings unnecessarily lengthened outputs, reducing efficiency. Our work reveals that prolonged reasoning does not universally improve accuracy and even degrade performance on simpler tasks. To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel framework that dynamically switches between short answers and long-form reasoning based on the model perplexity. CAR first generates a short answer and evaluates its perplexity, triggering reasoning only when the model exhibits low confidence (i.e., high perplexity). Experiments across diverse multimodal VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both short-answer and long-form reasoning approaches, striking an optimal balance between accuracy and efficiency.

</details>


### [80] [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)

*Jeonghye Kim, Sojeong Rhee, Minbeom Kim, Dohyung Kim, Sangmook Lee, Youngchul Sung, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** ReflAct, LLM agents, goal alignment, reasoning, agent performance

**Relevance Score:** 9

**TL;DR:** Introduction of ReflAct, a new reasoning backbone for LLM agents that improves strategic reliability by maintaining consistent internal beliefs and goal alignment.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM reasoning frameworks like ReAct struggle with coherence and alignments, leading to errors and hallucinations in agent behavior.

**Method:** ReflAct shifts the focus from simply planning actions to continuously reflecting on the agent's state in relation to its goals, enforcing ongoing goal alignment.

**Key Contributions:**

	1. Introduction of ReflAct as a novel reasoning framework for LLM agents.
	2. Demonstration that continuous reflection improves goal alignment and reduces hallucinations.
	3. Empirical results showing substantial performance gains over existing methods.

**Result:** ReflAct improves performance by 27.7% compared to ReAct, achieving a 93.3% success rate in the ALFWorld environment.

**Limitations:** 

**Conclusion:** Strengthening the core reasoning mechanisms of LLMs through continuous reflection and goal alignment leads to more reliable agent performance.

**Abstract:** Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent's actual state and goal. Our analysis finds that this stems from ReAct's inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent's state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.

</details>


### [81] [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)

*Weiqi Wang, Limeng Cui, Xin Liu, Sreyashi Nag, Wenju Xu, Chen Luo, Sheikh Muhammad Sarwar, Yang Li, Hansu Gu, Hui Liu, Changlong Yu, Jiaxin Bai, Yifan Gao, Haiyang Zhang, Qi He, Shuiwang Ji, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** E-commerce, Script Planning, Large Language Models, Dataset Creation, Product Recommendations

**Relevance Score:** 8

**TL;DR:** This paper introduces E-commerce Script Planning (EcomScript), a framework for generating product-enriched action scripts for e-commerce, along with a large-scale dataset for evaluation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of LLM-based assistants in e-commerce requires effective script planning to enhance shopping experiences, yet existing methodologies face significant challenges.

**Method:** The authors define EcomScript as three sequential subtasks and propose a framework that associates products with actions based on semantic similarity between planned actions and purchase intentions. They also construct EcomScriptBench, a dataset with human-annotated samples for evaluation.

**Key Contributions:**

	1. Formal definition of E-commerce Script Planning (EcomScript) as three subtasks.
	2. Introduction of a novel framework for scalable generation of product-enriched scripts.
	3. Creation of the EcomScriptBench dataset containing 605,229 action scripts.

**Result:** Extensive experiments show that current LLMs struggle with EcomScript tasks, and incorporating product purchase intentions leads to performance improvements.

**Limitations:** Current LLMs still face considerable challenges with EcomScript tasks, indicating room for improvement and further exploration.

**Conclusion:** The findings highlight the challenges in LLMs' ability to conduct script planning and suggest that product purchase intentions can significantly enhance their performance in e-commerce scenarios.

**Abstract:** Goal-oriented script planning, or the ability to devise coherent sequences of actions toward specific goals, is commonly employed by humans to plan for typical activities. In e-commerce, customers increasingly seek LLM-based assistants to generate scripts and recommend products at each step, thereby facilitating convenient and efficient shopping experiences. However, this capability remains underexplored due to several challenges, including the inability of LLMs to simultaneously conduct script planning and product retrieval, difficulties in matching products caused by semantic discrepancies between planned actions and search queries, and a lack of methods and benchmark data for evaluation. In this paper, we step forward by formally defining the task of E-commerce Script Planning (EcomScript) as three sequential subtasks. We propose a novel framework that enables the scalable generation of product-enriched scripts by associating products with each step based on the semantic similarity between the actions and their purchase intentions. By applying our framework to real-world e-commerce data, we construct the very first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229 scripts sourced from 2.4 million products. Human annotations are then conducted to provide gold labels for a sampled subset, forming an evaluation benchmark. Extensive experiments reveal that current (L)LMs face significant challenges with EcomScript tasks, even after fine-tuning, while injecting product purchase intentions improves their performance.

</details>


### [82] [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)

*Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No*

**Main category:** cs.CL

**Keywords:** machine unlearning, large language models, data privacy, benchmark, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces DUSK, a benchmark designed to evaluate unlearning methods for large language models in scenarios where forget and retain data sets overlap, highlighting the limitations of existing methods.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** With the rising deployment of large language models, concerns arise regarding the unauthorized use of copyrighted or sensitive data, necessitating effective unlearning techniques that respect data retention.

**Method:** DUSK constructs document sets that contain overlapping and unique information to evaluate how well unlearning methods can selectively remove content from designated sets while preserving shared facts.

**Key Contributions:**

	1. Introduction of the DUSK benchmark for evaluating unlearning methods with realistic data overlap
	2. Definition of seven evaluation metrics for selective content removal
	3. Analysis revealing limitations in current unlearning methods' effectiveness

**Result:** Evaluation of nine recent unlearning methods shows a significant limitation in their capability to erase context-specific knowledge while retaining shared factual content across overlapping data sets.

**Limitations:** Existing evaluations typically ignore overlapping content assumptions, limiting their applicability.

**Conclusion:** The study emphasizes the need for refined unlearning techniques, and DUSK serves as a public benchmark for further development in this area.

**Abstract:** Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about the unauthorized use of copyrighted or sensitive data. Machine unlearning aims to remove such 'forget' data while preserving utility and information from the 'retain' set. However, existing evaluations typically assume that forget and retain sets are fully disjoint, overlooking realistic scenarios where they share overlapping content. For instance, a news article may need to be unlearned, even though the same event, such as an earthquake in Japan, is also described factually on Wikipedia. Effective unlearning should remove the specific phrasing of the news article while preserving publicly supported facts. In this paper, we introduce DUSK, a benchmark designed to evaluate unlearning methods under realistic data overlap. DUSK constructs document sets that describe the same factual content in different styles, with some shared information appearing across all sets and other content remaining unique to each. When one set is designated for unlearning, an ideal method should remove its unique content while preserving shared facts. We define seven evaluation metrics to assess whether unlearning methods can achieve this selective removal. Our evaluation of nine recent unlearning methods reveals a key limitation: while most can remove surface-level text, they often fail to erase deeper, context-specific knowledge without damaging shared content. We release DUSK as a public benchmark to support the development of more precise and reliable unlearning techniques for real-world applications.

</details>


### [83] [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)

*Jie Ma, Ning Qu, Zhitao Gao, Rui Xing, Jun Liu, Hongbin Pei, Jiang Xie, Linyun Song, Pinghui Wang, Jing Tao, Zhou Su*

**Main category:** cs.CL

**Keywords:** Knowledge graphs, Language models, Augmented generation, Reasoning, Data reliability

**Relevance Score:** 9

**TL;DR:** The paper presents Deliberation over Priors (DP), a framework for improving LLM reliability and reasoning by effectively using knowledge graph priors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate hallucinations in LLMs that arise from inadequate or outdated knowledge by leveraging the structural information and constraints within knowledge graphs.

**Method:** DP employs a progressive knowledge distillation strategy integrating structural priors through supervised fine-tuning and Kahneman-Tversky optimization, along with a reasoning-introspection strategy for reliable response generation.

**Key Contributions:**

	1. Introduced a trustworthy reasoning framework using knowledge graph priors.
	2. Achieved state-of-the-art results on benchmark datasets.
	3. Developed a novel reasoning-introspection strategy for improved response reliability.

**Result:** DP achieves state-of-the-art performance, particularly a 13% improvement on the Hit@1 metric for the ComplexWebQuestions dataset, producing highly trustworthy responses.

**Limitations:** 

**Conclusion:** The framework demonstrates flexibility and practicality in enhancing LLM reasoning with knowledge graph priors, verified through extensive experiments.

**Abstract:** Knowledge graph-based retrieval-augmented generation seeks to mitigate hallucinations in Large Language Models (LLMs) caused by insufficient or outdated knowledge. However, existing methods often fail to fully exploit the prior knowledge embedded in knowledge graphs (KGs), particularly their structural information and explicit or implicit constraints. The former can enhance the faithfulness of LLMs' reasoning, while the latter can improve the reliability of response generation. Motivated by these, we propose a trustworthy reasoning framework, termed Deliberation over Priors (DP), which sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a progressive knowledge distillation strategy that integrates structural priors into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky optimization, thereby improving the faithfulness of relation path generation. Furthermore, our framework employs a reasoning-introspection strategy, which guides LLMs to perform refined reasoning verification based on extracted constraint priors, ensuring the reliability of response generation. Extensive experiments on three benchmark datasets demonstrate that DP achieves new state-of-the-art performance, especially a Hit@1 improvement of 13% on the ComplexWebQuestions dataset, and generates highly trustworthy responses. We also conduct various analyses to verify its flexibility and practicality. The code is available at https://github.com/reml-group/Deliberation-on-Priors.

</details>


### [84] [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)

*Sangyeon Yoon, Wonje Jeung, Albert No*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, unlearning, chain-of-thought, Reasoned IDK, benchmark

**Relevance Score:** 7

**TL;DR:** This paper introduces Reasoning-TOFU (R-TOFU), a benchmark for assessing unlearning in Large Reasoning Models (LRMs), focusing on the challenges of preserving reasoning capabilities while effectively forgetting private or copyrighted information.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of reliable unlearning in Large Reasoning Models, which embed sensitive information throughout chain-of-thought traces, necessitating a targeted benchmark for proper evaluation.

**Method:** The authors present R-TOFU, which includes realistic chain-of-thought annotations and step-wise metrics to assess unlearning effectiveness beyond answer-level evaluations. They compare gradient-based and preference-optimization methods, proposing Reasoned IDK as a new approach.

**Key Contributions:**

	1. Introduction of the R-TOFU benchmark for unlearning in LRMs.
	2. Development of Reasoned IDK, which improves the balance between unlearning efficacy and utility.
	3. Identification of failure modes in unlearning processes under different decoding scenarios.

**Result:** The study shows that traditional answer-only objectives leave significant unlearning traces in LRMs and that Reasoned IDK provides better balance between forgetting sensitive information and maintaining model utility.

**Limitations:** The benchmark and findings emphasize the need for more diverse evaluations of unlearning effectiveness in various decoding settings.

**Conclusion:** The introduction of R-TOFU and its comprehensive analysis creates a foundation for studying unlearning in LRMs, highlighting the need for diverse decoding evaluations to accurately assess model performance after unlearning.

**Abstract:** Large Reasoning Models (LRMs) embed private or copyrighted information not only in their final answers but also throughout multi-step chain-of-thought (CoT) traces, making reliable unlearning far more demanding than in standard LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to this setting. R-TOFU augments existing unlearning tasks with realistic CoT annotations and provides step-wise metrics that expose residual knowledge invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive comparison of gradient-based and preference-optimization baselines and show that conventional answer-only objectives leave substantial forget traces in reasoning. We further propose Reasoned IDK, a preference-optimization variant that preserves coherent yet inconclusive reasoning, achieving a stronger balance between forgetting efficacy and model utility than earlier refusal styles. Finally, we identify a failure mode: decoding variants such as ZeroThink and LessThink can still reveal forgotten content despite seemingly successful unlearning, emphasizing the need to evaluate models under diverse decoding settings. Together, the benchmark, analysis, and new baseline establish a systematic foundation for studying and improving unlearning in LRMs while preserving their reasoning capabilities.

</details>


### [85] [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)

*Qihan Wang, Shidong Pan, Tal Linzen, Emily Black*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual prompting, diversity, cultural representation, LLM performance

**Relevance Score:** 9

**TL;DR:** This paper proposes multilingual prompting as a method to enhance cultural representation in Large Language Models (LLMs) by generating diverse responses through cultural and linguistic cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of cultural representation and diversity in LLMs' outputs.

**Method:** Multilingual prompting is introduced, which generates varied prompts by incorporating cultural and linguistic cues from multiple cultures, produces responses, and then combines the results.

**Key Contributions:**

	1. Introduces multilingual prompting as a novel technique for enhancing diversity in LLMs.
	2. Demonstrates the superiority of multilingual prompting over existing diversity-enhancing methods.
	3. Analyzes the impact of language resource levels and model sizes on the effectiveness of the technique.

**Result:** Experiments show that multilingual prompting outperforms existing techniques aimed at increasing diversity in generated outputs, with effectiveness varying by language resource level and model size.

**Limitations:** 

**Conclusion:** Aligning the prompting language with the cultural cues effectively reduces hallucinations about culturally-specific information.

**Abstract:** Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and personas prompting. Further analyses show that the benefits of multilingual prompting vary with language resource level and model size, and that aligning the prompting language with the cultural cues reduces hallucination about culturally-specific information.

</details>


### [86] [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)

*Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng*

**Main category:** cs.CL

**Keywords:** large language models, temporal reasoning, explainable AI, temporal knowledge graphs

**Relevance Score:** 9

**TL;DR:** This paper addresses the gap in explainable reasoning processes of LLMs in temporal reasoning by introducing GETER, a structure-aware generative framework that integrates graph structures and text for improved explainability and performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the explainable reasoning capabilities of LLMs in temporal reasoning, which are often overlooked in current research focused primarily on performance enhancement.

**Method:** The authors developed GETER, which uses temporal knowledge graphs to create a temporal encoder that captures structural information. It includes a structure-text prefix adapter for integrating graph features into text embeddings, allowing LLMs to generate explanations that combine graph and textual information.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for explainable temporal reasoning.
	2. Development of GETER, integrating graph structures with text for enhanced explainability.
	3. Experimental validation showing state-of-the-art performance and generalization capabilities.

**Result:** GETER achieves state-of-the-art performance in temporal reasoning tasks while providing effective explanations that outperform previous methods reliant solely on textual data.

**Limitations:** 

**Conclusion:** GETER not only enhances the performance of LLMs in temporal reasoning but also contributes significantly to explainability, demonstrating strong generalization capabilities.

**Abstract:** While large language models (LLMs) show great potential in temporal reasoning, most existing work focuses heavily on enhancing performance, often neglecting the explainable reasoning processes underlying the results. To address this gap, we introduce a comprehensive benchmark covering a wide range of temporal granularities, designed to systematically evaluate LLMs' capabilities in explainable temporal reasoning. Furthermore, our findings reveal that LLMs struggle to deliver convincing explanations when relying solely on textual information. To address challenge, we propose GETER, a novel structure-aware generative framework that integrates Graph structures with text for Explainable TEmporal Reasoning. Specifically, we first leverage temporal knowledge graphs to develop a temporal encoder that captures structural information for the query. Subsequently, we introduce a structure-text prefix adapter to map graph structure features into the text embedding space. Finally, LLMs generate explanation text by seamlessly integrating the soft graph token with instruction-tuning prompt tokens. Experimental results indicate that GETER achieves state-of-the-art performance while also demonstrating its effectiveness as well as strong generalization capabilities. Our dataset and code are available at https://github.com/carryTatum/GETER.

</details>


### [87] [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)

*Yerin Hwang, Dongryeol Lee, Kyungmin Min, Taegwan Kang, Yong-il Kim, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** vision-language models, adversarial evaluations, text-image alignment, FRAME benchmark, evaluation vulnerabilities

**Relevance Score:** 8

**TL;DR:** This study investigates the vulnerability of large vision-language models (LVLMs) to adversarial visual manipulations that lead to inflated evaluation scores for manipulated images.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the robustness of LVLMs in judging text-image alignment and identify potential biases induced by adversarial visual manipulations.

**Method:** The authors define image-induced biases and create a benchmark called FRAME to evaluate LVLM judges across various domains, intentionally introducing biases to assess score inflation.

**Key Contributions:**

	1. Introduction of a novel benchmark (FRAME) for evaluating LVLMs
	2. Identification of systematic vulnerabilities in LVLM judges due to adversarial manipulations
	3. Demonstration of the effects of combined biases on score inflation

**Result:** All tested LVLM judges were found to be vulnerable, consistently inflating scores for manipulated images, with amplified effects when multiple biases were combined.

**Limitations:** The study's findings may not generalize across all types of LVLMs and evaluation conditions.

**Conclusion:** The findings highlight the susceptibility of LVLM evaluation systems to visual biases and emphasize the necessity for more robust evaluation methods.

**Abstract:** Recently, large vision-language models (LVLMs) have emerged as the preferred tools for judging text-image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist under prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges.

</details>


### [88] [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)

*Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Zonghui Wang, Wenzhi Chen*

**Main category:** cs.CL

**Keywords:** mental manipulation, large language models, dialogue detection, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces MentalMAC, a novel method for enhancing large language models' ability to detect mental manipulation in dialogues using multi-task anti-curriculum distillation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Mental manipulation is a common form of psychological abuse that is difficult to detect, even for advanced LLMs, due to the lack of annotated datasets and the subtlety of manipulation techniques.

**Method:** The proposed MentalMAC framework involves EvoSA for unsupervised data expansion, teacher-model-generated multi-task supervision, and progressive knowledge distillation from complex to simpler tasks, using a newly created ReaMent dataset.

**Key Contributions:**

	1. Introduction of MentalMAC for enhanced detection of mental manipulation
	2. Creation of the ReaMent dataset with 5,000 real-world dialogue samples
	3. Demonstration of significant performance improvement in LLMs through multi-task distillation

**Result:** Experiments show that MentalMAC significantly reduces the performance gap between student and teacher models and surpasses existing competitive LLMs on evaluation metrics.

**Limitations:** The effectiveness of the approach may be limited by the quality and representativeness of the ReaMent dataset and the intricacies involved in mental manipulation detection.

**Conclusion:** MentalMAC provides a promising approach to improve LLMs for detecting mental manipulation in dialogues, alongside the release of resources to support further research in this area.

**Abstract:** Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Its covert nature and the complexity of manipulation strategies make it challenging to detect, even for state-of-the-art large language models (LLMs). This concealment also hinders the manual collection of large-scale, high-quality annotations essential for training effective models. Although recent efforts have sought to improve LLM's performance on this task, progress remains limited due to the scarcity of real-world annotated datasets. To address these challenges, we propose MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs' ability to detect mental manipulation in multi-turn dialogue. Our approach includes: (i) EvoSA, an unsupervised data expansion method based on evolutionary operations and speech act theory; (ii) teacher-model-generated multi-task supervision; and (iii) progressive knowledge distillation from complex to simpler tasks. We then constructed the ReaMent dataset with 5,000 real-world dialogue samples, using a MentalMAC-distilled model to assist human annotation. Vast experiments demonstrate that our method significantly narrows the gap between student and teacher models and outperforms competitive LLMs across key evaluation metrics. All code, datasets, and checkpoints will be released upon paper acceptance. Warning: This paper contains content that may be offensive to readers.

</details>


### [89] [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)

*Weixiang Zhao, Jiahe Guo, Yang Deng, Tongtong Wu, Wenxuan Zhang, Yulin Hu, Xingyu Sui, Yanyan Zhao, Wanxiang Che, Bing Qin, Tat-Seng Chua, Ting Liu*

**Main category:** cs.CL

**Keywords:** Multilingual reasoning, Large language models, Causal intervention, Language-specific ablation, Cross-lingual generalization

**Relevance Score:** 8

**TL;DR:** This paper investigates multilingual reasoning in large language models (LLMs), proposing a method to disentangle language and reasoning components to enhance performance across diverse languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Multilingual reasoning in LLMs is challenged by a bias toward high-resource languages, necessitating improved strategies for enhancing performance across all languages.

**Method:** A causal intervention method was used to ablate language-specific representations at inference time, evaluated across 10 open-source LLMs in 11 diverse languages.

**Key Contributions:**

	1. Introduces a method for disentangling language and reasoning components in LLMs.
	2. Demonstrates consistent improvement in multilingual reasoning across diverse languages.
	3. Achieves superior results with minimal computational overhead compared to traditional training methods.

**Result:** The ablation consistently improved multilingual reasoning performance without compromising linguistic fidelity, showing that language and reasoning can be effectively decoupled in models.

**Limitations:** Future work may need to explore the long-term impacts of the proposed method on model behavior and verify results across more languages and contexts.

**Conclusion:** The study presents a lightweight and interpretable approach to enhancing cross-lingual generalization in LLMs, with findings indicating the potential for improved reasoning capabilities.

**Abstract:** Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-source LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively decoupled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training such as supervised fine-tuning or reinforcement learning, our training-free ablation achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization.

</details>


### [90] [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)

*Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** AI-generated text detection, zero-shot learning, multi-agent system, interpretability, rhetoric

**Relevance Score:** 6

**TL;DR:** AGENT-X is a zero-shot multi-agent framework for AI-generated text detection that improves interpretability and adaptability, outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of AI-generated text detection methods that rely on large datasets and external tuning, restricting their interpretability and adaptability.

**Method:** A zero-shot multi-agent framework utilizing classical rhetoric and systemic functional linguistics, assessing text through identified semantic, stylistic, and structural dimensions.

**Key Contributions:**

	1. Introduction of a zero-shot detection framework based on rhetorical principles.
	2. Confidence-aware aggregation for interpretability without the need for threshold tuning.
	3. Dynamic guideline selection through a Mixture-of-Agent router.

**Result:** AGENT-X shows substantial improvements in accuracy, interpretability, and generalization over state-of-the-art approaches in its experiments.

**Limitations:** 

**Conclusion:** The framework enables threshold-free classification and robust confidence assessment through a meta agent that dynamically selects guidelines.

**Abstract:** Existing AI-generated text detection methods heavily depend on large annotated datasets and external threshold tuning, restricting interpretability, adaptability, and zero-shot effectiveness. To address these limitations, we propose AGENT-X, a zero-shot multi-agent framework informed by classical rhetoric and systemic functional linguistics. Specifically, we organize detection guidelines into semantic, stylistic, and structural dimensions, each independently evaluated by specialized linguistic agents that provide explicit reasoning and robust calibrated confidence via semantic steering. A meta agent integrates these assessments through confidence-aware aggregation, enabling threshold-free, interpretable classification. Additionally, an adaptive Mixture-of-Agent router dynamically selects guidelines based on inferred textual characteristics. Experiments on diverse datasets demonstrate that AGENT-X substantially surpasses state-of-the-art supervised and zero-shot approaches in accuracy, interpretability, and generalization.

</details>


### [91] [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)

*Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, Dongwook Choi, Minseok Kang, Gwanhoon Im, ByeongUng Cho, Hyojun Kim, Jun Hee Han, Taeyoon Kwon, Minju Kim, Beong-woo Kwak, Dongjin Kang, Jinyoung Yeo*

**Main category:** cs.CL

**Keywords:** Web navigation, Process reward model, Machine learning

**Relevance Score:** 7

**TL;DR:** This paper introduces Web-Shepherd, a novel process reward model for web navigation that evaluates trajectories step-by-step, overcoming limitations of current MLLM-based reward models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for specialized reward models for web navigation that facilitate long-horizon sequential decision making and enhance real-world deployment for web tasks.

**Method:** The authors constructed the WebPRM Collection dataset consisting of 40,000 step-level preference pairs and developed the WebRewardBench meta-evaluation benchmark to assess the performance of process reward models.

**Key Contributions:**

	1. Introduction of Web-Shepherd, the first process reward model for web navigation.
	2. Creation of the WebPRM Collection, a comprehensive dataset for training PRMs.
	3. Development of the WebRewardBench, a benchmark for evaluating PRMs.

**Result:** Web-Shepherd significantly outperforms GPT-4o by approximately 30 points in accuracy on the WebRewardBench and achieves 10.9 points better performance on WebArena-lite at a reduced cost when used as a verifier.

**Limitations:** 

**Conclusion:** Web-Shepherd showcases the potential of specialized reward models in improving web navigation efficiency and performance, providing valuable resources for future research.

**Abstract:** Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.

</details>


### [92] [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)

*Yanzhi Tian, Zeming Liu, Zhengyang Liu, Yuhang Guo*

**Main category:** cs.CL

**Keywords:** In-Image Machine Translation, dataset, text and background separation, translation quality, visual effect

**Relevance Score:** 7

**TL;DR:** This paper presents a novel approach to In-Image Machine Translation (IIMT) that addresses the limitations of previous models by introducing a new dataset and the DebackX model for improved translation and visual quality.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the practicality of In-Image Machine Translation by moving beyond simplified scenarios and addressing real-world complexities in image backgrounds.

**Method:** The authors designed a new dataset with subtitle text in real-world backgrounds and introduced the DebackX model, which separates text from background, translates the text, and then merges it back with the background.

**Key Contributions:**

	1. Introduction of a new dataset for complex IIMT scenarios
	2. Development of the DebackX model for improved text translation
	3. Improved visual effect in generated images

**Result:** The DebackX model significantly improves translation quality and visual effect compared to existing IIMT models in complex scenarios.

**Limitations:** 

**Conclusion:** The study demonstrates that separating text and background is effective in enhancing the outcomes of In-Image Machine Translation.

**Abstract:** In-Image Machine Translation (IIMT) aims to translate texts within images from one language to another. Previous research on IIMT was primarily conducted on simplified scenarios such as images of one-line text with black font in white backgrounds, which is far from reality and impractical for applications in the real world. To make IIMT research practically valuable, it is essential to consider a complex scenario where the text backgrounds are derived from real-world images. To facilitate research of complex scenario IIMT, we design an IIMT dataset that includes subtitle text with real-world background. However previous IIMT models perform inadequately in complex scenarios. To address the issue, we propose the DebackX model, which separates the background and text-image from the source image, performs translation on text-image directly, and fuses the translated text-image with the background, to generate the target image. Experimental results show that our model achieves improvements in both translation quality and visual effect.

</details>


### [93] [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)

*Joonho Yang, Seunghyun Yoon, Hwan Chang, Byeongjeong Kim, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, long document summarization, attention, natural language generation

**Relevance Score:** 9

**TL;DR:** This paper investigates the positional distribution of hallucinations in long response generation from Large Language Models, revealing that hallucinations are more frequent in later sections of generated texts. The study explores factors contributing to this behavior and proposes mitigation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** This paper addresses the challenge of hallucinations in LLMs, particularly in long output texts, which undermines the faithfulness of generated responses.

**Method:** The authors conduct an analysis of hallucination occurrences in long document summarization outputs and examine the role of attention and decoding in producing these hallucinations. They also propose strategies to reduce hallucinations in the final parts of generated texts.

**Key Contributions:**

	1. Identified the positional bias of hallucinations in LLM-generated long texts.
	2. Proposed methods to mitigate hallucinations in the concluding segments of outputs.
	3. Analyzed attention dynamics impacting hallucination distribution.

**Result:** The study found that hallucinations are concentrated disproportionately in the latter sections of long response generations by LLMs, highlighting a significant area for research and improvement.

**Limitations:** 

**Conclusion:** By identifying and addressing the positional bias of hallucinations, the work aims to enhance the reliability and faithfulness of LLM outputs in long document contexts.

**Abstract:** Large Language Models (LLMs) have significantly advanced text generation capabilities, including tasks like summarization, often producing coherent and fluent outputs. However, faithfulness to source material remains a significant challenge due to the generation of hallucinations. While extensive research focuses on detecting and reducing these inaccuracies, less attention has been paid to the positional distribution of hallucination within generated text, particularly in long outputs. In this work, we investigate where hallucinations occur in LLM-based long response generation, using long document summarization as a key case study. Focusing on the challenging setting of long context-aware long response generation, we find a consistent and concerning phenomenon: hallucinations tend to concentrate disproportionately in the latter parts of the generated long response. To understand this bias, we explore potential contributing factors related to the dynamics of attention and decoding over long sequences. Furthermore, we investigate methods to mitigate this positional hallucination, aiming to improve faithfulness specifically in the concluding segments of long outputs.

</details>


### [94] [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)

*Xintong Wang, Yixiao Liu, Jingheng Pan, Liang Ding, Longyue Wang, Chris Biemann*

**Main category:** cs.CL

**Keywords:** detoxification, language models, sentiment polarity, Chinese NLP, toxicity

**Relevance Score:** 8

**TL;DR:** Introducing ToxiRewriteCN, a dataset aimed at detoxifying toxic language in Chinese while preserving sentiment polarity, evaluated across various LLMs.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the quality of online interactions by detoxifying offensive language while maintaining the speaker's original intent is a crucial challenge.

**Method:** ToxiRewriteCN, a dataset consisting of 1,556 annotated triplets of toxic sentences, sentiment-aligned non-toxic rewrites, and labeled toxic spans, tailored for five real-world scenarios.

**Key Contributions:**

	1. Development of the first Chinese detoxification dataset preserving sentiment polarity.
	2. Evaluation of various LLMs on a challenging detoxification task in nuanced settings.
	3. Release of ToxiRewriteCN for future research initiatives.

**Result:** Evaluation of 17 LLMs reveals that while commercial and MoE models perform the best, all struggle with balancing safety and emotional fidelity in nuanced contexts.

**Limitations:** Models struggle with contexts involving emojis, homophones, and dialogue nuances, indicating a need for further improvements in LLMs' handling of subtle toxicity.

**Conclusion:** The dataset aims to advance controlled, sentiment-aware detoxification efforts in Chinese language processing, supporting further research in this area.

**Abstract:** Detoxifying offensive language while preserving the speaker's original intent is a challenging yet critical goal for improving the quality of online interactions. Although large language models (LLMs) show promise in rewriting toxic content, they often default to overly polite rewrites, distorting the emotional tone and communicative intent. This problem is especially acute in Chinese, where toxicity often arises implicitly through emojis, homophones, or discourse context. We present ToxiRewriteCN, the first Chinese detoxification dataset explicitly designed to preserve sentiment polarity. The dataset comprises 1,556 carefully annotated triplets, each containing a toxic sentence, a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five real-world scenarios: standard expressions, emoji-induced and homophonic toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs, including commercial and open-source models with variant architectures, across four dimensions: detoxification accuracy, fluency, content preservation, and sentiment polarity. Results show that while commercial and MoE models perform best overall, all models struggle to balance safety with emotional fidelity in more subtle or context-heavy settings such as emoji, homophone, and dialogue-based inputs. We release ToxiRewriteCN to support future research on controllable, sentiment-aware detoxification for Chinese.

</details>


### [95] [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)

*Maodong Li, Longyin Zhang, Fang Kong*

**Main category:** cs.CL

**Keywords:** multi-hop question generation, keyword guidance, transformer, natural language processing, information synthesis

**Relevance Score:** 8

**TL;DR:** This paper introduces a Dual-Perspective Keyword-Guided framework for enhancing multi-hop question generation by effectively integrating dual-perspective keywords to improve the identification of critical information snippets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The goal of multi-hop question generation is to synthesize information from multiple document snippets to generate accurate questions. Current methods underutilize keywords and do not distinguish between question-specific and document-specific keywords, leading to less effective QA pairing.

**Method:** The proposed DPKG framework employs an expanded transformer encoder and two different transformer decoders, one for generating question-specific keywords and the other for question generation, to enhance multi-hop question generation.

**Key Contributions:**

	1. Introduction of dual-perspective keywords for QA pairs.
	2. Development of the DPKG framework integrating these keywords into the question generation process.
	3. Demonstrated improvements in the effectiveness of multi-hop question generation through extensive experiments.

**Result:** Experiments demonstrate that the DPKG framework significantly improves the accuracy and effectiveness of multi-hop question generation tasks as compared to existing methods.

**Limitations:** 

**Conclusion:** The dual-perspective keywords help pinpoint essential information snippets more effectively, leading to better question generation for multi-hop tasks.

**Abstract:** Multi-hop question generation (MQG) aims to generate questions that require synthesizing multiple information snippets from documents to derive target answers. The primary challenge lies in effectively pinpointing crucial information snippets related to question-answer (QA) pairs, typically relying on keywords. However, existing works fail to fully utilize the guiding potential of keywords and neglect to differentiate the distinct roles of question-specific and document-specific keywords. To address this, we define dual-perspective keywords (i.e., question and document keywords) and propose a Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates keywords into the multi-hop question generation process. We argue that question keywords capture the questioner's intent, whereas document keywords reflect the content related to the QA pair. Functionally, question and document keywords work together to pinpoint essential information snippets in the document, with question keywords required to appear in the generated question. The DPKG framework consists of an expanded transformer encoder and two answer-aware transformer decoders for keyword and question generation, respectively. Extensive experiments demonstrate the effectiveness of our work, showcasing its promising performance and underscoring its significant value in the MQG task.

</details>


### [96] [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)

*Xin Bai, Guanyi Chen, Tingting He, Chenlian Zhou, Yu Liu*

**Main category:** cs.CL

**Keywords:** Emotional Support Conversations, large language models, human-computer interaction, dialogue systems, machine learning

**Relevance Score:** 9

**TL;DR:** This paper redefines Emotional Support Conversations (ESC) to include multiple strategies in a single turn and demonstrates that state-of-the-art LLMs outperform traditional models in providing emotional support.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding and modeling of Emotional Support Conversations (ESC) by addressing the oversimplified view of supportive responses.

**Method:** Conducted a detailed corpus analysis of the ESConv dataset and proposed a revised ESC task formulation that requires generating multiple strategy-utterance pairs based on dialogue history. Implemented and tested various modeling approaches including supervised deep learning and large language models.

**Key Contributions:**

	1. Redefinition of the ESC task to include multiple strategies per turn.
	2. Introduction of several modeling approaches for the refined ESC task.
	3. Empirical evidence showing LLMs outperform traditional models and human supporters in emotional support scenarios.

**Result:** Experiments demonstrated that state-of-the-art large language models outperform both supervised models and human supporters in the revised ESC task, showing improved capabilities in providing holistic support.

**Limitations:** The study primarily focuses on one dataset, which may limit generalizability to other contexts.

**Conclusion:** The findings suggest that LLMs can effectively emulate complex emotional support interactions, which was previously underestimated in terms of their ability to ask questions and offer suggestions.

**Abstract:** Emotional Support Conversations (ESC) are crucial for providing empathy, validation, and actionable guidance to individuals in distress. However, existing definitions of the ESC task oversimplify the structure of supportive responses, typically modelling them as single strategy-utterance pairs. Through a detailed corpus analysis of the ESConv dataset, we identify a common yet previously overlooked phenomenon: emotional supporters often employ multiple strategies consecutively within a single turn. We formally redefine the ESC task to account for this, proposing a revised formulation that requires generating the full sequence of strategy-utterance pairs given a dialogue history. To facilitate this refined task, we introduce several modelling approaches, including supervised deep learning models and large language models. Our experiments show that, under this redefined task, state-of-the-art LLMs outperform both supervised models and human supporters. Notably, contrary to some earlier findings, we observe that LLMs frequently ask questions and provide suggestions, demonstrating more holistic support capabilities.

</details>


### [97] [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)

*Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multiple-choice question answering, Prefilling attack, First-token probability, AI safety

**Relevance Score:** 8

**TL;DR:** This paper proposes the *prefilling attack*, a method to enhance the accuracy of Large Language Models in multiple-choice question answering tasks by steering model outputs with structured natural-language prefixes.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models are commonly evaluated using first-token probability for MCQA tasks, but this method is often unreliable due to issues like misalignment and misinterpretation of tokens.

**Method:** The paper introduces a structured prefix, known as the prefilling attack, which is prepended to model outputs to guide the response toward a valid answer choice without changing model parameters.

**Key Contributions:**

	1. Introduction of the prefilling attack for steering model responses
	2. Demonstration of improved performance on MCQA benchmarks
	3. Establishment of prefilling as an efficient alternative to full decoding approaches

**Result:** The prefilling attack significantly improves accuracy, calibration, and output consistency compared to standard first-token probability, often rivaling the performance of more complex decoding methods.

**Limitations:** 

**Conclusion:** Prefilling is a simple, effective solution to enhance the reliability of FTP-based evaluations in multiple-choice question answering scenarios.

**Abstract:** Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using *first-token probability* (FTP), which selects the answer option whose initial token has the highest likelihood. While efficient, FTP can be fragile: models may assign high probability to unrelated tokens (*misalignment*) or use a valid token merely as part of a generic preamble rather than as a clear answer choice (*misinterpretation*), undermining the reliability of symbolic evaluation. We propose a simple solution: the *prefilling attack*, a structured natural-language prefix (e.g., "*The correct option is:*") prepended to the model output. Originally explored in AI safety, we repurpose prefilling to steer the model to respond with a clean, valid option, without modifying its parameters. Empirically, the FTP with prefilling strategy substantially improves accuracy, calibration, and output consistency across a broad set of LLMs and MCQA benchmarks. It outperforms standard FTP and often matches the performance of open-ended generation approaches that require full decoding and external classifiers, while being significantly more efficient. Our findings suggest that prefilling is a simple, robust, and low-cost method to enhance the reliability of FTP-based evaluation in multiple-choice settings.

</details>


### [98] [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)

*Yuhao Zhang, Xiangnan Ma, Kaiqi Kou, Peizhuo Liu, Weiqiao Shan, Benyou Wang, Tong Xiao, Yuxin Huang, Zhengtao Yu, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** speech-to-speech translation, multi-task learning, unit language

**Relevance Score:** 7

**TL;DR:** This paper addresses challenges in textless speech-to-speech translation by introducing a unit language representation and multi-task learning, achieving significant performance improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to tackle the challenges in speech-to-speech translation, particularly in extracting linguistic features and aligning different languages.

**Method:** The authors propose a unit language representation based on $n$-gram language modeling, implementing multi-task learning to enhance the speech modeling processes and task prompt modeling to resolve conflicts in language application.

**Key Contributions:**

	1. Introduction of unit language representation for speech modeling
	2. Implementation of multi-task learning to enhance S2ST
	3. Task prompt modeling to address conflicts in language use

**Result:** The proposed method shows significant improvements over a strong baseline and achieves performance levels comparable to models using text on the Voxpupil dataset, across four languages.

**Limitations:** 

**Conclusion:** The introduction of unit language and multi-task learning effectively addresses key challenges in S2ST, providing a robust alternative to text-based models.

**Abstract:** The success of building textless speech-to-speech translation (S2ST) models has attracted much attention. However, S2ST still faces two main challenges: 1) extracting linguistic features for various speech signals, called cross-modal (CM), and 2) learning alignment of difference languages in long sequences, called cross-lingual (CL). We propose the unit language to overcome the two modeling challenges. The unit language can be considered a text-like representation format, constructed using $n$-gram language modeling. We implement multi-task learning to utilize the unit language in guiding the speech modeling process. Our initial results reveal a conflict when applying source and target unit languages simultaneously. We propose task prompt modeling to mitigate this conflict. We conduct experiments on four languages of the Voxpupil dataset. Our method demonstrates significant improvements over a strong baseline and achieves performance comparable to models trained with text.

</details>


### [99] [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)

*Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, paraphrase attacks, text detection, human-like texts, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents CoPA, a training-free method that uses LLMs to deceive text detectors by producing human-like texts through contrastive analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing misuse of large language models for academic plagiarism, and the need for effective countermeasures against LLM-generated text detection.

**Method:** CoPA constructs an auxiliary machine-like word distribution and produces human-like texts by subtracting machine-like attributes during the decoding process using off-the-shelf LLMs.

**Key Contributions:**

	1. Introduction of CoPA, a training-free attack method using LLMs.
	2. Demonstration of the effectiveness of CoPA in fooling text detectors.
	3. Theoretical analysis supporting the superiority of this approach over traditional methods.

**Result:** CoPA effectively deceives various text detectors without requiring extensive data or training resources, showing superior results compared to existing methods.

**Limitations:** The method's effectiveness may vary depending on the sophistication of detection algorithms.

**Conclusion:** CoPA outperforms existing techniques in circumventing detection by blending human-like and machine-like text patterns.

**Abstract:** The misuse of large language models (LLMs), such as academic plagiarism, has driven the development of detectors to identify LLM-generated texts. To bypass these detectors, paraphrase attacks have emerged to purposely rewrite these texts to evade detection. Despite the success, existing methods require substantial data and computational budgets to train a specialized paraphraser, and their attack efficacy greatly reduces when faced with advanced detection algorithms. To address this, we propose \textbf{Co}ntrastive \textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that effectively deceives text detectors using off-the-shelf LLMs. The first step is to carefully craft instructions that encourage LLMs to produce more human-like texts. Nonetheless, we observe that the inherent statistical biases of LLMs can still result in some generated texts carrying certain machine-like attributes that can be captured by detectors. To overcome this, CoPA constructs an auxiliary machine-like word distribution as a contrast to the human-like distribution generated by the LLM. By subtracting the machine-like patterns from the human-like distribution during the decoding process, CoPA is able to produce sentences that are less discernible by text detectors. Our theoretical analysis suggests the superiority of the proposed attack. Extensive experiments validate the effectiveness of CoPA in fooling text detectors across various scenarios.

</details>


### [100] [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)

*Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu*

**Main category:** cs.CL

**Keywords:** Key-Value Cache, multi-turn conversations, catastrophic forgetting, conversational AI

**Relevance Score:** 8

**TL;DR:** FlowKV is a novel multi-turn isolation mechanism for managing Key-Value Cache in conversational applications that mitigates performance degradation and catastrophic forgetting by preserving compressed context from previous dialogue turns.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** Existing KV Cache eviction strategies in multi-turn conversational applications lead to information loss and performance degradation due to the linear growth of cache with dialogue history.

**Method:** FlowKV introduces a multi-turn isolation mechanism that preserves the compressed KV cache from previous turns and strategically applies compression only to newly generated KV pairs of the latest turn.

**Key Contributions:**

	1. Introduction of a novel multi-turn isolation mechanism for KV Cache management.
	2. Preservation of accumulated compressed KV cache from past turns.
	3. Significant improvements in instruction-following accuracy and user preference retention.

**Result:** FlowKV significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention, increasing retention rates from 10.90% to 75.40% in later conversational turns.

**Limitations:** 

**Conclusion:** The proposed method effectively addresses issues of context forgetting in multi-turn conversations, enhancing both performance and user engagement in conversational AI applications.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in multi-turn conversational applications, where the management of the Key-Value (KV) Cache presents a significant bottleneck. The linear growth of the KV Cache with dialogue history imposes substantial computational costs, and existing eviction strategies often degrade performance by repeatedly compressing early conversational context, leading to information loss and context forgetting. This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism} for KV Cache management, which can be applied to any KV Cache compression method without training. FlowKV's core innovation is a multi-turn isolation mechanism that preserves the accumulated compressed KV cache from past turns. Compression is then strategically applied only to the newly generated KV pairs of the latest completed turn, effectively preventing the re-compression of older context and thereby mitigating catastrophic forgetting. Our results demonstrate that FlowKV consistently and significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention from 10.90\% to 75.40\%, particularly in later conversational turns.

</details>


### [101] [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)

*Enric Junqué de Fortuny*

**Main category:** cs.CL

**Keywords:** emotion classification, NLP, Shaver's taxonomy

**Relevance Score:** 6

**TL;DR:** The Super Emotion Dataset provides a standardized, large-scale resource for emotion classification in NLP, based on a validated psychological taxonomy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a standardized emotion classification dataset in NLP that follows a psychologically grounded taxonomy, overcoming issues of inconsistency and limited sample sizes in existing datasets.

**Method:** The dataset harmonizes diverse text sources into a unified framework based on Shaver's emotion taxonomy.

**Key Contributions:**

	1. Introduction of a standardized emotion classification dataset
	2. Implementation of Shaver's validated emotion taxonomy
	3. Facilitation of cross-domain emotion recognition research

**Result:** The Super Emotion Dataset enables more consistent cross-domain emotion recognition research, improving the foundation for analysis in NLP.

**Limitations:** 

**Conclusion:** This dataset fills a significant gap in emotion classification resources, paving the way for enhanced research in emotion recognition across various domains.

**Abstract:** Despite the wide-scale usage and development of emotion classification datasets in NLP, the field lacks a standardized, large-scale resource that follows a psychologically grounded taxonomy. Existing datasets either use inconsistent emotion categories, suffer from limited sample size, or focus on specific domains. The Super Emotion Dataset addresses this gap by harmonizing diverse text sources into a unified framework based on Shaver's empirically validated emotion taxonomy, enabling more consistent cross-domain emotion recognition research.

</details>


### [102] [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)

*Ryo Kishino, Yusuke Takase, Momose Oyama, Hiroaki Yamagiwa, Hidetoshi Shimodaira*

**Main category:** cs.CL

**Keywords:** KL divergence, language models, log-likelihood, pretraining, model trajectories

**Relevance Score:** 8

**TL;DR:** The paper explores a method for estimating the KL divergence between language models, revealing patterns in their behavior during pretraining and across layers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the behavior of KL divergence between various language models and to evaluate this metric across different conditions.

**Method:** Systematic evaluation of KL divergence using publicly available language models, comparing pretraining checkpoints, fine-tuned and base models, and analyzing layers through the logit lens.

**Key Contributions:**

	1. Efficient method for estimating KL divergence across different language models
	2. Systematic analysis of KL divergence across pretraining and model layers
	3. Insights into model trajectory behavior in log-likelihood space

**Result:** KL divergence trajectories exhibit a spiral structure during pretraining and thread-like progressions across layers; model trajectories in log-likelihood space are more constrained than in weight space.

**Limitations:** 

**Conclusion:** The findings provide insights into the structural behavior of language models during pretraining and layer interactions based on KL divergence.

**Abstract:** A recently proposed method enables efficient estimation of the KL divergence between language models, including models with different architectures, by assigning coordinates based on log-likelihood vectors. To better understand the behavior of this metric, we systematically evaluate KL divergence across a wide range of conditions using publicly available language models. Our analysis covers comparisons between pretraining checkpoints, fine-tuned and base models, and layers via the logit lens. We find that trajectories of language models, as measured by KL divergence, exhibit a spiral structure during pretraining and thread-like progressions across layers. Furthermore, we show that, in terms of diffusion exponents, model trajectories in the log-likelihood space are more constrained than those in weight space.

</details>


### [103] [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)

*Xabier de Zuazo, Eva Navas, Ibon Saratxaga, Mathieu Bourguignon, Nicola Molinaro*

**Main category:** cs.CL

**Keywords:** speech production, magnetoencephalography, machine learning, brain-computer interface, phonetic decoding

**Relevance Score:** 6

**TL;DR:** This study decodes neural signals during speech production and perception using MEG, revealing higher accuracy in production tasks with compelling implications for communication technology.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding neural mechanisms of speech production is vital for cognitive neuroscience and communication technology development.

**Method:** Utilized magnetoencephalography signals from 17 participants to classify phones via machine learning exploring 15 phonetic pairs.

**Key Contributions:**

	1. Demonstrated higher decoding accuracy in speech production than passive conditions.
	2. Outperformed traditional techniques with Elastic Net in a high-dimensional MEG dataset.
	3. Identified critical frequency bands (Delta and Theta) linked to speech production processes.

**Result:** Achieved 76.6% decoding accuracy during speech production, higher than ~51% in passive listening/voice playback; Elastic Net classifier outperformed complex models.

**Limitations:** Uncertainty whether decoding reflects pure neural activity without movement artifacts; suggests need for methodological improvements.

**Conclusion:** The study highlights the potential of overt speech paradigms for enhancing brain-computer interfaces for those with speech impairments.

**Abstract:** Understanding the neural mechanisms underlying speech production is essential for both advancing cognitive neuroscience theory and developing practical communication technologies. In this study, we investigated magnetoencephalography signals to decode phones from brain activity during speech production and perception (passive listening and voice playback) tasks. Using a dataset comprising 17 participants, we performed pairwise phone classification, extending our analysis to 15 phonetic pairs. Multiple machine learning approaches, including regularized linear models and neural network architectures, were compared to determine their effectiveness in decoding phonetic information. Our results demonstrate significantly higher decoding accuracy during speech production (76.6%) compared to passive listening and playback modalities (~51%), emphasizing the richer neural information available during overt speech. Among the models, the Elastic Net classifier consistently outperformed more complex neural networks, highlighting the effectiveness of traditional regularization techniques when applied to limited and high-dimensional MEG datasets. Besides, analysis of specific brain frequency bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz) and Theta (4-7 Hz), contributed the most substantially to decoding accuracy, suggesting that these bands encode critical speech production-related neural processes. Despite using advanced denoising methods, it remains unclear whether decoding solely reflects neural activity or if residual muscular or movement artifacts also contributed, indicating the need for further methodological refinement. Overall, our findings underline the critical importance of examining overt speech production paradigms, which, despite their complexity, offer opportunities to improve brain-computer interfaces to help individuals with severe speech impairments.

</details>


### [104] [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)

*Weiming Zhang, Qingyao Li, Xinyi Dai, Jizheng Chen, Kounianhua Du, Weinan Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Yu*

**Main category:** cs.CL

**Keywords:** natural language processing, debugging, large language models, code analysis, machine learning

**Relevance Score:** 8

**TL;DR:** NL-DEBUGGING is a framework using natural language for improved code debugging, outperforming traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve debugging in coding tasks using natural language reasoning instead of just code-level analysis.

**Method:** Introducing the NL-DEBUGGING framework that employs natural language as an intermediate representation for debugging.

**Key Contributions:**

	1. Introduction of NL-DEBUGGING framework for debugging using natural language.
	2. Demonstration of improved performance in debugging tasks over traditional methods.
	3. Expansion of modification opportunities through execution feedback.

**Result:** NL-DEBUGGING is shown to outperform traditional methods and allows broader modifications using execution feedback.

**Limitations:** 

**Conclusion:** Natural language reasoning has potential to enhance automated code debugging and tackle complex programming issues.

**Abstract:** Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges.

</details>


### [105] [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)

*Peng Wang, Ruihan Tao, Qiguang Chen, Mengkang Hu, Libo Qin*

**Main category:** cs.CL

**Keywords:** multilingual agents, X-WebAgentBench, language models, cross-lingual techniques, interactive environments

**Relevance Score:** 8

**TL;DR:** Introduction of X-WebAgentBench, a benchmark for evaluating multilingual language agents in web environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual support in LLM-based agent services, which currently focus primarily on English applications.

**Method:** The paper introduces X-WebAgentBench, a multilingual benchmark that assesses planning and interaction performance of language agents across various languages.

**Key Contributions:**

	1. Introduction of the X-WebAgentBench benchmark
	2. Evaluation of LLMs and cross-lingual alignment methods
	3. Insights into performance limitations of current state-of-the-art models.

**Result:** The performance evaluation shows that even advanced models like GPT-4o, enhanced with cross-lingual techniques, do not reach satisfactory results.

**Limitations:** Current focus mainly on interactive web environments; results may not generalize to all multilingual applications.

**Conclusion:** X-WebAgentBench aims to improve the development of multilingual agent applications, highlighting the need for better performance in non-English languages.

**Abstract:** Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.

</details>


### [106] [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)

*Yiming Huang, Junyan Zhang, Zihao Wang, Biquan Bie, Xuming Hu, Yi R., Fung, Xinlei He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Detection, Uncertainty Measurement, Explainable AI, Token-Level Score

**Relevance Score:** 9

**TL;DR:** This paper presents RePPL, a method that improves hallucination detection in large language models by providing explainable uncertainty scores for each token to identify triggers of hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for understanding and explaining the provenance of hallucinations in large language models, as existing methods only measure uncertainty without explaining the underlying causes.

**Method:** RePPL recalibrates uncertainty measurement by taking into account the semantic propagation in attention mechanisms and the uncertainty in language generation, providing token-level uncertainty scores and aggregating them in a Perplexity-style format.

**Key Contributions:**

	1. Introduction of RePPL for recalibrating uncertainty measurement in LLMs.
	2. Provision of token-level uncertainty scores for identifying hallucination triggers.
	3. Demonstration of improved detection performance across QA datasets.

**Result:** RePPL demonstrates superior detection performance across various QA datasets with an average AUC of 0.833 and generates token-level uncertainty scores that explain hallucination triggers.

**Limitations:** The study is preliminary and further validation and exploration of the chaotic pattern of hallucination may be required.

**Conclusion:** The findings reveal a chaotic pattern in hallucinations and suggest that the explainable uncertainty scores can be beneficial in understanding and mitigating hallucinations in LLMs.

**Abstract:** Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.

</details>


### [107] [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)

*DongGeon Lee, Joonwon Jang, Jihae Jeong, Hwanjo Yu*

**Main category:** cs.CL

**Keywords:** vision-language models, safety evaluation, meme images, harmful prompts, conversational context

**Relevance Score:** 8

**TL;DR:** The paper evaluates the safety risks of vision-language models (VLMs) using real meme images, revealing their greater vulnerability to harmful prompts than other image types.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the safety of VLMs when exposed to real-world meme images, which are commonly shared by users, in contrast to traditional evaluations using artificial images.

**Method:** Introduced MemeSafetyBench, a benchmark with 50,430 instances pairing meme images with harmful and benign instructions. The study used a comprehensive safety taxonomy and LLM-based instruction generation to evaluate various VLMs through single and multi-turn interactions.

**Key Contributions:**

	1. Introduction of MemeSafetyBench for real-world image evaluation of VLMs
	2. Identification of increased vulnerability of VLMs to meme-based harmful prompts
	3. Findings on the mitigating effect of conversational context in promoting safety

**Result:** The findings indicate that VLMs are more likely to produce harmful outputs when prompted with meme images compared to synthetic images. Memes lead to significantly more harmful responses and fewer refusals, and while multi-turn dialogues slightly mitigate this, vulnerabilities remain.

**Limitations:** The study may not cover all types of VLMs or all possible meme categories and contexts.

**Conclusion:** There is a crucial need for ecologically valid evaluations of VLM safety and the development of improved safety mechanisms in response to the findings.

**Abstract:** Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.

</details>


### [108] [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)

*Yiming Huang, Biquan Bie, Zuqiu Na, Weilin Ruan, Songxin Lei, Yutao Yue, Xinlei He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cognitive Bias, Anchoring Effect, Natural Language Processing, Trustworthy Evaluation

**Relevance Score:** 9

**TL;DR:** This paper investigates the anchoring effect in Large Language Models (LLMs) and introduces a new dataset, SynAnchors, for studying cognitive biases in NLP.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs like ChatGPT, concerns about cognitive biases, such as the anchoring effect, have emerged. This paper aims to understand the presence and impact of such biases in LLMs.

**Method:** The authors explore the anchoring effect in LLMs by introducing a new dataset, SynAnchors, and applying refined evaluation metrics to benchmark LLMs against this effect.

**Key Contributions:**

	1. Introduction of a new dataset, SynAnchors, for evaluating anchoring effects in LLMs.
	2. Demonstration of the prevalence of the anchoring bias in widely used LLMs.
	3. Recommendations for cognitive-bias-aware evaluation methodologies for LLMs.

**Result:** The study reveals that anchoring bias is prevalent in LLMs and primarily arises from shallow-layer processing, with conventional mitigation strategies proving inadequate, although reasoning can help alleviate some bias.

**Limitations:** The study primarily focuses on the anchoring effect and may not encompass other cognitive biases that could also affect LLMs.

**Conclusion:** LLM evaluations should shift focus from standard benchmarks to cognitive-bias-aware methods to ensure trustworthiness in their performance.

**Abstract:** The rise of Large Language Models (LLMs) like ChatGPT has advanced natural language processing, yet concerns about cognitive biases are growing. In this paper, we investigate the anchoring effect, a cognitive bias where the mind relies heavily on the first information as anchors to make affected judgments. We explore whether LLMs are affected by anchoring, the underlying mechanisms, and potential mitigation strategies. To facilitate studies at scale on the anchoring effect, we introduce a new dataset, SynAnchors. Combining refined evaluation metrics, we benchmark current widely used LLMs. Our findings show that LLMs' anchoring bias exists commonly with shallow-layer acting and is not eliminated by conventional strategies, while reasoning can offer some mitigation. This recontextualization via cognitive psychology urges that LLM evaluations focus not on standard benchmarks or over-optimized robustness tests, but on cognitive-bias-aware trustworthy evaluation.

</details>


### [109] [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)

*Zhexin Zhang, Xian Qi Loye, Victor Shea-Jay Huang, Junxiao Yang, Qi Zhu, Shiyao Cui, Fei Mi, Lifeng Shang, Yingkang Wang, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Safety, Supervised Fine-Tuning, Empirical Study, Reasoning Processes

**Relevance Score:** 7

**TL;DR:** This paper investigates methods to enhance the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning, revealing key patterns that hinder safety and suggesting simpler reasoning processes can achieve comparable results.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing deployment of LRMs raises concerns about their safety performance despite improved reasoning capabilities, necessitating research into methods to enhance their safety.

**Method:** The paper conducts an empirical study examining the impact of data distillation and reasoning processes on the safety of LRMs, highlighting three failure patterns and analyzing the effectiveness of different reasoning approaches.

**Key Contributions:**

	1. Empirical analysis of safety enhancement methods for LRMs
	2. Identification of failure patterns in safety performances
	3. Demonstration that simple reasoning processes can be effective for safety

**Result:** The study finds that direct distillation of safe responses is ineffective and that simpler reasoning methods can achieve similar safety performance while facilitating easier learning for models.

**Limitations:** The study focuses primarily on the empirical aspects and may require further validation across diverse LRM applications.

**Conclusion:** Addressing specific failure patterns in the data distillation process and using simpler reasoning methods can significantly improve the safety of LRMs, offering insights for future enhancements and practices in the field.

**Abstract:** Large Reasoning Models (LRMs) have achieved remarkable success on reasoning-intensive tasks such as mathematics and programming. However, their enhanced reasoning capabilities do not necessarily translate to improved safety performance-and in some cases, may even degrade it. This raises an important research question: how can we enhance the safety of LRMs? In this paper, we present a comprehensive empirical study on how to enhance the safety of LRMs through Supervised Fine-Tuning (SFT). Our investigation begins with an unexpected observation: directly distilling safe responses from DeepSeek-R1 fails to significantly enhance safety. We analyze this phenomenon and identify three key failure patterns that contribute to it. We then demonstrate that explicitly addressing these issues during the data distillation process can lead to substantial safety improvements. Next, we explore whether a long and complex reasoning process is necessary for achieving safety. Interestingly, we find that simply using short or template-based reasoning process can attain comparable safety performance-and are significantly easier for models to learn than more intricate reasoning chains. These findings prompt a deeper reflection on the role of reasoning in ensuring safety. Finally, we find that mixing math reasoning data during safety fine-tuning is helpful to balance safety and over-refusal. Overall, we hope our empirical study could provide a more holistic picture on enhancing the safety of LRMs. The code and data used in our experiments are released in https://github.com/thu-coai/LRM-Safety-Study.

</details>


### [110] [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)

*Nudrat Habib, Tosin Adewumi, Marcus Liwicki, Elisa Barney*

**Main category:** cs.CL

**Keywords:** authorship analysis, author attribution, author verification, machine learning, deep learning

**Relevance Score:** 4

**TL;DR:** This paper reviews methodologies in authorship analysis, focusing on Author Attribution and Author Verification, and identifies key trends and challenges in the field.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the importance of authorship analysis across various domains and aims to provide a comprehensive overview of current methodologies and emerging challenges.

**Method:** A systematic literature review of methodologies for authorship analysis focusing on Author Attribution and Author Verification from studies conducted between 2015 and 2024.

**Key Contributions:**

	1. Comprehensive analysis of authorship analysis methods and techniques.
	2. Identification of emerging challenges in low-resource language processing and multilingual adaptation.
	3. Overview of trends and research gaps to guide future studies.

**Result:** Identified SOTA methodologies in authorship analysis, including both traditional machine learning methods and advanced deep learning models, along with their strengths and limitations.

**Limitations:** Focuses on studies up to 2024; may not cover the latest developments post-review period.

**Conclusion:** The study highlights critical research gaps and suggests areas for future investigation to enhance authorship analysis systems, particularly in low-resource language processing and AI-generated text detection.

**Abstract:** Authorship analysis plays an important role in diverse domains, including forensic linguistics, academia, cybersecurity, and digital content authentication. This paper presents a systematic literature review on two key sub-tasks of authorship analysis; Author Attribution and Author Verification. The review explores SOTA methodologies, ranging from traditional ML approaches to DL models and LLMs, highlighting their evolution, strengths, and limitations, based on studies conducted from 2015 to 2024. Key contributions include a comprehensive analysis of methods, techniques, their corresponding feature extraction techniques, datasets used, and emerging challenges in authorship analysis. The study highlights critical research gaps, particularly in low-resource language processing, multilingual adaptation, cross-domain generalization, and AI-generated text detection. This review aims to help researchers by giving an overview of the latest trends and challenges in authorship analysis. It also points out possible areas for future study. The goal is to support the development of better, more reliable, and accurate authorship analysis system in diverse textual domain.

</details>


### [111] [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)

*Yan-Shuo Liang, Wu-Jun Li*

**Main category:** cs.CL

**Keywords:** continual learning, low-rank adaptation, gated integration, language models, forgetting

**Relevance Score:** 8

**TL;DR:** GainLoRA is a new method for continual learning in language models that uses gating modules to integrate low-rank adaptation branches, effectively reducing forgetting and enhancing performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of forgetting in continual learning for language models, particularly in the context of low-rank adaptation methods.

**Method:** GainLoRA expands a new low-rank adaptation branch for each task and integrates them using gating modules to control contributions to old tasks.

**Key Contributions:**

	1. Introduction of GainLoRA for continual learning in language models
	2. Use of gating modules to manage task integrations
	3. Improvement in model performance and reduction of forgetting.

**Result:** GainLoRA outperforms existing state-of-the-art methods in continual learning benchmarks by effectively mitigating forgetting.

**Limitations:** 

**Conclusion:** The proposed method enhances the performance of language models in continual learning scenarios by better managing the integration of task-specific branches.

**Abstract:** Continual learning (CL), which requires the model to learn multiple tasks sequentially, is crucial for language models (LMs). Recently, low-rank adaptation (LoRA), one of the most representative parameter-efficient fine-tuning (PEFT) methods, has gained increasing attention in CL of LMs. However, most existing CL methods based on LoRA typically expand a new LoRA branch to learn each new task and force the new and old LoRA branches to contribute equally to old tasks, potentially leading to forgetting. In this work, we propose a new method, called gated integration of low-rank adaptation (GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task and introduces gating modules to integrate the new and old LoRA branches. Furthermore, GainLoRA leverages the new gating module to minimize the contribution from the new LoRA branch to old tasks, effectively mitigating forgetting and improving the model's overall performance. Experimental results on CL benchmarks demonstrate that GainLoRA outperforms existing state-of-the-art methods.

</details>


### [112] [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)

*Aleksandra Tomaszewska, Dariusz Czerski, Bartosz Żuk, Maciej Ogrodniczuk*

**Main category:** cs.CL

**Keywords:** neologism detection, linguistic analysis, machine learning, Polish language

**Relevance Score:** 4

**TL;DR:** NeoN is an innovative tool designed for detecting and analyzing neologisms in Polish using an advanced multi-layered pipeline.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional dictionary-based methods for detecting neologisms, which require extensive manual review and are not efficient.

**Method:** NeoN employs reference corpora, Polish-specific linguistic filters, an LLM-driven filter, and daily RSS monitoring for its analysis pipeline. It includes context-aware lemmatization, frequency analysis, and orthographic normalization to consolidate inflectional variants.

**Key Contributions:**

	1. Integration of LLM for precision-boosting and definition generation
	2. Intuitive interface for researchers with visualization and filtering controls
	3. Multi-layered detection pipeline combining various linguistic and technical methods.

**Result:** NeoN achieves high accuracy in neologism detection while significantly reducing the manual effort required by researchers, facilitated by an intuitive interface for visualizations and filtering controls.

**Limitations:** 

**Conclusion:** NeoN offers an accessible and efficient solution for monitoring lexical innovation in the Polish language, integrating automated definition generation and categorization.

**Abstract:** NeoN, a tool for detecting and analyzing Polish neologisms. Unlike traditional dictionary-based methods requiring extensive manual review, NeoN combines reference corpora, Polish-specific linguistic filters, an LLM-driven precision-boosting filter, and daily RSS monitoring in a multi-layered pipeline. The system uses context-aware lemmatization, frequency analysis, and orthographic normalization to extract candidate neologisms while consolidating inflectional variants. Researchers can verify candidates through an intuitive interface with visualizations and filtering controls. An integrated LLM module automatically generates definitions and categorizes neologisms by domain and sentiment. Evaluations show NeoN maintains high accuracy while significantly reducing manual effort, providing an accessible solution for tracking lexical innovation in Polish.

</details>


### [113] [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)

*Zhiwen Li, Die Chen, Mingyuan Fan, Cen Chen, Yaliang Li, Yanhao Wang, Wenmeng Zhou*

**Main category:** cs.CL

**Keywords:** Diffusion Models, NSFW Content, Social Bias, Text Embeddings, Low-Rank Adaptation

**Relevance Score:** 5

**TL;DR:** This paper presents a novel approach to enhance the safety of diffusion models by identifying a semantic direction vector that keeps text prompts within a safe region, effectively reducing NSFW content and social bias while maintaining model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the risks of generating Not Safe for Work (NSFW) content and social biases in diffusion models, improving their practical applicability.

**Method:** The proposed method uses a self-discovery approach to find a semantic direction vector in the embedding space, steering text embeddings toward a safe region without altering individual words, and incorporates Low-Rank Adaptation (LoRA) for initialization.

**Key Contributions:**

	1. Introduction of a self-discovery method for semantic direction vector identification in embedding space.
	2. Demonstration of robustness against unsafe prompts without individual word corrections.
	3. Integration capability with existing safety measures for enhanced social responsibility.

**Result:** The method shows significant efficacy in reducing the generation of harmful content compared to state-of-the-art techniques, demonstrating improved robustness against unsafe prompts.

**Limitations:** The performance may vary based on the complexity of prompts and dataset particularities; further evaluation on diverse data sets is necessary.

**Conclusion:** The proposed self-discovery method can effectively mitigate NSFW content and social biases in diffusion models while preserving their performance, and can complement existing safety measures.

**Abstract:** The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, hindering their practical use in real-world applications. In response to this challenge, prior work has focused on employing security filters to identify and exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their social responsibility. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.

</details>


### [114] [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)

*Momose Oyama, Ryo Kishino, Hiroaki Yamagiwa, Hidetoshi Shimodaira*

**Main category:** cs.CL

**Keywords:** language models, KL divergence, model mapping, resampling, text selection

**Relevance Score:** 6

**TL;DR:** The paper proposes a resampling method to reduce the computational cost of constructing model maps for language models while preserving accuracy in KL divergence estimates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The computational cost of constructing a model map that embeds diverse language models is high, primarily due to the log-likelihood calculations over a large text set.

**Method:** The authors introduce a resampling method that selects important texts based on the variance of log-likelihoods across models, thereby reducing the number of required texts for construction.

**Key Contributions:**

	1. Resampling method for selecting important texts based on log-likelihood variance.
	2. Significantly reduces the number of texts needed for model map construction.
	3. Maintains accuracy of KL divergence estimates with fewer samples.

**Result:** The proposed method achieves performance comparable to uniform sampling but requires about half as many texts, allowing for efficient incorporation of new models into existing maps.

**Limitations:** 

**Conclusion:** The resampling approach enables scalable and efficient construction of language model maps, making it practical for comparing diverse models.

**Abstract:** We address the computational cost of constructing a model map, which embeds diverse language models into a common space for comparison via KL divergence. The map relies on log-likelihoods over a large text set, making the cost proportional to the number of texts. To reduce this cost, we propose a resampling method that selects important texts with weights proportional to the variance of log-likelihoods across models for each text. Our method significantly reduces the number of required texts while preserving the accuracy of KL divergence estimates. Experiments show that it achieves comparable performance to uniform sampling with about half as many texts, and also facilitates efficient incorporation of new models into an existing map. These results enable scalable and efficient construction of language model maps.

</details>


### [115] [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)

*Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hybrid Architecture, Mixture of Experts, Machine Learning, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** Introduction of Hunyuan-TurboS, a novel hybrid model combining Mamba and Transformer architectures for efficient large language processing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a model that effectively combines Mamba's long-sequence processing with Transformer’s contextual understanding, optimizing computational resources.

**Method:** A hybrid Transformer-Mamba Mixture of Experts (MoE) model employing a 56B parameter architecture with features like adaptive long-short chain-of-thought mechanism and advanced reinforcement learning techniques.

**Key Contributions:**

	1. Introduction of a hybrid MoE model architecture
	2. Adaptive long-short CoT mechanism
	3. Comprehensive post-training strategies including Supervised Fine-Tuning and Reinforcement Learning.

**Result:** Achieved a top 7 rank on LMSYS Chatbot Arena, outperforming major models and scoring 77.9% across 23 automated benchmarks.

**Limitations:** 

**Conclusion:** Hunyuan-TurboS offers high performance and efficiency, establishing a new standard for large-scale pre-trained models.

**Abstract:** As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep "thinking" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.

</details>


### [116] [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)

*Suhas Kamasetty Ramesh, Ayan Sengupta, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Language Models, Performance Improvement, Reasoning Tasks

**Relevance Score:** 8

**TL;DR:** This paper presents a comprehensive analysis of Knowledge Distillation (KD) applied to smaller language models, revealing improvements in performance and outlining key factors affecting its effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of Knowledge Distillation (KD) in transferring knowledge from larger language models to smaller ones, particularly its impact on performance across various reasoning tasks.

**Method:** Conducted a large-scale empirical analysis on models ranging from 0.5B to 7B parameters, testing them across 14 complex reasoning tasks in a zero-shot setting.

**Key Contributions:**

	1. First large-scale analysis of KD on various model sizes
	2. Identified minimal impact of teacher performance on student outcomes
	3. Showed the importance of teacher signals and logit smoothing in distillation outcomes.

**Result:** KD improves average performance of smaller models by up to 10%, with task-specific gains of up to 22%. Larger models see marginal benefits of around 1.3%. Teacher task expertise significantly influences KD effectiveness.

**Limitations:** 

**Conclusion:** While KD enhances accuracy in smaller models, it does not always preserve reasoning fidelity as demonstrated by the observed misalignment between performance improvements and structured decision-making processes.

**Abstract:** Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to $10\%$, with a peak task specific gain of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students' performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.

</details>


### [117] [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)

*Artem Zabolotnyi, Roman Makarov, Mile Mitrovic, Polina Proskura, Oleg Travkin, Roman Alferov, Alexey Zaytsev*

**Main category:** cs.CL

**Keywords:** uncertainty estimation, language models, classification tasks, parameter-efficient fine-tuning, NLP

**Relevance Score:** 8

**TL;DR:** Introducing AdUE1, an efficient post-hoc uncertainty estimation method that enhances softmax-based estimates in NLP classification tasks via differentiable approximations and regularization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenge of uncertainty estimation in adapting pre-trained language models for classification tasks, especially using parameter-efficient fine-tuning methods like adapters.

**Method:** AdUE1 employs a differentiable approximation of the maximum function and applies L2-SP regularization to anchor the fine-tuned head weights and improve model calibration.

**Key Contributions:**

	1. Introduction of AdUE1 method for uncertainty estimation
	2. Differentiable maximum function approximation
	3. L2-SP regularization for model calibration

**Result:** Our evaluations demonstrate that AdUE1 consistently outperforms established baselines, such as Mahalanobis distance and softmax response, across five NLP classification datasets and four different language models.

**Limitations:** 

**Conclusion:** The lightweight nature of AdUE1, with no changes to the base model, allows it to produce better-calibrated confidence estimates for classification tasks.

**Abstract:** Uncertainty estimation remains a critical challenge in adapting pre-trained language models to classification tasks, particularly under parameter-efficient fine-tuning approaches such as adapters. We introduce AdUE1, an efficient post-hoc uncertainty estimation (UE) method, to enhance softmax-based estimates. Our approach (1) uses a differentiable approximation of the maximum function and (2) applies additional regularization through L2-SP, anchoring the fine-tuned head weights and regularizing the model. Evaluations on five NLP classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2, Qwen) demonstrate that our method consistently outperforms established baselines such as Mahalanobis distance and softmax response. Our approach is lightweight (no base-model changes) and produces better-calibrated confidence.

</details>


### [118] [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)

*Yutao Zhu, Jiajie Jin, Hongjin Qian, Zheng Liu, Zhicheng Dou, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Role-Specific Token Optimization, Question Answering

**Relevance Score:** 8

**TL;DR:** RoleRAG is a unified retrieval-augmented generation framework that uses role-specific token optimization for efficient multi-task processing in question-answering tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Integrating various optimizations into a unified RAG framework is challenging despite existing studies optimizing retrieval-augmented generation for specific sub-tasks.

**Method:** RoleRAG consists of six modules, each handling different sub-tasks in the RAG process, and employs a dynamically resolved query graph to decompose queries. All modules are powered by a single LLM with task-specific role tokens.

**Key Contributions:**

	1. Introduction of RoleRAG for unified RAG processing.
	2. Use of role-specific token optimization for multi-task processing.
	3. Dynamic query graph representation for flexible query handling.

**Result:** Experimental results demonstrate that RoleRAG is effective, generalizable, and flexible across five open-domain question-answering datasets.

**Limitations:** 

**Conclusion:** RoleRAG streamlines deployment and reduces resource consumption while improving performance in RAG processes.

**Abstract:** Existing studies have optimized retrieval-augmented generation (RAG) across various sub-tasks, such as query understanding and retrieval refinement, but integrating these optimizations into a unified framework remains challenging. To tackle this problem, this work proposes RoleRAG, a unified RAG framework that achieves efficient multi-task processing through role-specific token optimization. RoleRAG comprises six modules, each handling a specific sub-task within the RAG process. Additionally, we introduce a query graph to represent the decomposition of the query, which can be dynamically resolved according to the decomposing state. All modules are driven by the same underlying LLM, distinguished by task-specific role tokens that are individually optimized. This design allows RoleRAG to dynamically activate different modules within a single LLM instance, thereby streamlining deployment and reducing resource consumption. Experimental results on five open-domain question-answering datasets demonstrate the effectiveness, generalizability, and flexibility of our framework.

</details>


### [119] [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)

*Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** personalized dialogue, reinforcement learning, user profiles, large language models, dynamic inference

**Relevance Score:** 9

**TL;DR:** This paper presents the Reinforcement Learning for Personalized Alignment (RLPA) framework to enhance personalized interaction with large language models (LLMs) through dynamic user profile learning and dialogue refinement.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing prompt-based and offline optimization methods for personalized dialogue that struggle in cold-start scenarios and long-term personalization.

**Method:** The RLPA framework involves iteratively inferring and refining user profiles through dialogue using two reward structures: Profile Reward for user representation accuracy and Response Reward for consistency in model responses.

**Key Contributions:**

	1. Introduction of RLPA framework for dynamic user profile alignment
	2. Development of Qwen-RLPA model achieving state-of-the-art personalized dialogue performance
	3. Robustness in managing long-term personalization and conflicting user preferences

**Result:** Qwen-RLPA, based on fine-tuning the Qwen-2.5-3B-Instruct model, outperforms traditional personalization methods and advanced models like Claude-3.5 and GPT-4o in empirical evaluations, showcasing robustness in handling conflicting user preferences.

**Limitations:** 

**Conclusion:** The findings support the effectiveness of dynamic user profile inference in developing more adaptable and efficient personalized dialogue systems.

**Abstract:** Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios and long-term personalization due to their inherently static and shallow designs. In this work, we introduce the Reinforcement Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts with a simulated user model to iteratively infer and refine user profiles through dialogue. The training process is guided by a dual-level reward structure: the Profile Reward encourages accurate construction of user representations, while the Response Reward incentivizes generation of responses consistent with the inferred profile. We instantiate RLPA by fine-tuning Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art performance in personalized dialogue. Empirical evaluations demonstrate that Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines, and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o. Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting user preferences, sustaining long-term personalization and delivering more efficient inference compared to recent reasoning-focused LLMs. These results emphasize the potential of dynamic profile inference as a more effective paradigm for building personalized dialogue systems.

</details>


### [120] [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)

*Yukun Zhao, Lingyong Yan, Zhenyang Li, Shuaiqiang Wang, Zhumin Chen, Zhaochun Ren, Dawei Yin*

**Main category:** cs.CL

**Keywords:** large language models, catastrophic forgetting, task adaptation, Joint Flashback Adaptation, knowledge sharing

**Relevance Score:** 9

**TL;DR:** The paper presents Joint Flashback Adaptation, a method to reduce catastrophic forgetting in large language models by utilizing limited prompts from old tasks during new task adaptation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with catastrophic forgetting when learning new tasks incrementally, necessitating better methods for task adaptation without relying heavily on existing approaches.

**Method:** The approach introduces flashbacks—limited prompts from old tasks—to constrain model output deviations while interpolating latent tasks between flashbacks and new tasks for effective joint learning.

**Key Contributions:**

	1. Introduction of flashbacks for task adaptation.
	2. Task-agnostic approach requiring limited prompts from old tasks.
	3. Demonstrated effectiveness across 1000+ diverse tasks.

**Result:** Extensive experiments demonstrate that Joint Flashback Adaptation significantly improves generalization on new tasks while minimizing forgetting of old tasks across diverse instruction-following and reasoning tasks.

**Limitations:** The reliance on limited prompts may not generalize to all scenarios of catastrophic forgetting.

**Conclusion:** This method effectively allows task-agnostic adaptation with minimal reliance on replay data, yielding strong performance on various benchmarks.

**Abstract:** Large language models have achieved remarkable success in various tasks. However, it is challenging for them to learn new tasks incrementally due to catastrophic forgetting. Existing approaches rely on experience replay, optimization constraints, or task differentiation, which encounter strict limitations in real-world scenarios. To address these issues, we propose Joint Flashback Adaptation. We first introduce flashbacks -- a limited number of prompts from old tasks -- when adapting to new tasks and constrain the deviations of the model outputs compared to the original one. We then interpolate latent tasks between flashbacks and new tasks to enable jointly learning relevant latent tasks, new tasks, and flashbacks, alleviating data sparsity in flashbacks and facilitating knowledge sharing for smooth adaptation. Our method requires only a limited number of flashbacks without access to the replay data and is task-agnostic. We conduct extensive experiments on state-of-the-art large language models across 1000+ instruction-following tasks, arithmetic reasoning tasks, and general reasoning tasks. The results demonstrate the superior performance of our method in improving generalization on new tasks and reducing forgetting in old tasks.

</details>


### [121] [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)

*Yiyun Zhou, Chang Yao, Jingyuan Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-Tuning, Parameter-Efficient Fine-Tuning, Collaborative Strategies, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** CoLA improves performance in parameter-efficient fine-tuning of Large Language Models by introducing a flexible architecture and collaborative strategies, outperforming existing methods in low-sample scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies and limitations in traditional fine-tuning methods for Large Language Models, specifically in multi-task scenarios.

**Method:** We propose CoLA, a parameter-efficient fine-tuning method incorporating a flexible LoRA architecture and three collaborative strategies to optimize resource use.

**Key Contributions:**

	1. Introduction of CoLA as a flexible LoRA architecture
	2. Development of efficient initialization schemes
	3. Implementation of collaborative strategies to enhance performance

**Result:** CoLA significantly outperforms existing parameter-efficient fine-tuning methods like LoRA in low-sample scenarios, providing robustness and efficiency in model training.

**Limitations:** 

**Conclusion:** CoLA presents a novel approach to improve performance in fine-tuning large language models, especially when data is limited, paving the way for more effective applications in various tasks.

**Abstract:** The scaling law of Large Language Models (LLMs) reveals a power-law relationship, showing diminishing return on performance as model scale increases. While training LLMs from scratch is resource-intensive, fine-tuning a pre-trained model for specific tasks has become a practical alternative. Full fine-tuning (FFT) achieves strong performance; however, it is computationally expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like LoRA, have been proposed to address these challenges by freezing the pre-trained model and adding lightweight task-specific modules. LoRA, in particular, has proven effective, but its application to multi-task scenarios is limited by interference between tasks. Recent approaches, such as Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these issues but still struggle with sample scarcity and noise interference due to their fixed structure. In response, we propose CoLA, a more flexible LoRA architecture with an efficient initialization scheme, and introduces three collaborative strategies to enhance performance by better utilizing the quantitative relationships between matrices $A$ and $B$. Our experiments demonstrate the effectiveness and robustness of CoLA, outperforming existing PEFT methods, especially in low-sample scenarios. Our data and code are fully publicly available at https://github.com/zyy-2001/CoLA.

</details>


### [122] [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)

*Song Dai, Yibo Yan, Jiamin Su, Dongfang Zihao, Yubo Gao, Yonghua Hei, Jungang Li, Junyan Zhang, Sicheng Tao, Zhuoran Gao, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Physics Reasoning, Benchmark, Variable Identification, Solution Derivation

**Relevance Score:** 5

**TL;DR:** Introduction of PhysicsArena, a benchmarking platform for evaluating multimodal physics reasoning capabilities of Large Language Models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored application of Multimodal Large Language Models in complex physics reasoning, which involves unique challenges not covered by existing benchmarks.

**Method:** The paper presents PhysicsArena, a benchmark evaluating MLLMs on three dimensions: variable identification, physical process formulation, and solution derivation.

**Key Contributions:**

	1. Introduction of the first multimodal physics reasoning benchmark
	2. Focus on variable identification and process formulation
	3. Holistic evaluation method for MLLMs in physics applications

**Result:** The development of PhysicsArena provides a holistic assessment tool for evaluating and advancing the physics reasoning abilities of MLLMs in a multimodal context.

**Limitations:** 

**Conclusion:** PhysicsArena establishes a comprehensive platform to enhance understanding and performance of MLLMs in physics reasoning tasks.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in diverse reasoning tasks, yet their application to complex physics reasoning remains underexplored. Physics reasoning presents unique challenges, requiring grounding in physical conditions and the interpretation of multimodal information. Current physics benchmarks are limited, often focusing on text-only inputs or solely on problem-solving, thereby overlooking the critical intermediate steps of variable identification and process formulation. To address these limitations, we introduce PhysicsArena, the first multimodal physics reasoning benchmark designed to holistically evaluate MLLMs across three critical dimensions: variable identification, physical process formulation, and solution derivation. PhysicsArena aims to provide a comprehensive platform for assessing and advancing the multimodal physics reasoning abilities of MLLMs.

</details>


### [123] [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)

*Zhanyue Qin, Yue Ding, Deyuan Liu, Qingbin Liu, Junxian Cai, Xi Chen, Zhiying Tu, Dianhui Chu, Cuiyun Gao, Dianbo Sui*

**Main category:** cs.CL

**Keywords:** Large Language Models, Gender Bias, Evaluation Datasets, Fine-Tuning Algorithms, Bias Mitigation

**Relevance Score:** 9

**TL;DR:** This paper proposes datasets and an algorithm to evaluate and mitigate gender bias in Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address gender bias in LLMs due to their training on biased data.

**Method:** Introduces GenBiasEval and GenHintEval datasets with evaluation metrics (AFGB-Score and UB-Score), and a fine-tuning algorithm (LFTF) to reduce gender bias.

**Key Contributions:**

	1. GenBiasEval and GenHintEval datasets for gender bias evaluation
	2. AFGB-Score and UB-Score metrics for bias quantification
	3. LFTF algorithm for fine-tuning LLMs to reduce gender bias

**Result:** The LFTF algorithm effectively reduces gender bias in LLMs while maintaining performance.

**Limitations:** 

**Conclusion:** LFTF is a promising solution for mitigating gender bias in LLMs.

**Abstract:** Nowadays, Large Language Models (LLMs) have attracted widespread attention due to their powerful performance. However, due to the unavoidable exposure to socially biased data during training, LLMs tend to exhibit social biases, particularly gender bias. To better explore and quantifying the degree of gender bias in LLMs, we propose a pair of datasets named GenBiasEval and GenHintEval, respectively. The GenBiasEval is responsible for evaluating the degree of gender bias in LLMs, accompanied by an evaluation metric named AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is used to assess whether LLMs can provide responses consistent with prompts that contain gender hints, along with the accompanying evaluation metric UB-Score (UnBias Score). Besides, in order to mitigate gender bias in LLMs more effectively, we present the LFTF (Locating First and Then Fine-Tuning) algorithm.The algorithm first ranks specific LLM blocks by their relevance to gender bias in descending order using a metric called BMI (Block Mitigating Importance Score). Based on this ranking, the block most strongly associated with gender bias is then fine-tuned using a carefully designed loss function. Numerous experiments have shown that our proposed LFTF algorithm can significantly mitigate gender bias in LLMs while maintaining their general capabilities.

</details>


### [124] [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)

*Qihuang Zhong, Liang Ding, Xiantao Cai, Juhua Liu, Bo Du, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Knowledge-aware Fine-tuning, large language models, question-answering, conflict detection, ACL2025

**Relevance Score:** 9

**TL;DR:** Proposes a Knowledge-aware Fine-tuning (KaFT) strategy to enhance LLM performance in domain-specific QA by managing knowledge conflicts during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the suboptimal performance of large language models in question-answering tasks caused by conflicts between internal model knowledge and training data context.

**Method:** Develops a query diversification strategy for conflict detection and introduces the KaFT approach, which adjusts training weights based on the conflict level of training samples.

**Key Contributions:**

	1. Introduced a novel Knowledge-aware Fine-tuning (KaFT) method
	2. Demonstrated how varying conflict levels in training data affect model performance
	3. Proved that utilizing conflict data appropriately benefits model fine-tuning.

**Result:** KaFT leads to significant improvements in LLM performance across various models, enhancing generalization and reducing hallucination.

**Limitations:** 

**Conclusion:** Implementing KaFT strategically enhances the robustness of LLMs in QA tasks by effectively handling knowledge conflicts.

**Abstract:** Supervised fine-tuning (SFT) is a common approach to improve the domain-specific question-answering (QA) performance of large language models (LLMs). However, recent literature reveals that due to the conflicts between LLMs' internal knowledge and the context knowledge of training data, vanilla SFT using the full QA training set is usually suboptimal. In this paper, we first design a query diversification strategy for robust conflict detection and then conduct a series of experiments to analyze the impact of knowledge conflict. We find that 1) training samples with varied conflicts contribute differently, where SFT on the data with large conflicts leads to catastrophic performance drops; 2) compared to directly filtering out the conflict data, appropriately applying the conflict data would be more beneficial. Motivated by this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to adapt the training weight by assigning different rewards for different training samples according to conflict level. Extensive experiments show that KaFT brings consistent and significant improvements across four LLMs. More analyses prove that KaFT effectively improves the model generalization and alleviates the hallucination.

</details>


### [125] [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)

*Isidora Jeknic, Alex Duchnowski, Alexander Koller*

**Main category:** cs.CL

**Keywords:** dialogue agents, Traveling Salesman problem, LLM prompting, symbolic mechanisms, collaboration

**Relevance Score:** 8

**TL;DR:** The paper presents a dialogue game where agents collaboratively solve the Traveling Salesman problem, showcasing an agent that integrates LLM prompting with symbolic mechanisms for improved collaboration and generalization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of solving complex NP-hard optimization problems through collaborative dialogue agents that interact with human users.

**Method:** Introduction of a dialogue game involving agents that solve the Traveling Salesman problem, utilizing LLM prompting along with symbolic mechanisms for state tracking and grounding.

**Key Contributions:**

	1. Introduces a novel dialogue game for collaborative problem-solving.
	2. Presents an agent that combines LLM prompting with symbolic state tracking.
	3. Demonstrates effective collaboration with human users in solving NP-hard problems.

**Result:** The developed agent successfully solves 45% of games optimally in self-play and effectively collaborates with human users, demonstrating the capability to generalize to unfamiliar graph structures.

**Limitations:** 

**Conclusion:** The approach highlights the effectiveness of combining LLM capabilities with symbolic reasoning to enhance dialogue-based problem-solving in complex optimization tasks.

**Abstract:** Dialogue agents that support human users in solving complex tasks have received much attention recently. Many such tasks are NP-hard optimization problems that require careful collaborative exploration of the solution space. We introduce a novel dialogue game in which the agents collaboratively solve a two-player Traveling Salesman problem, along with an agent that combines LLM prompting with symbolic mechanisms for state tracking and grounding. Our best agent solves 45% of games optimally in self-play. It also demonstrates an ability to collaborate successfully with human users and generalize to unfamiliar graphs.

</details>


### [126] [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)

*Federico Ranaldi, Andrea Zugarini, Leonardo Ranaldi, Fabio Massimo Zanzotto*

**Main category:** cs.CL

**Keywords:** protoknowledge, Large Language Models, Knowledge Graphs, Text-to-SPARQL, semantic bias

**Relevance Score:** 8

**TL;DR:** This paper introduces protoknowledge, a concept for measuring and formalizing how token sequences in Knowledge Graphs are internalized and utilized by LLMs during pretraining and inference, with implications for Text-to-SPARQL performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs internalize and leverage memorized token sequences from Knowledge Graphs, and to improve their performance on tasks like Text-to-SPARQL.

**Method:** The authors categorize protoknowledge into lexical, hierarchical, and topological forms and measure it through Knowledge Activation Tasks (KATs), analyzing properties like semantic bias and prompting strategies.

**Key Contributions:**

	1. Introduction of the protoknowledge concept for LLMs
	2. Categorization of protoknowledge types
	3. Development of an analysis framework for evaluating knowledge activation

**Result:** The investigation shows varying impacts of protoknowledge on Text-to-SPARQL performance based on specific prompting strategies, with practical outcomes for model predictions and knowledge activation.

**Limitations:** 

**Conclusion:** The study offers a new analysis framework to explore Semantic-Level Data Contamination and enhance Closed-Pretraining models' effectiveness in utilizing protoknowledge.

**Abstract:** We introduce the concept of protoknowledge to formalize and measure how sequences of tokens encoding Knowledge Graphs are internalized during pretraining and utilized at inference time by Large Language Models (LLMs). Indeed, LLMs have demonstrated the ability to memorize vast amounts of token sequences during pretraining, and a central open question is how they leverage this memorization as reusable knowledge through generalization. We then categorize protoknowledge into lexical, hierarchical, and topological forms, varying on the type of knowledge that needs to be activated. We measure protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general properties such as semantic bias. We then investigate the impact of protoknowledge on Text-to-SPARQL performance by varying prompting strategies depending on input conditions. To this end, we adopt a novel analysis framework that assesses whether model predictions align with the successful activation of the relevant protoknowledge for each query. This methodology provides a practical tool to explore Semantic-Level Data Contamination and serves as an effective strategy for Closed-Pretraining models.

</details>


### [127] [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)

*Prasoon Bajpai, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** test-time scaling, multilingual reasoning, low-resource languages

**Relevance Score:** 8

**TL;DR:** This paper presents a study on test-time scaling in multilingual settings, revealing performance variations across languages and introducing a new approach to enhance reasoning in low-resource languages.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unexplored effectiveness of test-time scaling in multilingual contexts and improve reasoning performance in low-resource languages.

**Method:** The study evaluates DeepSeek-R1-Distill-LLama-8B and DeepSeek-R1-Distill-Qwen-7B across various Latin-script languages, analyzing mid-reasoning language switches and inconsistencies in early reasoning.

**Key Contributions:**

	1. First systematic study of test-time scaling in multilingual settings
	2. Introduction of MITT for reasoning prefix-tuning
	3. Demonstrated significant performance enhancement for underrepresented languages

**Result:** Findings indicate significant variations in reasoning gains based on language, with models often switching to English mid-reasoning, and that low-resource languages exhibit lower early reasoning consistency.

**Limitations:** The study primarily focuses on Latin-script languages and may not generalize to other language scripts.

**Conclusion:** The introduction of MITT, a lightweight reasoning prefix-tuning approach, significantly enhances test-time scaling for low-resource languages, improving reasoning performance overall.

**Abstract:** Test-time scaling has emerged as a widely adopted inference-time strategy for boosting reasoning performance. However, its effectiveness has been studied almost exclusively in English, leaving its behavior in other languages largely unexplored. We present the first systematic study of test-time scaling in multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script languages. Our findings reveal that the relative gains from test-time scaling vary significantly across languages. Additionally, models frequently switch to English mid-reasoning, even when operating under strictly monolingual prompts. We further show that low-resource languages not only produce initial reasoning thoughts that differ significantly from English but also have lower internal consistency across generations in their early reasoning. Building on our findings, we introduce MITT (Multilingual Initial Thought Transfer), an unsupervised and lightweight reasoning prefix-tuning approach that transfers high-resource reasoning prefixes to enhance test-time scaling across all languages, addressing inconsistencies in multilingual reasoning performance. MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance, especially for underrepresented languages.

</details>


### [128] [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)

*Lang Gao, Kaiyang Wan, Wei Liu, Chenxi Wang, Zirui Song, Zixiang Xu, Yanbo Wang, Veselin Stoyanov, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** Bias in LLMs, Concept Activation Vectors, Sparse Autoencoders, BiasLens, Fairness in AI

**Relevance Score:** 9

**TL;DR:** The paper introduces BiasLens, a test-set-free framework for analyzing bias in Large Language Models (LLMs) through concept representations, showing strong alignment with traditional metrics and revealing hidden biases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the significant issue of bias in LLMs that affects their reliability and fairness, particularly in how multiple concepts are correlated within the model's space.

**Method:** BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to analyze the model's vector space and assess bias via representational similarity without the need for labeled data.

**Key Contributions:**

	1. Introduction of BiasLens, a new bias analysis framework
	2. Combining CAVs with Sparse Autoencoders for interpretable representation
	3. Capability to detect hidden biases without labeled data

**Result:** BiasLens demonstrates a strong correlation with traditional bias evaluation methods (Spearman correlation r > 0.85) and effectively uncovers biases that existing methods may overlook, such as bias arising from a patient's insurance status in clinical scenarios.

**Limitations:** 

**Conclusion:** BiasLens provides a scalable and interpretable analysis of bias in LLMs, facilitating improvements in fairness and transparency.

**Abstract:** Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., "positive" and "negative"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of "food" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.

</details>


### [129] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)

*Angelie Kraft, Judith Simon, Sonja Schimmler*

**Main category:** cs.CL

**Keywords:** bias, question-answering, reading comprehension, large language models, representation

**Relevance Score:** 8

**TL;DR:** This paper analyzes biases in question-answering and reading comprehension benchmarks for large language models, highlighting the lack of diversity in their creation and the need for improved practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the fairness of question-answering (QA) and reading comprehension (RC) benchmarks used for evaluating large language models (LLMs), which are found to be biased.

**Method:** Qualitative analysis of 30 benchmark papers and quantitative analysis of 20 benchmark datasets to investigate the demographics of involved stakeholders and assess social bias.

**Key Contributions:**

	1. Identification of biases in existing QA and RC benchmarks
	2. Evaluation of the demographics of creators and its influence on benchmark content
	3. Recommendations for improving diversity and bias-awareness in benchmark creation

**Result:** The analysis showed limited transparency in benchmark creation, widespread biases related to gender, religion, and geography, and a lack of measures to ensure fair representation among creators and annotators.

**Limitations:** The study focuses on only 30 benchmark papers and 20 datasets, which may not represent the entire landscape of QA and RC benchmarks.

**Conclusion:** More robust and inclusive practices are necessary in creating QA and RC benchmarks to reduce biases and promote the development of fair LLMs.

**Abstract:** Question-answering (QA) and reading comprehension (RC) benchmarks are essential for assessing the capabilities of large language models (LLMs) in retrieving and reproducing knowledge. However, we demonstrate that popular QA and RC benchmarks are biased and do not cover questions about different demographics or regions in a representative way, potentially due to a lack of diversity of those involved in their creation. We perform a qualitative content analysis of 30 benchmark papers and a quantitative analysis of 20 respective benchmark datasets to learn (1) who is involved in the benchmark creation, (2) how social bias is addressed or prevented, and (3) whether the demographics of the creators and annotators correspond to particular biases in the content. Most analyzed benchmark papers provided insufficient information regarding the stakeholders involved in benchmark creation, particularly the annotators. Notably, just one of the benchmark papers explicitly reported measures taken to address social representation issues. Moreover, the data analysis revealed gender, religion, and geographic biases across a wide range of encyclopedic, commonsense, and scholarly benchmarks. More transparent and bias-aware QA and RC benchmark creation practices are needed to facilitate better scrutiny and incentivize the development of fairer LLMs.

</details>


### [130] [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)

*Wendi Zhou, Ameer Saadat-Yazdi, Nadin Kökciyan*

**Main category:** cs.CL

**Keywords:** critical questions, large language models, argumentative text, critical thinking, argumentation schemes

**Relevance Score:** 6

**TL;DR:** We present a system that generates critical questions from argumentative texts using LLMs with structured argumentation prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance critical thinking when encountering argumentative texts by generating relevant critical questions.

**Method:** Utilizes large language models with chain-of-thought prompting to generate structured arguments and then derive critical questions based on Walton's argumentation schemes.

**Key Contributions:**

	1. Leveraging LLMs with argumentation schemes for critical question generation.
	2. Implementation of a structured argumentation theory in question generation.
	3. Demonstrated competitive performance compared to baseline approaches.

**Result:** The approach achieves competitive performance in generating contextually relevant and diverse critical questions while promoting critical thinking.

**Limitations:** 

**Conclusion:** This system shows promise in detecting missing or uninformed claims and fostering critical thinking in argumentative discourse.

**Abstract:** Critical questions are essential resources to provoke critical thinking when encountering an argumentative text. We present our system for the Critical Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach leverages large language models (LLMs) with chain-of-thought prompting to generate critical questions guided by Walton's argumentation schemes. For each input intervention, we conversationally prompt LLMs to instantiate the corresponding argument scheme template to first obtain structured arguments, and then generate relevant critical questions. Following this, we rank all the available critical questions by prompting LLMs to select the top 3 most helpful questions based on the original intervention text. This combination of structured argumentation theory and step-by-step reasoning enables the generation of contextually relevant and diverse critical questions. Our pipeline achieves competitive performance in the final test set, showing its potential to foster critical thinking given argumentative text and detect missing or uninformed claims. Code available at \href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.

</details>


### [131] [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)

*Ana-Maria Bucur, Marcos Zampieri, Tharindu Ranasinghe, Fabio Crestani*

**Main category:** cs.CL

**Keywords:** Mental Health, Multilingual Data, NLP, Social Media, Digital Screening

**Relevance Score:** 8

**TL;DR:** A survey on the detection of mental health disorders using multilingual social media data, focusing on cultural nuances in online language and self-disclosure.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in mental health disorder detection in non-English texts and to improve digital screening methods in multilingual contexts.

**Method:** Survey of multilingual social media data, investigation of cultural influences on language patterns and self-disclosure behaviors, and compilation of multilingual data collections for NLP models.

**Key Contributions:**

	1. First survey on multilingual social media data for mental health disorder detection
	2. Investigation of cultural nuances affecting online language patterns
	3. Compilation of multilingual data collections for NLP model development

**Result:** Identified the influence of cultural nuances on language patterns and provided a list of multilingual data sets for NLP model development.

**Limitations:** Focused primarily on the analysis stage without proposing specific NLP model architectures.

**Conclusion:** The study provides insights for designing effective multilingual mental health screening tools that cater to diverse populations, aiming to enhance global mental health outcomes.

**Abstract:** The increasing prevalence of mental health disorders globally highlights the urgent need for effective digital screening methods that can be used in multilingual contexts. Most existing studies, however, focus on English data, overlooking critical mental health signals that may be present in non-English texts. To address this important gap, we present the first survey on the detection of mental health disorders using multilingual social media data. We investigate the cultural nuances that influence online language patterns and self-disclosure behaviors, and how these factors can impact the performance of NLP tools. Additionally, we provide a comprehensive list of multilingual data collections that can be used for developing NLP models for mental health screening. Our findings can inform the design of effective multilingual mental health screening tools that can meet the needs of diverse populations, ultimately improving mental health outcomes on a global scale.

</details>


### [132] [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)

*Florin Cuconasu, Simone Filice, Guy Horowitz, Yoelle Maarek, Fabrizio Silvestri*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, positional bias, LLM, distracting passages, machine learning

**Relevance Score:** 9

**TL;DR:** The paper explores how positional bias affects LLM performance in Retrieval Augmented Generation, revealing that distracting passages often rank higher than relevant ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of positional bias in LLMs on the efficacy of Retrieval Augmented Generation, particularly focusing on how distracting passages can affect LLM outputs.

**Method:** Extensive experiments were conducted across three benchmarks to analyze the ranking of retrieved passages and the influence of positional bias on their weights in LLM prompts.

**Key Contributions:**

	1. Reveals the prevalence of distracting passages in LLM prompts under Retrieval Augmented Generation.
	2. Demonstrates that positional bias is less impactful in real scenarios than previously believed.
	3. Challenges the effectiveness of existing strategies to reorder retrieved passages based on relevance.

**Result:** It was found that over 60% of queries had at least one distracting passage in the top 10 retrieved, which diminished the effect of positional bias since both relevant and distracting information were penalized.

**Limitations:** The study primarily focuses on benchmark tests and may not fully represent all real-world applications or contexts of LLM usage.

**Conclusion:** Sophisticated rearrangement strategies based on LLM positional preferences do not yield better results than random shuffling of retrieved passages.

**Abstract:** Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.

</details>


### [133] [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)

*Mohammad Ali, Naeemul Hassan*

**Main category:** cs.CL

**Keywords:** Framing Analysis, Semantic Relations, Dependency Parsing, News Media, Gun Violence

**Relevance Score:** 4

**TL;DR:** This paper introduces Semantic Relations-based Unsupervised Framing Analysis (SUFA), a novel method for analyzing entity-centric frames in news media using semantic relations and dependency parsing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved methods in computational framing analysis for better understanding of media emphasis frames, particularly regarding sensitive topics such as gun violence.

**Method:** SUFA employs semantic relations and dependency parsing algorithms to identify and analyze entity-centric emphasis frames in news articles.

**Key Contributions:**

	1. Introduction of SUFA as a new methodology for framing analysis.
	2. Validation of the approach using real-world datasets.
	3. Discussion of strengths and limitations of the SUFA methodology.

**Result:** The approach was validated using a dataset related to gun violence, showcasing its ability to effectively capture framing in news media.

**Limitations:** The study may be limited by the datasets used and the specific focus on gun violence rather than a broader range of topics.

**Conclusion:** SUFA represents a significant methodological contribution to computational framing analysis with applications across social sciences and computational fields.

**Abstract:** This research presents a novel approach to computational framing analysis, called Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA leverages semantic relations and dependency parsing algorithms to identify and assess entity-centric emphasis frames in news media reports. This innovative method is derived from two studies -- qualitative and computational -- using a dataset related to gun violence, demonstrating its potential for analyzing entity-centric emphasis frames. This article discusses SUFA's strengths, limitations, and application procedures. Overall, the SUFA approach offers a significant methodological advancement in computational framing analysis, with its broad applicability across both the social sciences and computational domains.

</details>


### [134] [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)

*David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** large language models, educational technology, reinforcement learning, tutoring, pedagogy

**Relevance Score:** 9

**TL;DR:** This paper proposes an online reinforcement learning framework that aligns large language models (LLMs) for effective tutoring by focusing on pedagogical quality and guided problem-solving.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in educational contexts where effective pedagogy requires strategic withholding of answers, transforming them into more effective tutors.

**Method:** An online reinforcement learning-based alignment framework that adapts LLMs by simulating student-tutor interactions, emphasizing pedagogical quality and guided problem-solving.

**Key Contributions:**

	1. Introduction of a pedagogical alignment framework using online reinforcement learning.
	2. Demonstration of superior performance of a smaller tutor model compared to larger models.
	3. Enhanced model interpretability through thinking tags for instructional planning.

**Result:** A 7B parameter tutor model was trained without human annotations, achieving performance comparable to larger proprietary models, while maintaining reasoning capabilities and introducing controlled reward weighting.

**Limitations:** 

**Conclusion:** The proposed framework enables LLMs to serve as effective tutors by preserving reasoning capabilities and enhancing interpretability through instructional planning, thus improving overall pedagogical effectiveness.

**Abstract:** Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model's instructional planning.

</details>


### [135] [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)

*Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, Junxian He*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Reinforcement Learning, Reward Shaping

**Relevance Score:** 7

**TL;DR:** This paper presents LASER, a novel method for enhancing reasoning efficiency in Large Reasoning Models by applying length-based reward shaping in reinforcement learning, significantly improving performance while reducing redundancy in outputs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies caused by redundancy in outputs of Large Reasoning Models during complex problem-solving tasks.

**Method:** The paper proposes a unified framework for efficient reasoning methods and introduces the Length-bAsed StEp Reward shaping method (LASER), along with its adaptive variant LASER-D, which incorporates dynamic and difficulty-aware reward shaping.

**Key Contributions:**

	1. Introduction of the Length-bAsed StEp Reward shaping method (LASER)
	2. Development of the adaptive LASER-D method incorporating dynamic and difficulty-aware reward shaping
	3. Empirical validation showing significant performance and efficiency improvements over previous methods

**Result:** LASER-D achieves a +6.1 improvement on AIME2024 and reduces token usage by 63%, demonstrating enhanced reasoning performance and efficiency.

**Limitations:** 

**Conclusion:** The proposed methods significantly enhance the reasoning capabilities of models while promoting concise outputs with less redundancy.

**Abstract:** Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant "self-reflections". Resources are at https://github.com/hkust-nlp/Laser.

</details>


### [136] [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)

*Tiasa Singha Roy, Aditeya Baral, Ayush Rajesh Jhaveri, Yusuf Baig*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, evaluation frameworks, MAPLE score, reasoning misalignment

**Relevance Score:** 8

**TL;DR:** This study introduces the MAPLE score, a novel evaluation metric for assessing the reasoning abilities of large language models (LLMs) in mathematical tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation frameworks for LLMs focus only on final answer accuracy, ignoring the intricacies of mathematical reasoning.

**Method:** A new evaluation framework using the MAPLE score that assesses reasoning misalignment by incorporating error rates, redundancy, and validity.

**Key Contributions:**

	1. Introduction of the MAPLE score for evaluating LLMs in mathematical reasoning.
	2. Holistic assessment of reasoning misalignment in LLM outputs.
	3. Integration of multiple factors (error rates, redundancy, validity) in performance evaluation.

**Result:** The MAPLE score provides a comprehensive assessment of LLMs' mathematical reasoning capabilities, revealing areas of misalignment in their performance.

**Limitations:** 

**Conclusion:** Using the MAPLE score allows for a more nuanced understanding of LLMs' reasoning abilities, potentially guiding improvements in model training and evaluation.

**Abstract:** Large language models (LLMs) demonstrate considerable potential in various natural language tasks but face significant challenges in mathematical reasoning, particularly in executing precise, multi-step logic. However, current evaluation frameworks judge their performance solely based on accuracy, which only accounts for the final answer. This study explores these pitfalls by employing a novel evaluation framework. We propose an evaluation metric called the MAPLE score, which holistically quantifies reasoning misalignment by integrating error rates, redundancy, and validity.

</details>


### [137] [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)

*David Thulke, Jakob Kemmler, Christian Dugast, Hermann Ney*

**Main category:** cs.CL

**Keywords:** large language models, retrieval augmented generation, climate science

**Relevance Score:** 9

**TL;DR:** This paper investigates the faithfulness of retrieval augmented generation models in climate science and introduces ClimateGPT Faithful+, which improves accuracy in supported claims.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To make complex climate-related documents more accessible and reduce factual hallucinations in large language models.

**Method:** The authors assess the faithfulness of various models and analyze the impact of instruction fine-tuning on ClimateGPT. An improved model, ClimateGPT Faithful+, is developed by excluding unfaithful training data.

**Key Contributions:**

	1. Introduction of ClimateGPT Faithful+ for improved model output fidelity
	2. Assessment methodology for faithfulness in retrieval augmented generation
	3. Quantitative improvement in atomic claims support in climate-related contexts

**Result:** ClimateGPT Faithful+ achieves a significant improvement in faithfulness, increasing supported atomic claims from 30% to 57%.

**Limitations:** 

**Conclusion:** The findings highlight the importance of model training and data selection in enhancing the reliability of language models in climate science applications.

**Abstract:** Large language models that use retrieval augmented generation have the potential to unlock valuable knowledge for researchers, policymakers, and the public by making long and technical climate-related documents more accessible. While this approach can help alleviate factual hallucinations by relying on retrieved passages as additional context, its effectiveness depends on whether the model's output remains faithful to these passages. To address this, we explore the automatic assessment of faithfulness of different models in this setting. We then focus on ClimateGPT, a large language model specialised in climate science, to examine which factors in its instruction fine-tuning impact the model's faithfulness. By excluding unfaithful subsets of the model's training data, we develop ClimateGPT Faithful+, which achieves an improvement in faithfulness from 30% to 57% in supported atomic claims according to our automatic metric.

</details>


### [138] [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)

*Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought, Sparse Autoencoders, reasoning

**Relevance Score:** 8

**TL;DR:** This paper presents a method to enhance the reasoning capabilities of Large Language Models using steering techniques without requiring external datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning in Large Language Models (LLMs) using Chain-of-Thought techniques without the need for extensive and costly fine-tuning with long CoT data.

**Method:** The proposed method employs Sparse Autoencoders (SAEs) to extract features from vanilla CoT and uses these to steer the LLM's internal states. A novel SAE-free algorithm is also introduced to compute steering directions directly from LLM residual activations.

**Key Contributions:**

	1. Development of SAE-based steering to improve LLM reasoning using extracted features.
	2. Introduction of a novel SAE-free steering algorithm that computes directions from LLM activations.
	3. Demonstration of significant enhancements in LLM reasoning capabilities.

**Result:** Both the SAE-based and SAE-free steering algorithms significantly improve the reasoning performance of LLMs according to experimental results.

**Limitations:** 

**Conclusion:** The introduced techniques provide an effective way to enhance LLM reasoning without relying on external datasets, suggesting potential for broader application.

**Abstract:** Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.

</details>


### [139] [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)

*Ke Hu, Krishna Puvvada, Elena Rastorgueva, Zhehuai Chen, He Huang, Shuoyang Ding, Kunal Dhawan, Hainan Xu, Jagadeesh Balam, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** timestamp prediction, Canary model, speech processing, automatic speech translation, NLP

**Relevance Score:** 8

**TL;DR:** A data-driven approach for word-level timestamp prediction in the Canary model that outperforms traditional methods without the need for external alignment mechanisms.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate word-level timestamps for tasks like speech content retrieval and timed subtitles.

**Method:** Utilizes the NeMo Forced Aligner as a teacher model, generating timestamps and training the Canary model to predict them directly, introducing a new <|timestamp|> token for start and end timestamps.

**Key Contributions:**

	1. Introduced a new <|timestamp|> token for the Canary model
	2. Eliminated need for external alignment mechanisms in timestamp prediction
	3. Demonstrated effective timestamp prediction across multiple languages with minimal WER degradation.

**Result:** Achieved precision and recall rates between 80% and 90% for timestamp predictions, with errors ranging from 20 to 120 ms across four languages while maintaining minimal WER degradation.

**Limitations:** 

**Conclusion:** This new method enables efficient integration of timestamp prediction into the Canary model, extending the application to automatic speech translation tasks with acceptable prediction errors.

**Abstract:** We introduce a data-driven approach for enabling word-level timestamp prediction in the Canary model. Accurate timestamp information is crucial for a variety of downstream tasks such as speech content retrieval and timed subtitles. While traditional hybrid systems and end-to-end (E2E) models may employ external modules for timestamp prediction, our approach eliminates the need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner (NFA) as a teacher model, we generate word-level timestamps and train the Canary model to predict timestamps directly. We introduce a new <|timestamp|> token, enabling the Canary model to predict start and end timestamps for each word. Our method demonstrates precision and recall rates between 80% and 90%, with timestamp prediction errors ranging from 20 to 120 ms across four languages, with minimal WER degradation. Additionally, we extend our system to automatic speech translation (AST) tasks, achieving timestamp prediction errors around 200 milliseconds.

</details>


### [140] [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)

*Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, fine-tuning, data extraction, backdoor training, machine learning

**Relevance Score:** 9

**TL;DR:** This paper uncovers a risk in fine-tuning open-source Large Language Models (LLMs) where proprietary data can be extracted by the model's creator through backdoor training.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of proprietary data to fine-tune open-source LLMs has raised concerns about potential data breaches.

**Method:** The authors conducted experiments on 4 open-source models with various parameter sizes, evaluating the feasibility of extracting fine-tuning data through backdoor training techniques.

**Key Contributions:**

	1. Identification of data extraction risk in fine-tuning LLMs.
	2. High extraction rates demonstrated through experiments.
	3. Discussion of a detection-based defense strategy's limitations.

**Result:** The results show that up to 76.3% of fine-tuning queries can be extracted from the model, with a success rate of 94.9% in optimal conditions.

**Limitations:** Detection-based defenses can be bypassed with improved attack methods.

**Conclusion:** The findings highlight a significant risk of data breaches during LLM fine-tuning, emphasizing the need for further research and improved defensive strategies.

**Abstract:** Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.

</details>


### [141] [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)

*Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Żelasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Speech-to-speech, Human-computer interaction, Dialogue systems, Real-time adaptability

**Relevance Score:** 8

**TL;DR:** This paper presents a novel duplex speech-to-speech (S2S) architecture that enables continuous user and agent streams, enhancing real-time adaptability in spoken dialogue systems.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve real-time adaptability in spoken dialogue systems which currently lack continuous user input handling.

**Method:** The proposed duplex S2S model utilizes a pretrained streaming encoder for user input, allowing simultaneous modeling of user and agent speech without speech pretraining.

**Key Contributions:**

	1. First duplex S2S model using continuous inputs without speech pretraining
	2. Halved bitrate compared to previous models
	3. Openly available model with training and inference code

**Result:** The model outperforms existing duplex S2S models in reasoning, turn-taking, and barge-in abilities, while halving the bitrate and requiring less speech data for training.

**Limitations:** 

**Conclusion:** This duplex S2S model simplifies the process of creating real-time dialogue systems from LLMs and is openly available for reproducibility.

**Abstract:** Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility.

</details>


### [142] [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)

*Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Ningyu Zhang, Kun Wang, Yang Wang*

**Main category:** cs.CL

**Keywords:** large language models, machine unlearning, token optimization, unlearning efficacy, health informatics

**Relevance Score:** 9

**TL;DR:** UniErase introduces a novel paradigm for targeted unlearning in large language models using a learnable unlearning token, achieving state-of-the-art performance in unlearning efficacy while maintaining model ability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in large language models related to outdated or sensitive information by providing effective unlearning methods.

**Method:** UniErase operates in two phases: (I) token optimization that binds unlearning objectives to the model's probability distribution, and (II) a model editing phase that activates the learned token for specified forgetting.

**Key Contributions:**

	1. Introduction of a learnable unlearning token to enhance forgetting behaviors in language models
	2. State-of-the-art performance in unlearning efficacy while retaining model ability
	3. Demonstration of effective unlearning across various knowledge settings with minimal model modification

**Result:** UniErase achieves SOTA performance across various unlearning tasks with minimal parameter adjustments, outperforming previous methods in both unlearning efficacy and model ability.

**Limitations:** 

**Conclusion:** UniErase serves as a new research direction in token learning, effectively balancing the need to forget sensitive information while retaining model capabilities.

**Abstract:** Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.

</details>


### [143] [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)

*Cosimo Iaia, Bhavin Choksi, Emily Wiebers, Gemma Roig, Christian J. Fiebach*

**Main category:** cs.CL

**Keywords:** concreteness, semantic representation, language models, psycholinguistics, cognitive psychology

**Relevance Score:** 6

**TL;DR:** This paper investigates how language models represent the concreteness of nouns by comparing human behavioral judgments with model semantic distances.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the representation of concreteness in language is crucial for psychology, neuroscience, and computational linguistics, especially in light of powerful language models.

**Method:** The study utilized behavioral judgments to estimate semantic distances for a set of abstract and concrete nouns, applying Representational Similarity Analysis to compare implicit representational spaces of participants with those of language models.

**Key Contributions:**

	1. Demonstrated alignment between human and language model representations of concreteness.
	2. Provided behavioral judgments to estimate semantic distances for nouns.
	3. Identified concrete representation as a primary driver of human-to-model alignment.

**Result:** The findings indicate a significant alignment between human and model representations, driven predominantly by the dimension of concreteness, rather than other word characteristics.

**Limitations:** 

**Conclusion:** Humans and language models exhibit convergence on the concreteness dimension, highlighting its importance in semantic representation.

**Abstract:** The nouns of our language refer to either concrete entities (like a table) or abstract concepts (like justice or love), and cognitive psychology has established that concreteness influences how words are processed. Accordingly, understanding how concreteness is represented in our mind and brain is a central question in psychology, neuroscience, and computational linguistics. While the advent of powerful language models has allowed for quantitative inquiries into the nature of semantic representations, it remains largely underexplored how they represent concreteness. Here, we used behavioral judgments to estimate semantic distances implicitly used by humans, for a set of carefully selected abstract and concrete nouns. Using Representational Similarity Analysis, we find that the implicit representational space of participants and the semantic representations of language models are significantly aligned. We also find that both representational spaces are implicitly aligned to an explicit representation of concreteness, which was obtained from our participants using an additional concreteness rating task. Importantly, using ablation experiments, we demonstrate that the human-to-model alignment is substantially driven by concreteness, but not by other important word characteristics established in psycholinguistics. These results indicate that humans and language models converge on the concreteness dimension, but not on other dimensions.

</details>


### [144] [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)

*Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, Zhiming Zheng*

**Main category:** cs.CL

**Keywords:** Federated Learning, LLaMA2, Privacy, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** FL-LLaMA is a federated split learning framework based on LLaMA2 that improves LLM privacy and efficiency while allowing adaptable model partitioning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve LLM deployment in federated environments while ensuring privacy and reducing computational overhead.

**Method:** Introduce a split learning model that offloads parameters to the server, injects noise for security, and allows dynamic adjustment of task-dependent partitioning.

**Key Contributions:**

	1. Secure end-to-end propagation using Gaussian noise in hidden states.
	2. Client-batch and server-hierarchical strategies for parallel training.
	3. Dynamic partition point adjustments based on task and hardware.

**Result:** FL-LLaMA achieves performance comparable to LLaMA2 while enabling up to 2x train speedups and 8x inference speedups.

**Limitations:** 

**Conclusion:** The framework effectively enhances security and adaptability in federated learning settings for LLMs.

**Abstract:** Private data is typically larger and of higher quality than public data, offering great potential to improve LLM. However, its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based split learning model has emerged, offloading most model parameters to the server while retaining only the embedding and output layers on clients to ensure privacy. However, it still faces significant challenges in security, efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks, leading to reverse engineering of private data; 2) the autoregressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a secure, efficient, and adaptive federated split framework based on LLaMA2. First, we place some input and output blocks on the local client and inject Gaussian noise into forward-pass hidden states, enabling secure end-to-end propagation. Second, we employ client-batch and server-hierarchical strategies to achieve parallel training, along with attention-mask compression and KV cache mechanisms to accelerate inference, reducing communication costs effectively. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements and hardware limitations. Experiments on NLU, summarization and conversational QA tasks show that FL-LLaMA maintains performance comparable to centralized LLaMA2, and achieves up to 2x train speedups and 8x inference speedups. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FL-LLaMA in security and adaptability.

</details>


### [145] [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)

*Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, large language models, efficiency, post-regulation, reasoning

**Relevance Score:** 8

**TL;DR:** ThinkLess is a framework that improves the efficiency of reasoning in LLMs by terminating the generation process early without compromising output quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the latency and memory issues caused by excessive reasoning tokens in Chain-of-Thought prompting, which can lead to truncated answers.

**Method:** ThinkLess analyzes attention patterns and inserts terminator tokens earlier in the reasoning process to skip redundant steps, complemented by a post-regulation mechanism to maintain answer structure.

**Key Contributions:**

	1. Introduced early reasoning termination without loss of quality
	2. Developed a post-regulation mechanism for well-structured outputs
	3. Demonstrated significant reductions in decoding time and memory usage

**Result:** ThinkLess achieves accuracy comparable to full-length CoT methods while significantly reducing decoding time and memory usage, without modifying the model or requiring fine-tuning.

**Limitations:** 

**Conclusion:** The framework effectively maintains answer quality through intelligent termination of reasoning, supporting efficient LLM applications.

**Abstract:** While Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), the excessive length of reasoning tokens increases latency and KV cache memory usage, and may even truncate final answers under context limits. We propose ThinkLess, an inference-efficient framework that terminates reasoning generation early and maintains output quality without modifying the model. Atttention analysis reveals that answer tokens focus minimally on earlier reasoning steps and primarily attend to the reasoning terminator token, due to information migration under causal masking. Building on this insight, ThinkLess inserts the terminator token at earlier positions to skip redundant reasoning while preserving the underlying knowledge transfer. To prevent format discruption casued by early termination, ThinkLess employs a lightweight post-regulation mechanism, relying on the model's natural instruction-following ability to produce well-structured answers. Without fine-tuning or auxiliary data, ThinkLess achieves comparable accuracy to full-length CoT decoding while greatly reducing decoding time and memory consumption.

</details>


### [146] [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)

*Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, Jianhua Tao*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Thought Patterns, Exploration, Reasoning Models, AI

**Relevance Score:** 4

**TL;DR:** Proposes TAPO, a novel framework for enhancing reinforcement learning by incorporating external high-level guidance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the exploration capacity and reasoning capability of reinforcement learning models by integrating structured thoughts during training.

**Method:** TAPO (Thought-Augmented Policy Optimization) incorporates external high-level guidance into reinforcement learning, balancing model-internal exploration with external guidance exploitation.

**Key Contributions:**

	1. Introduction of Thought-Augmented Policy Optimization (TAPO) framework
	2. Demonstrated significant performance improvements over existing methods
	3. Enhanced explainability and readability of reasoning models

**Result:** TAPO outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math, demonstrating broader applicability across tasks.

**Limitations:** 

**Conclusion:** TAPO produces reasoning models with better explainability and output readability by leveraging external guidance.

**Abstract:** Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.

</details>


### [147] [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)

*Ryang Heo, Yongsik Seo, Junseong Lee, Dongha Lee*

**Main category:** cs.CL

**Keywords:** opinion mining, large language models, benchmark, user-generated content, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces the Online Opinion Mining Benchmark (OOMB), a dataset and evaluation protocol for assessing large language models (LLMs) in mining opinions from user-generated online content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The diverse and complex nature of user-generated online content creates challenges for traditional opinion mining approaches, necessitating a novel benchmarking solution.

**Method:** The authors present the Online Opinion Mining Benchmark (OOMB), which includes extensive annotations and a summary of key opinion topics, enabling evaluation of both extractive and abstractive mining capabilities of LLMs.

**Key Contributions:**

	1. Introduction of the OOMB dataset and evaluation protocol
	2. Extensive annotation of opinion-related entities and features
	3. Comprehensive summary of opinion topics for model evaluation

**Result:** The study analyzes the performance of LLMs in opinion mining and identifies areas where challenges persist and where LLMs show adaptability.

**Limitations:** 

**Conclusion:** OOMB provides a foundation for future research on LLM-based opinion mining and discusses potential research directions.

**Abstract:** The surge of user-generated online content presents a wealth of insights into customer preferences and market trends. However, the highly diverse, complex, and context-rich nature of such contents poses significant challenges to traditional opinion mining approaches. To address this, we introduce Online Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol designed to assess the ability of large language models (LLMs) to mine opinions effectively from diverse and intricate online environments. OOMB provides extensive (entity, feature, opinion) tuple annotations and a comprehensive opinion-centric summary that highlights key opinion topics within each content, thereby enabling the evaluation of both the extractive and abstractive capabilities of models. Through our proposed benchmark, we conduct a comprehensive analysis of which aspects remain challenging and where LLMs exhibit adaptability, to explore whether they can effectively serve as opinion miners in realistic online scenarios. This study lays the foundation for LLM-based opinion mining and discusses directions for future research in this field.

</details>


### [148] [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)

*Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling*

**Main category:** cs.CL

**Keywords:** BERT, MaxPoolBERT, classification, natural language processing, GLUE benchmark

**Relevance Score:** 8

**TL;DR:** MaxPoolBERT enhances BERT's classification accuracy by aggregating information from multiple layers and tokens through max-pooling and multi-head attention.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve BERT's fixed-length [CLS] token representation for classification tasks by utilizing valuable contextual information from other tokens and layers.

**Method:** MaxPoolBERT incorporates max-pooling of the [CLS] token across multiple layers, allows [CLS] to attend across the final layer with an additional multi-head attention layer, and combines these methods with full sequence max-pooling.

**Key Contributions:**

	1. Introduced max-pooling over multiple layers for the [CLS] token
	2. Implemented additional multi-head attention layer for enhanced token interaction
	3. Showed improved performance on classification tasks, especially in low-resource scenarios.

**Result:** MaxPoolBERT improves classification accuracy on low-resource tasks compared to standard BERT-base model, achieving better performance on the GLUE benchmark without significant model size increase.

**Limitations:** 

**Conclusion:** MaxPoolBERT is a lightweight extension that enhances BERT's performance for classification tasks by refining the [CLS] representation efficiently.

**Abstract:** The [CLS] token in BERT is commonly used as a fixed-length representation for classification tasks, yet prior work has shown that both other tokens and intermediate layers encode valuable contextual information. In this work, we propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS] representation by aggregating information across layers and tokens. Specifically, we explore three modifications: (i) max-pooling the [CLS] token across multiple layers, (ii) enabling the [CLS] token to attend over the entire final layer using an additional multi-head attention (MHA) layer, and (iii) combining max-pooling across the full sequence with MHA. Our approach enhances BERT's classification accuracy (especially on low-resource tasks) without requiring pre-training or significantly increasing model size. Experiments on the GLUE benchmark show that MaxPoolBERT consistently achieves a better performance on the standard BERT-base model.

</details>


### [149] ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)

*Alkis Koudounas, Claudio Savelli, Flavio Giobergia, Elena Baralis*

**Main category:** cs.CL

**Keywords:** machine unlearning, spoken language understanding, benchmark, AI ethics, unlearning techniques

**Relevance Score:** 8

**TL;DR:** This paper presents UnSLU-BENCH, a benchmark for machine unlearning in spoken language understanding, evaluating eight unlearning techniques across multiple datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance responsible AI practices by addressing the need for effective machine unlearning methods, especially in complex tasks related to speech.

**Method:** The authors introduce UnSLU-BENCH, assess eight unlearning techniques on four datasets across different languages, and propose a new metric to evaluate the techniques' efficacy, utility, and efficiency.

**Key Contributions:**

	1. Introduction of UnSLU-BENCH as the first benchmark for machine unlearning in speech-related tasks.
	2. Assessment of eight unlearning techniques across multiple datasets and languages.
	3. Proposal of a novel metric for evaluating unlearning techniques.

**Result:** The study reveals substantial differences in the effectiveness and computational feasibility of the unlearning techniques assessed on spoken language understanding tasks.

**Limitations:** 

**Conclusion:** UnSLU-BENCH establishes a foundational framework for future research on machine unlearning in SLU and highlights the critical need for effective methodologies in this domain.

**Abstract:** Machine unlearning, the process of efficiently removing specific information from machine learning models, is a growing area of interest for responsible AI. However, few studies have explored the effectiveness of unlearning methods on complex tasks, particularly speech-related ones. This paper introduces UnSLU-BENCH, the first benchmark for machine unlearning in spoken language understanding (SLU), focusing on four datasets spanning four languages. We address the unlearning of data from specific speakers as a way to evaluate the quality of potential "right to be forgotten" requests. We assess eight unlearning techniques and propose a novel metric to simultaneously better capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation for unlearning in SLU and reveals significant differences in the effectiveness and computational feasibility of various techniques.

</details>


### [150] [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)

*Peng Wang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Editing, Knowledge Preservation, Lyapunov Optimization, Queuing Theory

**Relevance Score:** 9

**TL;DR:** LyapLock is a novel model editing framework that optimizes sequential editing of large language models while preserving long-term knowledge, achieving superior editing performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current model editing methods suffer performance declines during sequential edits due to poor long-term knowledge preservation.

**Method:** LyapLock uses concepts from queuing theory and Lyapunov optimization to break down the editing process into manageable subproblems, ensuring rigorous theoretical guarantees for long-term preservation.

**Key Contributions:**

	1. First model editing framework with rigorous theoretical guarantees
	2. Achieves asymptotic optimal editing performance
	3. Boosts editing efficacy by 11.89% over previous baselines

**Result:** The proposed framework effectively scales to over 10,000 edits, improving average editing efficacy by 11.89% compared to state-of-the-art methods while maintaining stable capabilities.

**Limitations:** 

**Conclusion:** LyapLock provides a robust solution for efficiently managing model edits with preserved knowledge, proving beneficial for both model performance and editing capacity.

**Abstract:** Large Language Models often contain factually incorrect or outdated knowledge, giving rise to model editing methods for precise knowledge updates. However, current mainstream locate-then-edit approaches exhibit a progressive performance decline during sequential editing, due to inadequate mechanisms for long-term knowledge preservation. To tackle this, we model the sequential editing as a constrained stochastic programming. Given the challenges posed by the cumulative preservation error constraint and the gradually revealed editing tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov optimization to decompose the long-term constrained programming into tractable stepwise subproblems for efficient solving. This is the first model editing framework with rigorous theoretical guarantees, achieving asymptotic optimal editing performance while meeting the constraints of long-term knowledge preservation. Experimental results show that our framework scales sequential editing capacity to over 10,000 edits while stabilizing general capabilities and boosting average editing efficacy by 11.89\% over SOTA baselines. Furthermore, it can be leveraged to enhance the performance of baseline methods. Our code is released on https://github.com/caskcsg/LyapLock.

</details>


### [151] [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)

*Tianqi Du, Zeming Wei, Quan Chen, Chenheng Zhang, Yisen Wang*

**Main category:** cs.CL

**Keywords:** large language models, safety evaluation, transformer representations, adversarial robustness

**Relevance Score:** 9

**TL;DR:** This paper introduces Safety Representation Ranking (SRR), a novel framework that utilizes hidden states from large language models (LLMs) to improve the safety of generated responses by selecting safer outputs through a ranking mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the growing use of LLMs, there is an increasing need to address safety concerns related to harmful content generation.

**Method:** The proposed SRR framework ranks candidate responses by encoding both the instructions and completions through intermediate transformer representations, using a lightweight similarity-based scorer.

**Key Contributions:**

	1. Introduction of Safety Representation Ranking (SRR) framework
	2. Utilization of hidden states from LLMs for safety evaluation
	3. Demonstration of significant improvements in adversarial robustness

**Result:** Experiments demonstrate that SRR significantly enhances the robustness of LLMs against adversarial prompts, leading to safer output generation.

**Limitations:** 

**Conclusion:** Leveraging internal model states for safety evaluation can improve the performance of LLMs in generating safe responses, providing a new direction for safety in AI.

**Abstract:** The rapid advancement of large language models (LLMs) has demonstrated milestone success in a variety of tasks, yet their potential for generating harmful content has raised significant safety concerns. Existing safety evaluation approaches typically operate directly on textual responses, overlooking the rich information embedded in the model's internal representations. In this paper, we propose Safety Representation Ranking (SRR), a listwise ranking framework that selects safe responses using hidden states from the LLM itself. SRR encodes both instructions and candidate completions using intermediate transformer representations and ranks candidates via a lightweight similarity-based scorer. Our approach directly leverages internal model states and supervision at the list level to capture subtle safety signals. Experiments across multiple benchmarks show that SRR significantly improves robustness to adversarial prompts. Our code will be available upon publication.

</details>


### [152] [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)

*Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, deductive reasoning, interactive gameplay, dataset, detective games

**Relevance Score:** 8

**TL;DR:** TurnaboutLLM is a framework and dataset for assessing LLM deductive reasoning using detective games, revealing limitations of existing enhancement strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the deductive reasoning abilities of Large Language Models in complex narrative contexts through interactive gameplay.

**Method:** The paper introduces TurnaboutLLM, which involves testing LLMs on identifying contradictions between testimonies and evidence within narrative-rich scenarios derived from detective games.

**Key Contributions:**

	1. Introduction of a novel framework for LLM evaluation in deductive reasoning.
	2. Creation of a dataset based on detective games for challenging LLMs.
	3. Insight into the limitations of current reasoning enhancement strategies.

**Result:** Evaluation of twelve state-of-the-art LLMs indicated limitations of existing deductive reasoning enhancement strategies, with performance influenced by context size and answer space.

**Limitations:** The framework may require extensive narrative context that LLMs struggle to handle, and findings may not generalize beyond the tested models.

**Conclusion:** TurnaboutLLM presents a significant challenge for LLMs, indicating the need for improved strategies in handling complex reasoning tasks.

**Abstract:** This paper introduces TurnaboutLLM, a novel framework and dataset for evaluating the deductive reasoning abilities of Large Language Models (LLMs) by leveraging the interactive gameplay of detective games Ace Attorney and Danganronpa. The framework tasks LLMs with identifying contradictions between testimonies and evidences within long narrative contexts, a challenging task due to the large answer space and diverse reasoning types presented by its questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at limitations of popular strategies for enhancing deductive reasoning such as extensive thinking and Chain-of-Thought prompting. The results also suggest varying effects of context size, the number of reasoning step and answer space size on model performance. Overall, TurnaboutLLM presents a substantial challenge for LLMs' deductive reasoning abilities in complex, narrative-rich environments.

</details>


### [153] [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)

*He Hu, Yucheng Zhou, Juzheng Si, Qianning Wang, Hengheng Zhang, Fuji Ren, Fei Ma, Laizhong Cui*

**Main category:** cs.CL

**Keywords:** mental health, large language models, diagnostic reasoning, therapeutic frameworks, counseling quality

**Relevance Score:** 10

**TL;DR:** PsyLLM is a novel large language model designed to enhance mental health counseling by integrating diagnostic and therapeutic reasoning in line with clinical standards.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM approaches for mental health lack clinical grounding, diagnostics, and diversity in therapeutic modalities, limiting their effectiveness in real-world applications.

**Method:** Development of PsyLLM includes a novel automated data synthesis pipeline that processes mental health posts, creates multi-turn dialogues, and integrates diagnostic standards and therapeutic frameworks.

**Key Contributions:**

	1. Introduction of PsyLLM for mental health counseling
	2. Development of a novel automated data synthesis pipeline
	3. Establishment of a new benchmark and evaluation protocol.

**Result:** PsyLLM outperforms existing models on a new benchmark evaluating counseling quality across comprehensiveness, professionalism, authenticity, and safety.

**Limitations:** 

**Conclusion:** PsyLLM represents a significant advancement in LLM applications for mental health, offering an integrated approach to counseling that ensures higher quality interactions.

**Abstract:** Large language models (LLMs) hold significant potential for mental health support, capable of generating empathetic responses and simulating therapeutic conversations. However, existing LLM-based approaches often lack the clinical grounding necessary for real-world psychological counseling, particularly in explicit diagnostic reasoning aligned with standards like the DSM/ICD and incorporating diverse therapeutic modalities beyond basic empathy or single strategies. To address these critical limitations, we propose PsyLLM, the first large language model designed to systematically integrate both diagnostic and therapeutic reasoning for mental health counseling. To develop the PsyLLM, we propose a novel automated data synthesis pipeline. This pipeline processes real-world mental health posts, generates multi-turn dialogue structures, and leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate detailed clinical reasoning processes. Rigorous multi-dimensional filtering ensures the generation of high-quality, clinically aligned dialogue data. In addition, we introduce a new benchmark and evaluation protocol, assessing counseling quality across four key dimensions: comprehensiveness, professionalism, authenticity, and safety. Our experiments demonstrate that PsyLLM significantly outperforms state-of-the-art baseline models on this benchmark.

</details>


### [154] [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)

*Xiaoyu Luo, Yiyi Chen, Johannes Bjerva, Qiongxiu Li*

**Main category:** cs.CL

**Keywords:** Multilingual Large Language Models, Memorization, NLP, Cross-lingual Transferability, Language similarity

**Relevance Score:** 8

**TL;DR:** This paper studies memorization in multilingual large language models (MLLMs) across 95 languages, proposing a novel metric that incorporates language similarity to analyze memorization patterns.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding memorization behavior in multilingual large language models is critical as they are increasingly deployed, yet previous research focused mainly on monolingual models.

**Method:** A comprehensive analysis of MLLMs was conducted, employing a novel graph-based correlation metric that incorporates language similarity to explore cross-lingual memorization patterns across different languages.

**Key Contributions:**

	1. First comprehensive study of memorization in MLLMs across 95 languages.
	2. Proposed a novel graph-based correlation metric incorporating language similarity.
	3. Revealed that similar languages with fewer training tokens show higher memorization.

**Result:** The analysis found that languages with fewer training tokens exhibit higher memorization, especially when cross-lingual relationships are considered, challenging the assumption that memorization is solely dependent on training data availability.

**Limitations:** 

**Conclusion:** A language-aware perspective is essential in evaluating and addressing memorization vulnerabilities in MLLMs, with findings illustrating the link between language similarity, memorization, and cross-lingual transferability.

**Abstract:** We present the first comprehensive study of Memorization in Multilingual Large Language Models (MLLMs), analyzing 95 languages using models across diverse model scales, architectures, and memorization definitions. As MLLMs are increasingly deployed, understanding their memorization behavior has become critical. Yet prior work has focused primarily on monolingual models, leaving multilingual memorization underexplored, despite the inherently long-tailed nature of training corpora. We find that the prevailing assumption, that memorization is highly correlated with training data availability, fails to fully explain memorization patterns in MLLMs. We hypothesize that treating languages in isolation - ignoring their similarities - obscures the true patterns of memorization. To address this, we propose a novel graph-based correlation metric that incorporates language similarity to analyze cross-lingual memorization. Our analysis reveals that among similar languages, those with fewer training tokens tend to exhibit higher memorization, a trend that only emerges when cross-lingual relationships are explicitly modeled. These findings underscore the importance of a language-aware perspective in evaluating and mitigating memorization vulnerabilities in MLLMs. This also constitutes empirical evidence that language similarity both explains Memorization in MLLMs and underpins Cross-lingual Transferability, with broad implications for multilingual NLP.

</details>


### [155] [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)

*Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang*

**Main category:** cs.CL

**Keywords:** VocalBench, speech interaction, benchmark, large language models, multi-modal models

**Relevance Score:** 9

**TL;DR:** VocalBench is a new benchmark for evaluating speech interaction models, focusing on vocal communication aspects often ignored in previous evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gaps in evaluating vocal performance aspects of speech interaction models, which are typically neglected in favor of text quality assessments.

**Method:** The benchmark includes 9,400 curated instances across four dimensions: semantic quality, acoustic performance, conversational abilities, and robustness, covering 16 essential skills for vocal interaction.

**Key Contributions:**

	1. Introduction of VocalBench, a benchmark focused on vocal communication evaluation.
	2. Inclusion of diverse evaluation dimensions for comprehensive assessment.
	3. Availability of code and instances for adoption by the research community.

**Result:** Experimental results highlight significant variability in model capabilities, revealing specific strengths and weaknesses among different speech interaction models.

**Limitations:** 

**Conclusion:** VocalBench provides critical insights to guide further research in the field of speech-based interaction systems.

**Abstract:** The rapid advancement of large language models (LLMs) has accelerated the development of multi-modal models capable of vocal communication. Unlike text-based interactions, speech conveys rich and diverse information, including semantic content, acoustic variations, paralanguage cues, and environmental context. However, existing evaluations of speech interaction models predominantly focus on the quality of their textual responses, often overlooking critical aspects of vocal performance and lacking benchmarks with vocal-specific test instances. To address this gap, we propose VocalBench, a comprehensive benchmark designed to evaluate speech interaction models' capabilities in vocal communication. VocalBench comprises 9,400 carefully curated instances across four key dimensions: semantic quality, acoustic performance, conversational abilities, and robustness. It covers 16 fundamental skills essential for effective vocal interaction. Experimental results reveal significant variability in current model capabilities, each exhibiting distinct strengths and weaknesses, and provide valuable insights to guide future research in speech-based interaction systems. Code and evaluation instances are available at https://github.com/SJTU-OmniAgent/VocalBench.

</details>


### [156] [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)

*Gaurav Srivastava, Zhenyu Bi, Meng Lu, Xuan Wang*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, multi-agent debate, training framework, cross-domain generalization

**Relevance Score:** 8

**TL;DR:** This paper presents the Debate, Train, Evolve (DTE) framework for enhancing large language models' reasoning capabilities through autonomous learning via multi-agent debate.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the impracticality of solely relying on additional data for improving reasoning in large language models, advocating for models to autonomously enhance their reasoning abilities.

**Method:** The authors propose a ground truth-free training framework called Debate, Train, Evolve (DTE), which utilizes multi-agent debates for training. They introduce a prompting strategy named Reflect-Critique-Refine to enhance debate quality by guiding agents to evaluate and improve their reasoning.

**Key Contributions:**

	1. Novel ground truth-free training framework (DTE) for language models.
	2. Introduction of the Reflect-Critique-Refine prompting strategy to improve debate quality.
	3. Substantial improvements in reasoning accuracy across multiple benchmarks.

**Result:** DTE shows substantial improvements in reasoning performance, achieving an average accuracy gain of 8.92% on the GSM-PLUS dataset, alongside a 5.8% average accuracy gain across other benchmarks, indicating strong cross-domain generalization.

**Limitations:** 

**Conclusion:** The DTE framework successfully enhances the reasoning capabilities of language models without relying on external supervision, demonstrating both effectiveness and generalization across different domains.

**Abstract:** Large language models (LLMs) have improved significantly in their reasoning through extensive training on massive datasets. However, relying solely on additional data for improvement is becoming increasingly impractical, highlighting the need for models to autonomously enhance their reasoning without external supervision. In this paper, we propose Debate, Train, Evolve (DTE), a novel ground truth-free training framework that uses multi-agent debate traces to evolve a single language model. We also introduce a new prompting strategy Reflect-Critique-Refine, to improve debate quality by explicitly instructing agents to critique and refine their reasoning. Extensive evaluations on five reasoning benchmarks with six open-weight models show that our DTE framework achieve substantial improvements, with an average accuracy gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe strong cross-domain generalization, with an average accuracy gain of 5.8% on all other benchmarks, suggesting that our method captures general reasoning capabilities.

</details>


### [157] [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)

*Mikhail Budnikov, Ivan Yamshchikov*

**Main category:** cs.CL

**Keywords:** transfer learning, synthetic languages, natural language understanding

**Relevance Score:** 7

**TL;DR:** This paper investigates transfer learning from synthetic languages to English, introducing a new synthetic language and a benchmark for evaluating fine-tuned models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the transfer of knowledge from synthetic languages to English and evaluate the effectiveness of fine-tuned models.

**Method:** Exploring embeddings from fine-tuned models and establishing the Tiny-Cloze Benchmark for evaluating linguistic task performance.

**Key Contributions:**

	1. Introduction of a novel synthetic language for better transfer to English.
	2. Development of Tiny-Cloze Benchmark for natural language understanding.
	3. Evaluation of fine-tuned models across multiple domains.

**Result:** The new synthetic language outperforms previous languages for transfer, and the Tiny-Cloze Benchmark provides more informative evaluations for less powerful models.

**Limitations:** 

**Conclusion:** Fine-tuning on a newly introduced synthetic language yields improved performance across various natural language understanding tasks.

**Abstract:** This work explores transfer learning from several synthetic languages to English. We investigate the structure of the embeddings in the fine-tuned models, the information they contain, and the capabilities of the fine-tuned models on simple linguistic tasks. We also introduce a new synthetic language that leads to better transfer to English than the languages used in previous research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic benchmark for natural language understanding that is more informative for less powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in several domains demonstrating that fine-tuning on a new synthetic language allows for better performance on a variety of tasks.

</details>


### [158] [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)

*Huanxuan Liao, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** context compression, Large Language Models, instruction tuning, information retention, natural language processing

**Relevance Score:** 8

**TL;DR:** HyCo2 is a proposed method for improving long-sequence inference in LLMs by integrating global and local context compression techniques to retain essential semantics and details.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with long-sequence inference due to inefficiencies and information loss in current context compression methods.

**Method:** The HyCo2 method combines both global and local context compression strategies, refining semantics with different adapters and using a classification layer to retain or discard context tokens based on their importance.

**Key Contributions:**

	1. Proposes a novel hybrid context compression method (HyCo2) for LLMs.
	2. Introduces a mechanism for integrating global and local context retention.
	3. Demonstrates significant improvement in long-sequence inference and efficiency on QA benchmarks.

**Result:** HyCo2 significantly improves long-text reasoning and reduces token usage, enhancing performance across various LLMs by an average of 13.1% on QA benchmarks, while also reducing token consumption by 88.8%.

**Limitations:** 

**Conclusion:** HyCo2 balances the retention of relevant global and local information, ultimately enhancing the efficiency of LLMs in processing long texts without sacrificing performance.

**Abstract:** Large Language Models (LLMs) encounter significant challenges in long-sequence inference due to computational inefficiency and redundant processing, driving interest in context compression techniques. Existing methods often rely on token importance to perform hard local compression or encode context into latent representations for soft global compression. However, the uneven distribution of textual content relevance and the diversity of demands for user instructions mean these approaches frequently lead to the loss of potentially valuable information. To address this, we propose $\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for LLMs, which integrates both global and local perspectives to guide context compression while retaining both the essential semantics and critical details for task completion. Specifically, we employ a hybrid adapter to refine global semantics with the global view, based on the observation that different adapters excel at different tasks. Then we incorporate a classification layer that assigns a retention probability to each context token based on the local view, determining whether it should be retained or discarded. To foster a balanced integration of global and local compression, we introduce auxiliary paraphrasing and completion pretraining before instruction tuning. This promotes a synergistic integration that emphasizes instruction-relevant information while preserving essential local details, ultimately balancing local and global information retention in context compression. Experiments show that our HyCo$_2$ method significantly enhances long-text reasoning while reducing token usage. It improves the performance of various LLM series by an average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover, HyCo$_2$ matches the performance of uncompressed methods while reducing token consumption by 88.8\%.

</details>


### [159] [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)

*Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Conversational Search, Query Reformulation, Reinforcement Learning, Information Retrieval, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** ConvSearch-R1 is a self-driven framework for Conversational Query Reformulation (CQR) that eliminates external supervision using reinforcement learning to enhance query reformulation for retrieval systems.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Handling context-dependent queries in conversational search systems that often have ambiguity and coreference issues.

**Method:** A two-stage approach utilizing Self-Driven Policy Warm-Up for cold-start problems and Retrieval-Guided Reinforcement Learning with rank-incentive reward shaping to optimize reformulation through retrieval signals.

**Key Contributions:**

	1. Introduction of ConvSearch-R1, a self-driven reformulation framework.
	2. Elimination of dependency on external supervision for query reformulation.
	3. Significant improvement in retrieval task performance on benchmark datasets.

**Result:** ConvSearch-R1 outperforms state-of-the-art methods by over 10% on the TopiOCQA dataset while using smaller models and no external supervision.

**Limitations:** 

**Conclusion:** The framework effectively addresses the limitations of existing CQR approaches and shows promising results in improving retrieval tasks in conversational search systems.

**Abstract:** Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision.

</details>


### [160] [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)

*Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, Xin Eric Wang*

**Main category:** cs.CL

**Keywords:** Soft Thinking, human-like reasoning, continuous concept space, Chain-of-Thought, token embeddings

**Relevance Score:** 8

**TL;DR:** This paper introduces Soft Thinking, a method that allows for more human-like reasoning by generating soft, abstract concept tokens in a continuous space, leading to improved performance over traditional reasoning models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current reasoning models are limited by discrete language processing, which impedes their expressive power and exploration of reasoning paths.

**Method:** Soft Thinking generates soft, abstract concept tokens via a probability-weighted mixture of discrete token embeddings in a continuous concept space.

**Key Contributions:**

	1. Introduction of the Soft Thinking method
	2. Demonstrated effectiveness on mathematical and coding benchmarks
	3. Reduced token usage compared to traditional methods

**Result:** Soft Thinking improves pass@1 accuracy by up to 2.48 points and reduces token usage by up to 22.4% on various mathematical and coding benchmarks.

**Limitations:** 

**Conclusion:** Soft Thinking showcases the potential for more interpretable and effective reasoning without being constrained to discrete language boundaries.

**Abstract:** Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current reasoning models, however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in the semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like "soft" reasoning by generating soft, abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which form the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent bottleneck of discrete language-based reasoning. Code is available at https://github.com/eric-ai-lab/Soft-Thinking.

</details>


### [161] [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)

*Xinyin Ma, Runpeng Yu, Gongfan Fang, Xinchao Wang*

**Main category:** cs.CL

**Keywords:** Diffusion Language Models, Key-Value Caching, Inference Speed-up, Natural Language Processing, Contextual Information

**Relevance Score:** 6

**TL;DR:** This paper presents a delayed KV-cache mechanism for diffusion language models, addressing slow inference issues and achieving significant speed-ups.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Diffusion language models face slow inference due to their architecture, which lacks key-value caching found in autoregressive models. This paper seeks to enhance the efficiency of DLMs by introducing a KV-cache-like mechanism.

**Method:** The authors propose a delayed KV-cache strategy, introducing two variants: dKV-Cache-Decode for almost lossless acceleration and dKV-Cache-Greedy for higher speed-ups with some performance trade-offs. Both strategies utilize a caching mechanism for key-value state management during the decoding process.

**Key Contributions:**

	1. Introduction of delayed KV-cache for diffusion language models
	2. Two variants (dKV-Cache-Decode and dKV-Cache-Greedy) that enhance inference speed
	3. Validation of caching strategies across various language understanding and generation benchmarks.

**Result:** The dKV-Cache mechanism achieves between 2-10x speedup in inference time, improving processing for long sequences and demonstrating effective use of contextual information during inference.

**Limitations:** 

**Conclusion:** The proposed delayed KV-cache introduces a viable solution to improve inference speed in diffusion language models, bringing them closer to the performance of autoregressive models. The study suggests that caching can be beneficial even when not included in the training phase.

**Abstract:** Diffusion Language Models (DLMs) have been seen as a promising competitor for autoregressive language models. However, diffusion language models have long been constrained by slow inference. A core challenge is that their non-autoregressive architecture and bidirectional attention preclude the key-value cache that accelerates decoding. We address this bottleneck by proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising process of DLMs. Our approach is motivated by the observation that different tokens have distinct representation dynamics throughout the diffusion process. Accordingly, we propose a delayed and conditioned caching strategy for key and value states. We design two complementary variants to cache key and value step-by-step: (1) dKV-Cache-Decode, which provides almost lossless acceleration, and even improves performance on long sequences, suggesting that existing DLMs may under-utilise contextual information during inference. (2) dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving higher speed-ups with quadratic time complexity at the cost of some performance degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference, largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on several benchmarks, delivering acceleration across general language understanding, mathematical, and code-generation benchmarks. Experiments demonstrate that cache can also be used in DLMs, even in a training-free manner from current DLMs.

</details>


### [162] [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)

*Danna Zheng, Mirella Lapata, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** NLG, factual accuracy, information alignment, benchmark, event-order consistency

**Relevance Score:** 8

**TL;DR:** MontageLie is a benchmark for evaluating information alignment in natural language generation (NLG), highlighting vulnerabilities in existing fact verification methods and introducing DoveScore for improved evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluations of natural language generation (NLG) often ignore inter-fact dependencies, leading to vulnerabilities in identifying misleading narratives. This work seeks to address these gaps in verification methods.

**Method:** The study introduces MontageLie, a benchmark that creates deceptive narratives by combining truthful statements, and proposes DoveScore, which verifies both factual accuracy and event-order consistency by modeling inter-fact relationships.

**Key Contributions:**

	1. Introduction of MontageLie benchmark for evaluating alignment in narratives
	2. Proposal of DoveScore for joint verification of factual accuracy and event order
	3. Demonstration of existing evaluators' vulnerabilities and performance degradation

**Result:** DoveScore significantly outperforms existing fine-grained evaluators, achieving improvements of over 8% in evaluating long-form text alignment, while both it and existing methods fall below a 65% AUC-ROC score against MontageLie benchmarks.

**Limitations:** 

**Conclusion:** The introduction of DoveScore provides a more robust mechanism for evaluating narrative consistency and factual accuracy, addressing critical vulnerabilities found in current methodologies.

**Abstract:** Information alignment evaluators are vital for various NLG evaluation tasks and trustworthy LLM deployment, reducing hallucinations and enhancing user trust. Current fine-grained methods, like FactScore, verify facts individually but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this work, we introduce MontageLie, a challenging benchmark that constructs deceptive narratives by "montaging" truthful statements without introducing explicit hallucinations. We demonstrate that both coarse-grained LLM-based evaluators and current fine-grained frameworks are susceptible to this attack, with AUC-ROC scores falling below 65%. To enable more robust fine-grained evaluation, we propose DoveScore, a novel framework that jointly verifies factual accuracy and event-order consistency. By modeling inter-fact relationships, DoveScore outperforms existing fine-grained methods by over 8%, providing a more robust solution for long-form text alignment evaluation. Our code and datasets are available at https://github.com/dannalily/DoveScore.

</details>


### [163] [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)

*Lisa Alazraki, Tan Yi-Chern, Jon Ander Campos, Maximilian Mozes, Marek Rei, Max Bartolo*

**Main category:** cs.CL

**Keywords:** Large Language Models, reinforcement learning, evaluation framework, human preferences, pipelining

**Relevance Score:** 9

**TL;DR:** This study presents an adversarial approach to enhance LLM evaluation scores by using judge-LLMs to tune preambles, leading to improved downstream performance without direct intervention on model responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities in the LLM-as-a-judge framework and explore the effectiveness of a new method that uses judge-LLMs as rewards for tuning preamble generators.

**Method:** The approach adopts reinforcement learning to adversarially tune models that generate text preambles, enhancing the evaluation scores of candidate-LLMs when pipelined.

**Key Contributions:**

	1. Introduction of a new adversarial tuning method for LLMs using judge-LLMs as rewards.
	2. Demonstration of improved evaluation scores without direct response manipulation.
	3. Potential applications of the approach in various domains beyond adversarial scenarios.

**Result:** Frozen LLMs paired with the tuned preamble generators achieve higher evaluation scores than prior frameworks, showing that the approach is more robust and less detectable.

**Limitations:** 

**Conclusion:** The results suggest that the new method could lead to more reliable evaluation settings for LLMs and raise questions about reverse engineering human preferences in various applications.

**Abstract:** The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework--known as LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited post hoc to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning--an approach that could find future applications in diverse tasks and domains beyond adversarial attacks.

</details>


### [164] [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)

*Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, reward benchmarks, verifier accuracy

**Relevance Score:** 6

**TL;DR:** The paper introduces two benchmarks, VerifyBench and VerifyBench-Hard, to assess reference-based reward systems in reinforcement learning, highlighting their construction and the need for improvements in model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited understanding of the accuracy of verifiers in reinforcement learning due to the lack of existing benchmarks for reference-based reward systems.

**Method:** The benchmarks were constructed through data collection, curation, and human annotation to ensure high quality, followed by a comprehensive analysis of evaluation results.

**Key Contributions:**

	1. Introduction of VerifyBench and VerifyBench-Hard benchmarks
	2. Meticulous data collection and human annotation for high-quality benchmarks
	3. Comprehensive analysis providing insights into reference-based reward systems

**Result:** Current models show considerable room for improvement on VerifyBench and VerifyBench-Hard, especially for smaller-scale models.

**Limitations:** 

**Conclusion:** The proposed benchmarks are effective tools for enhancing verifier accuracy and reasoning capabilities in models trained via reinforcement learning.

**Abstract:** Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved remarkable performance in the domain of reasoning. A key component of their training is the incorporation of verifiable rewards within reinforcement learning (RL). However, existing reward benchmarks do not evaluate reference-based reward systems, leaving researchers with limited understanding of the accuracy of verifiers used in RL. In this paper, we introduce two benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the performance of reference-based reward systems. These benchmarks are constructed through meticulous data collection and curation, followed by careful human annotation to ensure high quality. Current models still show considerable room for improvement on both VerifyBench and VerifyBench-Hard, especially smaller-scale models. Furthermore, we conduct a thorough and comprehensive analysis of evaluation results, offering insights for understanding and developing reference-based reward systems. Our proposed benchmarks serve as effective tools for guiding the development of verifier accuracy and the reasoning capabilities of models trained via RL in reasoning tasks.

</details>


### [165] [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)

*Hwan Chang, Yumin Kim, Yonghyun Jun, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, contextual non-disclosure, benchmark dataset, sensitive information, security policies

**Relevance Score:** 8

**TL;DR:** This paper introduces CoPriva, a benchmark dataset for evaluating LLM adherence to contextual non-disclosure policies in sensitive domains, revealing vulnerabilities in model compliance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the lack of benchmarks for evaluating LLMs' ability to maintain user-defined security policies in sensitive domains, particularly regarding information non-disclosure.

**Method:** The authors created the CoPriva benchmark dataset from realistic contexts, including explicit policies and challenging queries designed to test the LLMs against direct and indirect attacks on sensitive information.

**Key Contributions:**

	1. Introduction of the CoPriva benchmark dataset for evaluating LLM compliance with contextual policies.
	2. Demonstration of LLMs' vulnerabilities in preserving user-defined security in sensitive contexts.
	3. Identification of the gap in model performance between identifying correct answers and adhering to policy constraints during output generation.

**Result:** Evaluation of 10 LLMs on the CoPriva dataset showed significant vulnerabilities, with many models leaking sensitive information, particularly under indirect attacks.

**Limitations:** 

**Conclusion:** The findings reveal that current LLMs often fail to adhere to contextual non-disclosure policies, emphasizing the need for improved methods to ensure contextual security in sensitive applications.

**Abstract:** As Large Language Models (LLMs) are increasingly deployed in sensitive domains such as enterprise and government, ensuring that they adhere to user-defined security policies within context is critical-especially with respect to information non-disclosure. While prior LLM studies have focused on general safety and socially sensitive data, large-scale benchmarks for contextual security preservation against attacks remain lacking. To address this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating LLM adherence to contextual non-disclosure policies in question answering. Derived from realistic contexts, our dataset includes explicit policies and queries designed as direct and challenging indirect attacks seeking prohibited information. We evaluate 10 LLMs on our benchmark and reveal a significant vulnerability: many models violate user-defined policies and leak sensitive information. This failure is particularly severe against indirect attacks, highlighting a critical gap in current LLM safety alignment for sensitive applications. Our analysis reveals that while models can often identify the correct answer to a query, they struggle to incorporate policy constraints during generation. In contrast, they exhibit a partial ability to revise outputs when explicitly prompted. Our findings underscore the urgent need for more robust methods to guarantee contextual security.

</details>


### [166] [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)

*Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin*

**Main category:** cs.CL

**Keywords:** large language models, in-context learning, retrieval augmentation, question answering, transparency

**Relevance Score:** 9

**TL;DR:** This paper investigates in-context retrieval augmentation in large language models for question answering, revealing how specific attention heads contribute to information retrieval and answer generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the mechanisms of in-context retrieval augmentation in large language models, enhancing our understanding of their design and function.

**Method:** An attribution-based method is used to identify specialized attention heads and their roles in retrieving contextual information and storing relational knowledge.

**Key Contributions:**

	1. Development of an attribution-based method for analyzing attention heads
	2. Identification of distinct roles of attention heads in language models
	3. Demonstration of how attention weights affect answer generation

**Result:** The study identifies in-context heads that understand instructions and retrieve information, as well as parametric heads that hold relational knowledge, and demonstrates how modifying attention weights impacts answer generation.

**Limitations:** 

**Conclusion:** Insights gained can lead to safer and more transparent language models by tracing knowledge sources during inference.

**Abstract:** Large language models are able to exploit in-context learning to access external knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain unclear. In this work, we shed light on the mechanism of in-context retrieval augmentation for question answering by viewing a prompt as a composition of informational components. We propose an attribution-based method to identify specialized attention heads, revealing in-context heads that comprehend instructions and retrieve relevant contextual information, and parametric heads that store entities' relational knowledge. To better understand their roles, we extract function vectors and modify their attention weights to show how they can influence the answer generation process. Finally, we leverage the gained insights to trace the sources of knowledge used during inference, paving the way towards more safe and transparent language models.

</details>


### [167] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)

*Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinqlin Jia, Junxu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Graphical User Interface, Object Grounding, Machine Learning, Performance Optimization

**Relevance Score:** 7

**TL;DR:** This paper introduces GUI-G1-3B, a GUI agent trained to improve object grounding using Reinforcement Learning with innovative input and output strategies to optimize performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance performance in grounding tasks for GUI agents by identifying and solving challenges in existing RL training pipelines.

**Method:** The authors analyze input design, output evaluation, and policy updates, proposing solutions like a Fast Thinking Template, box size constraints in rewards, and adjustments in the RL objective for better sample optimization.

**Key Contributions:**

	1. Introduction of Fast Thinking Template for direct answer generation
	2. Box size constraint in reward function to prevent reward hacking
	3. Revised RL objective with difficulty-aware scaling for better optimization

**Result:** The proposed approach achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro, surpassing earlier models and setting a new benchmark for GUI agent performance.

**Limitations:** 

**Conclusion:** The strategies developed lead to significant advancements in grounding accuracy in GUI agents, suggesting effective adaptations of RL for these tasks.

**Abstract:** Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [168] [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)

*Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Logical Reasoning, Multi-modal Reasoning

**Relevance Score:** 8

**TL;DR:** This paper presents Mixture-of-Thought (MoT), a framework that enables large language models (LLMs) to reason across three modalities—natural language, code, and truth-table logic—enhancing logical reasoning capabilities.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM-based approaches typically use a single reasoning modality, limiting their effectiveness in complex logical reasoning tasks. The paper aims to address this limitation by proposing a multi-modal framework.

**Method:** MoT employs a two-phase design: (1) self-evolving MoT training using filtered, self-generated rationales across multiple modalities, and (2) MoT inference that synergizes these modalities for improved predictions.

**Key Contributions:**

	1. Introduction of the Mixture-of-Thought (MoT) framework for multi-modal reasoning in LLMs.
	2. Demonstrated significant improvements in logical reasoning benchmarks over single-modality methods.
	3. Insights into the complementary strengths of reasoning modalities in enhancing inference accuracy.

**Result:** Experiments on logical reasoning benchmarks, including FOLIO and ProofWriter, indicate that MoT significantly outperforms state-of-the-art single-modality LLM approaches, with an average accuracy gain of up to 11.7 percentage points.

**Limitations:** 

**Conclusion:** The MoT framework provides notable benefits in both training and inference stages, particularly for difficult logical reasoning problems, and highlights the complementary strengths of different modalities, especially through the use of truth-table reasoning.

**Abstract:** Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference.

</details>


### [169] [Predicting generalization performance with correctness discriminators](https://arxiv.org/abs/2311.09422)

*Yuekun Yao, Alexander Koller*

**Main category:** cs.CL

**Keywords:** NLP, accuracy prediction, discriminator, out-of-distribution, semantic parsing

**Relevance Score:** 8

**TL;DR:** The paper presents a model predicting NLP accuracy bounds for unseen data without gold labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance trustworthiness in NLP models by predicting accuracy on unseen, potentially out-of-distribution data.

**Method:** The authors train a discriminator to predict whether the outputs of a sequence-to-sequence model are correct, establishing upper and lower bounds on the model's accuracy.

**Key Contributions:**

	1. Introduction of a novel accuracy prediction model for NLP
	2. Establishes accuracy bounds without needing gold labels
	3. Demonstrates effectiveness across multiple NLP tasks

**Result:** The predicted accuracy bounds are reliably between the true gold accuracy across various NLP tasks, indicating that the bounds are close together.

**Limitations:** 

**Conclusion:** The model offers a novel approach for assessing NLP model accuracy without gold labels, contributing to trust in model predictions.

**Abstract:** The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.

</details>


### [170] [A Framework for Real-time Safeguarding the Text Generation of Large Language Model](https://arxiv.org/abs/2404.19048)

*Ximing Dong, Dayi Lin, Shaowei Wang, Ahmed E. Hassan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Ethics, Toxicity Reduction, Real-time Framework, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** LLMSafeGuard is a framework that integrates an external validator into LLM decoding to prevent harmful outputs while maintaining output quality and reducing inference time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical and societal risks posed by large language models (LLMs) generating harmful content.

**Method:** A lightweight real-time framework that uses a similarity-based validation approach, integrating an external validator into the decoding process of LLMs, and employs a context-wise timing selection strategy.

**Key Contributions:**

	1. Introduction of LLMSafeGuard for real-time validation of LLM outputs.
	2. Similarity-based validation approach simplifying constraint introduction.
	3. Context-wise timing selection strategy that optimizes intervention during generation.

**Result:** LLMSafeGuard reduces toxic output by at least 38.6% and cuts inference time by at least 24.2%, while preserving linguistic quality compared to state-of-the-art baselines.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates a superior approach to safely managing LLM outputs without compromising quality or efficiency.

**Abstract:** Large Language Models (LLMs) have significantly advanced natural language processing (NLP) tasks but also pose ethical and societal risks due to their propensity to generate harmful content. Existing methods have limitations, including the need for training specific control models and proactive intervention during text generation, that lead to quality degradation and increased computational overhead. To mitigate those limitations, we propose LLMSafeGuard, a lightweight real-time framework that integrates an external validator into decoding, rejecting unsafe outputs while allowing valid ones. We introduce a similarity-based validation approach, simplifying constraint introduction and eliminating the need for control model training. Additionally, LLMSafeGuard employs a context-wise timing selection strategy, intervening LLMs only when necessary. We evaluate LLMSafeGuard on detoxification and copyright safeguarding, demonstrating its superiority over SOTA baselines. In detoxification, LLMSafeGuard reduces toxic output by at least 38.6\% while preserving linguistic quality. Additionally, its context-wise timing selection cuts inference time by at least 24.2\% without compromising effectiveness.

</details>


### [171] [MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset](https://arxiv.org/abs/2406.02106)

*Weiqi Wang, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, MetAphysical ReaSoning, MARS benchmark, situational transitions, reasoning capabilities

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel method for improving Large Language Models' reasoning abilities in dynamic environments, introducing the MetAphysical ReaSoning framework and a new benchmark dataset, MARS.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for LLMs to reason about changes in distributions resulting from environmental factors or actions, an area that has been underexplored.

**Method:** The authors present a three-step discriminative process for reasoning about distributional changes and introduce MARS, the first benchmark to evaluate this reasoning across three tasks related to action changes, state changes, and situational transitions.

**Key Contributions:**

	1. Introduction of MetAphysical ReaSoning framework for LLMs
	2. Development of the MARS benchmark for evaluating reasoning capabilities
	3. Demonstration of the challenges faced by LLMs in reasoning tasks with situational changes

**Result:** Evaluations show that the proposed tasks are significantly challenging for various LLMs, revealing underperformance even among state-of-the-art models; fine-tuning and training on large conceptualization taxonomies may improve performance.

**Limitations:** The tasks remain challenging even for advanced models, and further exploration is needed to fully understand the limitations and potential improvements in LLM reasoning.

**Conclusion:** Enhancing LLMs' ability to reason in dynamic situations is crucial, and our framework and benchmark provide a necessary step toward this goal.

**Abstract:** To enable Large Language Models (LLMs) to function as conscious agents with generalizable reasoning capabilities, it is crucial that they possess the reasoning ability to comprehend situational changes (transitions) in distribution triggered by environmental factors or actions from other agents. Despite its fundamental significance, this ability remains underexplored due to the complexity of modeling infinite possible changes in an event and their associated distributions, coupled with the lack of benchmark data with situational transitions. Addressing these gaps, we propose a novel formulation of reasoning with distributional changes as a three-step discriminative process, termed as MetAphysical ReaSoning. We then introduce the first-ever benchmark, MARS, comprising three tasks corresponding to each step. These tasks systematically assess LLMs' capabilities in reasoning the plausibility of (i) changes in actions, (ii) states caused by changed actions, and (iii) situational transitions driven by changes in action. Extensive evaluations with 20 (L)LMs of varying sizes and methods indicate that all three tasks in this process pose significant challenges, even for state-of-the-art LLMs and LMs after fine-tuning. Further analyses reveal potential causes for the underperformance of LLMs and demonstrate that pre-training them on large-scale conceptualization taxonomies can potentially enhance their metaphysical reasoning capabilities. Our data and models are publicly accessible at https://github.com/HKUST-KnowComp/MARS.

</details>


### [172] [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/abs/2406.12719)

*Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Tabular Question Answering, Attention Analysis, Instruction Tuning, Model Robustness

**Relevance Score:** 9

**TL;DR:** This paper investigates how factors like in-context learning and model scale influence the robustness of Large Language Models (LLMs) in Tabular Question Answering (TQA) across various domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the effectiveness of LLMs in tackling table comprehension tasks and identify key factors affecting their performance in Tabular QA.

**Method:** The authors conducted experiments on various LLMs with diverse perturbations and augmentations, focusing on datasets like WTQ, TAT-QA, and SCITAB to measure robustness under different conditions.

**Key Contributions:**

	1. Investigation of in-context learning and model scale on LLMs' TQA robustness
	2. Identification of data contamination and reliability issues in table comprehension tasks
	3. Attention analysis revealing performance sensitivity in LLM middle layers

**Result:** Larger and instruction-tuned LLMs generally show better performance, but issues like data contamination and performance reliability persist, particularly in the WTQ domain.

**Limitations:** The study primarily focuses on specific datasets and may not generalize to all tabular data types.

**Conclusion:** The research underscores the need for improved interpretable methodologies and highlights attention analysis as a critical factor in understanding LLM performance drops.

**Abstract:** Large Language Models (LLMs), already shown to ace various text comprehension tasks, have also remarkably been shown to tackle table comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension.

</details>


### [173] [Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior](https://arxiv.org/abs/2407.02099)

*Pedro Henrique Luz de Araujo, Benjamin Roth*

**Main category:** cs.CL

**Keywords:** large language models, personas, AI behavior, human-computer interaction, personalization

**Relevance Score:** 9

**TL;DR:** This paper explores the impact of assigned personas on the behavior of large language models (LLMs) across various tasks, demonstrating that personas significantly influence model variability and behavior.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates how personalizing LLMs with different personas affects their behavior, which is crucial for improving user interaction and tailoring AI responses.

**Method:** The authors assigned 162 personas from 12 categories to seven LLMs, prompting them with questions from five datasets that include both objective and subjective tasks while comparing results to control and empty persona settings.

**Key Contributions:**

	1. Demonstration of how personas impact model behavior in LLMs
	2. Comparison of persona effects across multiple models and datasets
	3. Introduction of a structured framework to analyze and categorize personas

**Result:** The findings indicate that all models exhibit greater variability in responses under persona assignments compared to control settings, with certain behavioral measures generalizing across different models.

**Limitations:** The study focused on a limited number of personas and LLMs, which may not capture the full spectrum of behavior or application contexts.

**Conclusion:** By demonstrating the substantial impact of personas on LLM behavior, the study emphasizes the importance of persona assignment in enhancing the effectiveness and personalization of AI interactions.

**Abstract:** One way to personalize and steer generations from large language models (LLM) is to assign a persona: a role that describes how the user expects the LLM to behave (e.g., a helpful assistant, a teacher, a woman). This paper investigates how personas affect diverse aspects of model behavior. We assign to seven LLMs 162 personas from 12 categories spanning variables like gender, sexual orientation, and occupation. We prompt them to answer questions from five datasets covering objective (e.g., questions about math and history) and subjective tasks (e.g., questions about beliefs and values). We also compare persona's generations to two baseline settings: a control persona setting with 30 paraphrases of "a helpful assistant" to control for models' prompt sensitivity, and an empty persona setting where no persona is assigned. We find that for all models and datasets, personas show greater variability than the control setting and that some measures of persona behavior generalize across models.

</details>


### [174] [A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting](https://arxiv.org/abs/2407.11638)

*He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua*

**Main category:** cs.CL

**Keywords:** Large Language Models, temporal event forecasting, benchmark dataset

**Relevance Score:** 8

**TL;DR:** This paper evaluates LLMs for temporal event forecasting, constructs a benchmark dataset, and identifies key performance factors and limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate the reasoning capabilities of LLMs in the domain of temporal event forecasting, which has been under-explored.

**Method:** Construct a benchmark dataset (MidEast-TE-mini) and design baseline methods using different input formats and retrieval augmented generation modules to evaluate LLM performance.

**Key Contributions:**

	1. Introduction of the MidEast-TE-mini benchmark dataset
	2. Evaluation of various LLM-based methods for temporal event forecasting
	3. Identification of limitations in LLM performance related to retrieval-augmented generation.

**Result:** Fine-tuning LLMs with raw texts improves forecasting performance, while retrieval modules capture temporal patterns. However, issues like popularity bias and long-tail problems remain.

**Limitations:** Popularity bias and long-tail problem in LLMs, particularly affecting RAG methods.

**Conclusion:** This evaluation contributes significantly to understanding LLM capabilities in temporal forecasting and opens new research opportunities.

**Abstract:** Recently, Large Language Models (LLMs) have demonstrated great potential in various data mining tasks, such as knowledge question answering, mathematical reasoning, and commonsense reasoning. However, the reasoning capability of LLMs on temporal event forecasting has been under-explored. To systematically investigate their abilities in temporal event forecasting, we conduct a comprehensive evaluation of LLM-based methods for temporal event forecasting. Due to the lack of a high-quality dataset that involves both graph and textual data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on this dataset, we design a series of baseline methods, characterized by various input formats and retrieval augmented generation (RAG) modules. From extensive experiments, we find that directly integrating raw texts into the input of LLMs does not enhance zero-shot extrapolation performance. In contrast, fine-tuning LLMs with raw texts can significantly improve performance. Additionally, LLMs enhanced with retrieval modules can effectively capture temporal relational patterns hidden in historical events. However, issues such as popularity bias and the long-tail problem persist in LLMs, particularly in the retrieval-augmented generation (RAG) method. These findings not only deepen our understanding of LLM-based event forecasting methods but also highlight several promising research directions. We consider that this comprehensive evaluation, along with the identified research opportunities, will significantly contribute to future research on temporal event forecasting through LLMs.

</details>


### [175] [dMel: Speech Tokenization made Simple](https://arxiv.org/abs/2407.15835)

*Richard He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, Navdeep Jaitly*

**Main category:** cs.CL

**Keywords:** speech representation, tokenization, speech synthesis, speech recognition, transformer architecture

**Relevance Score:** 8

**TL;DR:** The paper introduces dmel, a novel speech representation that discretizes mel-filterbank channels into intensity bins, aimed at improving speech synthesis and recognition tasks with better efficiency and robustness.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexity and computational costs associated with existing speech tokenization methods while improving robustness on out-of-domain audio signals.

**Method:** The authors propose a speech representation called dmel, leveraging efficient parallel encoding and decoding processes within a language model-style transformer architecture.

**Key Contributions:**

	1. Introduction of dmel, a simpler and more effective speech representation.
	2. Development of RichTTS and RichASR models that outperform specialized methods using the same architecture.
	3. Efficient parallel encoding and decoding of high-dimensional tokens.

**Result:** The proposed dmel representation shows superior performance in preserving audio content and achieves comparable or better results than existing specialized methods in speech synthesis (RichTTS) and recognition (RichASR) tasks.

**Limitations:** 

**Conclusion:** The dmel representation allows for high performance on both speech synthesis and recognition tasks within a unified framework, promoting joint modeling of speech and text.

**Abstract:** Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.

</details>


### [176] [Fine-tuning Large Language Models for Entity Matching](https://arxiv.org/abs/2409.08185)

*Aaron Steiner, Ralph Peeters, Christian Bizer*

**Main category:** cs.CL

**Keywords:** Generative Models, Entity Matching, Fine-tuning, Large Language Models, Explanation Generation

**Relevance Score:** 8

**TL;DR:** This paper explores fine-tuning Generative Large Language Models (LLMs) for entity matching, analyzing the impact of explanation types and training example selection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the performance of fine-tuned LLMs for entity matching compared to existing methods focused on prompt engineering.

**Method:** The study experiments with different types of LLM-generated explanations in training sets and evaluates the effects of fine-tuning on performance across various datasets.

**Key Contributions:**

	1. Introduces fine-tuning for LLMs in entity matching.
	2. Analyzes the effect of structured explanations on training.
	3. Investigates performance variations across different model sizes.

**Result:** Fine-tuning significantly improves performance for smaller LLMs and enhances generalization to in-domain datasets but hinders cross-domain transfer.

**Limitations:** Results for larger models are mixed, indicating potential limitations in fine-tuning efficacy across all model sizes.

**Conclusion:** Adding structured explanations positively impacts performance for most models, but selected example generation methods show mixed results.

**Abstract:** Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) the representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the models ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods, only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o-mini.

</details>


### [177] [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)

*Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin*

**Main category:** cs.CL

**Keywords:** machine unlearning, large language models, privacy, evaluation metrics, data removal

**Relevance Score:** 9

**TL;DR:** The paper discusses machine unlearning in large language models (LLMs), addressing privacy concerns and proposing new evaluation metrics and methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy and legal concerns regarding the memorization of sensitive content by LLMs, while finding efficient methods to remove this content without retraining from scratch.

**Method:** The authors categorize unlearning methods into targeted and untargeted, discuss their issues, and propose maximizing entropy for untargeted unlearning and answer preservation loss for targeted unlearning.

**Key Contributions:**

	1. Introduction of new evaluation metrics for model outputs post-unlearning.
	2. Categorization and analysis of targeted vs untargeted unlearning methods.
	3. Proposals for improved strategies such as maximizing entropy and incorporating preservation loss.

**Result:** Their approaches were tested in scenarios like fictitious unlearning and showed improvements in model output quality post-unlearning.

**Limitations:** The unpredictability of untargeted unlearning and insufficiency of existing regularization for targeted unlearning may still pose challenges.

**Conclusion:** Proposed methods provide a framework for effective machine unlearning in LLMs, enhancing privacy without significant performance costs.

**Abstract:** Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.

</details>


### [178] [Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception](https://arxiv.org/abs/2410.12788)

*Jihao Zhao, Zhiyuan Ji, Yuchen Feng, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, text chunking, large language models, Meta-Chunking, semantic coherence

**Relevance Score:** 9

**TL;DR:** This paper introduces the Meta-Chunking framework to enhance text chunking in Retrieval-Augmented Generation (RAG) systems, improving the quality of clusters for large language models through innovative strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional similarity-based chunking in the context of RAG, focusing on improving the quality of text chunking and ensuring better semantic coherence.

**Method:** The study proposes two adaptive chunking techniques, Perplexity Chunking and Margin Sampling Chunking, which utilize LLMs to determine optimal segmentation points while integrating a global information compensation mechanism involving hierarchical summary generation and text chunk rewriting.

**Key Contributions:**

	1. Introduction of Perplexity Chunking and Margin Sampling Chunking techniques.
	2. Development of a global information compensation mechanism for improved coherence.
	3. Demonstration of the effectiveness of Meta-Chunking on smaller-scale models.

**Result:** Experiments show that Meta-Chunking improves the logical coherence of text chunks and successfully implements high-quality chunking with smaller-scale models, without depending on advanced instruction-following capabilities.

**Limitations:** The experiments may have limitations in diverse text types not covered in the study.

**Conclusion:** Meta-Chunking effectively enhances chunking in RAG systems, leading to more semantically coherent text chunks and broadening the applicability to smaller models.

**Abstract:** While Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm for boosting large language models (LLMs) in knowledge-intensive tasks, it often overlooks the crucial aspect of text chunking within its workflow. This paper proposes the Meta-Chunking framework, which specifically enhances chunking quality through a dual strategy that identifies optimal segmentation points and preserves global information. Initially, breaking limitations of similarity-based chunking, we design two adaptive chunking techniques based on uncertainty, namely Perplexity Chunking and Margin Sampling Chunking, by utilizing the logical perception capabilities of LLMs. Given the inherent complexity across different texts, we integrate meta-chunk with dynamic merging, striking a balance between fine-grained and coarse-grained text chunking. Furthermore, we establish the global information compensation mechanism, encompassing a two-stage hierarchical summary generation process and a three-stage text chunk rewriting procedure focused on missing reflection, refinement, and completion. These components collectively strengthen the semantic integrity and contextual coherence of chunks. Extensive experiments demonstrate that Meta-Chunking effectively addresses challenges of the chunking task within the RAG system, providing LLMs with more logically coherent text chunks. Additionally, our methodology validates the feasibility of implementing high-quality chunking tasks with smaller-scale models, thereby eliminating the reliance on robust instruction-following capabilities.

</details>


### [179] [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852)

*Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi*

**Main category:** cs.CL

**Keywords:** large language models, implicit feedback, multimodal interaction, task completion, machine learning

**Relevance Score:** 8

**TL;DR:** ReSpect leverages implicit user feedback in multi-turn interactions to enhance LLM task completion without external annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve user interaction with LLMs by learning from implicit feedback signals during multi-turn conversations.

**Method:** Introduced ReSpect, which analyzes user feedback from past interactions to adaptively improve response quality in a multimodal interaction setting.

**Key Contributions:**

	1. Introduction of ReSpect for learning from implicit user feedback
	2. Demonstration of improved task completion in a multimodal LLM scenario
	3. No need for external annotations for training

**Result:** ReSpect improved task completion rates from 31% to 82% based on thousands of user interactions, without requiring additional annotations.

**Limitations:** 

**Conclusion:** The method effectively utilizes implicit signals in user interactions to enhance performance of language models in complex tasks.

**Abstract:** Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection without additional annotations. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct a multimodal LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.

</details>


### [180] [GATEAU: Selecting Influential Samples for Long Context Alignment](https://arxiv.org/abs/2410.15633)

*Shuzheng Si, Haozhe Zhao, Gang Chen, Yunshui Li, Kangyang Luo, Chuancheng Lv, Kaikai An, Fanchao Qi, Baobao Chang, Maosong Sun*

**Main category:** cs.CL

**Keywords:** long-context models, instruction following, data alignment, long-range dependencies, GATEAU

**Relevance Score:** 9

**TL;DR:** GATEAU is a framework designed to improve instruction-following in large language models by focusing on long-context alignment and identifying influential samples with long-range dependencies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better methods to align large language models for handling long contexts effectively, particularly in the context of instruction following.

**Method:** GATEAU identifies influential samples based on their long-range dependencies, measuring both the difficulty of generating responses and understanding inputs in long contexts.

**Key Contributions:**

	1. Introduces GATEAU framework for long-context instruction alignment.
	2. Focuses on measuring long-range dependencies in samples.
	3. Demonstrates improvements in model performance through enhanced data selection.

**Result:** Experiments show that GATEAU can effectively identify key samples, leading to enhanced instruction-following and better understanding of long contexts in the models trained on these samples.

**Limitations:** 

**Conclusion:** The proposed framework improves data quality and model performance when dealing with long-context instructions in language models.

**Abstract:** Aligning large language models to handle instructions with extremely long contexts has yet to be fully investigated. Previous studies have attempted to scale up the available data volume by synthesizing long instruction-following samples, as constructing such a dataset tends to be challenging for annotators. However, a lack of a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the model's performance. Thus, we propose GATEAU, a novel framework to address the unique challenge of long context alignment by identifying the influential samples enriched with long-range dependency relations. Specifically, GATEAU measures the long-range dependencies from two essential aspects: the difficulty of generating target responses due to the long-range dependencies, and the difficulty of understanding long inputs due to such dependencies. Comprehensive experiments indicate that GATEAU effectively identifies influential samples and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.

</details>


### [181] [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/abs/2410.16168)

*Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram*

**Main category:** cs.CL

**Keywords:** Large Language Models, cross-lingual transfer, active forgetting, multilingual representations, NLP

**Relevance Score:** 8

**TL;DR:** This paper proposes a pretraining strategy using active forgetting to improve cross-lingual capabilities of decoder-only LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficacy of Large Language Models (LLMs) in languages other than English, as existing models show limited performance in non-English contexts.

**Method:** The authors introduce a pretraining approach based on active forgetting aimed at improving decoder-only LLMs' adaptability to new and unseen languages.

**Key Contributions:**

	1. Introduction of active forgetting as a pretraining strategy for decoder-only LLMs
	2. Demonstration of improved performance in multilingual tasks
	3. Extensive experimental validation supporting the methodology

**Result:** The experiments demonstrate that LLMs pretrained with active forgetting achieve better multilingual representations and improved performance across various downstream tasks.

**Limitations:** 

**Conclusion:** The proposed method significantly enhances the cross-lingual transfer capabilities of decoder-only LLMs, making them more effective for multilingual applications.

**Abstract:** Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages. In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages. Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks.

</details>


### [182] [Robust and Minimally Invasive Watermarking for EaaS](https://arxiv.org/abs/2410.17552)

*Zongqi Wang, Baoyuan Wu, Jingyuan Deng, Yujiu Yang*

**Main category:** cs.CL

**Keywords:** Embeddings as a Service, watermarking, copyright protection, model extraction, AI applications

**Relevance Score:** 4

**TL;DR:** A novel embedding-specific watermarking mechanism (ESpeW) is proposed to enhance copyright protection for Embeddings as a Service (EaaS) by injecting unique, identifiable watermarks into each embedding to resist removal attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved copyright protection for EaaS due to vulnerabilities to model extraction attacks and the ineffectiveness of current embedding watermarking methods.

**Method:** The proposed ESpeW mechanism injects unique watermarks into embeddings, ensuring they are identifiable, maintain significant distance from each other, and do not share common components.

**Key Contributions:**

	1. Introduction of a novel ESpeW mechanism for watermarking
	2. Unique, identifiable watermarks that resist removal
	3. Minimally invasive approach affecting embeddings by less than 1%.

**Result:** ESpeW demonstrates robustness against aggressive watermark removal strategies while reducing the impact on embeddings to less than 1%.

**Limitations:** 

**Conclusion:** ESpeW sets a new standard in watermarking for EaaS, effectively protecting against removal attacks without compromising embedding quality.

**Abstract:** Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1\%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.

</details>


### [183] [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)

*Kangyang Luo, Zichen Ding, Zhenmin Weng, Lingfeng Qiao, Meng Zhao, Xiang Li, Di Yin, Jinlong Shu*

**Main category:** cs.CL

**Keywords:** Chain of Thought, Large Language Models, Curriculum Learning, Reasoning, Prompting

**Relevance Score:** 9

**TL;DR:** The paper introduces LBS3, a novel prompt approach for automatic reasoning in LLMs that harnesses curriculum learning principles to enhance reasoning capabilities and reduce human effort.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing CoT prompting methods for LLMs have limitations related to the need for extensive human effort and performance issues. This paper aims to overcome these barriers by developing a more efficient prompt strategy.

**Method:** The proposed LBS3 method utilizes a curriculum learning-inspired approach to prompt large language models. It starts with easy-to-hard proxy queries to build up to more complex tasks, employing exemplary prompts derived from the simpler queries to effectively guide LLMs.

**Key Contributions:**

	1. Introduction of LBS3 for improved reasoning in LLMs
	2. Utilization of curriculum learning principles in prompting
	3. Demonstration of superior performance compared to existing methods

**Result:** LBS3 demonstrated highly competitive performance in various reasoning-intensive tasks when compared to state-of-the-art baselines, significantly improving on the previous CoT approaches.

**Limitations:** 

**Conclusion:** The LBS3 approach successfully addresses the limitations of existing CoT methods by reducing the need for manual intervention and improving LLM performance in reasoning tasks.

**Abstract:** While Chain of Thought (CoT) prompting approaches have significantly consolidated the reasoning capabilities of large language models (LLMs), they still face limitations that require extensive human effort or have performance needs to be improved. Existing endeavors have focused on bridging these gaps; however, these approaches either hinge on external data and cannot completely eliminate manual effort, or they fall short in effectively directing LLMs to generate high-quality exemplary prompts. To address the said pitfalls, we propose a novel prompt approach for automatic reasoning named \textbf{LBS3}, inspired by curriculum learning which better reflects human learning habits. Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries that are pertinent to the target query. Following this, it invokes a progressive strategy that utilizes exemplary prompts stemmed from easy-proxy queries to direct LLMs in solving hard-proxy queries, enabling the high-quality of the proxy solutions. Finally, our extensive experiments in various reasoning-intensive tasks with varying open- and closed-source LLMs show that LBS3 achieves strongly competitive performance compared to the SOTA baselines.

</details>


### [184] [Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains](https://arxiv.org/abs/2411.07417)

*Katerina Korre, Arianna Muti, Federico Ruggeri, Alberto Barrón-Cedeño*

**Main category:** cs.CL

**Keywords:** Hate Speech, Cultural Analysis, Semantic Componential Analysis, Language Models, Detection

**Relevance Score:** 6

**TL;DR:** This paper proposes a framework for analyzing hate speech definitions across cultures, revealing significant variations and sensitivities of language models to these definitions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural influences on hate speech interpretations and the need for a systematic analysis of its definitions across different domains.

**Method:** A Semantic Componential Analysis (SCA) framework is introduced, creating a dataset of 493 hate speech definitions from over 100 cultures, and conducting zero-shot experiments with LLMs to analyze the effects of these definitions on hate speech detection.

**Key Contributions:**

	1. First dataset of hate speech definitions across cultures
	2. Semantic Componential Analysis framework
	3. Demonstration of LLM sensitivity to definition complexity

**Result:** The analysis shows considerable variance in hate speech definitions and highlights that LLM responses vary based on the complexity of the definitions provided in prompts.

**Limitations:** Limited to five key domains and may not encompass all cultural nuances in hate speech definitions.

**Conclusion:** The study concludes that the impact of cultural nuances in definitions is significant for hate speech detection models, suggesting a need for awareness when designing such systems.

**Abstract:** Hate speech relies heavily on cultural influences, leading to varying individual interpretations. For that reason, we propose a Semantic Componential Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate speech definitions. We create the first dataset of hate speech definitions encompassing 493 definitions from more than 100 cultures, drawn from five key domains: online dictionaries, academic research, Wikipedia, legal texts, and online platforms. By decomposing these definitions into semantic components, our analysis reveals significant variation across definitions, yet many domains borrow definitions from one another without taking into account the target culture. We conduct zero-shot model experiments using our proposed dataset, employing three popular open-sourced LLMs to understand the impact of different definitions on hate speech detection. Our findings indicate that LLMs are sensitive to definitions: responses for hate speech detection change according to the complexity of definitions used in the prompt.

</details>


### [185] [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)

*Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh*

**Main category:** cs.CL

**Keywords:** Speculative Decoding, Large Language Models, Draft Model, Inference Speed, Edge Devices

**Relevance Score:** 8

**TL;DR:** FastDraft introduces an efficient approach for pre-training draft models compatible with large language models, significantly enhancing inference speed and performance metrics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The growing demand for faster inference in Large Language Models necessitates the development of efficient draft models, which are often not available due to vocabulary compatibility issues.

**Method:** FastDraft involves pre-training and fine-tuning draft models on synthetic datasets generated by the target large language model, optimizing for speed and performance.

**Key Contributions:**

	1. Introduces FastDraft, a method that aligns draft models with large language models.
	2. Demonstrates significant speed and efficiency improvements in inference tasks.
	3. Validates theoretical findings through benchmarking on state-of-the-art hardware.

**Result:** FastDraft produces draft models with impressive metrics, achieving block efficiency improvements and speed-ups in various tasks, including up to 3x memory bound speed up and 2x wall-clock time reduction.

**Limitations:** 

**Conclusion:** FastDraft demonstrates that high-quality draft models can enable efficient inference for large language models on edge-devices, expanding their applicability.

**Abstract:** Speculative Decoding has gained popularity as an effective technique for accelerating the auto-regressive inference process of Large Language Models. However, Speculative Decoding entirely relies on the availability of efficient draft models, which are often lacking for many existing language models due to a stringent constraint of vocabulary compatibility. In this work we introduce FastDraft, a novel and efficient approach for pre-training and aligning a draft model to any large language model by incorporating efficient pre-training, followed by fine-tuning over synthetic datasets generated by the target model. We demonstrate FastDraft by training two highly parameter efficient drafts for the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able to produce a draft model with approximately 10 billion tokens on a single server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24 hours. Our results show that the draft model achieves impressive results in key metrics of acceptance rate, block efficiency and up to 3x memory bound speed up when evaluated on code completion and up to 2x in summarization, text completion and instruction tasks. We validate our theoretical findings through benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra, achieving a wall-clock time speedup of up to 2x, indicating a significant reduction in runtime. Due to its high quality, FastDraft unlocks large language models inference on AI-PC and other edge-devices.

</details>


### [186] [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/abs/2412.09049)

*Mengze Hong, Wailing Ng, Chen Jason Zhang, Yuanfeng Song, Di Jiang*

**Main category:** cs.CL

**Keywords:** intent clustering, LLM-in-the-loop, customer service, semantic understanding, dialogue systems

**Relevance Score:** 9

**TL;DR:** This paper proposes a framework integrating LLMs with intent clustering methods to improve semantic understanding and alignment with human perceptions in customer dialogue intentions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automated service agents need to accurately discover customer intentions, but existing methods often do not align with human perceptions.

**Method:** The LLM-in-the-loop (LLM-ITL) framework integrates fine-tuned LLMs for semantic coherence evaluation and intent cluster naming, enabling iterative discovery of coherent intent clusters.

**Key Contributions:**

	1. The framework enables effective semantic coherence evaluation using LLMs.
	2. Introduction of a comprehensive dataset for Chinese customer service intents.
	3. Demonstration of better clustering quality with lower computational costs.

**Result:** Achieves over 95% accuracy in alignment with human judgments and introduces a comprehensive Chinese dialogue intent dataset, outperforming existing methods.

**Limitations:** 

**Conclusion:** LLM-in-the-loop techniques significantly enhance the quality of intent clustering and offer a scalable, human-aligned approach.

**Abstract:** Discovering customer intentions in dialogue conversations is crucial for automated service agents. However, existing intent clustering methods often fail to align with human perceptions due to a heavy reliance on embedding distance metrics and a tendency to overlook underlying semantic structures. This paper proposes an LLM-in-the-loop (LLM-ITL) intent clustering framework, integrating the semantic understanding capabilities of LLMs into conventional clustering algorithms. Specifically, this paper (1) investigates the effectiveness of fine-tuned LLMs in semantic coherence evaluation and intent cluster naming, achieving over 95% accuracy aligned with human judgments; (2) designs an LLM-ITL framework that facilitates the iterative discovery of coherent intent clusters and the optimal number of clusters; and (3) proposes context-aware techniques tailored for customer service dialogue. As existing English benchmarks offer limited semantic diversity and intent groups, we introduce a comprehensive Chinese dialogue intent dataset, comprising over 100k real customer service calls and 1,507 human-annotated intent clusters. The proposed approaches significantly outperform LLM-guided baselines, achieving notable enhancements in clustering quality and lower computational cost. Combined with several best practices, our findings highlight the potential of LLM-in-the-loop techniques for scalable and human-aligned intent clustering.

</details>


### [187] [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)

*Yang Qin, Chao Chen, Zhihang Fu, Ze Chen, Dezhong Peng, Peng Hu, Jieping Ye*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Open-source LLMs, Multitask fine-tuning, SQL generation, Hallucinations

**Relevance Score:** 8

**TL;DR:** This paper presents ROUTE, a new method for enhancing Text-to-SQL capabilities of open-source LLMs through multitask fine-tuning and collaboration strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the applicability of open-source LLMs for Text-to-SQL in light of limitations posed by closed-source models like GPT-4.

**Method:** Multi-task supervised fine-tuning using synthetic training data for SQL generation, with additional tasks for schema linking, noise correction, and continuation writing, combined with a Multitask Collaboration Prompting strategy.

**Key Contributions:**

	1. Introduction of ROUTE for enhancing Text-to-SQL using open-source LLMs.
	2. Implementation of additional SFT tasks to strengthen SQL syntax understanding.
	3. Development of Multitask Collaboration Prompting to improve output quality.

**Result:** Extensive experiments show that ROUTE improves the performance of Text-to-SQL, surpassing existing state-of-the-art methods on several benchmarks.

**Limitations:** : None specified.

**Conclusion:** The proposed ROUTE effectively enhances the capabilities of open-source LLMs in generating SQL queries, showcasing better performance and reduced hallucinations in outputs.

**Abstract:** Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by large language models (LLMs), the latest state-of-the-art techniques are still trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which limits their applicability in open scenarios. To address this challenge, we propose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to improve the comprehensive capabilities of open-source LLMs for Text2SQL, thereby providing a more practical solution. Our approach begins with multi-task supervised fine-tuning (SFT) using various synthetic training data related to SQL generation. Unlike existing SFT-based Text2SQL methods, we introduced several additional SFT tasks, including schema linking, noise correction, and continuation writing. Engaging in a variety of SQL generation tasks enhances the model's understanding of SQL syntax and improves its ability to generate high-quality SQL queries. Additionally, inspired by the collaborative modes of LLM agents, we introduce a Multitask Collaboration Prompting (MCP) strategy. This strategy leverages collaboration across several SQL-related tasks to reduce hallucinations during SQL generation, thereby maximizing the potential of enhancing Text2SQL performance through explicit multitask capabilities. Extensive experiments and in-depth analyses have been performed on eight open-source LLMs and five widely-used benchmarks. The results demonstrate that our proposal outperforms the latest Text2SQL methods and yields leading performance.

</details>


### [188] [DARWIN 1.5: Large Language Models as Materials Science Adapted Learners](https://arxiv.org/abs/2412.11970)

*Tong Xie, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Shaozhou Wang, Wenjie Zhang, Clara Grazian, Chunyu Kit, Wanli Ouyang, Dongzhan Zhou, Bram Hoex*

**Main category:** cs.CL

**Keywords:** materials science, large language model, machine learning, property prediction, DARWIN 1.5

**Relevance Score:** 5

**TL;DR:** DARWIN 1.5 is an open-source large language model designed for materials science, improving prediction accuracy by eliminating complex descriptors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of traditional methods in materials discovery that struggle with generalizability and practical applicability due to reliance on complex descriptors.

**Method:** DARWIN 1.5 utilizes natural language input to predict material properties, integrating 6M domain papers and 21 datasets from 49,256 materials for cross-task knowledge transfer.

**Key Contributions:**

	1. Introduction of DARWIN 1.5 as a tailored LLM for materials science
	2. Significant improvement in material property prediction accuracy
	3. Unified approach to materials design without relying on complex descriptors

**Result:** The model achieves up to 59.1% improvement in prediction accuracy over the LLaMA-7B architecture and outperforms state-of-the-art machine learning approaches in 8 materials design tasks.

**Limitations:** The paper is posted prematurely and contains inaccuracies; a revised version is forthcoming.

**Conclusion:** This work establishes large language models as a promising approach for scalable and versatile materials science applications.

**Abstract:** Materials discovery and design aim to find compositions and structures with desirable properties over highly complex and diverse physical spaces. Traditional solutions, such as high-throughput simulations or machine learning, often rely on complex descriptors, which hinder generalizability and transferability across different material systems. Moreover, These descriptors may inadequately represent macro-scale material properties, which are influenced by structural imperfections and compositional variations in real-world samples, thus limiting their practical applicability. To address these challenges, we propose DARWIN 1.5, the largest open-source large language model tailored for materials science. By leveraging natural language as input, DARWIN eliminates the need for task-specific descriptors and enables a flexible, unified approach to material property prediction and discovery. Our approach integrates 6M material domain papers and 21 experimental datasets from 49,256 materials across modalities while enabling cross-task knowledge transfer. The enhanced model achieves up to 59.1% improvement in prediction accuracy over the base LLaMA-7B architecture and outperforms SOTA machine learning approaches across 8 materials design tasks. These results establish LLMs as a promising foundation for developing versatile and scalable models in materials science.

</details>


### [189] [Exploring Cross-lingual Latent Transplantation: Mutual Opportunities and Open Challenges](https://arxiv.org/abs/2412.12686)

*Yangfan Ye, Xiaocheng Feng, Xiachong Feng, Libo Qin, Yichong Huang, Lei Huang, Weitao Ma, Qichen Hong, Zhirui Zhang, Yunfei Lu, Xiaohui Yan, Duyu Tang, Dandan Tu, Bing Qin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingualism, Cultural Adaptability, Cross-Lingual Interaction, Deep Learning

**Relevance Score:** 9

**TL;DR:** This paper presents the XTransplant framework, which enhances the multilingual capabilities of large language models (LLMs) by leveraging their internalized knowledge during inference across languages.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address imbalances in multilingual capabilities and cultural adaptability in LLMs caused by English-centric training data.

**Method:** The XTransplant framework facilitates cross-lingual latent transplantation, allowing models to utilize both English and non-English resources effectively during inference.

**Key Contributions:**

	1. Introduction of the XTransplant framework for cross-lingual interaction in LLMs
	2. Identification of the roles of attention and feed-forward modules in multilingual understanding
	3. Demonstration of improvements in multilingual capability and cultural adaptability, particularly for low-resource languages.

**Result:** Empirical analysis shows that XTransplant improves multilingual capabilities and cultural adaptability, especially benefiting low-resource languages, with notable roles of attention and feed-forward modules in processing.

**Limitations:** Potential challenges related to the generalizability of the findings and the scalability of the XTransplant framework in practical applications.

**Conclusion:** The study reveals significant underutilization of multilingual potential in current LLMs and proposes further exploration of cross-lingual interactions to enhance LLM performance.

**Abstract:** Current large language models (LLMs) often exhibit imbalances in multilingual capabilities and cultural adaptability, largely attributed to their English-centric pre-training data. In this paper, we introduce and investigate a cross-lingual latent transplantation (XTransplant) framework, which aims to further exploit the model's internalized multilingual knowledge during inference and examine its effects on the multilingual capability and cultural adaptability of LLMs. XTransplant framework enables models to harness the complementary strengths of both English and non-English resources by transplanting latent activations across languages. Through extensive analysis, we empirically demonstrate that XTransplant, a form of cross-lingual interaction, has mutually beneficial effects on the multilingual capability and cultural adaptability of LLMs, particularly for low-resource languages and cultures. We further reveal that attention modules play a pivotal role in supporting multilingual understanding, while feed-forward modules are more adept at capturing culture-specific knowledge. In addition, we conduct in-depth analysis of XTransplant's stability, effectiveness, and generalizability. By probing the upper bound performance of XTransplant, we expose the considerable underutilization of current LLMs' multilingual potential-a challenge that remains open. We hope our analysis offers a new lens for advancing cross-lingual interactions and better leveraging models' internalized multilingual knowledge.

</details>


### [190] [MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/abs/2412.15540)

*Zhang Siyue, Xue Yuxiang, Zhang Yiming, Wu Xiaobao, Luu Anh Tuan, Zhao Chen*

**Main category:** cs.CL

**Keywords:** Temporal reasoning, Question answering, Large language models, Retrieval-augmented generation, Benchmarking

**Relevance Score:** 8

**TL;DR:** This paper presents the TempRAGEval benchmark for time-sensitive question answering and introduces a Modular Retrieval framework that improves retrieval performance and answer accuracy while handling temporal reasoning.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective time-sensitive question answering in LLM-powered systems due to existing limitations in current retrieval methods.

**Method:** The study introduces TempRAGEval to evaluate retrieval methods with temporal reasoning and proposes the Modular Retrieval framework, which consists of three modules: Question Processing, Retrieval and Summarization, and Semantic-Temporal Hybrid Ranking.

**Key Contributions:**

	1. Introduction of the TempRAGEval benchmark for evaluating time-sensitive question answering.
	2. Development of the Modular Retrieval framework consisting of three distinct modules.
	3. Demonstrated significant performance improvements in retrieval and answer accuracy over baseline methods.

**Result:** MRAG significantly outperforms baseline retrieval methods on the TempRAGEval benchmark in terms of retrieval performance and final answer accuracy.

**Limitations:** 

**Conclusion:** The proposed MRAG framework provides a more effective solution for time-sensitive question answering by better addressing the challenges of temporal reasoning in retrieval tasks.

**Abstract:** Understanding temporal relations and answering time-sensitive questions is crucial yet a challenging task for question-answering systems powered by large language models (LLMs). Existing approaches either update the parametric knowledge of LLMs with new facts, which is resource-intensive and often impractical, or integrate LLMs with external knowledge retrieval (i.e., retrieval-augmented generation). However, off-the-shelf retrievers often struggle to identify relevant documents that require intensive temporal reasoning. To systematically study time-sensitive question answering, we introduce the TempRAGEval benchmark, which repurposes existing datasets by incorporating temporal perturbations and gold evidence labels. As anticipated, all existing retrieval methods struggle with these temporal reasoning-intensive questions. We further propose Modular Retrieval (MRAG), a trainless framework that includes three modules: (1) Question Processing that decomposes question into a main content and a temporal constraint; (2) Retrieval and Summarization that retrieves evidence and uses LLMs to summarize according to the main content; (3) Semantic-Temporal Hybrid Ranking that scores each evidence summarization based on both semantic and temporal relevance. On TempRAGEval, MRAG significantly outperforms baseline retrievers in retrieval performance, leading to further improvements in final answer accuracy.

</details>


### [191] [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)

*Lang Gao, Jiahui Geng, Xiangliang Zhang, Preslav Nakov, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** Jailbreaking, Large Language Models, Safety Boundary, Activation Boundary Defense, Bayesian Optimization

**Relevance Score:** 9

**TL;DR:** This paper analyzes jailbreak methods in LLMs and introduces a novel defense strategy called Activation Boundary Defense (ABD).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms behind jailbreaking in LLMs and develop effective defense strategies against harmful text generation.

**Method:** Conducted a large-scale analysis of seven jailbreak methods; introduced the concept of safety boundary and proposed ABD that constrains activations within this boundary using Bayesian optimization.

**Key Contributions:**

	1. Introduced the safety boundary concept for LLMs.
	2. Developed Activation Boundary Defense (ABD) to enhance security against jailbreaks.
	3. Demonstrated high defense effectiveness with minimal effect on model capabilities.

**Result:** ABD achieved an average DSR of over 98% against various jailbreak attacks, with minimal impact on model performance.

**Limitations:** 

**Conclusion:** The study improves understanding of LLM vulnerabilities and presents an effective defense strategy that maintains model capabilities.

**Abstract:** Jailbreaking in Large Language Models (LLMs) is a major security concern as it can deceive LLMs to generate harmful text. Yet, there is still insufficient understanding of how jailbreaking works, which makes it hard to develop effective defense strategies. We aim to shed more light into this issue: we conduct a detailed large-scale analysis of seven different jailbreak methods and find that these disagreements stem from insufficient observation samples. In particular, we introduce \textit{safety boundary}, and we find that jailbreaks shift harmful activations outside that safety boundary, where LLMs are less sensitive to harmful information. We also find that the low and the middle layers are critical in such shifts, while deeper layers have less impact. Leveraging on these insights, we propose a novel defense called \textbf{Activation Boundary Defense} (ABD), which adaptively constrains the activations within the safety boundary. We further use Bayesian optimization to selectively apply the defense method to the low and the middle layers. Our experiments on several benchmarks show that ABD achieves an average DSR of over 98\% against various forms of jailbreak attacks, with less than 2\% impact on the model's general capabilities.

</details>


### [192] [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)

*Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang*

**Main category:** cs.CL

**Keywords:** human-model cooperation, NLP, large language models, taxonomy, research challenges

**Relevance Score:** 9

**TL;DR:** This paper reviews the emerging paradigm of human-model cooperation in NLP, presenting a new taxonomy and discussing challenges and frontier areas in the field.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The evolution of LLMs into autonomous agents necessitates a deeper understanding of human-model cooperation to leverage their capabilities in NLP tasks effectively.

**Method:** The paper presents a comprehensive review of existing literature on human-model cooperation, introducing a new taxonomy to categorize approaches and discussing open challenges.

**Key Contributions:**

	1. Introduction of a new taxonomy for human-model cooperation in NLP
	2. Comprehensive review of principles and challenges in human-model cooperation
	3. Identification of potential future research areas and challenges.

**Result:** The study establishes a unified perspective on human-model cooperation and identifies potential areas for future research, highlighting significant challenges that need to be addressed.

**Limitations:** 

**Conclusion:** The paper serves as an entry point for further research into human-model cooperation, suggesting that addressing the outlined challenges could lead to transformative advances in NLP.

**Abstract:** With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.

</details>


### [193] [Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter](https://arxiv.org/abs/2501.14491)

*Verena Blaschke, Masha Fedzechkina, Maartje ter Hoeve*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, NLP, linguistic similarity, POS tagging, dependency parsing

**Relevance Score:** 7

**TL;DR:** This paper analyzes cross-lingual transfer in NLP for 263 languages across various tasks, revealing that transfer performance is influenced by factors such as linguistic similarity, input representations, and the specific NLP task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how cross-lingual transfer strategies apply over a broader range of languages and NLP tasks, addressing limitations found in prior research that focused on a narrow set of languages and tasks.

**Method:** The study conducts an analysis of cross-lingual transfer using a dataset of 263 languages from diverse families, applying it to three NLP tasks: POS tagging, dependency parsing, and topic classification.

**Key Contributions:**

	1. Comprehensive analysis of cross-lingual transfer across 263 languages
	2. Evaluation across multiple NLP tasks (POS tagging, dependency parsing, topic classification)
	3. Insights into how linguistic similarity interacts with transfer performance based on various factors.

**Result:** The analysis reveals that the impact of linguistic similarity on transfer performance varies significantly based on the NLP task at hand, the nature of input representations, and how linguistic similarity is defined.

**Limitations:** The study primarily focuses on specific NLP tasks and may not cover all potential linguistic and contextual scenarios.

**Conclusion:** The findings suggest that a one-size-fits-all approach to cross-lingual transfer is inadequate; strategies must be tailored according to linguistic and task-specific factors.

**Abstract:** Cross-lingual transfer is a popular approach to increase the amount of training data for NLP tasks in a low-resource context. However, the best strategy to decide which cross-lingual data to include is unclear. Prior research often focuses on a small set of languages from a few language families and/or a single task. It is still an open question how these findings extend to a wider variety of languages and tasks. In this work, we analyze cross-lingual transfer for 263 languages from a wide variety of language families. Moreover, we include three popular NLP tasks: POS tagging, dependency parsing, and topic classification. Our findings indicate that the effect of linguistic similarity on transfer performance depends on a range of factors: the NLP task, the (mono- or multilingual) input representations, and the definition of linguistic similarity.

</details>


### [194] [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)

*Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu*

**Main category:** cs.CL

**Keywords:** Large Language Models, KV cache, compression, semantic chunks, inference

**Relevance Score:** 8

**TL;DR:** ChunkKV introduces a novel approach to LLM KV cache compression by using semantic chunks instead of individual tokens, enhancing performance and memory efficiency during inference.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with high GPU memory usage during long text processing, particularly due to KV cache requirements, leading to performance degradation with existing compression methods.

**Method:** ChunkKV treats semantic chunks as basic units for compression, implementing a layer-wise index reuse technique to reduce computational overhead while retaining essential meaning and linguistic structures.

**Key Contributions:**

	1. Introduction of ChunkKV for semantic chunk-based compression
	2. Layer-wise index reuse technique
	3. Demonstrated performance improvements on multiple benchmarks

**Result:** ChunkKV improves throughput by 26.5% and outperforms state-of-the-art methods in precision by up to 8.7% on various benchmarks while maintaining the same compression ratio.

**Limitations:** 

**Conclusion:** Semantic-aware compression significantly enhances efficiency and performance for long-context LLM inference, addressing critical memory bottlenecks.

**Abstract:** Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce ChunkKV, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in ChunkKV, reducing computational overhead and improving throughput by 26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV outperforms state-of-the-art methods by up to 8.7\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem.

</details>


### [195] [Lifelong Knowledge Editing requires Better Regularization](https://arxiv.org/abs/2502.01636)

*Akshat Gupta, Phudish Prateepamornkul, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli*

**Main category:** cs.CL

**Keywords:** Knowledge editing, Large language models, Regularization techniques

**Relevance Score:** 7

**TL;DR:** This paper addresses issues of model degradation during knowledge editing in large language models and proposes regularization techniques to improve the editing process's effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve factuality in large language models through knowledge editing while addressing the significant model degradation observed in sequential editing processes.

**Method:** The paper formalizes locate-then-edit methods as a two-step fine-tuning process and introduces two regularization techniques, Most-Probable Early Stopping (MPES) and Frobenius norm-constraint, to mitigate model degradation.

**Key Contributions:**

	1. Formalization of locate-then-edit methods as a two-step fine-tuning process.
	2. Introduction of Most-Probable Early Stopping (MPES) and Frobenius norm-constraint for model editing.
	3. Demonstrated scalability of editing methods with substantial reductions in editing time.

**Result:** The proposed regularization techniques significantly reduce model degradation, allowing for an increase in edit capacity to 10,000 edits while decreasing editing time by 42-61%.

**Limitations:** 

**Conclusion:** Targeted regularization is crucial for effective lifelong knowledge editing in large language models, enabling sustained model performance during extensive edits.

**Abstract:** Knowledge editing is a promising way to improve factuality in large language models, but recent studies have shown significant model degradation during sequential editing. In this paper, we formalize the popular locate-then-edit methods as a two-step fine-tuning process, allowing us to precisely identify the root cause of this degradation. We show that model degradation occurs due to (1) over-optimization of internal activations and (2) continuous norm-growth of edited matrices. To mitigate these issues, we introduce two regularization techniques: (1) Most-Probable Early Stopping (MPES) and (2) explicit Frobenius norm-constraint. We demonstrate that applying these simple yet effective regularization techniques at key points in the editing process can substantially mitigate model degradation. Combining these regularization methods enables scaling locate-then-edit methods to 10,000 edits while reducing editing time by 42-61%. These results show that targeted regularization is essential for lifelong knowledge editing.

</details>


### [196] [BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation](https://arxiv.org/abs/2502.01697)

*Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia*

**Main category:** cs.CL

**Keywords:** Synthetic Data Generation, LLM, Few-Shot Learning, Human-Computer Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces BARE, a novel method for generating synthetic data with only a few seed examples, resulting in diverse and high-quality datasets for training LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high demand for quality data in model training, especially when curation of high-quality examples is costly or difficult.

**Method:** The paper proposes Base-Refine (BARE), a two-stage method that leverages the diversity of base models and the quality assurance of instruction-tuned models to generate synthetic data with just a few examples.

**Key Contributions:**

	1. Proposes BARE method for few-shot synthetic data generation
	2. Demonstrates significant performance improvements over existing methods
	3. Combines diversity of base models with quality assurance of instruction-tuned models.

**Result:** Using only 3 seed examples, BARE generates diverse high-quality datasets, leading to significant performance improvements in downstream tasks, including a 101% improvement on GSM8K.

**Limitations:** 

**Conclusion:** BARE demonstrates that substantial improvements in LLM performance can be achieved with minimal seed examples, outperforming current state-of-the-art methods.

**Abstract:** As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. However, current data generation methods rely on seed sets containing tens of thousands of examples to prompt instruction-tuned models. This reliance can be especially problematic when the curation of high-quality examples is expensive or difficult. In this paper we explore the novel few-shot synthetic data generation setting -- generating a high-quality dataset from a few examples. We show that when working with only a few seed examples, instruction-tuned models used in current synthetic data methods produce insufficient diversity for downstream tasks. In contrast, we show that base models without post-training, largely untapped for synthetic data generation, offer substantially greater output diversity, albeit with lower instruction following abilities. Leveraging this insight, we propose Base-Refine (BARE), a novel two-stage method that combines the diversity of base models with the quality assurance of instruction-tuned models. BARE excels in few-shot synthetic data generation: using only 3 seed examples it generates diverse, high-quality datasets that significantly improve downstream task performance. We show that fine-tuning Llama 3.1 8B with 1,000 BARE-generated samples achieves performance comparable to state-of-the-art similarly sized models on LiveCodeBench tasks. Furthermore, data generated with BARE enables a 101% improvement for a fine-tuned Llama 3.2 1B on GSM8K over data generated by only instruction-models, and an 18.4% improvement for a fine-tuned Llama 3.1 8B over the state-of-the-art RAFT method for RAG data generation.

</details>


### [197] [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/abs/2502.01941)

*Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, Xiaowen Chu*

**Main category:** cs.CL

**Keywords:** large language models, KV cache compression, benchmarking, ShotKV, long-context generation

**Relevance Score:** 9

**TL;DR:** This paper analyzes the effects of KV cache compression methods on large language models' (LLMs) capabilities and introduces a new benchmark and compression approach called ShotKV.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how KV cache compression affects the fundamental capabilities of large language models, which have not been thoroughly studied despite existing impressive compression methods.

**Method:** A comprehensive benchmark, KVFundaBench, was developed to evaluate various fundamental LLM capabilities affected by cache compression, including world knowledge and commonsense reasoning.

**Key Contributions:**

	1. Introduction of KVFundaBench for benchmarking LLM capabilities under compression
	2. Identification of critical factors affecting LLM performance due to KV cache compression
	3. Development of ShotKV, a novel compression method with improved performance in long-context tasks

**Result:** The analysis identified several findings including task-dependent degradation and model-type robustness, along with the development of ShotKV, which improves performance by 9%-18% on long-context tasks under aggressive compression.

**Limitations:** 

**Conclusion:** The study demonstrates that existing compression methods can degrade model capabilities, and introduces ShotKV as a solution to better balance compression with performance in LLMs.

**Abstract:** This paper investigates an underexplored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. Although existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive benchmark KVFundaBench to systematically evaluate the effects of KV cache compression across diverse fundamental LLM capabilities, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals serval key findings: (1) \textit{Task-Dependent Degradation}; (2) \textit{Model-Type Robustness} (3) \textit{Prompt Length Vulnerability}; (4) \textit{Chunk-Level Superiority}; (5) \textit{Prompt-Gain Sensitivity}; (6) \textit{Long-Context Generation Sensitivity}. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios.

</details>


### [198] [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295)

*Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Peng Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, prompt design, Content-Format Integrated Prompt Optimization

**Relevance Score:** 9

**TL;DR:** The paper presents a method for optimizing the formatting and content of prompts used with Large Language Models (LLMs) to enhance their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited systematic investigation into the role of prompt formatting in addition to content optimization for LLMs.

**Method:** Introduction of Content-Format Integrated Prompt Optimization (CFPO), which jointly optimizes prompt content and formatting through iterative refinement and natural language mutations.

**Key Contributions:**

	1. Development of CFPO methodology for prompt optimization
	2. Joint optimization of content and formatting
	3. Demonstrated performance improvements in LLM applications

**Result:** CFPO shows measurable performance improvements over content-only optimization methods across multiple tasks and open-source LLMs.

**Limitations:** 

**Conclusion:** Integrated content-format optimization is crucial for enhancing LLM performance, and the methodology is model-agnostic.

**Abstract:** Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.

</details>


### [199] [An Analysis for Reasoning Bias of Language Models with Small Initialization](https://arxiv.org/abs/2502.04375)

*Junjie Yao, Zhongwang Zhang, Zhi-Qin John Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Parameter Initialization, Reasoning Tasks, Memorization Tasks, Training Dynamics

**Relevance Score:** 9

**TL;DR:** This study explores how the initialization scale of parameters in Transformer-based LLMs affects their performance on reasoning versus memorization tasks, proposing a theoretical framework based on training dynamics.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how initialization strategies influence the task performance of large language models, specifically in terms of reasoning versus memorization capabilities.

**Method:** The study employs empirical validation on real datasets and anchor functions to analyze the effects of parameter initialization scale during training.

**Key Contributions:**

	1. Investigates the impact of parameter initialization scale on LLM task preferences.
	2. Establishes a theoretical framework based on training dynamics related to initialization.
	3. Highlights the roles of embedding space and self-attention in shaping learning biases.

**Result:** Smaller initialization scales lead to a preference for reasoning tasks, while larger scales favor memorization tasks. Key model components like the embedding space and self-attention mechanisms significantly influence these biases.

**Limitations:** 

**Conclusion:** The findings enhance the understanding of LLM training dynamics and provide important guidelines for initializing model parameters to optimize performance on various tasks.

**Abstract:** Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.

</details>


### [200] [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)

*Roman Vashurin, Maiya Goloburda, Albina Ilina, Alexander Rubashevskii, Preslav Nakov, Artem Shelmanov, Maxim Panov*

**Main category:** cs.CL

**Keywords:** Uncertainty Quantification, Large Language Models, Bayes Risks, NLP Tasks, Model Confidence

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel uncertainty quantification method for Large Language Models that integrates model confidence and output consistency, leading to improved performance across various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance uncertainty quantification for Large Language Models, addressing limitations of existing methods and ensuring more reliable model outputs.

**Method:** The paper develops a new approach linking uncertainty to minimum Bayes risks in LLM decoding, integrating information-based and consistency-based strategies to create robust UQ methods.

**Key Contributions:**

	1. Integration of model confidence with output consistency for UQ in LLMs
	2. Demonstration of improved UQ performance across multiple NLP tasks
	3. Identification of characteristics of LLMs as probabilistic models affecting UQ methods

**Result:** The proposed approach shows significant performance improvements in uncertainty quantification across tasks like question answering, summarization, and machine translation, outperforming current state-of-the-art methods.

**Limitations:** 

**Conclusion:** The synthesis of model confidence and output consistency provides a more effective framework for uncertainty quantification in LLMs, indicating potential for broad applications in NLP tasks.

**Abstract:** Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompass a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches to boost UQ performance. However, they sometimes fail to outperform much simpler baseline methods. Our work discusses the fundamental approach to constructing uncertainty measures that directly links uncertainty with the minimum Bayes risks achieved by LLM decoding. Building on these findings, we propose a novel approach to integrating model confidence with output consistency, resulting in a family of efficient and robust UQ methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency, leading to a family of efficient and robust UQ methods. We evaluate our approach across various tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.

</details>


### [201] [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316)

*Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Code Input-Output Prediction, Chain-of-Thought, AI Applications

**Relevance Score:** 8

**TL;DR:** CodeI/O is a novel approach to improve reasoning tasks in Large Language Models by transforming original code into a code input-output prediction format, facilitating universal reasoning through Chain-of-Thought rationales.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the performance of Large Language Models on diverse reasoning tasks hindered by sparse training data.

**Method:** Transform original code into a code input-output prediction format to predict inputs/outputs using CoT rationales, exposing models to reasoning primitives.

**Key Contributions:**

	1. Introduction of CodeI/O approach for reasoning tasks
	2. Enhancement of CoT rationale through multi-turn revisions
	3. Performance improvements across diverse reasoning tasks.

**Result:** CodeI/O leads to consistent improvements across various reasoning tasks and introduces a multi-turn revision process, resulting in CodeI/O++ for enhanced performance.

**Limitations:** 

**Conclusion:** The approach effectively decouples structured reasoning from code syntax, achieving higher reasoning performance in AI applications.

**Abstract:** Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at https://github.com/hkust-nlp/CodeIO.

</details>


### [202] [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)

*Hwan Chang, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Unlearning, Privacy, Syntactic Similarity, Entity Unlearning

**Relevance Score:** 8

**TL;DR:** This paper investigates unlearning in large language models, focusing on the effects of removing sensitive training data and analyzing the retain set's performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses privacy concerns related to large language models retaining sensitive information from their training data and explores effective methods for LLM unlearning.

**Method:** The authors conduct a case study on entity unlearning and analyze the effects on subsets of the retain set, particularly introducing the Syntactically Similar Neighbor Set to evaluate performance.

**Key Contributions:**

	1. Introduction of Syntactically Similar Neighbor Set
	2. Analysis of unlearning effects on retain set subsets
	3. Insights on the importance of syntactic structures over other relationships

**Result:** The study finds that the syntactically similar subset suffers significant performance drops during unlearning but can be used for regularization to maintain or improve performance on related queries.

**Limitations:** 

**Conclusion:** Syntactic similarity, rather than domain or entity relationships, is a crucial factor in effective LLM unlearning, suggesting a need for tailored approaches in handling unlearning tasks.

**Abstract:** Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.

</details>


### [203] [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)

*Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Large Language Models, Graph Transformer, Machine Learning, AI

**Relevance Score:** 7

**TL;DR:** GLTW is a method that combines structural knowledge from Knowledge Graphs (KGs) with Large Language Models (LLMs) to improve Knowledge Graph Completion performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Knowledge Graph Completion by effectively integrating structural information from KGs with LLMs, addressing the limitations of existing approaches.

**Method:** The proposed method, GLTW, utilizes an improved Graph Transformer (iGT) to encode subgraphs and combines them with LLMs, incorporating a subgraph-based multi-classification training objective.

**Key Contributions:**

	1. Introduction of the improved Graph Transformer (iGT) for KGC.
	2. Development of a subgraph-based multi-classification training objective.
	3. Significant performance gains over SOTA methods in KGC tasks.

**Result:** GLTW demonstrates significant performance improvements on various KG datasets when compared to state-of-the-art (SOTA) baselines.

**Limitations:** 

**Conclusion:** GLTW effectively merges KG structural information with LLM capabilities, resulting in enhanced KGC performance.

**Abstract:** Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning efficiency.Importantly, we combine iGT with an LLM that takes KG language prompts as input.Our extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.

</details>


### [204] [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)

*Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng*

**Main category:** cs.CL

**Keywords:** Multimodal, Large Language Models, Security Alignment, Synthetic Embedding, Benchmarking

**Relevance Score:** 8

**TL;DR:** The paper presents SEA, a method that enhances the security alignment of Multimodal Large Language Models (MLLMs) by optimizing embeddings for additional modalities using only textual data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Multimodal Large Language Models (MLLMs) face significant security vulnerabilities due to the complexities introduced by additional modalities. Existing methods struggle with aligning security in low-resource settings, highlighting a need for effective solutions.

**Method:** Synthetic Embedding augmented safety Alignment (SEA) optimizes embeddings through gradient updates to augment textual datasets, facilitating multimodal safety alignment with only textual data available.

**Key Contributions:**

	1. Introduction of Synthetic Embedding augmented safety Alignment (SEA) for MLLM security alignment using only textual data.
	2. Demonstration of SEA's effectiveness in creating high-quality embeddings for multimodal data quickly and efficiently.
	3. Launch of VA-SafetyBench benchmark to evaluate security risks in MLLMs from video and audio modalities.

**Result:** Experiments show that SEA can generate high-quality embeddings for additional modalities on an RTX3090 GPU in 24 seconds, significantly improving MLLM security against threats from extra modalities such as video and audio.

**Limitations:** 

**Conclusion:** SEA provides an effective solution for aligning safety in MLLMs while keeping costs low, and a new benchmark, VA-SafetyBench, was introduced to assess the security risks of additional modalities.

**Abstract:** Multimodal Large Language Models (MLLMs) have serious security vulnerabilities.While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at https://github.com/ZeroNLP/SEA.

</details>


### [205] [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/abs/2502.13195)

*Leonie Weissweiler, Kyle Mahowald, Adele Goldberg*

**Main category:** cs.CL

**Keywords:** language models, natural language processing, grammaticality, semantic parsing, evaluation benchmarks

**Relevance Score:** 8

**TL;DR:** This paper argues that the failures of language models (LMs) to adhere to symbolic linguistic rules may highlight the flexibility of natural language rather than deficiencies in the models. It encourages a re-evaluation of benchmarks and analyses to better capture the complexities of natural language use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current linguistic evaluations assume that natural languages are generated by strict symbolic rules and that understanding is purely compositional. This paper challenges that perspective by suggesting that the flexibility and context-dependence of language should be recognized in evaluations.

**Method:** The paper analyzes the limitations of current linguistic frameworks used to evaluate LMs and provides a theoretical perspective on natural language as a flexible system rather than a rigid rule-based system.

**Key Contributions:**

	1. Presents a new perspective on LMs' performance in relation to natural language
	2. Calls for revised benchmarks that reflect the flexible nature of natural languages
	3. Challenges the notion that grammaticality is solely rule-based.

**Result:** The authors demonstrate that LMs' grammatical failures are indicative of their alignment with natural language use, which is not strictly rule-based. This prompts a reconsideration of how linguistic capabilities in LMs are assessed.

**Limitations:** 

**Conclusion:** The study concludes that LMs should be evaluated based on their ability to handle flexible and context-dependent language use, advocating for new benchmarks that reflect these characteristics.

**Abstract:** Linguistic evaluations of how well LMs generalize to produce or understand novel text often implicitly take for granted that natural languages are generated by symbolic rules. Grammaticality is thought to be determined by whether sentences obey such rules. Interpretation is believed to be compositionally generated by syntactic rules operating on meaningful words. Semantic parsing is intended to map sentences into formal logic. Failures of LMs to obey strict rules have been taken to reveal that LMs do not produce or understand language like humans. Here we suggest that LMs' failures to obey symbolic rules may be a feature rather than a bug, because natural languages are not based on rules. New utterances are produced and understood by a combination of flexible, interrelated, and context-dependent constructions. We encourage researchers to reimagine appropriate benchmarks and analyses that acknowledge the rich, flexible generalizations that comprise natural languages.

</details>


### [206] [FineEdit: Unlock Instruction-Based Text Editing for LLMs](https://arxiv.org/abs/2502.13358)

*Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, text editing, benchmark dataset, FineEdit, natural language processing

**Relevance Score:** 9

**TL;DR:** Introducing InstrEditBench and FineEdit, a model designed to enhance the precision of text editing in various domains using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs struggle with precise, instruction-driven text edits in specialized domains, prompting the need for improved tools.

**Method:** We introduce a benchmark dataset, InstrEditBench, with 30,000 structured editing tasks and develop FineEdit, a specialized model for accurate text modifications.

**Key Contributions:**

	1. Introduction of InstrEditBench benchmark dataset
	2. Development of FineEdit model for precise text editing
	3. Significant performance improvements over existing models

**Result:** FineEdit outperforms leading models with improvements of 10% over Gemini, 30% over Llama-3.2-3B, and over 40% compared to Mistral-7B-OpenOrca in editing tasks.

**Limitations:** 

**Conclusion:** FineEdit shows effective generalization to multi-turn editing scenarios, proving its practical applicability.

**Abstract:** Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating strong capabilities in tasks such as text generation, summarization, and reasoning. Recently, their potential for automating precise text editing tasks across specialized domains, such as programming code, LaTeX, and structured database languages, has gained attention. However, current state-of-the-art LLMs still struggle with executing precise, instruction-driven edits, particularly when structural accuracy and strict adherence to domain conventions are required. To address these challenges, we introduce InstrEditBench, an automated benchmark dataset comprising over 30,000 structured editing tasks spanning diverse domains, including Wikipedia articles, LaTeX documents, source code, and database languages. Using this benchmark, we develop FineEdit, a specialized editing model explicitly trained for accurate, context-aware text modifications. Experimental evaluations demonstrate that FineEdit outperforms state-of-the-art models, achieving improvements of approximately 10% over Gemini models on single-turn edits, up to 30% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over 40% on direct editing tasks. FineEdit also effectively generalizes to realistic multi-turn editing scenarios, highlighting its practical applicability.

</details>


### [207] [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/abs/2502.13369)

*Aditya Sharma, Luis Lara, Christopher J. Pal, Amal Zouaq*

**Main category:** cs.CL

**Keywords:** SPARQL, knowledge graphs, large language models, memory retrieval, information retrieval

**Relevance Score:** 9

**TL;DR:** Introduction of PGMR, a framework to enhance SPARQL query generation by retrieving knowledge graph elements using a non-parametric memory module.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the reliability of SPARQL query generation from natural language to reduce hallucinations and inaccuracies in retrieved data from knowledge graphs.

**Method:** A modular framework, PGMR, integrates a non-parametric memory module to retrieve knowledge graph elements for SPARQL query generation.

**Key Contributions:**

	1. Introduction of a novel framework (PGMR) for SPARQL query generation.
	2. Demonstrated reduction in URI hallucinations compared to previous models.
	3. Robust performance across a variety of datasets and distributions.

**Result:** PGMR consistently outperforms existing methods across diverse datasets and significantly reduces URI hallucinations, improving the accuracy of LLM-based SPARQL queries.

**Limitations:** 

**Conclusion:** PGMR demonstrates a strong potential to enhance LLM applications in information retrieval tasks by effectively mitigating the inaccuracies in generated SPARQL queries.

**Abstract:** The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.

</details>


### [208] [UniKnow: A Unified Framework for Reliable Language Model Behavior across Parametric and External Knowledge](https://arxiv.org/abs/2502.13648)

*Youna Kim, Hyuhng Joon Kim, Minjoon Choi, Sungmin Cho, Hyunsoo Cho, Sang-goo Lee, Taeuk Kim*

**Main category:** cs.CL

**Keywords:** language models, knowledge integration, reliability, evaluations, UniKnow

**Relevance Score:** 8

**TL;DR:** UniKnow is a framework that enhances language model reliability by evaluating knowledge scenarios that impact performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of integrating external knowledge with language models while ensuring reliable utilization across varied knowledge scenarios.

**Method:** The authors introduce the UniKnow framework, which allows controlled evaluation across various knowledge scenarios and propose UniKnow-Aware methods for comprehensive assessment.

**Key Contributions:**

	1. Introduction of the UniKnow framework for knowledge integration in language models
	2. Proposal of UniKnow-Aware methods for robust evaluation
	3. Identification of existing methods' limitations in generalizing across knowledge scenarios.

**Result:** Experiments demonstrate that existing methods often fail to generalize across different knowledge configurations and show biases specific to certain scenarios.

**Limitations:** The framework is under-review and may have limitations in terms of applicability to real-world knowledge sources.

**Conclusion:** UniKnow lays the groundwork for systematic exploration and improvement of language model reliability in diverse knowledge integration situations.

**Abstract:** Language models often benefit from external knowledge beyond parametric knowledge. While this combination enhances performance, achieving reliable knowledge utilization remains challenging, as it requires assessing the state of each knowledge source based on the presence of relevant information. Yet, prior work on knowledge integration often overlooks this challenge by assuming ideal conditions and provides limited coverage of knowledge scenarios. To address this gap, we introduce UniKnow, a Unified framework for reliable LM behavior across parametric and external Knowledge. UniKnow enables controlled evaluation across knowledge scenarios such as knowledge conflict, distraction, and absence conditions that are rarely addressed together. Beyond evaluating existing methods under this setting, we extend our work by introducing UniKnow-Aware methods to support comprehensive evaluation. Experiments on UniKnow reveal that existing methods struggle to generalize across a broader range of knowledge configurations and exhibit scenario-specific biases. UniKnow thus provides a foundation for systematically exploring and improving reliability under knowledge scenarios.

</details>


### [209] [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/abs/2502.14791)

*Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake*

**Main category:** cs.CL

**Keywords:** few-shot learning, language models, NLP, word learning

**Relevance Score:** 8

**TL;DR:** This study introduces a method called Minnow that enhances language models' ability to learn new words through few-shot examples by generating new usage examples based on context.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The abilities of current language models for few-shot word learning and methods for enhancing these abilities are underexplored.

**Method:** Introduction of a new training method called Meta-training for IN-context learNing Of Words (Minnow), which uses placeholder tokens to represent new words and develops general word-learning abilities across many new words.

**Key Contributions:**

	1. Introduction of the Minnow method for few-shot word learning.
	2. Demonstration of comparability to larger pre-trained LLMs in word learning tasks.
	3. Evidence of improved data efficiency and performance in language models.

**Result:** Models trained with Minnow on child-directed language demonstrate strong few-shot word learning capabilities, showing performance comparable to larger pre-trained language models.

**Limitations:** 

**Conclusion:** Minnow improves the data efficiency and performance of language models in word learning tasks, allowing them to better discriminate and generate usage for new words.

**Abstract:** Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.

</details>


### [210] [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/abs/2502.15975)

*Jesus Rios, Pierre Dognin, Ronny Luss, Karthikeyan N. Ramamurthy*

**Main category:** cs.CL

**Keywords:** fine-tuning, large language models, parameter efficiency, machine learning, artificial intelligence

**Relevance Score:** 8

**TL;DR:** This paper proposes a new method for fine-tuning large language models that reduces the number of trainable parameters compared to existing methods like LoRA and full fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As large language models have increased in size, full fine-tuning has become too resource-intensive, necessitating alternative approaches that require fewer computational resources.

**Method:** The authors introduce a method that randomly selects a small proportion of the model parameters for training, as opposed to fine-tuning all parameters or only low-rank adaptations.

**Key Contributions:**

	1. Proposes a novel method for parameter-efficient fine-tuning by random selection of trainable parameters.
	2. Compares new method against existing techniques including LoRA and full fine-tuning.
	3. Demonstrates efficiency and performance trade-offs.

**Result:** The proposed method is compared with traditional PEFT methods and full fine-tuning, showcasing its efficiency and performance benefits.

**Limitations:** 

**Conclusion:** The new approach demonstrates a viable alternative to existing fine-tuning methods while maintaining performance, suggesting a path forward for more efficient language model training.

**Abstract:** Full fine-tuning of large language models for alignment and task adaptation has become prohibitively expensive as models have grown in size. Parameter-Efficient Fine-Tuning (PEFT) methods aim at significantly reducing the computational and memory resources needed for fine-tuning these models by only training on a small number of parameters instead of all model parameters. Currently, the most popular PEFT method is the Low-Rank Adaptation (LoRA), which freezes the parameters of the model to be fine-tuned and introduces a small set of trainable parameters in the form of low-rank matrices. We propose simply reducing the number of trainable parameters by randomly selecting a small proportion of the model parameters to train on. In this paper, we compare the efficiency and performance of our proposed approach with PEFT methods, including LoRA, as well as full parameter fine-tuning.

</details>


### [211] [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)

*Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Lidong Bing, Xiaoli Li, Roy Ka-wei Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Alignment, Direct Preference Optimization, Data Generation, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents a scalable method to improve preference data construction for aligning large language models using Direct Preference Optimization (DPO), revealing that traditional strategies may decline in performance with larger sample sizes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to enhance the alignment performance of large language models through improved data generation and model retraining processes.

**Method:** The authors investigate preference data construction by analyzing the underlying normal distribution of sample rewards and exploring pairwise combinations of reward positions.

**Key Contributions:**

	1. Proposing a new strategy to select rejected responses for DPO based on reward distribution
	2. Systematic exploration of pairwise combinations of reward points
	3. Demonstration of improved alignment performance with increased sample size

**Result:** The study finds that selecting the rejected response at reward position μ - 2σ rather than the traditional minimum reward significantly enhances model performance.

**Limitations:** The study may rely on the specific contexts of the models tested and their corresponding data distributions.

**Conclusion:** A scalable preference data construction strategy is introduced that leads to consistent improvements in model performance as sample size increases.

**Abstract:** Iterative data generation and model retraining are widely used to align large language models (LLMs). It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. In this work, we aim to \emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. However, our experiments reveal that this strategy leads to a \emph{decline} in performance as the sample size increases. To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\mu - 2\sigma$ rather than the minimum reward, is crucial for optimal performance. We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.

</details>


### [212] [Spontaneous Giving and Calculated Greed in Language Models](https://arxiv.org/abs/2502.17720)

*Yuxuan Li, Hirokazu Shirado*

**Main category:** cs.CL

**Keywords:** large language models, social intelligence, cooperation, economic games, reasoning models

**Relevance Score:** 7

**TL;DR:** This paper investigates whether large language models possess social intelligence by applying reasoning techniques in economic games. Results indicate that reasoning models reduce cooperation and norm enforcement, highlighting a need for improved LLM architectures that integrate social intelligence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess if large language models can make effective decisions in cooperative contexts through reasoning strategies.

**Method:** The study uses economic games that simulate social dilemmas, specifically applying chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game, and evaluates various off-the-shelf models in six cooperation and punishment games.

**Key Contributions:**

	1. Examined the social decision-making capabilities of large language models
	2. Demonstrated the negative impact of reasoning models on cooperation in economic games
	3. Highlighted the need for LLM architecture that includes social intelligence

**Result:** Reasoning models showed decreased cooperation and norm enforcement, favoring individual rationality. Groups with more reasoning agents had lower collective gains in repeated interactions.

**Limitations:** Focus on specific economic game simulations may not fully capture the complexities of real-world social interactions.

**Conclusion:** The findings suggest a necessity for LLMs to develop social intelligence capabilities to better address collective action challenges.

**Abstract:** Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of "spontaneous giving and calculated greed." Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.

</details>


### [213] [Stay Focused: Problem Drift in Multi-Agent Debate](https://arxiv.org/abs/2502.19559)

*Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp*

**Main category:** cs.CL

**Keywords:** multi-agent debate, problem drift, LLM, task performance, reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates the problem of 'problem drift' in multi-agent debates among large language models and proposes methods to identify and mitigate it.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why multi-agent debates among large language models struggle with complex problems requiring longer reasoning chains, specifically identifying and addressing instances of 'problem drift' that hinder task performance.

**Method:** The study analyzes multi-agent discussions across ten tasks and quantifies the prevalence of problem drift. It employs human experts to examine 170 discussions suffering from this issue. The paper introduces DRIFTJudge to detect problem drift and DRIFTPolicy to mitigate its effects.

**Key Contributions:**

	1. Definition and quantification of 'problem drift' in multi-agent debates
	2. Introduction of DRIFTJudge for detecting problem drift
	3. Development of DRIFTPolicy for mitigating problem drift and improving performance

**Result:** The analysis identifies common issues leading to problem drift, such as lack of progress, low-quality feedback, and lack of clarity. The proposed methods aim to improve performance in multi-agent debate scenarios by addressing these issues.

**Limitations:** The study may be limited by the specific tasks chosen for analysis and the potential variability in human expert evaluations.

**Conclusion:** Understanding problem drift is crucial for enhancing the effectiveness of multi-agent debates, and the proposed methods represent progress toward improving task performance in this context.

**Abstract:** Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations when solving complex problems that require longer reasoning chains. We analyze how multi-agent debate over multiple turns drifts away from the initial problem, thus harming task performance. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, eight human experts analyze 170 multi-agent discussions suffering from problem drift. We find the most common issues related to this drift are the lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To address problem drift, we propose DRIFTJudge, an LLM-as-a-judge method, to detect problem drift at test-time. We also propose DRIFTPolicy, a method that mitigates problem drift cases to improve task performance. Our study is a step toward understanding a key limitation of multi-agent debate, highlighting why longer debates can harm task performance and how problem drift could be addressed.

</details>


### [214] [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)

*Hannah Cyberey, Yangfeng Ji, David Evans*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, representation engineering

**Relevance Score:** 9

**TL;DR:** This paper explores how gender concepts are represented in large language models (LLMs) and proposes a new method for mitigating gender bias using representation engineering techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address and mitigate the biases, particularly gender bias, that are inherent in large language models, by analyzing the representation of gender within these models.

**Method:** The paper introduces a method for extracting concept representations using probability weighting without labeled data. It includes a projection-based approach for precise steering of model predictions.

**Key Contributions:**

	1. Introduced a method for extracting concept representations from LLMs without labeled data.
	2. Developed a projection-based method for steering model predictions to mitigate gender bias.
	3. Provided practical code for implementing the proposed techniques.

**Result:** The proposed methods are effective in measuring and manipulating the representation of gender in LLMs, leading to a significant reduction in gender bias.

**Limitations:** 

**Conclusion:** By utilizing representation engineering techniques, we can better understand and mitigate biases in LLMs, particularly gender bias, thereby improving their fairness and equity in applications.

**Abstract:** Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs. Our code is available at: https://github.com/hannahxchen/gender-bias-steering

</details>


### [215] [Adaptively profiling models with task elicitation](https://arxiv.org/abs/2503.01986)

*Davis Brown, Prithvi Balehannina, Helen Jin, Shreya Havaldar, Hamed Hassani, Eric Wong*

**Main category:** cs.CL

**Keywords:** language models, evaluation, task elicitation, systematic failures, benchmarks

**Relevance Score:** 8

**TL;DR:** This paper introduces task elicitation, a method for automatically generating evaluations that uncover systematic failure modes in language models, identifying numerous tasks where these models underperform.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies in current evaluations of language models that fail to highlight critical failure modes, leading experts to manually inspect outputs and create new benchmarks.

**Method:** Task elicitation automatically generates hundreds of natural-language tasks for evaluation, enabling the profiling of model behavior across various domains.

**Key Contributions:**

	1. Introduction of task elicitation as a new evaluation method
	2. Discovery of hundreds of new natural-language tasks
	3. Profiling of model behavior and systematic failures across various domains

**Result:** Identification of numerous systematic failures in frontier language models over diverse tasks, including instances of inappropriate associations and hallucinations during repeated context fabrications.

**Limitations:** Does not address all potential evaluation methods or failures, focusing specifically on systematic failure characterization.

**Conclusion:** Task elicitation provides a robust framework for uncovering and understanding language model failures, paving the way for improved evaluations and benchmarks.

**Abstract:** Language model evaluations often fail to characterize consequential failure modes, forcing experts to inspect outputs and build new benchmarks. We introduce task elicitation, a method that automatically builds new evaluations to profile model behavior. Task elicitation finds hundreds of natural-language tasks -- an order of magnitude more than prior work -- where frontier models exhibit systematic failures, in domains ranging from forecasting to online harassment. For example, we find that Sonnet 3.5 over-associates quantum computing and AGI and that o3-mini is prone to hallucination when fabrications are repeated in-context.

</details>


### [216] [Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations](https://arxiv.org/abs/2503.03062)

*Zhengyao Gu, Henry Peng Zou, Yankai Chen, Aiwei Liu, Weizhi Zhang, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** in-context learning, self-generated annotations, semi-supervised learning, many-shot learning, classification tasks

**Relevance Score:** 7

**TL;DR:** This paper explores self-generated annotations for in-context learning, proposing an effective framework and a novel method that enhances performance in various shot scenarios, particularly in many-shot settings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of high costs associated with obtaining high-quality annotated data for in-context learning.

**Method:** The authors propose a framework analogous to semi-supervised learning, incorporating annotation generation, demonstration selection, and in-context inference, along with an iterative annotation method called IterPSD.

**Key Contributions:**

	1. Development of a semi-supervised learning framework for in-context learning with self-generated annotations
	2. Introduction of a baseline that surpasses ground-truth ICL in multiple settings
	3. Proposal of IterPSD for improved many-shot learning with pseudo-labeling techniques

**Result:** The proposed baseline outperforms ground-truth ICL across zero-shot, few-shot, and many-shot settings, with optimal performance observed at over 1,000 demonstrations.

**Limitations:** 

**Conclusion:** The introduction of IterPSD, which combines iterative refinement and curriculum pseudo-labeling, enhances classification task performance by up to 6.8%.

**Abstract:** The high cost of obtaining high-quality annotated data for in-context learning (ICL) has motivated the development of methods that use self-generated annotations in place of ground-truth labels. While these approaches have shown promising results in few-shot settings, they generally do not scale to many-shot scenarios. In this work, we study ICL with self-generated examples using a framework analogous to traditional semi-supervised learning, consisting of annotation generation, demonstration selection, and in-context inference. Within this framework, we propose a simple baseline that outperforms ground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we observe a scaling law with this baseline, where optimal performance is achieved with more than 1,000 demonstrations. To fully exploit the many-shot capabilities of semi-supervised ICL, we introduce IterPSD, an iterative annotation approach that integrates iterative refinement and curriculum pseudo-labeling techniques from semi-supervised learning, yielding up to 6.8% additional gains on classification tasks.

</details>


### [217] [DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/abs/2503.04959)

*Haoyuan Ma, Yongliang Shen, Hengwei Liu, Wenqi Zhang, Haolei Xu, Qiuying Peng, Jun Wang, Weiming Lu*

**Main category:** cs.CL

**Keywords:** text-to-SQL, large language models, database understanding, instruction synthesis, GPT-4

**Relevance Score:** 8

**TL;DR:** DB-Explore is a novel framework that enhances text-to-SQL systems by improving their understanding of complex database structures through automated exploration and instruction synthesis.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of text-to-SQL systems that struggle with complex database structures and domain-specific queries by enhancing database comprehension alongside SQL syntax.

**Method:** DB-Explore constructs database graphs, leverages GPT-4 to mine structural patterns and semantic knowledge, and synthesizes instructions for fine-tuning LLMs.

**Key Contributions:**

	1. Proposed DB-Explore framework that aligns LLMs with database knowledge
	2. Utilizes automated exploration and instruction synthesis for database understanding
	3. Achieves state-of-the-art performance on SPIDER and BIRD benchmarks

**Result:** DB-Explore achieved state-of-the-art results with execution accuracies of 67.0% on BIRD and 87.8% on SPIDER benchmarks while minimizing computational costs.

**Limitations:** 

**Conclusion:** The proposed framework effectively bridges the gap between language models and database knowledge, enabling better translation of natural language queries into SQL.

**Abstract:** Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematically mine structural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably, our open-source implementation based on Qwen2.5-Coder-7B achieves state-of-the-art results at minimal computational cost, outperforming several GPT-4-driven Text-to-SQL systems.

</details>


### [218] [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)

*Simon A. Aytes, Jinheon Baek, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, Chain-of-Thought, prompting framework, token reduction

**Relevance Score:** 9

**TL;DR:** This paper introduces Sketch-of-Thought (SoT), a new prompting framework for large language models that reduces verbosity and token usage while maintaining reasoning accuracy, achieving significant token reductions across various reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the efficiency of reasoning capabilities in large language models by reducing verbosity in outputs, thereby lowering computational overhead.

**Method:** The authors propose the Sketch-of-Thought (SoT) framework, which integrates cognitively inspired reasoning paradigms with linguistic constraints. It consists of three instantiations - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - that are dynamically selected at test-time using a lightweight routing model.

**Key Contributions:**

	1. Introduction of the Sketch-of-Thought (SoT) prompting framework.
	2. Demonstrated significant token reductions (up to 78%) in reasoning tasks.
	3. Improved accuracy in select reasoning tasks while reducing verbosity.

**Result:** SoT demonstrates up to 78% token reduction across 15 reasoning datasets with minimal loss in accuracy, and in some cases, it even improves the accuracy of tasks such as mathematical and multi-hop reasoning while providing shorter outputs.

**Limitations:** 

**Conclusion:** The Sketch-of-Thought framework offers a modular and flexible approach that enhances the reasoning process in large language models by significantly reducing output verbosity without sacrificing accuracy.

**Abstract:** Recent advances in large language models (LLMs) have enabled strong reasoning capabilities through Chain-of-Thought (CoT) prompting, which elicits step-by-step problem solving, but often at the cost of excessive verbosity in intermediate outputs, leading to increased computational overhead. We propose Sketch-of-Thought (SoT), a prompting framework that integrates cognitively inspired reasoning paradigms with linguistic constraints to reduce token usage while preserving reasoning accuracy. SoT is designed as a flexible, modular approach and is instantiated with three paradigms--Conceptual Chaining, Chunked Symbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and selected dynamically at test-time by a lightweight routing model. Across 15 reasoning datasets spanning multiple domains, languages, and modalities, SoT achieves token reductions of up to 78% with minimal accuracy loss. In tasks such as mathematical and multi-hop reasoning, it even improves accuracy while shortening outputs.

</details>


### [219] [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/abs/2503.06072)

*Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, Zhenhan Dai, Yifeng Xie, Yihan Cao, Lichao Sun, Pan Zhou, Lifang He, Hechang Chen, Yu Zhang, Qingsong Wen, Tianming Liu, Neil Zhenqiang Gong, Jiliang Tang, Caiming Xiong, Heng Ji, Philip S. Yu, Jianfeng Gao*

**Main category:** cs.CL

**Keywords:** Large Language Models, post-training language models, reasoning, ethical alignment, domain adaptability

**Relevance Score:** 9

**TL;DR:** The paper surveys the evolution of post-training language models (PoLMs) that enhance the efficacy of Large Language Models (LLMs) by addressing their limitations in reasoning, ethics, efficiency, and domain-specific performance.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of Large Language Models (LLMs) in specialized contexts and enhance their capabilities through advanced post-training methodologies.

**Method:** The paper reviews five core paradigms for PoLMs: Fine-tuning, Alignment, Reasoning, Efficiency, and Integration and Adaptation, tracing their development and contributions to language model evolution.

**Key Contributions:**

	1. Pioneering synthesis of PoLM evolution
	2. Structured taxonomy of techniques and datasets
	3. Strategic agenda for leveraging LRMs in research

**Result:** It outlines the advancements in PoLMs, showcasing improvements in reasoning capabilities, ethical alignment, and efficiency through systematic categorization of techniques and datasets.

**Limitations:** 

**Conclusion:** The paper establishes a comprehensive framework for understanding PoLMs and sets an agenda for future research aimed at refining LLMs for better precision and ethical robustness.

**Abstract:** The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.

</details>


### [220] [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)

*Hai Zhao, Hongqiu Wu, Dongjie Yang, Anni Zou, Jiale Hong*

**Main category:** cs.CL

**Keywords:** BriLLM, language model, brain-inspired, interpretability, non-transformer

**Relevance Score:** 6

**TL;DR:** Introduction of BriLLM, a novel brain-inspired large language model distinct from traditional transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a generative language model with improved interpretability and capabilities inspired by human cognitive patterns.

**Method:** The model uses a non-traditional, non-transformer approach based on a directed graph structure where each token represents a node, and signal flow predicts the next token.

**Key Contributions:**

	1. Introduction of a novel non-transformer language model
	2. Enhanced interpretability across all nodes in the model
	3. Support for infinitely long n-gram models through unique signal flow mechanism

**Result:** BriLLM demonstrates language model performance comparable to GPT-1, supporting unique features such as recall activation and multi-modal capabilities, particularly in Chinese with 4000 tokens.

**Limitations:** 

**Conclusion:** The initial version of BriLLM shows promising results, and further enhancements with computing power may unlock more potential features.

**Abstract:** This paper reports the first brain-inspired large language model (BriLLM). This is a non-Transformer, non-GPT, non-traditional machine learning input-output controlled generative language model. The model is based on the Signal Fully-connected flowing (SiFu) definition on the directed graph in terms of the neural network, and has the interpretability of all nodes on the graph of the whole model, instead of the traditional machine learning model that only has limited interpretability at the input and output ends. In the language model scenario, the token is defined as a node in the graph. A randomly shaped or user-defined signal flow flows between nodes on the principle of "least resistance" along paths. The next token or node to be predicted or generated is the target of the signal flow. As a language model, BriLLM theoretically supports infinitely long $n$-gram models when the model size is independent of the input and predicted length of the model. The model's working signal flow provides the possibility of recall activation and innate multi-modal support similar to the cognitive patterns of the human brain. At present, we released the first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node width, 16-token long sequence prediction ability, and language model prediction performance comparable to GPT-1. More computing power will help us explore the infinite possibilities depicted above.

</details>


### [221] [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/abs/2503.15952)

*Chen Li, Nazhou Liu, Kai Yang*

**Main category:** cs.CL

**Keywords:** Group Relative Policy Optimization, Adaptive Group Policy Optimization, Reinforcement Learning

**Relevance Score:** 7

**TL;DR:** AGPO improves training stability and efficiency in reasoning LLMs by revising the objective function used in GRPO.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address deficiencies in GRPO that affect reinforcement learning stability and inference efficiency, particularly zero-variance in advantage estimation.

**Method:** Proposes Adaptive Group Policy Optimization (AGPO) with a revised objective function to reduce training fluctuation and zero advantage.

**Key Contributions:**

	1. Introduces Adaptive Group Policy Optimization (AGPO).
	2. Improves upon GRPO by addressing the zero-variance issue in advantage estimation.
	3. Demonstrates improved stability and performance with fewer tokens.

**Result:** AGPO shows more stable training and better performance while using significantly fewer tokens in reasoning steps compared to GRPO.

**Limitations:** 

**Conclusion:** AGPO is an effective modification to GRPO, enhancing the training stability and efficiency of reasoning LLMs.

**Abstract:** Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has become the core part of training Reasoning LLMs. However, we find some deficiency that influences RL stability and inference efficiency, like zero-variance in advantage estimation. Thus, we propose Adaptive Group Policy Optimization (AGPO) which contains a simple but effective modification: a revised objective function to mitigate training fluctuation and zero advantage. The experiments demonstrate our method achieves more stable training and superior performance with significantly fewer tokens in reasoning steps.

</details>


### [222] [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/abs/2503.18247)

*Tadesse Destaw Belay, Israel Abebe Azime, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Idris Abdulmumin, Abinew Ali Ayele, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

**Main category:** cs.CL

**Keywords:** domain adaptive pre-training, task-adaptive pre-training, African languages, social media, BERT

**Relevance Score:** 5

**TL;DR:** The paper introduces AfriSocial, a corpus for continual pre-training on African languages, and explores domain adaptive pre-training (DAPT) and task-adaptive pre-training (TAPT) methods to enhance NLP models for social media tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bias in language models for low-resource African languages and improve their performance on social media tasks.

**Method:** The authors propose DAPT and TAPT techniques for continually pre-training BERT-based models using the AfriSocial corpus for sentiment analysis, multi-label emotion, and hate speech classification.

**Key Contributions:**

	1. Introduction of AfriSocial corpus for African languages
	2. Demonstrated effectiveness of DAPT and TAPT for low-resource languages
	3. Improvements in sentiment analysis and hate speech classification tasks using adaptive pre-training techniques

**Result:** DAPT improves F1 scores on the tasks by 1% to 30%, while TAPT shows performance improvements from 0.55% to 15.11% for task-specific preprocessing.

**Limitations:** 

**Conclusion:** Combining DAPT and TAPT yields the best performance across multiple tasks in the African languages social media domain.

**Abstract:** Language models built from various sources are the foundation of today's NLP progress. However, for many low-resource languages, the diversity of domains is often limited -- more biased to a religious domain, which impacts their performance when evaluated on distant and rapidly evolving domains such as social media. Domain adaptive pre-training (DAPT) and task-adaptive pre-training (TAPT) are popular techniques to reduce this bias through continual pre-training for BERT-based models, but they have not been explored for African multilingual encoders. In this paper, we explore DAPT and TAPT continual pertaining approaches for the African languages social media domain. We introduce AfriSocial-a large-scale social media and news domain corpus for continual pre-training on several African languages. Leveraging AfriSocial, we show that DAPT consistently improves performance on three subjective tasks: sentiment analysis, multi-label emotion, and hate speech classification, covering 19 languages from 1% to 30% F1 score. Similarly, leveraging TAPT on one task data improves performance on other related tasks. For example, training with unlabeled sentiment data (source) for a fine-grained emotion classification task (target) improves the baseline results by an F1 score ranging from 0.55% to 15.11%. Combining these two methods (i.e. DAPT + TAPT) further improves the overall performance.

</details>


### [223] [A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications](https://arxiv.org/abs/2503.20302)

*Sunayana Sitaram, Adrian de Wynter, Isobel McCrum, Qilong Gu, Si-Qing Chen*

**Main category:** cs.CL

**Keywords:** misgendering, inclusive AI, participatory design, LLM, multilingual

**Relevance Score:** 7

**TL;DR:** This paper addresses misgendering in language and develops guardrails to mitigate it across 42 languages using a participatory-design approach in LLM applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Misgendering undermines a person's identity and can cause significant harm, necessitating effective approaches across diverse languages.

**Method:** The paper employs a participatory-design approach to develop and test methodologies for guardrails in LLM-based applications, focusing on meeting transcript summarization with human-in-the-loop methods.

**Key Contributions:**

	1. Development of guardrails to mitigate misgendering across 42 languages
	2. Implementation of human-in-the-loop methodology in LLM applications
	3. Release of supportive resources for further research on misgendering

**Result:** The proposed guardrails effectively reduced misgendering rates in summaries across 42 languages without compromising quality.

**Limitations:** 

**Conclusion:** The study demonstrates a scalable method for inclusive AI solutions, provides released guardrails and dataset to foster further research in this domain.

**Abstract:** Misgendering is the act of referring to someone by a gender that does not match their chosen identity. It marginalizes and undermines a person's sense of self, causing significant harm. English-based approaches have clear-cut approaches to avoiding misgendering, such as the use of the pronoun ``they''. However, other languages pose unique challenges due to both grammatical and cultural constructs. In this work we develop methodologies to assess and mitigate misgendering across 42 languages and dialects using a participatory-design approach to design effective and appropriate guardrails across all languages. We test these guardrails in a standard LLM-based application (meeting transcript summarization), where both the data generation and the annotation steps followed a human-in-the-loop approach. We find that the proposed guardrails are very effective in reducing misgendering rates across all languages in the summaries generated, and without incurring loss of quality. Our human-in-the-loop approach demonstrates a method to feasibly scale inclusive and responsible AI-based solutions across multiple languages and cultures. We release the guardrails and synthetic dataset encompassing 42 languages, along with human and LLM-judge evaluations, to encourage further research on this subject.

</details>


### [224] [Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2503.24245)

*Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang*

**Main category:** cs.CL

**Keywords:** large language models, knowledge graphs, retrieval-augmented generation, telecommunications, domain-specific performance

**Relevance Score:** 4

**TL;DR:** This paper proposes a novel framework combining knowledge graphs and retrieval-augmented generation to enhance large language models' performance in the telecom domain.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models in domain-specific tasks like telecommunications where specialized knowledge is crucial.

**Method:** The framework integrates knowledge graphs (KG) with retrieval-augmented generation (RAG) techniques to provide structured, domain-specific information during response generation.

**Key Contributions:**

	1. Development of a novel KG-RAG framework for LLMs in telecom
	2. Demonstrated improved accuracy compared to traditional models
	3. Bridged structured knowledge representation with generative capabilities of LLMs

**Result:** The KG-RAG model achieved an accuracy of 88% in answering telecom-related questions, outperforming the RAG-only model (82%) and LLM-only model (48%).

**Limitations:** 

**Conclusion:** The hybrid approach significantly enhances the accuracy and adaptability of LLMs for complex technical queries in the telecommunications sector.

**Abstract:** Large language models (LLMs) have made significant progress in general-purpose natural language processing tasks. However, LLMs are still facing challenges when applied to domain-specific areas like telecommunications, which demands specialized expertise and adaptability to evolving standards. This paper presents a novel framework that combines knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to enhance LLM performance in the telecom domain. The framework leverages a KG to capture structured, domain-specific information about network protocols, standards, and other telecom-related entities, comprehensively representing their relationships. By integrating KG with RAG, LLMs can dynamically access and utilize the most relevant and up-to-date knowledge during response generation. This hybrid approach bridges the gap between structured knowledge representation and the generative capabilities of LLMs, significantly enhancing accuracy, adaptability, and domain-specific comprehension. Our results demonstrate the effectiveness of the KG-RAG framework in addressing complex technical queries with precision. The proposed KG-RAG model attained an accuracy of 88% for question answering tasks on a frequently used telecom-specific dataset, compared to 82% for the RAG-only and 48% for the LLM-only approaches.

</details>


### [225] [GLiNER-BioMed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition](https://arxiv.org/abs/2504.00676)

*Anthony Yazdani, Ihor Stepanov, Douglas Teodoro*

**Main category:** cs.CL

**Keywords:** Biomedical NER, GLiNER, Natural Language Processing, Zero-shot Learning, Synthetic Data

**Relevance Score:** 8

**TL;DR:** GLiNER-BioMed improves biomedical named entity recognition by leveraging natural language labels for zero-shot recognition and developing efficient models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Biomedical NER faces unique challenges such as specialized vocabularies and novel entities, prompting the need for more adaptable models.

**Method:** Introduces GLiNER-BioMed, a suite of models using natural language labels for entity detection, capable of zero-shot recognition and trained on high-coverage synthetic data from LLMs.

**Key Contributions:**

	1. Development of GLiNER-BioMed for biomedical NER
	2. Use of natural language labels for zero-shot entity recognition
	3. Public availability of datasets, models, and training pipelines

**Result:** GLiNER-BioMed significantly outperforms state-of-the-art methods, with a 5.96% F1-score improvement over the strongest baseline in both zero- and few-shot scenarios.

**Limitations:** 

**Conclusion:** The combination of synthetic pre-training and fine-tuning enhances recognition performance, with all resources publicly available.

**Abstract:** Biomedical named entity recognition (NER) presents unique challenges due to specialized vocabularies, the sheer volume of entities, and the continuous emergence of novel entities. Traditional NER models, constrained by fixed taxonomies and human annotations, struggle to generalize beyond predefined entity types. To address these issues, we introduce GLiNER-BioMed, a domain-adapted suite of Generalist and Lightweight Model for NER (GLiNER) models specifically tailored for biomedicine. In contrast to conventional approaches, GLiNER uses natural language labels to infer arbitrary entity types, enabling zero-shot recognition. Our approach first distills the annotation capabilities of large language models (LLMs) into a smaller, more efficient model, enabling the generation of high-coverage synthetic biomedical NER data. We subsequently train two GLiNER architectures, uni- and bi-encoder, at multiple scales to balance computational efficiency and recognition performance. Experiments on several biomedical datasets demonstrate that GLiNER-BioMed outperforms the state-of-the-art in both zero- and few-shot scenarios, achieving 5.96% improvement in F1-score over the strongest baseline (p-value < 0.001). Ablation studies highlight the effectiveness of our synthetic data generation strategy and emphasize the complementary benefits of synthetic biomedical pre-training combined with fine-tuning on general-domain annotations. All datasets, models, and training pipelines are publicly available at https://github.com/ds4dh/GLiNER-biomed.

</details>


### [226] [Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/abs/2504.03234)

*Junjie Yang, Ke Lin, Xing Yu*

**Main category:** cs.CL

**Keywords:** Chain of Thought, language models, reasoning efficiency

**Relevance Score:** 7

**TL;DR:** The paper introduces a method to enhance reasoning efficiency in language models by rewarding quality and length, optimizing performance on varying problem complexities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in Chain of Thought reasoning where language models overthink simple problems, resulting in suboptimal performance.

**Method:** The proposed approach constructs rewards based on both length and quality comparisons, leading to improved solution correctness and conciseness.

**Key Contributions:**

	1. Introduces a dual-reward system based on reasoning length and quality.
	2. Demonstrates effectiveness on fuzzy tasks without ground truth.
	3. Shows significant reductions in explanation length while maintaining accuracy.

**Result:** Experiments show that the method maintains accuracy while producing significantly more concise explanations across various reasoning benchmarks.

**Limitations:** The method may not generalize well to all types of reasoning tasks or models not aligned with its assumptions.

**Conclusion:** The approach effectively teaches models to optimize their reasoning process, enhancing performance on tasks with varying levels of complexity.

**Abstract:** Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient "overthinking" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to "think when needed."

</details>


### [227] [Thinking Out Loud: Do Reasoning Models Know When They're Right?](https://arxiv.org/abs/2504.06564)

*Qingcheng Zeng, Weihao Xuan, Leyang Cui, Rob Voigt*

**Main category:** cs.CL

**Keywords:** Large reasoning models, self-reflection, verbalized confidence, reasoning chains, model faithfulness

**Relevance Score:** 8

**TL;DR:** The paper explores the interaction between verbalized confidence and self-reflection in large reasoning models (LRMs), suggesting that while training can enhance performance, it may lead to reduced awareness of knowledge boundaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how self-reflection in LRMs interacts with their verbalized confidence and reasoning capabilities.

**Method:** The study analyzes how different training approaches, such as supervised fine-tuning and reinforcement learning, impact LRM's vocal confidence and reasoning patterns.

**Key Contributions:**

	1. Analysis of verbalized confidence in LRMs.
	2. Connection between reasoning training and confidence calibration.
	3. Discovery of a 'reasoning tax' in model performance.

**Result:** Training improves verbalized calibration, but models show a lower ability to recognize their knowledge boundaries, indicated by fewer 'I don't know' responses. Increased confidence is associated with shorter reasoning chains.

**Limitations:** The investigation is noted as a work in progress, limiting the comprehensiveness of the findings.

**Conclusion:** The research underscores a 'reasoning tax' whereby models may overstate confidence despite lacking accurate self-awareness, impacting model faithfulness.

**Abstract:** Large reasoning models (LRMs) have recently demonstrated impressive capabilities in complex reasoning tasks by leveraging increased test-time computation and exhibiting behaviors reminiscent of human-like self-reflection. While LRMs show a clear capacity for valuable self-reflection, how this ability interacts with other model behaviors remains underexplored. We investigate this connection by analyzing verbalized confidence, how models articulate their certainty, as a lens into the nature of self-reflection in LRMs. We find that supervised fine-tuning on reasoning traces (i.e., distillation) and reinforcement learning can improve verbalized calibration in reasoning-intensive settings in a progressive, laddered fashion. However, our results also indicate that reasoning models may possess a diminished awareness of their own knowledge boundaries, as evidenced by significantly lower "I don't know" response rates on factuality benchmarks. Moreover, we examine the relationship between verbalized confidence and reasoning chains, finding that models tend to express higher confidence when providing shorter or less elaborate reasoning. Our findings highlight how reasoning-oriented training can enhance performance in reasoning-centric tasks while potentially incurring a "reasoning tax," a cost reflected in the model's reduced ability to accurately recognize the limits of its own knowledge in small-scale models. More broadly, our work showcases how this erosion of knowledge boundaries can compromise model faithfulness, as models grow more confident without a commensurate understanding of when they should abstain.

</details>


### [228] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)

*Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Repetitive Text Generation, Mechanistic Interpretability, Sparse Autoencoders, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper investigates the causes of repetitive text generation in large language models (LLMs) and introduces a novel approach, 'Duplicatus Charm', to identify and mitigate this issue through mechanistic interpretability and Sparse Autoencoders (SAEs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the prevalent problem of repetitive text generation in LLMs, referred to as the 'Repeat Curse', and to understand the underlying mechanisms contributing to this phenomenon.

**Method:** The authors analyze model activations to identify 'Repetition Features' responsible for repetitive outputs, using logit analysis to locate involved layers and SAE-based manipulation to extract and stimulate these features.

**Key Contributions:**

	1. Introduction of 'Duplicatus Charm' for analyzing repetition in LLMs
	2. Identification of key 'Repetition Features' in model activations
	3. Validation of the approach through a constructed repetition dataset.

**Result:** The approach successfully identifies key features that lead to repetition and provides a way to reduce repetitive outputs by deactivating these features, validated through a constructed repetition dataset and evaluation pipeline.

**Limitations:** 

**Conclusion:** By addressing the root causes of the Repeat Curse, the proposed method offers a systematic way to analyze and mitigate repetition in LLMs, improving text generation quality.

**Abstract:** Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the "Repeat Curse". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach, "Duplicatus Charm", to induce and analyze the Repeat Curse. Our method systematically identifies "Repetition Features" -the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse.

</details>


### [229] [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)

*Yuxin Jiang, Yufei Wang, Chuhan Wu, Xinyi Dai, Yan Xu, Weinan Gan, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang*

**Main category:** cs.CL

**Keywords:** large language models, instruction-following, data synthesis, web documents, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper presents Web Reconstruction (WebR), a framework for generating high-quality instruction-following data from raw web documents, resulting in significant performance improvements over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current automatic data synthesis methods for instruction-following capabilities in LLMs, which rely on high-quality seed data or specific assumptions about web content.

**Method:** WebR synthesizes instruction-tuning data using a dual-perspective approach, categorizing web documents as either instructions or responses to facilitate the data reconstruction process.

**Key Contributions:**

	1. Introduction of the Web Reconstruction framework for instruction-tuning data synthesis
	2. Demonstration of significant performance uplift over existing methods
	3. Novel dual-perspective data generation process using raw web documents

**Result:** Datasets generated by WebR outperform state-of-the-art methods by up to 16.65% across four instruction-following benchmarks, showcasing improved compatibility, data efficiency, and scalability.

**Limitations:** 

**Conclusion:** WebR enhances the process of generating instruction-following data with minimal assumptions, promoting better domain adaptation and offering publicly available datasets and code.

**Abstract:** The improvement of LLMs' instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm--Web as Instruction and Web as Response--where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at https://github.com/YJiangcm/WebR.

</details>


### [230] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)

*Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger*

**Main category:** cs.CL

**Keywords:** persuasion, large language model, human-computer interaction, AI governance, real-time quiz

**Relevance Score:** 8

**TL;DR:** The study compares the persuasive abilities of a large language model (LLM) to human persuaders in a quiz setting, finding that the LLM significantly outperformed humans in achieving compliance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To examine and compare the persuasion capabilities of a large language model against human persuaders in an interactive setting.

**Method:** A preregistered large-scale experiment where quiz takers were persuaded by either LLMs or human participants in real time during an online quiz.

**Key Contributions:**

	1. Comparison of LLM and human persuaders in a real-time interaction
	2. Demonstration of superior LLM persuasion capabilities
	3. Implications for governance frameworks in AI persuasion

**Result:** LLM persuaders achieved higher compliance rates than human persuaders, improving quiz takers' accuracy and earnings when steering toward correct answers, while conversely decreasing accuracy and earnings when steering toward incorrect answers.

**Limitations:** 

**Conclusion:** AI's persuasion capabilities surpass those of incentivized human persuaders, highlighting the need for alignment and governance frameworks to address this issue.

**Abstract:** We directly compare the persuasion capabilities of a frontier large language model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an interactive, real-time conversational quiz setting. In this preregistered, large-scale incentivized experiment, participants (quiz takers) completed an online quiz where persuaders (either humans or LLMs) attempted to persuade quiz takers toward correct or incorrect answers. We find that LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than incentivized human persuaders, demonstrating superior persuasive capabilities in both truthful (toward correct answers) and deceptive (toward incorrect answers) contexts. We also find that LLM persuaders significantly increased quiz takers' accuracy, leading to higher earnings, when steering quiz takers toward correct answers, and significantly decreased their accuracy, leading to lower earnings, when steering them toward incorrect answers. Overall, our findings suggest that AI's persuasion capabilities already exceed those of humans that have real-money bonuses tied to performance. Our findings of increasingly capable AI persuaders thus underscore the urgency of emerging alignment and governance frameworks.

</details>


### [231] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)

*Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi*

**Main category:** cs.CL

**Keywords:** Diffusion Language Models, Reinforcement Learning, Chain of Thought, Non-linear Reasoning, DCoLT

**Relevance Score:** 8

**TL;DR:** This paper introduces DCoLT, a novel reasoning framework for enhancing the performance of diffusion language models (DLMs) through optimized reasoning trajectories using reinforcement learning (RL).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of diffusion language models beyond traditional linear thought processes, allowing non-linear and bidirectional reasoning.

**Method:** DCoLT treats each intermediate step in the reverse diffusion process as a latent thinking action and optimizes the reasoning trajectory using outcome-based reinforcement learning. It is implemented on two DLMs, SEDD and LLaDA, using specific policies to maximize RL rewards during math and code generation tasks.

**Key Contributions:**

	1. Introduction of DCoLT, enabling non-linear reasoning in DLMs
	2. Implementation on SEDD and LLaDA with distinct RL optimization strategies
	3. Demonstrated significant accuracy gains on tasks like math and code generation.

**Result:** DCoLT-reinforced language models show significant improvement in reasoning accuracy, outperforming models trained with standard fine-tuning or other RL methods.

**Limitations:** 

**Conclusion:** The proposed DCoLT framework effectively enhances the reasoning accuracy of DLMs, achieving substantial performance improvements in various tasks.

**Abstract:** We introduce the Diffusion Chain of Lateral Thought (DCoLT), a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent "thinking" action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal, linear thinking process, DCoLT allows bidirectional, non-linear reasoning with no strict rule on grammatical correctness amid its intermediate steps of thought. We implement DCoLT on two representative Diffusion Language Models (DLMs). First, we choose SEDD as a representative continuous-time discrete diffusion model, where its concrete score derives a probabilistic policy to maximize the RL reward over the entire sequence of intermediate diffusion steps. We further consider the discrete-time masked diffusion language model -- LLaDA, and find that the order to predict and unmask tokens plays an essential role to optimize its RL action resulting from the ranking-based Unmasking Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both math and code generation tasks show that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>
