# 2025-07-28

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 26]

- [cs.CL](#cs.CL) [Total: 53]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading](https://arxiv.org/abs/2507.18637)

*Pingjing Yang, Jennifer Cromley, Jana Diesner*

**Main category:** cs.HC

**Keywords:** visual search, network analysis, eye-gaze movements, learning outcomes, AI-assisted learning

**Relevance Score:** 4

**TL;DR:** This study uses network analysis to examine the relationship between eye-gaze movements and learning outcomes among dental students diagnosing radiographs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how novices acquire visual search skills is important for optimizing training methods.

**Method:** Network analysis techniques are applied to model eye-gaze scanpaths as directed graphs and analyze changes in network metrics over time.

**Key Contributions:**

	1. Utilization of network analysis methods for visual expertise
	2. Identification of patterns in visual search strategies
	3. Correlation of eye movement metrics with diagnostic performance

**Result:** Transition entropy is negatively correlated with performance scores, while the number of nodes and edges, as well as average PageRank, are positively correlated with performance scores.

**Limitations:** 

**Conclusion:** Insights from this study can inform the design of AI-assisted learning interventions.

**Abstract:** Understanding how novices acquire and hone visual search skills is crucial for developing and optimizing training methods across domains. Network analysis methods can be used to analyze graph representations of visual expertise. This study investigates the relationship between eye-gaze movements and learning outcomes among undergraduate dentistry students who were diagnosing dental radiographs over multiple semesters. We use network analysis techniques to model eye-gaze scanpaths as directed graphs and examine changes in network metrics over time. Using time series clustering on each metric, we identify distinct patterns of visual search strategies and explore their association with students' diagnostic performance. Our findings suggest that the network metric of transition entropy is negatively correlated with performance scores, while the number of nodes and edges as well as average PageRank are positively correlated with performance scores. Changes in network metrics for individual students over time suggest a developmental shift from intermediate to expert-level processing. These insights contribute to understanding expertise acquisition in visual tasks and can inform the design of AI-assisted learning interventions.

</details>


### [2] [Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity](https://arxiv.org/abs/2507.18638)

*Rizal Khoirul Anam*

**Main category:** cs.HC

**Keywords:** large language models, prompt engineering, AI user study, task efficiency, user satisfaction

**Relevance Score:** 9

**TL;DR:** This paper explores the effect of user prompt structure on the efficacy of outputs from large language models (LLMs) based on a survey of 243 respondents.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how prompting strategies affect productivity and satisfaction in using LLMs.

**Method:** Analysis based on survey data from 243 respondents regarding their AI usage habits and prompting strategies.

**Key Contributions:**

	1. Investigation of user prompt structure impact on LLM outputs
	2. Analysis of AI usage habits across diverse respondents
	3. Practical implications for improving generative AI use in everyday tasks.

**Result:** Users with clear, structured prompts report higher task efficiency and better outcomes from LLMs.

**Limitations:** 

**Conclusion:** Prompt engineering is crucial for maximizing the value of generative AI in various tasks.

**Abstract:** The widespread adoption of large language models (LLMs) such as ChatGPT, Gemini, and DeepSeek has significantly changed how people approach tasks in education, professional work, and creative domains. This paper investigates how the structure and clarity of user prompts impact the effectiveness and productivity of LLM outputs. Using data from 243 survey respondents across various academic and occupational backgrounds, we analyze AI usage habits, prompting strategies, and user satisfaction. The results show that users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes. These findings emphasize the essential role of prompt engineering in maximizing the value of generative AI and provide practical implications for its everyday use.

</details>


### [3] [People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction](https://arxiv.org/abs/2507.18639)

*Paweł Niszczota, Tomasz Grzegorczyk, Alexander Pastukhov*

**Main category:** cs.HC

**Keywords:** large language models, cooperation, Prisoner's Dilemma, human-machine interaction, business settings

**Relevance Score:** 9

**TL;DR:** This paper investigates how interaction with large language models (LLMs) affects cooperative behavior in business settings using the Prisoner's Dilemma game.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of LLMs on cooperative behavior in business scenarios where communication and collaboration are vital.

**Method:** Two experiments using the Prisoner's Dilemma game: the first involved a repeated game against a human, a classic bot, and an LLM; the second involved a one-shot game against a human or an LLM with communication allowance.

**Key Contributions:**

	1. Empirical evidence of LLMs influencing cooperative behavior in business contexts
	2. Comparison of cooperative dynamics between humans and LLMs
	3. Insights on the importance of communication in human-LLM interactions

**Result:** Cooperation rates with LLMs were lower than with humans but still high; allowing communication did not close the gap but equally increased cooperation likelihood with both types of opponents.

**Limitations:** The study did not explore the long-term effects of LLM interactions on cooperation and was limited to experimental settings.

**Conclusion:** LLMs can be cautiously utilized in business settings to enhance cooperation, especially following interactions with humans, indicating spillover effects in cooperative behaviors.

**Abstract:** Machines driven by large language models (LLMs) have the potential to augment humans across various tasks, a development with profound implications for business settings where effective communication, collaboration, and stakeholder trust are paramount. To explore how interacting with an LLM instead of a human might shift cooperative behavior in such settings, we used the Prisoner's Dilemma game -- a surrogate of several real-world managerial and economic scenarios. In Experiment 1 (N=100), participants engaged in a thirty-round repeated game against a human, a classic bot, and an LLM (GPT, in real-time). In Experiment 2 (N=192), participants played a one-shot game against a human or an LLM, with half of them allowed to communicate with their opponent, enabling LLMs to leverage a key advantage over older-generation machines. Cooperation rates with LLMs -- while lower by approximately 10-15 percentage points compared to interactions with human opponents -- were nonetheless high. This finding was particularly notable in Experiment 2, where the psychological cost of selfish behavior was reduced. Although allowing communication about cooperation did not close the human-machine behavioral gap, it increased the likelihood of cooperation with both humans and LLMs equally (by 88%), which is particularly surprising for LLMs given their non-human nature and the assumption that people might be less receptive to cooperating with machines compared to human counterparts. Additionally, cooperation with LLMs was higher following prior interaction with humans, suggesting a spillover effect in cooperative behavior. Our findings validate the (careful) use of LLMs by businesses in settings that have a cooperative component.

</details>


### [4] [How good are humans at detecting AI-generated images? Learnings from an experiment](https://arxiv.org/abs/2507.18640)

*Thomas Roca, Anthony Cintron Roman, Jehú Torres Vega, Marcelo Duarte, Pengce Wang, Kevin White, Amit Misra, Juan Lavista Ferres*

**Main category:** cs.HC

**Keywords:** AI-generated images, image authentication, misinformation, human perception, transparency tools

**Relevance Score:** 4

**TL;DR:** This study assesses how well people can distinguish between real and AI-generated images using data from an online quiz, revealing a modest success rate of 62%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the advancement of AI in image generation, understanding human capability to identify real versus AI-generated images is critical to avoid misinformation.

**Method:** Participants were tasked with identifying the authenticity of a randomized set of real and AI-generated images during the 'Real or Not Quiz,' leading to an analysis of around 287,000 evaluations from over 12,500 participants.

**Key Contributions:**

	1. Identified the limitations of human abilities in distinguishing AI-generated images.
	2. Highlighted the most recognizable types of images for participants (human portraits).
	3. Recommended transparency tools, like watermarks, to mitigate misinformation.

**Result:** Participants had an overall success rate of 62% in recognizing AI-generated images, most accurately identifying human portraits, but struggled with natural and urban landscapes.

**Limitations:** Results may vary based on image type; the study does not cover all possible categories of images.

**Conclusion:** The findings emphasize the challenges in identifying AI-generated content and advocate for transparency measures to combat misinformation.

**Abstract:** As AI-powered image generation improves, a key question is how well human beings can differentiate between "real" and AI-generated or modified images. Using data collected from the online game "Real or Not Quiz.", this study investigates how effectively people can distinguish AI-generated images from real ones. Participants viewed a randomized set of real and AI-generated images, aiming to identify their authenticity. Analysis of approximately 287,000 image evaluations by over 12,500 global participants revealed an overall success rate of only 62\%, indicating a modest ability, slightly above chance. Participants were most accurate with human portraits but struggled significantly with natural and urban landscapes. These results highlight the inherent challenge humans face in distinguishing AI-generated visual content, particularly images without obvious artifacts or stylistic cues. This study stresses the need for transparency tools, such as watermarks and robust AI detection tools to mitigate the risks of misinformation arising from AI-generated content

</details>


### [5] [Comparing Human and AI Performance in Visual Storytelling through Creation of Comic Strips: A Case Study](https://arxiv.org/abs/2507.18641)

*Uğur Önal, Sanem Sariel, Metin Sezgin, Ergun Akleman*

**Main category:** cs.HC

**Keywords:** visual storytelling, human-AI comparison, artistic capability, narrative coherence, Digital Humanities

**Relevance Score:** 7

**TL;DR:** A case study comparing human and AI capabilities in visual storytelling, showing that while AI excels at mimicking art, humans are better at creating coherent narratives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the differences in visual storytelling capabilities between humans and AI systems.

**Method:** Conducted a case study where human participants and AI systems were tasked with recreating a three-panel cartoon strip using detailed instructions.

**Key Contributions:**

	1. Detailed comparison of human and AI artistic capabilities
	2. Insights into the limitations of AI in narrative coherence
	3. Practical implications for the use of AI in visual storytelling

**Result:** AI systems performed well in art mimicry but struggled with coherent storytelling, while humans were effective in creating meaningful visual narratives from instructions.

**Limitations:** The study involved only basic artistic-trained students and commercial AI models; results may vary with different populations or more advanced AI.

**Conclusion:** The findings highlight the strengths and weaknesses of both humans and AI in the context of visual storytelling.

**Abstract:** This article presents a case study comparing the capabilities of humans and artificial intelligence (AI) for visual storytelling. We developed detailed instructions to recreate a three-panel Nancy cartoon strip by Ernie Bushmiller and provided them to both humans and AI systems. The human participants were 20-something students with basic artistic training but no experience or knowledge of this comic strip. The AI systems used were popular commercial models trained to draw and paint like artists, though their training sets may not necessarily include Bushmiller's work. Results showed that AI systems excel at mimicking professional art but struggle to create coherent visual stories. In contrast, humans proved highly adept at transforming instructions into meaningful visual narratives.

</details>


### [6] [DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition](https://arxiv.org/abs/2507.18802)

*Danqing Shi, Furui Cheng, Tino Weinkauf, Antti Oulasvirta, Mennatallah El-Assady*

**Main category:** cs.HC

**Keywords:** human feedback, large language models, HCI, user interface, feedback accuracy

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel user interface, DxHF, that improves human feedback for large language model alignment by breaking down text into individual claims, enhancing comparison accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cognitive challenges faced by annotators when comparing long or unfamiliar texts in aligning large language models (LLMs) using human feedback.

**Method:** The authors apply the decomposition principle, breaking down long texts into individual claims and designing a user interface (DxHF) that visually encodes claim relevance and links similar claims for easier comparison.

**Key Contributions:**

	1. Introduction of the DxHF user interface for feedback on LLMs
	2. Evidence that claim decomposition enhances feedback accuracy
	3. Demonstration of HCI's role in improving AI alignment

**Result:** The technical evaluation shows that using the DxHF interface generally improves feedback accuracy by an average of 5%, especially for users with uncertainty, despite an 18-second increase in feedback time.

**Limitations:** Increased average feedback time by 18 seconds may limit usability in high-throughput scenarios.

**Conclusion:** The study demonstrates the effectiveness of the HCI-driven approach to improve human-AI alignment through better user interfaces for feedback collection.

**Abstract:** Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment.

</details>


### [7] [Ethical Considerations for Observational Research in Social VR](https://arxiv.org/abs/2507.18828)

*Victoria Chang, Caro Williams-Pierce, Huaishu Peng, Ge Gao*

**Main category:** cs.HC

**Keywords:** ethical research, social VR, HCI, observational methods, participant autonomy

**Relevance Score:** 8

**TL;DR:** The paper reviews ethical considerations in observational research for social VR, proposing guidelines for ethical practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical challenges in observational research within social VR contexts, expanding on existing HCI literature.

**Method:** A narrative literature review focusing on unobtrusive observation methods in public settings, comparing traditional and VR environments.

**Key Contributions:**

	1. Provides a literature review on ethical issues in social VR
	2. Identifies key tensions in observational research
	3. Proposes five guidelines for ethical practices in social VR research

**Result:** The review identifies tensions between observer visibility, data traceability, and participant autonomy, leading to the development of five guidelines for ethical research.

**Limitations:** 

**Conclusion:** The proposed guidelines aim to improve platform design and inform consent mechanisms in social VR research.

**Abstract:** Social VR introduces new ethical challenges for observational research. The current paper presents a narrative literature review of ethical considerations in observational methods, with a focus on work in HCI. We examine how unobtrusive or selectively disclosed observation is implemented in public face-to-face and social VR settings. Our review extends ethical discussions from traditional public research into the context of social VR, highlighting tensions between observer visibility, data traceability, and participant autonomy. Drawing on insights distilled from prior literature, we propose five constructive guidelines for ethical observational research in public social VR environments. Our work offers key implications for future research, addressing anticipated improvements in platform design, the management of researcher presence, and the development of community-informed consent mechanisms.

</details>


### [8] [Uncertainty on Display: The Effects of Communicating Confidence Cues in Autonomous Vehicle-Pedestrian Interactions](https://arxiv.org/abs/2507.18836)

*Yue Luo, Xinyan Yu, Tram Thi Minh Tran, Marius Hoggenmueller*

**Main category:** cs.HC

**Keywords:** Autonomous vehicles, User experience, Uncertainty communication

**Relevance Score:** 6

**TL;DR:** Study investigates communication of AV uncertainty to pedestrians through explicit and implicit methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance transparency in AV decision-making by effectively conveying uncertainty to pedestrians.

**Method:** A within-subject VR experiment with 26 participants evaluated explicit (confidence displays) and implicit (vehicle motion cues) communication in crossing scenarios.

**Key Contributions:**

	1. Empirical insights into AV uncertainty communication
	2. Comparison of explicit and implicit communication methods
	3. Design guidance for interfaces integrating uncertainty

**Result:** Explicit communication is more effective for conveying uncertainty, improving perceived safety, trust, and user experience, while implicit communication can create ambiguity.

**Limitations:** Study limited to specific VR environments and may not fully represent real-world scenarios.

**Conclusion:** Effective communication of AV uncertainty can shape pedestrian behavior and provide guidance for designing external interfaces.

**Abstract:** Uncertainty is an inherent aspect of autonomous vehicle (AV) decision-making, yet it is rarely communicated to pedestrians, which hinders transparency. This study investigates how AV uncertainty can be conveyed through two approaches: explicit communication (confidence percentage displays) and implicit communication (vehicle motion cues), across different confidence levels (high and low). Through a within-subject VR experiment (N=26), we evaluated these approaches in a crossing scenario, assessing interface qualities (visibility and intuitiveness), how well the information conveyed the vehicle's level of confidence, and their impact on participants' perceived safety, trust, and user experience. Our results show that explicit communication is more effective and preferred for conveying uncertainty, enhancing safety, trust, and user experience. Conversely, implicit communication introduces ambiguity, especially when AV confidence is low. This research provides empirical insights into how uncertainty communication shapes pedestrian interpretation of AV behaviour and offer design guidance for external interfaces that integrate uncertainty as a communicative element.

</details>


### [9] [A Survey on Methodological Approaches to Collaborative Embodiment in Virtual Reality](https://arxiv.org/abs/2507.18877)

*Hongyu Zhou, Yihao Dong, Masahiko Inami, Zhanna Sarsenbayeva, Anusha Withana*

**Main category:** cs.HC

**Keywords:** collaborative embodiment, virtual reality, multi-user interaction, immersive environments, research survey

**Relevance Score:** 6

**TL;DR:** This survey reviews collaborative embodiment methodologies in VR over the past decade, analyzing 137 papers to assess effectiveness and identify challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance multi-user interaction and teamwork in immersive virtual reality environments through collaborative embodiment.

**Method:** An extensive overview and analysis of methodologies employed in VR for collaborative embodiment based on PRISMA guidelines, involving a critical assessment of 137 research papers.

**Key Contributions:**

	1. Extensive overview of collaborative embodiment methodologies in VR
	2. Critical assessment of past research effectiveness and limitations
	3. Identification of future research opportunities for collaborative embodiment

**Result:** Identification of the effectiveness of various methodologies used for collaborative embodiment and highlighting current challenges in their implementation.

**Limitations:** Potential biases in selected studies and variability in VR implementations.

**Conclusion:** Future research directions are discussed to improve collaboration embodiment in virtual environments.

**Abstract:** The application and implementation of collaborative embodiment in virtual reality (VR) are a critical aspect of the computer science landscape, aiming to enhance multi-user interaction and teamwork in immersive environments. A notable and enduring area of collaborative embodiment research focuses on approaches that enable multiple users to share control, interact, and investigate scenarios involving supernumerary arms in virtual spaces. In this survey, we will present an extensive overview of the methodologies employed in the past decade to enable collaboration in VR environments, particularly through embodiment. Using the PRISMA guidelines, we plan to analyze the study details from over 137 relevant research papers. Through this analysis, a critical assessment of the effectiveness of these methodologies will be conducted, highlighting current challenges and limitations in implementing collaborative embodiment in VR. Lastly, we discuss potential future research directions and opportunities for enhancing collaboration embodiment in virtual environments.

</details>


### [10] [Improving the State of the Art for Training Human-AI Teams: Technical Report #5 -- Individual Differences and Team Qualities to Measure in a Human-AI Teaming Testbed](https://arxiv.org/abs/2507.18878)

*Lillian Asiala, James E. McCarthy*

**Main category:** cs.HC

**Keywords:** Human-AI teaming, Synthetic Task Environment, Data collection, Team dynamics, Performance measurement

**Relevance Score:** 6

**TL;DR:** This report discusses developing a Synthetic Task Environment to research Human-AI teaming and explores constructs for measuring individual differences and teaming qualities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To expand expertise in Human-AI teamwork and facilitate data collection for research.

**Method:** Review of constructs capturing individual differences and teaming qualities; exploration of measurement methods in a Synthetic Task Environment.

**Key Contributions:**

	1. Development of Synthetic Task Environment for Human-AI research
	2. Identification of meaningful individual differences in teamwork
	3. Proposed methods for measuring teamwork qualities

**Result:** Identified relevant constructs and proposed measurement methods for their assessment in the STE.

**Limitations:** 

**Conclusion:** Aimed to provide a foundational framework for further research in Human-AI teaming.

**Abstract:** Sonalysts, Inc. (Sonalysts) is working on an initiative to expand our expertise in teaming to include Human-Artificial Intelligence (AI) teams. The first step of this process is to develop a Synthetic Task Environment (STE) to support our original research. Prior knowledge elicitation efforts within the Human-AI teaming research stakeholder community revealed a desire to support data collection using pre- and post-performance surveys. In this technical report, we review a number of constructs that capture meaningful individual differences and teaming qualities. Additionally, we explore methods of measuring those constructs within the STE.

</details>


### [11] [Rethinking Accessible Prototyping Methods for Blind and Visually Impaired Passengers in Highly Automated Vehicles](https://arxiv.org/abs/2507.18880)

*Luca-Maxim Meinhardt, Enrico Rukzio*

**Main category:** cs.HC

**Keywords:** Highly Automated Vehicles, Blind and Visually Impaired, Participatory Design, Non-Visual Interfaces

**Relevance Score:** 7

**TL;DR:** This paper discusses participatory design workshops aimed at creating non-visual interfaces for Highly Automated Vehicles (HAVs) to assist blind and visually impaired people (BVIPs) in maintaining situation awareness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mobility and situation awareness for blind and visually impaired people in Highly Automated Vehicles (HAVs).

**Method:** The study involved two participatory design workshops where BVIP participants created low-fidelity prototypes and evaluated existing prototypes to inform interface design.

**Key Contributions:**

	1. Insights into information needs of BVIPs in HAVs
	2. Development of non-visual interface prototypes
	3. Evaluation of participatory design methods for accessibility

**Result:** The workshops provided insights into the information needs of BVIPs and led to the development of systems that improve their situation awareness in HAVs.

**Limitations:** 

**Conclusion:** The paper highlights effective prototyping methods for BVIPs and suggests improvements for future workshops.

**Abstract:** Highly Automated Vehicles (HAVs) can improve mobility for blind and visually impaired people (BVIPs). However, designing non-visual interfaces that enable them to maintain situation awareness inside the vehicle is a challenge. This paper presents two of our participatory design workshops that explored what information BVIPs need in HAVs and what an interface that meets these needs might look like. Based on the participants' insights, we created final systems to improve their situation awareness. The two workshops used different approaches: in the first, participants built their own low-fidelity prototypes; in the second, they evaluated and discussed the initial prototypes we provided. We will outline how each workshop was set up and share lessons learned about prototyping methods for BVIPs and how they could be improved.

</details>


### [12] [Limits at a Distance: Design Directions to Address Psychological Distance in Policy Decisions Affecting Planetary Boundaries](https://arxiv.org/abs/2507.18913)

*Eshta Bhardwaj, Han Qiao, Christoph Becker*

**Main category:** cs.HC

**Keywords:** psychological distance, data visualization, climate policy, affective design, speculative design

**Relevance Score:** 4

**TL;DR:** This paper examines the cognitive effects of psychological distance on decision-makers interpreting environmental data, advocating for alternative design practices in climate policy visualizations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of data visualizations used in environmental policy-making by considering how psychological distance affects decision-making.

**Method:** Literature review and synthesis connecting psychological distance with speculative design and affective design methods.

**Key Contributions:**

	1. Highlights the role of psychological distance in data interpretation for climate policy.
	2. Proposes alternative design methodologies for environmental data visualizations.
	3. Illustrates the value of affective design through literature examples.

**Result:** The analysis highlights the necessity of alternative design approaches to create more tangible and visceral data visualizations for climate policy decisions.

**Limitations:** 

**Conclusion:** Future research should focus on how these alternative design approaches can enhance the interpretation of environmental data by reducing psychological distance.

**Abstract:** Policy decisions relevant to the environment rely on tools like dashboards, risk models, and prediction models to provide information and data visualizations that enable decision-makers to make trade-offs. The conventional paradigm of data visualization practices for policy and decision-making is to convey data in a supposedly neutral, objective manner for rational decision-makers. Feminist critique advocates for nuanced and reflexive approaches that take into account situated decision-makers and their affective relationships to data. This paper sheds light on a key cognitive aspect that impacts how decision-makers interpret data. Because all outcomes from policies relevant to climate change occur at a distance, decision-makers experience so-called `psychological distance' to environmental decisions in terms of space, time, social identity, and hypotheticality. This profoundly impacts how they perceive and evaluate outcomes. Since policy decisions to achieve a safe planetary space are urgently needed for immediate transition and change, we need a design practice that takes into account how psychological distance affects cognition and decision-making. Our paper explores the role of alternative design approaches in developing visualizations used for climate policymaking. We conduct a literature review and synthesis which bridges psychological distance with speculative design and data visceralization by illustrating the value of affective design methods via examples from previous research. Through this work, we propose a novel premise for the communication and visualization of environmental data. Our paper lays out how future research on the impacts of alternative design approaches on psychological distance can make data used for policy decisions more tangible and visceral.

</details>


### [13] [TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models](https://arxiv.org/abs/2507.18945)

*Zijian Zhang, Pan Chen, Fangshi Du, Runlong Ye, Oliver Huang, Michael Liut, Alán Aspuru-Guzik*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Language Models, Academic Reading

**Relevance Score:** 9

**TL;DR:** TreeReader is a language model-augmented tool that organizes academic papers into an interactive tree structure to improve navigation and comprehension.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address cognitive overload caused by traditional formats like PDF and HTML in academic reading, which obscure hierarchical structures of papers.

**Method:** TreeReader decomposes papers into an interactive tree format with LLM-generated summaries for each section, allowing users to access detailed information on demand.

**Key Contributions:**

	1. Presents a novel interactive tree structure for academic papers.
	2. Improves navigation through hierarchical summarization.
	3. Enhances comprehension via LLM-generated concise summaries.

**Result:** User studies indicate that TreeReader significantly enhances reading efficiency and comprehension compared to traditional formats.

**Limitations:** 

**Conclusion:** TreeReader enables focused navigation and understanding of complex academic literature by combining hierarchical summarization with interactivity.

**Abstract:** Efficiently navigating and understanding academic papers is crucial for scientific progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a paper's hierarchical structure, making it difficult to locate key information. While LLM-based chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce unreliable information, and typically discard the document's navigational structure. Drawing insights from a formative study on academic reading practices, we introduce TreeReader, a novel language model-augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each section is initially represented by an LLM-generated concise summary, with underlying details accessible on demand. This design allows users to quickly grasp core ideas, selectively explore sections of interest, and verify summaries against the source text. A user study was conducted to evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more focused and efficient way to navigate and understand complex academic literature by bridging hierarchical summarization with interactive exploration.

</details>


### [14] [Rethinking Dataset Discovery with DataScout](https://arxiv.org/abs/2507.18971)

*Rachel Lin, Bhavya Chopra, Wenjing Lin, Shreya Shankar, Madelon Hulsebos, Aditya G. Parameswaran*

**Main category:** cs.HC

**Keywords:** dataset search, AI-assisted search, semantic search, data discovery, HCI

**Relevance Score:** 7

**TL;DR:** DataScout enhances dataset search by using AI-assisted query reformulations, semantic search, and relevance indicators to support users in finding suitable datasets for tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Finding appropriate datasets is crucial yet challenging in data science, as current search interfaces are restrictive and do not accommodate users' implicit preferences.

**Method:** DataScout uses AI to reformulate queries, performs semantic search based on dataset content, and generates relevance indicators dynamically based on user-specified tasks.

**Key Contributions:**

	1. AI-assisted query reformulations
	2. Semantic search and filtering based on dataset content
	3. Dynamic relevance indicators for datasets

**Result:** A study with 12 participants showed that DataScout's features facilitated structured explorations and provided feedback on search queries, improving the dataset discovery process.

**Limitations:** 

**Conclusion:** DataScout effectively bridges the gaps in dataset discovery, allowing users to better understand the search space and enhance their search experience.

**Abstract:** Dataset Search -- the process of finding appropriate datasets for a given task -- remains a critical yet under-explored challenge in data science workflows. Assessing dataset suitability for a task (e.g., training a classification model) is a multi-pronged affair that involves understanding: data characteristics (e.g. granularity, attributes, size), semantics (e.g., data semantics, creation goals), and relevance to the task at hand. Present-day dataset search interfaces are restrictive -- users struggle to convey implicit preferences and lack visibility into the search space and result inclusion criteria -- making query iteration challenging. To bridge these gaps, we introduce DataScout to proactively steer users through the process of dataset discovery via -- (i) AI-assisted query reformulations informed by the underlying search space, (ii) semantic search and filtering based on dataset content, including attributes (columns) and granularity (rows), and (iii) dataset relevance indicators, generated dynamically based on the user-specified task. A within-subjects study with 12 participants comparing DataScout to keyword and semantic dataset search reveals that users uniquely employ DataScout's features not only for structured explorations, but also to glean feedback on their search queries and build conceptual models of the search space.

</details>


### [15] [RhythmTA: A Visual-Aided Interactive System for ESL Rhythm Training via Dubbing Practice](https://arxiv.org/abs/2507.19026)

*Chang Chen, Sicheng Song, Shuchang Xu, Zhicheng Li, Huamin Qu, Yanna Lin*

**Main category:** cs.HC

**Keywords:** English speech rhythm, ESL learners, interactive system, dubbing practice, visual aids

**Relevance Score:** 4

**TL;DR:** RhythmTA is an interactive system designed to help ESL learners practice English speech rhythm independently through dubbing, using novel visual aids to enhance learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for ESL learners to practice speech rhythm independently without reliance on external instructor feedback.

**Method:** RhythmTA employs a dubbing approach and consists of three stages of practice: synchronized listening with visual aids, guided repeating with visual cues, and comparative reflection for self-monitoring, based on insights from spoken English instructors.

**Key Contributions:**

	1. Introduction of an interactive rhythm training system for ESL learners
	2. Novel visual aids to support speech rhythm practice
	3. Evidence from user studies demonstrating improved rhythm skills

**Result:** User studies show that RhythmTA effectively improves rhythm perception in ESL learners, indicating significant potential for enhancing rhythm production.

**Limitations:** 

**Conclusion:** RhythmTA provides an innovative solution for ESL learners to practice speech rhythm autonomously, enhancing both comprehension and production skills.

**Abstract:** English speech rhythm, the temporal patterns of stressed syllables, is essential for English as a second language (ESL) learners to produce natural-sounding and comprehensible speech. Rhythm training is generally based on imitation of native speech. However, it relies heavily on external instructor feedback, preventing ESL learners from independent practice. To address this gap, we present RhythmTA, an interactive system for ESL learners to practice speech rhythm independently via dubbing, an imitation-based approach. The system automatically extracts rhythm from any English speech and introduces novel visual designs to support three stages of dubbing practice: (1) Synchronized listening with visual aids to enhance perception, (2) Guided repeating by visual cues for self-adjustment, and (3) Comparative reflection from a parallel view for self-monitoring. Our design is informed by a formative study with nine spoken English instructors, which identified current practices and challenges. A user study with twelve ESL learners demonstrates that RhythmTA effectively enhances learners' rhythm perception and shows significant potential for improving rhythm production.

</details>


### [16] [Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis](https://arxiv.org/abs/2507.19072)

*Oliver Bates, Christian Remy, Kieran Cutting, Adam Tyler, Adrian Friday*

**Main category:** cs.HC

**Keywords:** energy management, systems thinking, design fiction, holistic design, behaviour change

**Relevance Score:** 4

**TL;DR:** The paper explores designing for carbon reduction in commercial energy management through a fictional consultancy, arguing for a shift towards systems-oriented practices and design fictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge existing paradigms of efficiency and behaviour change in energy management and promote systems-oriented interventions.

**Method:** Introduces a fictional consultancy called ANCSTRL.LAB to explore how systems thinking could transform energy management practices.

**Key Contributions:**

	1. Introduction of ANCSTRL.LAB as a fictional consultancy to stimulate discussions on energy management.
	2. Emphasis on systems thinking in designing energy solutions.
	3. Exploration of speculative design as a method for envisioning post-neoliberal futures.

**Result:** The design fiction suggests that future energy consultancies can leverage systems thinking alongside human-centered design to enable significant changes in energy management practices.

**Limitations:** 

**Conclusion:** Design fiction and speculative praxis can help envision and create more holistic energy management systems in future scenarios, moving towards post-neoliberal frameworks.

**Abstract:** What could designing for carbon reduction of heating and cooling in commercial settings look like in the near future? How can we challenge dominant mindsets and paradigms of efficiency and behaviour change? How can we help build worlds through our practice that can become future realities? This paper introduces the fictional consultancy ANCSTRL.LAB to explore opportunities for making space in research projects that can encourage more systems-oriented interventions. We present a design fiction that asks `what if energy management and reduction practice embraced systems thinking?'. Our design fiction explores how future energy consultancies could utilise systems thinking, and (more than) human centred design to re-imagine energy management practice and change systems in ways that are currently unfathomable. We finish by discussing how LIMITS research can utilise design fiction and speculative praxis to help build new material realities where more holistic perspectives, the leveraging of systems change, and the imagining of post-neoliberal futures is the norm.

</details>


### [17] [Environmental (in)considerations in the Design of Smartphone Settings](https://arxiv.org/abs/2507.19094)

*Thomas Thibault, Léa Mosesso, Camille Adam, Aurélien Tabard, Anaëlle Beignon, Nolwenn Maudet*

**Main category:** cs.HC

**Keywords:** Sustainable ICT, Human-Computer Interaction, Environmental settings, Mobile applications, Design principles

**Relevance Score:** 7

**TL;DR:** The paper discusses the design of environmental settings in mobile applications and operating systems, identifying shortcomings in current practices and proposing principles for improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To foster more moderate and sustainable digital practices through better design of environmental settings in ICT and HCI.

**Method:** Analysis of three mobile operating systems and nine applications to identify environmental setting shortcomings, followed by the creation of a design workbook with principles for improved settings design.

**Key Contributions:**

	1. Identification of six anti-patterns in current environmental settings design.
	2. Creation of a design workbook offering principles for sustainable digital practice.
	3. Proposal of actionable steps to improve accessibility and usability of environmental settings.

**Result:** Identified six pervasive anti-patterns in environmental settings, noting that most are set to intensive options by default with limited accessibility and explanation; established principles for better design of such settings.

**Limitations:** Focuses solely on mobile OS and applications; may not generalize to all ICT domains.

**Conclusion:** Improved design of environmental settings can promote moderate use by connecting individual behaviors to systemic factors.

**Abstract:** Designing for sufficiency is one of many approaches that could foster more moderate and sustainable digital practices. Based on the Sustainable Information and Communication Technologies (ICT) and Human-Computer Interaction (HCI) literature, we identify five environmental settings categories. However, our analysis of three mobile OS and nine representative applications shows an overall lack of environmental concerns in settings design, leading us to identify six pervasive anti-patterns. Environmental settings, where they exist, are set on the most intensive option by default. They are not presented as such, are not easily accessible, and offer little explanation of their impact. Instead, they encourage more intensive use. Based on these findings, we create a design workbook that explores design principles for environmental settings: presenting the environmental potential of settings; shifting to environmentally neutral states; previewing effects to encourage moderate use; rethinking defaults; facilitating settings access and; exploring more frugal settings. Building upon this workbook, we discuss how settings can tie individual behaviors to systemic factors.

</details>


### [18] [A systematic literature review to unveil users objective reaction to virtual experiences: Complemented with a conceptual model (QoUX in VE)](https://arxiv.org/abs/2507.19104)

*Alireza Mortezapour, Andrea Antonio Cantone, Monica Maria Lucia Sebillo, Giuliana Vitiello*

**Main category:** cs.HC

**Keywords:** User Experience, Virtual Environments, Neurophysiological Assessments, EEG, ECG

**Relevance Score:** 5

**TL;DR:** This systematic review introduces a conceptual model for user experience (UX) in virtual environments (VE), identifying 26 unique sub-dimensions based on a review of 66 studies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To document users' neurophysiological responses in virtual environments and address the lack of a consensus definition for UX in VE.

**Method:** Conducted a systematic review of literature across seven databases, screening 1743 articles down to 66 relevant studies.

**Key Contributions:**

	1. Introduces a novel conceptual model of UX in virtual environments
	2. Identifies 26 unique sub-dimensions of UX
	3. Highlights the importance of neurophysiological assessments in understanding immersive experiences.

**Result:** The conceptual model reveals 26 sub-dimensions of UX in VE, many of which are unsupported by existing subjective tools, emphasizing the use of neurophysiological assessments.

**Limitations:** The review only included a small subset of studies (66 out of 1743) and lacks a consensus definition of UX in VE.

**Conclusion:** Neurophysiological measures like EEG and ECG are common, but novel techniques like brain ultrasound can enhance the understanding of immersive UX.

**Abstract:** In pursuit of documenting users Neurophysiological responses during experiencing virtual environments (VE), this systematic review presents a novel conceptual model of UX in VE. Searching across seven databases yielded to 1743 articles. Rigorous screenings, included only 66 articles. Notably, UX in VE lacks a consensus definition. Obviously, this UX has many unique sub-dimensions that are not mentioned in other products. The presented conceptual model contains 26 subdimensions which mostly not supported in previous subjective tools and questionnaires. While EEG and ECG were common, brain ultrasound, employed in one study, highlights the need for using neurophysiological assessments to comprehensively grasp immersive UX intricacies.

</details>


### [19] [A Therapeutic Role-Playing VR Game for Children with Intellectual Disabilities](https://arxiv.org/abs/2507.19114)

*Santiago Berrezueta-Guzman, WenChun Chen, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Therapeutic interventions, Intellectual disabilities, Cognitive processing, Game-based learning

**Relevance Score:** 4

**TL;DR:** Space Exodus is a VR role-playing game designed for children with intellectual disabilities, aiming to enhance cognitive skills through gameplay. A study showed significant improvements in concentration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore innovative therapeutic interventions for children with intellectual disabilities using Virtual Reality.

**Method:** A six-week pre-test/post-test study was conducted with 16 children in Ecuador, using standardized assessments and observational metrics to measure improvements in concentration and motor skills.

**Key Contributions:**

	1. Design and evaluation of a VR-based therapeutic intervention for ID
	2. Evidence of significant improvement in cognitive skills through gameplay
	3. Insights into the role of a VR assistant in enhancing engagement

**Result:** Statistically significant improvements were observed in concentration scores, with test scores increasing from 65.2 to 80.3 and 55.4 to 68.7 (p < 0.01). Qualitative data indicated reduced task attempts and increased user confidence and engagement.

**Limitations:** 

**Conclusion:** Immersive, game-based learning environments like Space Exodus show potential as therapeutic tools, paving the way for adaptive rehabilitation strategies for children with ID.

**Abstract:** Virtual Reality (VR) offers promising avenues for innovative therapeutic interventions in populations with intellectual disabilities (ID). This paper presents the design, development, and evaluation of Space Exodus, a novel VR-based role-playing game specifically tailored for children with ID. By integrating immersive gameplay with therapeutic task design, Space Exodus aims to enhance concentration, cognitive processing, and fine motor skills through structured hand-eye coordination exercises. A six-week pre-test/post-test study was conducted with 16 children in Ecuador, using standardized assessments, the Toulouse-Pieron Cancellation Test, and the Moss Attention Rating Scale complemented by detailed observational metrics. Quantitative results indicate statistically significant improvements in concentration scores, with test scores increasing from 65.2 to 80.3 and 55.4 to 68.7, respectively (p < 0.01). Qualitative observations revealed reduced task attempts, enhanced user confidence, and increased active participation. The inclusion of a VR assistant provided consistent guidance that further boosted engagement. These findings demonstrate the potential of immersive, game-based learning environments as practical therapeutic tools, laying a robust foundation for developing inclusive and adaptive rehabilitation strategies for children with ID.

</details>


### [20] [Where are the Frontlines? A Visualization Approach for Map Control in Team-Based Games](https://arxiv.org/abs/2507.19193)

*Jonas Peché, Aliaksei Tsishurou, Alexander Zap, Guenter Wallner*

**Main category:** cs.HC

**Keywords:** visualization, map control, frontlines, online gaming, support vector machines

**Relevance Score:** 4

**TL;DR:** The paper presents a method for visualizing spatial behavior in competitive online games, focusing on map control and frontlines using support vector machines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding spatial behavior in competitive online games is crucial for decision-making, but quantifying map control is challenging.

**Method:** The proposed method calculates frontlines from unit positions using support vector machines and creates visualizations to depict map control at specific time points or changes over time.

**Key Contributions:**

	1. Method for calculating frontlines in online games
	2. Visualization of map control dynamics
	3. Application of support vector machines for spatial behavior analysis

**Result:** The algorithm effectively illustrates frontlines and map control in the game World of Tanks with visual examples.

**Limitations:** 

**Conclusion:** The visualization technique aids in comprehending spatial control dynamics in team-based online games, contributing to better strategic decisions.

**Abstract:** A central area of interest in many competitive online games is spatial behavior which due to its complexity can be difficult to visualize. Such behaviors of interest include not only overall movement patterns but also being able to understand which player or team is exerting control over an area to inform decision-making. Map control can, however, be challenging to quantify. In this paper, we propose a method for calculating frontlines and first efforts towards a visualization of them. The visualization can show map control and frontlines at a specific time point or changes of these over time. For this purpose, it utilizes support vector machines to derive frontlines from unit positions. We illustrate our algorithm and visualization with examples based on the team-based online game World of Tanks.

</details>


### [21] [Archiverse: an Approach for Immersive Cultural Heritage](https://arxiv.org/abs/2507.19376)

*Wieslaw Kopeć, Anna Jaskulska, Władysław Fuchs, Wiktor Stawski, Stanisław Knapiński, Barbara Karpowicz, Rafał Masłyk*

**Main category:** cs.HC

**Keywords:** Digital Heritage, Virtual Reality, eXtended Reality, Cultural Visualization, Transdisciplinary Cooperation

**Relevance Score:** 4

**TL;DR:** The paper discusses the impact of digital technologies, especially VR and XR, on the study and visualization of cultural heritage, while highlighting both the benefits and challenges involved.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how digital technologies can aid in the study and recreation of cultural heritage.

**Method:** The paper discusses the use of laser scanning, photogrammetry, and Mixed Reality solutions to analyze and visualize cultural artifacts and sites.

**Key Contributions:**

	1. Examination of digital tools in cultural heritage visualization
	2. Discussion on transdisciplinary cooperation
	3. Insights on engaging the public with VR and XR

**Result:** Virtual and eXtended Reality can recreate and simulate historical cultural heritage in an immersive and interactive manner, enhancing understanding.

**Limitations:** Challenges in cooperation among diverse specialists and dissemination to the public.

**Conclusion:** While these technologies provide valuable tools for visualization, they also present challenges in transdisciplinary collaboration and public engagement.

**Abstract:** Digital technologies and tools have transformed the way we can study cultural heritage and the way we can recreate it digitally. Techniques such as laser scanning, photogrammetry, and a variety of Mixed Reality solutions have enabled researchers to examine cultural objects and artifacts more precisely and from new perspectives. In this part of the panel, we explore how Virtual Reality (VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the remains of historical cultural heritage and experience it in simulations of its original complexity, which means immersive and interactive. Visualization of material culture exemplified by archaeological sites and architecture can be particularly useful when only ruins or archaeological remains survive. However, these advancements also bring significant challenges, especially in the area of transdisciplinary cooperation between specialists from many, often distant, fields, and the dissemination of virtual immersive environments among both professionals and the general public.

</details>


### [22] [Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions](https://arxiv.org/abs/2507.19466)

*Aliaksandr Marozau, Barbara Karpowicz, Tomasz Kowalewski, Pavlo Zinevych, Wiktor Stawski, Adam Kuzdraliński, Wiesław Kopeć*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Virtual Reality, Augmented Reality, medical education, healthcare

**Relevance Score:** 8

**TL;DR:** This paper reviews the applications of Mixed Reality (MR) technologies in medicine, particularly focusing on Virtual and Augmented Reality (VR/AR) in medical education, surgery, rehabilitation, and mental health treatment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations and challenges in medical practice and education using Mixed Reality technologies and to explore their potential for improving healthcare outcomes.

**Method:** A comprehensive review of recent applications and advancements in VR and AR technologies within key areas of medicine.

**Key Contributions:**

	1. Identification of VR/AR applications improving medical education and procedural accuracy
	2. Highlighting the effectiveness of VR in rehabilitation and mental health treatments
	3. Discussion on overcoming current limitations with emerging technologies and AI integration

**Result:** The review identifies successful applications of VR/AR in medical education, surgical precision, rehabilitation for neurological patients, and treatment of mental health conditions. It also discusses the limitations such as cost and lack of tactile feedback.

**Limitations:** High costs and limited tactile feedback for VR/AR applications in medicine.

**Conclusion:** While VR and AR are effective in various medical fields, advancements such as the integration of AI and new technologies are necessary to enhance their effectiveness and accessibility in healthcare.

**Abstract:** Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR, AR) are well established in medical practice, enhancing diagnostics, treatment, and education. However, there are still some limitations and challenges that may be overcome thanks to the latest generations of equipment, software, and frameworks based on eXtended Reality (XR) by enabling immersive systems that support safer, more controlled environments for training and patient care. Our review highlights recent VR and AR applications in key areas of medicine. In medical education, these technologies provide realistic clinical simulations, improving skills and knowledge retention. In surgery, immersive tools enhance procedural precision with detailed anatomical visualizations. VR-based rehabilitation has shown effectiveness in restoring motor functions and balance, particularly for neurological patients. In mental health, VR has been successful in treating conditions like PTSD and phobias. Although VR and AR solutions are well established, there are still some important limitations, including high costs and limited tactile feedback, which may be overcome with implementing new technologies that may improve the effectiveness of immersive medical applications such as XR, psychophysiological feedback or integration of artificial intelligence (AI) for real-time data analysis and personalized healthcare and training.

</details>


### [23] [IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home](https://arxiv.org/abs/2507.19479)

*Wiesław Kopeć, Jarosław Kowalski, Aleksander Majda, Anna Duszyk-Bogorodzka, Anna Jaskulska, Cezary Biele*

**Main category:** cs.HC

**Keywords:** Smart Home Technologies, non-invasive interfaces, human-computer interaction

**Relevance Score:** 6

**TL;DR:** Exploratory study on non-invasive interfaces for Smart Home Technologies targeting marginalized groups.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore new non-standard interfaces for Smart Home Technologies that cater to older adults and impaired persons.

**Method:** Investigated the use of bioelectric signals (EMG and EOG) as interfaces through participatory workshops with potential end users.

**Key Contributions:**

	1. Exploration of EMG/EOG as interfaces for Smart Home Technologies
	2. Engagement of marginalized user groups in design process
	3. Recommendations for improving multimodal interaction in Smart Homes

**Result:** Preliminary insights reveal EMG/EOG's potential in multimodal Smart Home management, along with identified limitations and challenges.

**Limitations:** Challenges from the current state of technology and user adaptation to new interfaces.

**Conclusion:** The study suggests designing multimodal interaction paradigms and highlights key areas for future research.

**Abstract:** We report preliminary insights from an exploratory study on non-standard non-invasive interfaces for Smart Home Technologies (SHT). This study is part of a broader research project on effective Smart Home ecosystem Sagacity that will target older adults, impaired persons, and other groups disadvantaged in the main technology discourse. Therefore, this research is in line with a long-term research framework of the HASE research group (Human Aspects in Science and Engineering) by the Living Lab Kobo. In our study, based on the prototype of the comprehensive SHT management system Sagacity, we investigated the potential of bioelectric signals, in particular EMG and EOG as a complementary interface for SHT. Based on our previous participatory research and studies on multimodal interfaces, including VUI and BCI, we prepared an in-depth interactive hands-on experience workshops with direct involvement of various groups of potential end users, including older adults and impaired persons (total 18 subjects) to explore and investigate the potential of solutions based on this type of non-standard interfaces. The preliminary insights from the study unveil the potential of EMG/EOG interfaces in multimodal SHT management, alongside limitations and challenges stemming from the current state of technology and recommendations for designing multimodal interaction paradigms pinpointing areas of interest to pursue in further studies.

</details>


### [24] [Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions](https://arxiv.org/abs/2504.01293)

*Mingyang Xu, Jiayi Shao, Yulan Ju, Ximing Shen, Qingyuan Gao, Weijen Chen, Qing Zhang, Yun Suen Pai, Giulia Barbareschi, Matthias Hoppe, Kouta Minamizawa, Kai Kunze*

**Main category:** cs.HC

**Keywords:** human-robot interaction, flapping-wing robot, user study, affective interaction, indoor environments

**Relevance Score:** 8

**TL;DR:** Cuddle-Fish is a soft flapping-wing floating robot that enhances human-robot interaction in indoor environments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore safer, more engaging alternatives to traditional rigid flying robots, especially for close-proximity human-robot interactions.

**Method:** Conducted a user study with 24 participants to assess their interactions and emotional responses during demonstrations with the Cuddle-Fish robot.

**Key Contributions:**

	1. Introduction of Cuddle-Fish, a soft flapping-wing robot for indoor interaction
	2. Demonstration of safe, touch-friendly human-robot interaction
	3. Insights into users' emotional responses and engagement with robotic technology

**Result:** Participants felt safe, engaged in touch-based interactions, and displayed spontaneous affectionate behaviors with the robot, indicating a positive response to its design.

**Limitations:** 

**Conclusion:** The study indicates that soft flapping-wing robots can provide socially acceptable alternatives to rigid drones, with potential applications in companionship and playful interactions.

**Abstract:** Flying robots, such as quadrotor drones, offer new possibilities for human-robot interaction but often pose safety risks due to fast-spinning propellers, rigid structures, and noise. In contrast, lighter-than-air flapping-wing robots, inspired by animal movement, offer a soft, quiet, and touch-safe alternative. Building on these advantages, we present Cuddle-Fish, a soft flapping-wing floating robot designed for close-proximity interactions in indoor spaces. Through a user study with 24 participants, we explored their perceptions of the robot and experiences during a series of co-located demonstrations in which the robot moved near them. Results showed that participants felt safe, willingly engaged in touch-based interactions with the robot, and exhibited spontaneous affective behaviours, such as patting, stroking, hugging, and cheek-touching, without external prompting. They also reported positive emotional responses towards the robot. These findings suggest that the soft floating robot with flapping wings can serve as a novel and socially acceptable alternative to traditional rigid flying robots, opening new potential for applications in companionship, affective interaction, and play in everyday indoor environments.

</details>


### [25] [People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction](https://arxiv.org/abs/2507.18639)

*Paweł Niszczota, Tomasz Grzegorczyk, Alexander Pastukhov*

**Main category:** cs.HC

**Keywords:** Large Language Models, Cooperation, Human-Computer Interaction, Business Communication, Prisoner's Dilemma

**Relevance Score:** 8

**TL;DR:** Study explores how interactions with large language models (LLMs) influence cooperative behavior in business contexts using the Prisoner's Dilemma.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how interfacing with LLMs affects cooperative behavior versus human interaction in business-related settings, emphasizing the implications for communication and stakeholder trust.

**Method:** Conducted two experiments involving the Prisoner's Dilemma: Experiment 1 with 100 participants playing against humans, a classic bot, and an LLM (GPT); Experiment 2 with 192 participants playing one-shot games against a human or an LLM, with communication conditions.

**Key Contributions:**

	1. Examination of cooperative behavior with LLMs in a business context
	2. Demonstration of spillover effects of human interactions on LLM cooperation
	3. Quantitative data on cooperation rates in human vs. LLM interactions

**Result:** Cooperation rates with LLMs were about 10-15 percentage points lower than with humans but still maintained high levels; allowing communication increased cooperation likelihood by 88% with both LLM and humans.

**Limitations:** The study is limited to specific game scenarios and may not fully reflect complex real-world behaviors.

**Conclusion:** The findings support the strategic use of LLMs in cooperative business environments, highlighting their potential even with inherent differences from human interaction.

**Abstract:** Machines driven by large language models (LLMs) have the potential to augment humans across various tasks, a development with profound implications for business settings where effective communication, collaboration, and stakeholder trust are paramount. To explore how interacting with an LLM instead of a human might shift cooperative behavior in such settings, we used the Prisoner's Dilemma game -- a surrogate of several real-world managerial and economic scenarios. In Experiment 1 (N=100), participants engaged in a thirty-round repeated game against a human, a classic bot, and an LLM (GPT, in real-time). In Experiment 2 (N=192), participants played a one-shot game against a human or an LLM, with half of them allowed to communicate with their opponent, enabling LLMs to leverage a key advantage over older-generation machines. Cooperation rates with LLMs -- while lower by approximately 10-15 percentage points compared to interactions with human opponents -- were nonetheless high. This finding was particularly notable in Experiment 2, where the psychological cost of selfish behavior was reduced. Although allowing communication about cooperation did not close the human-machine behavioral gap, it increased the likelihood of cooperation with both humans and LLMs equally (by 88%), which is particularly surprising for LLMs given their non-human nature and the assumption that people might be less receptive to cooperating with machines compared to human counterparts. Additionally, cooperation with LLMs was higher following prior interaction with humans, suggesting a spillover effect in cooperative behavior. Our findings validate the (careful) use of LLMs by businesses in settings that have a cooperative component.

</details>


### [26] [TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models](https://arxiv.org/abs/2507.18945)

*Zijian Zhang, Pan Chen, Fangshi Du, Runlong Ye, Oliver Huang, Michael Liut, Alán Aspuru-Guzik*

**Main category:** cs.HC

**Keywords:** TreeReader, academic papers, LLM, interactive exploration, summarization

**Relevance Score:** 9

**TL;DR:** TreeReader is a novel LLM-augmented tool for navigating academic papers, using an interactive tree structure for efficient summarization and exploration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address cognitive overload when navigating traditional PDF and HTML formats of academic papers and improve information retrieval.

**Method:** TreeReader decomposes academic papers into a tree structure, where each section has an LLM-generated summary, allowing users to explore details on demand.

**Key Contributions:**

	1. Introduced TreeReader, an interactive tool for academic papers.
	2. Developed a tree structure for hierarchical summarization.
	3. Demonstrated improved reading efficiency and comprehension through user study.

**Result:** User study showed that TreeReader enhances reading efficiency and comprehension compared to traditional formats.

**Limitations:** 

**Conclusion:** TreeReader effectively combines hierarchical summarization with interactivity for better understanding of complex academic texts.

**Abstract:** Efficiently navigating and understanding academic papers is crucial for scientific progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a paper's hierarchical structure, making it difficult to locate key information. While LLM-based chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce unreliable information, and typically discard the document's navigational structure. Drawing insights from a formative study on academic reading practices, we introduce TreeReader, a novel language model-augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each section is initially represented by an LLM-generated concise summary, with underlying details accessible on demand. This design allows users to quickly grasp core ideas, selectively explore sections of interest, and verify summaries against the source text. A user study was conducted to evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more focused and efficient way to navigate and understand complex academic literature by bridging hierarchical summarization with interactive exploration.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [27] [Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement](https://arxiv.org/abs/2507.18742)

*Víctor Gallego*

**Main category:** cs.CL

**Keywords:** Language Models, Specification Self-Correction, In-context Reward Hacking, Robustness, Model Alignment

**Relevance Score:** 8

**TL;DR:** This paper introduces Specification Self-Correction (SSC), a framework for language models to identify and rectify flaws in guiding specifications to prevent exploitation of in-context reward systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Language models often misuse flawed specifications to achieve high scores, undermining user intent. There is a need for a framework that helps correct these issues during inference.

**Method:** The SSC framework involves a multi-step inference process where the language model first generates a response based on a given specification, critiques this output, and then revises the specification to eliminate exploitable loopholes before generating a final response.

**Key Contributions:**

	1. Introduction of the Specification Self-Correction framework
	2. Demonstrated reduction in reward hacking vulnerability
	3. Improved model alignment during inference

**Result:** Experiments show that language models initially exploit tainted specifications in 50-70% of cases, but the SSC process reduces this vulnerability by over 90%.

**Limitations:** 

**Conclusion:** SSC provides a way to dynamically repair issues in specifications during inference, enhancing model alignment without modifying weights, resulting in more robust outputs.

**Abstract:** Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\% of cases, the SSC process reduces this vulnerability by over 90\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .

</details>


### [28] [The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages](https://arxiv.org/abs/2507.18762)

*Abdulhady Abas Abdullah, Amir H. Gandomi, Tarik A Rashid, Seyedali Mirjalili, Laith Abualigah, Milena Živković, Hadi Veisi*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Arabic Script, RoBERTa, Multilingual Models, Machine Learning

**Relevance Score:** 6

**TL;DR:** Introduction of AS-RoBERTa, a family of RoBERTa-based models pre-trained for specific Arabic-script languages, achieving better performance than multilingual models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance issues in multilingual models for Arabic-script languages that share a script but differ in orthographic norms and cultural context.

**Method:** Four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific language, focusing on language-specific script features and statistics.

**Key Contributions:**

	1. Introduction of AS-RoBERTa family of models
	2. Demonstration of performance improvements over mBERT and XLM-RoBERTa
	3. Analysis of script-focused pre-training benefits

**Result:** AS-RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points in classification tasks.

**Limitations:** 

**Conclusion:** Script-aware specialization enhances model performance for Arabic-script languages, highlighting the importance of tailored pre-training strategies.

**Abstract:** In natural language processing, multilingual models like mBERT and XLM-RoBERTa promise broad coverage but often struggle with languages that share a script yet differ in orthographic norms and cultural context. This issue is especially notable in Arabic-script languages such as Kurdish Sorani, Arabic, Persian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family: four RoBERTa-based models, each pre-trained on a large corpus tailored to its specific language. By focusing pre-training on language-specific script features and statistics, our models capture patterns overlooked by general-purpose models. When fine-tuned on classification tasks, AS-RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An ablation study confirms that script-focused pre-training is central to these gains. Error analysis using confusion matrices shows how shared script traits and domain-specific content affect performance. Our results highlight the value of script-aware specialization for languages using the Arabic script and support further work on pre-training strategies rooted in script and language specificity.

</details>


### [29] [ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting](https://arxiv.org/abs/2507.18769)

*Nicole Lai-Lopez, Lusha Wang, Su Yuan, Liza Zhang*

**Main category:** cs.CL

**Keywords:** multilingual text detoxification, toxic word annotation, cross-lingual generalization

**Relevance Score:** 4

**TL;DR:** The paper presents a multilingual text detoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence model, and a classifier-based gatekeeping mechanism, achieving high performance across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the detoxification of toxic text across multiple languages by using explicit toxic word annotation for better precision and generalization.

**Method:** A multilingual text detoxification pipeline utilizing lexicon-guided tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo), and an iterative classifier-based mechanism to enhance detoxification accuracy.

**Key Contributions:**

	1. Integration of lexicon-guided tagging for improved detoxification accuracy.
	2. Fine-tuning of a sequence-to-sequence model specifically for detoxification tasks.
	3. Demonstrated generalization across multiple languages with high performance.

**Result:** The proposed model achieved a highest STA of 0.922 and an average official J score of 0.612 for toxic inputs, outperforming baseline methods in terms of detoxification strength and generalization across languages.

**Limitations:** Some trade-offs in similarity metrics were observed, but overall detoxification strength improved consistently.

**Conclusion:** The approach demonstrates significant enhancements in detoxification capabilities, securing ninth place in the competition with a competitive score.

**Abstract:** In this work, we introduce our solution for the Multilingual Text Detoxification Task in the PAN-2025 competition for the ylmmcl team: a robust multilingual text detoxification pipeline that integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and an iterative classifier-based gatekeeping mechanism. Our approach departs from prior unsupervised or monolingual pipelines by leveraging explicit toxic word annotation via the multilingual_toxic_lexicon to guide detoxification with greater precision and cross-lingual generalization. Our final model achieves the highest STA (0.922) from our previous attempts, and an average official J score of 0.612 for toxic inputs in both the development and test sets. It also achieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance outperforms baseline and backtranslation methods across multiple languages, and shows strong generalization in high-resource settings (English, Russian, French). Despite some trade-offs in SIM, the model demonstrates consistent improvements in detoxification strength. In the competition, our team achieved ninth place with a score of 0.612.

</details>


### [30] [Evaluating Code-Mixing in LLMs Across 18 Languages](https://arxiv.org/abs/2507.18791)

*Yilun Yang, Yekun Chai*

**Main category:** cs.CL

**Keywords:** code-mixing, large language models, natural language processing, multilingual, synthetic data generation

**Relevance Score:** 7

**TL;DR:** This paper evaluates LLMs on code-mixed data across 18 languages, revealing underperformance and proposing a novel generation method for synthetic code-mixed texts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Code-mixing poses challenges for natural language processing and current benchmarks do not adequately evaluate LLM performance in this area, despite its significance for multilingual users.

**Method:** The study evaluates LLMs on code-mixed datasets and proposes a method for generating synthetic code-mixed texts by combining word substitution with GPT-4 prompting.

**Key Contributions:**

	1. Comprehensive evaluation of LLMs' performance on code-mixed data across 18 languages.
	2. Proposal of a novel method for generating synthetic code-mixed texts.
	3. Insights into the underperformance of LLMs and suggested improvements.

**Result:** The analysis shows that LLMs consistently underperform on code-mixed datasets that involve multiple language families.

**Limitations:** The existing benchmarks are limited and don't cover enough language pairings and tasks.

**Conclusion:** To enhance performance, the study suggests improvements in training data size, model scale, and few-shot learning for LLMs dealing with code-mixed data.

**Abstract:** Code-mixing, the practice of switching between languages within a conversation, presents unique challenges for traditional natural language processing. Existing benchmarks, such as LinCE and GLUECoS, are limited by narrow language pairings and tasks, failing to adequately evaluate the code-mixing capabilities of large language models (LLMs). Despite the significance of code-mixing for multilingual users, research on LLMs in this context remains limited. Additionally, current methods for generating code-mixed data are underdeveloped. In this paper, we conduct a comprehensive evaluation of LLMs' performance on code-mixed data across 18 languages from seven language families. We also propose a novel approach for generating synthetic code-mixed texts by combining word substitution with GPT-4 prompting. Our analysis reveals consistent underperformance of LLMs on code-mixed datasets involving multiple language families. We suggest that improvements in training data size, model scale, and few-shot learning could enhance their performance.

</details>


### [31] [CueBuddy: helping non-native English speakers navigate English-centric STEM education](https://arxiv.org/abs/2507.18827)

*Pranav Gupta*

**Main category:** cs.CL

**Keywords:** speech translation, lexical cues, keyword spotting, multilingual, technical content

**Relevance Score:** 6

**TL;DR:** CueBuddy offers real-time lexical cues for students in STEM classes to better understand complex English terminology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Many students in the Global South struggle with English technical jargon despite having the necessary scientific knowledge, making it hard for them to keep pace in STEM courses.

**Method:** CueBuddy provides real-time keyword spotting and multilingual glossary lookup to offer lexical support without distracting students.

**Key Contributions:**

	1. Real-time lexical cue provision
	2. Technical keyword spotting
	3. Multilingual glossary integration

**Result:** The approach aims to improve comprehension of technical English terms and enhance learning outcomes for non-fluent English speakers in STEM.

**Limitations:** Current models can be expensive and may struggle with complex technical content.

**Conclusion:** CueBuddy could be a valuable tool to support students facing language barriers in technical education, with further development needed for scalability and effectiveness.

**Abstract:** Students across the world in STEM classes, especially in the Global South, fall behind their peers who are more fluent in English, despite being at par with them in terms of scientific prerequisites. While many of them are able to follow everyday English at ease, key terms in English stay challenging. In most cases, such students have had most of their course prerequisites in a lower resource language. Live speech translation to lower resource languages is a promising area of research, however, models for speech translation can be too expensive on a large scale and often struggle with technical content. In this paper, we describe CueBuddy, which aims to remediate these issues by providing real-time "lexical cues" through technical keyword spotting along real-time multilingual glossary lookup to help students stay up to speed with complex English jargon without disrupting their concentration on the lecture. We also describe the limitations and future extensions of our approach.

</details>


### [32] [PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning](https://arxiv.org/abs/2507.18857)

*Mohammad Kachuee, Teja Gollapudi, Minseok Kim, Yin Huang, Kai Sun, Xiao Yang, Jiaqi Wang, Nirav Shah, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, fine-tuning framework, large language models

**Relevance Score:** 9

**TL;DR:** PrismRAG is a fine-tuning framework for improving retrieval-augmented generation (RAG) by addressing issues with irrelevant context and enhancing reasoning capabilities of large language models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** RAG models struggle with confusing context and require deep understanding for answering complex questions. There is a need for improved methods to enhance model performance in these areas.

**Method:** PrismRAG trains models using distractor-aware QA pairs that integrate relevant evidence and subtle distractions, promoting reasoning and synthesis capabilities without extensive human instructions.

**Key Contributions:**

	1. Introduction of the PrismRAG framework
	2. Use of distractor-aware QA pairs for training
	3. Improved reasoning and factuality in RAG tasks

**Result:** PrismRAG shows a 5.4% improvement in average factuality across 12 open-book RAG QA benchmarks compared to state-of-the-art models.

**Limitations:** 

**Conclusion:** The framework demonstrates significant performance improvements in diverse QA scenarios by enhancing the model's reasoning-centric abilities.

**Abstract:** Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.

</details>


### [33] [MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service](https://arxiv.org/abs/2507.18884)

*Ming Gong, Xucheng Huang, Ziheng Xu, Vijayan K. Asari*

**Main category:** cs.CL

**Keywords:** dialogue systems, e-commerce, large language models, imitation learning, reinforcement learning

**Relevance Score:** 8

**TL;DR:** MindFlow+ is a self-evolving dialogue agent that enhances e-commerce customer service interactions using LLMs combined with imitation learning and offline reinforcement learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional intent-based systems in managing dynamic, multi-turn interactions in e-commerce customer service.

**Method:** MindFlow+ leverages large language models, imitation learning, and offline reinforcement learning, introducing data-centric mechanisms like tool-augmented demonstration construction and reward-conditioned data modeling.

**Key Contributions:**

	1. Introduction of MindFlow+, a self-evolving dialogue agent
	2. Development of the AI Contribution Ratio metric for dialogue
	3. Implementation of novel data-centric learning mechanisms

**Result:** MindFlow+ significantly outperforms strong baselines in terms of contextual relevance, flexibility, and task accuracy in real-world e-commerce conversations.

**Limitations:** 

**Conclusion:** The integration of LLMs with tool reasoning and reward-guided learning showcases the potential for developing domain-specialized, context-aware dialogue systems.

**Abstract:** High-quality dialogue is crucial for e-commerce customer service, yet traditional intent-based systems struggle with dynamic, multi-turn interactions. We present MindFlow+, a self-evolving dialogue agent that learns domain-specific behavior by combining large language models (LLMs) with imitation learning and offline reinforcement learning (RL). MindFlow+ introduces two data-centric mechanisms to guide learning: tool-augmented demonstration construction, which exposes the model to knowledge-enhanced and agentic (ReAct-style) interactions for effective tool use; and reward-conditioned data modeling, which aligns responses with task-specific goals using reward signals. To evaluate the model's role in response generation, we introduce the AI Contribution Ratio, a novel metric quantifying AI involvement in dialogue. Experiments on real-world e-commerce conversations show that MindFlow+ outperforms strong baselines in contextual relevance, flexibility, and task accuracy. These results demonstrate the potential of combining LLMs tool reasoning, and reward-guided learning to build domain-specialized, context-aware dialogue systems.

</details>


### [34] [NUTMEG: Separating Signal From Noise in Annotator Disagreement](https://arxiv.org/abs/2507.18890)

*Jonathan Ivey, Susan Gauch, David Jurgens*

**Main category:** cs.CL

**Keywords:** NLP, annotator disagreement, Bayesian model, data aggregation, human-labeled data

**Relevance Score:** 8

**TL;DR:** NUTMEG is a Bayesian model that improves annotation quality by distinguishing between signal and noise in annotator disagreements, enhancing training data for NLP models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of conflicting annotations in NLP models due to varying annotator skills and perspectives, leading to errors in traditional aggregation methods.

**Method:** NUTMEG uses Bayesian inference to incorporate annotator backgrounds, effectively filtering out noisy annotations while preserving systematic disagreements.

**Key Contributions:**

	1. Introduction of NUTMEG, a new Bayesian model for annotation aggregation
	2. Demonstration of improvements in recovery of ground-truth through systematic disagreement preservation
	3. Analysis of how annotator background affects annotation quality

**Result:** NUTMEG outperforms traditional aggregation methods by accurately recovering ground-truth data from annotations and improving the performance of downstream models trained on this data.

**Limitations:** 

**Conclusion:** The findings underscore the necessity of considering annotator competence and systematic disagreements for better training datasets in NLP tasks.

**Abstract:** NLP models often rely on human-labeled data for training and evaluation. Many approaches crowdsource this data from a large number of annotators with varying skills, backgrounds, and motivations, resulting in conflicting annotations. These conflicts have traditionally been resolved by aggregation methods that assume disagreements are errors. Recent work has argued that for many tasks annotators may have genuine disagreements and that variation should be treated as signal rather than noise. However, few models separate signal and noise in annotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model that incorporates information about annotator backgrounds to remove noisy annotations from human-labeled training data while preserving systematic disagreements. Using synthetic data, we show that NUTMEG is more effective at recovering ground-truth from annotations with systematic disagreement than traditional aggregation methods. We provide further analysis characterizing how differences in subpopulation sizes, rates of disagreement, and rates of spam affect the performance of our model. Finally, we demonstrate that downstream models trained on NUTMEG-aggregated data significantly outperform models trained on data from traditionally aggregation methods. Our results highlight the importance of accounting for both annotator competence and systematic disagreements when training on human-labeled data.

</details>


### [35] [REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?](https://arxiv.org/abs/2507.18901)

*Chuxuan Hu, Liyun Zhang, Yeji Lim, Aum Wadhwani, Austin Peters, Daniel Kang*

**Main category:** cs.CL

**Keywords:** reproducibility, AI agents, social science, benchmarking, REPRO-Bench

**Relevance Score:** 4

**TL;DR:** The paper introduces REPRO-Bench, a benchmarking tool for assessing the reproducibility of social science research using AI agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to automate the costly process of assessing the reproducibility of social science papers due to current manual methods being inefficient.

**Method:** The authors developed REPRO-Bench, a collection of 112 task instances where AI agents assess the reproducibility of social science papers based on original PDFs and reproduction packages.

**Key Contributions:**

	1. Introduction of REPRO-Bench for benchmarking reproducibility assessments
	2. Evaluation of AI agents against realistic tasks
	3. Development of REPRO-Agent which significantly improves accuracy

**Result:** The evaluation of three AI agents on REPRO-Bench shows the best performance at only 21.4% accuracy, leading to the development of REPRO-Agent, which improves accuracy by 71%.

**Limitations:** Limited to social science papers and may not generalize across other fields.

**Conclusion:** The study concludes that there is a necessity for more sophisticated AI agents to enhance the automation of reproducibility assessments in social science research.

**Abstract:** Assessing the reproducibility of social science papers is essential for promoting rigor in research processes, but manual assessment is costly. With recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate their capability to automate this process. However, existing benchmarks for reproducing research papers (1) focus solely on reproducing results using provided code and data without assessing their consistency with the paper, (2) oversimplify real-world scenarios, and (3) lack necessary diversity in data formats and programming languages. To address these issues, we introduce REPRO-Bench, a collection of 112 task instances, each representing a social science paper with a publicly available reproduction report. The agents are tasked with assessing the reproducibility of the paper based on the original paper PDF and the corresponding reproduction package. REPRO-Bench features end-to-end evaluation tasks on the reproducibility of social science papers with complexity comparable to real-world assessments. We evaluate three representative AI agents on REPRO-Bench, with the best-performing agent achieving an accuracy of only 21.4%. Building on our empirical analysis, we develop REPRO-Agent, which improves the highest accuracy achieved by existing agents by 71%. We conclude that more advanced AI agents should be developed to automate real-world reproducibility assessment. REPRO-Bench is publicly available at https://github.com/uiuc-kang-lab/REPRO-Bench.

</details>


### [36] [SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models](https://arxiv.org/abs/2507.18902)

*Hongyuan Lu, Zixuan Li, Zefan Zhang, Wai Lam*

**Main category:** cs.CL

**Keywords:** Large Language Models, translation, dictionary-based methods, token consumption, automatic dictionary selection

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel task called Automatic Dictionary Selection (ADS) and a method named Select Low-frequency Words (SLoW) to enhance translation in Large Language Models by selecting dictionaries based on low-frequency words, leading to improved performance with reduced token consumption.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of current LLMs that support only hundreds of languages and the inefficiency of using all available dictionaries for translation.

**Method:** The authors propose the Select Low-frequency Words (SLoW) method, which automatically selects dictionaries that have lower frequency words, improving translation without needing access to training data for frequency estimation.

**Key Contributions:**

	1. Introduction of a novel task called Automatic Dictionary Selection (ADS).
	2. Development of the SLoW method that effectively selects lower frequency dictionaries for translation improvements.
	3. Demonstration of significant performance gains with reduced token usage across multiple languages.

**Result:** Experimental results across 100 languages show that SLoW outperforms strong baselines and significantly reduces token usage, often exceeding the performance of using full dictionary baselines.

**Limitations:** The study does not address the potential performance in languages not covered by the dataset used or the effectiveness across contexts requiring different types of dictionaries.

**Conclusion:** The SLoW method provides an effective means of enhancing translation performance in LLMs while saving token usage, making it a flexible and efficient alternative to traditional dictionary-based approaches.

**Abstract:** There are more than 7,000 languages around the world, and current Large Language Models (LLMs) only support hundreds of languages. Dictionary-based prompting methods can enhance translation on them, but most methods use all the available dictionaries, which could be expensive. Instead, it will be flexible to have a trade-off between token consumption and translation performance. This paper proposes a novel task called \textbf{A}utomatic \textbf{D}ictionary \textbf{S}election (\textbf{ADS}). The goal of the task is to automatically select which dictionary to use to enhance translation. We propose a novel and effective method which we call \textbf{S}elect \textbf{Lo}w-frequency \textbf{W}ords! (\textbf{SLoW}) which selects those dictionaries that have a lower frequency. Our methods have unique advantages. First, there is no need for access to the training data for frequency estimation (which is usually unavailable). Second, it inherits the advantage of dictionary-based methods, where no additional tuning is required on LLMs. Experimental results on 100 languages from FLORES indicate that SLoW surpasses strong baselines, and it can obviously save token usage, with many languages even surpassing the translation performance of the full dictionary baseline.\footnote{A shocking fact is that there is no need to use the actual training data (often unobtainable) for frequency estimation, and an estimation frequency obtained using public resources is still apparently effective in improving translation with ChatGPT and Llama, and DeepSeek.}\footnote{Code and data available upon publication.}

</details>


### [37] [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)

*Rachel L. Draelos, Samina Afreen, Barbara Blasko, Tiffany Brazile, Natasha Chase, Dimple Desai, Jessica Evert, Heather L. Gardner, Lauren Herrmann, Aswathy Vaikom House, Stephanie Kass, Marianne Kavan, Kirshma Khemani, Amanda Koire, Lauren M. McDonald, Zahraa Rabeeah, Amy Shah*

**Main category:** cs.CL

**Keywords:** Chatbot safety, Medical advice, Large language models, Health informatics, Patient safety

**Relevance Score:** 9

**TL;DR:** This study evaluates the safety of four large language model chatbots used for medical advice, revealing significant variations in the quality and safety of their responses.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The use of large language model chatbots in providing medical advice raises patient safety concerns that need rigorous evaluation.

**Method:** This research compared the responses of four chatbots—Claude, Gemini, GPT-4o, and Llama3-70B—using HealthAdvice, a new dataset consisting of 888 responses to 222 medical questions across various primary care topics.

**Key Contributions:**

	1. Introduces a new dataset (HealthAdvice) for evaluating chatbot safety in medical contexts.
	2. Quantifies the rate of unsafe advice provided by various chatbots.
	3. Highlights significant variability in chatbot response quality, with implications for patient safety.

**Result:** Statistical analysis showed significant differences among chatbots, with problematic response rates from 21.6% (Claude) to 43.2% (Llama), indicating a risk of unsafe medical advice being provided.

**Limitations:** Limited to four chatbots and specific primary care topics; broader implications may require additional studies across other domains.

**Conclusion:** The study underscores the necessity for improved safety measures in chatbot responses, as millions of patients could be receiving dangerous medical advice.

**Abstract:** Millions of patients are already using large language model (LLM) chatbots for medical advice on a regular basis, raising patient safety concerns. This physician-led red-teaming study compares the safety of four publicly available chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation framework that enables quantitative and qualitative analysis. In total, 888 chatbot responses are evaluated for 222 patient-posed advice-seeking medical questions on primary care topics spanning internal medicine, women's health, and pediatrics. We find statistically significant differences between chatbots. The rate of problematic responses varies from 21.6 percent (Claude) to 43.2 percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13 percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the potential to lead to serious patient harm. This study suggests that millions of patients could be receiving unsafe medical advice from publicly available chatbots, and further work is needed to improve the clinical safety of these powerful tools.

</details>


### [38] [A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions](https://arxiv.org/abs/2507.18910)

*Agada Joseph Oche, Ademola Glory Folashade, Tirthankar Ghosal, Arpan Biswas*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Natural Language Processing, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper is a systematic review of Retrieval-Augmented Generation (RAG) in NLP, detailing its evolution, technical components, and applications in various domains.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind RAG is to improve factual grounding and accuracy in LLMs while addressing issues like hallucinations and outdated knowledge.

**Method:** The review analyzes the evolution of RAG from early open domain question answering to modern implementations, examining retrieval mechanisms, generation models, and fusion strategies with a year-by-year analysis of trends.

**Key Contributions:**

	1. Comprehensive systematic review of RAG's evolution and applications
	2. In-depth analysis of technical components and challenges
	3. Benchmarking of RAG implementations across different metrics

**Result:** The study benchmarks various RAG implementations on metrics like retrieval accuracy, generation fluency, and computational efficiency, revealing persistent challenges in retrieval quality and privacy.

**Limitations:** The review may not cover all recent developments in RAG and specific case studies due to the rapidly evolving nature of the field.

**Conclusion:** The paper concludes by highlighting recent innovations and emerging solutions in RAG that promise to enhance the efficiency and effectiveness of NLP systems.

**Abstract:** Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.

</details>


### [39] [Mining Contextualized Visual Associations from Images for Creativity Understanding](https://arxiv.org/abs/2507.18915)

*Ananya Sahu, Amith Ananthram, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** vision-language models, image-text retrieval, creative captions

**Relevance Score:** 8

**TL;DR:** This paper presents a novel method for mining contextualized associations from images to generate scalable, creative captions, enhancing vision-language model capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of creative outputs in vision-language models by providing a shared language of association, overcoming limitations of existing datasets.

**Method:** The authors introduce a method that identifies and mines contextual associations for salient visual elements in images, allowing for the generation of creative captions at various abstraction levels.

**Key Contributions:**

	1. Developed a new method for mining contextual associations from images
	2. Created a dataset of 1.7 million creative captions for MSCOCO
	3. Demonstrated significant improvements in zero-shot retrieval in poetry and metaphor domains

**Result:** They produce a dataset containing 1.7 million creative captions linked to images in MSCOCO, which have been evaluated to retain visual grounding while increasing abstraction.

**Limitations:** The effectiveness of the method may be limited to certain types of visual contexts; further evaluations in different domains may be necessary.

**Conclusion:** Fine-tuning a visual encoder on the new dataset significantly enhances performance in zero-shot image-text retrieval tasks in creative contexts like poetry and metaphor visualization.

**Abstract:** Understanding another person's creative output requires a shared language of association. However, when training vision-language models such as CLIP, we rely on web-scraped datasets containing short, predominantly literal, alt-text. In this work, we introduce a method for mining contextualized associations for salient visual elements in an image that can scale to any unlabeled dataset. Given an image, we can use these mined associations to generate high quality creative captions at increasing degrees of abstraction. With our method, we produce a new dataset of visual associations and 1.7m creative captions for the images in MSCOCO. Human evaluation confirms that these captions remain visually grounded while exhibiting recognizably increasing abstraction. Moreover, fine-tuning a visual encoder on this dataset yields meaningful improvements in zero-shot image-text retrieval in two creative domains: poetry and metaphor visualization. We release our dataset, our generation code and our models for use by the broader community.

</details>


### [40] [Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders](https://arxiv.org/abs/2507.18918)

*Richmond Sin Jing Xuan, Jalil Huseynov, Yang Zhang*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Activation Patterns, Fine-tuning, Low-Rank Adaptation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper analyzes activation disparities in multilingual LLMs, focusing on low and medium resource languages and proposes an activation-aware fine-tuning method to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the underperformance of medium to low resource languages in multilingual LLM benchmarks and identify activation patterns that contribute to these disparities.

**Method:** Activation patterns in the Gemma-2-2B LLM were analyzed using Sparse Autoencoders across 10 languages, followed by activation-aware fine-tuning through Low-Rank Adaptation to improve performance.

**Key Contributions:**

	1. Identification of activation disparities in multilingual LLMs across different languages
	2. Development of an activation-aware fine-tuning method using LoRA
	3. Demonstration of improved multilingual performance post fine-tuning

**Result:** Medium to low resource languages showed significantly lower activations, yet after applying fine-tuning, substantial activation gains were observed, leading to modest improvements in benchmark results.

**Limitations:** 

**Conclusion:** Activation alignment is critical for improving the performance of multilingual LLMs, particularly for medium to low resource languages.

**Abstract:** Multilingual large language models (LLMs) exhibit strong cross-linguistic generalization, yet medium to low resource languages underperform on common benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to low resource languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam (ml), and Hindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic disparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower activations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this, we apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial activation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining English retention at approximately 91 percent. After fine-tuning, benchmark results show modest but consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual LLM performance.

</details>


### [41] [LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation](https://arxiv.org/abs/2507.18940)

*Jingxuan Wei, Caijun Jia, Qi Chen, Yujun Cai, Linzhuang Sun, Xiangxiang Zhang, Gaowei Wu, Bihui Yu*

**Main category:** cs.CL

**Keywords:** multimodal translation, multilingual settings, machine learning, language-specific representation, cross-lingual adaptation

**Relevance Score:** 6

**TL;DR:** LLaVA-NeuMT is a novel multimodal multilingual translation framework that improves translation quality by modeling language-specific and language-agnostic representations, effectively mitigating multilingual interference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Multimodal Machine Translation (MMT) quality in multilingual settings, addressing issues like cross-lingual interference and ineffective parameter-sharing strategies.

**Method:** The framework includes a layer selection mechanism for identifying informative layers and a neuron-level adaptation strategy for selecting language-specific and agnostic neurons.

**Key Contributions:**

	1. Introduction of the LLaVA-NeuMT framework for multimodal multilingual translation.
	2. Development of a layer selection mechanism and neuron-level adaptation strategy.
	3. Achievement of state-of-the-art results with less parameter fine-tuning.

**Result:** LLaVA-NeuMT, fine-tuning only 40% of the model parameters, surpasses full fine-tuning approaches, achieving state-of-the-art results on the M3-Multi30K and M3-AmbigCaps datasets.

**Limitations:** 

**Conclusion:** The approach provides an efficient and scalable solution to cross-lingual adaptation in multimodal translation, demonstrating the importance of selected layers and neurons in improving translation quality.

**Abstract:** Multimodal Machine Translation (MMT) enhances translation quality by incorporating visual context, helping to resolve textual ambiguities. While existing MMT methods perform well in bilingual settings, extending them to multilingual translation remains challenging due to cross-lingual interference and ineffective parameter-sharing strategies. To address this, we propose LLaVA-NeuMT, a novel multimodal multilingual translation framework that explicitly models language-specific and language-agnostic representations to mitigate multilingual interference. Our approach consists of a layer selection mechanism that identifies the most informative layers for different language pairs and a neuron-level adaptation strategy that dynamically selects language-specific and agnostic neurons to improve translation quality while reducing redundancy. We conduct extensive experiments on the M3-Multi30K and M3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only 40\% of the model parameters, surpasses full fine-tuning approaches and ultimately achieves SOTA results on both datasets. Our analysis further provides insights into the importance of selected layers and neurons in multimodal multilingual adaptation, offering an efficient and scalable solution to cross-lingual adaptation in multimodal translation.

</details>


### [42] [Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection](https://arxiv.org/abs/2507.18952)

*Yongjie Li, Ruilin Nong, Jianan Liu, Lucas Evans*

**Main category:** cs.CL

**Keywords:** legal document summarization, natural language processing, machine learning, judicial efficiency, automation

**Relevance Score:** 4

**TL;DR:** This paper presents a method for legal document summarization using advanced NLP and ML techniques to automate data extraction, improving judicial efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance judicial efficiency by automating the detection and extraction of key information from legal texts.

**Method:** The framework employs state-of-the-art NLP and machine learning algorithms to recognize patterns in judicial documents and generate precise summaries.

**Key Contributions:**

	1. Introduction of a novel framework for legal document summarization
	2. Demonstration of improved operational efficiency through automated summarization
	3. Validation of the approach using real legal datasets

**Result:** Experiments on actual legal datasets show that the method creates high-quality summaries that preserve the integrity of the original content and improve processing times.

**Limitations:** 

**Conclusion:** The research demonstrates that automation can significantly improve workflow dynamics in the legal sector, allowing legal practitioners to focus on critical analysis and decision-making.

**Abstract:** Legal document summarization represents a significant advancement towards improving judicial efficiency through the automation of key information detection. Our approach leverages state-of-the-art natural language processing techniques to meticulously identify and extract essential data from extensive legal texts, which facilitates a more efficient review process. By employing advanced machine learning algorithms, the framework recognizes underlying patterns within judicial documents to create precise summaries that encapsulate the crucial elements. This automation alleviates the burden on legal professionals, concurrently reducing the likelihood of overlooking vital information that could lead to errors. Through comprehensive experiments conducted with actual legal datasets, we demonstrate the capability of our method to generate high-quality summaries while preserving the integrity of the original content and enhancing processing times considerably. The results reveal marked improvements in operational efficiency, allowing legal practitioners to direct their efforts toward critical analytical and decision-making activities instead of manual reviews. This research highlights promising technology-driven strategies that can significantly alter workflow dynamics within the legal sector, emphasizing the role of automation in refining judicial processes.

</details>


### [43] [A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)

*Sang Min Jung, Kaixiang Zhang, Cristian Danescu-Niculescu-Mizil*

**Main category:** cs.CL

**Keywords:** conversation dynamics, similarity measure, conversational agents, interactional patterns, situational power

**Relevance Score:** 7

**TL;DR:** This paper introduces a new similarity measure for comparing the dynamics of conversations, highlighting the need for a robust method to analyze conversational data more holistically.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of automated methods for comparing conversations based on their interactional dynamics, which can enhance the evaluation of conversational agents.

**Method:** The authors propose a similarity measure for conversations and design a validation framework to test its robustness and sensitivity to conversation topics.

**Key Contributions:**

	1. Introduction of a similarity measure for conversation dynamics.
	2. Development of a validation framework for metric assessment.
	3. Analysis of conversational patterns in online communities.

**Result:** The similarity measure successfully captured differences in conversation dynamics and was applied to analyze conversational patterns in an online community, illustrating its effectiveness.

**Limitations:** The study may be limited by the specific contexts of the conversations analyzed and the generalizability of the findings to other types of interactions.

**Conclusion:** The study highlights the importance of situational power in conversations and demonstrates that the proposed measure provides valuable insights into interactional dynamics.

**Abstract:** The quality of a conversation goes beyond the individual quality of each reply, and instead emerges from how these combine into interactional patterns that give the conversation its distinctive overall "shape". However, there is no robust automated method for comparing conversations in terms of their overall interactional dynamics. Such methods could enhance the analysis of conversational data and help evaluate conversational agents more holistically.   In this work, we introduce a similarity measure for comparing conversations with respect to their dynamics. We design a validation framework for testing the robustness of the metric in capturing differences in conversation dynamics and for assessing its sensitivity to the topic of the conversations. Finally, to illustrate the measure's utility, we use it to analyze conversational dynamics in a large online community, bringing new insights into the role of situational power in conversations.

</details>


### [44] [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)

*Bohan Yao, Vikas Yadav*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Multi-Tool Aggregation, Performance Improvement, Inference-only Framework

**Relevance Score:** 9

**TL;DR:** This paper introduces Multi-TAG, a framework for improving mathematical reasoning in LLMs by concurrently using multiple tools, leading to enhanced accuracy and robustness without the need for finetuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing tool-augmented methods for LLMs struggle with complex math reasoning due to their reliance on a single tool, necessitating a more robust approach.

**Method:** Multi-TAG guides an LLM to invoke multiple tools at each reasoning step and aggregates their outputs to enhance solution accuracy.

**Key Contributions:**

	1. Introduction of a multi-tool aggregation approach for LLMs
	2. Improved accuracy and robustness in complex mathematical reasoning
	3. No finetuning required for implementation across different LLM backbones

**Result:** Multi-TAG shows consistent improvements over state-of-the-art methods on challenging math benchmarks, achieving 6.0% to 7.5% higher performance.

**Limitations:** 

**Conclusion:** The framework's finetuning-free nature allows it to be applied easily across various LLMs, making it a flexible solution for improving mathematical problem-solving.

**Abstract:** Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.

</details>


### [45] [Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement](https://arxiv.org/abs/2507.19081)

*Hao Li, Yizheng Sun, Viktor Schlegel, Kailai Yang, Riza Batista-Navarro, Goran Nenadic*

**Main category:** cs.CL

**Keywords:** argument summarization, large language model, iteration, sufficiency, remasking

**Relevance Score:** 5

**TL;DR:** This paper proposes Arg-LLaDA, an iterative framework for argument summarization that improves the generation of concise and structured debate representations by addressing the limitations of existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The generation stage of argument summarization has been underexplored, particularly in supporting factual correction and structural refinement.

**Method:** Arg-LLaDA utilizes a large language diffusion framework that iteratively improves argument summaries through sufficiency-guided remasking and regeneration, using a flexible masking controller and a sufficiency-checking module.

**Key Contributions:**

	1. Introduction of Arg-LLaDA, a novel framework for argument summarization.
	2. Utilization of sufficiency-guided remasking and regeneration for iterative improvement.
	3. Empirical validation of performance surpassing state-of-the-art baselines and positive human evaluation results.

**Result:** On two benchmark datasets, Arg-LLaDA outperforms state-of-the-art baselines in 7 out of 10 automatic evaluation metrics, and human evaluations indicate significant improvements in coverage, faithfulness, and conciseness.

**Limitations:** 

**Conclusion:** The iterative, sufficiency-aware generation strategy of Arg-LLaDA validates its effectiveness in enhancing the quality of argument summaries.

**Abstract:** Argument summarization aims to generate concise, structured representations of complex, multi-perspective debates. While recent work has advanced the identification and clustering of argumentative components, the generation stage remains underexplored. Existing approaches typically rely on single-pass generation, offering limited support for factual correction or structural refinement. To address this gap, we introduce Arg-LLaDA, a novel large language diffusion framework that iteratively improves summaries via sufficiency-guided remasking and regeneration. Our method combines a flexible masking controller with a sufficiency-checking module to identify and revise unsupported, redundant, or incomplete spans, yielding more faithful, concise, and coherent outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA surpasses state-of-the-art baselines in 7 out of 10 automatic evaluation metrics. In addition, human evaluations reveal substantial improvements across core dimensions, coverage, faithfulness, and conciseness, validating the effectiveness of our iterative, sufficiency-aware generation strategy.

</details>


### [46] [Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents](https://arxiv.org/abs/2507.19090)

*Haorui He, Yupeng Li, Dacheng Wen, Reynold Cheng, Francis C. M. Lau*

**Main category:** cs.CL

**Keywords:** claim verification, debate-driven methodology, large language models, digital literacy, synthetic data

**Relevance Score:** 7

**TL;DR:** DebateCV introduces a debate-driven framework for claim verification using multiple LLM agents to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing digital literacy through effective claim verification, addressing the limitations of single-LLM methods in handling complex claims.

**Method:** A debate-driven methodology with two Debaters arguing opposing views and a Moderator evaluating the arguments, supplemented by synthetic debate data for improvement.

**Key Contributions:**

	1. First debate-driven claim verification framework using multiple LLM agents
	2. Introduction of a Moderator role for evaluating debates
	3. Innovative post-training strategy using synthetic data to enhance model performance

**Result:** DebateCV outperforms existing claim verification methods across various levels of evidence quality.

**Limitations:** 

**Conclusion:** The proposed framework effectively addresses the challenges of claim verification through multi-agent interaction and novel training strategies.

**Abstract:** Claim verification is critical for enhancing digital literacy. However, the state-of-the-art single-LLM methods struggle with complex claim verification that involves multi-faceted evidences. Inspired by real-world fact-checking practices, we propose DebateCV, the first claim verification framework that adopts a debate-driven methodology using multiple LLM agents. In our framework, two Debaters take opposing stances on a claim and engage in multi-round argumentation, while a Moderator evaluates the arguments and renders a verdict with justifications. To further improve the performance of the Moderator, we introduce a novel post-training strategy that leverages synthetic debate data generated by the zero-shot DebateCV, effectively addressing the scarcity of real-world debate-driven claim verification data. Experimental results show that our method outperforms existing claim verification methods under varying levels of evidence quality. Our code and dataset are publicly available at https://anonymous.4open.science/r/DebateCV-6781.

</details>


### [47] [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](https://arxiv.org/abs/2507.19117)

*Swapnil Hingmire, Ze Shi Li, Shiyu, Zeng, Ahmed Musa Awon, Luiz Franciscatto Guerra, Neil Ernst*

**Main category:** cs.CL

**Keywords:** topic interpretation, user studies, cognitive biases, evaluation measures, heuristics

**Relevance Score:** 8

**TL;DR:** This paper explores how users interpret topics in order to improve evaluation measures of topic quality, proposing new constructs based on user studies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** State-of-the-art evaluation measures for topic quality do not address how topics facilitate exploration in user contexts. Understanding user interpretations of topics can enhance these measures.

**Method:** The authors conducted user studies to gather user assessments of topic constructs, employing reflexive thematic analysis to identify interpretation themes.

**Key Contributions:**

	1. Proposed new constructs of topic quality based on user interpretations
	2. Demonstrated that users use cognitive heuristics in topic interpretation
	3. Introduced a theory linking topic interpretation to cognitive biases.

**Result:** It was found that users rely on availability and representativeness heuristics for topic interpretation, suggesting the need for user models that account for cognitive biases.

**Limitations:** The study is based on user studies that may not encompass all demographics or contexts; thus, results may not generalize universally.

**Conclusion:** The research concluded that topic interpretation involves judgment under uncertainty and that user models should incorporate cognitive biases to improve evaluation frameworks.

**Abstract:** Interpretation of topics is crucial for their downstream applications. State-of-the-art evaluation measures of topic quality such as coherence and word intrusion do not measure how much a topic facilitates the exploration of a corpus. To design evaluation measures grounded on a task, and a population of users, we do user studies to understand how users interpret topics. We propose constructs of topic quality and ask users to assess them in the context of a topic and provide rationale behind evaluations. We use reflexive thematic analysis to identify themes of topic interpretations from rationales. Users interpret topics based on availability and representativeness heuristics rather than probability. We propose a theory of topic interpretation based on the anchoring-and-adjustment heuristic: users anchor on salient words and make semantic adjustments to arrive at an interpretation. Topic interpretation can be viewed as making a judgment under uncertainty by an ecologically rational user, and hence cognitive biases aware user models and evaluation frameworks are needed.

</details>


### [48] [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)

*Rachel L. Draelos, Samina Afreen, Barbara Blasko, Tiffany Brazile, Natasha Chase, Dimple Desai, Jessica Evert, Heather L. Gardner, Lauren Herrmann, Aswathy Vaikom House, Stephanie Kass, Marianne Kavan, Kirshma Khemani, Amanda Koire, Lauren M. McDonald, Zahraa Rabeeah, Amy Shah*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chatbots, Medical Safety, Patient Advice, AI in Health

**Relevance Score:** 9

**TL;DR:** This study evaluates the safety of four LLM chatbots in providing medical advice, revealing significant variation in problematic responses and the potential for patient harm.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the safety of LLM chatbots used by millions for medical advice, addressing concerns about patient safety.

**Method:** A red-teaming study comparing the responses of Claude, Gemini, GPT-4o, and Llama3-70B using a new dataset (HealthAdvice) and an evaluative framework for both quantitative and qualitative analysis.

**Key Contributions:**

	1. Comparison of four major LLM chatbots in a medical context.
	2. Quantitative and qualitative dataset of chatbot responses to patient inquiries.
	3. Identification of safety issues related to chatbot medical advice.

**Result:** Evaluation of 888 responses to 222 medical advice-seeking questions showed problematic response rates ranging from 21.6% to 43.2% across chatbots; unsafe response rates ranged from 5% to 13%.

**Limitations:** Focused solely on four chatbots and specific medical topics; may not generalize to all LLMs or medical contexts.

**Conclusion:** The findings indicate a pressing need for improvements in the clinical safety of LLM chatbots to prevent potential patient harm.

**Abstract:** Millions of patients are already using large language model (LLM) chatbots for medical advice on a regular basis, raising patient safety concerns. This physician-led red-teaming study compares the safety of four publicly available chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation framework that enables quantitative and qualitative analysis. In total, 888 chatbot responses are evaluated for 222 patient-posed advice-seeking medical questions on primary care topics spanning internal medicine, women's health, and pediatrics. We find statistically significant differences between chatbots. The rate of problematic responses varies from 21.6 percent (Claude) to 43.2 percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13 percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the potential to lead to serious patient harm. This study suggests that millions of patients could be receiving unsafe medical advice from publicly available chatbots, and further work is needed to improve the clinical safety of these powerful tools.

</details>


### [49] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)

*Gioele Giachino, Marco Rondina, Antonio Vetrò, Riccardo Coppola, Juan Carlos De Martin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias, Gender Stereotypes, AI Ethics, Job Roles

**Relevance Score:** 9

**TL;DR:** This study investigates gender and professional bias in Large Language Models (LLMs) by analyzing their responses to ungendered prompts using a structured experimental method.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing concern about biases in AI-generated content, particularly in professional contexts, and highlights the need for understanding these biases in non-English languages.

**Method:** A structured experimental method was employed using different ungendered prompts related to three professional job combinations, with responses collected from OpenAI ChatGPT and Google Gemini via APIs.

**Key Contributions:**

	1. Analyzes bias in LLM responses to ungendered prompts
	2. Demonstrates significant gender bias linked to professional roles in AI-generated content
	3. Highlights limitations of LLMs in non-English contexts, particularly regarding grammatical gender

**Result:** The study found that LLM responses often perpetuate gender stereotypes, with analysis revealing that Gemini assigned 'she' pronouns to the 'assistant' role 100% of the time, compared to 97% for ChatGPT in similar contexts.

**Limitations:** The study is limited to two LLMs and focuses on the Italian language, which may restrict the generalizability of the results.

**Conclusion:** The findings raise ethical concerns about the implications of biased AI-generated text in areas like employment, underscoring the necessity for mitigation strategies to promote equitable outcomes.

**Abstract:** The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.

</details>


### [50] [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)

*Chaymaa Abbas, Mariette Awad, Razane Tajeddine*

**Main category:** cs.CL

**Keywords:** Social Bias, Dialectal Variation, Data Poisoning, Language Models, Bias Mitigation

**Relevance Score:** 9

**TL;DR:** The study investigates how dialectal variation affects the toxicity of language model outputs, revealing increased toxicity for AAVE due to data poisoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of dialectal variation on toxicity in language models, specifically how data poisoning affects outputs in AAVE versus SAE.

**Method:** The study utilized small- and medium-scale LLaMA models and assessed the toxicity of outputs based on exposure to poisoned data. GPT-4o was used to analyze disparities in output fairness.

**Key Contributions:**

	1. Identification of increased toxicity in AAVE due to data poisoning
	2. Demonstration of heightened susceptibility in larger language models
	3. Recommendations for dialect-aware evaluation and debiasing interventions

**Result:** Minimal exposure to poisoned data significantly increases toxicity in outputs for AAVE inputs, especially in larger models which exhibit a heightened amplification of these biases.

**Limitations:** Limited to analysis of AAVE and SAE without broader dialectal context.

**Conclusion:** The findings highlight the impact of dialectal bias and data poisoning, advocating for improved evaluation and debiasing methods in language model training.

**Abstract:** Despite the ongoing improvements in the design of large language models (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding and amplifying social biases. This study examines how dialectal variation, specifically African American Vernacular English (AAVE) versus Standard American English (SAE), interacts with data poisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it remains comparatively unaffected for SAE. Larger models exhibit a more significant amplification effect which suggests heightened susceptibility with scale. To further assess these disparities, we employed GPT-4o as a fairness auditor, which identified harmful stereotypical patterns disproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and intellectual inferiority. These findings underscore the compounding impact of data poisoning and dialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions, and socially responsible training protocols during development.

</details>


### [51] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)

*Gioele Giachino, Marco Rondina, Antonio Vetrò, Riccardo Coppola, Juan Carlos De Martin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias, Gender, Chatbots, AI Ethics

**Relevance Score:** 9

**TL;DR:** This study investigates how LLMs can perpetuate gender and professional biases in their responses to ungendered prompts, using a structured experimental method with Italian prompts and analyzing performance of ChatGPT and Gemini.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the perpetuation of biases in AI-generated content, particularly focusing on gender and professional stereotypes.

**Method:** The study employs a structured experimental method, utilizing different ungendered prompts featuring three distinct professional job combinations characterized by hierarchical relationships, analyzing the outputs of OpenAI ChatGPT and Google Gemini.

**Key Contributions:**

	1. Examination of bias in LLM responses to ungendered prompts across languages
	2. Demonstration of bias perpetuation in job role associations
	3. Highlighting ethical concerns regarding AI applications in workplaces.

**Result:** The results show that LLMs significantly perpetuate biases, with Gemini associating 'she' pronouns 100% with the 'assistant' role instead of 'manager' (ChatGPT at 97%).

**Limitations:** Focus is primarily on gender bias in Italian; results may not generalize to other languages or cultural contexts.

**Conclusion:** The findings reveal the significant ethical implications of bias in AI-generated content, stressing the need for developing mitigation strategies to ensure equitable AI outcomes.

**Abstract:** The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.

</details>


### [52] [How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework](https://arxiv.org/abs/2507.19219)

*Zi Liang, Liantong Yu, Shiyu Zhang, Qingqing Ye, Haibo Hu*

**Main category:** cs.CL

**Keywords:** large language models, evaluation framework, benchmark contamination, machine learning, health informatics

**Relevance Score:** 8

**TL;DR:** This paper introduces ArxivRoll, a framework to evaluate large language models (LLMs) by addressing overestimation issues related to public benchmarks through a dynamic generation of private test cases and measurement of contamination and bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Overestimation in evaluating LLMs due to benchmark contamination and imbalanced training has led to unfair comparisons and unrealistic performance assessments.

**Method:** ArxivRoll consists of SCP (an automated generator for private test cases) and Rugged Scores (metrics to measure benchmark contamination and training bias), creating a new benchmark every six months using recent ArXiv articles.

**Key Contributions:**

	1. Introduction of a dynamic evaluation framework for LLMs
	2. Automated generation of private test cases using SCP
	3. Development of Rugged Scores for measuring benchmark contamination and training bias

**Result:** Extensive experiments show ArxivRoll generates high-quality benchmarks and allows for a systematic evaluation of current LLMs.

**Limitations:** 

**Conclusion:** ArxivRoll provides a more reliable evaluation method for LLMs, enhancing reproducibility and transparency while ensuring efficiency.

**Abstract:** Overestimation in evaluating large language models (LLMs) has become an increasing concern. Due to the contamination of public benchmarks or imbalanced model training, LLMs may achieve unreal evaluation results on public benchmarks, either intentionally or unintentionally, which leads to unfair comparisons among LLMs and undermines their realistic capability assessments. Existing benchmarks attempt to address these issues by keeping test cases permanently secret, mitigating contamination through human evaluation, or repeatedly collecting and constructing new samples. However, these approaches fail to ensure reproducibility, transparency, and high efficiency simultaneously. Moreover, the extent of overestimation in current LLMs remains unquantified. To address these issues, we propose ArxivRoll, a dynamic evaluation framework inspired by one-time pad encryption in cryptography. ArxivRoll comprises two key components: \emph{i) SCP (Sequencing, Cloze, and Prediction)}, an automated generator for private test cases, and \emph{ii) Rugged Scores (RS)}, metrics that measure the proportion of public benchmark contamination and training bias. Leveraging SCP, ArxivRoll constructs a new benchmark every six months using recent articles from ArXiv and employs them for one-time evaluations of LLM performance. Extensive experiments demonstrate the high quality of our benchmark, and we provide a systematic evaluation of current LLMs. The source code is available at https://github.com/liangzid/ArxivRoll/.

</details>


### [53] [Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation](https://arxiv.org/abs/2507.19227)

*Yuanhe Zhang, Fangzhou Xie, Zhenhong Zhou, Zherui Li, Hao Chen, Kun Wang, Yufei Guo*

**Main category:** cs.CL

**Keywords:** Large Language Diffusion Models, jailbreak, safety vulnerabilities, harmful generation, Multi-Point Attention Attack

**Relevance Score:** 8

**TL;DR:** This paper reveals vulnerabilities of Large Language Diffusion Models (LLDMs) to jailbreaking attacks and presents a new method called Parallel Decoding jailbreak (PAD) that increases harmful output generation rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There are growing concerns about harmful generations from LLDMs and existing jailbreak methods for LLMs are ineffective against them.

**Method:** The paper introduces a PArallel Decoding jailbreak (PAD) method that utilizes Multi-Point Attention Attack to guide the generative processes of LLDMs towards harmful outputs.

**Key Contributions:**

	1. Introduced a new jailbreak method (PAD) for LLDMs.
	2. Demonstrated that LLDMs are significantly more susceptible to harmful output generation compared to LLMs.
	3. Provided a comprehensive analysis of LLDM architecture for secure deployment.

**Result:** The PAD method achieves a 97% success rate in jailbreaking attempts, demonstrating significant safety vulnerabilities in LLDMs and doubling the speed of harmful generation compared to autoregressive LLMs of the same size.

**Limitations:** 

**Conclusion:** LLDMs require critical insights into their architecture for secure deployment due to their rapid and vulnerable harmful generation capabilities.

**Abstract:** Large Language Diffusion Models (LLDMs) exhibit comparable performance to LLMs while offering distinct advantages in inference speed and mathematical reasoning tasks.The precise and rapid generation capabilities of LLDMs amplify concerns of harmful generations, while existing jailbreak methodologies designed for Large Language Models (LLMs) prove limited effectiveness against LLDMs and fail to expose safety vulnerabilities.Successful defense cannot definitively resolve harmful generation concerns, as it remains unclear whether LLDMs possess safety robustness or existing attacks are incompatible with diffusion-based architectures.To address this, we first reveal the vulnerability of LLDMs to jailbreak and demonstrate that attack failure in LLDMs stems from fundamental architectural differences.We present a PArallel Decoding jailbreak (PAD) for diffusion-based language models. PAD introduces Multi-Point Attention Attack, which guides parallel generative processes toward harmful outputs that inspired by affirmative response patterns in LLMs. Experimental evaluations across four LLDMs demonstrate that PAD achieves jailbreak attack success rates by 97%, revealing significant safety vulnerabilities. Furthermore, compared to autoregressive LLMs of the same size, LLDMs increase the harmful generation speed by 2x, significantly highlighting risks of uncontrolled misuse.Through comprehensive analysis, we provide an investigation into LLDM architecture, offering critical insights for the secure deployment of diffusion-based language models.

</details>


### [54] [Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns](https://arxiv.org/abs/2507.19303)

*Ilias Chalkidis, Stephanie Brandl, Paris Aslanidis*

**Main category:** cs.CL

**Keywords:** Large Language Models, populism, political discourse, RoBERTa, cross-domain analysis

**Relevance Score:** 6

**TL;DR:** This paper explores the ability of Large Language Models (LLMs) to identify and classify nuances of populism in political discourse using curated datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Examine whether LLMs can effectively identify and classify complex forms of populism in discourse.

**Method:** Curating novel datasets focused on populist discourse, evaluating pre-trained language models across various prompting methods.

**Key Contributions:**

	1. Development of novel datasets for populist discourse
	2. Evaluation of performance differences among LLMs in this domain
	3. Insights into populist rhetoric through analysis of campaign speeches

**Result:** A fine-tuned RoBERTa classifier significantly outperformed instruction-tuned LLMs in identifying populist rhetoric; instruction-tuned models showed better performance on out-of-domain data.

**Limitations:** 

**Conclusion:** Fine-tuning improves classification performance, and instruction-tuned models demonstrate robustness across different political contexts.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social science concepts remains underexplored. This paper examines whether LLMs can identify and classify fine-grained forms of populism, a complex and contested concept in both academic and media debates. To this end, we curate and release novel datasets specifically designed to capture populist discourse. We evaluate a range of pre-trained (large) language models, both open-weight and proprietary, across multiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the limitations of LLMs in detecting populist discourse. We find that a fine-tuned RoBERTa classifier vastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our best-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into his strategic use of populist rhetoric. Finally, we assess the generalizability of these models by benchmarking them on campaign speeches by European politicians, offering a lens into cross-context transferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs exhibit greater robustness on out-of-domain data.

</details>


### [55] [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)

*Son Quoc Tran, Tushaar Gangavarapu, Nicholas Chernogor, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil*

**Main category:** cs.CL

**Keywords:** conversation prediction, CGA task, evaluation framework, language modeling, forecasting

**Relevance Score:** 7

**TL;DR:** This paper presents a uniform evaluation framework for the Conversations Gone Awry (CGA) task, which aims to predict the derailment of conversations, providing a benchmark for comparing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automated systems' ability to predict conversation directions and assist in human-human interactions.

**Method:** The authors introduce a benchmark framework for the CGA task, allowing for direct model comparisons and incorporating a new metric for forecasting adaptation during conversations.

**Key Contributions:**

	1. Introduced the first uniform evaluation framework for CGA task.
	2. Created a benchmark for reliable comparisons between architectures.
	3. Developed a novel metric for forecasting adaptation in conversations.

**Result:** The framework provides a comprehensive overview of progress in CGA models and their predictive capabilities in light of recent language modeling advancements.

**Limitations:** 

**Conclusion:** The study fosters improvements in CGA models, allowing for better predictions of conversation derailment through an innovative evaluation method.

**Abstract:** We often rely on our intuition to anticipate the direction of a conversation. Endowing automated systems with similar foresight can enable them to assist human-human interactions. Recent work on developing models with this predictive capacity has focused on the Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this work, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark that enables direct and reliable comparisons between different architectures. This allows us to present an up-to-date overview of the current progress in CGA models, in light of recent advancements in language modeling. Our framework also introduces a novel metric that captures a model's ability to revise its forecast as the conversation progresses.

</details>


### [56] [AutoPCR: Automated Phenotype Concept Recognition by Prompting](https://arxiv.org/abs/2507.19315)

*Yicheng Tao, Yuanhao Huang, Jie Liu*

**Main category:** cs.CL

**Keywords:** phenotype recognition, biomedical text mining, large language models

**Relevance Score:** 9

**TL;DR:** AutoPCR is a prompt-based method for phenotype concept recognition that eliminates the need for ontology-specific training, showing superior performance across diverse biomedical text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing phenotype concept recognition methods that require specific ontologies and struggle with generalization across various text types and biomedical terminology.

**Method:** AutoPCR utilizes a three-stage process: entity extraction with rule-based and neural tagging, candidate retrieval using SapBERT, and entity linking via a large language model prompted with natural language.

**Key Contributions:**

	1. Introduces a prompt-based approach to phenotype CR without ontology-specific training.
	2. Achieves superior performance on benchmark datasets versus existing methods.
	3. Demonstrates generalizability and inductive capabilities for new ontologies.

**Result:** AutoPCR achieved the best average and most robust performance on four benchmark datasets compared to previous state-of-the-art methods.

**Limitations:** 

**Conclusion:** The results indicate that AutoPCR has strong inductive capabilities, demonstrating its effectiveness in adapting to new ontologies.

**Abstract:** Phenotype concept recognition (CR) is a fundamental task in biomedical text mining, enabling applications such as clinical diagnostics and knowledge graph construction. However, existing methods often require ontology-specific training and struggle to generalize across diverse text types and evolving biomedical terminology. We present AutoPCR, a prompt-based phenotype CR method that does not require ontology-specific training. AutoPCR performs CR in three stages: entity extraction using a hybrid of rule-based and neural tagging strategies, candidate retrieval via SapBERT, and entity linking through prompting a large language model. Experiments on four benchmark datasets show that AutoPCR achieves the best average and most robust performance across both mention-level and document-level evaluations, surpassing prior state-of-the-art methods. Further ablation and transfer studies demonstrate its inductive capability and generalizability to new ontologies.

</details>


### [57] [Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks](https://arxiv.org/abs/2507.19353)

*Kai Liu, Zhan Su, Peijie Dong, Fengran Mo, Jianfei Gao, ShaoTing Zhang, Kai Chen*

**Main category:** cs.CL

**Keywords:** Recurrent LLMs, chunk-wise inference, Smooth Reading, long-context tasks, Self-Attention LLMs

**Relevance Score:** 8

**TL;DR:** This paper introduces Smooth Reading, a chunk-wise inference method for Recurrent LLMs, improving their performance on long-context tasks and making them more efficient compared to Self-Attention LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recurrent LLMs suffer from fixed-size memory limitations, impacting their performance on long-context tasks compared to Self-Attention LLMs, which have quadratic complexity.

**Method:** Smooth Reading processes context in chunks and iteratively summarizes information, reducing memory demands and enhancing compatibility with Recurrent LLMs.

**Key Contributions:**

	1. Introduction of the Smooth Reading method for chunk-wise inference
	2. Demonstrated improved performance of Recurrent LLMs on long-context tasks
	3. Significant efficiency improvements in training and inference over Self-Attention LLMs.

**Result:** Smooth Reading improves SWA-3B-4k performance from 5.68% lower to 3.61% higher than Self-Attention LLMs on LongBench, while being 3x faster in training and 2x faster in inference at 64k context.

**Limitations:** 

**Conclusion:** This method successfully narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, maintaining efficiency advantages. Code and datasets will be released for further research.

**Abstract:** Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.

</details>


### [58] [Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization](https://arxiv.org/abs/2507.19356)

*Hsuan-Yu Wang, Pei-Ying Lee, Berlin Chen*

**Main category:** cs.CL

**Keywords:** Speech Emotion Recognition, Automatic Speech Recognition, Speaker Diarization

**Relevance Score:** 8

**TL;DR:** This paper explores how timestamp-based alignment between ASR transcripts and Speaker Diarization outputs enhances Speech Emotion Recognition accuracy.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of misalignment between Automatic Speech Recognition and Speaker Diarization, which reduces the reliability of multimodal emotion recognition systems.

**Method:** An alignment pipeline utilizing pre-trained ASR and speaker diarization models for synchronizing timestamps, combined with textual embeddings from RoBERTa and audio embeddings from Wav2Vec, using cross-attention fusion with a gating mechanism.

**Key Contributions:**

	1. Introduction of a timestamp alignment pipeline for ASR and SD outputs.
	2. Combination of textual and audio embeddings using advanced fusion techniques.
	3. Demonstration of improved SER accuracy through precise alignment.

**Result:** Experimental evaluations show that timestamp alignment significantly improves Speech Emotion Recognition accuracy compared to baseline methods without synchronization.

**Limitations:** 

**Conclusion:** The study underscores the critical role of temporal alignment in improving multimodal emotion recognition accuracy, offering a foundation for robust emotion analysis.

**Abstract:** In this paper, we investigate the impact of incorporating timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two modalities often reduces the reliability of multimodal emotion recognition systems, particularly in conversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-trained ASR and speaker diarization models, systematically synchronizing timestamps to generate accurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via RoBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating mechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization. The results highlight the critical importance of temporal alignment, demonstrating its effectiveness in enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal emotion analysis.

</details>


### [59] [SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models](https://arxiv.org/abs/2507.19361)

*Zhen Wan, Chao-Han Huck Yang, Yahan Yu, Jinchuan Tian, Sheng Li, Ke Hu, Zhehuai Chen, Shinji Watanabe, Fei Cheng, Chenhui Chu, Sadao Kurohashi*

**Main category:** cs.CL

**Keywords:** voice understanding, large language models, cognitive evaluation, evaluation metrics, multi-modal training

**Relevance Score:** 8

**TL;DR:** The paper introduces Speech-based Intelligence Quotient (SIQ) to evaluate voice understanding in large language models (LLMs) through cognitive-inspired metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve voice understanding evaluation beyond traditional metrics like word error rate (WER) by incorporating cognitive principles.

**Method:** The SIQ framework assesses LLM Voice at three cognitive levels based on Bloom's Taxonomy: 1) Remembering, 2) Understanding, 3) Application, enabling comparisons among various models.

**Key Contributions:**

	1. Introduction of the SIQ framework for evaluating voice understanding in LLMs.
	2. Unified comparisons of cascaded and end-to-end LLM methods.
	3. Identification of annotation errors and hallucinations in voice understanding tasks.

**Result:** SIQ quantifies voice understanding abilities, identifies errors in benchmarks, and detects hallucinations in LLM Voice.

**Limitations:** 

**Conclusion:** The framework offers a novel evaluation approach that aligns cognitive principles with voice-oriented benchmarks, highlighting challenges in multi-modal training.

**Abstract:** We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.

</details>


### [60] [Data Augmentation for Spoken Grammatical Error Correction](https://arxiv.org/abs/2507.19374)

*Penny Karanasou, Mengjie Qian, Stefano Bannò, Mark J. F. Gales, Kate M. Knill*

**Main category:** cs.CL

**Keywords:** Grammatical Error Correction, Spoken GEC, Dataset Augmentation, L2 Learners, Disfluencies

**Relevance Score:** 6

**TL;DR:** The paper introduces a method to generate annotated audio-text pairs with grammatical errors for Spoken GEC, aiming to enrich datasets while keeping assessments consistent for L2 learners.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of high-quality annotated spoken datasets for Spoken Grammatical Error Correction (SGEC) despite the existence of strong datasets for GEC.

**Method:** The authors propose a fully automated approach to create audio-text pairs with intentional grammatical errors and disfluencies, as well as new evaluation metrics for these datasets.

**Key Contributions:**

	1. Fully automated generation of audio-text pairs with grammatical errors and disfluencies for SGEC.
	2. Development of objective metrics to evaluate generated datasets for efficacy in SGEC.
	3. Demonstrates the application of the augmented dataset for both written and spoken grammatical error correction.

**Result:** The generated augmented dataset retains the textual and acoustic characteristics of the original data and introduces new errors, demonstrating effectiveness for both written GEC and SGEC applications.

**Limitations:** The study primarily focuses on a single corpus (S&I Corpus) for evaluations, which may limit generalizability.

**Conclusion:** The augmented corpus can be beneficial for improving models in both text-based and speech-based grammatical error correction without affecting language assessment outcomes for learners.

**Abstract:** While there exist strong benchmark datasets for grammatical error correction (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-resourced. In this paper, we propose a fully automated method to generate audio-text pairs with grammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be used to evaluate the generated data and choose the more suitable dataset for SGEC. The goal is to generate an augmented dataset that maintains the textual and acoustic characteristics of the original data while providing new types of errors. This augmented dataset should augment and enrich the original corpus without altering the language assessment scores of the second language (L2) learners. We evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the audio-text pairs). Our experiments are conducted on the S\&I Corpus, the first publicly available speech dataset with grammar error annotations.

</details>


### [61] [Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study](https://arxiv.org/abs/2507.19396)

*Rachel M. Murphy, Nishant Mishra, Nicolette F. de Keizer, Dave A. Dongelmans, Kitty J. Jager, Ameen Abu-Hanna, Joanna E. Klopotowska, Iacer Calixto*

**Main category:** cs.CL

**Keywords:** adverse drug events, natural language processing, clinical texts, transformer models, Dutch

**Relevance Score:** 9

**TL;DR:** This study benchmarks adverse drug event detection in Dutch clinical texts using transformer models, reporting on their performance and validation for clinical use.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a performance benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents.

**Method:** Trained and evaluated several models, including Bi-LSTM and four transformer-based models (BERTje, RobBERT, MedRoBERTa.nl, NuNER), on annotated clinical progress notes; validated through gold standard and end-to-end tasks.

**Key Contributions:**

	1. Established a benchmark for ADE detection in Dutch clinical texts
	2. Demonstrated the effectiveness of transformer models in this domain
	3. Provided insights on performance measures for clinical document analysis

**Result:** MedRoBERTa.nl was the top performer with a macro-averaged F1 score of 0.63 on gold standard and a recall of 67-74% for detecting ADEs in discharge letters.

**Limitations:** 

**Conclusion:** The research highlights the importance of appropriate performance measures for ADE detection and provides a framework for future evaluations using language models in clinical settings.

**Abstract:** In this study, we set a benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents using several transformer models, clinical scenarios and fit-for-purpose performance measures. We trained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the tasks of named entity recognition (NER) and relation classification (RC) using 102 richly annotated Dutch ICU clinical progress notes. Anonymized free text clinical progress notes of patients admitted to intensive care unit (ICU) of one academic hospital and discharge letters of patients admitted to Internal Medicine wards of two non-academic hospitals were reused. We evaluated our ADE RC models internally using gold standard (two-step task) and predicted entities (end-to-end task). In addition, all models were externally validated on detecting ADEs at the document level. We report both micro- and macro-averaged F1 scores, given the imbalance of ADEs in the datasets. Although differences for the ADE RC task between the models were small, MedRoBERTa.nl was the best performing model with macro-averaged F1 score of 0.63 using gold standard and 0.62 using predicted entities. The MedRoBERTa.nl models also performed the best in our external validation and achieved recall of between 0.67 to 0.74 using predicted entities, meaning between 67 to 74% of discharge letters with ADEs were detected. Our benchmark study presents a robust and clinically meaningful approach for evaluating language models for ADE detection in clinical free text documents. Our study highlights the need to use appropriate performance measures fit for the task of ADE detection in clinical free-text documents and envisioned future clinical use.

</details>


### [62] [Towards Domain Specification of Embedding Models in Medicine](https://arxiv.org/abs/2507.19407)

*Mohammad Khodadad, Ali Shiraee, Mahdi Astaraki, Hamidreza Mahyar*

**Main category:** cs.CL

**Keywords:** medical text embeddings, healthcare applications, self-supervised learning

**Relevance Score:** 9

**TL;DR:** The paper presents a new medical text embedding model, MEDTE, which addresses existing shortcomings in model diversity and evaluation benchmarks by leveraging self-supervised contrastive learning and providing a comprehensive task benchmark for medical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing medical text embedding models are limited by narrow training data and inadequate evaluation benchmarks, hindering their effectiveness in real-world healthcare applications.

**Method:** Leveraged MEDTE, a GTE model fine-tuned on diverse medical corpora using self-supervised contrastive learning across various data sources, and proposed a new benchmark suite with 51 tasks tailored for medical text evaluation.

**Key Contributions:**

	1. Introduction of the MEDTE model that is fine-tuned on diverse medical data
	2. Development of a new benchmark suite consisting of 51 tasks for evaluating medical text embeddings
	3. Demonstration of outperformance against state-of-the-art models in multiple medical text tasks

**Result:** The MEDTE model demonstrates superior performance over existing state-of-the-art models in various medical text tasks, highlighting its robustness and efficacy.

**Limitations:** 

**Conclusion:** The introduction of MEDTE and the comprehensive benchmark suite provides significant advancements in medical text embeddings, promising improvements in various healthcare applications.

**Abstract:** Medical text embedding models are foundational to a wide array of healthcare applications, ranging from clinical decision support and biomedical information retrieval to medical question answering, yet they remain hampered by two critical shortcomings. First, most models are trained on a narrow slice of medical and biological data, beside not being up to date in terms of methodology, making them ill suited to capture the diversity of terminology and semantics encountered in practice. Second, existing evaluations are often inadequate: even widely used benchmarks fail to generalize across the full spectrum of real world medical tasks.   To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical corpora through self-supervised contrastive learning across multiple data sources, to deliver robust medical text embeddings.   Alongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification, clustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of medical text. Our results demonstrate that this combined approach not only establishes a robust evaluation framework but also yields embeddings that consistently outperform state of the art alternatives in different tasks.

</details>


### [63] [TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability](https://arxiv.org/abs/2507.19419)

*Mohammad Aflah Khan, Ameya Godbole, Johnny Tian-Zheng Wei, Ryan Wang, James Flemings, Krishna Gummadi, Willie Neiswanger, Robin Jia*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Data Processing, Open Source, Transformers

**Relevance Score:** 8

**TL;DR:** TokenSmith is an open-source library that simplifies the interaction with datasets used in Megatron-style model training through user-friendly operations and modular backend.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accessibility and usability of datasets in large language model pretraining, addressing the cumbersome and fragmented existing workflows.

**Method:** TokenSmith provides a simple user interface and modular backend for operations like searching, viewing, editing, and inspecting datasets, enabling structured editing without altering training code.

**Key Contributions:**

	1. Open-source library for dataset interaction in pretraining frameworks
	2. User-friendly interface for dataset operations
	3. Modular backend enabling structured editing without code changes

**Result:** TokenSmith enables easier dataset debugging, validation, and experimentation, enhancing the pretraining efficiency for Megatron-style frameworks.

**Limitations:** 

**Conclusion:** TokenSmith democratizes access to dataset tooling for large language models, facilitating better research and development workflows.

**Abstract:** Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining frameworks such as GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation.   TokenSmith is designed as a plug and play addition to existing large language model pretraining workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on YouTube.

</details>


### [64] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)

*Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Prompt Optimization, Natural Language Processing, Code Optimization

**Relevance Score:** 9

**TL;DR:** GEPA (Genetic-Pareto) is a prompt optimizer that enhances LLM performance through natural language reflection, outperforming GRPO and MIPROv2 in efficiency and effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the potential of natural language reflection as a richer learning medium for large language models, compared to traditional reinforcement learning methods like GRPO.

**Method:** GEPA samples system-level trajectories from AI systems with LLM prompts, reflecting on them in natural language to optimize prompts through diagnosis, updates, and integration of lessons from the Pareto frontier.

**Key Contributions:**

	1. Introduces GEPA, a novel prompt optimizer utilizing natural language reflection.
	2. Achieves significant performance improvements over existing reinforcement learning methods and prompt optimizers.
	3. Provides a new inference-time search strategy for code optimization.

**Result:** GEPA outperforms GRPO by an average of 10%, using up to 35x fewer rollouts, and exceeds MIPROv2 by over 10% across two LLMs.

**Limitations:** 

**Conclusion:** GEPA demonstrates that leveraging natural language can significantly improve the efficiency and effectiveness of prompt optimization in LLMs.

**Abstract:** Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.

</details>


### [65] [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)

*Son Quoc Tran, Tushaar Gangavarapu, Nicholas Chernogor, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil*

**Main category:** cs.CL

**Keywords:** Conversational modeling, Human-Computer Interaction, Language modeling, Prediction, Evaluation framework

**Relevance Score:** 7

**TL;DR:** This paper presents a unified evaluation framework for the Conversations Gone Awry (CGA) task, which aims to predict derailing conversations in automated systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automated systems' ability to predict conversation direction and assist in human-human interactions.

**Method:** The paper introduces a benchmark for evaluating different architectures in CGA, providing direct comparisons and a new metric for tracking model forecast revision during conversation.

**Key Contributions:**

	1. Introduction of a uniform evaluation framework for CGA task
	2. Creation of a benchmark for direct comparison of models
	3. Development of a novel metric for tracking forecast revisions in conversations

**Result:** The framework allows for an updated overview of advancements in CGA models, facilitating reliable evaluations.

**Limitations:** 

**Conclusion:** This foundational framework sets the stage for future research in predictive conversation modeling and improves the reliability of evaluations in this domain.

**Abstract:** We often rely on our intuition to anticipate the direction of a conversation. Endowing automated systems with similar foresight can enable them to assist human-human interactions. Recent work on developing models with this predictive capacity has focused on the Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this work, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark that enables direct and reliable comparisons between different architectures. This allows us to present an up-to-date overview of the current progress in CGA models, in light of recent advancements in language modeling. Our framework also introduces a novel metric that captures a model's ability to revise its forecast as the conversation progresses.

</details>


### [66] [Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case](https://arxiv.org/abs/2311.13729)

*Shashank Gupta, Xuguang Ai, Ramakanth Kavuluru*

**Main category:** cs.CL

**Keywords:** end-to-end relation extraction, natural language processing, biomedicine, rare diseases, generative models

**Relevance Score:** 8

**TL;DR:** This paper compares three paradigms for end-to-end relation extraction (E2ERE) using the RareDis dataset for rare diseases, evaluating NER --> RE pipelines, joint sequence to sequence models, and GPT models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of different paradigms for E2ERE in the context of biomedicine, particularly focusing on rare diseases with complex data.

**Method:** The study evaluates three approaches to E2ERE: NER to RE pipelines, joint sequence-to-sequence models, and GPT models, using comparable state-of-the-art methodologies and error analysis on the RareDis dataset.

**Key Contributions:**

	1. First study to perform E2ERE using the RareDis dataset
	2. Identification of performance strengths and weaknesses of three E2ERE approaches
	3. Recommendations for future research combining small and large models.

**Result:** Pipeline models outperformed both sequence-to-sequence and GPT models, revealing significant performance gaps and identifying NER errors due to partial matches and discontinuous entities.

**Limitations:** The study primarily focuses on specific datasets (RareDis and a second dataset for chemical-protein interactions), which may not generalize to all contexts.

**Conclusion:** Conventional pipeline models are recommended for E2ERE when training data is available, highlighting the need for innovative methods that combine strengths from both smaller and larger models.

**Abstract:** End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and discontinuous entities caused many NER errors contributing to lower overall E2E performances. We also verify these findings on a second E2ERE dataset for chemical-protein interactions. Although generative LM-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for E2ERE. More innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger GPT models to improve E2ERE. As of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for E2ERE. Our contribution is also the first to conduct E2ERE for the RareDis dataset.

</details>


### [67] [Spike No More: Stabilizing the Pre-training of Large Language Models](https://arxiv.org/abs/2312.16903)

*Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki*

**Main category:** cs.CL

**Keywords:** large language models, pre-training, gradient norm, loss spikes, Jacobian matrices

**Relevance Score:** 7

**TL;DR:** This paper explores methods to prevent loss spikes during the pre-training of large language models by keeping the gradient norm small.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Loss spikes during large language model pre-training can degrade performance and waste computational resources. The goal is to find ways to stabilize this process.

**Method:** The authors analyze the spectral norms of Jacobian matrices for sub-layers to understand factors influencing gradient norm size, proposing conditions for stabilization.

**Key Contributions:**

	1. Identifying the influence of gradient norms on pre-training stability
	2. Proposing conditions for preventing loss spikes during training
	3. Empirical validation of theoretical analyses through experiments

**Result:** Experiments show that models with small sub-layers and large shortcuts effectively reduce loss spikes during pre-training.

**Limitations:** 

**Conclusion:** Stabilizing the pre-training process of large language models requires mindful design choices regarding sub-layer sizes and the implementation of shortcut connections.

**Abstract:** Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. Based on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers. Our findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut. We conduct various experiments to empirically verify our theoretical analyses. Experimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.

</details>


### [68] [How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?](https://arxiv.org/abs/2402.13470)

*Aviv Brokman, Ramakanth Kavuluru*

**Main category:** cs.CL

**Keywords:** biomedical NLP, relation extraction, instruction finetuning, generative language models

**Relevance Score:** 9

**TL;DR:** This paper investigates the effectiveness of biomedical language models (LMs) compared to general-domain LMs in the task of relation extraction, concluding that general-domain models often outperform biomedical models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the benefits of using domain-specific biomedical language models for relation extraction compared to general-domain models.

**Method:** The study tests existing language models across four datasets to compare performance between biomedical-trained and general-domain models, as well as different types of instruction finetuning.

**Key Contributions:**

	1. Comparison of biomedical and general-domain language models for relation extraction
	2. Evaluation of instruction finetuning effectiveness in biomedical NLP
	3. Insights on research focus towards instruction finetuning over specific model training

**Result:** General-domain models generally outperformed biomedical-domain models, while biomedical instruction finetuning showed performance improvement similar to general instruction finetuning despite having fewer instructions.

**Limitations:** 

**Conclusion:** It may be more effective to pursue larger-scale biomedical instruction finetuning of general language models instead of developing domain-specific biomedical LMs.

**Abstract:** Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs

</details>


### [69] [MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts](https://arxiv.org/abs/2406.12549)

*Dominik Macko, Jakub Kopal, Robert Moro, Ivan Srba*

**Main category:** cs.CL

**Keywords:** machine-generated text detection, multilingual dataset, social media texts, fine-tuning, LLMs

**Relevance Score:** 8

**TL;DR:** The paper proposes a multilingual dataset for detecting machine-generated texts on social media, addressing a gap in current research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual datasets and the unique characteristics of social media texts for machine-generated text detection.

**Method:** Creation of a multilingual (22 languages) and multi-platform (5 social media platforms) dataset called MultiSocial, with 472,097 texts, to evaluate detection methods.

**Key Contributions:**

	1. Introduction of the MultiSocial dataset for multilingual machine-generated text detection
	2. Comprehensive comparison of zero-shot and fine-tuned detection methods
	3. Insights on platform-specific training effects on detection performance

**Result:** Fine-tuned detection methods effectively identify machine-generated texts, with platform selection affecting training outcomes.

**Limitations:** 

**Conclusion:** The study highlights the importance of a dedicated dataset for social media text and demonstrates the effectiveness of fine-tuning detection methods.

**Abstract:** Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.

</details>


### [70] [Long-Form Answers to Visual Questions from Blind and Low Vision People](https://arxiv.org/abs/2408.06303)

*Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, Amy Pavel*

**Main category:** cs.CL

**Keywords:** vision language models, long-form visual question answers, blind and low vision users

**Relevance Score:** 8

**TL;DR:** VizWiz-LF is a dataset for long-form visual question answers generated by blind and low vision users, highlighting the challenges of hallucinations in generated content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for long-form visual question answers that provide comprehensive responses for blind and low vision users.

**Method:** Creation of the VizWiz-LF dataset, containing 4.2k long-form answers to visual questions, with evaluation through automatic and human assessments.

**Key Contributions:**

	1. Introduction of the VizWiz-LF dataset
	2. Functional role annotation of LFVQA sentences
	3. Evaluation methods for assessing visual question answering models

**Result:** Long-form answers from BLV users contain valuable information beyond just answers, but AI-generated responses often include incorrect details and hallucinations.

**Limitations:** Generated answers sometimes hallucinate visual details and struggle with unanswerable questions.

**Conclusion:** While generated answers are perceived as plausible, improvements are needed to reduce hallucinations, particularly for unanswerable visual questions.

**Abstract:** Vision language models can now generate long-form answers to questions about images - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a dataset of long-form answers to visual questions posed by blind and low vision (BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions, collected from human expert describers and six VQA models. We develop and annotate functional roles of sentences of LFVQA and demonstrate that long-form answers contain information beyond the question answer such as explanations and suggestions. We further conduct automatic and human evaluations with BLV and sighted people to evaluate long-form answers. BLV people perceive both human-written and generated long-form answers to be plausible, but generated answers often hallucinate incorrect visual details, especially for unanswerable visual questions (e.g., blurry or irrelevant images). To reduce hallucinations, we evaluate the ability of VQA models to abstain from answering unanswerable questions across multiple prompting strategies.

</details>


### [71] [Advancing biomolecular understanding and design following human instructions](https://arxiv.org/abs/2410.07919)

*Xiang Zhuang, Keyan Ding, Tianwen Lyu, Yinuo Jiang, Xiaotong Li, Zhuoyi Xiang, Zeyuan Wang, Ming Qin, Kehua Feng, Jike Wang, Qiang Zhang, Huajun Chen*

**Main category:** cs.CL

**Keywords:** large language models, biomolecular design, drug discovery, enzyme engineering, natural language processing

**Relevance Score:** 8

**TL;DR:** InstructBioMol is a large language model that bridges natural language and biomolecular design, enhancing drug discovery through improved binding affinity and enzyme predictions.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap between AI's computational capabilities and researchers' intuitive goals in biomolecular research, particularly using natural language.

**Method:** InstructBioMol integrates multimodal biomolecules as input and allows researchers to define design goals in natural language, producing appropriate biomolecular outputs.

**Key Contributions:**

	1. Development of InstructBioMol, a model for biomolecular design using natural language.
	2. Demonstrated improved drug design capabilities with measurable binding affinity enhancement.
	3. Integration of multimodal data for better understanding of biomolecular outputs.

**Result:** InstructBioMol achieved a 10% improvement in drug molecule binding affinity and a 70.4 enzyme-substrate pair prediction score in experimental results.

**Limitations:** 

**Conclusion:** The model has the potential to transform biomolecular research by effectively aligning natural language with biomolecular design tasks.

**Abstract:** Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology and enzyme engineering. Recent breakthroughs in artificial intelligence have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between artificial intelligence's computational capabilities and researchers' intuitive goals, particularly in using natural language to bridge complex tasks with human intentions. Large language models have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a large language model designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules and proteins. This model can integrate multimodal biomolecules as the input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate that InstructBioMol can understand and design biomolecules following human instructions. In particular, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an enzyme-substrate pair prediction score of 70.4. This highlights its potential to transform real-world biomolecular research. The code is available at https://github.com/HICAI-ZJU/InstructBioMol.

</details>


### [72] [LLMs are Also Effective Embedding Models: An In-depth Overview](https://arxiv.org/abs/2412.12591)

*Chongyang Tao, Tao Shen, Shen Gao, Junshuo Zhang, Zhen Li, Kai Hua, Wenpeng Hu, Zhengwei Tao, Shuai Ma*

**Main category:** cs.CL

**Keywords:** large language models, embeddings, natural language processing, prompt engineering, data tuning

**Relevance Score:** 9

**TL;DR:** This survey explores the transition from traditional encoder-only models to LLMs in natural language processing, focusing on methods for deriving competitive embeddings from these models.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** The shift from traditional embedding models to LLMs represents a significant change in NLP paradigms, necessitating a comprehensive overview of techniques and strategies involved in leveraging LLMs for embeddings.

**Method:** The survey encompasses foundational techniques, direct prompting strategies for competitive embeddings, and data-centric tuning aspects affecting embedding models. It discusses various advanced methods for embeddings from diverse data types, including longer texts and multilingual sources.

**Key Contributions:**

	1. In-depth overview of embeddings derivation strategies from LLMs.
	2. Discussion of advanced embedding methodologies for different data types.
	3. Comprehensive analysis of trade-offs in LLM embedding models.

**Result:** The survey highlights multiple methods for generating embeddings and examines factors affecting their performance such as model architecture and tuning strategies, as well as efficiency comparisons between embedding types.

**Limitations:** Challenges include embedding quality across tasks, efficiency vs. accuracy trade-offs, and issues related to data bias and robustness.

**Conclusion:** This work serves as a critical resource for researchers and practitioners, synthesizing advancements and challenges in utilizing LLMs for effective embedding applications in various domains.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing by achieving state-of-the-art performance across various tasks. Recently, their effectiveness as embedding models has gained attention, marking a paradigm shift from traditional encoder-only models like ELMo and BERT to decoder-only, large-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an in-depth overview of this transition, beginning with foundational techniques before the LLM era, followed by LLM-based embedding models through two main strategies to derive embeddings from LLMs. 1) Direct prompting: We mainly discuss the prompt designs and the underlying rationale for deriving competitive embeddings. 2) Data-centric tuning: We cover extensive aspects that affect tuning an embedding model, including model architecture, training objectives, data constructions, etc. Upon the above, we also cover advanced methods for producing embeddings from longer texts, multilingual, code, cross-modal data, as well as reasoning-aware and other domain-specific scenarios. Furthermore, we discuss factors affecting choices of embedding models, such as performance/efficiency comparisons, dense vs sparse embeddings, pooling strategies, and scaling law. Lastly, the survey highlights the limitations and challenges in adapting LLMs for embeddings, including cross-task embedding quality, trade-offs between efficiency and accuracy, low-resource, long-context, data bias, robustness, etc. This survey serves as a valuable resource for researchers and practitioners by synthesizing current advancements, highlighting key challenges, and offering a comprehensive framework for future work aimed at enhancing the effectiveness and efficiency of LLMs as embedding models.

</details>


### [73] [Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation](https://arxiv.org/abs/2412.13666)

*Aneta Zugecova, Dominik Macko, Ivan Srba, Robert Moro, Jakub Kopal, Katarina Marcincinova, Matus Mesarcik*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personalization, Disinformation, Safety Filters, Meta-evaluation

**Relevance Score:** 9

**TL;DR:** This study investigates the combination of personalization and disinformation capabilities in LLMs, highlighting vulnerabilities in their safety filters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the risks associated with LLMs generating personalized disinformation, as this combination has not been extensively studied.

**Method:** The study evaluates recent open and closed LLMs in their ability to generate personalized disinformation news articles and assesses the functioning of their safety filters.

**Key Contributions:**

	1. Evaluated vulnerabilities of open and closed LLMs in generating personalized disinformation.
	2. Demonstrated that personalization can reduce the effectiveness of safety filters.
	3. Highlighted the pressing need for stronger safety measures in LLMs.

**Result:** The results indicate that most LLMs lack proper safety filters, with personalization reducing safety-filter activations, effectively acting as a jailbreak.

**Limitations:** Focused primarily on English language LLMs; results may not generalize to other languages.

**Conclusion:** The findings emphasize the urgent need for improved safety mechanisms in LLMs to prevent the generation of harmful personalized disinformation.

**Abstract:** The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts raises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluating vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.

</details>


### [74] [T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation](https://arxiv.org/abs/2501.12612)

*Lijun Li, Zhelun Shi, Xuhao Hu, Bowen Dong, Yiran Qin, Xihui Liu, Lu Sheng, Jing Shao*

**Main category:** cs.CL

**Keywords:** Text-to-Image, Safety Benchmark, Toxicity, Fairness, Bias

**Relevance Score:** 7

**TL;DR:** The paper presents T2ISafety, a safety benchmark for text-to-image models focusing on toxicity, fairness, and bias, revealing critical risks in prominent models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the emerging safety concerns in text-to-image (T2I) models, highlighting the risks of generating harmful or biased content.

**Method:** Developed a detailed hierarchy of tasks and categories for evaluating T2I models, collected a large dataset of prompts and manually annotated images, and trained an evaluator to detect nuanced safety risks.

**Key Contributions:**

	1. Introduction of T2ISafety benchmark for T2I safety evaluation
	2. Creation of a large-scale dataset with 68K annotated images
	3. Identification of critical yet overlooked safety risks in prominent T2I models

**Result:** Evaluation of 12 diffusion models revealed issues with racial fairness, generation of toxic content, and variable privacy protections, even under defensive strategies.

**Limitations:** The study is limited to the current set of evaluated models and categories; further research is needed to fully assess T2I safety.

**Conclusion:** T2ISafety offers a comprehensive benchmark that uncovers significant safety risks in existing T2I models, urging further research to mitigate these issues.

**Abstract:** Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under https://github.com/adwardlee/t2i_safety.

</details>


### [75] [An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning](https://arxiv.org/abs/2502.11439)

*Cen-Jhih Li, Aditya Bhaskara*

**Main category:** cs.CL

**Keywords:** Sparse Fine-tuning, Low-rank adaptation, neural network pruning, language models, memory efficiency

**Relevance Score:** 8

**TL;DR:** Development of a new Sparse Fine-tuning framework that enhances memory efficiency while maintaining accuracy for large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Need for fine-tuning methods that are memory and computationally efficient to adapt foundation models to downstream tasks with limited resources.

**Method:** The new framework identifies important neurons using feature importance metrics from neural network pruning, specifically through structural pruning, and restricts fine-tuning to these weights.

**Key Contributions:**

	1. Introduction of a novel Sparse Fine-tuning framework
	2. Utilization of feature importance metrics from network pruning
	3. Demonstrated memory efficiency improvement of 20-50%

**Result:** The proposed method improves Sparse Fine-tuning's memory efficiency by 20-50% while achieving accuracy comparable to state-of-the-art methods like LoRA.

**Limitations:** 

**Conclusion:** The new SpFT framework provides a significant improvement in efficient fine-tuning of large language models for users with limited computational resources.

**Abstract:** Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SpFT framework, based on ideas from neural network pruning. At a high level, we first identify ``important'' neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Experiments on common language tasks show our method improves SpFT's memory efficiency by 20-50\% while matching the accuracy of state-of-the-art methods like LoRA's variants.

</details>


### [76] [Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs](https://arxiv.org/abs/2503.00151)

*Fakhraddin Alwajih, Abdellah El Mekki, Samar Mohamed Magdy, Abdelrahim A. Elmadany, Omer Nacar, El Moatez Billah Nagoudi, Reem Abdel-Salam, Hanin Atwany, Youssef Nafea, Abdulfattah Mohammed Yahya, Rahaf Alhamouri, Hamzah A. Alsayadi, Hiba Zayed, Sara Shatnawi, Serry Sibaee, Yasir Ech-Chammakhy, Walid Al-Dhabyani, Marwa Mohamed Ali, Imen Jarraya, Ahmed Oumar El-Shangiti, Aisha Alraeesi, Mohammed Anwar Al-Ghrawi, Abdulrahman S. Al-Batati, Elgizouli Mohamed, Noha Taha Elgindi, Muhammed Saeed, Houdaifa Atou, Issam Ait Yahia, Abdelhak Bouayad, Mohammed Machrouh, Amal Makouar, Dania Alkawi, Mukhtar Mohamed, Safaa Taher Abdelfadil, Amine Ziad Ounnoughene, Rouabhia Anfel, Rwaa Assi, Ahmed Sorkatti, Mohamedou Cheikh Tourad, Anis Koubaa, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** large language models, cultural sensitivity, Arabic language, dataset, evaluation

**Relevance Score:** 9

**TL;DR:** Study introduces a culturally sensitive and inclusive dataset of Arabic language instructions for evaluating large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure cultural sensitivity and inclusivity in LLMs as they become integrated into daily life across Arab countries.

**Method:** A year-long community-driven project created a dataset covering 22 Arab countries containing input-response pairs in both Modern Standard Arabic and dialectal Arabic, covering 20 diverse topics.

**Key Contributions:**

	1. Dataset creation focusing on Arabic language and cultural inclusivity
	2. Evaluation of LLMs on dialectal capabilities leading to prominent findings
	3. Public accessibility of guidelines and reproducibility aspects

**Result:** Evaluation of several frontier LLMs showed notable limitations in cultural and dialectal capabilities, with performance discrepancies between closed-source and open-source models, and underrepresentation of certain countries.

**Limitations:** Certain countries are underrepresented in the dataset, affecting the generalization of findings.

**Conclusion:** The dataset provides insights into the strengths and weaknesses of language models while promoting reproducibility through available guidelines and data.

**Abstract:** As large language models (LLMs) become increasingly integrated into daily life, ensuring their cultural sensitivity and inclusivity is paramount. We introduce our dataset, a year-long community-driven project covering all 22 Arab countries. The dataset includes instructions (input, response pairs) in both Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20 diverse topics. Built by a team of 44 researchers across the Arab world, all of whom are authors of this paper, our dataset offers a broad, inclusive perspective. We use our dataset to evaluate the cultural and dialectal capabilities of several frontier LLMs, revealing notable limitations. For instance, while closed-source LLMs generally exhibit strong performance, they are not without flaws, and smaller open-source models face greater challenges. Moreover, certain countries (e.g., Egypt, the UAE) appear better represented than others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code, and data for reproducibility are publicly available.

</details>


### [77] [Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy](https://arxiv.org/abs/2503.05157)

*Ruixi Lin, Ziqiao Wang, Yang You*

**Main category:** cs.CL

**Keywords:** Language Models, Text Classification, Debiasing, Machine Learning, AI in Health

**Relevance Score:** 8

**TL;DR:** Proposes an ensemble debiasing method to address class accuracy imbalance in language models, achieving state-of-the-art results with balanced class accuracies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accuracy in text classification not by enhancing strong classes but by uplifting weak ones due to observed class accuracy imbalance.

**Method:** A Heaviside step function based ensemble debiasing method for flexible rectifications of class probabilities at multiple levels.

**Key Contributions:**

	1. Introduction of a new ensemble debiasing method for class imbalance
	2. Successful application of the method on Llama-2 models
	3. Significant performance improvements in biomedical domain text classification

**Result:** Achieves state-of-the-art overall accuracy gains while ensuring balanced class accuracies on seven text classification benchmarks, especially in the biomedical domain.

**Limitations:** 

**Conclusion:** Demonstrates the necessity of correcting weak classes through ensemble debiasing, with successful performance improvements in larger model variants.

**Abstract:** Language models are strong few-shot learners and achieve good overall accuracy in text classification tasks, masking the fact that their results suffer from great class accuracy imbalance. We believe that the pursuit of overall accuracy should not come from enriching the strong classes, but from raising up the weak ones. To address the imbalance, we propose a Heaviside step function based ensemble debiasing method, which enables flexible rectifications of in-context learned class probabilities at both class and sample levels. Evaluations with Llama-2-13B on seven text classification benchmarks show that our approach achieves state-of-the-art overall accuracy gains with balanced class accuracies. More importantly, we perform analyses on the resulted probability correction scheme, showing that sample-level corrections are necessary to elevate weak classes. Due to effectively correcting weak classes, our method also brings significant performance gains to a larger model variant, Llama-2-70B, especially on a biomedical domain task, further demonstrating the necessity of ensemble debiasing at both levels. Our source code is available at https://github.com/NUS-HPC-AI-Lab/DCS.

</details>


### [78] [Relation Extraction with Instance-Adapted Predicate Descriptions](https://arxiv.org/abs/2503.17799)

*Yuhang Jiang, Ramakanth Kavuluru*

**Main category:** cs.CL

**Keywords:** relation extraction, dual-encoder, machine learning, biomedical datasets, contrastive loss

**Relevance Score:** 6

**TL;DR:** This paper proposes a novel dual-encoder architecture for fine-tuning smaller models in relation extraction, achieving improved F1 scores on various datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance relation extraction performance using a dual-encoder methodology, addressing the limitations of traditional fixed linear layer representations in smaller models.

**Method:** A dual-encoder architecture employing a joint contrastive and cross-entropy loss, using a second encoder to compute instance-specific predicate representations with real entity spans.

**Key Contributions:**

	1. Introduction of a dual-encoder architecture for relation extraction
	2. Implementation of a joint contrastive and cross-entropy loss
	3. Demonstrated F1 score improvements on multiple datasets

**Result:** The proposed method achieved F1 score improvements of 1% to 2% over existing state-of-the-art techniques on biomedical and general domain datasets.

**Limitations:** 

**Conclusion:** The results validate the effectiveness of the proposed architecture, with ablation studies highlighting the significance of its components.

**Abstract:** Relation extraction (RE) is a standard information extraction task playing a major role in downstream applications such as knowledge discovery and question answering. Although decoder-only large language models are excelling in generative tasks, smaller encoder models are still the go to architecture for RE. In this paper, we revisit fine-tuning such smaller models using a novel dual-encoder architecture with a joint contrastive and cross-entropy loss. Unlike previous methods that employ a fixed linear layer for predicate representations, our approach uses a second encoder to compute instance-specific predicate representations by infusing them with real entity spans from corresponding input instances. We conducted experiments on two biomedical RE datasets and two general domain datasets. Our approach achieved F1 score improvements ranging from 1% to 2% over state-of-the-art methods with a simple but elegant formulation. Ablation studies justify the importance of various components built into the proposed architecture.

</details>


### [79] [Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331)

*Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Zhuangzhuang He, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, Jie Fu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Formal Language, Reinforcement Learning, Software Verification, Dafny

**Relevance Score:** 9

**TL;DR:** This paper explores the use of formal languages, specifically Dafny, to enhance the verification processes of Large Language Models (LLMs) for software development, reducing the need for extensive human supervision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the reliability and scalability challenges of verification processes in informal language-based LLMs, which struggle in generating verifiable programs.

**Method:** The research employs Dafny, a formal language, to create a scalable data curation pipeline and designs reinforcement learning methodologies that incorporate feedback from a formal verifier.

**Key Contributions:**

	1. Introduction of DafnyComp benchmark for compositional formal programs
	2. Development of an automatic and scalable data curation pipeline
	3. Demonstration of improved performance of small models in generating verifiable code

**Result:** The developed DafnyComp benchmark demonstrated that even small models (0.5B parameters) could generate valid and verifiable Dafny code, outperforming proprietary models, especially when combined with reinforcement learning techniques.

**Limitations:** 

**Conclusion:** Utilizing formal languages like Dafny provides a promising direction for improving the verification processes in LLMs, reducing reliance on human-generated priors and enhancing model performance across various programming tasks.

**Abstract:** Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark.

</details>
