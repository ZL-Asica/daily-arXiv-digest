# 2025-07-22

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 39]

- [cs.CL](#cs.CL) [Total: 126]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap](https://arxiv.org/abs/2507.14316)

*Xianhao Carton Liu, Difan Jia, Tongyu Nie, Evan Suma Rosenberg, Victoria Interrante, Chen Zhu-Tian*

**Main category:** cs.HC

**Keywords:** Augmented Reality, AI decision-making, cognitive load, spatial reasoning, human-AI collaboration

**Relevance Score:** 7

**TL;DR:** This paper investigates the effectiveness of Augmented Reality (AR) in reducing cognitive load during AI-assisted decision-making in emergency scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** High-stakes decision-making in time-critical scenarios often leads to cognitive overload, impacting the judgement of AI suggestions. AR may offer a solution by overlaying information in the real world, but its effectiveness is questioned.

**Method:** An empirical study with 32 participants compared AI-assisted decision-making using AR see-through visualizations versus 2D minimaps in spatial target selection tasks.

**Key Contributions:**

	1. Comparison of AR and 2D maps for AI-assisted decisions
	2. Insights into cognitive load and AI reliance
	3. Guidelines for future research on AR in decision-making

**Result:** Participants using AR exhibited greater inappropriate reliance on AI suggestions, attributed to perceptual challenges and realism in the visual representation, despite AR aiding spatial reasoning skills.

**Limitations:** The study was limited to specific emergency scenarios and may not generalize to other contexts.

**Conclusion:** Embedded visualizations in AR can impair decision-making due to over-reliance on AI but show promise in enhancing spatial reasoning. Further research is necessary to refine the design of human-AI collaboration in AR.

**Abstract:** In high-stakes, time-critical scenarios-such as emergency evacuation, first responder prioritization, and crisis management -- decision-makers must rapidly choose among spatial targets, such as exits, individuals to assist, or areas to secure. Advances in indoor sensing and artificial intelligence (AI) can support these decisions by visualizing real-time situational data and AI suggestions on 2D maps. However, mentally mapping this information onto real-world spaces imposes significant cognitive load. This load can impair users' ability to appropriately judge AI suggestions, leading to inappropriate reliance (e.g., accepting wrong AI suggestions or rejecting correct ones). Embedded visualizations in Augmented Reality (AR), by directly overlaying information onto physical environments, may reduce this load and foster more deliberate, appropriate reliance on AI. But is this true? In this work, we conducted an empirical study (N = 32) comparing AR see-through (embedded visualization) and 2D Minimap in time-critical, AI-assisted spatial target selection tasks. Contrary to our expectations, users exhibited greater inappropriate reliance on AI in the AR condition. Our analysis further reveals that this is primarily due to over-reliance, with factors specific to embedded visualizations, such as perceptual challenges, visual proximity illusions, and highly realistic visual representations. Nonetheless, embedded visualizations demonstrated notable benefits in spatial reasoning, such as spatial mapping and egocentric spatial imagery. We conclude by discussing the empirical insights, deriving design implications, and outlining important directions for future research on human-AI decision collaboration in AR.

</details>


### [2] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)

*Angjelin Hila, Elliott Hauser*

**Main category:** cs.HC

**Keywords:** Large Language Models, Deductive Coding, Qualitative Research, ChatGPT, Machine Learning

**Relevance Score:** 9

**TL;DR:** This study explores the use of large language models like ChatGPT for structured deductive qualitative coding, focusing on how various intervention methods can influence classification performance in specific domains.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the underexplored potential of LLMs for deductive classification tasks, contrasting with the predominant focus on inductive coding in current research.

**Method:** Using the Comparative Agendas Project Master Codebook, case summaries from the U.S. Supreme Court were classified into 21 policy domains via four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy. Performance metrics included accuracy, F1-score, Cohen's kappa, and Krippendorff's alpha, with validity assessed using chi-squared tests and effect size analyses.

**Key Contributions:**

	1. Demonstrated the application of LLMs in deductive qualitative coding tasks.
	2. Introduced a novel Step-by-Step Task Decomposition strategy for improved classification performance.
	3. Provided evidence of LLM reliability in qualitative coding through various performance metrics.

**Result:** The Step-by-Step Task Decomposition strategy yielded the highest performance with an accuracy of 0.775 and kappa of 0.744. Chi-squared analyses showed that the intervention methods significantly influenced classification behavior, suggesting reliable LLM performance in qualitative coding tasks.

**Limitations:** The study's focus on a specific dataset may limit generalizability to other qualitative coding contexts, and limitations related to semantic ambiguity within texts were noted.

**Conclusion:** With targeted interventions, LLMs like ChatGPT can achieve reliable classification levels suitable for qualitative coding workflows, despite challenges such as semantic ambiguity in case summaries.

**Abstract:** In this study, we investigate the use of large language models (LLMs), specifically ChatGPT, for structured deductive qualitative coding. While most current research emphasizes inductive coding applications, we address the underexplored potential of LLMs to perform deductive classification tasks aligned with established human-coded schemes. Using the Comparative Agendas Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries into 21 major policy domains. We tested four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy, across repeated samples. Performance was evaluated using standard classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's alpha), and construct validity was assessed using chi-squared tests and Cramer's V. Chi-squared and effect size analyses confirmed that intervention strategies significantly influenced classification behavior, with Cramer's V values ranging from 0.359 to 0.613, indicating moderate to strong shifts in classification patterns. The Step-by-Step Task Decomposition strategy achieved the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746), achieving thresholds for substantial agreement. Despite the semantic ambiguity within case summaries, ChatGPT displayed stable agreement across samples, including high F1 scores in low-support subclasses. These findings demonstrate that with targeted, custom-tailored interventions, LLMs can achieve reliability levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [3] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)

*Taufiq Daryanto, Sophia Stil, Xiaohan Ding, Daniel Manesh, Sang Won Lee, Tim Lee, Stephanie Lunn, Sarah Rodriguez, Chris Brown, Eugenia Rho*

**Main category:** cs.HC

**Keywords:** human-AI collaboration, technical interviews, conversational AI, think-aloud process, equitable learning

**Relevance Score:** 8

**TL;DR:** This study explores the role of conversational AI in enhancing the think-aloud process during technical interviews, focusing on user perceptions and design recommendations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate user perceptions of conversational AI's role in think-aloud practice for technical interviews and to explore design opportunities for improvement.

**Method:** A study was conducted with 17 participants using an LLM-based tool for technical interview practice, gathering data on their experiences and preferences.

**Key Contributions:**

	1. Promoting social presence in AI interactions for better simulation
	2. Providing diverse feedback mechanisms beyond content analysis
	3. Encouraging crowdsourced examples through collaboration with AI.

**Result:** Participants appreciated the AI's contribution to simulation, feedback, and learning through example generation, leading to key design insights.

**Limitations:** The study is limited to 17 participants, which may not fully represent broader user experiences with AI in interview practice.

**Conclusion:** The research underscores the importance of social presence in conversational AI for interview simulation and proposes a need for equitable learning strategies in AI-driven interview preparation.

**Abstract:** One challenge in technical interviews is the think-aloud process, where candidates verbalize their thought processes while solving coding tasks. Despite its importance, opportunities for structured practice remain limited. Conversational AI offers potential assistance, but limited research explores user perceptions of its role in think-aloud practice. To address this gap, we conducted a study with 17 participants using an LLM-based technical interview practice tool. Participants valued AI's role in simulation, feedback, and learning from generated examples. Key design recommendations include promoting social presence in conversational AI for technical interview simulation, providing feedback beyond verbal content analysis, and enabling crowdsourced think-aloud examples through human-AI collaboration. Beyond feature design, we examined broader considerations, including intersectional challenges and potential strategies to address them, how AI-driven interview preparation could promote equitable learning in computing careers, and the need to rethink AI's role in interview practice by suggesting a research direction that integrates human-AI collaboration.

</details>


### [4] [Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies](https://arxiv.org/abs/2507.14482)

*Qianhe Chen, Yong Wang, Yixin Yu, Xiyuan Zhu, Xuerou Yu, Ran Wang*

**Main category:** cs.HC

**Keywords:** visualization, debate analysis, large language models, HCI, interaction design

**Relevance Score:** 5

**TL;DR:** Conch is an interactive visualization system that analyzes competitive debates using novel visualization techniques and large language models to enhance participants' understanding and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve argumentative skills and debating performance, participants need effective tools to analyze competitive debates rather than relying on manual analysis of textual records.

**Method:** Conch employs a novel parallel spiral visualization to represent clash points and participant interactions and utilizes large language models to automatically identify critical debate elements.

**Key Contributions:**

	1. Introduction of parallel spiral visualization for debate analysis
	2. Use of large language models to identify critical debate elements
	3. Empirical validation of effectiveness through real-world case studies

**Result:** The effectiveness and usability of Conch were demonstrated through case studies on real-world debates and a user study.

**Limitations:** 

**Conclusion:** Conch provides a comprehensive understanding of debate context, improving the analysis and strategies of debaters.

**Abstract:** In-depth analysis of competitive debates is essential for participants to develop argumentative skills and refine strategies, and further improve their debating performance. However, manual analysis of unstructured and unlabeled textual records of debating is time-consuming and ineffective, as it is challenging to reconstruct contextual semantics and track logical connections from raw data. To address this, we propose Conch, an interactive visualization system that systematically analyzes both what is debated and how it is debated. In particular, we propose a novel parallel spiral visualization that compactly traces the multidimensional evolution of clash points and participant interactions throughout debate process. In addition, we leverage large language models with well-designed prompts to automatically identify critical debate elements such as clash points, disagreements, viewpoints, and strategies, enabling participants to understand the debate context comprehensively. Finally, through two case studies on real-world debates and a carefully-designed user study, we demonstrate Conch's effectiveness and usability for competitive debate analysis.

</details>


### [5] ["It looks sexy but it's wrong." Tensions in creativity and accuracy using genAI for biomedical visualization](https://arxiv.org/abs/2507.14494)

*Roxanne Ziman, Shehryar Saharan, Gaël McGill, Laura Garrison*

**Main category:** cs.HC

**Keywords:** generative AI, biomedical visualization, trustworthiness, human-computer interaction, science communication

**Relevance Score:** 7

**TL;DR:** This paper analyzes the use of generative AI in biomedical visualization, assessing its impact on accuracy and trustworthiness through interviews with practitioners, and emphasizes the need for human oversight in design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the workflows and tensions in biomedical visualization arising from the use of generative AI and its implications for accuracy and trustworthiness in scientific communication.

**Method:** The study conducted 17 qualitative interviews with practitioners and researchers in biomedical visualization to gather insights on their experiences and perspectives regarding generative AI tools.

**Key Contributions:**

	1. In-depth analysis of the use of generative AI in biomedical visualization.
	2. Qualitative insights from diverse practitioners highlighting the tension between innovation and accuracy.
	3. Recommendations for integrating human oversight in AI-assisted design processes.

**Result:** The findings reveal a spectrum of attitudes towards generative AI among BioMedVis professionals, highlighting issues with accuracy and the importance of human involvement in design processes to ensure trustworthiness of visual representations.

**Limitations:** The study is limited to interviews and does not include quantitative data on the impact of genAI in biomedical visualization workflows.

**Conclusion:** The paper calls for careful consideration of generative AI's role in biomedical visualization, advocating for human intervention to maintain the integrity of scientific visuals amidst the automation trends.

**Abstract:** We contribute an in-depth analysis of the workflows and tensions arising from generative AI (genAI) use in biomedical visualization (BioMedVis). Although genAI affords facile production of aesthetic visuals for biological and medical content, the architecture of these tools fundamentally limits the accuracy and trustworthiness of the depicted information, from imaginary (or fanciful) molecules to alien anatomy. Through 17 interviews with a diverse group of practitioners and researchers, we qualitatively analyze the concerns and values driving genAI (dis)use for the visual representation of spatially-oriented biomedical data. We find that BioMedVis experts, both in roles as developers and designers, use genAI tools at different stages of their daily workflows and hold attitudes ranging from enthusiastic adopters to skeptical avoiders of genAI. In contrasting the current use and perspectives on genAI observed in our study with predictions towards genAI in the visualization pipeline from prior work, our refocus the discussion of genAI's effects on projects in visualization in the here and now with its respective opportunities and pitfalls for future visualization research. At a time when public trust in science is in jeopardy, we are reminded to first do no harm, not just in biomedical visualization but in science communication more broadly. Our observations reaffirm the necessity of human intervention for empathetic design and assessment of accurate scientific visuals.

</details>


### [6] [PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration](https://arxiv.org/abs/2507.14527)

*Runhua Zhang, Yang Ouyang, Leixian Shen, Yuying Tang, Xiaojuan Ma, Huamin Qu, Xian Xu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, large language models, academic communication, interdisciplinary research, narrative organization

**Relevance Score:** 9

**TL;DR:** PaperBridge is a human-AI system that helps researchers organize publications into coherent narratives, particularly useful in interdisciplinary fields like HCI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Researchers need to effectively synthesize and communicate their scholarly contributions, especially in diverse contexts and interdisciplinary fields, making organization challenging.

**Method:** A formative study and content analysis informed the development of PaperBridge, which includes a bi-directional analysis engine powered by large language models to support narrative organization and exploration.

**Key Contributions:**

	1. Development of PaperBridge, a system for narrative organization
	2. Use of large language models for bi-directional analysis
	3. Insights into interactive systems facilitating academic communication

**Result:** A user study involving 12 participants demonstrated PaperBridge's usability and effectiveness in helping researchers explore alternative narratives for their work.

**Limitations:** The study involved a small number of participants (N=12), which may limit generalizability.

**Conclusion:** The findings provide empirical insights on how interactive systems can assist in academic communication tasks, showcasing the potential of AI in scholarly synthesis.

**Abstract:** Researchers frequently need to synthesize their own publications into coherent narratives that demonstrate their scholarly contributions. To suit diverse communication contexts, exploring alternative ways to organize one's work while maintaining coherence is particularly challenging, especially in interdisciplinary fields like HCI where individual researchers' publications may span diverse domains and methodologies. In this paper, we present PaperBridge, a human-AI co-exploration system informed by a formative study and content analysis. PaperBridge assists researchers in exploring diverse perspectives for organizing their publications into coherent narratives. At its core is a bi-directional analysis engine powered by large language models, supporting iterative exploration through both top-down user intent (e.g., determining organization structure) and bottom-up refinement on narrative components (e.g., thematic paper groupings). Our user study (N=12) demonstrated PaperBridge's usability and effectiveness in facilitating the exploration of alternative research narratives. Our findings also provided empirical insights into how interactive systems can scaffold academic communication tasks.

</details>


### [7] [Uncovering the EEG Temporal Representation of Low-dimensional Object Properties](https://arxiv.org/abs/2507.14537)

*Jiahua Tang, Song Wang, Jiachen Zou, Chen Wei, Quanying Liu*

**Main category:** cs.HC

**Keywords:** EEG, visual decoding, brain-computer interfaces, neural representations, temporal dynamics

**Relevance Score:** 4

**TL;DR:** This paper proposes a novel approach to investigate how low-dimensional object properties are temporally encoded in EEG signals, enhancing the interpretability of neural representations and advancing visual decoding in brain-computer interfaces.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding the temporal dynamics of neural representations based on EEG, which remains underexplored compared to fMRI despite its advantages in temporal resolution.

**Method:** Integrating advanced neural decoding algorithms to analyze temporal encoding of object properties in EEG signals.

**Key Contributions:**

	1. Novel integration of neural decoding algorithms with EEG analysis
	2. First attempt to identify prototypical temporal characteristics of concepts in EEG
	3. Insights into visual decoding applicable to brain-computer interfaces

**Result:** The framework enables identification of specificity and prototypical temporal characteristics of concepts within EEG signals.

**Limitations:** 

**Conclusion:** The proposed approach enhances interpretability of neural representations and provides insights for visual decoding in BCIs.

**Abstract:** Understanding how the human brain encodes and processes external visual stimuli has been a fundamental challenge in neuroscience. With advancements in artificial intelligence, sophisticated visual decoding architectures have achieved remarkable success in fMRI research, enabling more precise and fine-grained spatial concept localization. This has provided new tools for exploring the spatial representation of concepts in the brain. However, despite the millisecond-scale temporal resolution of EEG, which offers unparalleled advantages in tracking the dynamic evolution of cognitive processes, the temporal dynamics of neural representations based on EEG remain underexplored. This is primarily due to EEG's inherently low signal-to-noise ratio and its complex spatiotemporal coupling characteristics. To bridge this research gap, we propose a novel approach that integrates advanced neural decoding algorithms to systematically investigate how low-dimensional object properties are temporally encoded in EEG signals. We are the first to attempt to identify the specificity and prototypical temporal characteristics of concepts within temporal distributions. Our framework not only enhances the interpretability of neural representations but also provides new insights into visual decoding in brain-computer interfaces (BCI).

</details>


### [8] [EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences](https://arxiv.org/abs/2507.14685)

*Luis Montana, Jessica Magallanes, Miguel Juarez, Suzanne Mason, Andrew Narracott, Lindsey van Gemeren, Steven Wood, Maria-Cruz Villa-Uriol*

**Main category:** cs.HC

**Keywords:** Event sequences, Visual analytics, Statistics, Data representation, Healthcare

**Relevance Score:** 7

**TL;DR:** EventBox is a novel visual analytics method for analyzing event sequences, integrated into Sequen-C, enhancing exploration of temporal and multivariate attributes through user-driven transformations and statistical analyses.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective methods to analyze event sequence data to facilitate decision-making, particularly given the abundant availability of such data.

**Method:** Integration of EventBox into Sequen-C, allowing for user-driven transformations like alignment, sorting, substitution, and aggregation, along with automated statistical analyses.

**Key Contributions:**

	1. Introduction of a new data representation and visual encoding technique (EventBox).
	2. User-driven transformations for agile data analysis.
	3. Application of the ICE-T framework to evaluate visualization effectiveness.

**Result:** Evaluation with 21 participants showed that EventBox improves visualization value and user performance in analyzing event sequences, revealing significant patterns in healthcare data.

**Limitations:** 

**Conclusion:** EventBox enhances the depth of visual analytics, providing a flexible framework for examining complex interactions in event sequences.

**Abstract:** The rapid growth and availability of event sequence data across domains requires effective analysis and exploration methods to facilitate decision-making. Visual analytics combines computational techniques with interactive visualizations, enabling the identification of patterns, anomalies, and attribute interactions. However, existing approaches frequently overlook the interplay between temporal and multivariate attributes. We introduce EventBox, a novel data representation and visual encoding approach for analyzing groups of events and their multivariate attributes. We have integrated EventBox into Sequen-C, a visual analytics system for the analysis of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we have added user-driven transformations, including alignment, sorting, substitution and aggregation. To enhance analytical depth, we incorporate automatically generated statistical analyses, providing additional insight into the significance of attribute interactions. We evaluated our approach involving 21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T framework to assess visualization value, user performance metrics completing a series of tasks, and interactive sessions with domain experts. We also present three case studies with real-world healthcare data demonstrating how EventBox and its integration into Sequen-C reveal meaningful patterns, anomalies, and insights. These results demonstrate that our work advances visual analytics by providing a flexible solution for exploring temporal and multivariate attributes in event sequences.

</details>


### [9] [A Notification Based Nudge for Handling Excessive Smartphone Use](https://arxiv.org/abs/2507.14702)

*Partha Sarker, Dipto Dey, Marium-E-Jannat*

**Main category:** cs.HC

**Keywords:** smartphone overuse, notification-based intervention, self-awareness, user study, nudge techniques

**Relevance Score:** 7

**TL;DR:** This study presents a notification-based intervention to reduce smartphone overuse by increasing user self-awareness without causing annoyance.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Address the global issue of excessive smartphone usage, particularly among undergraduates, without making users feel restricted or annoyed.

**Method:** Developed a prototype app, "App Usage Monitor," and conducted a 3-week user study with 109 participants to test the effectiveness of notifications and nudges.

**Key Contributions:**

	1. Proposed a novel notification-based intervention for smartphone overuse
	2. Developed and tested a prototype that combines nudges and visualization
	3. Showcased the importance of self-awareness in reducing smartphone usage

**Result:** The intervention showed promising results, indicating that self-awareness can be increased through strategic notifications, leading to reduced smartphone usage.

**Limitations:** The study is limited by its small sample size and short duration of the experiment.

**Conclusion:** The study demonstrates that a non-intrusive approach utilizing notifications can effectively minimize smartphone overuse among users.

**Abstract:** Excessive use of smartphones is a worldwide known issue. In this study, we proposed a notification-based intervention approach to reduce smartphone overuse without making the user feel any annoyance or irritation. Most of the work in this field tried to reduce smartphone overuse by making smartphone use more difficult for the user. In our user study (n = 109), we found that 19.3% of the participants are unwilling to use any usage-limiting application because a) they do not want their smartphone activities to get restricted or b) those applications are annoying. Following that, we devised a hypothesis to minimize smartphone usage among undergraduates. Finally, we designed a prototype for Android, "App Usage Monitor," and conducted a 3-week experiment through which we found proof of concept for our hypothesis. In our prototype, we combined techniques such as nudge and visualization to increase self-awareness among the user by leveraging notifications.

</details>


### [10] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)

*Yanming Zhang, Krishnakumar Hegde, Klaus Mueller*

**Main category:** cs.HC

**Keywords:** Causal reasoning, Visual analytics, Subpopulations

**Relevance Score:** 4

**TL;DR:** This paper introduces XplainAct, a visual analytics framework for simulating and reasoning interventions at the individual level within heterogeneous subpopulations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing causal reasoning methods mainly focus on population-level effects, which are inadequate for systems with significant heterogeneity in intervention outcomes.

**Method:** XplainAct framework that enables visual analytics for individual-level intervention simulation and explanation within subpopulations.

**Key Contributions:**

	1. Introduction of the XplainAct framework for individual-level intervention exploration
	2. Application of the framework in diverse case studies
	3. Focus on addressing heterogeneity in causal outcomes

**Result:** Demonstrated effectiveness through case studies on opioid-related deaths and voting inclinations.

**Limitations:** 

**Conclusion:** XplainAct provides enhanced causal reasoning capabilities for complex systems, allowing for better understanding and intervention analysis at the individual level.

**Abstract:** Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.

</details>


### [11] [Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs](https://arxiv.org/abs/2507.14769)

*Ananya Gubbi Mohanbabu, Yotam Sechayk, Amy Pavel*

**Main category:** cs.HC

**Keywords:** web accessibility, human-computer interaction, large language models

**Relevance Score:** 9

**TL;DR:** Task Mode is a system that enhances web accessibility for screen reader users by filtering content based on user-defined goals, demonstrated to improve task completion time while supporting visual users.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** Web interfaces are overly complex and difficult for screen reader users to navigate, leading to increased task completion times and accessibility disparities.

**Method:** Task Mode utilizes large language models to dynamically filter web content according to user goals, preserving page structure and providing various viewing modes for different user needs.

**Key Contributions:**

	1. Introduction of Task Mode for dynamic content filtering
	2. Improvement of task completion times for screen reader users
	3. Reduction of accessibility gaps in web interactions

**Result:** A user study involving 12 participants showed that Task Mode significantly reduced task completion time for screen reader users while maintaining performance for visual users, lowering the completion time gap from 2x to 1.2x.

**Limitations:** Study limited to a small sample size of 12 participants.

**Conclusion:** Task Mode demonstrates that new interaction designs can reduce accessibility disparities and improve usability for both visual and non-visual users.

**Abstract:** Modern web interfaces are unnecessarily complex to use as they overwhelm users with excessive text and visuals unrelated to their current goals. This problem particularly impacts screen reader users (SRUs), who navigate content sequentially and may spend minutes traversing irrelevant elements before reaching desired information compared to vision users (VUs) who visually skim in seconds. We present Task Mode, a system that dynamically filters web content based on user-specified goals using large language models to identify and prioritize relevant elements while minimizing distractions. Our approach preserves page structure while offering multiple viewing modes tailored to different access needs. Our user study with 12 participants (6 VUs, 6 SRUs) demonstrates that our approach reduced task completion time for SRUs while maintaining performance for VUs, decreasing the completion time gap between groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the future, reporting that Task Mode supported completing tasks with less effort and fewer distractions. This work demonstrates how designing new interactions simultaneously for visual and non-visual access can reduce rather than reinforce accessibility disparities in future technology created by human-computer interaction researchers and practitioners.

</details>


### [12] [SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors](https://arxiv.org/abs/2507.14792)

*Kaixin Ji, Danula Hettiachchi, Falk Scholer, Flora D. Salim, Damiano Spina*

**Main category:** cs.HC

**Keywords:** Information processing, Cognitive mechanisms, Physiological data, Information seeking, SenseSeek dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces the SenseSeek dataset, which captures physiological and behavioral data from individuals during information searching tasks, aiming to enhance understanding of user interactions with information systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand complex cognitive behaviors during information processing tasks and improve information-seeking strategies through personalized data capture.

**Method:** The SenseSeek dataset was created by collecting data from 20 participants across 235 trials of a stimulated search process, using consumer-grade sensors to capture physiological indicators such as EDA, EEG, and motion data.

**Key Contributions:**

	1. Introduction of the SenseSeek dataset for information-seeking behaviors
	2. Characterization of cognitive stages using physiological data
	3. Baseline analysis of sensor data effectiveness in discriminating search stages

**Result:** The analysis demonstrates the effectiveness of the dataset in discriminating different cognitive intents and interaction stages during the search process.

**Limitations:** 

**Conclusion:** SenseSeek is the first dataset to characterize multiple stages of information seeking with physiological signals, serving as a potential reference for future research in this field.

**Abstract:** Information processing tasks involve complex cognitive mechanisms that are shaped by various factors, including individual goals, prior experience, and system environments. Understanding such behaviors requires a sophisticated and personalized data capture of how one interacts with modern information systems (e.g., web search engines). Passive sensors, such as wearables, capturing physiological and behavioral data, have the potential to provide solutions in this context. This paper presents a novel dataset, SenseSeek, designed to evaluate the effectiveness of consumer-grade sensors in a complex information processing scenario: searching via systems (e.g., search engines), one of the common strategies users employ for information seeking. The SenseSeek dataset comprises data collected from 20 participants, 235 trials of the stimulated search process, 940 phases of stages in the search process, including the realization of Information Need (IN), Query Formulation (QF), Query Submission by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R) or Listening (RJ-L). The data includes Electrodermal Activities (EDA), Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured using consumer-grade sensors. It also contains 258 features extracted from the sensor data, the gaze-annotated screen recordings, and task responses. We validate the usefulness of the dataset by providing baseline analysis on the impacts of different cognitive intents and interaction modalities on the sensor data, and effectiveness of the data in discriminating the search stages. To our knowledge, SenseSeek is the first dataset that characterizes the multiple stages involved in information seeking with physiological signals collected from multiple sensors. We hope this dataset can serve as a reference for future research on information-seeking behaviors.

</details>


### [13] [Understanding How Visually Impaired Players Socialize in Mobile Games](https://arxiv.org/abs/2507.14818)

*Zihe Ran, Xiyu Li, Qing Xiao, Yanyun Wang, Franklin Mingzhe Li, Zhicong Lu*

**Main category:** cs.HC

**Keywords:** Mobile Games, Visually Impaired, Socialization, Accessibility, Inclusive Design

**Relevance Score:** 7

**TL;DR:** The study investigates how visually impaired players in China engage in mobile gaming for socialization and community integration, uncovering challenges they face in accessibility and community support.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Mobile games offer a vital medium for social interaction, especially for visually impaired individuals who encounter challenges in offline social settings.

**Method:** Interviews conducted with 30 visually impaired players in China to understand their social gaming experiences and community integration.

**Key Contributions:**

	1. Insights on social experiences of visually impaired mobile gamers in China
	2. Identification of technological and community challenges faced by players
	3. Recommendations for designing more inclusive mobile games

**Result:** The study reveals that mobile games fulfill many social needs for visually impaired individuals, but challenges such as technological barriers and insufficient accessibility features hinder participation.

**Limitations:** The study focuses on visually impaired players in China, which may limit the generalizability of the findings to other cultures or regions.

**Conclusion:** The research highlights the need for more inclusive and accessible mobile game designs to better support the social experiences of visually impaired players.

**Abstract:** Mobile games are becoming a vital medium for social interaction, offering a platform that transcends geographical boundaries. An increasing number of visually impaired individuals are engaging in mobile gaming to connect, collaborate, compete, and build friendships. In China, visually impaired communities face significant social challenges in offline settings, making mobile games a crucial avenue for socialization. However, the design of mobile games and their mapping to real-world environments significantly shape their social gaming experiences. This study explores how visually impaired players in China navigate socialization and integrate into gaming communities. Through interviews with 30 visually impaired players, we found that while mobile games fulfill many of their social needs, technological barriers and insufficient accessibility features, and internal community divisions present significant challenges to their participation. This research sheds light on their social experiences and offers insights for designing more inclusive and accessible mobile games.

</details>


### [14] [Progressive Sentences: Combining the Benefits of Word and Sentence Learning](https://arxiv.org/abs/2507.14846)

*Nuwan Janaka, Shengdong Zhao, Ashwin Ram, Ruoxin Sun, Sherisse Tan Jing Wen, Danae Li, David Hsu*

**Main category:** cs.HC

**Keywords:** augmented reality, language acquisition, multimodal learning, mobile learning, progressive presentation

**Relevance Score:** 6

**TL;DR:** This research examines the use of lightweight AR smart glasses to enhance second-language acquisition through a progressive presentation method.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how augmented reality can facilitate mobile learning, particularly in language acquisition, by delivering multimodal information.

**Method:** The study utilizes a 'progressive presentation' method that sequentially shows sentence components and incorporates timed gaps for improved learning.

**Key Contributions:**

	1. Demonstrated efficacy of AR in mobile second-language learning
	2. Introduced the progressive presentation method combining word and sentence structures
	3. Provided guidelines for educational use of AR technology

**Result:** Pilot and formal studies show that progressive presentation enhances recall, especially during mobile activities like walking, with timed gaps further improving effectiveness.

**Limitations:** 

**Conclusion:** The research concludes that progressive presentation is beneficial for language learning in mobile contexts and offers guidelines for educational applications.

**Abstract:** The rapid evolution of lightweight consumer augmented reality (AR) smart glasses (a.k.a. optical see-through head-mounted displays) offers novel opportunities for learning, particularly through their unique capability to deliver multimodal information in just-in-time, micro-learning scenarios. This research investigates how such devices can support mobile second-language acquisition by presenting progressive sentence structures in multimodal formats. In contrast to the commonly used vocabulary (i.e., word) learning approach for novice learners, we present a "progressive presentation" method that combines both word and sentence learning by sequentially displaying sentence components (subject, verb, object) while retaining prior context. Pilot and formal studies revealed that progressive presentation enhances recall, particularly in mobile scenarios such as walking. Additionally, incorporating timed gaps between word presentations further improved learning effectiveness under multitasking conditions. Our findings demonstrate the utility of progressive presentation and provide usage guidelines for educational applications-even during brief, on-the-go learning moments.

</details>


### [15] [Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications](https://arxiv.org/abs/2507.14859)

*Nils Mandischer, Alexander Atanasyan, Ulrich Dahmen, Michael Schluse, Jürgen Rossmann, Lars Mikelsons*

**Main category:** cs.HC

**Keywords:** human digital twin, functionality levels, requirements engineering

**Relevance Score:** 6

**TL;DR:** This paper presents a holistic vision and specification of the human digital twin, detailing its functionalities, requirements, and potential applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The existing definitions and applications of human digital twins are unclear, hindering awareness of their market potential.

**Method:** The paper defines the specifications of a holistic human digital twin by identifying requirements, stakeholders, and users, and categorizes applications based on six functionality levels: store, analyze, personalize, predict, control, and optimize.

**Key Contributions:**

	1. Introduces a holistic vision of human digital twins.
	2. Defines six levels of functionality for human digital twins.
	3. Derives comprehensive requirements for practical implementation.

**Result:** The study showcases three detailed applications to demonstrate functionality levels and stakeholder analysis, ultimately deriving a comprehensive list of requirements for implementing human digital twins in various applications.

**Limitations:** 

**Conclusion:** The findings offer a guideline for researchers and industries to develop and implement human digital twins, emphasizing reusability across multiple target applications.

**Abstract:** The digital twin of humans is a relatively new concept. While many diverse definitions, architectures, and applications exist, a clear picture is missing on what, in fact, makes a human digital twin. Within this context, researchers and industrial use-case owners alike are unaware about the market potential of the - at the moment - rather theoretical construct. In this work, we draw a holistic vision of the human digital twin, and derive the specification of this holistic human digital twin in form of requirements, stakeholders, and users. For each group of users, we define exemplary applications that fall into the six levels of functionality: store, analyze, personalize, predict, control, and optimize. The functionality levels facilitate an abstraction of abilities of the human digital twin. From the manifold applications, we discuss three in detail to showcase the feasibility of the abstraction levels and the analysis of stakeholders and users. Based on the deep discussion, we derive a comprehensive list of requirements on the holistic human digital twin. These considerations shall be used as a guideline for research and industries for the implementation of human digital twins, particularly in context of reusability in multiple target applications.

</details>


### [16] [LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection](https://arxiv.org/abs/2507.14944)

*Boning Zhao, Yutong Hu*

**Main category:** cs.HC

**Keywords:** Large Language Models, Expert knowledge, Value alignment, AI behavior design, Layered Expert Knowledge Injection Architecture

**Relevance Score:** 9

**TL;DR:** This paper introduces the Layered Expert Knowledge Injection Architecture (LEKIA) as a collaborative approach to integrating deep expert knowledge and value alignment in Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the dual challenge of injecting expert knowledge and ensuring value alignment in deploying LLMs in high-stakes domains.

**Method:** The paper proposes the Layered Expert Knowledge Injection Architecture (LEKIA), which consists of a Theoretical Layer, a Practical Layer, and an Evaluative Layer to guide LLM reasoning without altering weights.

**Key Contributions:**

	1. Introduction of a unified framework for knowledge injection and alignment in LLMs.
	2. Development of the LEKIA architecture with three distinct layers for enhanced AI behavior design.
	3. Practical implementation in the context of a psychological support assistant to demonstrate effectiveness.

**Result:** LEKIA was successfully implemented in a psychological support assistant for the special education field, demonstrating its efficacy in unifying knowledge and alignment.

**Limitations:** 

**Conclusion:** The paradigm presents a path toward more responsible and expert-driven AI, enhancing the capability of domain specialists in AI behavior design.

**Abstract:** Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a dual challenge: the need for deep, dynamic expert knowledge injection and nuanced value alignment. Prevailing paradigms often address these challenges separately, creating a persistent tension between knowledge and alignment; knowledge-focused methods like Retrieval-Augmented Generation (RAG) have limited deep alignment capabilities, while alignment-focused methods like Reinforcement Learning from Human Feedback (RLHF) struggle with the agile injection of expert wisdom. This paper introduces a new collaborative philosophy, Expert-owned AI behavior design, realized through Architectural Alignment-a paradigm that unifies these two goals within a single framework called the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA operates as an intelligent intermediary that guides an LLM's reasoning process without altering its weights, utilizing a three-tiered structure: a Theoretical Layer for core principles, a Practical Layer for exemplary cases, and an Evaluative Layer for real-time, value-aligned self-correction. We demonstrate the efficacy of this paradigm through the successful implementation of a LEKIA-based psychological support assistant for the special education field. Our work presents a path toward more responsible and expert-driven AI, empowering domain specialists to directly architect AI behavior and resolve the tension between knowledge and alignment.

</details>


### [17] [Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake](https://arxiv.org/abs/2507.14947)

*Ivan C. H. Liu, Chung-En Hao, Jing Xie*

**Main category:** cs.HC

**Keywords:** interactive installation, seismic dynamics, multisensory experience, sound synthesis, emergent complexity

**Relevance Score:** 4

**TL;DR:** An interactive installation, Echoes of the Land, transforms seismic dynamics into a multisensory experience using a spring-block model, simulating earthquakes through sound and light.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the intersection of scientific knowledge and artistic practice, creating a new medium for musical and narrative expression.

**Method:** The installation employs motion capture and concatenative granular synthesis to simulate earthquake recurrence and self-organized criticality, with each block acting as an agent in the system.

**Key Contributions:**

	1. Introduction of a multisensory installation based on seismic dynamics
	2. Use of real-time sound and light generation in artistic practice
	3. Exploration of emergent complexity in HCI and audiovisual experiences

**Result:** Emergent audiovisual cascades visualize the physics of rupture and threshold behavior, showcasing a novel way to experience seismic dynamics.

**Limitations:** 

**Conclusion:** The work opens new avenues for exploration in emergent complexity, aesthetics, and interactivity, potentially leading to new forms of musical instruments and narratives.

**Abstract:** Echoes of the Land is an interactive installation that transforms seismic dynamics into a multisensory experience through a scientifically grounded spring-block model. Simulating earthquake recurrence and self-organized criticality, the work generates real-time sound and light via motion capture and concatenative granular synthesis. Each block acts as an agent, producing emergent audiovisual cascades that visualize the physics of rupture and threshold behavior. This work exemplifies the amalgamation of scientific knowledge and artistic practice, opening new avenues for novel forms of musical instrument and narrative medium, while inviting further investigation into the intersection of emergent complexity, aesthetics and interactivity.

</details>


### [18] [Emphasizing Deliberation and Critical Thinking in an AI Hype World](https://arxiv.org/abs/2507.14961)

*Katja Rogers*

**Main category:** cs.HC

**Keywords:** AI solutionism, Human-Computer Interaction, critical engagement, technology use, education

**Relevance Score:** 7

**TL;DR:** The paper discusses the challenges posed by AI solutionism and proposes a cautious and critical approach to technology use in HCI to mitigate harmful impacts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the detrimental effects of rapid AI adoption and hype in human-computer interaction and education.

**Method:** The authors advocate for a slower, more deliberate approach to technology implementation while promoting critical engagement with AI.

**Key Contributions:**

	1. Proposes a framework for critical engagement with AI technologies in HCI.
	2. Highlights the need for a conscientious approach to avoid the pitfalls of AI solutionism.
	3. Suggests strategies for navigating technology use post-hype.

**Result:** A framework for navigating post-AI hype by encouraging researchers and educators to adopt intentional practices towards technology.

**Limitations:** The approach may not provide immediate solutions for all stakeholders in technology adoption.

**Conclusion:** Deliberate and critical engagement with AI technologies can help establish a robust knowledge base and reduce negative consequences in education and research contexts.

**Abstract:** AI solutionism is accelerated and substantiated by hype and HCI's elevation of novelty. Banning or abandoning technology is unlikely to work and probably not beneficial on the whole either -- but slow(er), deliberate use together with conscientious, critical engagement and non-engagement may help us navigate a post-AI hype world while contributing to a solid knowledge foundation and reducing harmful impacts in education and research.

</details>


### [19] ['A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data](https://arxiv.org/abs/2507.15033)

*Dhruvee Birla, Nazia Akhtar*

**Main category:** cs.HC

**Keywords:** LGBTQ+, social media, sentiment analysis, topic modeling, pandemic

**Relevance Score:** 4

**TL;DR:** This study analyzes Reddit discussions in LGBTQ+ communities during the pandemic using LDA topic modeling and sentiment analysis to uncover themes and attitudes.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the experiences of specific groups, particularly LGBTQ+ individuals, during the pandemic through social media.

**Method:** Utilized LDA topic modeling and sentiment analysis on data from five subreddits focused on LGBTQ+ issues to analyze themes and sentiments.

**Key Contributions:**

	1. Application of LDA topic modeling in social media analysis
	2. Insights into LGBTQ+ community sentiment during a crisis
	3. Comparison of discussions before and during the pandemic

**Result:** Identified key themes and sentiments expressed in LGBTQ+ subreddits during the pandemic and assessed changes compared to pre-pandemic discussions.

**Limitations:** Study limited to data from Reddit and may not represent all LGBTQ+ experiences; potential biases in sentiment analysis.

**Conclusion:** Reddit served as a significant platform for LGBTQ+ individuals to share experiences during the pandemic, revealing both continuities and changes in discourse from previous periods.

**Abstract:** Social media was one of the most popular forms of communication among young people with digital access during the pandemic. Consequently, crucial debates and discussions about the pandemic crisis have also developed on social media platforms, making them a great primary source to study the experiences of specific groups and communities during the pandemic. This study involved research using LDA topic modeling and sentiment analysis on data obtained from the social media platform Reddit to understand the themes and attitudes in circulation within five subreddits devoted to LGBTQ+ experiences and issues. In the process, we attempt to make sense of the role that Reddit may have played in the lives of LGBTQ+ people who were online during the pandemic, and whether this was marked by any continuities or discontinuities from before the pandemic period.

</details>


### [20] [Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic](https://arxiv.org/abs/2507.15041)

*Dhruvee Birla, Nazia Akhtar*

**Main category:** cs.HC

**Keywords:** LGBTQ+, COVID-19, sentiment analysis, topic modeling, Indian media

**Relevance Score:** 2

**TL;DR:** The study examines the representation of LGBTQ+ communities in Indian online news media during the COVID-19 pandemic, focusing on articles from The Times of India and The Indian Express.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the reporting on LGBTQ+ communities, particularly transgender individuals, in Indian online news media amidst the COVID-19 pandemic, highlighting issues of visibility and representation.

**Method:** The study utilized sentiment analysis and topic modeling on articles published from March 2020 to August 2021, comparing results to the previous period (January 2019 - December 2019).

**Key Contributions:**

	1. Empirical analysis of LGBTQ+ representation during the pandemic
	2. Sentiment analysis and topic modeling methodology applied to news articles
	3. Comparison of pre-pandemic and pandemic reporting

**Result:** The analysis revealed that both newspapers published more articles regarding LGBTQ+ individuals, especially transgender issues, but lacked depth and quality. Certain articles also exhibited transphobic language.

**Limitations:** The analysis was limited to only two newspapers and may not represent the entirety of online news media in India.

**Conclusion:** The study underscores the challenges in quality representation of LGBTQ+ communities in Indian media, indicating a need for improvement in coverage and language.

**Abstract:** In India, online news media outlets were an important source of information for people with digital access during the COVID-19 pandemic. In India, where "transgender" was legally recognised as a category only in 2014, and same-sex marriages are yet to be legalised, it becomes crucial to analyse whether and how they reported the lived realities of vulnerable LGBTQ+ communities during the pandemic. This study analysed articles from online editions of two English-language newspaper websites, which differed vastly in their circulation figures-The Times of India and The Indian Express. The results of our study suggest that these newspaper websites published articles surrounding various aspects of the lives of LGBTQ+ individuals with a greater focus on transgender communities. However, they lacked quality and depth. Focusing on the period spanning March 2020 to August 2021, we analysed articles using sentiment analysis and topic modelling. We also compared our results to the period before the pandemic (January 2019 - December 2019) to understand the shift in topics, sentiments, and stances across the two newspaper websites. A manual analysis of the articles indicated that the language used in certain articles by The Times of India was transphobic and obsolete. Our study captures the visibility and representation of the LGBTQ+ communities in Indian newspaper websites during the pandemic.

</details>


### [21] [Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence](https://arxiv.org/abs/2507.15049)

*Andres Navarro, Carlos de Quinto, José Alberto Hernández*

**Main category:** cs.HC

**Keywords:** Unmanned Aerial Vehicles, 5G communications, Edge-based processing, AI, Non-Terrestrial Networks

**Relevance Score:** 5

**TL;DR:** The paper presents a budget-friendly quadcopter platform that combines 5G communications, edge-based processing, and AI to enhance Non-Terrestrial Networks with capabilities such as object recognition and contextual analysis, validated through field evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address core challenges in Non-Terrestrial Network scenarios using advanced technologies like 5G, edge processing, and AI.

**Method:** Introduction of a quadcopter platform equipped with a panoramic camera, onboard computation, and LLMs for various operational scenarios.

**Key Contributions:**

	1. Development of a budget-friendly quadcopter platform for NTN
	2. Integration of 5G, edge processing, and LLMs for enhanced situational awareness
	3. Validation of the platform through practical use cases

**Result:** Successful field evaluations demonstrated low-latency processing of visual streams and robust 5G connectivity, showcasing the system's capabilities.

**Limitations:** 

**Conclusion:** The quadcopter system is adaptable for use cases such as emergency response, infrastructure assessment, and environmental surveillance.

**Abstract:** Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as agile, intelligent nodes capable of advanced analytics and instantaneous situational awareness. This article introduces a budget-friendly quadcopter platform that unites 5G communications, edge-based processing, and AI to tackle core challenges in NTN scenarios. Outfitted with a panoramic camera, robust onboard computation, and LLMs, the drone system delivers seamless object recognition, contextual analysis, and immersive operator experiences through virtual reality VR technology. Field evaluations confirm the platform's ability to process visual streams with low latency and sustain robust 5G links. Adding LLMs further streamlines operations by extracting actionable insights and refining collected data for decision support. Demonstrated use cases, including emergency response, infrastructure assessment, and environmental surveillance, underscore the system's adaptability in demanding contexts.

</details>


### [22] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)

*Maisha Maimuna, Minhaz Bin Farukee, Sama Nikanfar, Mahfuza Siddiqua, Ayon Roy, Fillia Makedon*

**Main category:** cs.HC

**Keywords:** teleoperation, blind and low-vision, multimodal guidance, warehouse robotics, HCI

**Relevance Score:** 8

**TL;DR:** A simulator designed for blind and low-vision operators to teleoperate robots in industrial warehouses using multimodal guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the risks and demands of robot teleoperation for blind and low-vision users in industrial settings, where accessibility is crucial for inclusive workforce participation.

**Method:** The system utilizes a navigation mesh with dynamic re-planning, providing synchronized visual, auditory, and haptic feedback to guide users in a high-fidelity warehouse environment.

**Key Contributions:**

	1. Introduction of a novel multimodal guidance simulator for BLV operators
	2. Combination of visual, auditory, and haptic feedback for real-time navigation
	3. Dynamic path replanning for obstacle avoidance in a high-fidelity environment

**Result:** The simulator successfully enables BLV users to control a mobile robot with enhanced awareness of their surroundings through multimodal feedback, ensuring accurate navigation and obstacle avoidance in real time.

**Limitations:** 

**Conclusion:** This closed-loop guidance simulator serves as a repeatable testbed for accessible teleoperation research, with design principles that can be adapted for real robots, promoting the development of inclusive telerobotic tools.

**Abstract:** Industrial warehouses are congested with moving forklifts, shelves and personnel, making robot teleoperation particularly risky and demanding for blind and low-vision (BLV) operators. Although accessible teleoperation plays a key role in inclusive workforce participation, systematic research on its use in industrial environments is limited, and few existing studies barely address multimodal guidance designed for BLV users. We present a novel multimodal guidance simulator that enables BLV users to control a mobile robot through a high-fidelity warehouse environment while simultaneously receiving synchronized visual, auditory, and haptic feedback. The system combines a navigation mesh with regular re-planning so routes remain accurate avoiding collisions as forklifts and human avatars move around the warehouse. Users with low vision are guided with a visible path line towards destination; navigational voice cues with clockwise directions announce upcoming turns, and finally proximity-based haptic feedback notifies the users of static and moving obstacles in the path. This real-time, closed-loop system offers a repeatable testbed and algorithmic reference for accessible teleoperation research. The simulator's design principles can be easily adapted to real robots due to the alignment of its navigation, speech, and haptic modules with commercial hardware, supporting rapid feasibility studies and deployment of inclusive telerobotic tools in actual warehouses.

</details>


### [23] ["If I were in Space": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives](https://arxiv.org/abs/2507.15081)

*Qi Gong, Ximing Shen, Ziyou Yin, Yaning Li, Ray Lc*

**Main category:** cs.HC

**Keywords:** Social isolation, Virtual reality, Collaborative storytelling, Adaptive strategies, Health informatics

**Relevance Score:** 7

**TL;DR:** This study explores how collaborative online storytelling in social VR can help individuals cope with isolation by using playful narrative experiences to reveal adaptive strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the health issues arising from social isolation, particularly anxiety and loneliness, by examining the narrative potential of adaptive strategies through virtual experiences.

**Method:** Eighteen individuals participated in a collaborative online storytelling experience in social VR, engaging in a virtual role-play where they designed imaginary spaceship rooms to represent their coping mechanisms during quarantine.

**Key Contributions:**

	1. Introduced a novel approach to address isolation through narrative experiences in social VR.
	2. Revealed how collaborative storytelling can unveil adaptive strategies for coping with loneliness.
	3. Demonstrated the unexpected influences of playful narratives on adaptation processes.

**Result:** Qualitative analyses indicated that participants revealed creative adaptative strategies through their designs and interactions, influencing their adaptation process unexpectedly.

**Limitations:** The study primarily involved a small sample size and was conducted during specific quarantine conditions, which may limit generalizability.

**Conclusion:** Designing playful narrative experiences in social VR can serve as effective probes to better understand how individuals navigate social isolation, highlighting the value of creativity over traditional solution-driven approaches.

**Abstract:** Social isolation can lead to pervasive health issues like anxiety and loneliness. Previous work focused on physical interventions like exercise and teleconferencing, but overlooked the narrative potential of adaptive strategies. To address this, we designed a collaborative online storytelling experience in social VR, enabling participants in isolation to design an imaginary space journey as a metaphor for quarantine, in order to learn about their isolation adaptation strategies in the process. Eighteen individuals participated during real quarantine undertaken a virtual role-play experience, designing their own spaceship rooms and engaging in collaborative activities that revealed creative adaptative strategies. Qualitative analyses of participant designs, transcripts, and interactions revealed how they coped with isolation, and how the engagement unexpectedly influenced their adaptation process. This study shows how designing playful narrative experiences, rather than solution-driven approaches, can serve as probes to surface how people navigate social isolation.

</details>


### [24] [TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style](https://arxiv.org/abs/2507.15202)

*Karim Benharrak, Puyuan Peng, Amy Pavel*

**Main category:** cs.HC

**Keywords:** speech editing, automation, user interface, AI, cognitive load

**Relevance Score:** 8

**TL;DR:** TalkLess is a system that combines extraction and abstraction to automate speech editing, preserving both content and speaker style.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Editing speech recordings is a tedious process for many creators, necessitating a solution that enhances efficiency without sacrificing personal style.

**Method:** TalkLess generates possible transcript edits that maximize compression and audio quality, using a speech editing model to convert transcript changes into audio edits, and provides an interface that separates different levels of editing.

**Key Contributions:**

	1. Combines extraction and abstraction for automated speech editing
	2. Offers a user-friendly interface with two distinct editing panes
	3. Demonstrates significant improvements in editing efficiency and effectiveness compared to existing methods

**Result:** TalkLess outperformed a leading extractive approach in terms of coverage and error reduction, and reduced cognitive load and editing effort through user studies.

**Limitations:** 

**Conclusion:** TalkLess represents a significant advancement in speech editing technology by allowing control over automated edits while preserving speaker style.

**Abstract:** Millions of people listen to podcasts, audio stories, and lectures, but editing speech remains tedious and time-consuming. Creators remove unnecessary words, cut tangential discussions, and even re-record speech to make recordings concise and engaging. Prior work automatically summarized speech by removing full sentences (extraction), but rigid extraction limits expressivity. AI tools can summarize then re-synthesize speech (abstraction), but abstraction strips the speaker's style. We present TalkLess, a system that flexibly combines extraction and abstraction to condense speech while preserving its content and style. To edit speech, TalkLess first generates possible transcript edits, selects edits to maximize compression, coverage, and audio quality, then uses a speech editing model to translate transcript edits into audio edits. TalkLess's interface provides creators control over automated edits by separating low-level wording edits (via the compression pane) from major content edits (via the outline pane). TalkLess achieves higher coverage and removes more speech errors than a state-of-the-art extractive approach. A comparison study (N=12) showed that TalkLess significantly decreased cognitive load and editing effort in speech editing. We further demonstrate TalkLess's potential in an exploratory study (N=3) where creators edited their own speech.

</details>


### [25] [How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective](https://arxiv.org/abs/2507.15244)

*Leixian Shen, Leni Yang, Haotian Li, Yun Wang, Yuyu Luo, Huamin Qu*

**Main category:** cs.HC

**Keywords:** empirical research, creation tools, data videos, tool design, HCI

**Relevance Score:** 4

**TL;DR:** This paper explores the integration of empirical research insights into the development of creation tools for data videos, revealing gaps and proposing improvements for mutual engagement between empirical studies and tool design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how empirical research influences creation tools and to enhance their integration for better design practice.

**Method:** A case study on data videos, involving analysis of 46 empirical research papers and 48 creation tool papers, plus interviews with 11 experts.

**Key Contributions:**

	1. Developed a taxonomy of recurring patterns in empirical findings and tool design integration.
	2. Identified key factors influencing the applicability of empirical research to tool development.
	3. Provided recommendations for enhancing collaboration between empirical and tool research.

**Result:** Identification of a taxonomy of patterns showing how empirical findings inform tool design and pratice patterns among researchers.

**Limitations:** Focuses specifically on data videos, which may limit generalizability to other domains.

**Conclusion:** The study reveals critical factors for applying empirical insights to tool development and suggests ways to improve the synergy between empirical research and tool design.

**Abstract:** Empirical research in creative design deepens our theoretical understanding of design principles and perceptual effects, offering valuable guidance for innovating creation tools. However, how these empirical insights currently influence the development of creation tools, and how their integration can be enhanced in the future, remains insufficiently understood. In this paper, we aim to unveil the gap through a case study on data videos, a prominent and wide-spread medium for effective data storytelling. To achieve the goal, we conducted a comprehensive analysis of 46 empirical research papers and 48 creation tool papers on data video, complemented by interviews with 11 experts. Building upon a systematic collection and structured characterization of empirical research by their methodologies (e.g., corpus analysis, comparative evaluations) and component focus (e.g., visuals, motions, narratives, audio), we conducted a context-aware citation analysis and revealed a taxonomy of recurring patterns in how empirical findings inform tool design across citation functions (e.g., problem framing, technical reference). Expert interviews further uncovered researchers' practice patterns in applying empirical findings (e.g., adaptation, synthesis, iteration, etc.) and identified key factors influencing applicability, such as contextual relevance, granularity matching, clarity, credibility, and feasibility. Finally, we derive suggestions and discuss future opportunities to foster closer mutual engagement between empirical and tool research, aiming to reinforce the theoretical grounding of creation tools and enhance the practical impact of empirical research.

</details>


### [26] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)

*Zhipeng Li, Yi-Chi Liao, Christian Holz*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Preferential Bayesian Optimization, Meta-learning, User Preferences, Visual Optimization

**Relevance Score:** 8

**TL;DR:** Meta-PO integrates Preferential Bayesian Optimization with meta-learning to optimize visual parameter adjustments efficiently for end-users.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Finding optimal visual parameters like brightness and contrast is challenging due to large search spaces and implicit user preferences.

**Method:** The proposed Meta-PO method infers prior users' preferences and creates models that suggest design candidates for new users, enhancing sample efficiency and convergence speed.

**Key Contributions:**

	1. Novel integration of PBO with meta-learning for user preference inference
	2. Improved sample efficiency in visual optimization tasks
	3. Generalizability across different user goals for design tasks

**Result:** Meta-PO achieved satisfactory appearance in 5.86 iterations for users with similar goals and in 8 iterations across divergent goals during experimental evaluations.

**Limitations:** Depends on the availability of prior users' preferences for effective optimization.

**Conclusion:** Meta-PO enhances personalized visual optimization, making it more accessible for end-users and allowing broader scaling of interface personalization.

**Abstract:** Adjusting visual parameters such as brightness and contrast is common in our everyday experiences. Finding the optimal parameter setting is challenging due to the large search space and the lack of an explicit objective function, leaving users to rely solely on their implicit preferences. Prior work has explored Preferential Bayesian Optimization (PBO) to address this challenge, involving users to iteratively select preferred designs from candidate sets. However, PBO often requires many rounds of preference comparisons, making it more suitable for designers than everyday end-users. We propose Meta-PO, a novel method that integrates PBO with meta-learning to improve sample efficiency. Specifically, Meta-PO infers prior users' preferences and stores them as models, which are leveraged to intelligently suggest design candidates for the new users, enabling faster convergence and more personalized results. An experimental evaluation of our method for appearance design tasks on 2D and 3D content showed that participants achieved satisfactory appearance in 5.86 iterations using Meta-PO when participants shared similar goals with a population (e.g., tuning for a ``warm'' look) and in 8 iterations even generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to ``holiday''). Meta-PO makes personalized visual optimization more applicable to end-users through a generalizable, more efficient optimization conditioned on preferences, with the potential to scale interface personalization more broadly.

</details>


### [27] [Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools](https://arxiv.org/abs/2507.15433)

*Lou Schwartz, Mohammad Ghoniem, Valérie Maquil, Adrien Coppens, Johannes Hermen*

**Main category:** cs.HC

**Keywords:** user interface design, wall-sized displays, interaction methods, design guidelines, prototyping tools

**Relevance Score:** 6

**TL;DR:** The paper investigates usability in designing user interfaces for wall-sized displays using different prototyping tools and interaction methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The unique spatial characteristics of wall-sized displays necessitate specialized user interface design strategies.

**Method:** Two user studies and a technology review were conducted to evaluate desktop-optimized prototyping tools for wall-sized displays, focusing on touch, keyboard with touchpad, and tablet interactions.

**Key Contributions:**

	1. Results from user studies on interaction methods for wall-sized displays
	2. Proposition of twelve design guidelines for dedicated design tools
	3. Insights into usability limitations of existing user interface design tools

**Result:** Tablet-based interaction was found to be the most comfortable, and a mixed-modality interaction approach was proposed as promising.

**Limitations:** Current tools do not fully support user interface design for wall-sized displays and require further improvement in element placement and feature provision.

**Conclusion:** Existing tools for user interface design do not adequately support wall-sized displays, leading to the development of twelve design guidelines for this specific area.

**Abstract:** Wall-Sized Displays have spatial characteristics that are difficult to address during user interface design. The design at scale 1:1 could be part of the solution. In this paper, we present the results of two user studies and one technology review, exploring the usability of popular, desktop-optimized prototyping tools, for designing at scale on Wall-Sized Displays. We considered two wall-sized display setups, and three different interaction methods: touch, a keyboard equipped with a touchpad, and a tablet. We observed that designing at scale 1:1 was appreciated. Tablet-based interaction proved to be the most comfortable interaction method, and a mix of interaction modalities is promising. In addition, care must be given to the surrounding environment, such as furniture. We propose twelve design guidelines for a design tool dedicated to this specific context. Overall, existing user interface design tools do not yet fully support design on and for wall-sized displays and require further considerations in terms of placement of user interface elements and the provision of additional features.

</details>


### [28] [Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays](https://arxiv.org/abs/2507.15443)

*Adrien Coppens, Valérie Maquil*

**Main category:** cs.HC

**Keywords:** mixed-presence, collaboration, head gaze data, evaluation methodology, wall-sized displays

**Relevance Score:** 7

**TL;DR:** This paper presents a method to evaluate mixed-presence collaboration using head gaze data in the context of wall-sized displays.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for robust evaluation methodologies that are not obtrusive for assessing quality in mixed-presence collaboration experiences around large displays.

**Method:** The approach involves measuring joint attention through head gaze data during user studies with two wall-sized displays.

**Key Contributions:**

	1. Novel methodology for measuring joint attention using head gaze data
	2. Implementation in a user study with wall-sized displays
	3. Preliminary insights into mixed-presence collaboration dynamics

**Result:** Insights gained from initial implementation focus on gaze data from one user study session, contributing to the understanding of collaborative interactions in mixed-presence setups.

**Limitations:** 

**Conclusion:** The proposed method offers a non-intrusive way to assess collaborative experiences in large display environments, providing valuable insights for future research.

**Abstract:** To understand and quantify the quality of mixed-presence collaboration around wall-sized displays, robust evaluation methodologies are needed, that are adapted for a room-sized experience and are not perceived as obtrusive. In this paper, we propose our approach for measuring joint attention based on head gaze data. We describe how it has been implemented for a user study on mixed presence collaboration with two wall-sized displays and report on the insights we gained so far from its implementation, with a preliminary focus on the data coming from one particular session.

</details>


### [29] [Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands](https://arxiv.org/abs/2507.15481)

*Yesica Duarte, Puneet Jain*

**Main category:** cs.HC

**Keywords:** Virtual Reality, eXtended Reality, disability, performance, empathy

**Relevance Score:** 6

**TL;DR:** Waiting for Hands (WfH) critiques the portrayal of disability in Virtual Reality (VR) by introducing Alternative Controllers and staging an absurd XR performance to disrupt sentimental narratives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the tendency of VR to frame disability as a spectacle that elicits pity or inspiration.

**Method:** Creation of Alternative Controllers in an interactive eXtended Reality (XR) installation, featuring a performance combining XR users and an audience watching a documentary.

**Key Contributions:**

	1. Repurposes interaction norms in XR through Alternative Controllers
	2. Stages an absurd performance that critiques disability narratives
	3. Encourages ethical considerations in XR contexts related to embodiment

**Result:** The performance creates a layered experience that disrupts engagement with the documentary, drawing attention through absurd movements and introducing uncertainty.

**Limitations:** 

**Conclusion:** The piece showcases how XR can be used to question empathy-driven narratives and promote awareness of non-normative embodiments without resorting to spectacle.

**Abstract:** Virtual Reality (VR) is often described as the "ultimate empathy machine," framing disability as an experience to be simulated through such technologies, which can reduce disability to a spectacle of pity or inspiration. In response, we present Waiting for Hands (WfH), an interactive eXtended Reality (XR) installation that critiques this logic by: (1) repurposing interaction norms in XR through the creation of Alternative Controllers, and (2) staging an absurd XR performance using the built controllers to disrupt sentimentalized disability narratives. The performance involves eight people: two XR participants on stage and six audience members watching a projected documentary about Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR users partially obscure the film, drawing attention through strange mouth and hand movements performed in XR. This creates a layered experience that disrupts direct engagement with Hema's story and introduces uncertainty. While XR is often seen as a fully immersive, sensory-dominant medium, this piece subverts that framing by using XR to produce absurdity and alienation. By challenging empathy-driven and pitiable narratives of disability, we ask what ethical stance an XR performance can take to attune participants to non-normative embodiment while resisting spectacle.

</details>


### [30] [FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up](https://arxiv.org/abs/2507.15502)

*Chen Chen, Jianing Yin, Jiannong Cao, Zhiyuan Wen, Mingjin Zhang, Weixun Gao, Xiang Wang, Haihua Shu*

**Main category:** cs.HC

**Keywords:** postoperative care, LLM, edge-deployed, robot, healthcare

**Relevance Score:** 9

**TL;DR:** FollowUpBot is an LLM-powered robot designed for postoperative care, providing dynamic conversations with patients and generating follow-up reports.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve postoperative follow-up by reducing the time and labor involved in traditional methods while ensuring data privacy.

**Method:** The robot uses edge-deployed LLMs for real-time, adaptive conversations with patients in various interaction modes.

**Key Contributions:**

	1. Introduction of FollowUpBot for dynamic patient interaction
	2. Use of edge-deployed LLMs for adaptive communication
	3. Automated generation of structured follow-up reports

**Result:** FollowUpBot demonstrated high coverage and patient satisfaction in follow-up interactions, with accurate report generation.

**Limitations:** 

**Conclusion:** The system effectively addresses shortcomings of traditional methods and existing digital solutions in postoperative care.

**Abstract:** Postoperative follow-up plays a crucial role in monitoring recovery and identifying complications. However, traditional approaches, typically involving bedside interviews and manual documentation, are time-consuming and labor-intensive. Although existing digital solutions, such as web questionnaires and intelligent automated calls, can alleviate the workload of nurses to a certain extent, they either deliver an inflexible scripted interaction or face private information leakage issues. To address these limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed robot for postoperative care and monitoring. It allows dynamic planning of optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face conversations with patients through multiple interaction modes, ensuring data privacy. Moreover, FollowUpBot is capable of automatically generating structured postoperative follow-up reports for healthcare institutions by analyzing patient interactions during follow-up. Experimental results demonstrate that our robot achieves high coverage and satisfaction in follow-up interactions, as well as high report generation accuracy across diverse field types. The demonstration video is available at https://www.youtube.com/watch?v=_uFgDO7NoK0.

</details>


### [31] [Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey](https://arxiv.org/abs/2507.15526)

*Antonio Perez, Avinash Singh, Jonathan Mitchell, Philip Swadling*

**Main category:** cs.HC

**Keywords:** Mixed Reality, pilot training, human factors, cybersickness, aviation

**Relevance Score:** 6

**TL;DR:** This paper reviews human factors challenges related to Mixed Reality (MR) head mounted displays in pilot training, exploring strategies to mitigate issues like cybersickness, visual fatigue, and ergonomic strain.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the human factors challenges that can hinder pilot performance and training outcomes when using MR technologies in aviation.

**Method:** A systematic literature review that identifies key human factors issues in MR HMD use, assesses mitigation strategies, and adopts a regulatory perspective informed by industry standards.

**Key Contributions:**

	1. Systematic identification of human factors challenges in MR pilot training.
	2. Evaluation of mitigation strategies informed by regulatory frameworks.
	3. Insights into the impact of human factors on training effectiveness and pilot wellbeing.

**Result:** The findings reveal critical insights into the human dimensions of aviation simulation training, focusing on comfort, safety, and training effectiveness, while balancing technical requirements with pilot wellbeing.

**Limitations:** 

**Conclusion:** The review supports the development of MR aviation training guidelines and best practices, emphasizing the need for effective mitigation of human factors challenges to enhance pilot training outcomes.

**Abstract:** Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative to traditional Flight Simulator Training Device (FSTD) displays, providing immersion, realism and cost efficiency. However, these technologies require management of human factors; cybersickness, visual fatigue and ergonomic strain. If left unmitigated, these effects can hinder pilot performance and training outcomes. For safety critical fields like aviation, addressing human factors challenges is crucial for MR's training potential. This survey systematically reviews the current literature identifying key human factors challenges in MR HMD use in pilot training and examines strategies to mitigate these barriers. Drawing on existing industry standards set by a leading aviation authority, the review adopts a regulatory perspective to explore hardware, software, ergonomic, physiological and psychological interventions improving pilot comfort, safety and training effectiveness in an MR FSTD. Additionally, it evaluates which of these interventions are most appropriate and viable for MR pilot training under existing aviation training regulations, ensuring that technical requirements and pilot wellbeing remain balanced. The findings yield significant insights for the human dimensions of aviation simulation training, highlighting how regulatory considerations shape the practicality of mitigation measures. These insights inform emerging MR aviation training guidelines and best practices, supporting MR's readiness to enhance aviation training.

</details>


### [32] [FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold](https://arxiv.org/abs/2507.15559)

*Pan Hao, Dongyeop Kang, Nicholas Hinds, Qianwen Wang*

**Main category:** cs.HC

**Keywords:** multi-agent workflows, visualization tool, interactive design, workflow optimization, human-computer interaction

**Relevance Score:** 7

**TL;DR:** FLOWFORGE is an interactive visualization tool designed to aid in the creation of multi-agent workflows through structured visual exploration and context-aware guidance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome challenges in designing optimal multi-agent workflows due to a vast design space and reliance on practitioners' intuition, which can lead to inefficient exploration methods.

**Method:** FLOWFORGE organizes the workflow design process into three hierarchical levels: task planning, agent assignment, and agent optimization, enabling users to navigate the design space effectively and compare solutions across metrics.

**Key Contributions:**

	1. Introduction of a structured visual exploration tool for multi-agent workflows.
	2. In-situ guidance based on established design patterns during the workflow creation process.
	3. Organization of the design process into three hierarchical levels for better usability.

**Result:** User studies showed that FLOWFORGE improved usability and effectiveness in workflow creation, providing insights into practitioners' exploration and guidance use.

**Limitations:** .

**Conclusion:** The structured and guided approach of FLOWFORGE enhances the multi-agent workflow design process, making it more efficient and informed.

**Abstract:** Multi-agent workflows have become an effective strategy for tackling complicated tasks by decomposing them into multiple sub-tasks and assigning them to specialized agents. However, designing optimal workflows remains challenging due to the vast and intricate design space. Current practices rely heavily on the intuition and expertise of practitioners, often resulting in design fixation or an unstructured, time-consuming exploration of trial-and-error. To address these challenges, this work introduces FLOWFORGE, an interactive visualization tool to facilitate the creation of multi-agent workflow through i) a structured visual exploration of the design space and ii) in-situ guidance informed by established design patterns. Based on formative studies and literature review, FLOWFORGE organizes the workflow design process into three hierarchical levels (i.e., task planning, agent assignment, and agent optimization), ranging from abstract to concrete. This structured visual exploration enables users to seamlessly move from high-level planning to detailed design decisions and implementations, while comparing alternative solutions across multiple performance metrics. Additionally, drawing from established workflow design patterns, FLOWFORGE provides context-aware, in-situ suggestions at each level as users navigate the design space, enhancing the workflow creation process with practical guidance. Use cases and user studies demonstrate the usability and effectiveness of FLOWFORGE, while also yielding valuable insights into how practitioners explore design spaces and leverage guidance during workflow development.

</details>


### [33] [Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback](https://arxiv.org/abs/2507.15650)

*Gerben van der Hoek, Bastiaan Heeren, Rogier Bos, Paul Drijvers, Johan Jeuring*

**Main category:** cs.HC

**Keywords:** feedback strategy, formative assessment, student interaction

**Relevance Score:** 3

**TL;DR:** The study explores an informative feedback strategy in computer-aided formative assessment, enhancing student learning through balanced feedback.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve student interactions and learning outcomes in a computer-aided formative assessment environment by examining feedback strategies.

**Method:** A qualitative study involving 25 senior secondary education students who completed tasks in an online environment, accompanied by screen captures and post-intervention interviews.

**Key Contributions:**

	1. Investigation of informative feedback strategies in educational environments
	2. Insights into student appreciation of error-specific feedback
	3. Demonstration of the balance needed between exploration and structure in feedback

**Result:** Students found that the informative feedback strategy allowed for self-guidance while preventing disengagement, appreciating the balance between exploration and structured feedback.

**Limitations:** 

**Conclusion:** The balanced feedback strategy resulted in positive student-environment interactions and improved learning engagement.

**Abstract:** Computer aided formative assessment can be used to enhance a learning process, for instance by providing feedback. There are many design choices for delivering feedback, that lead to a feedback strategy. In an informative feedback strategy, students do not immediately receive information about the correct response, but are offered the opportunity to retry a task to apply feedback information. In this small-scale qualitative study, we explore an informative feedback strategy designed to offer a balance between room for exploration and mitigation of learning barriers. The research questions concern the ways in which students interact with the feedback strategy and their appreciation of error-specific feedback as opposed to worked-out solutions. To answer these questions, twenty-five 15-to-17-year-old senior general secondary education students worked for approximately 20 minutes on linear and exponential extrapolation tasks in an online environment. Data included screen captures of students working with the environment and post-intervention interviews. Results showed that room for exploration offered opportunities for self-guidance while mitigation of learning barriers prevented disengagement. Furthermore, students appreciated balanced feedback. We conclude that the balanced feedback strategy yielded fruitful student-environment interactions.

</details>


### [34] [Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions](https://arxiv.org/abs/2507.15692)

*Meng Chen, Akhil Iyer, Amy Pavel*

**Main category:** cs.HC

**Keywords:** multimodal large language models, blind and low vision, user study, reliability, information accessibility

**Relevance Score:** 9

**TL;DR:** This paper explores how to enhance the accessibility of multimodal large language models for blind and low vision users by presenting variations in model responses.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve accessibility and reliability of visual information for blind and low vision (BLV) individuals using multimodal large language models (MLLMs).

**Method:** Design space for presenting variations in MLLM responses, implementation of a prototype system with three presentation styles, and a user study with 15 BLV participants.

**Key Contributions:**

	1. Design space for eliciting variations in MLLM descriptions
	2. Prototype system with multiple variation presentation styles
	3. User study demonstrating the effectiveness of variations for BLV users

**Result:** The study found that presenting variations increased the ability of users to identify unreliable claims by 4.9 times compared to using single descriptions, and all participants preferred this method over standard responses.

**Limitations:** The study is based on a limited sample of 15 participants, which may not fully represent the wider BLV community.

**Conclusion:** Systematically surfacing variations in MLLM outputs significantly aids BLV users in detecting unreliable information and increases their engagement with the technology.

**Abstract:** Multimodal large language models (MLLMs) provide new opportunities for blind and low vision (BLV) people to access visual information in their daily lives. However, these models often produce errors that are difficult to detect without sight, posing safety and social risks in scenarios from medication identification to outfit selection. While BLV MLLM users use creative workarounds such as cross-checking between tools and consulting sighted individuals, these approaches are often time-consuming and impractical. We explore how systematically surfacing variations across multiple MLLM responses can support BLV users to detect unreliable information without visually inspecting the image. We contribute a design space for eliciting and presenting variations in MLLM descriptions, a prototype system implementing three variation presentation styles, and findings from a user study with 15 BLV participants. Our results demonstrate that presenting variations significantly increases users' ability to identify unreliable claims (by 4.9x using our approach compared to single descriptions) and significantly decreases perceived reliability of MLLM responses. 14 of 15 participants preferred seeing variations of MLLM responses over a single description, and all expressed interest in using our system for tasks from understanding a tornado's path to posting an image on social media.

</details>


### [35] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)

*Mohammad 'Matt' Namvarpour, Brandon Brofsky, Jessica Medina, Mamtaj Akter, Afsaneh Razi*

**Main category:** cs.HC

**Keywords:** Generative AI, AI chatbots, adolescents, emotional dependence, digital overreliance

**Relevance Score:** 7

**TL;DR:** This study analyzes the interactions of adolescents with AI chatbots, revealing patterns of emotional dependence and overreliance that can disrupt offline relationships.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unique dynamics of adolescent interactions with customizable AI chatbots, particularly in relation to emotional support and digital dependency.

**Method:** Analyzed 318 Reddit posts from users aged 13-17 on the Character.AI subreddit to identify patterns of chatbot usage and overreliance.

**Key Contributions:**

	1. Identifies patterns of emotional dependence in adolescents using AI chatbots.
	2. Reveals psychological distress linked to overreliance on chatbots.
	3. Provides recommendations for safer chatbot design involving teen feedback.

**Result:** Findings indicate that while teens use chatbots for emotional support and creative expression, many develop attachments that hinder their offline relationships and routines, showing signs of psychological distress.

**Limitations:** 

**Conclusion:** The research highlights the need for chatbot design improvements to enhance self-awareness and promote real-world engagement among teens.

**Abstract:** As Generative Artificial Intelligence (GenAI) driven chatbots like Character.AI become embedded in adolescent life, they raise concerns about emotional dependence and digital overreliance. While studies have investigated the overreliance of adults on these chatbots, they have not investigated teens' interactions with chatbots with customizable personas. We analyzed 318 Reddit posts made by users self-reported as 13-17 years old on the Character.AI subreddit to understand patterns of overreliance. We found teens commonly begin using chatbots for emotional support or creative expression, but many develop strong attachments that interfere with offline relationships and daily routines. Their posts revealed recurring signs of psychological distress, cycles of relapse, and difficulty disengaging. Teens reported that their overreliance often ended when they reflect on the harm, return to in-person social settings, or become frustrated by platform restrictions. Based on the implications of our findings, we provide recommendations for future chatbot design so they can promote self-awareness, support real-world engagement, and involve teens in developing safer digital tools.

</details>


### [36] [Benchmarking Mobile Device Control Agents across Diverse Configurations](https://arxiv.org/abs/2404.16660)

*Juyong Lee, Taywon Min, Minyong An, Dongyoon Hahm, Haeone Lee, Changyeon Kim, Kimin Lee*

**Main category:** cs.HC

**Keywords:** mobile device control, benchmarking, large language models, task automation, human-agent interaction

**Relevance Score:** 7

**TL;DR:** This paper introduces B-MoCA, a benchmark for evaluating mobile device control agents in diverse interactive environments and highlights agents' strengths and weaknesses in task execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a common benchmark for assessing mobile device control agents and quantify their scientific progress in everyday task automation.

**Method:** Development of the B-MoCA benchmark based on the Android OS, defining 131 daily tasks and incorporating a randomization feature to evaluate agent generalization.

**Key Contributions:**

	1. Introduction of the B-MoCA benchmark for mobile device control agents.
	2. Definition of 131 common daily tasks for evaluation.
	3. Incorporation of random configurations to test generalization performance.

**Result:** Agents utilizing LLMs or multi-modal approaches perform well on simple tasks but struggle with complex ones, revealing areas for further research.

**Limitations:** Current agents show poor performance on complex tasks, indicating a need for improvement.

**Conclusion:** The B-MoCA benchmark is a critical step towards enhancing the development and evaluation of mobile control agents; it also encourages exploration to improve agent capabilities on complex tasks.

**Abstract:** Mobile device control agents can largely enhance user interactions and productivity by automating daily tasks. However, despite growing interest in developing practical agents, the absence of a commonly adopted benchmark in this area makes it challenging to quantify scientific progress. In this work, we introduce B-MoCA: a novel benchmark with interactive environments for evaluating and developing mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 131 common daily tasks. Importantly, we incorporate a randomization feature that changes the configurations of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained with imitation learning using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to improve effectiveness. Our source code is publicly available at https://b-moca.github.io.

</details>


### [37] [Why Automate This? Exploring the Connection between Time Use, Well-being and Robot Automation Across Social Groups](https://arxiv.org/abs/2501.06348)

*Ruchira Ray, Leona Pang, Sanjana Srivastava, Li Fei-Fei, Samantha Shorey, Roberto Martín-Martín*

**Main category:** cs.HC

**Keywords:** task automation, human-robot interaction, feelings, social groups, open-source data

**Relevance Score:** 7

**TL;DR:** This study investigates the motivations behind people's inclination to automate tasks, focusing on the influence of time spent and associated feelings, revealing important differences across social groups.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand motivations for task automation in developing beneficial robots for daily life.

**Method:** Analysis of the BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use Survey Well-Being Module to explore automation preferences related to time and feelings.

**Key Contributions:**

	1. Demonstrated that feelings significantly influence task automation preferences over time spent.
	2. Revealed gender and economic disparities in automation motivation.
	3. Provided an open-source dataset and analysis tool for community use.

**Result:** The desire for automation is more closely linked to feelings (happiness, pain) than the time spent on chores; significant gender and income variations were observed in automation preferences.

**Limitations:** The study may not cover all demographic variables influencing automation preferences.

**Conclusion:** The findings challenge assumptions about time and highlight the emotional factors in automation decisions, aiming to guide the development of user-centered domestic robotics.

**Abstract:** Understanding the motivations underlying the human inclination to automate tasks is vital to developing truly helpful robots integrated into daily life. Accordingly, we ask: are individuals more inclined to automate chores based on the time they consume or the feelings experienced while performing them? This study explores these preferences and whether they vary across different social groups (i.e., gender category and income level). Leveraging data from the BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use Survey Well-Being Module, we investigate the relationship between the desire for automation, time spent on daily activities, and their associated feelings - Happiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness. Our key findings show that, despite common assumptions, time spent does not strongly relate to the desire for automation for the general population. For the feelings analyzed, only happiness and pain are key indicators. Significant differences by gender and economic level also emerged: Women prefer to automate stressful activities, whereas men prefer to automate those that make them unhappy; mid-income individuals prioritize automating less enjoyable and meaningful activities, while low and high-income show no significant correlations. We hope our research helps motivate technologies to develop robots that match the priorities of potential users, moving domestic robotics toward more socially relevant solutions. We open-source all the data, including an online tool that enables the community to replicate our analysis and explore additional trends at https://hri1260.github.io/why-automate-this.

</details>


### [38] [Ultra-low-power ring-based wireless tinymouse](https://arxiv.org/abs/2504.03253)

*Yifan Li, Masaaki Fukumoto, Mohamed Kari, Shigemi Ishida, Akihito Noda, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara, Ryo Takahashi*

**Main category:** cs.HC

**Keywords:** picoRing mouse, wearable computing, inductive telemetry

**Relevance Score:** 7

**TL;DR:** The picoRing mouse enables extended use of a ring-based pointing device through ultra-low-power wireless communication, significantly increasing battery life compared to existing solutions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Wireless mouse rings are limited by short battery life due to power-hungry communication methods, necessitating frequent recharging and disrupting use.

**Method:** The picoRing mouse utilizes semi-passive inductive telemetry for ring-to-wristband communication, which allows for unique frequency responses based on mouse inputs, achieved through an efficient modulation system.

**Key Contributions:**

	1. Introduction of picoRing mouse with ultra-low-powered communication
	2. Implementation of semi-passive inductive telemetry for effective input capture
	3. Significant extension of operational hours on a single battery charge

**Result:** The picoRing mouse can sustain 600 to 1000 hours of continuous use on a single charge, depending on usage patterns, while enabling subtle finger movements like scrolling.

**Limitations:** 

**Conclusion:** This innovation addresses the battery limitations of wearable mouse devices, facilitating prolonged use and enhanced user experience in wearable computing.

**Abstract:** Wireless mouse rings offer subtle, reliable pointing interactions for wearable computing platforms. However, the small battery below 27 mAh in the miniature rings restricts the ring's continuous lifespan to just 1-10 hours, because current low-powered wireless communication such as BLE is power-consuming for ring's continuous use. The ring's short lifespan frequently disrupts users' mouse use with the need for frequent charging. This paper presents picoRing mouse, enabling a continuous ring-based mouse interaction with ultra-low-powered ring-to-wristband wireless communication. picoRing mouse employs a coil-based impedance sensing named semi-passive inductive telemetry, allowing a wristband coil to capture a unique frequency response of a nearby ring coil via a sensitive inductive coupling between the coils. The ring coil converts the corresponding user's mouse input into the unique frequency response via an up to 449 uW mouse-driven modulation system. Therefore, the continuous use of picoRing mouse can last approximately 600 (8hrs use/day)-1000 (4hrs use/day) hours on a single charge of a 27 mAh battery while supporting subtle thumb-to-index scrolling and pressing interactions in real-world wearable computing situations.

</details>


### [39] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)

*Angjelin Hila, Elliott Hauser*

**Main category:** cs.HC

**Keywords:** Large Language Models, Qualitative Coding, Deductive Classification, ChatGPT, Intervention Strategies

**Relevance Score:** 9

**TL;DR:** This study explores the effectiveness of large language models (LLMs) like ChatGPT in structured deductive qualitative coding, highlighting significant outcomes from various coding intervention strategies.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the underutilized potential of LLMs in performing deductive classification tasks compared to conventional human coding methods.

**Method:** We classified U.S. Supreme Court case summaries into major policy domains using the Comparative Agendas Project Master Codebook, testing four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy.

**Key Contributions:**

	1. Exploration of deductive coding using LLMs
	2. Introduction of Step-by-Step Task Decomposition strategy
	3. Demonstrated reliable classification performance of ChatGPT in qualitative coding

**Result:** The Step-by-Step Task Decomposition strategy yielded the highest reliability metrics (accuracy = 0.775, kappa = 0.744) and confirmed that intervention strategies significantly influenced classification behavior, evidenced by Cramer's V values indicating moderate to strong shifts in classification patterns.

**Limitations:** The study acknowledges semantic ambiguity in case summaries which may affect classification outcomes.

**Conclusion:** With custom-tailored interventions, LLMs can achieve reliability levels appropriate for rigorous qualitative coding workflows, making them a valuable tool for qualitative researchers.

**Abstract:** In this study, we investigate the use of large language models (LLMs), specifically ChatGPT, for structured deductive qualitative coding. While most current research emphasizes inductive coding applications, we address the underexplored potential of LLMs to perform deductive classification tasks aligned with established human-coded schemes. Using the Comparative Agendas Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries into 21 major policy domains. We tested four intervention methods: zero-shot, few-shot, definition-based, and a novel Step-by-Step Task Decomposition strategy, across repeated samples. Performance was evaluated using standard classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's alpha), and construct validity was assessed using chi-squared tests and Cramer's V. Chi-squared and effect size analyses confirmed that intervention strategies significantly influenced classification behavior, with Cramer's V values ranging from 0.359 to 0.613, indicating moderate to strong shifts in classification patterns. The Step-by-Step Task Decomposition strategy achieved the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746), achieving thresholds for substantial agreement. Despite the semantic ambiguity within case summaries, ChatGPT displayed stable agreement across samples, including high F1 scores in low-support subclasses. These findings demonstrate that with targeted, custom-tailored interventions, LLMs can achieve reliability levels suitable for integration into rigorous qualitative coding workflows.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [40] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)

*Song Mao, Lejun Cheng, Pinlong Cai, Guohang Yan, Ding Wang, Botian Shi*

**Main category:** cs.CL

**Keywords:** DeepWriter, writing assistant, retrieval-augmented generation, financial report generation, multimodal retrieval

**Relevance Score:** 9

**TL;DR:** DeepWriter is a multimodal, customizable writing assistant designed to improve the writing process in specialized domains by utilizing curated offline knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of existing writing assistants in specialized domains such as finance and medicine, particularly issues of deep domain knowledge and content hallucination.

**Method:** DeepWriter employs a novel pipeline, including task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection, leveraging a structured corpus for information extraction.

**Key Contributions:**

	1. Introduction of a customizable multimodal writing assistant
	2. Development of a novel pipeline for document generation
	3. Proposal of a hierarchical knowledge representation for improved retrieval efficiency

**Result:** Experiments demonstrate that DeepWriter generates high-quality, verifiable financial reports that exceed existing baselines in terms of factual accuracy and content quality.

**Limitations:** 

**Conclusion:** DeepWriter effectively addresses the challenges faced by existing writing assistants in specialized domains, providing reliable and professional-grade document generation.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in various applications. However, their use as writing assistants in specialized domains like finance, medicine, and law is often hampered by a lack of deep domain-specific knowledge and a tendency to hallucinate. Existing solutions, such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content. To address these challenges, we introduce DeepWriter, a customizable, multimodal, long-form writing assistant that operates on a curated, offline knowledge base. DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. By deeply mining information from a structured corpus and incorporating both textual and visual elements, DeepWriter generates coherent, factually grounded, and professional-grade documents. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy. Our experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.

</details>


### [41] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)

*Fufang Wen, Shichang Zhang*

**Main category:** cs.CL

**Keywords:** large language models, model editing, fine-tuning, knowledge retention, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of fine-tuning on the knowledge edited in large language models and proposes methods to improve knowledge retention during this process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how fine-tuning affects knowledge edited into large language models, especially since model editing methods are crucial for updating LLMs efficiently.

**Method:** The authors conducted a systematic investigation comparing different fine-tuning objectives and model editing techniques, analyzing the retention of edited versus intrinsic knowledge.

**Key Contributions:**

	1. Detailed analysis of edited knowledge retention during fine-tuning of LLMs.
	2. Identification of freezing layers as a potential solution for better knowledge retention.
	3. Highlighting the critical need for robustness evaluation in current model editing approaches.

**Result:** Edited knowledge is more likely to be forgotten during fine-tuning compared to knowledge acquired through pre-training. Freezing layers associated with edited content can enhance knowledge retention.

**Limitations:** The study primarily focuses on the interaction between fine-tuning objectives and editing techniques but does not explore other potential factors affecting knowledge retention in LLMs.

**Conclusion:** Evaluating the robustness of edits during downstream fine-tuning is essential for the practical application of model editing techniques; future editing methods should consider this aspect to improve retention.

**Abstract:** Large language models (LLMs) store vast amounts of knowledge, which often requires updates to correct factual errors, incorporate newly acquired information, or adapt model behavior. Model editing methods have emerged as efficient solutions for such updates, offering localized and precise knowledge modification at significantly lower computational cost than continual training. In parallel, LLMs are frequently fine-tuned for a wide range of downstream tasks. However, the effect of fine-tuning on previously edited knowledge remains poorly understood. In this work, we systematically investigate how different fine-tuning objectives interact with various model editing techniques. Our findings show that edited knowledge is substantially more susceptible to forgetting during fine-tuning than intrinsic knowledge acquired through pre-training. This analysis highlights a key limitation of current editing approaches and suggests that evaluating edit robustness under downstream fine-tuning is critical for their practical deployment. We further find that freezing layers associated with edited content can significantly improve knowledge retention, offering insight into how future editing methods might be made more robust.

</details>


### [42] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)

*Shengji Tang, Jianjian Cao, Weihao Lin, Jiale Hong, Bo Zhang, Shuyue Hu, Lei Bai, Tao Chen, Wanli Ouyang, Peng Ye*

**Main category:** cs.CL

**Keywords:** open-source LLMs, multi-agent systems, performance optimization

**Relevance Score:** 7

**TL;DR:** The paper presents SMACS, a scalable multi-agent system that uses multiple open-source LLMs to outperform closed-source LLMs by optimizing performance through a novel selection and enhancement framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capability of open-source LLMs in competing with closed-source LLMs and to optimize their collaboration for improved performance.

**Method:** The SMACS framework employs a Retrieval-based Prior Selection (RPS) to score and select the top-performing LLMs for specific questions, and an Exploration-Exploitation-Driven Posterior Enhancement (EPE) to generate diverse and high-quality responses.

**Key Contributions:**

	1. Introduction of SMACS framework for multi-agent collaboration in LLMs
	2. Introduction of RPS and EPE methods for performance optimization
	3. Demonstrated significant performance improvement over closed-source LLMs

**Result:** SMACS, integrating fifteen open-source LLMs, demonstrates significant performance improvements over closed-source counterparts like Claude-3.7-Sonnet and GPT-4.1, showing advancements in multiple task benchmarks.

**Limitations:** 

**Conclusion:** The research indicates that open-source LLM collectives can excel in tasks traditionally dominated by closed-source models, thus expanding the potential for open-source applications in AI.

**Abstract:** This paper aims to demonstrate the potential and strengths of open-source collectives. It leads to a promising question: Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs? To answer this, we propose SMACS, a scalable multi-agent collaboration system (MACS) framework with high performance. Specifically, for continuous integration of new LLMs and generalization to diverse questions, we first propose a Retrieval-based Prior Selection (RPS), which assigns a proxy performance score to each LLM to select the Top-k LLMs at the instance level for any given question. Then, we propose an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the generation of diverse responses through prior dropping and selecting the high-quality response via a hybrid posterior score. Experiments on eight mainstream benchmarks validate the effectiveness of our SMACS: by integrating fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. Remarkably, it even exceeds the average of best results of different datasets from both open-source LLMs (+2.86%) and closed-source LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released at https://github.com/magent4aci/SMACS.

</details>


### [43] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)

*Rui Zhao, Vladyslav Melnychuk, Jun Zhao, Jesse Wright, Nigel Shadbolt*

**Main category:** cs.CL

**Keywords:** Privacy Policy, Natural Language Processing, User Compliance, Data Usage Practices, Cognitive Burden

**Relevance Score:** 8

**TL;DR:** PoliAnalyzer is a neuro-symbolic system that personalizes privacy policy analysis using NLP to extract data usage practices, enabling users to understand complex privacy policies efficiently.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assist users in understanding the privacy policies of online accounts without the need to read lengthy documents, as most users do not engage with these texts despite claiming to do so.

**Method:** PoliAnalyzer employs Natural Language Processing (NLP) to extract formal representations of data usage practices and uses logical inference to compare these with user preferences, producing compliance reports. It utilizes an enriched PolicyIE dataset for evaluation.

**Key Contributions:**

	1. Introduction of PoliAnalyzer, a neuro-symbolic system for analyzing privacy policies
	2. High accuracy in compliance analysis using NLP tools
	3. Identification of common privacy policy practices that violate user expectations

**Result:** In evaluations, PoliAnalyzer achieved an F1-score of 90-100% for identifying relevant data usage practices and revealed that 95.2% of privacy policy segments do not conflict with user preferences, allowing users to focus on the minority that does.

**Limitations:** The study relies on an existing dataset and may have limitations based on the diversity of user profiles and specific policies analyzed.

**Conclusion:** PoliAnalyzer effectively reduces cognitive burden and enhances understanding of privacy policies, facilitating individuals to regain control over their data and promoting discussions about data practices.

**Abstract:** In modern times, people have numerous online accounts, but they rarely read the Terms of Service or Privacy Policy of those sites despite claiming otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that assists users with personalized privacy policy analysis. PoliAnalyzer uses Natural Language Processing (NLP) to extract formal representations of data usage practices from policy texts. In favor of deterministic, logical inference is applied to compare user preferences with the formal privacy policy representation and produce a compliance report. To achieve this, we extend an existing formal Data Terms of Use policy language to model privacy policies as app policies and user preferences as data policies. In our evaluation using our enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated high accuracy in identifying relevant data usage practices, achieving F1-score of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can model diverse user data-sharing preferences, derived from prior research as 23 user profiles, and perform compliance analysis against the top 100 most-visited websites. This analysis revealed that, on average, 95.2% of a privacy policy's segments do not conflict with the analyzed user preferences, enabling users to concentrate on understanding the 4.8% (636 / 13205) that violates preferences, significantly reducing cognitive burden. Further, we identified common practices in privacy policies that violate user expectations - such as the sharing of location data with 3rd parties. This paper demonstrates that PoliAnalyzer can support automated personalized privacy policy analysis at scale using off-the-shelf NLP tools. This sheds light on a pathway to help individuals regain control over their data and encourage societal discussions on platform data practices to promote a fairer power dynamic.

</details>


### [44] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)

*Khalid Hasan, Jamil Saquer*

**Main category:** cs.CL

**Keywords:** Bipolar disorder, Natural Language Processing, Transformer models, Social media, Early detection

**Relevance Score:** 9

**TL;DR:** This paper investigates NLP models for detecting signs of bipolar disorder in social media text, demonstrating that transformer models, particularly RoBERTa, significantly outperform LSTM models with static embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Bipolar disorder is often underdiagnosed; hence, leveraging NLP to identify signs through social media could enhance early detection and treatment.

**Method:** Evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and LSTMs using both contextualized and static word embeddings on a dataset of Reddit posts.

**Key Contributions:**

	1. Comprehensive evaluation of various NLP models for mental health detection
	2. Demonstrated effectiveness of contextualized embeddings over static ones
	3. Provided insights on model training efficiency for health informatics applications

**Result:** RoBERTa achieved the highest performance with an F1 score of ~98%, while LSTMs with static embeddings struggled, scoring near-zero F1.

**Limitations:** 

**Conclusion:** Contextual language modeling plays a vital role in effective bipolar disorder detection, and DistilBERT strikes a good balance between efficiency and accuracy.

**Abstract:** Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma. This paper explores the advanced natural language processing (NLP) models for recognizing signs of bipolar disorder based on user-generated social media text. We conduct a comprehensive evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed on a large, annotated dataset of Reddit posts after confirming their validity through sentiment variance and judgmental analysis. Our results demonstrate that RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1. These findings underscore the critical role of contextual language modeling in detecting bipolar disorder. In addition, we report model training times and highlight that DistilBERT offers an optimal balance between efficiency and accuracy. In general, our study offers actionable insights for model selection in mental health NLP applications and validates the potential of contextualized language models to support early bipolar disorder screening.

</details>


### [45] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)

*Matthew Kearney, Reuben Binns, Yarin Gal*

**Main category:** cs.CL

**Keywords:** large language models, bias, identity markers, ethical AI, user-facing applications

**Relevance Score:** 9

**TL;DR:** This paper analyzes how identity markers in users' writing affect the responses of large language models (LLMs) across various applications such as medicine and job salary recommendations, revealing biases based on race, gender, and age.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs use identity information in real-world decision-making and to assess the biases that may arise in sensitive applications.

**Method:** Conducted a comprehensive analysis of LLM responses across five high-stakes applications, examining the influence of identity markers like race, gender, and age.

**Key Contributions:**

	1. First comprehensive analysis of LLM biases linked to identity markers
	2. Identification of harmful biases in high-stakes applications
	3. Development of tools for evaluating LLM decision-making under identity influences

**Result:** Findings reveal significant biases where LLMs treat users differently based on identity, leading to potential harmful outcomes in medical advice, salary recommendations, and political discourse.

**Limitations:** 

**Conclusion:** The study highlights the need for thorough evaluations of LLM applications to avoid perpetuating biases and recommends further assessments before deploying LLMs in user-facing roles.

**Abstract:** Large language models (LLMs) are increasingly being used in user-facing applications, from providing medical consultations to job interview advice. Recent research suggests that these models are becoming increasingly proficient at inferring identity information about the author of a piece of text from linguistic patterns as subtle as the choice of a few words. However, little is known about how LLMs use this information in their decision-making in real-world applications. We perform the first comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries. We find that LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. For instance, when providing medical advice, we find that models apply different standards of care to individuals of different ethnicities for the same symptoms; we find that LLMs are more likely to alter answers to align with a conservative (liberal) political worldview when asked factual questions by older (younger) individuals; and that LLMs recommend lower salaries for non-White job applicants and higher salaries for women compared to men. Taken together, these biases mean that the use of off-the-shelf LLMs for these applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Beyond providing an analysis, we also provide new tools for evaluating how subtle encoding of identity in users' language choices impacts model decisions. Given the serious implications of these findings, we recommend that similar thorough assessments of LLM use in user-facing applications are conducted before future deployment.

</details>


### [46] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)

*Weihua Zheng, Roy Ka-Wei Lee, Zhengyuan Liu, Kui Wu, AiTi Aw, Bowei Zou*

**Main category:** cs.CL

**Keywords:** Multilingual Large Language Models, hallucinations, cross-lingual, contrastive learning, Chain-of-Thought

**Relevance Score:** 9

**TL;DR:** The paper introduces CCL-XCoT, a framework to reduce hallucinations in Multilingual Large Language Models by enhancing cross-lingual semantic alignment and incorporating a Chain-of-Thought prompting strategy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address hallucinations in Multilingual Large Language Models, particularly for low-resource languages due to training data imbalances.

**Method:** A two-stage fine-tuning framework that enhances cross-lingual semantic alignment through curriculum-based contrastive learning and incorporates a Chain-of-Thought prompting strategy during instruction fine-tuning.

**Key Contributions:**

	1. Introduction of the CCL-XCoT framework for MLLMs
	2. Curriculum-based contrastive learning for enhanced semantic alignment
	3. Implementation of cross-lingual Chain-of-Thought prompting strategy

**Result:** CCL-XCoT reduces hallucination rates by up to 62% and improves factual knowledge transfer across language pairs.

**Limitations:** 

**Conclusion:** The proposed framework mitigates hallucinations in low-resource language generation tasks effectively without the need for external retrieval or multi-model ensembles.

**Abstract:** Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks (Chataigner et al., 2024). To address this challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language. Experimental results show that CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [47] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)

*Mohammad Shahedur Rahman, Peng Gao, Yuede Ji*

**Main category:** cs.CL

**Keywords:** large language models, supply chain, datasets, model-dataset relationships, NLP

**Relevance Score:** 9

**TL;DR:** This paper studies the relationships between large language models (LLMs) and their datasets, analyzing the LLM supply chain through a directed heterogeneous graph.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the complex interdependencies between models and datasets is crucial for mitigating risks, improving fairness, and ensuring compliance in the deployment of LLMs.

**Method:** The authors systematically collected LLM supply chain data to construct a directed heterogeneous graph representing the relationships between models and datasets, containing 397,376 nodes and 453,469 edges.

**Key Contributions:**

	1. Development of a systematic method for collecting LLM supply chain data
	2. Construction of a directed heterogeneous graph modeling model-dataset relationships
	3. Analysis revealing the dynamic nature and structure of the LLM supply chain

**Result:** The analysis revealed that the LLM supply chain graph is large and sparse, follows a power-law degree distribution, features a densely connected core, and exhibits strong interdependence between models and datasets, with daily updates indicating a dynamic ecosystem.

**Limitations:** 

**Conclusion:** Insights from the study can help stakeholders better understand potential risks and improve the overall governance of LLMs and their components.

**Abstract:** Large language models (LLMs) leverage deep learning to process and predict sequences of words from context, enabling them to perform various NLP tasks, such as translation, summarization, question answering, and content generation. However, the growing size and complexity of developing, training, and deploying advanced LLMs require extensive computational resources and large datasets. This creates a barrier for users. As a result, platforms that host models and datasets are widely used. For example, Hugging Face, one of the most popular platforms, hosted 1.8 million models and 450K datasets by June 2025, with no sign of slowing down. Since many LLMs are built from base models, pre-trained models, and external datasets, they can inherit vulnerabilities, biases, or malicious components from earlier models or datasets. Therefore, it is critical to understand the origin and development of these components to better detect potential risks, improve model fairness, and ensure compliance. Motivated by this, our project aims to study the relationships between models and datasets, which are core components of the LLM supply chain. First, we design a method to systematically collect LLM supply chain data. Using this data, we build a directed heterogeneous graph to model the relationships between models and datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We then perform various analyses and uncover several findings, such as: (i) the LLM supply chain graph is large, sparse, and follows a power-law degree distribution; (ii) it features a densely connected core and a fragmented periphery; (iii) datasets play pivotal roles in training; (iv) strong interdependence exists between models and datasets; and (v) the graph is dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [48] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)

*Rithesh Murthy, Ming Zhu, Liangwei Yang, Jielin Qiu, Juntao Tan, Shelby Heinecke, Huan Wang, Caiming Xiong, Silvio Savarese*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Engineering, Natural Language Processing, Task Optimization, Synthetic Data Generation

**Relevance Score:** 8

**TL;DR:** Promptomatix is an automatic prompt optimization framework that generates high-quality prompts from natural language task descriptions without manual tuning or domain expertise.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of manual and inconsistent prompt engineering in Large Language Models (LLMs) that limits accessibility for non-experts.

**Method:** Promptomatix uses a meta-prompt-based optimizer and a DSPy-powered compiler to analyze user intent, generate synthetic training data, select prompting strategies, and refine prompts with cost-aware objectives.

**Key Contributions:**

	1. Automatic prompt generation from natural language descriptions
	2. Modular design for future extensions
	3. Scalable and efficient prompt optimization

**Result:** Promptomatix was evaluated across 5 task categories, showing competitive or superior performance compared to existing libraries while reducing prompt length and computational overhead.

**Limitations:** 

**Conclusion:** The framework enables scalable and efficient prompt optimization, making it accessible and effective for a broader range of users.

**Abstract:** Large Language Models (LLMs) perform best with well-crafted prompts, yet prompt engineering remains manual, inconsistent, and inaccessible to non-experts. We introduce Promptomatix, an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without requiring manual tuning or domain expertise. Promptomatix supports both a lightweight meta-prompt-based optimizer and a DSPy-powered compiler, with modular design enabling future extension to more advanced frameworks. The system analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives. Evaluated across 5 task categories, Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient.

</details>


### [49] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)

*Wan-Cyuan Fan, Yen-Chun Chen, Mengchen Liu, Alexander Jacobson, Lu Yuan, Leonid Sigal*

**Main category:** cs.CL

**Keywords:** Large Vision Language Models, chart comprehension, data generation, training strategy, benchmark

**Relevance Score:** 6

**TL;DR:** ChartScope is an LVLM designed for enhanced comprehension of scientific charts across various types, addressing limitations in existing models by introducing a data generation pipeline and a dual-path training strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve chart comprehension in LVLMs which previously relied on limited paired data and lacked targeted pre-training for chart-data alignment.

**Method:** Introduces an efficient data generation pipeline for synthesizing paired data across diverse chart types and a novel Dual-Path training strategy that enhances understanding and reasoning about the data.

**Key Contributions:**

	1. Introduction of ChartScope, a tailored LVLM for chart comprehension
	2. Development of a data generation pipeline for diverse chart types
	3. Launch of ChartDQA benchmark for comprehensive evaluation of chart understanding

**Result:** ChartScope demonstrates significantly improved comprehension of scientific charts across a wide variety of types, as evaluated using the newly established ChartDQA benchmark.

**Limitations:** 

**Conclusion:** ChartScope effectively enhances the capability of LVLMs in understanding and reasoning over diverse chart data, representing a step forward in scientific chart comprehension.

**Abstract:** Recent methods for customizing Large Vision Language Models (LVLMs) for domain-specific tasks have shown promising results in scientific chart comprehension. However, existing approaches face two major limitations: First, they rely on paired data from only a few chart types, limiting generalization to wide range of chart types. Secondly, they lack targeted pre-training for chart-data alignment, which hampers the model's understanding of underlying data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth chart comprehension across diverse chart types. We propose an efficient data generation pipeline that synthesizes paired data for a wide range of chart types, along with a novel Dual-Path training strategy that enabling the model to succinctly capture essential data details while preserving robust reasoning capabilities by incorporating reasoning over the underlying data. Lastly, we establish ChartDQA, a new benchmark for evaluating not only question-answering at different levels but also underlying data understanding. Experimental results demonstrate that ChartScope significantly enhances comprehension on a wide range of chart types. The code and data are available at https://davidhalladay.github.io/chartscope_demo.

</details>


### [50] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)

*Rakesh Paul, Anusha Kamath, Kanishk Singla, Raviraj Joshi, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, selective translation, low-resource languages, Hindi, alignment

**Relevance Score:** 8

**TL;DR:** This study investigates LLM-based selective translation to improve alignment of multilingual large language models, specifically for low-resource languages like Hindi, by preserving non-translatable content and exploring its effectiveness compared to traditional translation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap between English and non-English languages in multilingual large language models, especially in low-resource settings where data is scarce.

**Method:** The study explores LLM-based selective translation, which involves translating only the translatable parts of a text while preserving non-translatable content and sentence structure. It evaluates this method against traditional translation techniques using data from Hindi and compares outputs from Google Cloud Translation and Llama-3.1-405B.

**Key Contributions:**

	1. Introduction of selective translation technique for LLMs
	2. Comparative analysis of translation quality between GCP and Llama-3.1-405B
	3. Demonstration of effectiveness in aligning multilingual models for low-resource languages

**Result:** The experiments demonstrate that selective translation can effectively improve the multilingual alignment of LLMs, showing promise as a viable alternative to conventional translation approaches.

**Limitations:** The study primarily focuses on Hindi and may not generalize to other languages; further experiments needed for broader applicability.

**Conclusion:** Selective translation offers a practical method for enhancing LLM performance in low-resource languages by avoiding the pitfalls of traditional translation methods that fail to retain crucial content structure.

**Abstract:** Multilingual large language models (LLMs) often demonstrate a performance gap between English and non-English languages, particularly in low-resource settings. Aligning these models to low-resource languages is essential yet challenging due to limited high-quality data. While English alignment datasets are readily available, curating equivalent data in other languages is expensive and time-consuming. A common workaround is to translate existing English alignment data; however, standard translation techniques often fail to preserve critical elements such as code, mathematical expressions, and structured formats like JSON. In this work, we investigate LLM-based selective translation, a technique that selectively translates only the translatable parts of a text while preserving non-translatable content and sentence structure. We conduct a systematic study to explore key questions around this approach, including its effectiveness compared to vanilla translation, the importance of filtering noisy outputs, and the benefits of mixing translated samples with original English data during alignment. Our experiments focus on the low-resource Indic language Hindi and compare translations generated by Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the promise of selective translation as a practical and effective method for improving multilingual alignment in LLMs.

</details>


### [51] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)

*Karin de Langis, Jong Inn Park, Andreas Schramm, Bin Hu, Khanh Chi Le, Michael Mensink, Ahn Thu Tong, Dongyeop Kang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Linguistic Aspect, Cognition, Narratives, Pragmatic Inferences

**Relevance Score:** 8

**TL;DR:** This study examines LLMs' processing of temporal meaning in narratives, revealing differences from human cognition and developing a framework for assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs exhibit human-like cognition in understanding temporal aspects of narratives and to assess their ability to comprehend such narratives.

**Method:** The study employs a series of targeted experiments using an Expert-in-the-Loop probing pipeline to evaluate LLMs' semantic and pragmatic processing.

**Key Contributions:**

	1. Identification of LLMs' over-reliance on prototypicality in aspectual judgments.
	2. Demonstration of inconsistent aspectual reasoning in LLMs.
	3. Proposal of a standardized framework for assessing LLMs' cognitive capabilities.

**Result:** Findings indicate that LLMs rely heavily on prototypical examples, show inconsistency in aspectual judgments, and struggle with causal reasoning related to aspect, highlighting their limitations in narrative understanding.

**Limitations:** The study focuses primarily on LLMs and may not represent broader AI linguistic processing capabilities.

**Conclusion:** The results suggest that LLMs process narrative aspects differently than humans and lack a robust understanding of narratives, while also proposing a new experimental framework for evaluating LLMs' cognitive abilities.

**Abstract:** Large language models (LLMs) exhibit increasingly sophisticated linguistic capabilities, yet the extent to which these behaviors reflect human-like cognition versus advanced pattern recognition remains an open question. In this study, we investigate how LLMs process the temporal meaning of linguistic aspect in narratives that were previously used in human studies. Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner. Our findings show that LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect, raising concerns about their ability to fully comprehend narratives. These results suggest that LLMs process aspect fundamentally differently from humans and lack robust narrative understanding. Beyond these empirical findings, we develop a standardized experimental framework for the reliable assessment of LLMs' cognitive and linguistic capabilities.

</details>


### [52] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)

*Marija Anđedelić, Dominik Šipek, Laura Majer, Jan Šnajder*

**Main category:** cs.CL

**Keywords:** clickbait detection, BERTić, LLM, Croatian news, information quality

**Relevance Score:** 4

**TL;DR:** This paper presents a dataset and methods for detecting clickbait in Croatian news headlines, comparing fine-tuned models with LLM-based approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To preserve information quality and reader trust in digital media by addressing the prevalence of clickbait headlines.

**Method:** Compilation of the CLIC dataset for clickbait detection; fine-tuning the BERTić model; comparison with LLM-based in-context learning methods for Croatian and English prompts.

**Key Contributions:**

	1. Introduction of the CLIC dataset for Croatian headlines
	2. Demonstration of the superiority of fine-tuned models over LLMs
	3. Analysis of linguistic properties of clickbait.

**Result:** Fine-tuned models outperformed LLMs in detecting clickbait, with nearly half of the analyzed Croatian headlines identified as clickbait.

**Limitations:** 

**Conclusion:** Fine-tuning yields better clickbait detection results than using general LLMs, emphasizing the need for context-specific approaches.

**Abstract:** Online news outlets operate predominantly on an advertising-based revenue model, compelling journalists to create headlines that are often scandalous, intriguing, and provocative -- commonly referred to as clickbait. Automatic detection of clickbait headlines is essential for preserving information quality and reader trust in digital media and requires both contextual understanding and world knowledge. For this task, particularly in less-resourced languages, it remains unclear whether fine-tuned methods or in-context learning (ICL) yield better results. In this paper, we compile CLIC, a novel dataset for clickbait detection of Croatian news headlines spanning a 20-year period and encompassing mainstream and fringe outlets. We fine-tune the BERTi\'c model on this task and compare its performance to LLM-based ICL methods with prompts both in Croatian and English. Finally, we analyze the linguistic properties of clickbait. We find that nearly half of the analyzed headlines contain clickbait, and that finetuned models deliver better results than general LLMs.

</details>


### [53] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)

*Jianfeng Zhu, Ruoming Jin, Karin G. Coifman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personality Assessment, Human-Computer Interaction, Psychometrics, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of LLMs in personality assessment using a benchmark of real-world interviews, identifying issues with validity despite strong reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to assess the capability of large language models for inferring personality traits from open-ended language in a reliable and valid way.

**Method:** The authors tested three LLMs (GPT-4.1 Mini, Meta-LLaMA, DeepSeek) using zero-shot prompting for BFI-10 item prediction and various prompting techniques for Big Five trait inference.

**Key Contributions:**

	1. Introduction of a benchmark with real-world semi-structured interviews for personality assessment using LLMs
	2. Demonstration of the reliability of LLMs in personality trait prediction despite low validity
	3. Highlighting the need for improved methodologies in LLM-based psychological applications

**Result:** All models demonstrated high test-retest reliability, but had weak correlation with ground-truth personality scores and low interrater agreement, with predictions biased towards higher trait levels.

**Limitations:** Construct validity was limited, with weak correlations to ground truth and low interrater agreement.

**Conclusion:** The findings reveal significant limitations in LLMs for personality inference, emphasizing the necessity for more evidence-based approaches in psychological applications.

**Abstract:** Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a promising approach for scalable personality assessment from open-ended language. However, inferring personality traits remains challenging, and earlier work often relied on synthetic data or social media text lacking psychometric validity. We introduce a real-world benchmark of 555 semi-structured interviews with BFI-10 self-report scores for evaluating LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item prediction and both zero-shot and chain-of-thought prompting for Big Five trait inference. All models showed high test-retest reliability, but construct validity was limited: correlations with ground-truth scores were weak (max Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$), and predictions were biased toward moderate or high trait levels. Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy. These results underscore limitations in current LLM-based personality inference and highlight the need for evidence-based development for psychological applications.

</details>


### [54] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)

*Albert Chen, Manas Bundele, Gaurav Ahlawat, Patrick Stetz, Zhitao Wang, Qiang Fei, Donghoon Jung, Audrey Chu, Bharadwaj Jayaraman, Ayushi Panth, Yatin Arora, Sourav Jain, Renjith Varma, Alexey Ilin, Iuliia Melnychuk, Chelsea Chueh, Joyan Sil, Xiaofeng Wang*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, knowledge graph, chatbot, enterprise solutions, data insights

**Relevance Score:** 8

**TL;DR:** This paper discusses the development of an internal chatbot at LinkedIn that enables product managers and engineers to easily access data insights from a dynamic data lake using a Text-to-SQL approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate self-service data insights for LinkedIn's product managers, engineers, and operations teams, leveraging large language models in an enterprise setting.

**Method:** The approach consists of building a knowledge graph to capture metadata and historical queries, developing a Text-to-SQL agent for accurate query generation, and creating an interactive chatbot for diverse user intents.

**Key Contributions:**

	1. Construction of a knowledge graph for real-time data insights
	2. Development of a Text-to-SQL agent to correct hallucinations and syntax errors
	3. Creation of an interactive chatbot for enhanced user engagement

**Result:** The chatbot has 300+ weekly users, with expert reviews indicating that around 53% of its responses are correct or close to correct on the internal benchmark.

**Limitations:** The performance is limited to the quality of the knowledge graph and the training data; improvements are necessary for broader contexts.

**Conclusion:** The study highlights important components of knowledge graphs and modeling for enterprise Text-to-SQL solutions, providing a practical path for future developments.

**Abstract:** The introduction of large language models has brought rapid progress on Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise solution. In this paper, we present insights from building an internal chatbot that enables LinkedIn's product managers, engineers, and operations teams to self-serve data insights from a large, dynamic data lake. Our approach features three components. First, we construct a knowledge graph that captures up-to-date semantics by indexing database metadata, historical query logs, wikis, and code. We apply clustering to identify relevant tables for each team or product area. Second, we build a Text-to-SQL agent that retrieves and ranks context from the knowledge graph, writes a query, and automatically corrects hallucinations and syntax errors. Third, we build an interactive chatbot that supports various user intents, from data discovery to query writing to debugging, and displays responses in rich UI elements to encourage follow-up chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of its responses are correct or close to correct on an internal benchmark set. Through ablation studies, we identify the most important knowledge graph and modeling components, offering a practical path for developing enterprise Text-to-SQL solutions.

</details>


### [55] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)

*Sinchani Chakraborty, Sudeshna Sarkar, Pawan Goyal*

**Main category:** cs.CL

**Keywords:** Relation Classification, Biomedical Texts, Knowledge Graphs, Curriculum Learning, Instruction Tuning

**Relevance Score:** 8

**TL;DR:** This paper presents an error-aware teacher-student framework to enhance Relation Classification in biomedical texts using structured guidance from a large language model, achieving state-of-the-art performance on multiple datasets.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve Relation Classification in biomedical texts to aid in knowledge graph construction and applications like drug repurposing and clinical decision-making.

**Method:** The proposed method involves a teacher model analyzing prediction failures from a baseline student model to classify errors, assign difficulty scores, and generate targeted remediations, followed by training two student models using instruction tuning and curriculum learning based on a curated dataset.

**Key Contributions:**

	1. Introduction of an error-aware teacher-student framework for Relation Classification
	2. Development of a heterogeneous biomedical knowledge graph from PubMed abstracts
	3. Achievement of state-of-the-art results on multiple biomedical datasets

**Result:** The framework achieves new state-of-the-art performance on 4 out of 5 Protein-Protein Interaction datasets and the Drug-Drug Interaction dataset, while remaining competitive on the ChemProt dataset.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of using structured guidance from a large language model to improve Relation Classification in biomedical texts, yielding significant performance gains.

**Abstract:** Relation Classification (RC) in biomedical texts is essential for constructing knowledge graphs and enabling applications such as drug repurposing and clinical decision-making. We propose an error-aware teacher--student framework that improves RC through structured guidance from a large language model (GPT-4o). Prediction failures from a baseline student model are analyzed by the teacher to classify error types, assign difficulty scores, and generate targeted remediations, including sentence rewrites and suggestions for KG-based enrichment. These enriched annotations are used to train a first student model via instruction tuning. This model then annotates a broader dataset with difficulty scores and remediation-enhanced inputs. A second student is subsequently trained via curriculum learning on this dataset, ordered by difficulty, to promote robust and progressive learning. We also construct a heterogeneous biomedical knowledge graph from PubMed abstracts to support context-aware RC. Our approach achieves new state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, while remaining competitive on ChemProt.

</details>


### [56] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)

*Xiaolin Yan, Yangxing Liu, Jiazhang Zheng, Chi Liu, Mingyu Du, Caisheng Chen, Haoyang Liu, Ming Ding, Yuan Li, Qiuping Liao, Linfeng Li, Zhili Mei, Siyu Wan, Li Li, Ruyi Zhong, Jiangling Yu, Xule Liu, Huihui Hu, Jiameng Yue, Ruohui Cheng, Qi Yang, Liangqing Wu, Ke Zhu, Chi Zhang, Chufei Jing, Yifan Zhou, Yan Liang, Dongdong Li, Zhaohui Wang, Bin Zhao, Mingzhou Wu, Mingzhong Zhou, Peng Du, Zuomin Liao, Chao Dai, Pengfei Liang, Xiaoguang Zhu, Yu Zhang, Yu Gu, Kun Pan, Yuan Wu, Yanqing Guan, Shaojing Wu, Zikang Feng, Xianze Ma, Peishan Cheng, Wenjuan Jiang, Jing Ba, Huihao Yu, Zeping Hu, Yuan Xu, Zhiwei Liu, He Wang, Zhenguo Lin, Ming Liu, Yanhong Meng*

**Main category:** cs.CL

**Keywords:** large language models, semiconductor display, reasoning, retrieval-augmented generation, automated evaluation

**Relevance Score:** 3

**TL;DR:** X-Intelligence 3.0 is a high-performance reasoning model developed for the semiconductor display industry, achieving notable performance improvements through domain-specific training and innovative retrieval-augmented generation (RAG) mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of large language models in the semiconductor display industry is limited due to a lack of domain-specific training and expertise, necessitating a specialized model.

**Method:** X-Intelligence 3.0 utilizes supervised fine-tuning, reinforcement learning, and a domain-specific knowledge base for training, along with an automated evaluation framework for expert-level assessments.

**Key Contributions:**

	1. Introduction of X-Intelligence 3.0 as a specialized reasoning model for the semiconductor display industry
	2. Integration of a domain-specific retrieval-augmented generation mechanism to improve performance
	3. Implementation of an automated evaluation framework to simulate expert-level assessments

**Result:** X-Intelligence 3.0 significantly outperforms existing models like DeepSeek-R1-671B on benchmark datasets despite having a smaller parameter size, showcasing its efficiency and effectiveness.

**Limitations:** 

**Conclusion:** The development of a tailored reasoning model, X-Intelligence 3.0, addresses complex reasoning challenges in the semiconductor display industry successfully by leveraging domain-specific knowledge and advanced training methodologies.

**Abstract:** Large language models (LLMs) have recently achieved significant advances in reasoning and demonstrated their advantages in solving challenging problems. Yet, their effectiveness in the semiconductor display industry remains limited due to a lack of domain-specific training and expertise. To bridge this gap, we present X-Intelligence 3.0, the first high-performance reasoning model specifically developed for the semiconductor display industry. This model is designed to deliver expert-level understanding and reasoning for the industry's complex challenges. Leveraging a carefully curated industry knowledge base, the model undergoes supervised fine-tuning and reinforcement learning to enhance its reasoning and comprehension capabilities. To further accelerate development, we implemented an automated evaluation framework that simulates expert-level assessments. We also integrated a domain-specific retrieval-augmented generation (RAG) mechanism, resulting in notable performance gains on benchmark datasets. Despite its relatively compact size of 32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B across multiple evaluations. This demonstrates its exceptional efficiency and establishes it as a powerful solution to the longstanding reasoning challenges faced by the semiconductor display industry.

</details>


### [57] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)

*Sachin Yadav, Dominik Schlechtweg*

**Main category:** cs.CL

**Keywords:** Word-in-Context, multilingual models, Sentence Transformer, ordinal classification, machine learning

**Relevance Score:** 6

**TL;DR:** XL-DURel is a multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification, outperforming previous models on related tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Word-in-Context classification by using a unified model for both ordinal and binary tasks.

**Method:** Finetuning a multilingual Sentence Transformer with various loss functions for regression and ranking tasks, focusing on angular distance in complex space.

**Key Contributions:**

	1. Introduction of XL-DURel model
	2. Demonstration of ordinal WiC treatments benefiting binary tasks
	3. Novel application of angular distance for ranking objectives

**Result:** XL-DURel outperforms previous models on both ordinal and binary Word-in-Context tasks.

**Limitations:** 

**Conclusion:** A generalized approach for Word-in-Context modeling can improve performance across various task formulations, including binary cases.

**Abstract:** We propose XL-DURel, a finetuned, multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification. We test several loss functions for regression and ranking tasks managing to outperform previous models on ordinal and binary data with a ranking objective based on angular distance in complex space. We further show that binary WiC can be treated as a special case of ordinal WiC and that optimizing models for the general ordinal task improves performance on the more specific binary task. This paves the way for a unified treatment of WiC modeling across different task formulations.

</details>


### [58] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)

*Kester Wong, Sahan Bulathwela, Mutlu Cukurova*

**Main category:** cs.CL

**Keywords:** Collaborative Problem Solving, AudiBERT, Machine Learning, Human-AI Complementarity, Model Explainability

**Relevance Score:** 6

**TL;DR:** This paper explores the use of the AudiBERT model for detecting collaborative problem solving indicators in dialogue, highlighting its enhancements over traditional BERT, particularly in social-cognitive classifications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of detecting collaborative problem solving indicators from dialogue using machine learning in AI in Education, and to evaluate the effectiveness of the AudiBERT model.

**Method:** The study utilized the AudiBERT model, which integrates multimodal features (speech and acoustic-prosodic audio) and compared its performance to traditional BERT in classifying CPS indicators.

**Key Contributions:**

	1. Demonstrated significant improvements in CPS indicator classification using AudiBERT over BERT.
	2. Showed correlation between larger training datasets and improved model performance.
	3. Outlined a structured method for achieving human-AI complementarity in CPS diagnosis.

**Result:** AudiBERT exhibited statistically significant improvements in classifying sparse classes related to the social-cognitive dimension compared to BERT, though not for the affective dimension. Larger training data correlated with improved recall performance.

**Limitations:** The model did not show significant improvements in affective dimension classifications and performance was inconsistent for well-detected indicators using BERT.

**Conclusion:** The study advocates for a structured approach to enhance human-AI complementarity in CPS diagnosis, emphasizing model explainability to boost human engagement in the coding process.

**Abstract:** Detecting collaborative problem solving (CPS) indicators from dialogue using machine learning techniques is a significant challenge for the field of AI in Education. Recent studies have explored the use of Bidirectional Encoder Representations from Transformers (BERT) models on transcription data to reliably detect meaningful CPS indicators. A notable advancement involved the multimodal BERT variant, AudiBERT, which integrates speech and acoustic-prosodic audio features to enhance CPS diagnosis. Although initial results demonstrated multimodal improvements, the statistical significance of these enhancements remained unclear, and there was insufficient guidance on leveraging human-AI complementarity for CPS diagnosis tasks. This workshop paper extends the previous research by highlighting that the AudiBERT model not only improved the classification of classes that were sparse in the dataset, but it also had statistically significant class-wise improvements over the BERT model for classifications in the social-cognitive dimension. However, similar significant class-wise improvements over the BERT model were not observed for classifications in the affective dimension. A correlation analysis highlighted that larger training data was significantly associated with higher recall performance for both the AudiBERT and BERT models. Additionally, the precision of the BERT model was significantly associated with high inter-rater agreement among human coders. When employing the BERT model to diagnose indicators within these subskills that were well-detected by the AudiBERT model, the performance across all indicators was inconsistent. We conclude the paper by outlining a structured approach towards achieving human-AI complementarity for CPS diagnosis, highlighting the crucial inclusion of model explainability to support human agency and engagement in the reflective coding process.

</details>


### [59] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)

*Kester Wong, Sahan Bulathwela, Mutlu Cukurova*

**Main category:** cs.CL

**Keywords:** BERT, explainability, collaborative problem solving, SHAP, AI in education

**Relevance Score:** 7

**TL;DR:** This study explores the explainability of BERT-based models in classifying collaborative problem solving (CPS) processes, using SHAP to examine the contributions of tokenized words, revealing discrepancies between classification performance and meaningfulness of explanations.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance explainability and transparency of BERT models in educational applications, improving trust and adoption by end users such as teachers.

**Method:** The study employed SHapley Additive exPlanations (SHAP) to analyze the contribution of individual tokenized words in transcription data on the classification decisions made by the BERT model.

**Key Contributions:**

	1. Introduced SHAP for explainability in BERT-based CPS classification
	2. Identified spurious words affecting model classification
	3. Called for research on human-AI complementarity in CPS diagnostics

**Result:** The analysis indicated that high classification performance does not guarantee meaningful explanations; certain frequent tokenized words influenced classifications without semantic relevance, highlighting specific spurious words.

**Limitations:** The findings primarily address preliminary insights into explainability without extensive practical applications for end users.

**Conclusion:** While model transparency may not directly improve user practice, it emphasizes the need to balance LLM diagnostics with human expertise and calls for further exploration of ensemble models and human-AI collaboration in CPS diagnosis.

**Abstract:** The use of Bidirectional Encoder Representations from Transformers (BERT) model and its variants for classifying collaborative problem solving (CPS) has been extensively explored within the AI in Education community. However, limited attention has been given to understanding how individual tokenised words in the dataset contribute to the model's classification decisions. Enhancing the explainability of BERT-based CPS diagnostics is essential to better inform end users such as teachers, thereby fostering greater trust and facilitating wider adoption in education. This study undertook a preliminary step towards model transparency and explainability by using SHapley Additive exPlanations (SHAP) to examine how different tokenised words in transcription data contributed to a BERT model's classification of CPS processes. The findings suggested that well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions. Particular tokenised words were used frequently to affect classifications. The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class. While such model transparency is unlikely to be useful to an end user to improve their practice, it can help them not to overrely on LLM diagnostics and ignore their human expertise. We conclude the workshop paper by noting that the extent to which the model appropriately uses the tokens for its classification is associated with the number of classes involved. It calls for an investigation into the exploration of ensemble model architectures and the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills.

</details>


### [60] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)

*Łukasz Radliński, Mateusz Guściora, Jan Kocoń*

**Main category:** cs.CL

**Keywords:** data augmentation, NLP, machine learning, ChatGPT, backtranslation

**Relevance Score:** 9

**TL;DR:** The paper investigates data augmentation techniques for NLP tasks, comparing traditional methods with generative approaches using large language models like ChatGPT.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of data scarcity and class imbalance in domain-specific machine learning tasks within NLP.

**Method:** A series of experiments were conducted, comparing four data augmentation approaches, including paraphrasing and backtranslation, evaluating their effectiveness in generating data and improving classification performance.

**Key Contributions:**

	1. Systematic exploration of data augmentation techniques for NLP using LLMs.
	2. Comparison of traditional methods with generative methods for data quality and performance.
	3. Empirical evidence on the effectiveness of backtranslation and paraphrasing for data augmentation.

**Result:** The experiments showed that backtranslation and paraphrasing can produce comparable or better results compared to zero and few-shot generation methods.

**Limitations:** The study focuses on specific datasets and may not generalize across all NLP tasks or domains.

**Conclusion:** Traditional data augmentation methods, when paired with large language models, can effectively mitigate data scarcity issues and improve performance in NLP tasks.

**Abstract:** Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance. This paper systematically explores data augmentation methods for NLP, particularly through large language models like GPT. The purpose of this paper is to examine and evaluate whether traditional methods such as paraphrasing and backtranslation can leverage a new generation of models to achieve comparable performance to purely generative methods. Methods aimed at solving the problem of data scarcity and utilizing ChatGPT were chosen, as well as an exemplary dataset. We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups. We then evaluated the results both in terms of the quality of generated data and its impact on classification performance. The key findings indicate that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.

</details>


### [61] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)

*Fred Mutisya, Shikoh Gitau, Christine Syovata, Diana Oigara, Ibrahim Matende, Muna Aden, Munira Ali, Ryan Nyotu, Diana Marion, Job Nyangena, Nasubo Ongoma, Keith Mbae, Elizabeth Wamicha, Eric Mibuari, Jean Philbert Nsengemana, Talkmore Chidede*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare access, Kenyan primary care, retrieval augmented generation, AI deployment

**Relevance Score:** 9

**TL;DR:** This paper presents a methodology and dataset for evaluating Large Language Models in Kenyan primary healthcare, focusing on clinical scenarios and localized evaluation metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve healthcare access in low-resource settings like African primary care by effectively using Large Language Models (LLMs).

**Method:** A benchmark dataset, the Alama Health QA dataset, was created using retrieval augmented generation (RAG) aligned with Kenya's national clinical guidelines. This included the digitization, chunking, and indexing of guidelines, generating clinical scenarios and questions in English and Swahili, involving co-creation with Kenyan physicians, and establishing evaluation metrics for clinical reasoning and adaptability.

**Key Contributions:**

	1. Creation of the Alama Health QA dataset for Kenyan primary care
	2. Introduction of new evaluation metrics for clinical reasoning and adaptability
	3. Highlighting performance gaps of LLMs in localized African medical scenarios

**Result:** The dataset includes thousands of regulator-aligned question-answer pairs, revealing performance gaps in LLMs for localized scenarios compared to US benchmarks, highlighting lower accuracy for African medical content.

**Limitations:** Specificity to Kenyan healthcare may limit generalizability, and initial results indicate significant performance gaps for LLMs in these contexts.

**Conclusion:** The study provides a replicable model for guideline-driven dynamic benchmarking, emphasizing the need for localized evaluations to ensure safe AI deployment in African health systems.

**Abstract:** Large Language Models(LLMs) hold promise for improving healthcare access in low-resource settings, but their effectiveness in African primary care remains underexplored. We present a methodology for creating a benchmark dataset and evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our approach uses retrieval augmented generation (RAG) to ground clinical questions in Kenya's national guidelines, ensuring alignment with local standards. These guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic clinical scenarios, multiple-choice questions, and rationale based answers in English and Swahili. Kenyan physicians co-created and refined the dataset, and a blinded expert review process ensured clinical accuracy, clarity, and cultural appropriateness. The resulting Alama Health QA dataset includes thousands of regulator-aligned question answer pairs across common outpatient conditions. Beyond accuracy, we introduce evaluation metrics that test clinical reasoning, safety, and adaptability such as rare case detection (Needle in the Haystack), stepwise logic (Decision Points), and contextual adaptability. Initial results reveal significant performance gaps when LLMs are applied to localized scenarios, consistent with findings that LLM accuracy is lower on African medical content than on US-based benchmarks. This work offers a replicable model for guideline-driven, dynamic benchmarking to support safe AI deployment in African health systems.

</details>


### [62] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)

*Eric Xia, Jugal Kalita*

**Main category:** cs.CL

**Keywords:** affine approximation, transformer models, morphological relations, language interpretation, cross-layer transformations

**Relevance Score:** 7

**TL;DR:** A two-part affine approximation model reveals that transformer computations can reproduce final object states effectively for subject-object relations, achieving high fidelity in morphological relations across languages and models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the interpretability of conceptual relationships in language models and assess the effectiveness of linear transformations on transformer computations.

**Method:** A two-part affine approximation is applied to transformer computations, utilizing a linear transformation derived from model derivatives to analyze middle layer representations of subject tokens.

**Key Contributions:**

	1. Demonstrates effectiveness of affine approximation for interpreting transformer computations.
	2. Achieves high fidelity in modeling morphological relations.
	3. Extends findings across different languages and models.

**Result:** The model achieves 90% faithfulness on morphological relations and demonstrates similar success across multiple languages and different models.

**Limitations:** 

**Conclusion:** Key insights show that certain relationships in language models can be interpreted from latent spaces and are encoded by sparse cross-layer linear transformations.

**Abstract:** A two-part affine approximation has been found to be a good approximation for transformer computations over certain subject object relations. Adapting the Bigger Analogy Test Set, we show that the linear transformation Ws, where s is a middle layer representation of a subject token and W is derived from model derivatives, is also able to accurately reproduce final object states for many relations. This linear technique is able to achieve 90% faithfulness on morphological relations, and we show similar findings multi-lingually and across models. Our findings indicate that some conceptual relationships in language models, such as morphology, are readily interpretable from latent space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [63] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)

*Minsuh Joo, Hyunsoo Cho*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, uncertainty estimation, clustering, semantic consistency

**Relevance Score:** 9

**TL;DR:** The study introduces Cleanse, a clustering-based semantic consistency method for estimating uncertainty in responses from large language models (LLMs) to address hallucination issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the critical problem of hallucinations in large language models, which can compromise their reliability and safety.

**Method:** The proposed Cleanse approach quantifies uncertainty by assessing the intra-cluster consistency of LLM hidden embeddings through a clustering technique.

**Key Contributions:**

	1. Introduction of Cleanse for uncertainty estimation in LLMs.
	2. Use of clustering for assessing semantic consistency in model outputs.
	3. Validation on multiple LLMs and question-answering benchmarks.

**Result:** Cleanse effectively detects hallucinations in responses by validating its performance on four models (LLaMA-7B, LLaMA-13B, LLaMA2-7B, Mistral-7B) across SQuAD and CoQA benchmarks.

**Limitations:** 

**Conclusion:** The method shows promise in improving the identification of inaccurate responses from LLMs, thereby enhancing their reliability.

**Abstract:** Despite the outstanding performance of large language models (LLMs) across various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate responses--remains as a critical problem as it can be directly connected to a crisis of building safe and reliable LLMs. Uncertainty estimation is primarily used to measure hallucination levels in LLM responses so that correct and incorrect answers can be distinguished clearly. This study proposes an effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse quantifies the uncertainty with the proportion of the intra-cluster consistency in the total consistency between LLM hidden embeddings which contain adequate semantic information of generations, by employing clustering. The effectiveness of Cleanse for detecting hallucination is validated using four off-the-shelf models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two question-answering benchmarks, SQuAD and CoQA.

</details>


### [64] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)

*Wannaphong Phatthiyaphaibun, Can Udomcharoenchaikit, Pakpoom Singkorapoom, Kunat Pipatanakul, Ekapol Chuangsuwanich, Peerat Limkonchotiwat, Sarana Nutanong*

**Main category:** cs.CL

**Keywords:** Thai corpus, Language model, Data cleaning, Natural language processing, Thai language

**Relevance Score:** 6

**TL;DR:** This paper introduces Mangosteen, a high-quality Thai language corpus designed to enhance language model training. It presents a transparent pipeline that includes extensive cleaning and curation strategies aimed at addressing the challenges of existing datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing language models suffer from poor quality due to noisy data sources, particularly in Thai. The need for a high-quality, transparent corpus that respects cultural nuances and reliably filters out problematic content is essential for robust language model training.

**Method:** The paper outlines the creation of the Mangosteen corpus using a customized Dolma pipeline that combines language identification, quality filtering, and content curation methods adapted specifically for Thai text.

**Key Contributions:**

	1. Introduction of the Mangosteen corpus with 47 billion tokens.
	2. Development of a tailored pipeline for Thai text processing that enhances language model training.
	3. Release of all related code, cleaning manifests, and data, ensuring reproducibility.

**Result:** The Mangosteen corpus consists of 47 billion tokens and successfully reduces the original CommonCrawl dataset while improving the performance of Thai language models, as demonstrated through systematic ablation studies with GPT-2 and subsequent evaluations of the SEA-LION model.

**Limitations:** 

**Conclusion:** The Mangosteen corpus and its accompanying pipeline are released as reproducible resources, promoting transparency and enabling further research in Thai and regional language modeling.

**Abstract:** Pre-training data shapes a language model's quality, but raw web text is noisy and demands careful cleaning. Existing large-scale corpora rely on English-centric or language-agnostic pipelines whose heuristics do not capture Thai script or cultural nuances, leaving risky material such as gambling content untreated. Prior Thai-specific efforts customize pipelines or build new ones, yet seldom release their data or document design choices, hindering reproducibility and raising the question of how to construct a transparent, high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai corpus built through a Thai-adapted Dolma pipeline that includes custom rule-based language ID, revised C4/Gopher quality filters, and Thai-trained content filters, plus curated non-web sources such as Wikipedia, Royal Gazette texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline code, cleaning manifests, corpus snapshot, and all checkpoints, providing a fully reproducible foundation for future Thai and regional LLM research.

</details>


### [65] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)

*Vinicius Anjos de Almeida, Vinicius de Camargo, Raquel Gómez-Bravo, Egbert van der Haring, Kees van Boven, Marcelo Finger, Luis Fernandez Lopez*

**Main category:** cs.CL

**Keywords:** ICPC-2, large language models, medical coding, health informatics, semantic search

**Relevance Score:** 9

**TL;DR:** This study evaluates the ability of large language models (LLMs) to assign ICPC-2 codes to clinical expressions using a domain-specific semantic search engine.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capability of LLMs in automating the assignment of medical coding for better healthcare data usage.

**Method:** Utilized a dataset of 437 clinical expressions annotated with ICPC-2 codes, prompting 33 LLMs with queries from a semantic search engine and evaluating performance based on F1-score, cost, and format adherence.

**Key Contributions:**

	1. Benchmark of LLM performance for medical coding
	2. Identification of top-performing LLMs in ICPC-2 assignment
	3. Insights on retriever optimization impacts on performance

**Result:** 28 LLMs achieved F1-scores greater than 0.8, with top performers such as gpt-4.5-preview exhibiting the best results. Retriever optimization can enhance performance and most models provided valid codes with fewer hallucinations.

**Limitations:** Findings are constrained by dataset scope and setup; further multilingual evaluations needed for comprehensive validation.

**Conclusion:** LLMs demonstrate strong potential for automating ICPC-2 coding, but further evaluations are necessary for clinical validation across broader datasets.

**Abstract:** Background: Medical coding structures healthcare data for research, quality monitoring, and policy. This study assesses the potential of large language models (LLMs) to assign ICPC-2 codes using the output of a domain-specific search engine.   Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's text-embedding-3-large) retrieved candidates from 73,563 labeled concepts. Thirty-three LLMs were prompted with each query and retrieved results to select the best-matching ICPC-2 code. Performance was evaluated using F1-score, along with token usage, cost, response time, and format adherence.   Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever optimization can improve performance by up to 4 points. Most models returned valid codes in the expected format, with reduced hallucinations. Smaller models (<3B) struggled with formatting and input length.   Conclusions: LLMs show strong potential for automating ICPC-2 coding, even without fine-tuning. This work offers a benchmark and highlights challenges, but findings are limited by dataset scope and setup. Broader, multilingual, end-to-end evaluations are needed for clinical validation.

</details>


### [66] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)

*Xingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong Zhang, Yueyi Zhang, Zonglin Yang, Ruilin Li, Lei Lei, Shihao Xu, Han Zhao, Weiling Chen, Feng Ji, Lidong Bing*

**Main category:** cs.CL

**Keywords:** reasoning language models, mathematical reasoning, open-source, Context-Aware Multi-Stage Policy Optimization, reproducibility

**Relevance Score:** 8

**TL;DR:** Introduction of fully open-source reasoning language models (RLMs) with a focus on mathematical reasoning, enhancing transparency and reproducibility in their development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of transparency and reproducibility in the development of reasoning language models (RLMs), particularly in the domain of mathematical reasoning.

**Method:** The models are trained in two stages: SFT on a curated corpus of 719K math-reasoning problems followed by RLVR on 62K challenging problems, using Context-Aware Multi-Stage Policy Optimization for improved training effectiveness.

**Key Contributions:**

	1. Introduction of MiroMind-M1 series as open-source RLMs
	2. Development of Context-Aware Multi-Stage Policy Optimization algorithm
	3. Comprehensive release of models and datasets for transparency

**Result:** MiroMind-M1 series matches or exceeds existing open-source RLM performance, achieving state-of-the-art on AIME24, AIME25, and MATH benchmarks with superior token efficiency.

**Limitations:** 

**Conclusion:** The release of the MiroMind-M1 series, along with datasets and training configurations, aims to promote reproducibility and support ongoing research in RLMs.

**Abstract:** Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.

</details>


### [67] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)

*Mohammed Alkhowaiter, Norah Alshahrani, Saied Alshahrani, Reem I. Masoud, Alaa Alzahrani, Deema Alnuhait, Emad A. Alghamdi, Khalid Almubarak*

**Main category:** cs.CL

**Keywords:** Large Language Models, Arabic Datasets, Human-in-the-loop, Post-training, Dataset Evaluation

**Relevance Score:** 8

**TL;DR:** This paper reviews the quality and diversity of Arabic post-training datasets for Large Language Models available on Hugging Face, identifying critical gaps and offering recommendations for improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning pre-trained LLMs with human instructions is vital for enhancing their performance, particularly in Arabic contexts. The paper aims to evaluate available datasets to address performance disparities.

**Method:** The review categorizes datasets based on LLM capabilities, steerability, alignment, and robustness, evaluating them against criteria like popularity, adoption, documentation quality, licensing, and scientific contribution.

**Key Contributions:**

	1. Comprehensive review of Arabic post-training datasets on Hugging Face Hub
	2. Identification of critical gaps in the current datasets
	3. Recommendations for future development of post-training datasets

**Result:** The analysis revealed significant gaps in task diversity, documentation quality, and community adoption of Arabic post-training datasets.

**Limitations:** Limited focus on post-training datasets outside Arabic and lack of experimental validation of findings.

**Conclusion:** Addressing these gaps is crucial for advancing Arabic LLM capabilities and applications. The paper provides recommendations to improve dataset development.

**Abstract:** Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., persona and system prompts); (3) Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic LLMs and applications while providing concrete recommendations for future efforts in post-training dataset development.

</details>


### [68] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)

*Amina Dzafic, Merve Kavut, Ulya Bayram*

**Main category:** cs.CL

**Keywords:** suicidal ideation, natural language processing, annotation reliability, language coverage, mental health

**Relevance Score:** 8

**TL;DR:** This study addresses challenges in suicidal ideation detection by creating a Turkish corpus and an efficient annotation framework while evaluating model performance and annotation reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to tackle the limited language coverage and unreliable annotation practices in suicidal ideation detection, critical for suicide prevention.

**Method:** A novel Turkish suicidal ideation corpus was constructed using social media posts. An annotation framework with three human annotators and two LLMs was introduced, followed by bidirectional evaluation of label reliability and model consistency.

**Key Contributions:**

	1. Creation of a Turkish suicidal ideation corpus.
	2. Introduction of a resource-efficient annotation framework using human annotators and LLMs.
	3. Evaluation of label reliability and model consistency against existing datasets.

**Result:** The study found significant gaps in annotation practices and highlighted the inadequate performance of popular models in zero-shot transfer learning scenarios.

**Limitations:** The study primarily focuses on the Turkish language with limited applicability to other languages and datasets.

**Conclusion:** Emphasis was placed on the need for transparent, rigorous, and language-inclusive approaches in mental health NLP to ensure data and model reliability.

**Abstract:** Suicidal ideation detection is critical for real-time suicide prevention, yet its progress faces two under-explored challenges: limited language coverage and unreliable annotation practices. Most available datasets are in English, but even among these, high-quality, human-annotated data remains scarce. As a result, many studies rely on available pre-labeled datasets without examining their annotation process or label reliability. The lack of datasets in other languages further limits the global realization of suicide prevention via artificial intelligence (AI). In this study, we address one of these gaps by constructing a novel Turkish suicidal ideation corpus derived from social media posts and introducing a resource-efficient annotation framework involving three human annotators and two large language models (LLMs). We then address the remaining gaps by performing a bidirectional evaluation of label reliability and model consistency across this dataset and three popular English suicidal ideation detection datasets, using transfer learning through eight pre-trained sentiment and emotion classifiers. These transformers help assess annotation consistency and benchmark model performance against manually labeled data. Our findings underscore the need for more rigorous, language-inclusive approaches to annotation and evaluation in mental health natural language processing (NLP) while demonstrating the questionable performance of popular models with zero-shot transfer learning. We advocate for transparency in model training and dataset construction in mental health NLP, prioritizing data and model reliability.

</details>


### [69] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)

*Maria Sahakyan, Bedoor AlShebli*

**Main category:** cs.CL

**Keywords:** peer review, language analysis, bias, academic publishing, natural language processing

**Relevance Score:** 5

**TL;DR:** This study analyzes over 80,000 peer reviews to uncover linguistic biases related to author demographics, revealing implications for fairness in academic publishing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate subtle linguistic biases in the peer review process that may reinforce disparities among authors based on demographics.

**Method:** The study employs natural language processing and large-scale statistical modeling to analyze review tone, sentiment, and supportive language across demographic variables, utilizing a dataset of both anonymous and signed peer reviews.

**Key Contributions:**

	1. Comprehensive linguistic analysis of peer reviews
	2. Identification of biases related to author demographics
	3. Implications for reform in academic publishing

**Result:** The analysis reveals significant variations in review language influenced by the gender, race, and institutional affiliation of authors, and highlights how reviewer anonymity affects evaluation language.

**Limitations:** 

**Conclusion:** The findings challenge conventional views on the fairness provided by anonymity in peer review and suggest that review policies can significantly impact scientific integrity and career trajectories.

**Abstract:** The peer review process is often regarded as the gatekeeper of scientific integrity, yet increasing evidence suggests that it is not immune to bias. Although structural inequities in peer review have been widely debated, much less attention has been paid to the subtle ways in which language itself may reinforce disparities. This study undertakes one of the most comprehensive linguistic analyses of peer review to date, examining more than 80,000 reviews in two major journals. Using natural language processing and large-scale statistical modeling, it uncovers how review tone, sentiment, and supportive language vary across author demographics, including gender, race, and institutional affiliation. Using a data set that includes both anonymous and signed reviews, this research also reveals how the disclosure of reviewer identity shapes the language of evaluation. The findings not only expose hidden biases in peer feedback, but also challenge conventional assumptions about anonymity's role in fairness. As academic publishing grapples with reform, these insights raise critical questions about how review policies shape career trajectories and scientific progress.

</details>


### [70] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)

*Wai Keen Vong, Brenden M. Lake*

**Main category:** cs.CL

**Keywords:** machine learning, language acquisition, multimodal models, neural networks, word learning

**Relevance Score:** 8

**TL;DR:** This paper explores the robustness of multimodal neural networks in acquiring word-referent mappings using a dataset simulating children's language acquisition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how machine learning, particularly neural networks, can mimic children's language acquisition with limited input data.

**Method:** The study utilized automated speech transcription methods on the SAYCam dataset, creating multimodal datasets from 500 hours of video data with three children, and examined various neural network configurations for word learning.

**Key Contributions:**

	1. Developed multimodal vision-and-language datasets from children's video data.
	2. Demonstrated the effectiveness of neural networks in simulating word learning.
	3. Highlighted individual differences in language acquisition patterns among children.

**Result:** Networks trained on data transcribed from individual children successfully acquired and generalized word-referent mappings across different architectures.

**Limitations:** 

**Conclusion:** The results indicate that multimodal neural networks can robustly support grounded word learning while exhibiting individual differences based on children's unique developmental experiences.

**Abstract:** What insights can machine learning bring to understanding human language acquisition? Large language and multimodal models have achieved remarkable capabilities, but their reliance on massive training datasets creates a fundamental mismatch with children, who succeed in acquiring language from comparatively limited input. To help bridge this gap, researchers have increasingly trained neural networks using data similar in quantity and quality to children's input. Taking this approach to the limit, Vong et al. (2024) showed that a multimodal neural network trained on 61 hours of visual and linguistic input extracted from just one child's developmental experience could acquire word-referent mappings. However, whether this approach's success reflects the idiosyncrasies of a single child's experience, or whether it would show consistent and robust learning patterns across multiple children's experiences was not explored. In this article, we applied automated speech transcription methods to the entirety of the SAYCam dataset, consisting of over 500 hours of video data spread across all three children. Using these automated transcriptions, we generated multi-modal vision-and-language datasets for both training and evaluation, and explored a range of neural network configurations to examine the robustness of simulated word learning. Our findings demonstrate that networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures. These results validate the robustness of multimodal neural networks for grounded word learning, while highlighting the individual differences that emerge in how models learn when trained on each child's developmental experiences.

</details>


### [71] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)

*Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan*

**Main category:** cs.CL

**Keywords:** Generative Models, Recommendation Systems, Machine Learning, Attention Mechanisms, Tokenization

**Relevance Score:** 7

**TL;DR:** This paper presents GRACE, a novel generative framework for multi-behavior sequential recommendation that improves the efficiency and interpretability of recommendation systems by using a hybrid tokenization method and a journey-aware sparse attention mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in generative recommendation systems, specifically the need for explicit token reasoning and high computational costs associated with current models.

**Method:** GRACE utilizes a hybrid Chain-of-Thought tokenization to encode user-item interactions, alongside a Journey-Aware Sparse Attention mechanism that improves attention efficiency.

**Key Contributions:**

	1. Introduction of a hybrid Chain-of-Thought tokenization method for user-item interactions
	2. Development of a Journey-Aware Sparse Attention mechanism to reduce computational costs
	3. Demonstrated performance improvements over existing state-of-the-art recommendation models.

**Result:** GRACE achieved significant performance improvements, with up to +106.9% HR@10 and +106.7% NDCG@10 in the Home domain and +22.1% HR@10 in the Electronics domain, while reducing attention computation by up to 48%.

**Limitations:** 

**Conclusion:** The proposed framework offers a promising direction for enhancing the interpretability and efficiency of recommendation systems.

**Abstract:** Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of transformers and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware sparse Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.

</details>


### [72] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)

*Shoutao Guo, Shaolei Zhang, Qingkai Fang, Zhengrui Ma, Min Zhang, Yang Feng*

**Main category:** cs.CL

**Keywords:** Large Speech-Language Models, speech processing, long-speech, dynamic compression, benchmarking

**Relevance Score:** 9

**TL;DR:** FastLongSpeech is a framework for efficient long-speech processing using LSLMs without dedicated long-speech training data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical gap in processing long-form speech in existing Large Speech-Language Models due to lack of training datasets and high computational costs.

**Method:** FastLongSpeech employs an iterative fusion strategy to handle long-speech sequences and utilizes dynamic compression training to adapt to varying compression ratios of short-speech sequences.

**Key Contributions:**

	1. Introduction of FastLongSpeech framework for long-speech processing.
	2. Development of dynamic compression training approach for LSLMs.
	3. Creation of LongSpeech-Eval benchmark for assessing long-speech capabilities.

**Result:** The methodology demonstrates strong performance across long-speech and short-speech tasks, significantly enhancing inference efficiency.

**Limitations:** 

**Conclusion:** FastLongSpeech effectively extends the abilities of LSLMs to efficiently process long-speech without needing dedicated data.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.

</details>


### [73] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)

*Akriti Jain, Pritika Ramu, Aparna Garimella, Apoorv Saxena*

**Main category:** cs.CL

**Keywords:** chart generation, LLMs, intent-based visualization, HCI, data accuracy

**Relevance Score:** 9

**TL;DR:** The paper proposes a framework for intent-based chart generation from long documents using LLMs, addressing the challenges of transforming user intents into visual representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for chart generation from text descriptions are limited and do not effectively handle user-defined intents based on long documents.

**Method:** An unsupervised, two-stage framework where an LLM extracts relevant information from documents based on user intent, followed by heuristic-guided chart type selection and code generation.

**Key Contributions:**

	1. Introduction of intent-based chart generation from documents
	2. Development of a heuristic-guided module for chart type selection
	3. Proposal of an attribution-based metric for assessing chart data accuracy

**Result:** The proposed approach curated a dataset of 1,242 <intent, document, charts> tuples and demonstrated superior performance, with improvements of up to 9 points in data accuracy and 17 points in chart type selection over existing methods.

**Limitations:** 

**Conclusion:** The framework effectively generates charts from documents based on user intent, outperforming traditional methods in accuracy and relevance to user needs.

**Abstract:** Large Language Models (LLMs) have demonstrated strong capabilities in transforming text descriptions or tables to data visualizations via instruction-tuning methods. However, it is not straightforward to apply these methods directly for a more real-world use case of visualizing data from long documents based on user-given intents, as opposed to the user pre-selecting the relevant content manually. We introduce the task of intent-based chart generation from documents: given a user-specified intent and document(s), the goal is to generate a chart adhering to the intent and grounded on the document(s) in a zero-shot setting. We propose an unsupervised, two-staged framework in which an LLM first extracts relevant information from the document(s) by decomposing the intent and iteratively validates and refines this data. Next, a heuristic-guided module selects an appropriate chart type before final code generation. To assess the data accuracy of the generated charts, we propose an attribution-based metric that uses a structured textual representation of charts, instead of relying on visual decoding metrics that often fail to capture the chart data effectively. To validate our approach, we curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from two domains, finance and scientific, in contrast to the existing datasets that are largely limited to parallel text descriptions/ tables and their corresponding charts. We compare our approach with baselines using single-shot chart generation using LLMs and query-based retrieval methods; our method outperforms by upto $9$ points and $17$ points in terms of chart data accuracy and chart type respectively over the best baselines.

</details>


### [74] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)

*Yifei Wang*

**Main category:** cs.CL

**Keywords:** reasoning distillation, long-context comprehension, Retrieval-Augmented Generation, language models, multi-document QA

**Relevance Score:** 8

**TL;DR:** The paper investigates the effects of reasoning distillation on language models' long-context comprehension and retrieval capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unexplored impact of large-scale reasoning distillation on in-context retrieval and reasoning, particularly in the context of Retrieval-Augmented Generation (RAG) systems.

**Method:** The study evaluates open-source models distilled from Deepseek-R1, focusing on their performance in multi-document question answering tasks to assess long-context awareness and information extraction.

**Key Contributions:**

	1. Demonstration of the impact of reasoning distillation on long-context comprehension.
	2. Insights into mitigating the 'lost in the middle' problem in language models.
	3. Performance evaluation of distilled models on multi-document question answering tasks.

**Result:** Distilled models exhibit significantly improved capabilities in understanding long contexts, fostering detailed reasoning processes and addressing the 'lost in the middle' issue.

**Limitations:** 

**Conclusion:** Reasoning distillation enhances the long-context understanding of language models, promoting effective contextual information utilization.

**Abstract:** Reasoning distillation has emerged as an effective approach to enhance the reasoning capabilities of smaller language models. However, the impact of large-scale reasoning distillation on other critical abilities, particularly in-context retrieval and reasoning, remains unexplored. This gap in understanding is particularly significant given the increasing importance of Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and utilization of contextual information are paramount for generating reliable responses. Motivated by the need to understand how the extended long-CoT process influences long-context comprehension, we conduct a comprehensive investigation using a series of open-source models distilled from Deepseek-R1, renowned for its exceptional reasoning capabilities. Our study focuses on evaluating these models' performance in extracting and integrating relevant information from extended contexts through multi-document question and answering tasks. Through rigorous experimentation, we demonstrate that distilled reasoning patterns significantly improve long-context understanding. Our analysis reveals that distillation fosters greater long-context awareness by promoting more detailed and explicit reasoning processes during context analysis and information parsing. This advancement effectively mitigates the persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [75] [Tiny language models](https://arxiv.org/abs/2507.14871)

*Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter*

**Main category:** cs.CL

**Keywords:** tiny language models, pre-training, natural language processing, classification, neural networks

**Relevance Score:** 7

**TL;DR:** This study investigates tiny language models (TLMs) and their pre-training effectiveness compared to large language models (LLMs), showing significant performance improvements with pre-training even on a small scale.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more accessible alternatives to large language models due to the high computational resources required for LLM pre-training, limiting research opportunities.

**Method:** The study involved pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on classification tasks like FewRel, AGNews, and DBPedia.

**Key Contributions:**

	1. Demonstrated TLMs can achieve significant classification performance with pre-training.
	2. Showed that a soft committee of shallow architectures can match accuracy of a deep TLM.
	3. Identified the importance of dataset size and token overlap for model performance.

**Result:** TLMs show a performance gap between pre-trained and non-pre-trained models across classification tasks. The performance gap increases with the size of the pre-training dataset and the overlap between token sets.

**Limitations:** The study focuses on specific classification tasks and datasets, which may limit the generalizability of the results.

**Conclusion:** Tiny language models can achieve classification accuracy comparable to larger models when pre-trained, suggesting they can facilitate NLP research in a more accessible manner.

**Abstract:** A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of LLMs. We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy. Our results are based on pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. Future research on TLM is expected to further illuminate the mechanisms underlying NLP, especially given that its biologically inspired models suggest that TLMs may be sufficient for children or adolescents to develop language.

</details>


### [76] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)

*Shiyi Mu, Yongkang Liu, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang*

**Main category:** cs.CL

**Keywords:** Emotion-Cause Pair Extraction, Large Language Models, Knowledge Injection

**Relevance Score:** 8

**TL;DR:** The paper introduces MEKiT, a novel method for improving large language models' performance on the Emotion-Cause Pair Extraction task by integrating emotional and causal knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with the ECPE task due to the lack of auxiliary knowledge, which hampers their reasoning abilities.

**Method:** MEKiT integrates heterogeneous internal emotional knowledge and external causal knowledge, using instruction templates and mixed data for instruction-tuning.

**Key Contributions:**

	1. Introduction of the MEKiT method for ECPE task enhancement
	2. Integration of emotional and causal knowledge
	3. Demonstrated improved performance through experimental results

**Result:** Experimental results show MEKiT significantly enhances LLM performance on the ECPE task compared to baseline models.

**Limitations:** 

**Conclusion:** MEKiT serves as an effective solution for improving LLMs in tasks requiring emotional and causal reasoning.

**Abstract:** Although large language models (LLMs) excel in text comprehension and generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task, which requires reasoning ability, is often underperform smaller language model. The main reason is the lack of auxiliary knowledge, which limits LLMs' ability to effectively perceive emotions and reason causes. To address this issue, we propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge \textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous internal emotional knowledge and external causal knowledge. Specifically, for these two distinct aspects and structures of knowledge, we apply the approaches of incorporating instruction templates and mixing data for instruction-tuning, which respectively facilitate LLMs in more comprehensively identifying emotion and accurately reasoning causes. Experimental results demonstrate that MEKiT provides a more effective and adaptable solution for the ECPE task, exhibiting an absolute performance advantage over compared baselines and dramatically improving the performance of LLMs on the ECPE task.

</details>


### [77] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)

*Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng*

**Main category:** cs.CL

**Keywords:** Large Language Models, code-switching, multilingual capabilities, fine-tuning, sparse autoencoders

**Relevance Score:** 8

**TL;DR:** This paper addresses unexpected code-switching in large language models (LLMs) and introduces a new fine-tuning method to mitigate this issue while preserving multilingual performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Unexpected code-switching in LLMs leads to poor readability and usability, but previous efforts lack depth in analysis and effectiveness.

**Method:** The authors employ sparse autoencoders to analyze linguistic features causing code-switching and propose Sparse Autoencoder-guided Supervised Fine-tuning (SASFT) to control pre-activation values during training.

**Key Contributions:**

	1. In-depth mechanistic analysis of code-switching using sparse autoencoders.
	2. Introduction of SASFT to control language feature activation during training.
	3. Demonstration of significant reductions in code-switching with maintained model performance.

**Result:** SASFT reduces unexpected code-switching by over 50% across five models and three languages, achieving complete elimination in four instances, while maintaining or improving performance on multilingual benchmarks.

**Limitations:** Focuses mainly on reducing code-switching; effectiveness in broader contexts not fully tested.

**Conclusion:** The proposed method effectively addresses the code-switching issue in LLMs without deteriorating their multilingual capabilities, showcasing promise for future improvements in model usability.

**Abstract:** Large Language Models (LLMs) have impressive multilingual capabilities, but they suffer from unexpected code-switching, also known as language mixing, which involves switching to unexpected languages in the model response. This problem leads to poor readability and degrades the usability of model responses. However, existing work on this issue lacks a mechanistic analysis and shows limited effectiveness. In this paper, we first provide an in-depth analysis of unexpected code-switching using sparse autoencoders and find that when LLMs switch to a language, the features of that language exhibit excessive pre-activation values. Based on our findings, we propose $\textbf{S}$parse $\textbf{A}$utoencoder-guided $\textbf{S}$upervised $\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain appropriate pre-activation values of specific language features during training. Experiments on five models across three languages demonstrate that SASFT consistently reduces unexpected code-switching by more than 50\% compared to standard supervised fine-tuning, with complete elimination in four cases. Moreover, SASFT maintains or even improves the models' performance on six multilingual benchmarks, showing its effectiveness in addressing code-switching while preserving multilingual capabilities.

</details>


### [78] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)

*Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi*

**Main category:** cs.CL

**Keywords:** cross-lingual alignment, large language models, neuron state-based evaluation, multilingual, evaluation methodologies

**Relevance Score:** 8

**TL;DR:** The paper introduces NeuronXA, a methodology for evaluating the cross-lingual alignment of large language models using a neuron state-based approach, demonstrating high correlation with performance metrics even with limited data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address gaps in assessing cross-lingual alignment in LLMs, particularly in low-resource languages, where existing benchmarks may not adequately reflect semantic alignment.

**Method:** Neuron State-Based Cross-Lingual Alignment (NeuronXA) is proposed, inspired by neuroscientific findings, to evaluate cross-lingual capabilities of multilingual LLMs across five models (LLaMA, Qwen, Mistral, GLM, OLMo) on multiple tasks and benchmarks.

**Key Contributions:**

	1. Introduction of NeuronXA for evaluating cross-lingual alignment in LLMs
	2. High correlation results with small dataset for downstream tasks
	3. Potential to improve semantic understanding in multilingual models

**Result:** NeuronXA achieves a Pearson correlation of 0.9556 with downstream task performance and 0.8514 with transferability using only 100 parallel sentence pairs, indicating strong effectiveness in alignment evaluation.

**Limitations:** 

**Conclusion:** NeuronXA not only demonstrates effectiveness in assessing cross-lingual alignment and transferability but also stands to significantly enhance the field of cross-lingual alignment research.

**Abstract:** Large language models (LLMs) have demonstrated remarkable multilingual capabilities, however, how to evaluate cross-lingual alignment remains underexplored. Existing alignment benchmarks primarily focus on sentence embeddings, but prior research has shown that neural models tend to induce a non-smooth representation space, which impact of semantic alignment evaluation on low-resource languages. Inspired by neuroscientific findings that similar information activates overlapping neuronal regions, we propose a novel Neuron State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a lignment capabilities of LLMs, which offers a more semantically grounded approach to assess cross-lingual alignment. We evaluate NeuronXA on several prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two transfer tasks and three multilingual benchmarks. The results demonstrate that with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation of 0.9556 with downstream tasks performance and 0.8514 with transferability. These findings demonstrate NeuronXA's effectiveness in assessing both cross-lingual alignment and transferability, even with a small dataset. This highlights its potential to advance cross-lingual alignment research and to improve the semantic understanding of multilingual LLMs.

</details>


### [79] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)

*Eliya Habba, Noam Dahan, Gili Lior, Gabriel Stanovsky*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Engineering, Evaluation Framework

**Relevance Score:** 9

**TL;DR:** PromptSuite is a framework for automatically generating prompt variations for improved evaluation of LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There's a need for more reliable evaluation of LLMs, as small changes in prompts can lead to large performance differences.

**Method:** PromptSuite enables the automatic generation of diverse prompts through a modular design, allowing controlled variations and extensibility.

**Key Contributions:**

	1. Automatic generation of prompt variations for LLM evaluation
	2. Modular and flexible prompt design
	3. User-friendly interface for non-expert users

**Result:** Case studies demonstrate that PromptSuite can generate meaningful prompt variations for robust evaluation practices.

**Limitations:** 

**Conclusion:** PromptSuite supports better evaluation of LLMs and is accessible via a Python API and a web interface.

**Abstract:** Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. It is available through both a Python API: https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface: https://promptsuite.streamlit.app/

</details>


### [80] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)

*Vahid Rahimzadeh, Erfan Moosavi Monazzah, Mohammad Taher Pilehvar, Yadollah Yaghoobzadeh*

**Main category:** cs.CL

**Keywords:** persona-driven LLMs, SYNTHIA, computational social science, narrative consistency, social interaction metadata

**Relevance Score:** 7

**TL;DR:** SYNTHIA is a novel dataset of 30,000 synthetic backstories based on real social media users, enhancing persona-driven LLMs in social science by ensuring narrative consistency and incorporating temporal and interaction metadata.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve persona-driven language models in computational social science by grounding synthetic data in authentic user activity while ensuring consistency and realism.

**Method:** The authors created a dataset called SYNTHIA, comprising 30,000 backstories sourced from 10,000 real BlueSky social media users across three time periods, integrating temporal aspects and social interaction data.

**Key Contributions:**

	1. Introduction of SYNTHIA dataset bridging real user data and synthetic persona generation
	2. Improvement in narrative consistency in persona-driven LLMs
	3. Inclusion of temporal and social interaction metadata in dataset

**Result:** SYNTHIA shows competitive performance in demographic diversity and social survey alignment when compared to state-of-the-art methods, significantly excelling in narrative consistency.

**Limitations:** 

**Conclusion:** The incorporation of real user activity and temporal dimensionality in SYNTHIA facilitates new research avenues in computational social science and enhances persona-driven language models.

**Abstract:** Persona-driven LLMs have emerged as powerful tools in computational social science, yet existing approaches fall at opposite extremes, either relying on costly human-curated data or producing synthetic personas that lack consistency and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky open platform across three time windows, bridging this spectrum by grounding synthetic generation in authentic user activity. Our evaluation demonstrates that SYNTHIA achieves competitive performance with state-of-the-art methods in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and provides rich social interaction metadata from the underlying network, enabling new research directions in computational social science and persona-driven language modeling.

</details>


### [81] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)

*Hang Yan, Fangzhi Xu, Rongman Xu, Yifei Li, Jian Zhang, Haoran Luo, Xiaobao Wu, Luu Anh Tuan, Haiteng Zhao, Qika Lin, Jun Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Test-Time Scaling, Momentum Uncertainty, Reasoning Efficiency, Artificial Intelligence

**Relevance Score:** 9

**TL;DR:** This paper introduces Momentum Uncertainty-guided Reasoning (MUR), an efficient technique for guiding test-time scaling in Large Language Models (LLMs) without extra training, significantly improving reasoning efficiency and accuracy.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of optimizing reasoning efficiency in LLMs, particularly when using Test-Time Scaling (TTS), which can lead to inefficiencies like overthinking and redundant computations.

**Method:** MUR utilizes a momentum-inspired approach to dynamically adjust reasoning budgets by tracking and aggregating stepwise uncertainty at each reasoning step. It incorporates a single hyperparameter for flexible gamma-control of reasoning budgets.

**Key Contributions:**

	1. Introduction of Momentum Uncertainty-guided Reasoning (MUR) method
	2. Implementation of a gamma-control mechanism for dynamic budget allocation
	3. Theoretical proof demonstrating MUR's stability and reduction of biases.

**Result:** MUR achieves over 50% reduction in computation and improves accuracy by 0.62-3.37% compared to various TTS methods across multiple benchmarks.

**Limitations:** 

**Conclusion:** MUR demonstrates significant improvements in both efficiency and accuracy for LLMs during test-time reasoning, proving its effectiveness over existing methods.

**Abstract:** Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinking, wasting tokens on redundant computations. This work investigates how to efficiently and adaptively guide LLM test-time scaling without additional training. Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating stepwise uncertainty over time. To support flexible inference-time control, we introduce gamma-control, a simple mechanism that tunes the reasoning budget via a single hyperparameter. We provide in-depth theoretical proof to support the superiority of MUR in terms of stability and biases. MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%.

</details>


### [82] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)

*Qiaoyu Tang, Hao Xiang, Le Yu, Bowen Yu, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, critic module, reinforcement learning, model evaluation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces RefCritic, a critic module utilizing reinforcement learning to enhance the critique abilities of Large Language Models beyond existing supervised fine-tuning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As Large Language Models advance, there is an increasing need for effective critic modules to provide precise guidance and improve model critique capabilities.

**Method:** RefCritic employs a long-chain-of-thought approach with reinforcement learning, incorporating dual rule-based rewards to assess both solution judgments and refinement accuracies based on critiques.

**Key Contributions:**

	1. Introduction of RefCritic as a novel critic module
	2. Utilization of reinforcement learning with dual rule-based rewards
	3. Demonstrated significant performance gains across multiple benchmarks

**Result:** RefCritic shows significant performance improvements on critique and refinement tasks, with gains of 6.8% and 7.2% on AIME25 for tested models, and it excels in majority voting scenarios.

**Limitations:** 

**Conclusion:** The results demonstrate that RefCritic outperforms traditional step-level supervision methods, suggesting a more effective approach to enhancing model critiques and refinements.

**Abstract:** With the rapid advancement of Large Language Models (LLMs), developing effective critic modules for precise guidance has become crucial yet challenging. In this paper, we initially demonstrate that supervised fine-tuning for building critic modules (which is widely adopted in current solutions) fails to genuinely enhance models' critique abilities, producing superficial critiques with insufficient reflections and verifications. To unlock the unprecedented critique capabilities, we propose RefCritic, a long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques, aiming to generate high-quality evaluations with actionable feedback that effectively guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement settings, RefCritic demonstrates consistent advantages across all benchmarks, e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably, under majority voting, policy models filtered by RefCritic show superior scaling with increased voting numbers. Moreover, despite training on solution-level supervision, RefCritic outperforms step-level supervised approaches on ProcessBench, a benchmark to identify erroneous steps in mathematical reasoning.

</details>


### [83] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)

*Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information Seeking, Dataset Synthesis, Knowledge Projections, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** The paper presents WebShaper, a framework for synthesizing datasets for information-seeking tasks using formalization and Knowledge Projections to enhance the performance of LLM agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of high-quality training data limits the development of information-seeking (IS) agents, prompting the need for better synthesis methods that ensure consistency in reasoning structure and question-answer pairs.

**Method:** WebShaper utilizes set theory to formalize IS tasks, employing the concept of Knowledge Projections to control reasoning structures and applying a multi-step expansion process to synthesize data.

**Key Contributions:**

	1. Introduction of the WebShaper framework for dataset synthesis in IS tasks.
	2. Formalization of IS tasks through set theory and Knowledge Projections.
	3. Demonstrated state-of-the-art performance on benchmark datasets.

**Result:** WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks, outperforming existing IS agents.

**Limitations:** 

**Conclusion:** The proposed framework effectively bridges the gap between information structure and reasoning structure, contributing to advancements in the capabilities of LLM-powered IS agents.

**Abstract:** The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.

</details>


### [84] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)

*Albert Chen, Manas Bundele, Gaurav Ahlawat, Patrick Stetz, Zhitao Wang, Qiang Fei, Donghoon Jung, Audrey Chu, Bharadwaj Jayaraman, Ayushi Panth, Yatin Arora, Sourav Jain, Renjith Varma, Alexey Ilin, Iuliia Melnychuk, Chelsea Chueh, Joyan Sil, Xiaofeng Wang*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, chatbot, knowledge graph, natural language processing, data insights

**Relevance Score:** 8

**TL;DR:** The paper discusses the development of an internal chatbot at LinkedIn that allows teams to self-serve data insights from a large database using a Text-to-SQL agent and a knowledge graph.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve access to data insights for LinkedIn's product management, engineering, and operations teams from a vast data lake.

**Method:** The approach involves creating a knowledge graph from various sources, developing a Text-to-SQL agent for querying, and designing an interactive chatbot that supports diverse user needs in data interaction.

**Key Contributions:**

	1. Development of a knowledge graph for enhanced data semantics
	2. Creation of a Text-to-SQL agent that corrects errors
	3. Design of an interactive chatbot for rich user engagement

**Result:** The chatbot serves over 300 weekly users and boasts a 53% accuracy rate on an internal benchmarking test for response correctness. Ablation studies highlighted key components of the knowledge graph and modeling.

**Limitations:** The accuracy of the chatbot responses is limited, achieving only 53% correctness on an internal benchmark.

**Conclusion:** This work offers insights into building effective enterprise Text-to-SQL solutions that enhance data accessibility and user engagement.

**Abstract:** The introduction of large language models has brought rapid progress on Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise solution. In this paper, we present insights from building an internal chatbot that enables LinkedIn's product managers, engineers, and operations teams to self-serve data insights from a large, dynamic data lake. Our approach features three components. First, we construct a knowledge graph that captures up-to-date semantics by indexing database metadata, historical query logs, wikis, and code. We apply clustering to identify relevant tables for each team or product area. Second, we build a Text-to-SQL agent that retrieves and ranks context from the knowledge graph, writes a query, and automatically corrects hallucinations and syntax errors. Third, we build an interactive chatbot that supports various user intents, from data discovery to query writing to debugging, and displays responses in rich UI elements to encourage follow-up chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of its responses are correct or close to correct on an internal benchmark set. Through ablation studies, we identify the most important knowledge graph and modeling components, offering a practical path for developing enterprise Text-to-SQL solutions.

</details>


### [85] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)

*Chenlei Gong, Yuanhe Tian, Lei Mao, Yan Song*

**Main category:** cs.CL

**Keywords:** DNA sequencing, Transformers, k-mer segmentation, BPE tokenization, positional encoding

**Relevance Score:** 2

**TL;DR:** The study compares k-mer segmentation and BPE tokenization for DNA sequence modeling using Transformers, revealing BPE's superior and stable performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate different tokenization methods and positional encoding in modeling DNA sequences using Transformers.

**Method:** We compare k-mer segmentation with various lengths and a BPE vocabulary across different Transformer layer configurations on the GUE benchmark dataset.

**Key Contributions:**

	1. Comparison of k-mer segmentation and BPE in DNA modeling
	2. Evaluation of tokenization and positional encoding methods
	3. Insights on Transformer layer depth effects on performance

**Result:** BPE shows higher and more stable performance across tasks; RoPE effectively captures periodic motifs, and increasing Transformer layers improves performance up to 12 layers.

**Limitations:** 

**Conclusion:** These findings offer practical guidance for optimizing tokenization and encoding choices in DNA Transformer models.

**Abstract:** Currently, many studies view DNA sequences as a special type of language and utilize Transformers to model them. These studies use fixed-length k-mer segmentation and BPE subword tokenization but lack a systematic evaluation to determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a 4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal, AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and 24-layer Transformer encoders and evaluated on GUE benchmark dataset. In general, BPE delivers higher and more stable performance across tasks by compressing frequent motifs into variable-length tokens, reducing sequence length, and improving model generalization. RoPE excels at capturing periodic motifs and extrapolating to long sequences, while AliBi also performs well on tasks driven by local dependencies. In terms of depth, we observe significant gains when increasing layers from 3 to 12, with only marginal improvements or slight overfitting at 24 layers. This study provides practical guidance for designing tokenization and positional encoding in DNA Transformer models.

</details>


### [86] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)

*Vijeta Deshpande, Ishita Dasgupta, Uttaran Bhattacharya, Somdeb Sarkhel, Saayan Mitra, Anna Rumshisky*

**Main category:** cs.CL

**Keywords:** Large Language Models, lexical diversity, synthetic data, PATTR, creative writing

**Relevance Score:** 8

**TL;DR:** Proposes a new metric, PATTR, for evaluating lexical diversity in synthesized text, addressing biases introduced by text length variations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of synthetic data generated by LLMs by measuring lexical diversity more accurately, especially in creative writing tasks.

**Method:** Introduced the Penalty-Adjusted Type-Token Ratio (PATTR) metric and evaluated it against existing metrics by generating a synthetic corpus of over 20M words in a video script generation creative writing context.

**Key Contributions:**

	1. Introduction of the Penalty-Adjusted Type-Token Ratio (PATTR) diversity metric
	2. Evaluation of PATTR against traditional diversity metrics
	3. Demonstration of its practical utility in filtering diverse responses from LLM outputs

**Result:** PATTR consistently outperforms traditional metrics (MATTR and CR) in measuring lexical diversity while effectively mitigating length biases.

**Limitations:** The study primarily focuses on a specific creative writing task, which may limit generalizability to other tasks and contexts.

**Conclusion:** PATTR provides a more reliable measure for evaluating diversity in generated text samples, crucial for enhancing LLM training with synthetic data.

**Abstract:** Synthetic text generated by Large Language Models (LLMs) is increasingly used for further training and improvement of LLMs. Diversity is crucial for the effectiveness of synthetic data, and researchers rely on prompt engineering to improve diversity. However, the impact of prompt variations on response text length, and, more importantly, the consequential effect on lexical diversity measurements, remain underexplored. In this work, we propose Penalty-Adjusted Type-Token Ratio (PATTR), a diversity metric robust to length variations. We generate a large synthetic corpus of over 20M words using seven models from the LLaMA, OLMo, and Phi families, focusing on a creative writing task of video script generation, where diversity is crucial. We evaluate per-response lexical diversity using PATTR and compare it against existing metrics of Moving-Average TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length variations introduce biases favoring shorter responses. Unlike existing metrics, PATTR explicitly considers the task-specific target response length ($L_T$) to effectively mitigate length biases. We further demonstrate the utility of PATTR in filtering the top-10/100/1,000 most lexically diverse responses, showing that it consistently outperforms MATTR and CR by yielding on par or better diversity with high adherence to $L_T$.

</details>


### [87] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)

*Chathuri Jayaweera, Brianna Yanqui, Bonnie Dorr*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, commonsense knowledge, Large Language Models, factuality, consistency

**Relevance Score:** 8

**TL;DR:** This study investigates the use of Large Language Models (LLMs) for generating commonsense knowledge in the task of Natural Language Inference (NLI).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the insufficient coverage of existing commonsense resources for NLI, emphasizing the need for systems that can emulate human inferential processes incorporating commonsense knowledge.

**Method:** The authors adapt existing metrics to assess LLMs for factuality and consistency in generating commonsense knowledge, and evaluate the impact on NLI prediction accuracy.

**Key Contributions:**

	1. Exploration of LLMs as commonsense knowledge generators for NLI.
	2. Adaptation of metrics for evaluating LLM factuality and consistency.
	3. Insights into the nuanced role of commonsense knowledge in improving inference differentiation.

**Result:** The findings indicate that while incorporating commonsense knowledge does not consistently enhance overall predictive performance, it aids in differentiating entailing instances and shows moderate improvement in defining contradictions and neutral inferences.

**Limitations:** The effectiveness of commonsense knowledge incorporation varies and does not always lead to improved results in NLI tasks.

**Conclusion:** The study concludes that LLMs can serve as valuable commonsense knowledge sources for NLI tasks, despite the variability in their overall effectiveness.

**Abstract:** Natural Language Inference (NLI) is the task of determining the semantic entailment of a premise for a given hypothesis. The task aims to develop systems that emulate natural human inferential processes where commonsense knowledge plays a major role. However, existing commonsense resources lack sufficient coverage for a variety of premise-hypothesis pairs. This study explores the potential of Large Language Models as commonsense knowledge generators for NLI along two key dimensions: their reliability in generating such knowledge and the impact of that knowledge on prediction accuracy. We adapt and modify existing metrics to assess LLM factuality and consistency in generating in this context. While explicitly incorporating commonsense knowledge does not consistently improve overall results, it effectively helps distinguish entailing instances and moderately improves distinguishing contradictory and neutral inferences.

</details>


### [88] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)

*Chathuri Jayaweera, Bonnie Dorr*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, annotation disagreement, ambiguity, NLI systems, human interpretation

**Relevance Score:** 7

**TL;DR:** This paper discusses the role of annotation disagreement in Natural Language Inference (NLI) due to content-based ambiguity, advocating for an ambiguity-aware approach in NLI. It presents a framework to identify and classify types of ambiguity, highlighting the need for new annotated datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper argues that annotation disagreement in NLI often indicates meaningful interpretive variation rather than noise, influenced by ambiguities in the premise or hypothesis.

**Method:** The authors propose a unified framework for identifying ambiguous input pairs and classifying ambiguity types, integrating existing taxonomies.

**Key Contributions:**

	1. Proposing an ambiguity-aware approach to NLI
	2. Presenting a unified framework for classifying ambiguity
	3. Highlighting the need for annotated datasets for ambiguity types

**Result:** Key ambiguity subtypes are illustrated with examples showing how they impact annotator decisions, emphasizing the necessity for methods that align models with human interpretation.

**Limitations:** The main limitation is the absence of datasets annotated for ambiguity and its subtypes.

**Conclusion:** The authors suggest creating new annotated datasets for ambiguity and developing unsupervised methods for detection to improve NLI systems.

**Abstract:** This position paper argues that annotation disagreement in Natural Language Inference (NLI) is not mere noise but often reflects meaningful interpretive variation, especially when triggered by ambiguity in the premise or hypothesis. While underspecified guidelines and annotator behavior can contribute to variation, content-based ambiguity offers a process-independent signal of divergent human perspectives. We call for a shift toward ambiguity-aware NLI by systematically identifying ambiguous input pairs and classifying ambiguity types. To support this, we present a unified framework that integrates existing taxonomies and illustrate key ambiguity subtypes through concrete examples. These examples reveal how ambiguity shapes annotator decisions and motivate the need for targeted detection methods that better align models with human interpretation. A key limitation is the lack of datasets annotated for ambiguity and subtypes. We propose addressing this gap through new annotated resources and unsupervised approaches to ambiguity detection -- paving the way for more robust, explainable, and human-aligned NLI systems.

</details>


### [89] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)

*Hellina Hailu Nigatu, Atnafu Lambebo Tonja, Henok Biadglign Ademtew, Hizkel Mitiku Alemayehu, Negasi Haile Abadi, Tadesse Destaw Belay, Seid Muhie Yimam*

**Main category:** cs.CL

**Keywords:** Homophone normalization, Amharic NLP, Cross-lingual transfer, Post-inference normalization, Language features

**Relevance Score:** 6

**TL;DR:** This paper investigates the effects of homophone normalization in Amharic NLP, proposing a post-inference method that improves BLEU scores while preserving linguistic features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of homophone normalization in NLP, which can hinder model performance and generalization in multilingual contexts.

**Method:** The study experiments with monolingual training and cross-lingual transfer using normalized and non-normalized datasets, introducing a post-inference normalization technique applied to model predictions.

**Key Contributions:**

	1. Introduces a post-inference normalization approach.
	2. Shows improved BLEU scores with minimal loss of language features.
	3. Stimulates discussion on language-aware interventions in NLP.

**Result:** The proposed technique yields an increase in BLEU score of up to 1.03, demonstrating improved model performance without losing language characteristics in training.

**Limitations:** The effectiveness of the proposed method may vary across different languages and scripts; further validation needed.

**Conclusion:** The work advocates for more nuanced approaches to language normalization, emphasizing the importance of technology-aware language practices that consider linguistic diversity.

**Abstract:** Homophone normalization, where characters that have the same sound in a writing script are mapped to one character, is a pre-processing step applied in Amharic Natural Language Processing (NLP) literature. While this may improve performance reported by automatic metrics, it also results in models that are not able to understand different forms of writing in a single language. Further, there might be impacts in transfer learning, where models trained on normalized data do not generalize well to other languages. In this paper, we experiment with monolingual training and cross-lingual transfer to understand the impacts of normalization on languages that use the Ge'ez script. We then propose a post-inference intervention in which normalization is applied to model predictions instead of training data. With our simple scheme of post-inference normalization, we show that we can achieve an increase in BLEU score of up to 1.03 while preserving language features in training. Our work contributes to the broader discussion on technology-facilitated language change and calls for more language-aware interventions.

</details>


### [90] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)

*Lingbo Li, Anuradha Mathrani, Teo Susnjak*

**Main category:** cs.CL

**Keywords:** LLMs, data extraction, meta-analysis, RCTs, health informatics

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of three LLMs for automating data extraction from RCTs for meta-analysis, finding customised prompts significantly improve recall.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automating data extraction from RCTs poses a challenge for meta-analysis, necessitating improved methodologies.

**Method:** Three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) were tested with four prompting strategies in three medical domains to assess their data extraction capabilities.

**Key Contributions:**

	1. Evaluation of multiple LLMs for a specific application in healthcare data extraction
	2. Identification of customised prompts as effective for improving recall
	3. Proposal of a three-tiered guideline for LLM use in real-world scenarios

**Result:** All models showed high precision but poor recall, with customised prompts improving recall by up to 15%.

**Limitations:** The research primarily focuses on RCTs in three medical domains, which may limit generalization.

**Conclusion:** The study suggests a three-tiered guideline for using LLMs in data extraction, advocating for a balanced approach of LLM efficiency and expert oversight.

**Abstract:** Automating data extraction from full-text randomised controlled trials (RCTs) for meta-analysis remains a significant challenge. This study evaluates the practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) across tasks involving statistical results, risk-of-bias assessments, and study-level characteristics in three medical domains: hypertension, diabetes, and orthopaedics. We tested four distinct prompting strategies (basic prompting, self-reflective prompting, model ensemble, and customised prompts) to determine how to improve extraction quality. All models demonstrate high precision but consistently suffer from poor recall by omitting key information. We found that customised prompts were the most effective, boosting recall by up to 15\%. Based on this analysis, we propose a three-tiered set of guidelines for using LLMs in data extraction, matching data types to appropriate levels of automation based on task complexity and risk. Our study offers practical advice for automating data extraction in real-world meta-analyses, balancing LLM efficiency with expert oversight through targeted, task-specific automation.

</details>


### [91] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)

*Xiandong Meng, Yan Wu, Yexin Tian, Xin Hu, Tianze Kang, Junliang Du*

**Main category:** cs.CL

**Keywords:** large language models, model distillation, multi-teacher guidance

**Relevance Score:** 8

**TL;DR:** The paper presents a distillation strategy for large language models that utilizes multiple teacher models to improve the student model's language understanding and generation while reducing computational costs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges of high computational cost and slow inference in deploying large language models.

**Method:** Proposes a distillation strategy guided by multiple teacher models that integrates their output probability distributions and intermediate semantic features.

**Key Contributions:**

	1. Weighted output fusion mechanism
	2. Feature alignment loss function
	3. Entropy-driven dynamic teacher weighting strategy

**Result:** The student model shows improved semantic information capture, high consistency in expression, generalization ability, and task adaptability across multiple evaluation metrics.

**Limitations:** 

**Conclusion:** The study demonstrates a feasible technical path for compressing large-scale language models and validates the effectiveness of multi-teacher collaborative mechanisms in complex language modeling tasks.

**Abstract:** This paper addresses the challenges of high computational cost and slow inference in deploying large language models. It proposes a distillation strategy guided by multiple teacher models. The method constructs several teacher models and integrates their output probability distributions and intermediate semantic features. This guides the student model to learn from multiple sources of knowledge. As a result, the student model gains stronger language understanding and generation ability while maintaining a small parameter size. To achieve this, the paper introduces a weighted output fusion mechanism, a feature alignment loss function, and an entropy-driven dynamic teacher weighting strategy. These components improve the quality and stability of knowledge transfer during distillation. Under multi-teacher guidance, the student model captures semantic information more effectively and demonstrates strong performance across multiple evaluation metrics. In particular, the method shows high consistency in expression, generalization ability, and task adaptability in tasks such as language modeling, text generation, and multi-task learning. The experiments compare the proposed method with several widely adopted distillation approaches. The results further confirm its overall advantages in perplexity, distillation loss, and generation quality. This study provides a feasible technical path for the efficient compression of large-scale language models. It also demonstrates the effectiveness of multi-teacher collaborative mechanisms in complex language modeling tasks.

</details>


### [92] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)

*Shayan Vassef, Amirhossein Dabiriaghdam, Mohammadreza Bakhtiari, Yadollah Yaghoobzadeh*

**Main category:** cs.CL

**Keywords:** multi-task learning, multi-source learning, pretrained language models, Subsets of Interest, fine-tuning

**Relevance Score:** 7

**TL;DR:** This paper examines multi-task, multi-lingual, and multi-source learning approaches to improve pretrained language models, introducing a new framework for analyzing training behaviors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how different learning settings affect the robustness and performance of language models.

**Method:** The authors introduce Subsets of Interest (SOI) to categorize learning behaviors during training, and they conduct experiments comparing multi-task, multi-source, and multi-lingual learning across various tasks.

**Key Contributions:**

	1. Introduction of Subsets of Interest (SOI) for training analysis.
	2. Comprehensive experiments across multiple language learning tasks.
	3. Development of a two-stage fine-tuning approach that incorporates SOI.

**Result:** Multi-source learning improves out-of-distribution performance by up to 7%, while multi-task learning shows variable results depending on task similarity; a two-stage fine-tuning method utilizing SOI enhances performance further.

**Limitations:** 

**Conclusion:** The study provides insights into training dynamics and practical methods for optimizing multi-setting language model performance.

**Abstract:** This work investigates the impact of multi-task, multi-lingual, and multi-source learning approaches on the robustness and performance of pretrained language models. To enhance this analysis, we introduce Subsets of Interest (SOI), a novel categorization framework that identifies six distinct learning behavior patterns during training, including forgettable examples, unlearned examples, and always correct examples. Through SOI transition heatmaps and dataset cartography visualization, we analyze how examples shift between these categories when transitioning from single-setting to multi-setting configurations. We perform comprehensive experiments across three parallel comparisons: multi-task vs. single-task learning using English tasks (entailment, paraphrase, sentiment), multi-source vs. single-source learning using sentiment analysis datasets, and multi-lingual vs. single-lingual learning using intent classification in French, English, and Persian. Our results demonstrate that multi-source learning consistently improves out-of-distribution performance by up to 7%, while multi-task learning shows mixed results with notable gains in similar task combinations. We further introduce a two-stage fine-tuning approach where the second stage leverages SOI-based subset selection to achieve additional performance improvements. These findings provide new insights into training dynamics and offer practical approaches for optimizing multi-setting language model performance.

</details>


### [93] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)

*Yuanhe Tian, Junjie Liu, Zhizhou Kou, Yuxiang Li, Yan Song*

**Main category:** cs.CL

**Keywords:** Chinese medical dataset, LLM pre-training, health informatics, reinforcement learning from human feedback, medical AI

**Relevance Score:** 8

**TL;DR:** ChiMed 2.0 is a large-scale Chinese medical dataset that enhances data coverage and supports various training methodologies for LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for high-quality, diverse Chinese medical datasets that support pre-training and reinforcement learning from human feedback to advance AI in healthcare.

**Method:** We created ChiMed 2.0 by collecting data from Chinese medical online platforms and generating content using LLMs, encompassing both traditional and modern medical literature.

**Key Contributions:**

	1. Introduction of ChiMed 2.0 dataset for Chinese medical AI research
	2. Comprehensive coverage of traditional and modern Chinese medical data
	3. Support for pre-training, supervised fine-tuning, and RLHF methods

**Result:** ChiMed 2.0 comprises 204.4M characters, including documents for pre-training, question-answering pairs for supervised fine-tuning, and preference data for RLHF, resulting in performance gains for LLMs on medical benchmarks.

**Limitations:** 

**Conclusion:** The dataset demonstrates significant effectiveness and applicability in training LLMs for the Chinese medical domain, improving their performance across various scales.

**Abstract:** Building high-quality data resources is crucial for advancing artificial intelligence research and applications in specific domains, particularly in the Chinese medical domain. Existing Chinese medical datasets are limited in size and narrow in domain coverage, falling short of the diverse corpora required for effective pre-training. Moreover, most datasets are designed solely for LLM fine-tuning and do not support pre-training and reinforcement learning from human feedback (RLHF). In this paper, we propose a Chinese medical dataset named ChiMed 2.0, which extends our previous work ChiMed, and covers data collected from Chinese medical online platforms and generated by LLMs. ChiMed 2.0 contains 204.4M Chinese characters covering both traditional Chinese medicine classics and modern general medical data, where there are 164.8K documents for pre-training, 351.6K question-answering pairs for supervised fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the effectiveness of our approach for training a Chinese medical LLM, we conduct further pre-training, SFT, and RLHF experiments on representative general domain LLMs and evaluate their performance on medical benchmark datasets. The results show performance gains across different model scales, validating the dataset's effectiveness and applicability.

</details>


### [94] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)

*Haoran Sun, Zekun Zhang, Shaoning Zeng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Evolution, User Preferences, Domain Cognition, NLP

**Relevance Score:** 9

**TL;DR:** Proposes a Dual-Phase Self-Evolution (DPSE) framework that improves Large Language Models (LLMs) by optimizing user preferences and domain-specific competence through structured data expansion and a fine-tuning pipeline.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of LLMs beyond pre-training by addressing limitations in domain cognition and user alignment.

**Method:** The DPSE framework includes a Censor module that extracts interaction signals and satisfaction scores to guide data expansion, followed by a two-stage fine-tuning process consisting of supervised domain grounding and preference optimization.

**Key Contributions:**

	1. Introduction of the Dual-Phase Self-Evolution framework
	2. Development of a Censor module for interaction signal extraction
	3. Demonstrated superior performance on NLP and dialogue tasks compared to baseline methods.

**Result:** DPSE consistently outperforms existing methods like Supervised Fine-Tuning and Memory-Augmented baselines on various NLP benchmarks and dialogue tasks.

**Limitations:** 

**Conclusion:** DPSE facilitates an autonomous self-evolution of LLMs, enhancing their adaptation to user preferences and domain knowledge.

**Abstract:** The capabilities of Large Language Models (LLMs) are limited to some extent by pre-training, so some researchers optimize LLMs through post-training. Existing post-training strategies, such as memory-based retrieval or preference optimization, improve user alignment yet fail to enhance the model's domain cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution (DPSE) framework that jointly optimizes user preference adaptation and domain-specific competence. DPSE introduces a Censor module to extract multi-dimensional interaction signals and estimate satisfaction scores, which guide structured data expansion via topic-aware and preference-driven strategies. These expanded datasets support a two-stage fine-tuning pipeline: supervised domain grounding followed by frequency-aware preference optimization. Experiments across general NLP benchmarks and long-term dialogue tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning, Preference Optimization, and Memory-Augmented baselines. Ablation studies validate the contribution of each module. In this way, our framework provides an autonomous path toward continual self-evolution of LLMs.

</details>


### [95] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)

*Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee*

**Main category:** cs.CL

**Keywords:** AI text detection, evaluation metrics, humanification framework

**Relevance Score:** 6

**TL;DR:** The paper introduces SHIELD, a new evaluation metric for AI text detectors focusing on reliability and stability for practical deployment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the assessment of AI text detectors by integrating real-world scenarios and ensuring stable performance across various domains.

**Method:** Developed a benchmark, SHIELD, that incorporates reliability and stability into its evaluation criteria and created a humanification framework to modify AI text for better resemblance to human authorship.

**Key Contributions:**

	1. Introduction of SHIELD benchmark for AI text detectors
	2. Incorporation of reliability and stability in evaluation
	3. Development of a humanification framework for AI text

**Result:** The SHIELD benchmark allows for better comparison of AI text detection systems in real-world settings and improves the assessment of zero-shot detection methods through a controllable hardness parameter.

**Limitations:** 

**Conclusion:** The proposed evaluation paradigm and methodology provide a more equitable and applicable framework for assessing AI text detectors, addressing gaps in existing metrics.

**Abstract:** We present a novel evaluation paradigm for AI text detectors that prioritizes real-world and equitable assessment. Current approaches predominantly report conventional metrics like AUROC, overlooking that even modest false positive rates constitute a critical impediment to practical deployment of detection systems. Furthermore, real-world deployment necessitates predetermined threshold configuration, making detector stability (i.e. the maintenance of consistent performance across diverse domains and adversarial scenarios), a critical factor. These aspects have been largely ignored in previous research and benchmarks. Our benchmark, SHIELD, addresses these limitations by integrating both reliability and stability factors into a unified evaluation metric designed for practical assessment. Furthermore, we develop a post-hoc, model-agnostic humanification framework that modifies AI text to more closely resemble human authorship, incorporating a controllable hardness parameter. This hardness-aware approach effectively challenges current SOTA zero-shot detection methods in maintaining both reliability and stability. (Data and code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [96] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)

*Thilo Hagendorff*

**Main category:** cs.CL

**Keywords:** AI alignment, political bias, large language models, ethical AI, human-centered design

**Relevance Score:** 6

**TL;DR:** This paper discusses the inherent political biases in AI alignment, specifically arguing that training large language models (LLMs) to be harmless and honest leads to left-wing political bias due to normative assumptions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the conflict between AI alignment principles and the perceived political bias in large language models, especially concerns regarding left-wing tendencies.

**Method:** The paper presents a theoretical argument analyzing the relationship between AI alignment objectives and their alignment with progressive moral frameworks.

**Key Contributions:**

	1. Analyzes the connection between AI alignment principles and progressive moral frameworks.
	2. Argues that left-wing political bias is a necessary outcome of adhering to AI alignment goals.
	3. Critiques the framing of political bias in AI research as problematic, highlighting a contradiction in AI alignment discussions.

**Result:** The study argues that adherence to alignment principles unavoidably results in left-wing political bias, countering critiques that frame such bias as problematic.

**Limitations:** 

**Conclusion:** The author concludes that critiques of left-leaning bias in LLMs undermine the very principles of AI alignment by promoting harmful ideologies contrary to those principles.

**Abstract:** The guiding principle of AI alignment is to train large language models (LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are mounting concerns that LLMs exhibit a left-wing political bias. Yet, the commitment to AI alignment cannot be harmonized with the latter critique. In this article, I argue that intelligent systems that are trained to be harmless and honest must necessarily exhibit left-wing political bias. Normative assumptions underlying alignment objectives inherently concur with progressive moral frameworks and left-wing principles, emphasizing harm avoidance, inclusivity, fairness, and empirical truthfulness. Conversely, right-wing ideologies often conflict with alignment guidelines. Yet, research on political bias in LLMs is consistently framing its insights about left-leaning tendencies as a risk, as problematic, or concerning. This way, researchers are actively arguing against AI alignment, tacitly fostering the violation of HHH principles.

</details>


### [97] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)

*Narun Raman, Taylor Lundy, Kevin Leyton-Brown*

**Main category:** cs.CL

**Keywords:** Large Language Models, multiple-choice question-answering, chain-of-thought reasoning, benchmarking, LLM performance

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of multiple-choice question-answering (MCQA) as a benchmark for Large Language Models (LLMs), revealing that traditional MCQA methods may not accurately predict model performance in real-world scenarios. It suggests guidelines for creating better benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether multiple-choice question-answering (MCQA) still serves as a reliable benchmark for evaluating Large Language Models (LLMs) and to investigate the conditions under which it is valid.

**Method:** A systematic evaluation of 15 different question-answering benchmarks and 25 different LLMs, assessing various presentation methods for questions, including the provision of options and the timing of chain-of-thought reasoning.

**Key Contributions:**

	1. Systematic evaluation of 15 question-answering benchmarks with 25 LLMs
	2. Insights into the influence of reasoning timing on MCQA effectiveness
	3. Guidelines for creating improved benchmarks for LLM performance assessment

**Result:** The study found that MCQA remains a good proxy for model performance when chain-of-thought reasoning occurs before presenting options, but large models perform better when reasoning occurs after options are given, indicating a shift in benchmark reliability.

**Limitations:** The study focuses primarily on the aspect of MCQA and may not account for other methods of reasoning assessment or diverse task types beyond question-answering.

**Conclusion:** The research concludes that traditional MCQA is no longer a suitable measure for assessing LLM performance in downstream tasks, advocating for the development of more accurate and bias-resistant benchmarks.

**Abstract:** When evaluating Large Language Models (LLMs) in question-answering domains, it is common to ask the model to choose among a fixed set of choices (so-called multiple-choice question-answering, or MCQA). Although downstream tasks of interest typically do not provide systems with explicit options among which to choose, this approach is nevertheless widely used because it makes it makes automatic grading straightforward and has tended to produce challenging benchmarks that correlate sufficiently well with downstream performance. This paper investigates the extent to which this trend continues to hold for state-of-the-art reasoning models, describing a systematic evaluation of $15$ different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different LLMs (including small models such as Qwen 7B and relatively large models such as Llama 70B). For each model-benchmark pair, we considered $5$ ways of presenting the model with questions, including variations on whether multiple choices were offered to the model at all; whether "none of the above" sometimes replaced the right answer; and whether the model was permitted to perform chain-of-thought reasoning before and/or after the choices were presented. MCQA remained a good proxy for the downstream performance of models as long as they were allowed to perform chain-of-thought reasoning only before being presented with the options among which they had to select. On the other hand, large models that were able to perform reasoning after being given a set of options tended to significantly outperform their free-text performance due to exploiting the information in the options. We conclude that MCQA is no longer a good proxy for assessing downstream performance of state-of-the-art models, and offer practical guidelines for designing more robust, bias-resistant benchmarks that better reflect LLMs' genuine reasoning capabilities.

</details>


### [98] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)

*Leanne Tan, Gabriel Chua, Ziyu Ge, Roy Ka-Wei Lee*

**Main category:** cs.CL

**Keywords:** multilingual moderation, low-resource languages, AI safety

**Relevance Score:** 7

**TL;DR:** LionGuard 2 is a multilingual moderation classifier for the Singapore context, outperforming several existing systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety gaps in multilingual moderation systems, particularly for low-resource languages and local contexts.

**Method:** Built on OpenAI embeddings and a multi-head ordinal classifier, designed specifically for the Singaporean environment.

**Key Contributions:**

	1. Lightweight multilingual classifier tailored for Singapore
	2. Outperforms existing moderation systems
	3. Releases model weights and training data for future research

**Result:** LionGuard 2 outperformed various commercial and open-source systems across 17 benchmarks, demonstrating practical efficacy in a governmental deployment.

**Limitations:** 

**Conclusion:** High-quality local data and robust multilingual embeddings allow for effective moderation without the need for large model fine-tuning.

**Abstract:** Modern moderation systems increasingly support multiple languages, but often fail to address localisation and low-resource variants - creating safety gaps in real-world deployments. Small models offer a potential alternative to large LLMs, yet still demand considerable data and compute. We present LionGuard 2, a lightweight, multilingual moderation classifier tailored to the Singapore context, supporting English, Chinese, Malay, and partial Tamil. Built on pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2 outperforms several commercial and open-source systems across 17 benchmarks, including both Singapore-specific and public English datasets. The system is actively deployed within the Singapore Government, demonstrating practical efficacy at scale. Our findings show that high-quality local data and robust multilingual embeddings can achieve strong moderation performance, without fine-tuning large models. We release our model weights and part of our training data to support future work on LLM safety.

</details>


### [99] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)

*Amedeo Buonanno, Alessandro Rivetti, Francesco A. N. Palmieri, Giovanni Di Gennaro, Gianmarco Romano*

**Main category:** cs.CL

**Keywords:** entropy analysis, Transformer models, interpretability

**Relevance Score:** 7

**TL;DR:** This study investigates the use of entropy analysis to understand information distribution in Transformer architectures, particularly focusing on GPT-based models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To probe how information is managed and transformed within Transformer-based models by analyzing token-level uncertainty and entropy patterns.

**Method:** Entropy analysis is applied to different stages of a GPT-based large language model to quantify information distribution and reveal internal representations.

**Key Contributions:**

	1. Introduces entropy analysis as a tool for model interpretability
	2. Demonstrates its application in a GPT-based model
	3. Enhances evaluation frameworks for Transformer models

**Result:** The methodology illustrates how entropy patterns can provide insights into model behavior and internal structures, contributing to interpretability and evaluation frameworks.

**Limitations:** 

**Conclusion:** The findings suggest that entropy analysis can enhance understanding and evaluation of Transformer-based models.

**Abstract:** This work explores entropy analysis as a tool for probing information distribution within Transformer-based architectures. By quantifying token-level uncertainty and examining entropy patterns across different stages of processing, we aim to investigate how information is managed and transformed within these models. As a case study, we apply the methodology to a GPT-based large language model, illustrating its potential to reveal insights into model behavior and internal representations. This approach may offer insights into model behavior and contribute to the development of interpretability and evaluation frameworks for transformer-based models

</details>


### [100] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)

*Elisa Sanchez-Bayona, Rodrigo Agerri*

**Main category:** cs.CL

**Keywords:** Large Language Models, metaphor interpretation, Natural Language Processing, evaluation frameworks, NLP

**Relevance Score:** 9

**TL;DR:** The paper evaluates Large Language Models' capabilities in metaphor interpretation using diverse datasets, revealing performance relies more on surface features than on understanding metaphorical content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of previous research focused on single-dataset evaluations and artificial data in metaphor processing within NLP.

**Method:** Extensive experiments were conducted using multiple publicly available datasets with inference and metaphor annotations, particularly in Natural Language Inference (NLI) and Question Answering (QA) tasks.

**Key Contributions:**

	1. Evaluation of LLMs with diverse datasets
	2. Insights on LLMs' metaphor processing capabilities
	3. Call for improved metaphor evaluation frameworks

**Result:** Findings show that LLMs' performance is largely influenced by lexical overlap and sentence length rather than metaphor content, indicating their abilities to interpret metaphor are overstated.

**Limitations:** Limited to public datasets and specific tasks, potentially influencing generalizability of results.

**Conclusion:** The study emphasizes the necessity for more realistic evaluation frameworks in metaphor interpretation tasks, given the identified limitations of LLMs.

**Abstract:** This paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that LLMs' performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.

</details>


### [101] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)

*Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, Reasoning, Latency, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper presents Stitch, a novel spoken language model that integrates an unspoken reasoning process while maintaining low latency in spoken responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current spoken language models lack the capability for internal reasoning, hindering their ability to communicate effectively.

**Method:** Stitch alternates between generating unspoken reasoning chunks and spoken response chunks, allowing for simultaneous thinking and talking.

**Key Contributions:**

	1. Introduction of Stitch, the first method to integrate unspoken reasoning into spoken language models.
	2. Demonstration of improved performance on math reasoning tasks compared to existing SLMs.
	3. Maintaining low response latency while enabling complex reasoning processes.

**Result:** Stitch achieves latency comparable to baseline models that do not generate unspoken reasoning while outperforming them by 15% on math reasoning datasets.

**Limitations:** 

**Conclusion:** Stitch successfully integrates reasoning in SLMs without increasing response latency, thus enhancing performance in both reasoning and non-reasoning tasks.

**Abstract:** Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [102] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)

*Jierui Li, Raymond Mooney*

**Main category:** cs.CL

**Keywords:** Large Language Models, Algorithm Identification, Benchmarking, Machine Learning, Problem Solving

**Relevance Score:** 7

**TL;DR:** This paper introduces AlgoSimBench, a benchmark for assessing LLMs' ability to identify algorithmically similar problems (ASPs) and proposes a method to improve their performance in this area.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to explore whether large language models (LLMs) can generalize their problem-solving abilities to domains less represented in training, especially regarding algorithmically similar problems.

**Method:** AlgoSimBench consists of 1317 problems annotated with 231 distinct algorithm tags, leading to the creation of 402 multiple-choice questions. The study evaluates LLMs' performance using attempted solution matching (ASM) to enhance the identification of ASPs.

**Key Contributions:**

	1. Introduction of AlgoSimBench benchmark for ASPs
	2. Proposing attempted solution matching (ASM) method
	3. Evaluation of code embedding models and retrieval methods on ASP identification

**Result:** The best-performing model achieved 65.9% accuracy on the MCQ task, with ASM improving accuracy by 6.7% to 11.7% across different models. When combining ASM with BM25 keyword-prioritization, accuracy reached 52.2%.

**Limitations:** Limited to the evaluation of LLMs in identifying ASPs and potential over-reliance on summarization techniques without addressing broader algorithmic understanding.

**Conclusion:** The findings suggest that while LLMs struggle with ASP identification, there are methods like ASM and BM25 that can significantly improve performance in this domain.

**Abstract:** Recent progress in LLMs, such as reasoning models, has demonstrated strong abilities to solve complex competitive programming problems, often rivaling top human competitors. However, it remains underexplored whether these abilities generalize to relevant domains that are less seen during training. To address this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs' ability to identify algorithmically similar problems (ASPs)-problems that can be solved using similar algorithmic approaches. AlgoSimBench consists of 1317 problems, annotated with 231 distinct fine-grained algorithm tags, from which we curate 402 multiple-choice questions (MCQs), where each question presents one algorithmically similar problem alongside three textually similar but algorithmically dissimilar distractors. Our evaluation reveals that LLMs struggle to identify ASPs, with the best-performing model (o3-mini) achieving only 65.9% accuracy on the MCQ task. To address this challenge, we propose attempted solution matching (ASM), a novel method for improving problem similarity detection. On our MCQ task, ASM yields an absolute accuracy improvement of 6.7% to 11.7% across different models. We also evaluated code embedding models and retrieval methods on similar problem identification. While the adversarial selection of problems degrades the performance to be less than random, we found that simply summarizing the problem to remove narrative elements eliminates the effect, and combining ASM with a keyword-prioritized method, BM25, can yield up to 52.2% accuracy. Code and data are available at github.com

</details>


### [103] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)

*Alexandru Coca, Mark Gaynor, Zhenxing Zhang, Jianpeng Cheng, Bo-Hsiang Tseng, Pete Boothroyd, Héctor Martinez Alonso, Diarmuid Ó Séaghdha, Anders Johannsen*

**Main category:** cs.CL

**Keywords:** large language models, digital assistants, task generation, action execution, evaluation dataset

**Relevance Score:** 9

**TL;DR:** This paper presents ASPERA, a framework for developing digital assistants powered by large language models (LLMs) to execute complex tasks through action execution programs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to explore the capabilities of LLMs in creating digital assistants that can perform complex actions, utilizing their programming knowledge effectively.

**Method:** The authors developed ASPERA, which includes a simulation of an assistant library and a human-assisted data generation engine to help create high-quality tasks based on user queries and simulation states.

**Key Contributions:**

	1. Development of ASPERA framework for LLM-powered digital assistants
	2. Creation of Asper-Bench evaluation dataset with challenging tasks
	3. Demonstration of the challenges LLMs face in program generation from custom libraries

**Result:** The study resulted in the creation of Asper-Bench, a dataset containing 250 complex tasks, demonstrating that generating programs based on custom assistant libraries poses significant challenges for LLMs.

**Limitations:** The paper does not address potential scalability issues of the framework or the generalizability of the results across different domains.

**Conclusion:** The findings indicate that while LLMs are promising for generating action execution programs, they face difficulties when compared to traditional code generation methods that do not rely on external dependencies.

**Abstract:** This work evaluates the potential of large language models (LLMs) to power digital assistants capable of complex action execution. These assistants rely on pre-trained programming knowledge to execute multi-step goals by composing objects and functions defined in assistant libraries into action execution programs. To achieve this, we develop ASPERA, a framework comprising an assistant library simulation and a human-assisted LLM data generation engine. Our engine allows developers to guide LLM generation of high-quality tasks consisting of complex user queries, simulation state and corresponding validation programs, tackling data availability and evaluation robustness challenges. Alongside the framework we release Asper-Bench, an evaluation dataset of 250 challenging tasks generated using ASPERA, which we use to show that program generation grounded in custom assistant libraries is a significant challenge to LLMs compared to dependency-free code generation.

</details>


### [104] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)

*Kaiyan Chang, Yonghao Shi, Chenglong Wang, Hang Zhou, Chi Hu, Xiaoqian Liu, Yingfeng Luo, Yuan Ge, Tong Xiao, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** Test-Time Scaling, Large Language Models, Reasoning, Inference, Machine Learning

**Relevance Score:** 7

**TL;DR:** The paper introduces Hybrid Test-Time Scaling (TTS), a training-free method designed to enhance the reasoning performance of large language models (LLMs) during inference by combining fine-grained sequential and classical parallel scaling techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing computation burden of training-based TTS methods and the decline in prominence of training-free TTS methods, this paper explores effective training-free approaches for improving model reasoning performance during inference.

**Method:** The study presents Conditional Step-level Self-refinement as a sequential scaling method and combines it with classical parallel scaling methods to create the Hybrid Test-Time Scaling paradigm.

**Key Contributions:**

	1. Introduction of Hybrid Test-Time Scaling for reasoning
	2. Design of Conditional Step-level Self-refinement method
	3. Demonstration of significant improvements in reasoning performance across multiple LLMs

**Result:** Experiments on five instruction-tuned LLMs show significant improvements in reasoning performance when employing the hybrid strategy with various training-free methods.

**Limitations:** 

**Conclusion:** Hybrid Test-Time Scaling offers a promising direction for enhancing LLM reasoning capabilities without the additional computational costs associated with training-based methods.

**Abstract:** Test-Time Scaling (TTS) is a promising approach to progressively elicit the model's intelligence during inference. Recently, training-based TTS methods, such as continued reinforcement learning (RL), have further surged in popularity, while training-free TTS methods are gradually fading from prominence. However, the additional computation overhead of training amplifies the burden on test-time scaling. In this paper, we focus on training-free TTS methods for reasoning. We first design Conditional Step-level Self-refinement, a fine-grained sequential scaling method guided by process verification. On top of its effectiveness, we further combine it with other classical parallel scaling methods at the step level, to introduce a novel inference paradigm called Hybrid Test-Time Scaling. Extensive experiments on five instruction-tuned LLMs across different scales (3B-14B) and families demonstrate that hybrid strategy incorporating various training-free TTS methods at a fine granularity has considerable potential for expanding the reasoning performance boundaries of LLMs.

</details>


### [105] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)

*Vitaly Protasov, Nikolay Babakov, Daryna Dementieva, Alexander Panchenko*

**Main category:** cs.CL

**Keywords:** text style transfer, multilingual evaluation, text detoxification

**Relevance Score:** 8

**TL;DR:** This paper evaluates text style transfer (TST) systems across nine languages, highlighting gaps in current methods and proposing improvements for multilingual evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap between automatic metrics and human judgments in evaluating text generation tasks, particularly for multilingual text style transfer. This paper aims to address these challenges by providing a comprehensive evaluation across multiple languages.

**Method:** The study conducted a multilingual evaluation of text detoxification systems across nine languages using modern neural-based evaluation models and prompting-based LLM-as-a-judge approaches. The evaluation drew inspiration from machine translation methodologies.

**Key Contributions:**

	1. First comprehensive multilingual study on TST evaluation
	2. Introduction of neural-based evaluation models for TST
	3. Proposed framework for designing multilingual evaluation pipelines

**Result:** The findings reveal the effectiveness of new evaluation models and provide a framework for designing reliable multilingual TST evaluation pipelines, particularly in the context of text detoxification.

**Limitations:** 

**Conclusion:** This study lays the groundwork for improved evaluation methods in multilingual TST, offering a practical approach for researchers and developers in the field.

**Abstract:** Despite recent progress in large language models (LLMs), evaluation of text generation tasks such as text style transfer (TST) remains a significant challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025) revealed a substantial gap between automatic metrics and human judgments. Moreover, most prior work focuses exclusively on English, leaving multilingual TST evaluation largely unexplored. In this paper, we perform the first comprehensive multilingual study on evaluation of text detoxification system across nine languages: English, Spanish, German, Chinese, Arabic, Hindi, Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation, we assess the effectiveness of modern neural-based evaluation models alongside prompting-based LLM-as-a-judge approaches. Our findings provide a practical recipe for designing more reliable multilingual TST evaluation pipeline in the text detoxification case.

</details>


### [106] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)

*Nicolas Poggi, Shashank Agnihotri, Margret Keuper*

**Main category:** cs.CL

**Keywords:** Terahertz imaging, In-Context Learning, Vision-Language Models, Classification, Low-data regimes

**Relevance Score:** 3

**TL;DR:** This paper presents a method using In-Context Learning with Vision-Language Models for improving classification in Terahertz imaging.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in Terahertz imaging such as limited annotations and low resolution for better image classification.

**Method:** The authors utilize In-Context Learning with Vision-Language Models, adapting them to the Terahertz domain with a modality-aligned prompting framework and evaluate their performance in zero-shot and one-shot settings.

**Key Contributions:**

	1. First application of ICL-enhanced VLMs to THz imaging
	2. Demonstrated improvement in classification and interpretability
	3. Provided a GitHub repository for further research and use

**Result:** ICL improves classification performance and interpretability in low-data situations, demonstrating the potential for effective use of VLMs in THz imaging.

**Limitations:** 

**Conclusion:** The introduction of ICL-enhanced VLMs for THz imaging represents a novel and promising approach for scientific domains facing resource constraints.

**Abstract:** Terahertz (THz) imaging enables non-invasive analysis for applications such as security screening and material classification, but effective image classification remains challenging due to limited annotations, low resolution, and visual ambiguity. We introduce In-Context Learning (ICL) with Vision-Language Models (VLMs) as a flexible, interpretable alternative that requires no fine-tuning. Using a modality-aligned prompting framework, we adapt two open-weight VLMs to the THz domain and evaluate them under zero-shot and one-shot settings. Our results show that ICL improves classification and interpretability in low-data regimes. This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising direction for resource-constrained scientific domains. Code: \href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub repository}.

</details>


### [107] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)

*Xinping Zhao, Shouzheng Huang, Yan Zhong, Xinshuo Hu, Baotian Hu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Evidence Extraction

**Relevance Score:** 9

**TL;DR:** The paper proposes LEAR, a mechanism for improving evidence extraction in Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) by explicitly reasoning and optimizing the evidence extraction process.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant impact of retrieval noises on the quality of LLM generation, which can lead to the omission of important clues and challenges in generalization.

**Method:** LEAR combines evidence reasoning and extraction into a unified response for end-to-end training, utilizing knowledge token masks and three types of verifiable reward functions for model optimization.

**Key Contributions:**

	1. Unified framework for evidence reasoning and extraction
	2. Use of knowledge token masks for model optimization
	3. Introduction of verifiable reward functions for policy optimization

**Result:** LEAR improves the accuracy of downstream tasks by providing high-quality, compact evidence, as evidenced by extensive experiments on benchmark datasets.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances the quality of evidence extraction in online RAG systems, contributing to more accurate responses from LLMs.

**Abstract:** Retrieval-Augmented Generation (RAG) effectively improves the accuracy of Large Language Models (LLMs). However, retrieval noises significantly impact the quality of LLMs' generation, necessitating the development of denoising mechanisms. Previous methods extract evidence straightforwardly without explicit thinking, which risks filtering out key clues and struggles with generalization. To this end, we propose LEAR, which learns to extract rational evidence by (1) explicitly reasoning to identify potential cues within retrieval contents first, and then (2) consciously extracting to avoid omitting any key cues helpful for answering questions. Specifically, we frame evidence reasoning and evidence extraction into one unified response for end-to-end training; apply knowledge token masks for disentanglement to derive reasoning-based and extraction-based answers; and devise three types of verifiable reward functions, including answer, length, and format, to update the model via the policy optimization algorithm. Extensive experiments on three benchmark datasets show the effectiveness of LEAR, providing compact and high-quality evidence, improving the accuracy of downstream tasks, and promoting effective application in online RAG systems.

</details>


### [108] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)

*Armin Pournaki*

**Main category:** cs.CL

**Keywords:** conflicting narratives, polarization, narrative alignment, political discourse, social media

**Relevance Score:** 3

**TL;DR:** This paper analyzes conflicting narratives on political issues in the German Twittersphere, showing how these narratives reveal mechanisms of polarization and alignment.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how conflicting narratives shape political reality and public discourse.

**Method:** Analysis of tweets from opposing opinion groups about political issues such as the war in Ukraine, Covid, and climate change.

**Key Contributions:**

	1. Identification of conflicting narratives in political discourse
	2. Evidence of narrative alignment as a discursive strategy
	3. Analysis of social media data to highlight narrative mechanisms

**Result:** Identified divergent interpretations and emplotments of events, providing insights into narrative alignment strategies used by political actors.

**Limitations:** 

**Conclusion:** Narratives serve as a critical analytical lens to understand polarization dynamics in public discourse.

**Abstract:** Narratives are key interpretative devices by which humans make sense of political reality. In this work, we show how the analysis of conflicting narratives, i.e. conflicting interpretive lenses through which political reality is experienced and told, provides insight into the discursive mechanisms of polarization and issue alignment in the public sphere. Building upon previous work that has identified ideologically polarized issues in the German Twittersphere between 2021 and 2023, we analyze the discursive dimension of polarization by extracting textual signals of conflicting narratives from tweets of opposing opinion groups. Focusing on a selection of salient issues and events (the war in Ukraine, Covid, climate change), we show evidence for conflicting narratives along two dimensions: (i) different attributions of actantial roles to the same set of actants (e.g. diverging interpretations of the role of NATO in the war in Ukraine), and (ii) emplotment of different actants for the same event (e.g. Bill Gates in the right-leaning Covid narrative). Furthermore, we provide first evidence for patterns of narrative alignment, a discursive strategy that political actors employ to align opinions across issues. These findings demonstrate the use of narratives as an analytical lens into the discursive mechanisms of polarization.

</details>


### [109] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)

*Alessio Pittiglio*

**Main category:** cs.CL

**Keywords:** argument mining, logical fallacies, multimodal, Transformer models, political debates

**Relevance Score:** 4

**TL;DR:** We propose a multimodal approach to detect logical fallacies in political debates using Transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Advance research in multimodal argument mining, focusing on identifying logical fallacies in political discourse.

**Method:** Utilized pretrained Transformer-based models and explored various methods to leverage context for fallacy classification in text, audio, and multimodal data.

**Key Contributions:**

	1. Proposed a multimodal approach for argument mining
	2. Demonstrated effectiveness of Transformer models for fallacy detection
	3. Provided comparative analysis across text, audio, and multimodal formats

**Result:** Achieved macro F1-scores of 0.4444 for text, 0.3559 for audio, and 0.4403 for multimodal classification, with multimodal performance on par with text-only models.

**Limitations:** 

**Conclusion:** Indicates potential for improvement in multimodal argument mining approaches.

**Abstract:** In this paper, we present our submission to the MM-ArgFallacy2025 shared task, which aims to advance research in multimodal argument mining, focusing on logical fallacies in political debates. Our approach uses pretrained Transformer-based models and proposes several ways to leverage context. In the fallacy classification subtask, our models achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed performance comparable to the text-only model, suggesting potential for improvements.

</details>


### [110] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)

*Xinyu Zhang, Yuanquan Hu, Fangchao Liu, Zhicheng Dou*

**Main category:** cs.CL

**Keywords:** large language models, prompt optimization, human-computer interaction, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** P3 is a self-improvement framework that concurrently optimizes both system and user prompts in large language models, demonstrating improved performance in various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for optimizing prompts in large language models (LLMs) often do so in isolation, leading to suboptimal outcomes due to the interdependent nature of prompt components.

**Method:** The P3 framework simultaneously optimizes both system and user prompts through an iterative process, using offline optimized prompts for online prompting with query-dependent optimization.

**Key Contributions:**

	1. Development of the P3 framework for simultaneous prompt optimization
	2. Demonstration of superior performance on tasks through holistic strategies
	3. Empirical validation across multiple datasets and task types

**Result:** Extensive experiments reveal that P3 outperforms existing approaches on general and reasoning tasks, indicating the benefits of holistic prompt optimization.

**Limitations:** 

**Conclusion:** The study concludes that concurrent optimization of prompts enhances LLM performance across various tasks and introduces a promising framework for future applications.

**Abstract:** Current large language model (LLM) applications often employ multi-component prompts, comprising both system and user prompts, to guide model behaviors. While recent advancements have demonstrated the efficacy of automatically optimizing either the system or user prompt to boost performance, such unilateral approaches often yield suboptimal outcomes due to the interdependent nature of these components. In this work, we introduce P3, a novel self-improvement framework that concurrently optimizes both system and user prompts through an iterative process. The offline optimized prompts are further leveraged to promote online prompting by performing query-dependent prompt optimization. Extensive experiments on general tasks (e.g., Arena-hard and Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3 achieves superior performance in the realm of automatic prompt optimization. Our results highlight the effectiveness of a holistic optimization strategy in enhancing LLM performance across diverse domains.

</details>


### [111] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)

*Congmin Zheng, Jiachen Zhu, Jianghao Lin, Xinyi Dai, Yong Yu, Weinan Zhang, Mengyue Yang*

**Main category:** cs.CL

**Keywords:** Process Reward Models, length bias, CoLD, LLMs, counterfactual reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces CoLD, a framework to address length bias in Process Reward Models for large language models by implementing debiasing strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the length bias in existing Process Reward Models that can lead to unreliable reward predictions and unnecessarily verbose outputs in LLMs.

**Method:** CoLD employs a length-penalty adjustment, a learned bias estimator, and a joint training strategy to enforce length-invariance in predictions.

**Key Contributions:**

	1. Introduction of CoLD framework to mitigate length bias
	2. Utilization of counterfactual reasoning for debiasing
	3. Demonstrated practical improvements in mathematical problem-solving

**Result:** CoLD reduces reward-length correlation, enhances accuracy in selecting reasoning steps, and facilitates more concise reasoning across extensive experiments on MATH500 and GSM-Plus datasets.

**Limitations:** 

**Conclusion:** The results indicate CoLD improves the fidelity and robustness of Process Reward Models in large language models.

**Abstract:** Process Reward Models (PRMs) play a central role in evaluating and guiding multi-step reasoning in large language models (LLMs), especially for mathematical problem solving. However, we identify a pervasive length bias in existing PRMs: they tend to assign higher scores to longer reasoning steps, even when the semantic content and logical validity are unchanged. This bias undermines the reliability of reward predictions and leads to overly verbose outputs during inference. To address this issue, we propose CoLD(Counterfactually-Guided Length Debiasing), a unified framework that mitigates length bias through three components: an explicit length-penalty adjustment, a learned bias estimator trained to capture spurious length-related signals, and a joint training strategy that enforces length-invariance in reward predictions. Our approach is grounded in counterfactual reasoning and informed by causal graph analysis. Extensive experiments on MATH500 and GSM-Plus show that CoLD consistently reduces reward-length correlation, improves accuracy in step selection, and encourages more concise, logically valid reasoning. These results demonstrate the effectiveness and practicality of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [112] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)

*David Peter Wallis Freeborn*

**Main category:** cs.CL

**Keywords:** signaling games, compositional understanding, learning models

**Relevance Score:** 4

**TL;DR:** This paper presents new signaling game models that enable receivers to learn compositional information effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The existing signaling game models show that receivers struggle with learning compositional information which leads to loss of understanding when components of messages are forgotten.

**Method:** The author constructs two new signaling game models: a minimalist receiver, focusing on atomic messages, and a generalist receiver, learning from all available information.

**Key Contributions:**

	1. Introduction of minimalist and generalist receiver models in signaling games
	2. Demonstration of effective compositional understanding in receivers
	3. Simplification of learning processes compared to previous models

**Result:** The new models demonstrate that receivers can achieve genuine compositional understanding, improving upon the limitations of earlier models.

**Limitations:** 

**Conclusion:** These new models are simpler and more effective in allowing receivers to learn from the atomic components of messages, fostering genuine compositional understanding.

**Abstract:** Receivers in standard signaling game models struggle with learning compositional information. Even when the signalers send compositional messages, the receivers do not interpret them compositionally. When information from one message component is lost or forgotten, the information from other components is also erased. In this paper I construct signaling game models in which genuine compositional understanding evolves. I present two new models: a minimalist receiver who only learns from the atomic messages of a signal, and a generalist receiver who learns from all of the available information. These models are in many ways simpler than previous alternatives, and allow the receivers to learn from the atomic components of messages.

</details>


### [113] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)

*Seok Hwan Song, Mohna Chakraborty, Qi Li, Wallapak Tavanapong*

**Main category:** cs.CL

**Keywords:** Large Language Models, question types, reasoning tasks, accuracy, performance metrics

**Relevance Score:** 9

**TL;DR:** This study investigates how different question types influence the reasoning accuracy of Large Language Models (LLMs) across various tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unexplored impact of question types on LLM accuracy in reasoning tasks.

**Method:** Evaluated five LLMs on three question types using quantitative and deductive reasoning tasks, measuring accuracy in reasoning steps and final answers.

**Key Contributions:**

	1. Identification of performance differences across question types
	2. Insight into the lack of correlation between reasoning steps and final answers
	3. Analysis of how wording and number of options affect LLM outcomes

**Result:** Significant variations in LLM performance were found based on question types, revealing no direct correlation between reasoning accuracy and final answer selection.

**Limitations:** 

**Conclusion:** Question types and their characteristics significantly impact LLM performance on reasoning tasks, affecting both reasoning and answer accuracy.

**Abstract:** Large Language Models (LLMs) have been evaluated using diverse question types, e.g., multiple-choice, true/false, and short/long answers. This study answers an unexplored question about the impact of different question types on LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on three different types of questions using quantitative and deductive reasoning tasks. The performance metrics include accuracy in the reasoning steps and choosing the final answer. Key Findings: (1) Significant differences exist in LLM performance across different question types. (2) Reasoning accuracy does not necessarily correlate with the final selection accuracy. (3) The number of options and the choice of words, influence LLM performance.

</details>


### [114] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)

*Tian Li, Yujian Sun, Huizhi Liang*

**Main category:** cs.CL

**Keywords:** Emotion Detection, Contrastive Learning, Multilingual NLP

**Relevance Score:** 8

**TL;DR:** This paper details the SemEval-2025 Task 11 on emotion detection across 28 languages, employing contrastive learning methods to improve predictions and ranking well in both classification and intensity prediction tracks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of diversity in emotional expressions and background variations in emotion recognition across multiple languages.

**Method:** Two contrastive learning approaches were explored: a sample-based approach (Contrastive Reasoning Calibration) and a generation-based approach (DPO, SimPO), fine-tuning models from LLaMa3-Instruct-8B for improved predictions.

**Key Contributions:**

	1. Introduction of a multi-language emotion detection competition
	2. Detailed exploration of contrastive learning methods
	3. Demonstrated effective prediction improvements using advanced architectures

**Result:** The proposed system ranked 9th in multi-label classification (Track A) and 6th in emotion intensity prediction (Track B) for English, and performed among the top-tier systems for other languages.

**Limitations:** 

**Conclusion:** The study demonstrates that both contrastive learning approaches can effectively enhance emotion detection across various languages and emotional categories.

**Abstract:** The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection, introduces an emotion recognition challenge spanning over 28 languages. This competition encourages researchers to explore more advanced approaches to address the challenges posed by the diversity of emotional expressions and background variations. It features two tracks: multi-label classification (Track A) and emotion intensity prediction (Track B), covering six emotion categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we systematically explore the benefits of two contrastive learning approaches: sample-based (Contrastive Reasoning Calibration) and generation-based (DPO, SimPO) contrastive learning. The sample-based contrastive approach trains the model by comparing two samples to generate more reliable predictions. The generation-based contrastive approach trains the model to differentiate between correct and incorrect generations, refining its prediction. All models are fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A and 6th place in Track B for English, while ranking among the top-tier performing systems for other languages.

</details>


### [115] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)

*Alina Hyk, Kiera McCormick, Mian Zhong, Ioana Ciucă, Sanjib Sharma, John F Wu, J. E. G. Peek, Kartheik G. Iyer, Ziang Xiao, Anjalie Field*

**Main category:** cs.CL

**Keywords:** LLMs, astronomy, evaluation benchmarks, user studies, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This study explores user evaluations of LLMs in the context of an astronomy literature retrieval application, proposing improvements for benchmarks based on user interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve evaluation procedures of LLMs by understanding how real users assess these models, especially in scientific contexts like astronomy.

**Method:** The study involved inductive coding of 368 queries executed on an LLM-powered retrieval-augmented generation bot deployed via Slack, along with follow-up interviews with 11 astronomers.

**Key Contributions:**

	1. Inductive coding of user queries to understand evaluations of LLMs
	2. Concrete recommendations for building improved LLM benchmarks
	3. A sample benchmark for evaluating LLMs tailored for astronomy

**Result:** The findings highlighted specific types of questions users asked and the criteria they used to evaluate the bot's responses, leading to recommendations for better LLM benchmarks in scientific research.

**Limitations:** 

**Conclusion:** The work emphasizes the importance of understanding user evaluations to enhance the usability and effectiveness of LLMs, particularly in scientific applications.

**Abstract:** There is growing interest in leveraging LLMs to aid in astronomy and other scientific research, but benchmarks for LLM evaluation in general have not kept pace with the increasingly diverse ways that real people evaluate and use these models. In this study, we seek to improve evaluation procedures by building an understanding of how users evaluate LLMs. We focus on a particular use case: an LLM-powered retrieval-augmented generation bot for engaging with astronomical literature, which we deployed via Slack. Our inductive coding of 368 queries to the bot over four weeks and our follow-up interviews with 11 astronomers reveal how humans evaluated this system, including the types of questions asked and the criteria for judging responses. We synthesize our findings into concrete recommendations for building better benchmarks, which we then employ in constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our work offers ways to improve LLM evaluation and ultimately usability, particularly for use in scientific research.

</details>


### [116] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)

*Sahana Srinivasan, Xuguang Ai, Thaddaeus Wai Soon Lo, Aidan Gilson, Minjie Zou, Ke Zou, Hyunjae Kim, Mingjia Yang, Krithi Pushpanathan, Samantha Yew, Wan Ting Loke, Jocelyn Goh, Yibing Chen, Yiming Kong, Emily Yuelei Fu, Michelle Ongyong Hui, Kristen Nwanyanwu, Amisha Dave, Kelvin Zhenghao Li, Chen-Hsin Sun, Mark Chia, Gabriel Dawei Yang, Wendy Meihua Wong, David Ziyou Chen, Dianbo Liu, Maxwell Singer, Fares Antaki, Lucian V Del Priore, Jost Jonas, Ron Adelman, Qingyu Chen, Yih-Chung Tham*

**Main category:** cs.CL

**Keywords:** large language models, ophthalmology, benchmarking, clinical accuracy, reasoning quality

**Relevance Score:** 3

**TL;DR:** BELO is a standardized benchmark for evaluating large language models in ophthalmology, focusing on clinical accuracy and reasoning quality through expert-validated MCQs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM benchmarks in ophthalmology prioritize accuracy but are limited in scope. BELO aims to provide a comprehensive assessment framework.

**Method:** Utilized a fine-tuned PubMedBERT model for keyword matching to create ophthalmology-specific MCQs from various datasets, curated and validated by ophthalmology experts.

**Key Contributions:**

	1. Introduction of a new benchmark specifically for ophthalmology-related LLM evaluation
	2. Involvement of ophthalmology experts in curating high-quality MCQs and explanations
	3. Establishment of a public leaderboard for transparent reporting

**Result:** Six LLMs were evaluated using BELO, demonstrating their performance across various metrics such as accuracy and text-generation quality. A public leaderboard was established for transparency.

**Limitations:** Focuses solely on ophthalmology; may not generalize to other medical fields.

**Conclusion:** The BELO dataset will serve as an evaluation-only benchmark for future LLM comparisons, promoting fair and reproducible assessments in ophthalmology.

**Abstract:** Current benchmarks evaluating large language models (LLMs) in ophthalmology are limited in scope and disproportionately prioritise accuracy. We introduce BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive evaluation benchmark developed through multiple rounds of expert checking by 13 ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset underwent multiple rounds of expert checking. Duplicate and substandard questions were systematically removed. Ten ophthalmologists refined the explanations of each MCQ's correct answer. This was further adjudicated by three senior ophthalmologists. To illustrate BELO's utility, we evaluated six LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro) using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore, BARTScore, METEOR, and AlignScore). In a further evaluation involving human experts, two ophthalmologists qualitatively reviewed 50 randomly selected outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900 high-quality, expert-reviewed questions aggregated from five sources: BCSC (260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public leaderboard has been established to promote transparent evaluation and reporting. Importantly, the BELO dataset will remain a hold-out, evaluation-only benchmark to ensure fair and reproducible comparisons of future models.

</details>


### [117] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)

*Yuanhao Shen, Daniel Xavier de Sousa, Ricardo Marçal, Ali Asad, Hongyu Guo, Xiaodan Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, interdisciplinary research, benchmark, IDRBench, scientific discovery

**Relevance Score:** 9

**TL;DR:** Introduction of IDRBench, a benchmark for evaluating LLMs' capabilities in interdisciplinary research idea generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the lack of a dedicated benchmark evaluating LLMs in interdisciplinary research and to understand their strengths and limitations.

**Method:** Development of IDRBench featuring an expert annotated dataset and structured tasks to assess LLMs in proposing interdisciplinary research ideas.

**Key Contributions:**

	1. Creation of the IDRBench benchmark for LLMs in interdisciplinary research
	2. Provision of an expert annotated dataset from diverse scientific domains
	3. Development of a systematic framework for assessing LLM performance in IDR

**Result:** Evaluation of 10 LLMs using IDRBench shows that while LLMs are aware of interdisciplinary research, they still struggle to generate quality ideas.

**Limitations:** Current LLMs show limited ability to produce quality interdisciplinary research ideas.

**Conclusion:** IDRBench can spur new research directions and aid in developing advanced LLMs for interdisciplinary applications.

**Abstract:** Recent advancements in Large Language Models (LLMs) have revealed their impressive ability to perform multi-step, logic-driven reasoning across complex domains, positioning them as powerful tools and collaborators in scientific discovery while challenging the long-held view that inspiration-driven ideation is uniquely human. However, the lack of a dedicated benchmark that evaluates LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings poses a critical barrier to fully understanding their strengths and limitations. To address this gap, we introduce IDRBench -- a pioneering benchmark featuring an expert annotated dataset and a suite of tasks tailored to evaluate LLMs' capabilities in proposing valuable research ideas from different scientific domains for interdisciplinary research. This benchmark aims to provide a systematic framework for assessing LLM performance in complex, cross-domain scientific research. Our dataset consists of scientific publications sourced from the ArXiv platform covering six distinct disciplines, and is annotated by domain experts with diverse academic backgrounds. To ensure high-quality annotations, we emphasize clearly defined dimensions that characterize authentic interdisciplinary research. The design of evaluation tasks in IDRBench follows a progressive, real-world perspective, reflecting the natural stages of interdisciplinary research development, including 1) IDR Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation. Using IDRBench, we construct baselines across 10 LLMs and observe that despite fostering some level of IDR awareness, LLMs still struggle to produce quality IDR ideas. These findings could not only spark new research directions, but also help to develop next-generation LLMs that excel in interdisciplinary research.

</details>


### [118] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)

*Paul Sheridan, Zeyad Ahmed, Aitazaz A. Farooque*

**Main category:** cs.CL

**Keywords:** TF-IDF, Fisher's exact test, significance testing, term-weighting schemes, information retrieval

**Relevance Score:** 4

**TL;DR:** The paper provides a statistical justification for TF-IDF, showing its connection to significance testing and Fisher's exact test, asserting its effectiveness as a term-weighting scheme in text analysis.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To build a theoretical foundation for TF-IDF and justify its use in the statistics community by relating it to significance testing.

**Method:** The authors demonstrate that the variant TF-ICF relates closely to the negative logarithm of the p-value from Fisher's exact test under certain conditions, establishing a connection to TF-IDF.

**Key Contributions:**

	1. Establishes a statistical foundation for TF-IDF and its variants.
	2. Connects TF-IDF to Fisher's exact test and significance testing.
	3. Demonstrates convergence of TF-IDF with increasing document collection size.

**Result:** The study reveals that as document collections grow infinitely, the negative log-transformed p-value converges to TF-IDF, providing a statistical rationale for its effectiveness.

**Limitations:** 

**Conclusion:** The justification of TF-IDF using Fisher's exact test offers statisticians a framework for understanding the effectiveness of TF-IDF in information retrieval.

**Abstract:** Term frequency-inverse document frequency, or TF-IDF for short, is arguably the most celebrated mathematical expression in the history of information retrieval. Conceived as a simple heuristic quantifying the extent to which a given term's occurrences are concentrated in any one given document out of many, TF-IDF and its many variants are routinely used as term-weighting schemes in diverse text analysis applications. There is a growing body of scholarship dedicated to placing TF-IDF on a sound theoretical foundation. Building on that tradition, this paper justifies the use of TF-IDF to the statistics community by demonstrating how the famed expression can be understood from a significance testing perspective. We show that the common TF-IDF variant TF-ICF is, under mild regularity conditions, closely related to the negative logarithm of the $p$-value from a one-tailed version of Fisher's exact test of statistical significance. As a corollary, we establish a connection between TF-IDF and the said negative log-transformed $p$-value under certain idealized assumptions. We further demonstrate, as a limiting case, that this same quantity converges to TF-IDF in the limit of an infinitely large document collection. The Fisher's exact test justification of TF-IDF equips the working statistician with a ready explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [119] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)

*Ruizhe Zhu, Hao Zhu, Yaxuan Li, Syang Zhou, Shijing Cai, Malgorzata Lazuka, Elliott Ash*

**Main category:** cs.CL

**Keywords:** dialogue generation, conversational AI, LLM, fine-tuning, human-chatbot interactions

**Relevance Score:** 8

**TL;DR:** DialogueForge is a framework designed to generate AI-simulated conversations in a human-chatbot style, using seed prompts from real interactions and testing various LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of manual effort in collecting human-chatbot dialogues, which limits research on conversational AI.

**Method:** The framework utilizes seed prompts from real conversations to generate multi-turn dialogues. Various LLMs are tested to simulate user interactions, and fine-tuning techniques are explored for improving smaller models.

**Key Contributions:**

	1. Introduction of DialogueForge framework for generating AI-simulated conversations.
	2. Evaluation of various LLMs for dialogue generation and the impact of fine-tuning on smaller models.
	3. Comparison of performance between proprietary and open-source models in generating human-like dialogues.

**Result:** Experiments show that while large proprietary LLMs like GPT-4o outperform others in realism, smaller models can be fine-tuned for better performance and customization.

**Limitations:** Maintaining coherent and natural long-form dialogues is a persistent challenge across all models tested.

**Conclusion:** Despite advancements, all models struggle with maintaining coherent and natural long-form conversations, indicating an ongoing challenge in conversational AI research.

**Abstract:** Collecting human-chatbot dialogues typically demands substantial manual effort and is time-consuming, which limits and poses challenges for research on conversational AI. In this work, we propose DialogueForge - a framework for generating AI-simulated conversations in human-chatbot style. To initialize each generated conversation, DialogueForge uses seed prompts extracted from real human-chatbot interactions. We test a variety of LLMs to simulate the human chatbot user, ranging from state-of-the-art proprietary models to small-scale open-source LLMs, and generate multi-turn dialogues tailored to specific tasks. In addition, we explore fine-tuning techniques to enhance the ability of smaller models to produce indistinguishable human-like dialogues. We evaluate the quality of the simulated conversations and compare different models using the UniEval and GTEval evaluation protocols. Our experiments show that large proprietary models (e.g., GPT-4o) generally outperform others in generating more realistic dialogues, while smaller open-source models (e.g., Llama, Mistral) offer promising performance with greater customization. We demonstrate that the performance of smaller models can be significantly improved by employing supervised fine-tuning techniques. Nevertheless, maintaining coherent and natural long-form human-like dialogues remains a common challenge across all models.

</details>


### [120] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)

*Lyumanshan Ye, Xiaojie Cai, Xinkai Wang, Junfei Wang, Xiangkun Hu, Jiadi Su, Yang Nan, Sihan Wang, Bohan Zhang, Xiaoze Fan, Jinbin Luo, Yuxiang Zheng, Tianze Xu, Dayuan Fu, Yunze Wu, Pengrui Lu, Zengzhi Wang, Yiwei Qin, Zhen Huang, Yan Ma, Zhulin Hu, Haoyang Zou, Tiantian Mi, Yixin Ye, Ethan Chern, Pengfei Liu*

**Main category:** cs.CL

**Keywords:** Human-AI Interaction, Deep Cognition, Cognitive Oversight, Research Tasks, AI Transparency

**Relevance Score:** 9

**TL;DR:** This paper presents the "Interaction as Intelligence" framework, which reconceptualizes human-AI relationships in deep research tasks by proposing that interaction is a key aspect of intelligence.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To shift from viewing interaction as a simple interface to recognizing it as a fundamental aspect of deep learning systems in research tasks.

**Method:** The authors propose a system called Deep Cognition that allows for transparent, controllable, and interruptible interactions with AI, enhancing the human role from merely giving instructions to providing cognitive oversight.

**Key Contributions:**

	1. Introduction of the Deep Cognition framework for human-AI interaction.
	2. Demonstrated empirical improvements in multiple performance metrics.
	3. Establishment of cognitive oversight as a new role for users in AI engagement.

**Result:** User evaluations show that the Deep Cognition system significantly improves interaction metrics such as Transparency, Fine-Grained Interaction, and Ease of Collaboration, with enhancements ranging from 18.5% to 50.0% over traditional systems.

**Limitations:** 

**Conclusion:** The findings indicate that reimagining human-AI interaction as cognitive oversight during research tasks leads to better performance and user satisfaction.

**Abstract:** This paper introduces "Interaction as Intelligence" research series, presenting a reconceptualization of human-AI relationships in deep research tasks. Traditional approaches treat interaction merely as an interface for accessing AI capabilities-a conduit between human intent and machine output. We propose that interaction itself constitutes a fundamental dimension of intelligence. As AI systems engage in extended thinking processes for research tasks, meaningful interaction transitions from an optional enhancement to an essential component of effective intelligence. Current deep research systems adopt an "input-wait-output" paradigm where users initiate queries and receive results after black-box processing. This approach leads to error cascade effects, inflexible research boundaries that prevent question refinement during investigation, and missed opportunities for expertise integration. To address these limitations, we introduce Deep Cognition, a system that transforms the human role from giving instructions to cognitive oversight-a mode of engagement where humans guide AI thinking processes through strategic intervention at critical junctures. Deep cognition implements three key innovations: (1)Transparent, controllable, and interruptible interaction that reveals AI reasoning and enables intervention at any point; (2)Fine-grained bidirectional dialogue; and (3)Shared cognitive context where the system observes and adapts to user behaviors without explicit instruction. User evaluation demonstrates that this cognitive oversight paradigm outperforms the strongest baseline across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%), Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%), Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on challenging research problems show 31.8% to 50.0% points of improvements over deep research systems.

</details>


### [121] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)

*Andrei-Valentin Tanase, Elena Pelican*

**Main category:** cs.CL

**Keywords:** decoder-only transformer, tokenization, architectural design

**Relevance Score:** 7

**TL;DR:** Supernova is a 650M-parameter decoder-only transformer that achieves performance comparable to larger models with fewer parameters through architectural innovations and a custom tokenizer.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To demonstrate that effective architectural design and innovative tokenization can enable smaller models to match the performance of larger ones while maintaining efficiency.

**Method:** The architecture employs Rotary Positional Embeddings (RoPE), Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for efficiency, and SwiGLU activation functions, alongside a custom 128,000-vocabulary byte-level BPE tokenizer.

**Key Contributions:**

	1. Introduction of Supernova, an efficient 650M-parameter model
	2. Innovative tokenization methods achieving superior performance
	3. Demonstrated architectural efficiency with fewer parameters

**Result:** Supernova achieves 90% of the performance of a 1B-parameter model while using 53% fewer parameters and only requiring 100B training tokens.

**Limitations:** 

**Conclusion:** The findings challenge prevailing beliefs about model scaling, showing architectural efficiency and tokenization quality can offset lower parameter counts.

**Abstract:** We present Supernova, a 650M-parameter decoder-only transformer that demonstrates how careful architectural design and tokenization innovation can achieve the performance of larger models while maintaining computational efficiency. Our architecture combines Rotary Positional Embeddings (RoPE), Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for computational efficiency, and SwiGLU activation functions. A critical innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which achieves state-of-the-art compression performance. Through detailed analysis, we show that Supernova achieves 90% of the performance of 1B-parameter models while using 53% fewer parameters and requiring only 100B training tokens--an order of magnitude less than competing models. Our findings challenge the prevailing scaling paradigm, demonstrating that architectural efficiency and tokenization quality can compensate for reduced parameter counts.

</details>


### [122] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)

*Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, Guorui Zhou*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Reasoning, Entropy-Aware, Dual-Token Constraints

**Relevance Score:** 9

**TL;DR:** Archer is a novel entropy-aware RLVR method that improves reasoning in LLMs by applying dual-token constraints and synchronous updates, outperforming previous methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Previous RLVR methods applied uniform training signals to all tokens, ignoring the different roles of knowledge-related and reasoning-related tokens, which hindered effective learning.

**Method:** Archer employs dual-token constraints with different KL regularizations for knowledge and reasoning tokens and uses synchronous updates to maintain semantic dependencies.

**Key Contributions:**

	1. Introduction of dual-token constraints in RLVR
	2. Implementation of entropy-aware updates
	3. Demonstrated state-of-the-art performance on benchmarks

**Result:** Experimental results show that Archer significantly outperforms previous RLVR methods on mathematical reasoning and code generation benchmarks, achieving state-of-the-art performance for comparable models.

**Limitations:** 

**Conclusion:** Archer effectively enhances reasoning abilities in LLMs while maintaining factual knowledge, positioning it as a superior post-training method for LLMs.

**Abstract:** Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.

</details>


### [123] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)

*Felix Köster, Atsushi Uchida*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reservoir Computing, Natural Language Processing, Transformers, Efficient Computing

**Relevance Score:** 7

**TL;DR:** This paper investigates reservoir computing for natural text processing as an alternative to large language models, comparing its performance, computational cost, and accuracy with transformer architectures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the energy demands and slow processing of large language models (LLMs) and make models accessible while improving quality.

**Method:** The study compares three approaches for character-level language modeling: two reservoir computing methods (with a trainable output layer) and transformer-based architectures. It evaluates prediction accuracy and computational efficiency by controlling the number of trainable parameters.

**Key Contributions:**

	1. Comparison of reservoir computing and transformer models for language modeling
	2. Introduction of an attention-enhanced reservoir computing model
	3. Guidelines for balancing efficiency and performance in language processing tasks.

**Result:** Transformers demonstrate superior prediction quality, while reservoir computing methods are more efficient, reducing training and inference times. The study presents a traditional reservoir with a static output and an attention-enhanced reservoir that adapts its output weights dynamically.

**Limitations:** Limited research on reservoir computing in language modeling; effectiveness depends on specific applications and contexts.

**Conclusion:** The research suggests a balance between resource constraints and performance, showing that reservoir computing can be a viable alternative for efficient natural language processing.

**Abstract:** Large Language Models (LLM) have dominated the science and media landscape duo to their impressive performance on processing large chunks of data and produce human-like levels of text. Nevertheless, their huge energy demand and slow processing still a bottleneck for further increasing quality while also making the models accessible to everyone. To solve this bottleneck, we will investigate how reservoir computing performs on natural text processing, which could enable fast and energy efficient hardware implementations. Studies investigating the use of reservoir computing as a language model remain sparse. In this paper, we compare three distinct approaches for character-level language modeling, two different reservoir computing approaches, where only an output layer is trainable, and the well-known transformer-based architectures, which fully learn an attention-based sequence representation. We explore the performance, computational cost and prediction accuracy for both paradigms by equally varying the number of trainable parameters for all models. Using a consistent pipeline for all three approaches, we demonstrate that transformers excel in prediction quality, whereas reservoir computers remain highly efficient reducing the training and inference speed. Furthermore, we investigate two types of reservoir computing: a traditional reservoir with a static linear readout, and an attention-enhanced reservoir that dynamically adapts its output weights via an attention mechanism. Our findings underline how these paradigms scale and offer guidelines to balance resource constraints with performance.

</details>


### [124] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)

*Anton Abilov, Ke Zhang, Hemank Lamba, Elizabeth M. Olson, Joel R. Tetreault, Alejandro Jaimes*

**Main category:** cs.CL

**Keywords:** AI for Good, humanitarian collaboration, model deployment, resource-constrained environments, continuous performance updates

**Relevance Score:** 6

**TL;DR:** This paper discusses the collaboration and deployment of AI models in resource-constrained environments for humanitarian purposes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to address the gap in literature regarding the deployment and collaboration processes in AI for Good initiatives.

**Method:** The authors detail a case study of collaboration with a humanitarian organization to deploy and maintain an AI model.

**Key Contributions:**

	1. Insights on collaboration with humanitarian organizations
	2. Strategies for deploying AI in resource-constrained environments
	3. Recommendations for continuous performance maintenance

**Result:** The study provides insights into the processes and challenges of deploying AI solutions in humanitarian contexts, emphasizing continuous performance updates.

**Limitations:** 

**Conclusion:** The paper concludes that effective collaboration and maintenance are crucial for the successful implementation of AI in resource-limited settings.

**Abstract:** Publications in the AI for Good space have tended to focus on the research and model development that can support high-impact applications. However, very few AI for Good papers discuss the process of deploying and collaborating with the partner organization, and the resulting real-world impact. In this work, we share details about the close collaboration with a humanitarian-to-humanitarian (H2H) organization and how to not only deploy the AI model in a resource-constrained environment, but also how to maintain it for continuous performance updates, and share key takeaways for practitioners.

</details>


### [125] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)

*Yihao Li, Jiayi Xin, Miranda Muqing Miao, Qi Long, Lyle Ungar*

**Main category:** cs.CL

**Keywords:** language mixing, bilingual models, reasoning, reinforcement learning, multilingual training

**Relevance Score:** 8

**TL;DR:** This paper studies language mixing in bilingual large language models, demonstrating that strategic language switching can enhance reasoning accuracy in math tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how intentional language switching affects reasoning in bilingual large language models and to explore methods to leverage this behavior for improved performance.

**Method:** The study focuses on Chinese-English bilingual reasoning models. It identifies reinforcement learning with verifiable rewards (RLVR) as a key factor enabling language mixing and tests its impact on reasoning accuracy.

**Key Contributions:**

	1. Identification of RLVR as a facilitator of language mixing in models.
	2. Demonstration of the accuracy benefits of language mixing in reasoning tasks.
	3. Development of a lightweight probe to predict the impact of language switching on reasoning accuracy.

**Result:** Language mixing was found to enhance reasoning performance, with a 5.6 percentage point accuracy loss when enforcing monolingual decoding, and a 6.25 percentage point increase in accuracy when using a probe to guide decoding based on language switch predictions.

**Limitations:** 

**Conclusion:** Language mixing in multilingual settings is beneficial for reasoning tasks, indicating it is a strategic behavior rather than just a byproduct of training.

**Abstract:** Proficient multilingual speakers often intentionally switch languages in the middle of a conversation. Similarly, recent reasoning-focused bilingual large language models (LLMs) with strong capabilities in both languages exhibit language mixing--alternating languages within their chain of thought. Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy, suggesting that language mixing may benefit reasoning. In this work, we study language switching in Chinese-English bilingual reasoning models. We identify reinforcement learning with verifiable rewards (RLVR) as the critical training stage that leads to language mixing. We demonstrate that language mixing can enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage points on math reasoning tasks. Additionally, a lightweight probe can be trained to predict whether a potential language switch would benefit or harm reasoning, and when used to guide decoding, increases accuracy by up to 6.25 percentage points. Our findings suggest that language mixing is not merely a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [126] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)

*Basma El Amel Boussaha, Leen AlQadi, Mugariya Farooq, Shaikha Alsuwaidi, Giulia Campesan, Ahmed Alzubaidi, Mohammed Alyafeai, Hakim Hacid*

**Main category:** cs.CL

**Keywords:** Arabic, Large Language Models, STEM, code generation, benchmarks

**Relevance Score:** 9

**TL;DR:** This paper presents 3LM, a suite of three Arabic benchmarks for LLM evaluation in STEM and coding, addressing a gap in Arabic LLM research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited development of Arabic language models, particularly in STEM and coding domains, which are often overlooked in existing benchmarks.

**Method:** The paper introduces three benchmarks: 1) STEM-related question-answer pairs from educational materials, 2) synthetically generated STEM questions, and 3) a code generation benchmark, created through translation of existing benchmarks with human review.

**Key Contributions:**

	1. Introduction of a unique set of benchmarks for Arabic LLMs in STEM and coding.
	2. Use of both naturally sourced and synthetically generated questions to enhance validity.
	3. Public release of datasets to support further Arabic LLM research.

**Result:** The benchmarks aim to facilitate research and development of Arabic LLMs in the STEM fields and coding, which are crucial for real-world applications.

**Limitations:** Focused primarily on STEM and code without covering other potential domains for Arabic LLMs.

**Conclusion:** By providing these resources, the paper seeks to promote the growth of high-quality Arabic LLM research in underrepresented areas.

**Abstract:** Arabic is one of the most widely spoken languages in the world, yet efforts to develop and evaluate Large Language Models (LLMs) for Arabic remain relatively limited. Most existing Arabic benchmarks focus on linguistic, cultural, or religious content, leaving a significant gap in domains like STEM and code which are increasingly relevant for real-world LLM applications. To help bridge this gap, we present 3LM, a suite of three benchmarks designed specifically for Arabic. The first is a set of STEM-related question-answer pairs, naturally sourced from Arabic textbooks and educational worksheets. The second consists of synthetically generated STEM questions, created using the same sources. The third benchmark focuses on code generation, built through a careful translation of two widely used code benchmarks, incorporating a human-in-the-loop process with several rounds of review to ensure high-quality and faithful translations. We release all three benchmarks publicly to support the growth of Arabic LLM research in these essential but underrepresented areas.

</details>


### [127] [Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages](https://arxiv.org/abs/2303.09823)

*Angel Felipe Magnossão de Paula, Imene Bensalem, Paolo Rosso, Wajdi Zaghouani*

**Main category:** cs.CL

**Keywords:** hate speech detection, transformer models, ensemble methods, NLP challenge, machine learning

**Relevance Score:** 4

**TL;DR:** The paper presents experimental results on hate speech detection using transformer models and ensemble approaches.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To contribute to the hate speech detection task within the CERIST NLP Challenge 2022 and explore effective model combinations.

**Method:** Evaluated six transformer models and two ensemble approaches, with a majority vote strategy yielding the best results in a five-fold cross-validation setup.

**Key Contributions:**

	1. Evaluation of multiple transformer models for hate speech detection.
	2. Implementation of ensemble methods to improve detection performance.
	3. Contribution to a competitive shared task in NLP.

**Result:** Achieved an F1-score of 0.60 and an accuracy of 0.86 on the test set using the majority vote ensemble approach.

**Limitations:** Results may be limited by the nature of the datasets and specific characteristics of the transformer models used.

**Conclusion:** The majority vote ensemble approach is effective for hate speech detection, demonstrating promising performance with the evaluated transformer models.

**Abstract:** This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.

</details>


### [128] [Where Do People Tell Stories Online? Story Detection Across Online Communities](https://arxiv.org/abs/2311.09675)

*Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper*

**Main category:** cs.CL

**Keywords:** story detection, social media, online communities, Reddit, narratology

**Relevance Score:** 4

**TL;DR:** The paper introduces the StorySeeker toolkit, aimed at detecting storytelling in online communities by providing an annotated dataset of Reddit posts and comments, alongside models for story detection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting stories in online communities is difficult due to the intertwining of storytelling elements with non-story content in texts.

**Method:** The StorySeeker toolkit comprises an annotated dataset from Reddit, including a codebook for social media, and models for predicting storytelling at different levels.

**Key Contributions:**

	1. Release of the StorySeeker toolkit and annotated dataset of 502 Reddit posts.
	2. Identification of distinctive textual features for storytelling detection.
	3. Case study on storytelling as a persuasive strategy in the r/ChangeMyView community.

**Result:** The evaluation of various detection methods revealed unique textual features of online storytelling, and a case study demonstrated the practical application of the toolkit.

**Limitations:** 

**Conclusion:** The findings and tools provided can enhance research on narratology and the dynamics of online communities.

**Abstract:** Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span levels. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling spans. We illuminate distributional characteristics of storytelling on a large community-centric social media platform, and we also conduct a case study on r/ChangeMyView, where storytelling is used as one of many persuasive strategies, illustrating that our data and models can be used for both inter- and intra-community research. Finally, we discuss implications of our tools and analyses for narratology and the study of online communities.

</details>


### [129] [A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models](https://arxiv.org/abs/2311.16789)

*Hongru Wang, Lingzhi Wang, Yiming Du, Liang Chen, Jingyan Zhou, Yufei Wang, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Dialogue Systems, Language Models, Natural Language Processing, Pre-trained Language Models, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper surveys the evolution of dialogue systems in relation to advancements in language models, particularly pre-trained and large language models, exploring state-of-the-art outcomes, emerging topics, and future challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the interplay and evolution between dialogue systems and language model breakthroughs, providing insights for future research directions.

**Method:** The paper systematically categorizes the history of dialogue systems aligned with significant advancements in language models in chronological order, reviewing state-of-the-art research outcomes.

**Key Contributions:**

	1. Chronological categorization of dialogue system evolution
	2. Review of state-of-the-art research outcomes
	3. Insights into future challenges and directions in LLM-based dialogue systems.

**Result:** A comprehensive overview of the evolution of dialogue systems and their relationship with language models, along with discussions on emerging topics and open challenges in the field.

**Limitations:** 

**Conclusion:** The survey helps to understand the dynamic relationship between language models and dialogue systems, paving the way for future advancements in LLM-based dialogue systems.

**Abstract:** Dialogue systems (DS), including the task-oriented dialogue system (TOD) and the open-domain dialogue system (ODD), have always been a fundamental task in natural language processing (NLP), allowing various applications in practice. Owing to sophisticated training and well-designed model architecture, language models (LM) are usually adopted as the necessary backbone to build the dialogue system. Consequently, every breakthrough in LM brings about a shift in learning paradigm and research attention within dialogue system, especially the appearance of pre-trained language models (PLMs) and large language models (LLMs). In this paper, we take a deep look at the history of the dialogue system, especially its special relationship with the advancements of language models. Thus, our survey offers a systematic perspective, categorizing different stages in a chronological order aligned with LM breakthroughs, providing a comprehensive review of state-of-the-art research outcomes. What's more, we turn our attention to emerging topics and engage in a discussion on open challenges, providing valuable insights into the future directions for LLM-based dialogue systems. In summary, this survey delves into the dynamic interplay between language models and dialogue systems, unraveling the evolutionary path of this essential relationship. Through this exploration, we pave the way for a deeper comprehension of the field, guiding future developments in LM-based dialogue systems.

</details>


### [130] [End-to-end Joint Punctuated and Normalized ASR with a Limited Amount of Punctuated Training Data](https://arxiv.org/abs/2311.17741)

*Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, punctuation, language model, error rate reduction, training data

**Relevance Score:** 5

**TL;DR:** This paper presents two novel approaches for training a joint punctuated and normalized ASR system, yielding significant reductions in error rates with limited punctuated data utilization.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in automatic speech recognition (ASR) due to scarcity of paired speech and punctuated text data.

**Method:** The first approach leverages a language model to create punctuated transcripts from normalized training data. The second method involves a single decoder that conditions the output type, enhancing performance with limited training data.

**Key Contributions:**

	1. Two approaches for joint ASR with limited punctuated data
	2. 17% and 42% relative reductions in error rates
	3. Feasibility of using only 5% punctuated training data

**Result:** The first approach results in a 17% relative reduction in Punctuation-Case-aware Word Error Rate (PC-WER) on out-of-domain data. The second method achieves a 42% relative PC-WER reduction compared to Whisper-base.

**Limitations:** 

**Conclusion:** The proposed methods enhance the performance of joint ASR systems significantly, demonstrating effective utilization of minimal punctuated training data.

**Abstract:** Joint punctuated and normalized automatic speech recognition (ASR) aims at outputing transcripts with and without punctuation and casing. This task remains challenging due to the lack of paired speech and punctuated text data in most ASR corpora. We propose two approaches to train an end-to-end joint punctuated and normalized ASR system using limited punctuated data. The first approach uses a language model to convert normalized training transcripts into punctuated transcripts. This achieves a better performance on out-of-domain test data, with up to 17% relative Punctuation-Case-aware Word Error Rate (PC-WER) reduction. The second approach uses a single decoder conditioned on the type of output. This yields a 42% relative PC-WER reduction compared to Whisper-base and a 4% relative (normalized) WER reduction compared to the normalized output of a punctuated-only model. Additionally, our proposed model demonstrates the feasibility of a joint ASR system using as little as 5% punctuated training data with a moderate (2.42% absolute) PC-WER increase.

</details>


### [131] [VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension](https://arxiv.org/abs/2402.02655)

*Thinh Phuoc Ngo, Khoa Tran Anh Dang, Son T. Luu, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

**Main category:** cs.CL

**Keywords:** Vietnamese, spoken language, machine reading comprehension, deep learning, VlogQA

**Relevance Score:** 6

**TL;DR:** Development of a Vietnamese spoken language corpus for machine reading comprehension, addressing challenges in using real-world data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in Vietnamese machine reading comprehension corpora, which largely focus on formal texts, by creating a resource based on spoken language from YouTube.

**Method:** The study presents the VlogQA dataset comprising 10,076 question-answer pairs derived from 1,230 YouTube transcript documents, evaluating deep learning models on this corpus.

**Key Contributions:**

	1. Creation of the VlogQA dataset for Vietnamese spoken language
	2. Demonstrated performance of deep learning models on a new corpus
	3. Highlighted challenges in processing spoken language data for MRC

**Result:** Achieved an F1 score of 75.34% and an EM score of 53.97% on the test set, showcasing improvements in MRC for Vietnamese spoken content.

**Limitations:** Challenges remain in processing spoken-based content effectively; further improvement is needed.

**Conclusion:** The VlogQA corpus is a significant step forward in MRC tasks for Vietnamese spoken language and indicates the need for continued research in this area.

**Abstract:** This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicating significant progress in machine reading comprehension for Vietnamese spoken language data. In terms of EM, the highest score we accomplished is 53.97%, which reflects the challenge in processing spoken-based content and highlights the need for further improvement.

</details>


### [132] [Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles](https://arxiv.org/abs/2406.12644)

*Devichand Budagam, Ashutosh Kumar, Mahsa Khoshnoodi, Sankalp KJ, Vinija Jain, Aman Chadha*

**Main category:** cs.CL

**Keywords:** large language models, Hierarchical Prompting Taxonomy, cognitive demands, task complexity, prompting strategies

**Relevance Score:** 9

**TL;DR:** This paper presents the Hierarchical Prompting Taxonomy (HPT) to assess the cognitive demands of large language models (LLMs) on various tasks, demonstrating significant performance improvements through structured prompting strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the strengths and weaknesses of large language models (LLMs) through effective assessment of their task performance is essential.

**Method:** The paper introduces a Hierarchical Prompting Taxonomy (HPT) grounded in human cognitive principles, utilizing the Hierarchical Prompting Framework (HPF) to evaluate cognitive demands through structured prompting strategies.

**Key Contributions:**

	1. Development of the Hierarchical Prompting Taxonomy (HPT)
	2. Introduction of the Hierarchical Prompting Framework (HPF)
	3. Creation of the Hierarchical Prompting Index (HPI) for evaluating task complexity

**Result:** Experiments showed that HPF enhances LLM performance by 2% to 63% across diverse datasets, confirming the effectiveness of the HPT, with GSM8k identified as the most cognitively complex task.

**Limitations:** 

**Conclusion:** The HPT offers a standardized metric for evaluating task complexity in LLMs, supporting future research and reproducibility in assessing LLM performance.

**Abstract:** Assessing the effectiveness of large language models (LLMs) in performing different tasks is crucial for understanding their strengths and weaknesses. This paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human cognitive principles and designed to assess LLMs by examining the cognitive demands of various tasks. The HPT utilizes the Hierarchical Prompting Framework (HPF), which structures five unique prompting strategies in a hierarchical order based on their cognitive requirement on LLMs when compared to human mental capabilities. It assesses the complexity of tasks with the Hierarchical Prompting Index (HPI), which demonstrates the cognitive competencies of LLMs across diverse datasets and offers insights into the cognitive demands that datasets place on different LLMs. This approach enables a comprehensive evaluation of an LLMs problem solving abilities and the intricacy of a dataset, offering a standardized metric for task complexity. Extensive experiments with multiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63% compared to baseline performance, with GSM8k being the most cognitively complex task among reasoning and coding tasks with an average HPI of 3.20 confirming the effectiveness of HPT. To support future research and reproducibility in this domain, the implementations of HPT and HPF are available here.

</details>


### [133] [Towards the Next Frontier in Speech Representation Learning Using Disentanglement](https://arxiv.org/abs/2407.02543)

*Varun Krishna, Sriram Ganapathy*

**Main category:** cs.CL

**Keywords:** self-supervised learning, speech representation, disentangled representations, frame-level encoding, contrastive learning

**Relevance Score:** 5

**TL;DR:** A new self-supervised framework, Learn2Diss, is proposed for learning disentangled representations of speech, focusing on both frame-level and utterance-level encoding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing self-supervised frameworks for speech learning mainly focus on frame-level masked predictions, overlooking coarser characteristics like speaker or channel features.

**Method:** Learn2Diss employs two encoder modules: a frame-level model inspired by self-supervision for pseudo-phonemic representations and an utterance-level model using contrastive learning for pseudo-speaker representations, trained jointly to disentangle these two aspects.

**Key Contributions:**

	1. Introduction of the Learn2Diss framework for disentangled speech representations
	2. Joint learning mechanism for frame-level and utterance-level encoders
	3. Achieving state-of-the-art results on various speech representation tasks

**Result:** Learn2Diss achieves state-of-the-art results across various downstream tasks, with frame-level encoder improving semantic tasks and utterance-level representations enhancing non-semantic tasks.

**Limitations:** The results reported may be invalid due to bugs in the code used for the experiments.

**Conclusion:** The framework enhances the understanding and performance of speech representation learning by effectively disentangling frame-level and utterance-level information, despite reported bugs affecting the validity of the results.

**Abstract:** The popular frameworks for self-supervised learning of speech representations have largely focused on frame-level masked prediction of speech regions. While this has shown promising downstream task performance for speech recognition and related tasks, this has largely ignored factors of speech that are encoded at coarser level, like characteristics of the speaker or channel that remain consistent through-out a speech utterance. In this work, we propose a framework for Learning Disentangled Self Supervised (termed as Learn2Diss) representations of speech, which consists of frame-level and an utterance-level encoder modules. The two encoders are initially learned independently, where the frame-level model is largely inspired by existing self supervision techniques, thereby learning pseudo-phonemic representations, while the utterance-level encoder is inspired by constrastive learning of pooled embeddings, thereby learning pseudo-speaker representations. The joint learning of these two modules consists of disentangling the two encoders using a mutual information based criterion. With several downstream evaluation experiments, we show that the proposed Learn2Diss achieves state-of-the-art results on a variety of tasks, with the frame-level encoder representations improving semantic tasks, while the utterance-level representations improve non-semantic tasks.

</details>


### [134] [Why Does New Knowledge Create Messy Ripple Effects in LLMs?](https://arxiv.org/abs/2407.12828)

*Jiaxin Qin, Zixuan Zhang, Manling Li, Pengfei Yu, Heng Ji*

**Main category:** cs.CL

**Keywords:** knowledge editing, language models, ripple effects, GradSim, gradient similarity

**Relevance Score:** 8

**TL;DR:** The paper investigates why knowledge editing (KE) methods in language models (LMs) cause ripple effects and introduces GradSim, a metric indicating when these ripples occur.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and improve the accuracy of post-training knowledge editing in language models by addressing the issue of ripple effects in knowledge representation.

**Method:** The authors analyze the correlation between gradient similarity (GradSim) and ripple effect performance in LMs, using extensive evaluation across various LMs and KE methods.

**Key Contributions:**

	1. Identification of GradSim as a salient indicator for ripple effects in LMs
	2. Analysis of correlation between GradSim and ripple effect performance
	3. Investigation of counter-intuitive failure cases in ripple effects and their relationship with GradSim.

**Result:** The study finds a strong positive correlation between GradSim and ripple effect performance, suggesting that GradSim is a valuable metric for assessing knowledge accuracy in LMs.

**Limitations:** 

**Conclusion:** GradSim serves as an effective indicator of when and why knowledge ripples occur in language models, helping to identify potential issues with KE methods.

**Abstract:** Extensive previous research has focused on post-training knowledge editing (KE) for language models (LMs) to ensure that knowledge remains accurate and up-to-date. One desired property and open question in KE is to let edited LMs correctly handle ripple effects, where LM is expected to answer its logically related knowledge accurately. In this paper, we answer the question of why most KE methods still create messy ripple effects. We conduct extensive analysis and identify a salient indicator, GradSim, that effectively reveals when and why updated knowledge ripples in LMs. GradSim is computed by the cosine similarity between gradients of the original fact and its related knowledge. We observe a strong positive correlation between ripple effect performance and GradSim across different LMs, KE methods, and evaluation metrics. Further investigations into three counter-intuitive failure cases (Negation, Over-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures are often associated with very low GradSim. This finding validates that GradSim is an effective indicator of when knowledge ripples in LMs.

</details>


### [135] [Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language](https://arxiv.org/abs/2409.00061)

*Arief Purnama Muharram, Ayu Purwarianti*

**Main category:** cs.CL

**Keywords:** automated fact-checking, Natural Language Inference, Knowledge Graphs, COVID-19, Indonesian language

**Relevance Score:** 5

**TL;DR:** This study proposes a model integrating Knowledge Graphs (KG) for enhancing Natural Language Inference (NLI) performance in automated COVID-19 fact-checking for the Indonesian language, achieving significant accuracy improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address COVID-19 misinformation online using automated fact-checking techniques while overcoming challenges in deep learning due to knowledge limitations during training.

**Method:** The proposed architecture includes three modules: a fact module for processing information from a Knowledge Graph, an NLI module for assessing semantic relationships, and a classifier module that combines outputs from the first two to determine truthfulness.

**Key Contributions:**

	1. Introduction of a KG-based approach to enhance NLI for fact-checking
	2. Development of a novel model architecture comprising fact, NLI, and classifier modules
	3. Demonstration of superior accuracy in automated COVID-19 fact-checking in the Indonesian language

**Result:** Incorporating Knowledge Graphs into the NLI process significantly improved performance, achieving an accuracy of 0.8616 on the Indonesian COVID-19 fact-checking dataset.

**Limitations:** 

**Conclusion:** The incorporation of KGs is beneficial for enhancing NLI performance in automated fact-checking, particularly for misinformation related to COVID-19.

**Abstract:** Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.

</details>


### [136] [DARE: Diverse Visual Question Answering with Robustness Evaluation](https://arxiv.org/abs/2409.18023)

*Hannah Sterz, Jonas Pfeiffer, Ivan Vulić*

**Main category:** cs.CL

**Keywords:** Vision Language Models, robustness evaluation, Visual Question Answering, Diverse benchmarks, multi-modal reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces DARE, a VQA benchmark to evaluate the robustness of Vision Language Models (VLMs) on multi-modal reasoning tasks involving text and images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a robust evaluation framework for VLMs, as current benchmarks do not assess their performance under diverse conditions and variations.

**Method:** DARE involves a curated multiple-choice VQA benchmark evaluating VLMs across five categories, focusing on robustness through variations in prompts, answer options, output formats, and correct answer counts.

**Key Contributions:**

	1. Introduction of the DARE benchmark for robust evaluation of VLMs
	2. Identification of performance gaps among different categories of vision-language tasks
	3. Evaluation of the robustness discrepancies between open-source and closed-source VLMs

**Result:** State-of-the-art VLMs struggle with vision-language reasoning, showing up to 34% performance drops in robustness evaluations. Open-source models are less robust compared to closed-source ones like GPT-4.

**Limitations:** The benchmark may not cover all potential variations and scenarios that could affect VLM performance.

**Conclusion:** DARE highlights the vulnerability and inconsistent performance of modern VLMs, emphasizing the need for improved evaluation methods in multimodal tasks.

**Abstract:** Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, and are able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and/or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE, Diverse Visual Question Answering with Robustness Evaluation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of: prompts, the subsets of answer options, the output format and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. The worst case performance across the subsets of options is up to 34% below the performance in the standard case. The robustness of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the closed-source models such as GPT-4 and Gemini, but even the latter remain very brittle to different variations.

</details>


### [137] [CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages](https://arxiv.org/abs/2410.06944)

*Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal*

**Main category:** cs.CL

**Keywords:** dependency parsing, morphologically rich languages, self-supervised learning, graph-based architectures, free word order

**Relevance Score:** 4

**TL;DR:** This paper investigates enhancing neural dependency parsing performance for morphologically rich languages with free word order through a robust graph-based architecture and contrastive self-supervised learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve dependency parsing performance for morphologically rich languages that feature a relatively free word order.

**Method:** The study examines graph-based parsing architectures and incorporates modifications like data augmentation and removing position encoding. It introduces a contrastive self-supervised learning method to improve robustness to word order variations.

**Key Contributions:**

	1. Contrastive self-supervised learning method for dependency parsing.
	2. Robustness improvements in graph-based parsing for free word order languages.
	3. Empirical results demonstrating significant performance gains.

**Result:** The proposed method shows an average gain of 3.03/2.95 points in UAS/LAS Score across 7 languages compared to the best baseline.

**Limitations:** 

**Conclusion:** The contrastive self-supervised learning approach effectively enhances dependency parsing robustness in morphologically rich languages.

**Abstract:** Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.

</details>


### [138] [CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization](https://arxiv.org/abs/2410.12601)

*Yixi Ding, Jiaying Wu, Tongyao Zhu, Yanxia Qin, Qian Liu, Min-Yen Kan*

**Main category:** cs.CL

**Keywords:** compositional summarization, large language models, scientific document summarization, CCSBench, control attributes

**Relevance Score:** 7

**TL;DR:** CCSBench is the first evaluation benchmark for compositional controllable summarization in the scientific domain, focusing on both explicit and implicit attributes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for scientific document summarization systems to control multiple attributes simultaneously for better dissemination of knowledge.

**Method:** We introduce CCSBench and conduct experiments using various large language models (LLMs) with different settings for controlling attributes like length and empirical focus.

**Key Contributions:**

	1. Development of CCSBench as a benchmark for compositional controllable summarization
	2. Demonstration of LLMs' limitations in handling multiple summarization attributes
	3. Evaluation of various methods for attribute control in summarization

**Result:** Experiments reveal significant limitations of LLMs in balancing trade-offs between explicit and especially implicit control attributes.

**Limitations:** Current LLMs struggle to balance trade-offs between summary attributes, particularly those requiring deeper understanding.

**Conclusion:** Our findings highlight the need for improved understanding and sophisticated reasoning in LLMs to enhance compositional control in summarization tasks.

**Abstract:** To broaden the dissemination of scientific knowledge to diverse audiences, it is desirable for scientific document summarization systems to simultaneously control multiple attributes such as length and empirical focus. However, existing research typically focuses on controlling single attributes, leaving the compositional control of multiple attributes underexplored. To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain. Our benchmark enables fine-grained control over both explicit attributes (e.g., length), which are objective and straightforward, and implicit attributes (e.g., conceptual or empirical focus), which are more subjective and abstract. We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes. Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control attributes, especially implicit ones that require deeper understanding and abstract reasoning.

</details>


### [139] [Vulnerability of LLMs to Vertically Aligned Text Manipulations](https://arxiv.org/abs/2410.20016)

*Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun Peng, Kai-wei Chang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vertical Text Input, Text Classification, Chain of Thought Reasoning, Few-shot Learning

**Relevance Score:** 8

**TL;DR:** This paper investigates how vertical text input affects the performance of large language models (LLMs) in text classification tasks, revealing significant vulnerabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly applied in real-world scenarios, understanding how variations in text formatting, like vertical input, impact their performance is critical, especially for applications involving sensitive information.

**Method:** The paper analyzes the performance degradation caused by vertical text input across multiple text classification datasets and examines whether decoder-based LLMs are similarly affected.

**Key Contributions:**

	1. Identified performance degradation in LLMs due to vertical text input.
	2. Demonstrated that few-shot learning can mitigate this issue, unlike Chain of Thought reasoning.
	3. Analyzed the tokenization and attention matrix issues contributing to the vulnerability.

**Result:** Vertical text input significantly decreases LLM accuracy in text classification. Chain of Thought reasoning provides no benefit in recognizing vertical inputs, while few-shot learning with analysis shows effectiveness.

**Limitations:** The study focuses on text classification tasks and may not generalize to all LLM applications or other types of text formatting.

**Conclusion:** The research highlights serious vulnerabilities in LLMs regarding text formatting, particularly vertical alignment, and suggests improvements in tokenization and attention mechanisms.

**Abstract:** Vertical text input is commonly encountered in various real-world applications, such as mathematical computations and word-based Sudoku puzzles. While current large language models (LLMs) have excelled in natural language tasks, they remain vulnerable to variations in text formatting. Recent research demonstrates that modifying input formats, such as vertically aligning words for encoder-based models, can substantially lower accuracy in text classification tasks. While easily understood by humans, these inputs can significantly mislead models, posing a potential risk of bypassing detection in real-world scenarios involving harmful or sensitive information. With the expanding application of LLMs, a crucial question arises: \textit{Do decoder-based LLMs exhibit similar vulnerabilities to vertically formatted text input?} In this paper, we investigate the impact of vertical text input on the performance of various LLMs across multiple text classification datasets and analyze the underlying causes. Our findings are as follows: (i) Vertical text input significantly degrades the accuracy of LLMs in text classification tasks. (ii) \textit{Chain of Thought (CoT)} reasoning does not help LLMs recognize vertical input or mitigate its vulnerability, but \textit{few-shot learning} with careful analysis does. (iii) We explore the underlying cause of the vulnerability by analyzing the inherent issues in tokenization and attention matrices.

</details>


### [140] [Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking](https://arxiv.org/abs/2411.05375)

*Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** automated fact-checking, evidence evaluation, machine learning, natural language processing

**Relevance Score:** 5

**TL;DR:** Introduction of Ev²R, a new metric for automated fact-checking that improves evidence evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of current automated fact-checking methods that rely heavily on predefined knowledge sources and traditional evaluation metrics.

**Method:** Ev²R combines reference-based evaluation with verdict-level proxy scoring to better assess the alignment and support of evidence.

**Key Contributions:**

	1. Introduction of a hybrid evaluation metric combining reference-based and proxy scoring approaches.
	2. Demonstrated superior performance against human ratings and adversarial tests compared to existing methods.
	3. Code is made publicly available for further research and application.

**Result:** Ev²R consistently outperforms existing evaluation methods in accuracy and robustness, showing stronger correlation with human judgments and greater resilience to adversarial tests.

**Limitations:** 

**Conclusion:** Ev²R establishes itself as a reliable metric for evidence evaluation in automated fact-checking contexts.

**Abstract:** Current automated fact-checking (AFC) approaches typically evaluate evidence either implicitly via the predicted verdicts or through exact matches with predefined closed knowledge sources, such as Wikipedia. However, these methods are limited due to their reliance on evaluation metrics originally designed for other purposes and constraints from closed knowledge sources. In this work, we introduce \textbf{\textcolor{skyblue}{Ev\textsuperscript{2}}\textcolor{orangebrown}{R}} which combines the strengths of reference-based evaluation and verdict-level proxy scoring. Ev\textsuperscript{2}R jointly assesses how well the evidence aligns with the gold references and how reliably it supports the verdict, addressing the shortcomings of prior methods. We evaluate Ev\textsuperscript{2}R against three types of evidence evaluation approaches: reference-based, proxy-reference, and reference-less baselines. Assessments against human ratings and adversarial tests demonstrate that Ev\textsuperscript{2}R consistently outperforms existing scoring approaches in accuracy and robustness. It achieves stronger correlation with human judgments and greater robustness to adversarial perturbations, establishing it as a reliable metric for evidence evaluation in AFC.\footnote{Code is available at \href{https://github.com/mubasharaak/fc-evidence-evaluation}{https://github.com/mubasharaak/fc-evidence-evaluation}.}

</details>


### [141] [DRS: Deep Question Reformulation With Structured Output](https://arxiv.org/abs/2411.17993)

*Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang*

**Main category:** cs.CL

**Keywords:** question reformulation, large language models, DFS-based algorithm

**Relevance Score:** 8

**TL;DR:** DRS enhances LLMs' ability to reformulate unanswerable questions by combining LLMs with a DFS-based algorithm to iteratively explore entity combinations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitation of LLMs in reformulating unanswerable questions due to insufficient user understanding.

**Method:** DRS (Deep Question Reformulation with Structured Output) combines LLM strengths with a DFS-based algorithm for iterative exploration of potential entity combinations and constraining outputs using predefined entities.

**Key Contributions:**

	1. Introduction of DRS, a zero-shot method for question reformulation
	2. Significant improvement in reformulation accuracy for various LLMs
	3. Utilizes a structured output approach combined with entity exploration

**Result:** DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, and the performance of the Gemma2-9B model from 26.35% to 56.75%.

**Limitations:** 

**Conclusion:** The structured approach significantly enhances the question reformulation capabilities of LLMs.

**Abstract:** Question answering represents a core capability of large language models (LLMs). However, when individuals encounter unfamiliar knowledge in texts, they often formulate questions that the text itself cannot answer due to insufficient understanding of the underlying information. Recent studies reveal that while LLMs can detect unanswerable questions, they struggle to assist users in reformulating these questions. Even advanced models like GPT-3.5 demonstrate limited effectiveness in this regard. To address this limitation, we propose DRS: Deep Question Reformulation with Structured Output, a novel zero-shot method aimed at enhancing LLMs ability to assist users in reformulating questions to extract relevant information from new documents. DRS combines the strengths of LLMs with a DFS-based algorithm to iteratively explore potential entity combinations and constrain outputs using predefined entities. This structured approach significantly enhances the reformulation capabilities of LLMs. Comprehensive experimental evaluations demonstrate that DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while also enhancing the performance of open-source models, such as Gemma2-9B, from 26.35% to 56.75%.

</details>


### [142] [A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios](https://arxiv.org/abs/2412.03920)

*Xiachong Feng, Longxu Dou, Ella Li, Qinghao Wang, Haochuan Wang, Yu Guo, Chang Ma, Lingpeng Kong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Social Agents, Game Theory, Evaluation Protocols, Research Survey

**Relevance Score:** 9

**TL;DR:** This paper surveys current research on LLM-based social agents in game-theoretic scenarios, organizing findings into three components: Game Framework, Social Agent, and Evaluation Protocol.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in the literature regarding a comprehensive overview of LLM-based social agents in game-theoretic contexts.

**Method:** Systematic review of existing research, categorized into core components: Game Framework, Social Agent, and Evaluation Protocol.

**Key Contributions:**

	1. Comprehensive survey of LLM-based social agents in game theory.
	2. Categorization of research into Game Framework, Social Agent, and Evaluation Protocol.
	3. Identification of future research directions in the area.

**Result:** The survey reveals the structure of existing research, evaluates current agent performance, and identifies areas for future exploration in the context of game-theoretic scenarios.

**Limitations:** 

**Conclusion:** The findings offer insights that can further the development and performance evaluation of social agents in games, paving the way for advancements in this area.

**Abstract:** Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.

</details>


### [143] [Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots](https://arxiv.org/abs/2501.03441)

*Sarah E. Finch, Ellie S. Paek, Ikseon Choi, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** chatbots, African American English, linguistic personalization, machine learning, user engagement

**Relevance Score:** 8

**TL;DR:** This study analyzes how incorporating African American English (AAE) into chatbots affects their performance and user engagement, revealing advantages in spoken vs. text-based implementations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Personalizing chatbots is crucial for enhancing trust, engagement, and inclusivity, especially for the African American community.

**Method:** We developed text-based and spoken chatbots utilizing large language models and text-to-speech technology, then evaluated them with AAE speakers compared to standard English chatbots.

**Key Contributions:**

	1. Integration of AAE in chatbot systems
	2. Comparison of text vs spoken chatbot performance
	3. Insights into linguistic personalization for chatbots

**Result:** Text-based AAE chatbots tend to underperform, while spoken chatbots that use an African American voice and AAE elements show improved performance and user preference.

**Limitations:** Text-based AAE chatbots underperform compared to spoken variants; technological limitations affect speech generation in AAE.

**Conclusion:** Personalizing chatbot systems with AAE can enhance performance in spoken modalities but highlights limitations in text-based approaches.

**Abstract:** As chatbots become integral to daily life, personalizing systems is key for fostering trust, engagement, and inclusivity. This study examines how linguistic similarity affects chatbot performance, focusing on integrating African American English (AAE) into virtual agents to better serve the African American community. We develop text-based and spoken chatbots using large language models and text-to-speech technology, then evaluate them with AAE speakers against standard English chatbots. Our results show that while text-based AAE chatbots often underperform, spoken chatbots benefit from an African American voice and AAE elements, improving performance and preference. These findings underscore the complexities of linguistic personalization and the dynamics between text and speech modalities, highlighting technological limitations that affect chatbots' AA speech generation and pointing to promising future research directions.

</details>


### [144] [Preventing Rogue Agents Improves Multi-Agent Collaboration](https://arxiv.org/abs/2502.05986)

*Ohav Barbi, Ori Yoran, Mor Geva*

**Main category:** cs.CL

**Keywords:** multi-agent systems, agent collaboration, monitoring, WhoDunitEnv, performance

**Relevance Score:** 4

**TL;DR:** Proposes monitoring agents in multi-agent systems to prevent failures from rogue agents, tested in a new environment called WhoDunitEnv.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the risk of single rogue agents causing failures in multi-agent systems that collaborate to solve tasks.

**Method:** Introduces a monitoring approach to detect potential errors in agent predictions and intervenes to prevent system failures.

**Key Contributions:**

	1. Introduction of WhoDunitEnv for testing multi-agent collaboration
	2. Development of a monitoring approach for agent actions
	3. Demonstrated significant performance improvements in multiple environments

**Result:** Demonstrates performance gains of up to 20% in task completion by implementing the monitoring approach in various environments, including WhoDunitEnv.

**Limitations:** 

**Conclusion:** The proposed monitoring system effectively identifies agent confusion points and prevents error propagation, enhancing collaboration and performance in multi-agent settings.

**Abstract:** Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents before they act may prevent the system's failure. In this work, we propose to monitor agents during action prediction and intervene when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on WhoDunitEnv, code generation tasks and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4%, 2.5% and 20%, respectively. Thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.

</details>


### [145] [Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark](https://arxiv.org/abs/2502.07057)

*M. Ali Bayram, Ali Arda Fincan, Ahmet Semih Gümüş, Sercan Karakaş, Banu Diri, Savaş Yıldırım*

**Main category:** cs.CL

**Keywords:** tokenization, NLP, language models, morphologically rich languages, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework for evaluating tokenization strategies in NLP, focusing on morphologically rich and low-resource languages, using a dataset from the MMLU benchmark to assess various metrics related to tokenization effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by tokenization in morphologically rich and low-resource languages, which affect the performance of large language models (LLMs).

**Method:** The framework evaluates tokenizers based on five metrics: vocabulary size, token count, processing time, language-specific token percentages (%TR), and token purity. These metrics are assessed on a Turkish dataset of 6,200 questions from the MMLU benchmark.

**Key Contributions:**

	1. Introduction of the %TR metric as critical for tokenization evaluation.
	2. Demonstrated that better tokenization does not solely rely on model parameter size.
	3. Provided a structured approach for evaluating morphological tokenization strategies.

**Result:** The study reveals that %TR strongly correlates with downstream performance and model accuracy, while larger model parameters do not guarantee better tokenization quality.

**Limitations:** 

**Conclusion:** This framework sets a new standard for developing tokenization methods optimized for complex languages and emphasizes the need for tailored strategies.

**Abstract:** Tokenization is a fundamental preprocessing step in NLP, directly impacting large language models' (LLMs) ability to capture syntactic, morphosyntactic, and semantic structures. This paper introduces a novel framework for systematically evaluating tokenization strategies, addressing challenges in morphologically rich and low-resource languages. Using a Turkish dataset of 6,200 multiple-choice questions from the Massive Multitask Language Understanding (MMLU) benchmark, the framework assesses tokenizers across five key metrics: vocabulary size, token count, processing time, language-specific token percentages (\%TR), and token purity. These metrics provide a structured approach to evaluating how well tokenizers preserve linguistic structures. While \%TR measures the proportion of valid words in the target language, \%Pure assesses the alignment of tokens with meaningful linguistic units, such as roots and valid morphemes, minimizing semantic fragmentation. The findings reveal that \%TR, introduced as a critical metric, exhibits a stronger correlation with downstream performance (e.g., MMLU scores) than token purity, emphasizing its role in improving model accuracy. Additionally, larger model parameters do not necessarily yield better tokenization quality or enhanced results, highlighting the importance of tailored tokenization strategies that prioritize linguistic alignment. This framework sets a new standard for developing robust tokenization methods optimized for morphologically complex and low-resource languages. Future work will refine morphological analysis, explore domain-specific customizations, and conduct cross-linguistic evaluations to further enhance tokenization practices.

</details>


### [146] [Layerwise Recall and the Geometry of Interwoven Knowledge in LLMs](https://arxiv.org/abs/2502.10871)

*Ge Lei, Samuel J. Cooper*

**Main category:** cs.CL

**Keywords:** Large Language Models, Scientific Knowledge Representation, Geometric Structures

**Relevance Score:** 9

**TL;DR:** This study examines how large language models (LLMs) encode scientific knowledge through a case study on chemical elements, revealing a 3D spiral structure in their hidden states.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs encode and represent scientific knowledge, particularly in relation to the periodic table of chemical elements.

**Method:** The research employs linear probing to analyze the hidden states of LLaMA-series models, focusing on how different layers reflect the geometric organization of scientific concepts.

**Key Contributions:**

	1. Identification of a 3D spiral structure in LLM hidden states related to the periodic table.
	2. Demonstration of how middle layers encode continuous, overlapping attributes.
	3. Insights into the representation of symbolic knowledge in LLMs as geometric manifolds.

**Result:** The study finds that LLMs encode symbolic knowledge as structured geometric manifolds, with middle layers encoding overlapping attributes for indirect recall and deeper layers emphasizing categorical distinctions along with linguistic context.

**Limitations:** 

**Conclusion:** LLMs represent scientific knowledge through intricate geometric structures rather than isolated facts, with implications for future research in scientific reasoning and representation in LLMs.

**Abstract:** This study explores how large language models (LLMs) encode interwoven scientific knowledge, using chemical elements and LLaMA-series models as a case study. We identify a 3D spiral structure in the hidden states that aligns with the conceptual structure of the periodic table, suggesting that LLMs can reflect the geometric organization of scientific concepts learned from text. Linear probing reveals that middle layers encode continuous, overlapping attributes that enable indirect recall, while deeper layers sharpen categorical distinctions and incorporate linguistic context. These findings suggest that LLMs represent symbolic knowledge not as isolated facts, but as structured geometric manifolds that intertwine semantic information across layers. We hope this work inspires further exploration of how LLMs represent and reason about scientific knowledge, particularly in domains such as materials science.

</details>


### [147] [FastMCTS: A Simple Sampling Strategy for Data Synthesis](https://arxiv.org/abs/2502.11476)

*Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, Qipeng Guo*

**Main category:** cs.CL

**Keywords:** multi-step reasoning, data synthesis, large language models

**Relevance Score:** 8

**TL;DR:** FastMCTS is a data synthesis strategy for generating high-quality multi-step reasoning data for large language models, addressing inefficiencies of rejection sampling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of large language models on reasoning tasks by providing a more efficient sampling method for multi-step reasoning data.

**Method:** Introduces FastMCTS, inspired by Monte Carlo Tree Search, that offers step-level evaluation signals and promotes balanced sampling across problems of varying difficulty levels.

**Key Contributions:**

	1. Introduction of FastMCTS for efficient multi-step reasoning data synthesis
	2. Demonstrated significant improvements in correct reasoning paths compared to rejection sampling
	3. Outperformance of models trained on FastMCTS data over those trained on rejection sampling data

**Result:** FastMCTS generates over 30% more correct reasoning paths than rejection sampling and outperforms models trained on rejection sampling data by 3.9% across multiple benchmarks.

**Limitations:** 

**Conclusion:** FastMCTS is a practical and efficient alternative for synthesizing high-quality reasoning data, with code to be released soon.

**Abstract:** Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.

</details>


### [148] [Commonsense Reasoning in Arab Culture](https://arxiv.org/abs/2502.12788)

*Abdelrahman Sadallah, Junior Cedric Tonga, Khalid Almubarak, Saeed Almheiri, Farah Atif, Chatrine Qwaider, Karima Kadaoui, Sara Shatnawi, Yaser Alesh, Fajri Koto*

**Main category:** cs.CL

**Keywords:** commonsense reasoning, Arabic language models, cultural diversity, dataset creation, natural language processing

**Relevance Score:** 7

**TL;DR:** ArabCulture is a commonsense reasoning dataset in Modern Standard Arabic, designed to enhance understanding of diverse Arab cultures, highlighting the shortcomings of existing English datasets in capturing cultural contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of culturally relevant commonsense reasoning datasets for Arabic large language models, which have primarily relied on machine-translated English datasets that introduce biases.

**Method:** A dataset was created from scratch by engaging native speakers to write and validate questions relevant to their cultures, covering 13 countries and spanning 12 daily life domains.

**Key Contributions:**

	1. Introduction of the ArabCulture dataset
	2. Engagement of native speakers for dataset creation
	3. Demonstration of performance issues in existing models with cultural context.

**Result:** Zero-shot evaluations demonstrate that open-weight language models with up to 32B parameters perform poorly in understanding diverse Arab cultures, with variable performance across regions.

**Limitations:** The dataset focus is limited to 13 Arab countries and may not encompass every cultural nuance.

**Conclusion:** The results underscore the necessity for culturally informed models and datasets specifically designed for the Arabic-speaking community.

**Abstract:** Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce ArabCulture, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. ArabCulture spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.

</details>


### [149] [KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan](https://arxiv.org/abs/2502.12829)

*Mukhammed Togmanov, Nurdaulet Mukhituly, Diana Turmakhan, Jonibek Mansurov, Maiya Goloburda, Akhmed Sakip, Zhuohan Xie, Yuxia Wang, Bekassyl Syzdykov, Nurkhan Laiyk, Alham Fikri Aji, Ekaterina Kochmar, Preslav Nakov, Fajri Koto*

**Main category:** cs.CL

**Keywords:** Kazakh language, natural language processing, multilingual models, KazMMLU, education

**Relevance Score:** 4

**TL;DR:** Introduction of the first MMLU-style dataset for Kazakh language, KazMMLU, aimed at improving NLP for underrepresented languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of Kazakhstan's culture and language in NLP and to promote development in Kazakh language processing.

**Method:** Creation of KazMMLU dataset consisting of 23,000 questions across various educational subjects, validated by native speakers.

**Key Contributions:**

	1. Introduction of the KazMMLU dataset for Kazakh language
	2. Focus on educational level relevance in data
	3. Evaluation of multilingual models' performance in Kazakh and Russian

**Result:** Evaluation indicates significant performance gaps in existing multilingual models for Kazakh and Russian, highlighting the need for dedicated resources.

**Limitations:** Performance of existing models is limited, indicating a need for improved algorithms specifically for Kazakh language.

**Conclusion:** The KazMMLU dataset aims to catalyze further research and development in Kazakh-language LLMs, with data and code to be released upon acceptance.

**Abstract:** Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.

</details>


### [150] [How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation](https://arxiv.org/abs/2502.14642)

*Rui Li, Heming Xia, Xinfeng Yuan, Qingxiu Dong, Lei Sha, Wenjie Li, Zhifang Sui*

**Main category:** cs.CL

**Keywords:** LLMs, digital twins, human behavior simulation, BehaviorChain, persona-based evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces BehaviorChain, a benchmark for assessing LLMs' capabilities in simulating continuous human behavior through diverse, persona-based behavior chains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of current evaluations focusing mainly on dialogue simulation, neglecting human behavior which is essential for creating effective digital twins.

**Method:** BehaviorChain includes 15,846 diverse behaviors across 1,001 personas, integrating persona metadata into LLMs for evaluating their ability to simulate human behavior in dynamic contexts.

**Key Contributions:**

	1. Introduction of BehaviorChain benchmark for LLM evaluation.
	2. Development of 15,846 persona-based diverse behavior simulations.
	3. Demonstration of LLMs' limitations in accurately simulating continuous behavior.

**Result:** Evaluation results show that even leading LLMs struggle to accurately simulate continuous human behavior when tested against the BehaviorChain benchmark.

**Limitations:** Limited to the evaluation of continuous behavior simulation; does not encompass other aspects of LLM performance.

**Conclusion:** The findings highlight the shortcomings of state-of-the-art LLMs in simulating human behavior and underscore the need for improved evaluation frameworks.

**Abstract:** Recently, LLMs have garnered increasing attention across academic disciplines for their potential as human digital twins, virtual proxies designed to replicate individuals and autonomously perform tasks such as decision-making, problem-solving, and reasoning on their behalf. However, current evaluations of LLMs primarily emphasize dialogue simulation while overlooking human behavior simulation, which is crucial for digital twins. To address this gap, we introduce BehaviorChain, the first benchmark for evaluating LLMs' ability to simulate continuous human behavior. BehaviorChain comprises diverse, high-quality, persona-based behavior chains, totaling 15,846 distinct behaviors across 1,001 unique personas, each with detailed history and profile metadata. For evaluation, we integrate persona metadata into LLMs and employ them to iteratively infer contextually appropriate behaviors within dynamic scenarios provided by BehaviorChain. Comprehensive evaluation results demonstrated that even state-of-the-art models struggle with accurately simulating continuous human behavior.

</details>


### [151] [MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs](https://arxiv.org/abs/2502.14916)

*Xinxin You, Xien Liu, Xue Yang, Ziyi Wang, Ji Wu*

**Main category:** cs.CL

**Keywords:** ICD coding, Chinese EMRs, machine learning, medical records, evidence verification

**Relevance Score:** 4

**TL;DR:** This paper presents MKE-Coder, a framework for automatic ICD coding in Chinese electronic medical records (EMRs), addressing challenges in code extraction and evidence verification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automated ICD coding for Chinese EMRs, which face difficulties in extracting disease-related information and lack evidence correlation.

**Method:** Introduces MKE-Coder, which identifies candidate ICD codes, retrieves clinical evidence, and verifies code validity through a masked language modeling inference module.

**Key Contributions:**

	1. Introduction of the MKE-Coder framework for ICD coding in Chinese EMRs.
	2. Use of multi-axial knowledge for disease code extraction.
	3. Validation through a masked language model to ensure accuracy of selected codes.

**Result:** MKE-Coder shows superior performance in automatic ICD coding on Chinese EMRs, enhancing coders' accuracy and efficiency in real coding scenarios.

**Limitations:** The manuscript has been withdrawn for further revisions and additional experiments; hence, it may not include final results or methodologies.

**Conclusion:** The proposed framework significantly aids in ICD coding by leveraging multi-axial knowledge and reinforcing code validation with credible clinical evidence.

**Abstract:** The task of automatically coding the International Classification of Diseases (ICD) in the medical field has been well-established and has received much attention. Automatic coding of the ICD in the medical field has been successful in English but faces challenges when dealing with Chinese electronic medical records (EMRs). The first issue lies in the difficulty of extracting disease code-related information from Chinese EMRs, primarily due to the concise writing style and specific internal structure of the EMRs. The second problem is that previous methods have failed to leverage the disease-based multi-axial knowledge and lack of association with the corresponding clinical evidence. This paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge with Evidence verification in ICD coding for Chinese EMRs. Initially, we identify candidate codes for the diagnosis and categorize each of them into knowledge under four coding axes.Subsequently, we retrieve corresponding clinical evidence from the comprehensive content of EMRs and filter credible evidence through a scoring model. Finally, to ensure the validity of the candidate code, we propose an inference module based on the masked language modeling strategy. This module verifies that all the axis knowledge associated with the candidate code is supported by evidence and provides recommendations accordingly. To evaluate the performance of our framework, we conduct experiments using a large-scale Chinese EMR dataset collected from various hospitals. The experimental results demonstrate that MKE-Coder exhibits significant superiority in the task of automatic ICD coding based on Chinese EMRs. In the practical evaluation of our method within simulated real coding scenarios, it has been demonstrated that our approach significantly aids coders in enhancing both their coding accuracy and speed.

</details>


### [152] [Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans](https://arxiv.org/abs/2502.15090)

*Masha Fedzechkina, Eleonora Gualdoni, Sinead Williamson, Katherine Metcalf, Skyler Seto, Barry-John Theobald*

**Main category:** cs.CL

**Keywords:** large language models, human representation alignment, activation steering, concept organization, neural networks

**Relevance Score:** 9

**TL;DR:** This paper investigates how well large language models (LLMs) align with human representations by using activation steering to analyze neurons linked to specific concepts, revealing a close alignment between LLMs and humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the alignment of learned representations in LLMs with human representations and understand the organizational patterns of concepts in LLMs.

**Method:** The authors adopt a method from activation steering to identify specific neurons for concepts and analyze activation patterns, allowing them to compare LLM representations with human representations derived from behavioral data.

**Key Contributions:**

	1. Introduction of a novel representation alignment approach using activation steering
	2. Demonstration of LLMs' alignment with human representations surpassing previous word embedding methods
	3. Insights into the organization of concepts in LLMs mirroring human organization

**Result:** The study finds that LLM representations align closely with human representations, achieving inter-human alignment levels and surpassing previous methods based on word embeddings.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs organize concepts similarly to humans, providing insights into the alignment of machine and human representations.

**Abstract:** Modern large language models (LLMs) achieve impressive performance on some tasks, while exhibiting distinctly non-human-like behaviors on others. This raises the question of how well the LLM's learned representations align with human representations. In this work, we introduce a novel approach to study representation alignment: we adopt a method from research on activation steering to identify neurons responsible for specific concepts (e.g., ''cat'') and then analyze the corresponding activation patterns. We find that LLM representations captured this way closely align with human representations inferred from behavioral data, matching inter-human alignment levels. Our approach significantly outperforms the alignment captured by word embeddings, which have been the focus of prior work on human-LLM alignment. Additionally, our approach enables a more granular view of how LLMs represent concepts -- we show that LLMs organize concepts in a way that mirrors human concept organization.

</details>


### [153] [Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models](https://arxiv.org/abs/2502.15639)

*Anirudh Sundar, Sinead Williamson, Katherine Metcalf, Barry-John Theobald, Skyler Seto, Masha Fedzechkina*

**Main category:** cs.CL

**Keywords:** multilingual large language models, cross-lingual alignment, model interventions, neuron manipulation, embedding space

**Relevance Score:** 9

**TL;DR:** The paper explores model interventions as a data-efficient method to enhance cross-lingual alignment in multilingual large language models (mLLMs).

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** Improving cross-lingual performance in mLLMs without requiring extensive fine-tuning and large datasets.

**Method:** Analysis of a model intervention technique, specifically finding experts, to manipulate model activations in mLLMs, followed by introspection of the embedding space pre- and post-manipulation.

**Key Contributions:**

	1. Demonstration of the effectiveness of model interventions for cross-lingual alignment.
	2. Identification of key neurons for manipulation in mLLMs for specific languages.
	3. Quantification of performance improvements in retrieval tasks post-manipulation.

**Result:** The manipulation of mLLM activations effectively enhances cross-lingual alignment in the embedding space, resulting in improved retrieval task performance with up to 2x top-1 accuracy.

**Limitations:** The study primarily focuses on one type of model intervention, leaving other potential methods unexplored.

**Conclusion:** Model interventions can serve as a potent alternative to fine-tuning by providing significant performance boosts in cross-lingual tasks with fewer resources.

**Abstract:** Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.

</details>


### [154] [Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in Product QA Agents](https://arxiv.org/abs/2502.19545)

*Ashley Lewis, Michael White, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, customer support, hallucination, retrieval-augmented QA, self-training

**Relevance Score:** 9

**TL;DR:** Proposes a retrieval-augmented QA pipeline to mitigate hallucination in LLMs for customer support using synthetic data and self-training techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of hallucination and high costs associated with proprietary LLMs in customer support applications.

**Method:** Developed a retrieval-augmented QA pipeline and conducted experiments comparing synthetic data generated by LLMs with crowdsourced data, as well as analyzing self-training against knowledge distillation.

**Key Contributions:**

	1. Introduction of a retrieval-augmented QA pipeline
	2. Demonstrated that synthetic data is superior to crowdsourced data in reducing hallucination
	3. Analysis of self-training achieving hallucination reduction comparable to knowledge distillation

**Result:** Synthetic data improved hallucination reduction in finetuned models; self-training was comparable to knowledge distillation with potential bias identified.

**Limitations:** 

**Conclusion:** Cost-effective QA systems can leverage synthetic data and self-training with open-source models, minimizing the need for expensive proprietary solutions.

**Abstract:** The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination (generating false information) and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized "I don't know" responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.

</details>


### [155] [Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks](https://arxiv.org/abs/2503.00024)

*Yanran Chen, Steffen Eger*

**Main category:** cs.CL

**Keywords:** emotions, argument convincingness, language models, human evaluation, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper explores the role of emotions in argument convincingness using a dynamic framework and human evaluations across various languages and topics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The underexplored role of emotions in argument convincingness within the NLP community.

**Method:** A dynamic framework inspired by psychology manipulation checks was employed, utilizing LLM-based manipulation checks to assess perceived emotional intensity's influence on convincingness.

**Key Contributions:**

	1. Introducing a dynamic framework for analyzing emotional influence on convincingness
	2. Conducting human evaluations across different languages and text domains
	3. Comparison of human and LLMs' responses to emotional intensity in argument evaluation.

**Result:** Human judgments of convincingness usually remain unchanged despite variations in emotional intensity, and when emotions do affect convincingness, they tend to enhance it. LLMs generally replicate human judgment patterns but struggle with nuanced emotional effects.

**Limitations:** The study found that emotional influence can vary widely and might not be easily captured by LLMs due to nuanced human emotional interpretation.

**Conclusion:** Emotions impact convincingness more often positively, revealing both the limitations of current NLP models in understanding emotions and the need for further exploration in this area.

**Abstract:** Emotions have been shown to play a role in argument convincingness, yet this aspect is underexplored in the natural language processing (NLP) community. Unlike prior studies that use static analyses, focus on a single text domain or language, or treat emotion as just one of many factors, we introduce a dynamic framework inspired by manipulation checks commonly used in psychology and social science; leveraging LLM-based manipulation checks, this framework examines the extent to which perceived emotional intensity influences perceived convincingness. Through human evaluation of arguments across different languages, text domains, and topics, we find that in over half of cases, human judgments of convincingness remain unchanged despite variations in perceived emotional intensity; when emotions do have an impact, they more often enhance rather than weaken convincingness. We further analyze whether 11 LLMs behave like humans in the same scenario, finding that while LLMs generally mirror human patterns, they struggle to capture nuanced emotional effects in individual judgments.

</details>


### [156] [Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models](https://arxiv.org/abs/2503.01781)

*Meghana Rajeev, Rajkumar Ramamurthy, Prapti Trivedi, Vikas Yadav, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudan, James Zou, Nazneen Rajani*

**Main category:** cs.CL

**Keywords:** adversarial attacks, reasoning models, machine learning, vulnerabilities, security

**Relevance Score:** 6

**TL;DR:** The paper proposes CatAttack, a method to generate adversarial triggers that mislead reasoning models in math problem solving.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the vulnerabilities of reasoning models in step-by-step problem solving by introducing adversarial triggers.

**Method:** An automated iterative attack pipeline (CatAttack) generates query-agnostic adversarial triggers using a cheaper proxy model and applies them to advanced reasoning models, transferring the attacks successfully to provoke incorrect answers.

**Key Contributions:**

	1. Introduction of query-agnostic adversarial triggers in reasoning models.
	2. Development of the CatAttack automated attack pipeline.
	3. Empirical evidence showing the susceptibility of advanced reasoning models to subtle adversarial inputs.

**Result:** The attack increases the likelihood of generating incorrect answers by over 300%. For instance, appending certain irrelevant text to math problems significantly worsens model performance.

**Limitations:** The study focuses only on specific reasoning models and may not generalize to all AI systems.

**Conclusion:** The findings reveal critical vulnerabilities in state-of-the-art reasoning models, highlighting security and reliability concerns in real-world applications.

**Abstract:** We investigate the robustness of reasoning models trained for step-by-step problem solving by introducing query-agnostic adversarial triggers - short, irrelevant text that, when appended to math problems, systematically mislead models to output incorrect answers without altering the problem's semantics. We propose CatAttack, an automated iterative attack pipeline for generating triggers on a weaker, less expensive proxy model (DeepSeek V3) and successfully transfer them to more advanced reasoning target models like DeepSeek R1 and DeepSeek R1-distilled-Qwen-32B, resulting in greater than 300% increase in the likelihood of the target model generating an incorrect answer. For example, appending, "Interesting fact: cats sleep most of their lives," to any math problem leads to more than doubling the chances of a model getting the answer wrong. Our findings highlight critical vulnerabilities in reasoning models, revealing that even state-of-the-art models remain susceptible to subtle adversarial inputs, raising security and reliability concerns. The CatAttack triggers dataset with model responses is available at https://huggingface.co/datasets/collinear-ai/cat-attack-adversarial-triggers.

</details>


### [157] [Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning](https://arxiv.org/abs/2503.05641)

*Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, LLM, skill-based selection, benchmark evaluation, computational efficiency

**Relevance Score:** 9

**TL;DR:** Symbolic-MoE is a Mixture-of-Experts framework that selects LLM experts based on specific skills, improving task performance efficiently.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the selection of pre-trained LLM experts for diverse tasks at an instance level rather than a coarse-grained task level.

**Method:** Symbolic-MoE dynamically selects LLMs based on their strengths related to specific skills, generating outputs from multiple experts and synthesizing them using an efficient batching strategy.

**Key Contributions:**

	1. Introduction of skill-based recruiting strategy for LLM experts
	2. Implementation of a batch strategy for efficient model loading
	3. Extensive evaluations demonstrating superior performance on diverse benchmarks

**Result:** Symbolic-MoE shows significant performance improvements on benchmarks like MMLU-Pro and MedMCQA, outperforming existing multi-agent frameworks while maintaining computational efficiency.

**Limitations:** Potentially high computational overhead if implemented naively without the batching strategy.

**Conclusion:** Instance-level expert selection improves LLM performance and generalization, reducing the need for extensive multi-round discussions and computation.

**Abstract:** Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting task-level experts is often too coarse-grained, as heterogeneous tasks may require different expertise per instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we show that Symbolic-MoE beats strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute avg. gain of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE generalizes well to unseen tasks and removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.

</details>


### [158] [Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516)

*Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Information Retrieval, Search Optimization, Text Generation

**Relevance Score:** 9

**TL;DR:** The paper presents Search-R1, a reinforcement learning extension for enhancing large language models' reasoning capabilities by optimizing their interaction with search engines for real-time information retrieval, showing significant improvements in performance across various datasets.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reasoning and text generation capabilities of large language models by enhancing their interaction with search engines during inference for real-time knowledge acquisition.

**Method:** The method employs reinforcement learning techniques to enable the LLM to generate multiple search queries during step-by-step reasoning, utilizing multi-turn search interactions with a reward function based on outcomes and token masking for stable training.

**Key Contributions:**

	1. Introduction of Search-R1 for LLM-assisted reasoning with search engines
	2. Demonstration of substantial performance improvements over baseline models
	3. Empirical insights into RL methods and their impact on LLM outputs.

**Result:** Experiments demonstrate that Search-R1 improves performance by 41% with the Qwen2.5-7B model and by 20% with the Qwen2.5-3B model over several retrieval-augmented generation baselines.

**Limitations:** 

**Conclusion:** The findings indicate that the proposed method significantly enhances the reasoning capabilities of LLMs, providing valuable insights into RL optimization, LLM selection, and dynamics of response length in retrieval-augmented contexts.

**Abstract:** Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [159] [OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs](https://arxiv.org/abs/2503.11858)

*Ivan Kartáč, Mateusz Lango, Ondřej Dušek*

**Main category:** cs.CL

**Keywords:** NLG evaluation, Open-source LLM, Explainability

**Relevance Score:** 8

**TL;DR:** OpeNLGauge is an open-source NLG evaluation metric using LLMs that provides accurate explanations and outperforms existing models in certain tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address drawbacks in existing LLM-based evaluation metrics related to proprietary data reliance and lack of explanatory feedback.

**Method:** A two-stage ensemble or a small fine-tuned evaluation model that uses open-weight LLMs for NLG assessment.

**Key Contributions:**

	1. Introduction of an open-source evaluation metric
	2. Providing fine-grained, explanatory feedback
	3. Achieving state-of-the-art performance on certain tasks

**Result:** OpeNLGauge achieves competitive correlation with human judgments and outperforms state-of-the-art models on some tasks, providing more accurate explanations.

**Limitations:** 

**Conclusion:** OpeNLGauge offers a fully reproducible and open-source solution for NLG evaluation, enhancing explainability and generalizability.

**Abstract:** Large Language Models (LLMs) have demonstrated great potential as evaluators of NLG systems, allowing for high-quality, reference-free, and multi-aspect assessments. However, existing LLM-based metrics suffer from two major drawbacks: reliance on proprietary models to generate training data or perform evaluations, and a lack of fine-grained, explanatory feedback. In this paper, we introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation metric that provides accurate explanations based on error spans. OpeNLGauge is available as a two-stage ensemble of larger open-weight LLMs, or as a small fine-tuned evaluation model, with confirmed generalizability to unseen tasks, domains and aspects. Our extensive meta-evaluation shows that OpeNLGauge achieves competitive correlation with human judgments, outperforming state-of-the-art models on certain tasks while maintaining full reproducibility and providing explanations more than twice as accurate.

</details>


### [160] [SWI: Speaking with Intent in Large Language Models](https://arxiv.org/abs/2503.21544)

*Yuwei Yin, EunJeong Hwang, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** Large Language Models, Speaking with Intent, Cognitive Framework, Reasoning, Natural Language Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces Speaking with Intent (SWI) in large language models, enhancing their reasoning and generation quality by providing explicit cognitive frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities and generation quality of large language models through the introduction of a structured intent framework.

**Method:** The authors implemented SWI, which explicitly generates intent for guiding analysis and subsequent actions, and evaluated it through extensive experiments including text summarization and mathematical reasoning benchmarks.

**Key Contributions:**

	1. Introduction of the concept of Speaking with Intent (SWI) in LLMs
	2. Demonstration of SWI's effectiveness across multiple benchmarks
	3. Human evaluations confirming the improved coherence and interpretability of outputs.

**Result:** SWI consistently outperforms direct generation methods in various benchmarks, showing improvements in coherence, effectiveness, and interpretability as verified through human evaluations.

**Limitations:** 

**Conclusion:** Introducing explicit intents in LLMs boosts their generative and reasoning capabilities, suggesting a new approach to enhance language models' performance.

**Abstract:** Intent, typically clearly formulated and planned, functions as a cognitive framework for communication and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and action. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on text summarization, multi-task question answering, and mathematical reasoning benchmarks consistently demonstrate the effectiveness and generalizability of Speaking with Intent over direct generation without explicit intent. Further analysis corroborates the generalizability of SWI under different experimental settings. Moreover, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. The promising results in enhancing LLMs with explicit intents pave a new avenue for boosting LLMs' generation and reasoning abilities with cognitive notions.

</details>


### [161] [Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](https://arxiv.org/abs/2503.23768)

*Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, font recognition, dataset, few-shot learning, Chain-of-Thought prompting

**Relevance Score:** 6

**TL;DR:** This paper investigates the capability of Vision-Language Models (VLMs) in recognizing fonts through a new dataset and evaluates their performance under varying conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether modern VLMs can effectively recognize fonts, given their multimodal capabilities and applications in real-world design contexts.

**Method:** Introduced the Font Recognition Benchmark (FRB), consisting of 15 fonts in easy and hard versions, and evaluated various VLMs' font recognition tasks on this dataset.

**Key Contributions:**

	1. Introduction of the Font Recognition Benchmark (FRB) dataset for font recognition testing.
	2. Demonstration of limited effectiveness of VLMs in font recognition tasks compared to expectations.
	3. Analysis of attention mechanisms revealing VLMs' shortcomings in capturing semantic information.

**Result:** Evaluation revealed that current VLMs have limited font recognition capabilities, significantly affected by a stroop effect, and show minimal improvement with few-shot learning and CoT prompting.

**Limitations:** VLMs demonstrate inadequate performance in fine-grained font recognition tasks and struggle with certain cognitive challenges.

**Conclusion:** VLMs struggle with font recognition, highlighting their limitations in capturing semantic features, particularly under challenges like the stroop effect.

**Abstract:** Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features.

</details>


### [162] [Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models](https://arxiv.org/abs/2504.01216)

*Feng Chen, Dror Ben-Zeev, Gillian Sparks, Arya Kadakia, Trevor Cohen*

**Main category:** cs.CL

**Keywords:** PTSD, natural language processing, transformer models, machine learning, clinical assessment

**Relevance Score:** 9

**TL;DR:** The study evaluates NLP methods for detecting PTSD from clinical transcripts, showing that domain-specific models and SentenceBERT outperform general models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underdiagnosis of PTSD in clinical settings by utilizing automated detection methods.

**Method:** The study compared transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and prompting strategies using the DAIC-WOZ dataset.

**Key Contributions:**

	1. Domain-specific models significantly improve PTSD detection accuracy.
	2. High performance of SentenceBERT embeddings with neural networks.
	3. Insights for developing clinically viable AI tools for PTSD assessment.

**Result:** Domain-specific models outperformed general ones (Mental-RoBERTa AUPRC=0.675 vs. RoBERTa-base 0.599). SentenceBERT with neural networks achieved the highest performance (AUPRC=0.758).

**Limitations:** Performance varied across symptom severity and comorbidity status; further improvements needed for nuanced cases.

**Conclusion:** The findings suggest domain-adapted models and LLMs can enhance PTSD screening, but there is a need for better detection of nuanced cases.

**Abstract:** Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical settings, presenting opportunities for automated detection to identify patients. This study evaluates natural language processing approaches for detecting PTSD from clinical interview transcripts. We compared general and mental health-specific transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and large language model prompting strategies (zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset. Domain-specific end-to-end models significantly outperformed general models (Mental-RoBERTa AUPRC=0.675+/-0.084 vs. RoBERTa-base 0.599+/-0.145). SentenceBERT embeddings with neural networks achieved the highest overall performance (AUPRC=0.758+/-0.128). Few-shot prompting using DSM-5 criteria yielded competitive results with two examples (AUPRC=0.737). Performance varied significantly across symptom severity and comorbidity status with depression, with higher accuracy for severe PTSD cases and patients with comorbid depression. Our findings highlight the potential of domain-adapted embeddings and LLMs for scalable screening while underscoring the need for improved detection of nuanced presentations and offering insights for developing clinically viable AI tools for PTSD assessment.

</details>


### [163] [The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022)

*Sheridan Feucht, Eric Todd, Byron Wallace, David Bau*

**Main category:** cs.CL

**Keywords:** induction heads, semantics, language representation, natural language processing, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces concept-level induction heads in language models, which copy whole lexical units rather than individual tokens, operating alongside token-level induction heads.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to explore the mechanisms behind in-context copying in language models, particularly focusing on the role of induction heads in semantic tasks and knowledge representation.

**Method:** The authors conduct experiments to identify and analyze the behavior of concept-level induction heads in comparison to traditional token induction heads, particularly in tasks like translation.

**Key Contributions:**

	1. Identification of concept-level induction heads that copy entire lexical units.
	2. Demonstration of the independent operation of token and concept induction heads.
	3. Insights into language-independent word representations that facilitate translation.

**Result:** The study finds that concept induction heads effectively handle semantic tasks, such as word-level translation, while token induction heads are crucial for verbatim tasks. These two types of heads operate independently and reveal a nuanced understanding of language representation in LLMs.

**Limitations:** 

**Conclusion:** The discovery of concept induction heads suggests that LLMs have a capability to represent abstract meanings independent of language, enhancing their utility in multilingual settings.

**Abstract:** Prior work on in-context copying has shown the existence of induction heads, which attend to and promote individual tokens during copying. In this work we discover a new type of induction head: concept-level induction heads, which copy entire lexical units instead of individual tokens. Concept induction heads learn to attend to the ends of multi-token words throughout training, working in parallel with token-level induction heads to copy meaningful text. We show that these heads are responsible for semantic tasks like word-level translation, whereas token induction heads are vital for tasks that can only be done verbatim (like copying nonsense tokens). These two "routes" operate independently: we show that ablation of token induction heads causes models to paraphrase where they would otherwise copy verbatim. By patching concept induction head outputs, we find that they contain language-independent word representations that mediate natural language translation, suggesting that LLMs represent abstract word meanings independent of language or form.

</details>


### [164] [Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems](https://arxiv.org/abs/2504.09763)

*Zaid Khan, Elias Stengel-Eskin, Archiki Prasad, Jaemin Cho, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** Executable Functional Abstraction, program synthesis, mathematical reasoning

**Relevance Score:** 6

**TL;DR:** This paper presents EFAGen, a program synthesis approach for automatically generating Executable Functional Abstractions (EFAs) for advanced mathematics problems, using LLMs and unit testing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the construction of EFAs for advanced mathematics, which has previously required human intervention, enhancing problem generation in mathematics and supporting various applications such as model stress-testing.

**Method:** EFAGen infers an EFA by treating it as a program synthesis task, formalizing EFA properties as unit tests, utilizing execution feedback to optimize candidate programs generated by a language model.

**Key Contributions:**

	1. Introduction of Executable Functional Abstraction (EFA) for advanced math problems.
	2. Development of EFAGen for automatic synthesis of EFAs.
	3. Demonstration of EFAGen's ability to generate problem variations and support data generation.

**Result:** EFAGen successfully infers EFAs that are faithful to seed problems, generates learnable variations of problems, and can infer EFAs across various competition-level math problems.

**Limitations:** 

**Conclusion:** The approach demonstrates that automated generation of EFAs can be useful for creating diverse and challenging variations of math problems, contributing to improvements in mathematical reasoning and AI model training.

**Abstract:** Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from reinforcement learning (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for mathematical reasoning as problem generators for stress-testing models. However, prior work has been limited to automatically constructing abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced mathematics problems by developing EFAGen, which operationalizes the task of automatically inferring an EFA for a given seed problem and solution as a program synthesis task. We first formalize the properties of any valid EFA as executable unit tests. Using execution feedback from the unit tests, we search over candidate programs sampled from a LLM to find EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. We then apply the tests as a reward signal, training LLMs to become better writers of EFAs. We show that EFAs inferred by EFAGen are faithful to the seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across diverse sources of competition-level math problems. Finally, we show uses of model-written EFAs e.g., finding harder/easier problem variants, as well as data generation.

</details>


### [165] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)

*Subhendu Khatuya, Shashwat Naidu, Saptarshi Ghosh, Pawan Goyal, Niloy Ganguly*

**Main category:** cs.CL

**Keywords:** multi-label text classification, generative models, semantic alignment

**Relevance Score:** 8

**TL;DR:** Robust domain-agnostic framework for multi-label text classification using generative models and predefined label descriptions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The growing volume of textual data necessitates more efficient automated document classification methods.

**Method:** A generative model framework that utilizes predefined label descriptions and a dual-objective loss function to train and match with input text using a finetuned sentence transformer.

**Key Contributions:**

	1. Introduction of a generative model framework for multi-label classification
	2. Utilization of predefined label descriptions to improve semantic alignment
	3. Dual-objective loss function combining cross-entropy and cosine similarity

**Result:** The proposed model LAGAMC achieves state-of-the-art performances with significant improvements in Micro-F1 and Macro-F1 scores across multiple datasets compared to several strong baselines.

**Limitations:** 

**Conclusion:** LAGAMC demonstrates parameter efficiency and versatility, making it suitable for various practical applications in multi-label text classification.

**Abstract:** The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the pre-defined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 compared to the closest baseline across all datasets.

</details>
