# 2025-08-19

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 23]

- [cs.CL](#cs.CL) [Total: 102]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [FairVizARD: A Visualization System for Assessing Multi-Party Fairness of Ride-Sharing Matching Algorithms](https://arxiv.org/abs/2508.11770)

*Ashwin Kumar, Sanket Shah, Meghna Lowalekar, Pradeep Varakantham, Alvitta Ottley, William Yeoh*

**Main category:** cs.HC

**Keywords:** fairness, ride-sharing, visualization, algorithms, user studies

**Relevance Score:** 4

**TL;DR:** FairVizARD is a visualization-based system designed to help users evaluate the fairness of ride-sharing matching algorithms, presenting results through animation and charting methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing interest in algorithms matching passengers with drivers in ride-sharing, and to explore the fairness of these algorithms among different parties.

**Method:** A visualization-based system employing spatio-temporal information and efficient visual techniques to present algorithmic results.

**Key Contributions:**

	1. Introduction of FairVizARD for visualizing fairness in ride-sharing algorithms
	2. Utilization of spatio-temporal information in visualizations
	3. Empirical validation through user studies and expert interviews

**Result:** User studies and an expert interview demonstrated that FairVizARD helps users evaluate fairness and broaden their understanding of the concept.

**Limitations:** 

**Conclusion:** FairVizARD provides a practical tool for real-world applications, enhancing the evaluation of fairness in ride-sharing matching algorithms.

**Abstract:** There is growing interest in algorithms that match passengers with drivers in ride-sharing problems and their fairness for the different parties involved (passengers, drivers, and ride-sharing companies). Researchers have proposed various fairness metrics for matching algorithms, but it is often unclear how one should balance the various parties' fairness, given that they are often in conflict. We present FairVizARD, a visualization-based system that aids users in evaluating the fairness of ride-sharing matching algorithms. FairVizARD presents the algorithms' results by visualizing relevant spatio-temporal information using animation and aggregated information in charts. FairVizARD also employs efficient techniques for visualizing a large amount of information in a user friendly manner, which makes it suitable for real-world settings. We conduct our experiments on a real-world large-scale taxi dataset and, through user studies and an expert interview, we show how users can use FairVizARD not only to evaluate the fairness of matching algorithms but also to expand on their notions of fairness.

</details>


### [2] [XR-First Design for Productivity: A Conceptual Framework for Enabling Efficient Task Switching in XR](https://arxiv.org/abs/2508.11778)

*Matt Gottsacker, Yahya Hmaiti, Mykola Maslych, Gerd Bruder, Joseph J. LaViola Jr., Gregory F. Welch*

**Main category:** cs.HC

**Keywords:** extended reality, context switching, user-centric design, knowledge work, task management

**Relevance Score:** 8

**TL;DR:** The paper presents a vision for improving context switching in extended reality (XR) environments, emphasizing the need for user-centric interfaces to facilitate multitasking and better interaction with spatial information applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing XR applications require users to exit one app to start another, impacting efficiency in knowledge work that benefits from quick context switching.

**Method:** The paper discusses a proposed user-centric paradigm for context and task switching in XR, aimed at guiding future development of related interfaces.

**Key Contributions:**

	1. Proposed a user-centric paradigm for context switching in XR environments.
	2. Identified limitations of current window-based metaphors for spatial information in XR.
	3. Outlined future research directions for developing efficient XR interfaces.

**Result:** A conceptual framework for XR-first interfaces that support efficient task management and multitasking in immersive environments.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of developing new interaction paradigms that support seamless context switching in XR applications for enhanced user efficiency.

**Abstract:** A core component of completing tasks efficiently in computer-supported knowledge work is the ability for users to rapidly switch their focus (and interaction) across different applications using various shortcuts and gestures. This feature set has been explored in research, and several modern consumer extended reality (XR) headsets now support loading multiple applications windows at once. However, many XR applications that are useful for knowledge work involve rich spatial information, which window-based metaphors do not sufficiently represent nor afford appropriate interaction. In modern XR headsets, such immersive applications run as siloed experiences, requiring the user to fully exit one before starting another. We present a vision for achieving an XR-first, user-centric paradigm for efficient context switching in XR to encourage and guide future research and development of XR context- and task-switching interfaces.

</details>


### [3] [Behavioral and Symbolic Fillers as Delay Mitigation for Embodied Conversational Agents in Virtual Reality](https://arxiv.org/abs/2508.11781)

*Denmar Mojan Gonzales, Snehanjali Kalamkar, Sophie JÃ¶rg, Jens Grubert*

**Main category:** cs.HC

**Keywords:** embodied conversational agents, virtual reality, filler types, user experience, interaction design

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of different filler types on user experience when interacting with ECAs in virtual reality, aiming to improve perceived response times and interaction quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the frustrations caused by response delays in embodied conversational agents by exploring effective filler types to enhance user experience.

**Method:** A within-subject study with 24 participants compared four strategies used during response delays: multimodal behavioral fillers, idle motion, and two types of symbolic indicators (progress bars, one as an embedded badge and one as an external thinking bubble).

**Key Contributions:**

	1. Identification of effective filler types for improving interactions with ECAs in VR.
	2. Demonstrated the limitations of symbolic indicators in enhancing user experience.
	3. Provided insights into user preferences regarding interaction types in virtual environments.

**Result:** Behavioral fillers significantly improved perceived response time and user perceptions of presence, humanlikeness, and naturalness, while symbolic indicators led to more gaze aversion without enhancing overall impressions of the agent.

**Limitations:** Study limited to specific types of fillers; results may not generalize to all ECAs or VR environments.

**Conclusion:** Participants preferred behavioral fillers over symbolic indicators, indicating their effectiveness in improving the user experience during delays in response.

**Abstract:** When communicating with embodied conversational agents (ECAs) in virtual reality, there might be delays in the responses of the agents lasting several seconds, for example, due to more extensive computations of the answers when large language models are used. Such delays might lead to unnatural or frustrating interactions. In this paper, we investigate filler types to mitigate these effects and lead to a more positive experience and perception of the agent. In a within-subject study, we asked 24 participants to communicate with ECAs in virtual reality, comparing four strategies displayed during the delays: a multimodal behavioral filler consisting of conversational and gestural fillers, a base condition with only idle motions, and two symbolic indicators with progress bars, one embedded as a badge on the agent, the other one external and visualized as a thinking bubble. Our results indicate that the behavioral filler improved perceived response time, three subscales of presence, humanlikeness, and naturalness. Participants looked away from the face more often when symbolic indicators were displayed, but the visualizations did not lead to a more positive impression of the agent or to increased presence. The majority of participants preferred the behavioral fillers, only 12.5% and 4.2% favored the symbolic embedded and external conditions, respectively.

</details>


### [4] [Two Sides to Every Story: Exploring Hybrid Design Teams' Perceptions of Psychological Safety on Slack](https://arxiv.org/abs/2508.11788)

*Marjan Naghshbandi, Sharon Ferguson, Alison Olechowski*

**Main category:** cs.HC

**Keywords:** Psychological Safety, Hybrid Teams, Human-Computer Interaction, Slack Indicators, Interpersonal Risk-Taking

**Relevance Score:** 8

**TL;DR:** This paper explores the Psychological Safety (PS) of hybrid engineering design teams and develops strategies to enhance team dynamics through Slack-based indicators.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of hybrid work affecting collaboration and team dynamics, and to enhance Psychological Safety in such teams.

**Method:** A mixed-methods study involving interviews with six hybrid engineering design teams to assess their perceptions of Psychological Safety indicators and their manifestation on Slack versus in-person.

**Key Contributions:**

	1. Developed Slack-based Psychological Safety indicators.
	2. Presented design examples for enhancing hybrid team dynamics.
	3. Outlined best practices for supporting interpersonal risk-taking in hybrid settings.

**Result:** Identified five facets of Psychological Safety in hybrid teams, four differences in PS perceptions on Slack and in-person, and developed 15 Slack-based PS indicators.

**Limitations:** 

**Conclusion:** The insights lead to design implications for instant-messaging platforms to improve Psychological Safety in hybrid teams and provide best practices for fostering interpersonal risk-taking.

**Abstract:** While the unique challenges of hybrid work can compromise collaboration and team dynamics, hybrid teams can thrive with well-informed strategies and tools that nurture interpersonal engagements. To inform future supports, we pursue a mixed-methods study of hybrid engineering design capstone teams' Psychological Safety (PS) (i.e., their climate of interpersonal risk-taking and mutual respect) to understand how the construct manifests in teams engaged in innovation. Using interviews, we study six teams' perceptions of PS indicators and how they present differently on Slack (when compared to in-person interactions). We then leverage the interview insights to design Slack-based PS indicators. We present five broad facets of PS in hybrid teams, four perceived differences of PS on Slack compared to in-person, and 15 Slack-based, PS indicators--the groundwork for future automated PS measurement on instant-messaging platforms. These insights produce three design implications and illustrative design examples for ways instant-messaging platforms can support Psychologically Safe hybrid teams, and best practices for hybrid teams to support interpersonal risk-taking and build mutual respect.

</details>


### [5] [RPKT: Learning What You Don't -- Know Recursive Prerequisite Knowledge Tracing in Conversational AI Tutors for Personalized Learning](https://arxiv.org/abs/2508.11892)

*Jinwen Tang, Qiming Guo, Zhicheng Tang*

**Main category:** cs.HC

**Keywords:** Educational Technology, Knowledge Tracing, Large Language Models, Adaptive Learning, Personalized Education

**Relevance Score:** 8

**TL;DR:** The paper presents a Recursive Prerequisite Knowledge Tracing (RPKT) system that utilizes LLMs for dynamic discovery of knowledge prerequisites to enhance personalized learning in educational systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Educational systems struggle with the 'unknown unknowns' problem, where learners cannot identify their knowledge gaps. This paper aims to address this challenge.

**Method:** The RPKT system uses large language models to recursively trace prerequisite concepts in real-time, implementing binary assessment interfaces to reduce cognitive load and providing personalized learning paths based on identified gaps.

**Key Contributions:**

	1. Introduction of a novel system for prerequisite knowledge tracing in education using LLMs
	2. Real-time dynamic discovery of knowledge gaps
	3. Personalized learning paths generated without predefined curricula

**Result:** Demonstrations show the system can uncover multiple layers of prerequisite dependencies and create customized hierarchical learning sequences without predefined curricula.

**Limitations:** 

**Conclusion:** RPKT has significant potential to enhance personalized education technology and enable adaptive learning across various academic domains.

**Abstract:** Educational systems often assume learners can identify their knowledge gaps, yet research consistently shows that students struggle to recognize what they don't know they need to learn-the "unknown unknowns" problem. This paper presents a novel Recursive Prerequisite Knowledge Tracing (RPKT) system that addresses this challenge through dynamic prerequisite discovery using large language models. Unlike existing adaptive learning systems that rely on pre-defined knowledge graphs, our approach recursively traces prerequisite concepts in real-time until reaching a learner's actual knowledge boundary. The system employs LLMs for intelligent prerequisite extraction, implements binary assessment interfaces for cognitive load reduction, and provides personalized learning paths based on identified knowledge gaps. Demonstration across computer science domains shows the system can discover multiple nested levels of prerequisite dependencies, identify cross-domain mathematical foundations, and generate hierarchical learning sequences without requiring pre-built curricula. Our approach shows great potential for advancing personalized education technology by enabling truly adaptive learning across any academic domain.

</details>


### [6] [Playing telephone with generative models: "verification disability," "compelled reliance," and accessibility in data visualization](https://arxiv.org/abs/2508.12192)

*Frank Elavsky, Cindy Xiong Bearfield*

**Main category:** cs.HC

**Keywords:** accessibility, bias, data visualization, generative models, AI

**Relevance Score:** 8

**TL;DR:** The paper examines the challenges of using generative models in data visualization for accessibility, specifically regarding biases that affect users with visual impairments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the implications of relying on AI-generated descriptions in data visualization, especially for individuals who are blind or have low vision.

**Method:** Collaboration between experts in accessibility and bias; exploration of model failures and biases in AI-generated visualizations through observational studies.

**Key Contributions:**

	1. Highlights the dangers of AI bias in accessibility contexts.
	2. Proposes recommendations for technologists, users with disabilities, and researchers.
	3. Explores human reliance on AI outputs without verification.

**Result:** Identified that users cannot verify AI model outputs, leading to a forced reliance on models for interpreting data visualizations, which could exacerbate bias concerns.

**Limitations:** Focus primarily on AI-generated content without extensive empirical testing of user experiences.

**Conclusion:** The study underscores the need to address algorithmic bias in AI-driven visualization tools and suggests paths forward for various stakeholders.

**Abstract:** This paper is a collaborative piece between two worlds of expertise in the field of data visualization: accessibility and bias. In particular, the rise of generative models playing a role in accessibility is a worrying trend for data visualization. These models are increasingly used to help author visualizations as well as generate descriptions of existing visualizations for people who are blind, low vision, or use assistive technologies such as screen readers. Sighted human-to-human bias has already been established as an area of concern for theory, research, and design in data visualization. But what happens when someone is unable to verify the model output or adequately interrogate algorithmic bias, such as a context where a blind person asks a model to describe a chart for them? In such scenarios, trust from the user is not earned, rather reliance is compelled by the model-to-human relationship. In this work, we explored the dangers of AI-generated descriptions for accessibility, playing a game of telephone between models, observing bias production in model interpretation, and re-interpretation of a data visualization. We unpack ways that model failure in visualization is especially problematic for users with visual impairments, and suggest directions forward for three distinct readers of this piece: technologists who build model-assisted interfaces for end users, users with disabilities leveraging models for their own purposes, and researchers concerned with bias, accessibility, or visualization.

</details>


### [7] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)

*Esra Mehmedova, Santiago Berrezueta-Guzman, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** gaze extraction, dynamic heatmaps, attention patterns, Apple Vision Pro, iTrace

**Relevance Score:** 8

**TL;DR:** iTrace is a gaze extraction application for Apple Vision Pro that uses novel click-based methods to generate dynamic heatmaps, enabling detailed analysis of attention patterns in various settings.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy limitations of the Apple Vision Pro that restrict direct access to continuous gaze data, facilitating enhanced analysis of user attention across different contexts.

**Method:** The study introduces iTrace, a client-server architecture that incorporates manual and automatic click-based gaze extraction methods to produce heatmaps from user gaze coordinates.

**Key Contributions:**

	1. Introduction of click-based gaze extraction techniques for the Apple Vision Pro
	2. Dynamic visualization of attention patterns through heatmaps
	3. Demonstration of usability and performance across different click methods

**Result:** The click methods, particularly using the 8BitDo controller, achieved higher data collection rates compared to dwell control, allowing for more detailed heatmap visualizations that exhibit various attention patterns during different tasks.

**Limitations:** Current gaze data restrictions imposed by the Apple Vision Pro limit broader application of iTrace.

**Conclusion:** iTrace offers strong potential for applications in education, environmental design, marketing, and clinical assessments, while emphasizing its use should be confined to research settings due to data privacy concerns.

**Abstract:** The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet the privacy restrictions on the device prevent direct access to continuous user gaze data. This study introduces iTrace, a novel application that overcomes these limitations through click-based gaze extraction techniques, including manual methods like a pinch gesture, and automatic approaches utilizing dwell control or a gaming controller. We developed a system with a client-server architecture that captures the gaze coordinates and transforms them into dynamic heatmaps for video and spatial eye tracking. The system can generate individual and averaged heatmaps, enabling analysis of personal and collective attention patterns.   To demonstrate its effectiveness and evaluate the usability and performance, a study was conducted with two groups of 10 participants, each testing different clicking methods. The 8BitDo controller achieved higher average data collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell control, enabling significantly denser heatmap visualizations. The resulting heatmaps reveal distinct attention patterns, including concentrated focus in lecture videos and broader scanning during problem-solving tasks. By allowing dynamic attention visualization while maintaining a high gaze precision of 91 %, iTrace demonstrates strong potential for a wide range of applications in educational content engagement, environmental design evaluation, marketing analysis, and clinical cognitive assessment. Despite the current gaze data restrictions on the Apple Vision Pro, we encourage developers to use iTrace only in research settings.

</details>


### [8] [Sketchar: Supporting Character Design and Illustration Prototyping Using Generative AI](https://arxiv.org/abs/2508.12333)

*Long Ling, Xinyi Chen, Ruoyu Wen, Toby Jia-Jun Li, Ray LC*

**Main category:** cs.HC

**Keywords:** Generative AI, Game Design, Interdisciplinary Collaboration

**Relevance Score:** 6

**TL;DR:** Sketchar is a Generative AI tool designed to enhance communication between game designers and illustrators by allowing designers to prototype characters and generate images from concepts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional workflows in game character design struggle with communication barriers between designers and illustrators due to differing backgrounds and skills.

**Method:** A mixed-method study was conducted to evaluate designer interactions with the Sketchar tool.

**Key Contributions:**

	1. Development of Sketchar, a GenAI tool for character design
	2. Empirical evidence of improved communication in design workflows
	3. Demonstration of GenAI's utility for non-artistic designers

**Result:** Reference images generated by Sketchar improved design refinement and were found to be useful in real-world workflows, especially for designers lacking artistic skills.

**Limitations:** 

**Conclusion:** GenAI, as demonstrated in the Sketchar tool, can significantly improve interdisciplinary collaboration in gaming by enabling designers to work beyond their artistic limitations.

**Abstract:** Character design in games involves interdisciplinary collaborations, typically between designers who create the narrative content, and illustrators who realize the design vision. However, traditional workflows face challenges in communication due to the differing backgrounds of illustrators and designers, the latter with limited artistic abilities. To overcome these challenges, we created Sketchar, a Generative AI (GenAI) tool that allows designers to prototype game characters and generate images based on conceptual input, providing visual outcomes that can give immediate feedback and enhance communication with illustrators' next step in the design cycle. We conducted a mixed-method study to evaluate the interaction between game designers and Sketchar. We showed that the reference images generated in co-creating with Sketchar fostered refinement of design details and can be incorporated into real-world workflows. Moreover, designers without artistic backgrounds found the Sketchar workflow to be more expressive and worthwhile. This research demonstrates the potential of GenAI in enhancing interdisciplinary collaboration in the game industry, enabling designers to interact beyond their own limited expertise.

</details>


### [9] [System-driven Interactive Design Support for Cloud Architecture: A Qualitative User Experience Study with Novice Engineers](https://arxiv.org/abs/2508.12385)

*Ryosuke Kohita, Akira Kasuga*

**Main category:** cs.HC

**Keywords:** cloud architecture, system-driven support, novice engineers, educational tools, cloud design principles

**Relevance Score:** 4

**TL;DR:** This study investigates a system-driven support tool for novice engineers in cloud architecture design, highlighting its effective guidance and educational value.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges faced by novice engineers in cloud architecture design due to ambiguous requirements and complex trade-offs.

**Method:** Qualitative examination of the experiences of 60 novice engineers using a system-driven cloud design support tool.

**Key Contributions:**

	1. Identification of the value of structured system guidance for novices
	2. Demonstration of the educational benefits of simulating and comparing architecture options
	3. Recommendations for enhancing system-driven support tools based on user experiences

**Result:** Participants found that structured guidance improved engagement and understanding of cloud design principles, aiding in the creation of initial architectures without needing to craft prompts.

**Limitations:** The study focuses only on novice engineers and may not generalize to experienced practitioners.

**Conclusion:** Improvements in adaptive information delivery, validation of system outputs, and integration with implementation workflows can enhance the educational and practical value of such support tools.

**Abstract:** Cloud architecture design presents significant challenges due to the necessity of clarifying ambiguous requirements and systematically addressing complex trade-offs, especially for novice engineers with limited cloud experience. While recent advances in the use of AI tools have broadened available options, system-driven approaches that offer explicit guidance and step-by-step information management may be especially effective in supporting novices during the design process. This study qualitatively examines the experiences of 60 novice engineers using such a system-driven cloud design support tool. The findings indicate that structured and proactive system guidance helps novices engage more effectively in architectural design, especially when addressing tasks where knowledge and experience gaps are most critical. For example, participants found it easier to create initial architectures and did not need to craft prompts themselves. In addition, participants reported that the ability to simulate and compare multiple architecture options enabled them to deepen their understanding of cloud design principles and trade-offs, demonstrating the educational value of system-driven support. The study also identifies areas for improvement, including more adaptive information delivery tailored to user expertise, mechanisms for validating system outputs, and better integration with implementation workflows such as infrastructure-as-code generation and deployment guidance. Addressing these aspects can further enhance the educational and practical value of system-driven support tools for cloud architecture design.

</details>


### [10] [When motivation can be more than a message: designing agents to boost physical activity](https://arxiv.org/abs/2508.12388)

*Alessandro Silacci, Maurizio Caon, Mauro Cherubini*

**Main category:** cs.HC

**Keywords:** virtual agents, physical activity, human-computer interaction, social features, user trust

**Relevance Score:** 8

**TL;DR:** Explores how virtual agents in physical activity interventions can be perceived as co-participants rather than just instructors, emphasizing the importance of visible effort and authenticity in user-agent interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate user perceptions of virtual agents in promoting physical activity and to enhance their relational depth in interventions.

**Method:** Thematic analysis of semi-structured interviews with 12 participants from a physical activity intervention.

**Key Contributions:**

	1. Proposes design directions for virtual agents emphasizing co-experienced exertion
	2. Identifies the importance of visible effort in fostering trust
	3. Highlights skepticism towards virtual agent authenticity in performance

**Result:** Participants valued agents that appeared to exert visible effort and expressed skepticism towards those that did not seem authentic or relatable, leading to insights on user trust and motivation.

**Limitations:** 

**Conclusion:** Designing agents that effectively simulate co-experienced exertion can enhance engagement and support in physical activity.

**Abstract:** Virtual agents are commonly used in physical activity interventions to support behavior change, often taking the role of coaches that deliver encouragement and feedback. While effective for compliance, this role typically lacks relational depth. This pilot study explores how such agents might be perceived not just as instructors, but as co-participants: entities that appear to exert effort alongside users. Drawing on thematic analysis of semi-structured interviews with 12 participants from a prior physical activity intervention, we examine how users interpret and evaluate agent effort in social comparison contexts. Our findings reveal a recurring tension between perceived performance and authenticity. Participants valued social features when they believed others were genuinely trying. In contrast, ambiguous or implausible activity levels undermined trust and motivation. Many participants expressed skepticism toward virtual agents unless their actions reflected visible effort or were grounded in relatable human benchmarks. Based on these insights, we propose early design directions for fostering co-experienced exertion in agents, including behavioral cues, narrative grounding, and personalized performance. These insights contribute to the design of more engaging, socially resonant agents capable of supporting co-experienced physical activity.

</details>


### [11] [fCrit: A Visual Explanation System for Furniture Design Creative Support](https://arxiv.org/abs/2508.12416)

*Vuong Nguyen, Gabriel Vigliensoni*

**Main category:** cs.HC

**Keywords:** Explainable AI, Furniture Design, Human-Centered AI

**Relevance Score:** 5

**TL;DR:** fCrit is an AI system that critiques furniture design, focusing on explainability and user-centric dialogue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve explainability in AI systems used in creative fields, particularly furniture design, by tailoring AI feedback to users' design language and thought processes.

**Method:** fCrit employs a multi-agent architecture supported by a structured design knowledge base to generate critiques that align with users' cognitive framing and dialogue.

**Key Contributions:**

	1. Introduction of fCrit, a dialogue-based critique system for furniture design.
	2. Focus on explainability through adaptation to users' cognitive and design language.
	3. Advancement of methods in Human-Centered Explainable AI in creative domains.

**Result:** The system allows for tailored critiques that enhance understanding and engagement between the AI and users in the furniture design context.

**Limitations:** 

**Conclusion:** fCrit advances Human-Centered Explainable AI (HCXAI) in creative practices by providing domain-specific, dialogic, and visually grounded AI support.

**Abstract:** We introduce fCrit, a dialogue-based AI system designed to critique furniture design with a focus on explainability. Grounded in reflective learning and formal analysis, fCrit employs a multi-agent architecture informed by a structured design knowledge base. We argue that explainability in the arts should not only make AI reasoning transparent but also adapt to the ways users think and talk about their designs. We demonstrate how fCrit supports this process by tailoring explanations to users' design language and cognitive framing. This work contributes to Human-Centered Explainable AI (HCXAI) in creative practice, advancing domain-specific methods for situated, dialogic, and visually grounded AI support.

</details>


### [12] [Say It, See It: A Systematic Evaluation on Speech-Based 3D Content Generation Methods in Augmented Reality](https://arxiv.org/abs/2508.12498)

*Yanming Xiu, Joshua Chilukuri, Shunav Sen, Maria Gorlatova*

**Main category:** cs.HC

**Keywords:** augmented reality, 3D content generation, text-to-3D, user study, generative pipelines

**Relevance Score:** 7

**TL;DR:** This paper presents a modular edge-assisted architecture for generating 3D content from text and images for augmented reality applications, evaluating user satisfaction and performance trade-offs in 3D generation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the creation of 3D content for AR applications by leveraging generative pipelines driven by natural input, such as speech, instead of manual asset creation.

**Method:** The authors designed a modular architecture that supports both text-to-3D and text-image-to-3D pathways and implemented four pipelines. They conducted an IRB-approved user study with 11 participants to assess various perceptual and usability metrics across different object prompts.

**Key Contributions:**

	1. Modular architecture for text and image to 3D generation
	2. User study evaluating perceptual and usability metrics
	3. Insights into trade-offs between quality and generation speed in AR

**Result:** Text-image-to-3D pipelines provided higher generation quality, achieving an average satisfaction score of 4.55/5. The best pipeline combined FLUX for image generation and Trellis for 3D generation. Direct text-to-3D pipelines, while faster, showed lower user satisfaction due to perceived quality deficits.

**Limitations:** The study was limited to a small number of participants and specific object prompts, which may affect the generalizability of findings.

**Conclusion:** User satisfaction is more influenced by perceptual quality than generation speed; users tolerate longer generation times if the output quality meets their expectations. This has implications for future AR applications and the trade-offs necessary in current 3D generation methods.

**Abstract:** As augmented reality (AR) applications increasingly require 3D content, generative pipelines driven by natural input such as speech offer an alternative to manual asset creation. In this work, we design a modular, edge-assisted architecture that supports both direct text-to-3D and text-image-to-3D pathways, enabling interchangeable integration of state-of-the-art components and systematic comparison of their performance in AR settings. Using this architecture, we implement and evaluate four representative pipelines through an IRB-approved user study with 11 participants, assessing six perceptual and usability metrics across three object prompts. Overall, text-image-to-3D pipelines deliver higher generation quality: the best-performing pipeline, which used FLUX for image generation and Trellis for 3D generation, achieved an average satisfaction score of 4.55 out of 5 and an intent alignment score of 4.82 out of 5. In contrast, direct text-to-3D pipelines excel in speed, with the fastest, Shap-E, completing generation in about 20 seconds. Our results suggest that perceptual quality has a greater impact on user satisfaction than latency, with users tolerating longer generation times when output quality aligns with expectations. We complement subjective ratings with system-level metrics and visual analysis, providing practical insights into the trade-offs of current 3D generation methods for real-world AR deployment.

</details>


### [13] [Organization Matters: A Qualitative Study of Organizational Dynamics in Red Teaming Practices For Generative AI](https://arxiv.org/abs/2508.12504)

*Bixuan Ren, EunJeong Cheon, Jianghui Li*

**Main category:** cs.HC

**Keywords:** Generative AI, Red teaming, Organizational dynamics, User-centered design, AI risk mitigation

**Relevance Score:** 6

**TL;DR:** This paper examines the organizational factors affecting the effectiveness of red teaming in generative AI, highlighting challenges and proposing user-centered approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical need for effective red teaming in the face of generative AI risks, particularly focusing on organizational dynamics that hinder these efforts.

**Method:** Qualitative analysis based on 15 semi-structured interviews with red teamers from various organizations.

**Key Contributions:**

	1. Identification of organizational dynamics affecting red teaming effectiveness
	2. Emphasis on the need for user-centered red teaming approaches
	3. Insights from interviews with red teamers that highlight real-world challenges.

**Result:** Identified challenges include marginalization of vulnerable red teamers, visibility issues of AI risks for users until post-deployment, and a deficiency in user-centered approaches to red teaming.

**Limitations:** 

**Conclusion:** Embedding red teaming throughout the development cycle of GenAI systems can help mitigate organizational dynamics that impair its effectiveness.

**Abstract:** The rapid integration of generative artificial intelligence (GenAI) across diverse fields underscores the critical need for red teaming efforts to proactively identify and mitigate associated risks. While previous research primarily addresses technical aspects, this paper highlights organizational factors that hinder the effectiveness of red teaming in real-world settings. Through qualitative analysis of 15 semi-structured interviews with red teamers from various organizations, we uncover challenges such as the marginalization of vulnerable red teamers, the invisibility of nuanced AI risks to vulnerable users until post-deployment, and a lack of user-centered red teaming approaches. These issues often arise from underlying organizational dynamics, including organizational resistance, organizational inertia, and organizational mediocracy. To mitigate these dynamics, we discuss the implications of user research for red teaming and the importance of embedding red teaming throughout the entire development cycle of GenAI systems.

</details>


### [14] [Towards Adaptive External Communication in Autonomous Vehicles: A Conceptual Design Framework](https://arxiv.org/abs/2508.12518)

*Tram Thi Minh Tran, Judy Kay, Stewart Worrall, Marius Hoggenmueller, Callum Parker, Xinyan Yu, Julie Stephany Berrio Perez, Mao Shan, Martin Tomitsch*

**Main category:** cs.HC

**Keywords:** external human-machine interfaces, autonomous vehicles, adaptive communication

**Relevance Score:** 4

**TL;DR:** This paper presents a conceptual design framework for adaptive external human-machine interfaces (eHMIs) that improve communication between autonomous vehicles and road users by dynamically adjusting to context and user variability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance interaction between autonomous vehicles and external road actors, adapting to the needs of varying contexts and promoting inclusivity.

**Method:** A conceptual design framework with three layers: Input, Processing, and Output, developed through theory-led abstraction and expert discussions.

**Key Contributions:**

	1. Introduction of a structured framework for adaptive eHMIs
	2. Focus on inclusivity and scalability in vehicle communication
	3. Addressing long-standing limitations in existing eHMI research

**Result:** The framework provides a systematic approach to designing and analyzing adaptive eHMIs, potentially addressing limitations in current eHMI research.

**Limitations:** The framework is conceptual and requires empirical validation in real-world applications.

**Conclusion:** The proposed framework can help in creating more effective eHMIs while also introducing new ethical and technical considerations.

**Abstract:** External Human-Machine Interfaces (eHMIs) are key to facilitating interaction between autonomous vehicles and external road actors, yet most remain reactive and do not account for scalability and inclusivity. This paper introduces a conceptual design framework for adaptive eHMIs-interfaces that dynamically adjust communication as road actors vary and context shifts. Using the cyber-physical system as a structuring lens, the framework comprises three layers: Input (what the system detects), Processing (how the system decides), and Output (how the system communicates). Developed through theory-led abstraction and expert discussion, the framework helps researchers and designers think systematically about adaptive eHMIs and provides a structured tool to design, analyse, and assess adaptive communication strategies. We show how such systems may resolve longstanding limitations in eHMI research while raising new ethical and technical considerations.

</details>


### [15] [The Future of Tech Labor: How Workers are Organizing and Transforming the Computing Industry](https://arxiv.org/abs/2508.12579)

*Cella M. Sum, Anna Konvicka, Mona Wang, Sarah E. Fox*

**Main category:** cs.HC

**Keywords:** unionization, tech workers, collective action, worker organizing, CSCW

**Relevance Score:** 3

**TL;DR:** This study explores the motivations and challenges faced by U.S.-based tech worker-organizers in their unionization efforts, highlighting the fragmented and unstable work environments that tech workers navigate.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unionization efforts among tech workers and improve their working conditions against unethical practices.

**Method:** Interviews with 44 U.S.-based tech worker-organizers from various job roles to examine their motivations, strategies, challenges, and visions for labor organizing.

**Key Contributions:**

	1. Insights into the motivations and challenges of tech worker organizers.
	2. Identification of community building as a critical aspect of labor organizing.
	3. Suggestions for supporting tech workers in the CSCW field.

**Result:** Findings indicate that tech workers often encounter disempowerment due to precarious work environments, which complicate their organizing efforts. However, they are fostering a resilient community and political awareness to push for labor changes.

**Limitations:** 

**Conclusion:** The study identifies essential structural and ideological forces affecting tech workers and suggests ways for the CSCW community to support their labor organizing efforts.

**Abstract:** The tech industry's shifting landscape and the growing precarity of its labor force have spurred unionization efforts among tech workers. These workers turn to collective action to improve their working conditions and to protest unethical practices within their workplaces. To better understand this movement, we interviewed 44 U.S.-based tech worker-organizers to examine their motivations, strategies, challenges, and future visions for labor organizing. These workers included engineers, product managers, customer support specialists, QA analysts, logistics workers, gig workers, and union staff organizers. Our findings reveal that, contrary to popular narratives of prestige and privilege within the tech industry, tech workers face fragmented and unstable work environments which contribute to their disempowerment and hinder their organizing efforts. Despite these difficulties, organizers are laying the groundwork for a more resilient tech worker movement through community building and expanding political consciousness. By situating these dynamics within broader structural and ideological forces, we identify ways for the CSCW community to build solidarity with tech workers who are materially transforming our field through their organizing efforts.

</details>


### [16] [Using AI for User Representation: An Analysis of 83 Persona Prompts](https://arxiv.org/abs/2508.13047)

*Joni Salminen, Danial Amin, Bernard Jansen*

**Main category:** cs.HC

**Keywords:** user personas, large language models, HCI, persona generation, user representation

**Relevance Score:** 8

**TL;DR:** Analysis of prompts used to generate user personas with LLMs, revealing trends and implications for user representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models are utilized in generating user personas across multiple studies and the impact of these methods on user representation.

**Method:** Analyzed 83 persona prompts from 27 research articles focusing on the generation of user personas using LLMs.

**Key Contributions:**

	1. Identification of common trends in persona prompt usage with LLMs
	2. Highlighting the potential limitations of generated personas
	3. Discussion on implications for user representation in HCI

**Result:** Findings indicate that the majority of generated personas are single and often concise; while demographic attributes are consistently included, rich descriptions are less common.

**Limitations:** Limited by the scope of analyzed research articles and prompts; potential for unexamined biases in LLM-generated content.

**Conclusion:** The findings suggest a trend towards basic persona generation which may limit the depth of user representation in research; implications for designing personas in HCI are discussed.

**Abstract:** We analyzed 83 persona prompts from 27 research articles that used large language models (LLMs) to generate user personas. Findings show that the prompts predominantly generate single personas. Several prompts express a desire for short or concise persona descriptions, which deviates from the tradition of creating rich, informative, and rounded persona profiles. Text is the most common format for generated persona attributes, followed by numbers. Text and numbers are often generated together, and demographic attributes are included in nearly all generated personas. Researchers use up to 12 prompts in a single study, though most research uses a small number of prompts. Comparison and testing multiple LLMs is rare. More than half of the prompts require the persona output in a structured format, such as JSON, and 74% of the prompts insert data or dynamic variables. We discuss the implications of increased use of computational personas for user representation.

</details>


### [17] [Ashes or Breath: Exploring Moral Dilemmas of Life and Cultural Legacy through Mixed Reality Gaming](https://arxiv.org/abs/2508.13074)

*Black Sun, Ge Kacy Fu, Shichao Guo*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Human-Computer Interaction, ethical dilemmas, immersive learning, empathy

**Relevance Score:** 6

**TL;DR:** The paper presents a Mixed Reality game called 'Ashes or Breath' that immerses players in moral dilemmas to enhance ethical reflection and empathy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of traditional teaching methods for moral dilemmas, which often lack emotional engagement and depth.

**Method:** Developed a Mixed Reality game utilizing head-mounted displays that places players in ethical crises, designed through an iterative values-centered process.

**Key Contributions:**

	1. Introduced a Mixed Reality game for ethical training
	2. Enhanced moral engagement through embodied interaction
	3. Demonstrated the use of MR for ethical decision-making

**Result:** Preliminary evaluations indicate the game intensifies empathy, deepens introspection, and prompts users to reconsider moral assumptions in immersive settings.

**Limitations:** 

**Conclusion:** Embedding moral dilemmas into everyday environments via MR-HMDs provides a powerful framework for ethics-based experiential learning in HCI.

**Abstract:** Traditional approaches to teaching moral dilemmas often rely on abstract, disembodied scenarios that limit emotional engagement and reflective depth. To address this gap, we developed \textit{Ashes or Breath}, a Mixed Reality game delivered via head-mounted displays(MR-HMDs). This places players in an ethical crisis: they must save a living cat or a priceless cultural artifact during a museum fire. Designed through an iterative, values-centered process, the experience leverages embodied interaction and spatial immersion to heighten emotional stakes and provoke ethical reflection. Players face irreversible, emotionally charged choices followed by narrative consequences in a reflective room, exploring diverse perspectives and societal implications. Preliminary evaluations suggest that embedding moral dilemmas into everyday environments via MR-HMDs intensifies empathy, deepens introspection, and encourages users to reconsider their moral assumptions. This work contributes to ethics-based experiential learning in HCI, positioning augmented reality not merely as a medium of interaction but as a stage for ethical encounter.

</details>


### [18] [At the Speed of the Heart: Evaluating Physiologically-Adaptive Visualizations for Supporting Engagement in Biking Exergaming in Virtual Reality](https://arxiv.org/abs/2508.13095)

*Oliver Hein, Sandra Wackerl, Changkun Ou, Florian Alt, Francesco Chiossi*

**Main category:** cs.HC

**Keywords:** exergames, wearable devices, physiological adaptation

**Relevance Score:** 7

**TL;DR:** A VR cycling simulator adapts based on users' heart rate zones to enhance engagement and control during exercise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in maintaining safe and effective intensity levels in exergames and leverage wearable physiological data for better user experiences.

**Method:** A user study with 50 participants compared eight visualization designs, followed by a lab study with 18 participants evaluating a physiology-adaptive exergame that adjusts visual feedback based on real-time heart rate data.

**Key Contributions:**

	1. Development of a VR cycling simulator that adapts to heart rate zones
	2. User studies evaluating visualization designs for engagement and control
	3. Demonstrated potential for improving workout regulation during exergaming

**Result:** Gamified elements, like NPCs, showed promise in enhancing engagement and exertion control. The system helped users maintain target heart rate zones without significant changes in exertion, enjoyment, or motivation ratings.

**Limitations:** Limited to a specific VR environment and small sample sizes in user studies.

**Conclusion:** Real-time physiological adaptation through NPC visualizations can improve workout regulation in exergaming, potentially making exercise more effective and engaging.

**Abstract:** Many exergames face challenges in keeping users within safe and effective intensity levels during exercise. Meanwhile, although wearable devices continuously collect physiological data, this information is seldom leveraged for real-time adaptation or to encourage user reflection. We designed and evaluated a VR cycling simulator that dynamically adapts based on users' heart rate zones. First, we conducted a user study (N=50) comparing eight visualization designs to enhance engagement and exertion control, finding that gamified elements like non-player characters (NPCs) were promising for feedback delivery. Based on these findings, we implemented a physiology-adaptive exergame that adjusts visual feedback to keep users within their target heart rate zones. A lab study (N=18) showed that our system has potential to help users maintain their target heart rate zones. Subjective ratings of exertion, enjoyment, and motivation remained largely unchanged between conditions. Our findings suggest that real-time physiological adaptation through NPC visualizations can improve workout regulation in exergaming.

</details>


### [19] [Choosing the Right Engine in the Virtual Reality Landscape](https://arxiv.org/abs/2508.13116)

*Santiago Berrezueta-Guzman, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Game Engines, Unreal Engine, Unity, AI Enhancements

**Relevance Score:** 6

**TL;DR:** A comparative analysis of Unreal Engine and Unity for VR development, assessing their capabilities and impact of AI enhancements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and compare the capabilities and trade-offs of Unreal Engine and Unity in the context of VR development, providing insights for developers.

**Method:** Empirical assessments and real-world case studies of large-scale VR projects, analyzing rendering fidelity, computational efficiency, and development workflows.

**Key Contributions:**

	1. Comprehensive comparative analysis of Unreal Engine and Unity.
	2. Insights into AI-driven enhancements in VR development.
	3. Practical recommendations for VR developers based on empirical data.

**Result:** Findings reveal key factors for choosing between the engines, including performance optimization and cross-platform compatibility, alongside the influence of AI-driven enhancements.

**Limitations:** 

**Conclusion:** Developers can optimize their VR projects by selecting the right engine based on technical requirements and leveraging AI technologies.

**Abstract:** Virtual reality (VR) development relies on game engines to provide real-time rendering, physics simulation, and interaction systems. Among the most widely used game engines, Unreal Engine and Unity dominate the industry, offering distinct advantages in graphics rendering, performance optimization, usability, resource requirements, and scalability. This study presents a comprehensive comparative analysis of both engines, evaluating their capabilities and trade-offs through empirical assessments and real-world case studies of large-scale VR projects. The findings highlight key factors such as rendering fidelity, computational efficiency, cross-platform compatibility, and development workflows. These provide practical insights for selecting the most suitable engine based on project-specific needs. Furthermore, emerging trends in artificial intelligence (AI)-driven enhancements, including Deep Learning Super Sampling (DLSS) and large language models (LLMs), are explored to assess their impact on VR development workflows. By aligning engine capabilities with technical and creative requirements, developers can overcome performance bottlenecks, enhance immersion, and streamline optimization techniques.   This study serves as a valuable resource for VR developers, researchers, and industry professionals, offering data-driven recommendations to navigate the evolving landscape of VR technology.

</details>


### [20] [Human Digital Twin: Data, Models, Applications, and Challenges](https://arxiv.org/abs/2508.13138)

*Rong Pan, Hongyue Sun, Xiaoyu Chen, Giulia Pedrielli, Jiapeng Huang*

**Main category:** cs.HC

**Keywords:** Human Digital Twins, machine learning, healthcare, anomaly detection, precision medicine

**Relevance Score:** 9

**TL;DR:** This paper reviews current approaches to Human Digital Twins (HDTs) in healthcare, emphasizing statistical and machine learning techniques, data integration, and ethical challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to explore how Human Digital Twins (HDTs) can enhance personalized healthcare through the integration of various data types for accurate health predictions.

**Method:** The paper reviews existing literature and methodologies related to HDT modeling, focusing on statistical methods, machine learning techniques for anomaly detection, and computational approaches for data integration.

**Key Contributions:**

	1. Overview of HDT modeling techniques
	2. Analysis of machine learning applications in health monitoring
	3. Discussion of ethical and regulatory issues in HDT deployment

**Result:** The review highlights the capabilities of HDTs in diagnosing health conditions, planning treatments, and predicting health trajectories, as well as identifies recent advances in related technologies.

**Limitations:** Limited exploration of specific case studies in real-world applications.

**Conclusion:** HDTs present significant potential for precision healthcare, but their deployment is hampered by ethical, technological, and regulatory challenges that must be addressed.

**Abstract:** Human digital twins (HDTs) are dynamic, data-driven virtual representations of individuals, continuously updated with multimodal data to simulate, monitor, and predict health trajectories. By integrating clinical, physiological, behavioral, and environmental inputs, HDTs enable personalized diagnostics, treatment planning, and anomaly detection. This paper reviews current approaches to HDT modeling, with a focus on statistical and machine learning techniques, including recent advances in anomaly detection and failure prediction. It also discusses data integration, computational methods, and ethical, technological, and regulatory challenges in deploying HDTs for precision healthcare.

</details>


### [21] [Development and User Experiences of a Novel Virtual Reality Task for Poststroke Visuospatial Neglect: An Exploratory Pilot Study](https://arxiv.org/abs/2312.12399)

*Andrew Danso, Patti Nijhuis, Alessandro Ansani, Martin Hartmann, Gulnara Minkkinen, Geoff Luck, Joshua S. Bamford, Sarah Faber, Kat R. Agres, Solange Glasser, Teppo SÃ¤rkÃ¤mÃ¶, Rebekah Rousi, Marc R. Thompson*

**Main category:** cs.HC

**Keywords:** virtual reality, visuospatial neglect, rehabilitation, audiovisual cues, physiotherapy

**Relevance Score:** 6

**TL;DR:** This study investigates the use of virtual reality (VR) as a rehabilitation tool for patients with visuospatial neglect (VSN), highlighting initial patient and physiotherapist experiences with an audiovisual cue task designed to improve spatial awareness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore innovative rehabilitation methods for individuals suffering from visuospatial neglect (VSN) which impairs their spatial awareness and functionality.

**Method:** A VR rehabilitation task was co-designed with physiotherapists, tested on two patients with VSN over 12 sessions, using audiovisual cues to engage neglected spatial areas.

**Key Contributions:**

	1. Innovative use of VR for VSN rehabilitation
	2. Co-designing rehabilitation tasks with physiotherapists
	3. Initial evidence of performance changes in patients with VSN using VR

**Result:** Initial testing with physiotherapists showed high usability and safety, while patient results indicated varied performance improvements: one patient showed nonsignificant improvements, while the other demonstrated significant reductions in task completion times over sessions.

**Limitations:** Limited sample size and scope limit generalizability of findings.

**Conclusion:** The findings suggest potential for VR intervention in VSN rehabilitation, but further research with larger cohorts is needed for validation.

**Abstract:** Background: Visuospatial neglect (VSN) affects spatial awareness, leading to functional and motor challenges. This case study explores virtual reality (VR) as a potential complementary tool for VSN rehabilitation.   Objective: Specifically, we aim to explore the initial experiences of patients and physiotherapists engaging with a novel protocol, using an audiovisual cue task to support VSN rehabilitation.   Methods: A preliminary VR task integrating audiovisual cues was co-designed with 2 physiotherapists. The task was then tested with 2 patients with VSN over 12 sessions. The intervention focused on engaging neglected spatial areas, with physiotherapists adapting the task to individual needs and monitoring responses.   Results: Initial testing with 2 trainee physiotherapists indicated high usability, engagement, and perceived safety. Two patients with VSN completed 12 VR sessions. For Patient A, completion times increased following the introduction of an audio cue, though modeling indicated a nonsignificant linear trend (beta = 0.08; P = .33) and a marginally significant downward curvature (beta = -0.001; P = .08). In contrast, Patient B showed a significant linear decrease in completion times (beta = -0.53; P = .009), with a quadratic trend indicating a performance minimum around session 10 (B = 0.007; P = .04). Intraweek variability also decreased. Motor scores (Box and Block Test and 9-Hole Peg Test) remained stable, and subjective feedback indicated improved mobility confidence and positive task engagement.   Conclusions: Further research with larger cohorts is needed to confirm the VR task's utility and refine the intervention.

</details>


### [22] [VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels](https://arxiv.org/abs/2410.12268)

*Chen Chen, Hannah K. Bako, Peihong Yu, John Hooker, Jeffrey Joyal, Simon C. Wang, Samuel Kim, Jessica Wu, Aoxue Ding, Lara Sandeep, Alex Chen, Chayanika Sinha, Zhicheng Liu*

**Main category:** cs.HC

**Keywords:** chart corpus, visualization research, semantic labeling, SVG charts, AI applications

**Relevance Score:** 6

**TL;DR:** This paper presents VISANATOMY, a chart corpus containing 942 real-world SVG charts with fine-grained semantic labels, aimed at enhancing visualization research and applications in AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current chart corpora have high-level labels that limit their utility; VISANATOMY aims to provide more detailed labeling to support broader applications.

**Method:** The corpus includes 942 SVG charts with over 383k labeled graphical elements, providing multi-level semantic information on types, roles, positions, groupings, and layouts of chart elements.

**Key Contributions:**

	1. Introduction of VISANATOMY corpus with detailed semantic labeling
	2. Comparison demonstrating the superiority of VISANATOMY to existing corpora
	3. Showcasing practical applications of the corpus in visualization research

**Result:** VISANATOMY shows superiority over existing chart corpora by allowing for applications like semantic role inference and accessibility content navigation.

**Limitations:** 

**Conclusion:** The paper highlights the potential of VISANATOMY to advance research in visualization and propose future improvements.

**Abstract:** Chart corpora, which comprise data visualizations and their semantic labels, are crucial for advancing visualization research. However, the labels in most existing corpora are high-level (e.g., chart types), hindering their utility for broader applications in the era of AI. In this paper, we contribute VISANATOMY, a corpus containing 942 real-world SVG charts produced by over 50 tools, encompassing 40 chart types and featuring structural and stylistic design variations. Each chart is augmented with multi-level fine-grained labels on its semantic components, including each graphical element's type, role, and position, hierarchical groupings of elements, group layouts, and visual encodings. In total, VISANATOMY provides labels for more than 383k graphical elements. We demonstrate the richness of the semantic labels by comparing VISANATOMY with existing corpora. We illustrate its usefulness through four applications: semantic role inference for SVG elements, chart semantic decomposition, chart type classification, and content navigation for accessibility. Finally, we discuss research opportunities to further improve VISANATOMY.

</details>


### [23] [Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process](https://arxiv.org/abs/2504.12488)

*Mohi Reza, Jeb Thomas-Mitchell, Peter Dushniku, Nathan Laundry, Joseph Jay Williams, Anastasia Kuzminykh*

**Main category:** cs.HC

**Keywords:** AI Writing Support, Human-Computer Interaction, Writers' Agency

**Relevance Score:** 8

**TL;DR:** This paper explores the impact of AI tools on the writing process and the preservation of writers' agency through a systematic review and interviews.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how generative AI affects writers' sense of agency and ownership during the writing process.

**Method:** Conducted a systematic review of 109 HCI papers using the PRISMA approach and complemented it with interviews of 15 writers from diverse backgrounds.

**Key Contributions:**

	1. Systematic review identifying key design strategies for AI writing support.
	2. Insights into how writers' preferences for AI intervention vary across the writing process.
	3. Recommendations for developing co-writing AI tools that respect human agency.

**Result:** Identified four design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback, along with insights on how different types of writers value AI intervention at various stages of writing.

**Limitations:** 

**Conclusion:** The study highlights the differences in preferences for AI assistance based on writers' goals and their implications for designing human-centered writing tools.

**Abstract:** As generative AI tools like ChatGPT become integral to everyday writing, critical questions arise about how to preserve writers' sense of agency and ownership when using these tools. Yet, a systematic understanding of how AI assistance affects different aspects of the writing process - and how this shapes writers' agency - remains underexplored. To address this gap, we conducted a systematic review of 109 HCI papers using the PRISMA approach. From this literature, we identify four overarching design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback - mapped across the four key cognitive processes in writing: planning, translating, reviewing, and monitoring. We complement this analysis with interviews of 15 writers across diverse domains. Our findings reveal that writers' desired levels of AI intervention vary across the writing process: content-focused writers (e.g., academics) prioritize ownership during planning, while form-focused writers (e.g., creatives) value control over translating and reviewing. Writers' preferences are also shaped by contextual goals, values, and notions of originality and authorship. By examining when ownership matters, what writers want to own, and how AI interactions shape agency, we surface both alignment and gaps between research and user needs. Our findings offer actionable design guidance for developing human-centered writing tools for co-writing with AI, on human terms.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [24] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)

*Maksym Shamrai, Vladyslav Hamolia*

**Main category:** cs.CL

**Keywords:** large language models, language representation, linguistic connections, metric space, machine learning

**Relevance Score:** 9

**TL;DR:** A novel framework utilizing LLM internal weight activations to create a metric space of languages, uncovering linguistic connections across 106 languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an automated method for representing linguistic characteristics without relying on hand-crafted features.

**Method:** The method computes weight importance scores using an adapted pruning algorithm to generate high-dimensional vector representations of languages.

**Key Contributions:**

	1. Development of a novel framework for language representation using LLMs
	2. Discovery of unexpected linguistic connections across languages
	3. Public availability of source code and tools for further research.

**Result:** The approach aligns well with established linguistic families and uncovers unexpected relationships among languages, suggesting historical contacts or language evolution.

**Limitations:** 

**Conclusion:** The framework offers insights into language relationships and is a significant advancement in linguistic analysis using LLMs.

**Abstract:** We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [25] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)

*Jonas van Elburg, Peter van der Putten, Maarten Marx*

**Main category:** cs.CL

**Keywords:** synthetic data, large language models, question-answering, human-benchmarks, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** The paper explores the effectiveness of synthetic QA data from LLMs as a substitute for human-labeled benchmarks, revealing that while it can rank RAG variants accurately in some scenarios, it struggles with generator comparison due to task and stylistic mismatches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if synthetic question-answer data from large language models can replace human-labeled benchmarks when such data is scarce.

**Method:** Two experiments were conducted: one with variable retriever parameters and fixed generators, and another with fixed retrievers and varying generators across four datasets (two open-domain and two proprietary).

**Key Contributions:**

	1. Assessment of synthetic QA data as substitutes for human benchmarks
	2. Evaluation of retriever and generator configurations on benchmark reliability
	3. Insights into limitations of synthetic benchmarks in RAG architecture comparisons

**Result:** Synthetic benchmarks were found to reliably rank RAGs based on retriever parameters, aligning well with human-labeled benchmarks, but inconsistencies arose when comparing different generator architectures.

**Limitations:** The inconsistency in RAG rankings when comparing generators suggests significant limitations in task alignment and potential stylistic bias.

**Conclusion:** The observed breakdown in ranking consistency suggests that mismatches in task requirements and biases in generator styles affect the reliability of synthetic benchmarks.

**Abstract:** We investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable. We assess the reliability of synthetic benchmarks across two experiments: one varying retriever parameters while keeping the generator fixed, and another varying the generator with fixed retriever parameters. Across four datasets, of which two open-domain and two proprietary, we find that synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.

</details>


### [26] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)

*Noah Kasmanoff, Rahul Zalkikar*

**Main category:** cs.CL

**Keywords:** Imitation Learning, Conversational AI, Dialog Models

**Relevance Score:** 7

**TL;DR:** This paper applies imitation learning to conversational AI, recovering a talking policy and a discriminator that assesses conversation quality. It highlights the limitations of dialog models based on discriminator results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance conversational AI by using imitation learning in dialog systems and analyze the effectiveness of such models through expert demonstrations.

**Method:** The paper implements imitation learning by training a policy on expert conversation demonstrations and developing a discriminator to distinguish between expert and synthetic conversations.

**Key Contributions:**

	1. Application of imitation learning to conversational AI
	2. Development of a discriminator for evaluating dialog quality
	3. Identification of limitations in dialog models through empirical results

**Result:** The developed policy demonstrates effective conversational abilities, while the discriminator's results reveal limitations in current dialog models.

**Limitations:** The discriminator indicates significant limitations in existing dialog models, suggesting the need for further refinement.

**Conclusion:** The findings suggest that imitation learning can be an effective approach for training conversational agents, but also highlight the need for further improvements in dialog models to mitigate adverse behaviors.

**Abstract:** Imitation learning is a proven method for creating a policy in the absence of rewards, by leveraging expert demonstrations. In this work, we apply imitation learning to conversation. In doing so, we recover a policy capable of talking to a user given a prompt (input state), and a discriminator capable of classifying between expert and synthetic conversation. While our policy is effective, we recover results from our discriminator that indicate the limitations of dialog models. We argue that this technique can be used to identify adverse behavior of arbitrary data models common for dialog oriented tasks.

</details>


### [27] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)

*Leo Peckham, Michael Ong, Naomi Nagy, Ewan Dunbar*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, transcription inconsistencies, bigram modeling

**Relevance Score:** 4

**TL;DR:** This study analyzes transcription inconsistencies in the Faetar ASR benchmark and evaluates the impact of bigram modeling and finite lexicon constraints.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate transcription inconsistencies in the Faetar Automatic Speech Recognition benchmark and their effects on ASR performance in a low-resource context.

**Method:** A small, hand-constructed lexicon was utilized to examine transcription inconsistencies and evaluate the effectiveness of bigram word-based language modeling and finite lexicon decoding.

**Key Contributions:**

	1. Analysis of transcription inconsistencies in ASR tasks
	2. Evaluation of bigram language modeling's effectiveness
	3. Demonstration of finite lexicon constraints improving ASR performance

**Result:** While transcription inconsistencies exist, they are not the primary challenge; bigram modeling offers no additional benefits, and using a finite lexicon can provide improvement.

**Limitations:** Focuses only on transcription inconsistencies without exploring other ASR challenges.

**Conclusion:** The task remains extremely challenging despite addressing transcription issues and modeling techniques.

**Abstract:** We examine the role of transcription inconsistencies in the Faetar Automatic Speech Recognition benchmark, a challenging low-resource ASR benchmark. With the help of a small, hand-constructed lexicon, we conclude that find that, while inconsistencies do exist in the transcriptions, they are not the main challenge in the task. We also demonstrate that bigram word-based language modelling is of no added benefit, but that constraining decoding to a finite lexicon can be beneficial. The task remains extremely difficult.

</details>


### [28] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)

*Tianyi Li, Yu Qin, Olivia R. Liu Sheng*

**Main category:** cs.CL

**Keywords:** large language models, academic peer review, text processing, scientific discovery, performance evaluation

**Relevance Score:** 8

**TL;DR:** This paper evaluates the potential of large language models (LLMs) in aiding academic peer review with various tasks, discovering limitations in their text-processing capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The potential of large language models (LLMs) to assist scientific discovery, particularly in the realm of academic peer review, is under scrutiny.

**Method:** The study organizes computer science tasks related to academic text processing into a workflow to assess LLMs, specifically focusing on content reproduction, comparison, scoring, and reflection roles of LLMs.

**Key Contributions:**

	1. Proposed a workflow for assessing LLMs in academic context
	2. Conducted rigorous performance evaluation using top Information Systems journals
	3. Documented specific limitations of LLMs in text-related tasks

**Result:** The evaluation of Google's Gemini revealed compromised performance in text processing tasks, with reliability in summaries and paraphrases but limitations in text ranking and grading.

**Limitations:** The paper primarily focuses on one LLM and does not explore comparative analysis with other models.

**Conclusion:** The study does not recommend the unchecked use of LLMs in peer reviews due to their inconsistent and often inadequate performance.

**Abstract:** How much large language models (LLMs) can aid scientific discovery, notably in assisting academic peer review, is in heated debate. Between a literature digest and a human-comparable research assistant lies their practical application potential. We organize individual tasks that computer science studies employ in separate terms into a guided and robust workflow to evaluate LLMs' processing of academic text input. We employ four tasks in the assessment: content reproduction/comparison/scoring/reflection, each demanding a specific role of the LLM (oracle/judgmental arbiter/knowledgeable arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs with questions that increasingly require intellectual capabilities towards a solid understanding of scientific texts to yield desirable solutions. We exemplify a rigorous performance evaluation with detailed instructions on the prompts. Adopting first-rate Information Systems articles at three top journals as the input texts and an abundant set of text metrics, we record a compromised performance of the leading LLM - Google's Gemini: its summary and paraphrase of academic text is acceptably reliable; using it to rank texts through pairwise text comparison is faintly scalable; asking it to grade academic texts is prone to poor discrimination; its qualitative reflection on the text is self-consistent yet hardly insightful to inspire meaningful research. This evidence against an endorsement of LLMs' text-processing capabilities is consistent across metric-based internal (linguistic assessment), external (comparing to the ground truth), and human evaluation, and is robust to the variations of the prompt. Overall, we do not recommend an unchecked use of LLMs in constructing peer reviews.

</details>


### [29] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)

*Krishna Chaitanya Marturi, Heba H. Elwazzan*

**Main category:** cs.CL

**Keywords:** Text Simplification, hallucination detection, LLMs, CLEF 2025, SimpleText

**Relevance Score:** 7

**TL;DR:** This paper presents a two-stage approach for scientific text simplification using large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for effective simplification of scientific texts to improve accessibility and understanding.

**Method:** The methodology includes generating structured plans for sentence-level simplification and producing concise summaries for document-level simplification, both guided by LLMs.

**Key Contributions:**

	1. Two-stage, LLM-based simplification framework
	2. Use of structured planning for sentence-level simplification
	3. Guided document simplification through concise summaries

**Result:** The proposed framework improves coherence and contextual fidelity in simplifications of scientific text.

**Limitations:** 

**Conclusion:** Leveraging LLMs in both sentence and document-level processes enhances the quality of scientific text simplification.

**Abstract:** In this paper, we present our approach for the CLEF 2025 SimpleText Task 1, which addresses both sentence-level and document-level scientific text simplification. For sentence-level simplification, our methodology employs large language models (LLMs) to first generate a structured plan, followed by plan-driven simplification of individual sentences. At the document level, we leverage LLMs to produce concise summaries and subsequently guide the simplification process using these summaries. This two-stage, LLM-based framework enables more coherent and contextually faithful simplifications of scientific text.

</details>


### [30] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)

*Krishna Chaitanya Marturi, Heba H. Elwazzan*

**Main category:** cs.CL

**Keywords:** Text Simplification, hallucination detection, LLMs, CLEF 2025, SimpleText

**Relevance Score:** 7

**TL;DR:** This paper discusses a methodology for detecting and evaluating creative generation and information distortion in scientific text simplification for the CLEF 2025 SimpleText Task 2.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of text simplification by detecting spurious generation and information distortion.

**Method:** An ensemble framework is constructed using a BERT-based classifier, semantic similarity measures, a natural language inference model, and LLM reasoning, combined with meta-classifiers.

**Key Contributions:**

	1. Development of an ensemble framework for text simplification
	2. Incorporation of LLM for post-editing of generated texts
	3. Improvement in detecting hallucinations and distortions in scientific simplification.

**Result:** The integration of multiple strategies enhances the robustness of detecting distortions and spurious generation in simplified texts.

**Limitations:** The effectiveness of the approach may vary based on the complexity of original texts and the training of the models used.

**Conclusion:** A grounded generation approach with an LLM-based post-editing system effectively revises simplifications based on original texts.

**Abstract:** In this paper, we describe our methodology for the CLEF 2025 SimpleText Task 2, which focuses on detecting and evaluating creative generation and information distortion in scientific text simplification. Our solution integrates multiple strategies: we construct an ensemble framework that leverages BERT-based classifier, semantic similarity measure, natural language inference model, and large language model (LLM) reasoning. These diverse signals are combined using meta-classifiers to enhance the robustness of spurious and distortion detection. Additionally, for grounded generation, we employ an LLM-based post-editing system that revises simplifications based on the original input texts.

</details>


### [31] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)

*Michael Flor, Xinyi Liu, Anna Feldman*

**Main category:** cs.CL

**Keywords:** idioms, psycholinguistics, computational linguistics, datasets, idiomaticity

**Relevance Score:** 5

**TL;DR:** This survey reviews idiom datasets in psycholinguistics and computational linguistics, analyzing trends and gaps between the two fields.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Idioms are complex phrases challenging to understand and process, necessitating a focused study on available datasets for effective usage in research.

**Method:** The survey reviews and analyzes 53 idiom datasets from psycholinguistics and computational linguistics, categorizing them based on content, form, and intended use.

**Key Contributions:**

	1. Comprehensive review of idiom datasets across two fields
	2. Identification of annotation trends and gaps
	3. Insights into idiomaticity detection and classification tasks

**Result:** The analysis identifies trends in annotation practices, coverage, and task framing, revealing a lack of correlation between psycholinguistic and computational approaches to idioms.

**Limitations:** Lack of correlation between psycholinguistic and computational research limits the potential for integrating findings.

**Conclusion:** Despite advancements in language coverage and task diversity in recent datasets, integration of insights from psycholinguistics and computational linguistics is still missing.

**Abstract:** Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies. This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. We present trends in annotation practices, coverage, and task framing across 53 datasets. Although recent efforts expanded language coverage and task diversity, there seems to be no relation yet between psycholinguistic and computational research on idioms.

</details>


### [32] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)

*Leigh Levinson, Christopher J. Agostino*

**Main category:** cs.CL

**Keywords:** human-computer interaction, large language models, biological rhythms, societal biases, contextual AI

**Relevance Score:** 9

**TL;DR:** The paper proposes a framework that integrates hormonal cycles into Large Language Models (LLMs) to enhance context relevance in AI systems, demonstrating performance variations in line with biological rhythms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** AI systems face challenges in determining contextually relevant information due to the frame problem; this work explores biological rhythms as filters for relevance.

**Method:** The framework embeds menstrual and circadian cycles into LLMs using prompts generated from periodic functions representing key hormones.

**Key Contributions:**

	1. A framework integrating biological rhythms into LLMs to enhance relevance filtering.
	2. Empirical evidence of emotional variations in language correlating with hormonal cycles.
	3. Insights into gender biases embedded in AI systems through language.

**Result:** Linguistic analysis showed emotional and stylistic variations corresponding to biological phases, with performance benchmarks revealing consistent variations aligning with hormonal cycles.

**Limitations:** The focus on hormonal cycles may not encompass the full complexity of human emotional and cognitive variability; further research is needed.

**Conclusion:** This method not only improves contextual AI but also highlights how societal biases related to gender and biology manifest in language models.

**Abstract:** Despite significant advances, AI systems struggle with the frame problem: determining what information is contextually relevant from an exponentially large possibility space. We hypothesize that biological rhythms, particularly hormonal cycles, serve as natural relevance filters that could address this fundamental challenge. We develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones including estrogen, testosterone, and cortisol. Across multiple state-of-the-art models, linguistic analysis reveals emotional and stylistic variations that track biological phases; sadness peaks during menstruation while happiness dominates ovulation and circadian patterns show morning optimism transitioning to nocturnal introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates subtle but consistent performance variations aligning with biological expectations, including optimal function in moderate rather than extreme hormonal ranges. This methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.

</details>


### [33] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)

*Julia Sammartino, Libby Barak, Jing Peng, Anna Feldman*

**Main category:** cs.CL

**Keywords:** euphemism detection, cross-lingual transfer, sequential fine-tuning, low-resource languages, multilingual models

**Relevance Score:** 6

**TL;DR:** The paper explores the impact of cross-lingual transfer through sequential fine-tuning on euphemism detection in five languages, demonstrating improved performance in low-resource languages like Yoruba and Turkish.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Euphemisms vary culturally and are ambiguous, which complicates euphemism detection for language models, particularly in low-resource settings.

**Method:** We compare sequential fine-tuning with monolingual and simultaneous fine-tuning techniques on models XLM-R and mBERT across languages including English, Spanish, Chinese, Turkish, and Yoruba.

**Key Contributions:**

	1. Sequential fine-tuning improves euphemism detection across multiple languages.
	2. Analysis of language pairings and typological features affecting performance.
	3. Comparison between XLM-R and mBERT in the context of euphemism detection.

**Result:** Sequential fine-tuning with a high-resource language improves detection performance in low-resource languages. XLM-R shows larger improvements but is sensitive to pretraining gaps, while mBERT offers stable results at lower performance.

**Limitations:** The results are sensitive to pretraining gaps and model selection may affect generalizability.

**Conclusion:** Sequential fine-tuning is an effective method for enhancing euphemism detection in multilingual models, especially beneficial for low-resource languages.

**Abstract:** Euphemisms are culturally variable and often ambiguous, posing challenges for language models, especially in low-resource settings. This paper investigates how cross-lingual transfer via sequential fine-tuning affects euphemism detection across five languages: English, Spanish, Chinese, Turkish, and Yoruba. We compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages like Yoruba and Turkish. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable, though lower, results. These findings highlight sequential fine-tuning as a simple yet effective strategy for improving euphemism detection in multilingual models, particularly when low-resource languages are involved.

</details>


### [34] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)

*Andrei-Valentin TÄnase, Elena Pelican*

**Main category:** cs.CL

**Keywords:** Tokenization, Natural Language Processing, Semantic Units, Machine Learning, Language Model Performance

**Relevance Score:** 8

**TL;DR:** SupraTok is a novel tokenization architecture that improves efficiency in natural language processing by learning semantic multi-word units and optimizing training data for better performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in model architectures, tokenization remains a bottleneck in natural language processing, necessitating innovative strategies for improvement.

**Method:** SupraTok introduces cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning to learn 'superword' tokens and enhance tokenization efficiency.

**Key Contributions:**

	1. Introduction of 'superword' tokens for improved semantic representation
	2. Improvements in tokenization efficiency surpassing existing tokenizers
	3. Competitive performance across 38 languages with a single model integration

**Result:** SupraTok achieves a 31% improvement in English tokenization efficiency and outperforms existing tokenizers across 38 languages, with significant gains in specific NLP benchmarks when integrated with a GPT-2 model.

**Limitations:** Further validation at larger model scales is necessary to fully understand the efficacy of the tokenization approach.

**Conclusion:** Efficient tokenization is crucial and can enhance the performance of language models, suggesting a promising avenue for future research.

**Abstract:** Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures. We present SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning that discovers multi-word semantic units, entropy-driven data curation that optimizes training corpus quality, and multi-phase curriculum learning for stable convergence. Our approach extends Byte-Pair Encoding by learning "superword" tokens, coherent multi-word expressions that preserve semantic unity while maximizing compression efficiency. SupraTok achieves 31% improvement in English tokenization efficiency (5.91 versus 4.51 characters per token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model (124M parameters) trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications. While these results are promising at this scale, further validation at larger model scales is needed. These findings suggest that efficient tokenization can complement architectural innovations as a path to improved language model performance.

</details>


### [35] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)

*Hui Ma, Bo Zhang, Jinpeng Hu, Zenglin Shi*

**Main category:** cs.CL

**Keywords:** Emotion Recognition, Conversational AI, Instruction Tuning

**Relevance Score:** 8

**TL;DR:** InitERC is a novel one-stage in-context instruction tuning framework for emotion recognition in conversation, enhancing alignment between speaker characteristics and conversational context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve emotion recognition in conversation by addressing the limitations of existing multi-stage instruction tuning methods in aligning speaker identity, contextual cues, and emotion states.

**Method:** InitERC employs in-context instruction tuning with components such as demonstration pool construction, example selection, prompt design, and instruction tuning.

**Key Contributions:**

	1. Proposed InitERC framework for ERC using in-context instruction tuning
	2. Demonstrated effectiveness through experiments on multiple datasets
	3. Investigated the impact of retrieval strategy, example ordering, and number of examples

**Result:** InitERC shows substantial performance improvements over state-of-the-art baselines in emotion recognition on three major datasets.

**Limitations:** 

**Conclusion:** The research validates that a one-stage in-context instruction tuning approach can effectively enhance ERC by better aligning speaker characteristics and emotional contexts.

**Abstract:** Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation, playing a vital role in empathetic artificial intelligence. With the growing of large language models (LLMs), instruction tuning has emerged as a critical paradigm for ERC. Existing studies mainly focus on multi-stage instruction tuning, which first endows LLMs with speaker characteristics, and then conducts context-aware instruction tuning to comprehend emotional states. However, these methods inherently constrains the capacity to jointly capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states within a unified framework. In this paper, we propose InitERC, a simple yet effective one-stage in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn speaker-context-emotion alignment from context examples via in-context instruction tuning. Specifically, InitERC comprises four components, i.e., demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. To explore the impact of in-context examples, we conduct a comprehensive study on three key factors: retrieval strategy, example ordering, and the number of examples. Extensive experiments on three widely used datasets demonstrate that our proposed InitERC achieves substantial improvements over the state-of-the-art baselines.

</details>


### [36] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)

*Punya Syon Pandey, Yongjin Yang, Jiarui Liu, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conversational Robustness Evaluation, multi-agent systems, game-theoretic interactions, linguistic diversity

**Relevance Score:** 8

**TL;DR:** Introduction of the Conversational Robustness Evaluation Score (CORE) metric to assess language use effectiveness in multi-agent LLM interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify the linguistic diversity in game-theoretic interactions among agents utilizing Large Language Models.

**Method:** CORE combines measures of cluster entropy, lexical repetition, and semantic similarity to evaluate dialog quality in LLM systems during competitive, cooperative, and neutral interactions.

**Key Contributions:**

	1. Introduction of the CORE metric for evaluating dialog quality in LLMs.
	2. Demonstration of linguistic differences in dialog across game-theoretic environments.
	3. Analysis linking social incentives with language adaptation through statistical measures.

**Result:** Cooperative settings show higher cluster entropy and vocabulary growth compared to competitive settings, suggesting that social incentives significantly affect language adaptation in LLM interactions.

**Limitations:** 

**Conclusion:** CORE serves as an effective tool for diagnosing linguistic robustness, offering insights into how social dynamics influence language usage among agents in multi-agent systems.

**Abstract:** Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.

</details>


### [37] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)

*Jie Lu, Du Jin, Hitomi Yanaka*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, temporal semantics, cross-linguistic evaluation

**Relevance Score:** 4

**TL;DR:** This paper presents a template-based Natural Language Inference (NLI) dataset for Chinese and Japanese, focusing on the perfect aspect and temporal inference challenges faced by LLMs.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of distinct grammatical forms for tense in the perfect aspect of Chinese and Japanese, which complicates Natural Language Inference (NLI).

**Method:** A linguistically motivated, template-based NLI dataset was constructed, consisting of 1,350 pairs per language, assessing temporal inference capabilities of LLMs.

**Key Contributions:**

	1. Creation of a NLI dataset for Chinese and Japanese focused on the perfect aspect.
	2. Demonstration of LLMs' challenges in temporal inference.
	3. Highlighting the need for linguistic diversity in NLI evaluation.

**Result:** Experiments show that even advanced LLMs have difficulty with temporal inference, especially in identifying tense and reference-time shifts.

**Limitations:** The dataset is limited to the perfect aspect and may not cover all temporal inference scenarios.

**Conclusion:** The findings emphasize the limitations of current models and the importance of cross-linguistic evaluation within temporal semantics.

**Abstract:** Unlike English, which uses distinct forms (e.g., had, has, will have) to mark the perfect aspect across tenses, Chinese and Japanese lack separate grammatical forms for tense within the perfect aspect, which complicates Natural Language Inference (NLI). Focusing on the perfect aspect in these languages, we construct a linguistically motivated, template-based NLI dataset (1,350 pairs per language). Experiments reveal that even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts. These findings highlight model limitations and underscore the need for cross-linguistic evaluation in temporal semantics. Our dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [38] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)

*Yue Wang, Liesheng Wei, Yuxiang Wang*

**Main category:** cs.CL

**Keywords:** Machine-generated text, Detection, Large Language Models, Adversarial frameworks, Linguistic dimensions

**Relevance Score:** 9

**TL;DR:** CAMF is a novel framework for detecting machine-generated text using multiple LLM agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the urgent need for effective detection of machine-generated text amidst risks like disinformation and threats to academic integrity.

**Method:** Introduced the Collaborative Adversarial Multi-agent Framework (CAMF) that utilizes multiple LLM-based agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation.

**Key Contributions:**

	1. Introduction of CAMF framework for MGT detection
	2. Three-phase process enhancing detection accuracy
	3. Empirical evidence of superiority over state-of-the-art methods

**Result:** CAMF significantly outperforms existing zero-shot machine-generated text detection techniques in empirical evaluations.

**Limitations:** 

**Conclusion:** The structured collaborative-adversarial approach of CAMF allows for a nuanced analysis of textual inconsistencies suggesting non-human authorship.

**Abstract:** Detecting machine-generated text (MGT) from contemporary Large Language Models (LLMs) is increasingly crucial amid risks like disinformation and threats to academic integrity. Existing zero-shot detection paradigms, despite their practicality, often exhibit significant deficiencies. Key challenges include: (1) superficial analyses focused on limited textual attributes, and (2) a lack of investigation into consistency across linguistic dimensions such as style, semantics, and logic. To address these challenges, we introduce the \textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent \textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple LLM-based agents. CAMF employs specialized agents in a synergistic three-phase process: \emph{Multi-dimensional Linguistic Feature Extraction}, \emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment Aggregation}. This structured collaborative-adversarial process enables a deep analysis of subtle, cross-dimensional textual incongruities indicative of non-human origin. Empirical evaluations demonstrate CAMF's significant superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [39] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)

*Shaozhe Yin, Jinyu Guo, Kai Shuang, Xia Liu, Ruize Ou*

**Main category:** cs.CL

**Keywords:** Continual Relation Extraction, Large Language Models, Cognitive Bias, Contrastive Learning, Instruction Tuning

**Relevance Score:** 9

**TL;DR:** This paper presents an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in Continual Relation Extraction (CRE), addressing cognitive biases and improving performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To continuously learn new relations in CRE while avoiding catastrophic forgetting and better address cognitive biases by focusing on error cases.

**Method:** An instruction-based continual contrastive tuning approach that separates training and memory data based on response correctness and employs dual-task fine-tuning.

**Key Contributions:**

	1. Introduction of an instruction-based continual contrastive tuning approach for LLMs in CRE.
	2. Dual-task fine-tuning strategy that separates data based on correctness of responses.
	3. Achieving state-of-the-art results on TACRED and FewRel datasets.

**Result:** The proposed model achieves new state-of-the-art performance in CRE on TACRED and FewRel datasets, highlighting the impact of focusing on error cases.

**Limitations:** 

**Conclusion:** This method effectively mitigates the cognitive bias gap between old and new relations in LLMs, improving their continual learning capabilities.

**Abstract:** Continual Relation Extraction (CRE) aims to continually learn new emerging relations while avoiding catastrophic forgetting. Existing CRE methods mainly use memory replay and contrastive learning to mitigate catastrophic forgetting. However, these methods do not attach importance to the error cases that can reveal the model's cognitive biases more effectively. To address this issue, we propose an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in CRE. Different from existing CRE methods that typically handle the training and memory data in a unified manner, this approach splits the training and memory data of each task into two parts respectively based on the correctness of the initial responses and treats them differently through dual-task fine-tuning. In addition, leveraging the advantages of LLM's instruction-following ability, we propose a novel instruction-based contrastive tuning strategy for LLM to continuously correct current cognitive biases with the guidance of previous data in an instruction-tuning manner, which mitigates the gap between old and new relations in a more suitable way for LLMs. We experimentally evaluate our model on TACRED and FewRel, and the results show that our model achieves new state-of-the-art CRE performance with significant improvements, demonstrating the importance of specializing in exploiting error cases.

</details>


### [40] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)

*Jinyi Han, Tingyun Li, Shisong Chen, Jie Shi, Xinyi Wang, Guanglei Yue, Jiaqing Liang, Xin Lin, Liqian Wen, Zulong Chen, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** large language models, confidence estimation, Backpropagation Confidence Integration

**Relevance Score:** 8

**TL;DR:** Introducing FineCE, a novel method for accurate and fine-grained confidence estimation for LLMs during text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improve the trustworthiness of LLM outputs by providing fine-grained confidence scores instead of coarse-grained estimates.

**Method:** Develop a comprehensive data pipeline for training and employ a Backward Confidence Integration (BCI) strategy to enhance confidence scores during inference.

**Key Contributions:**

	1. Development of FineCE method for confidence estimation
	2. Introduction of Backward Confidence Integration (BCI) strategy
	3. Three strategies for optimal confidence estimation position identification

**Result:** FineCE outperforms existing classical confidence estimation methods across multiple benchmark datasets.

**Limitations:** 

**Conclusion:** FineCE enhances the reliability of LLM-generated outputs by providing accurate confidence scores throughout the generation process.

**Abstract:** While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.

</details>


### [41] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)

*Yao Wu*

**Main category:** cs.CL

**Keywords:** large language models, multi-objective optimization, Jacobian decomposition

**Relevance Score:** 9

**TL;DR:** The paper introduces J6, a Jacobian-based method for optimizing multiple objectives in large language model adaptation, addressing the balance between factuality and confidence while providing insight into parameter interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** A need to balance factuality and confidence in large language model adaptation due to the complex interactions of prompt parameters.

**Method:** The J6 method decomposes the gradient interaction matrix into six components to facilitate both hard and soft decision-making in prompt parameter updates.

**Key Contributions:**

	1. Introduces a structured Jacobian-based approach to LLM adaptation.
	2. Decomposes gradient interaction into interpretable components.
	3. Provides both hard and soft strategies for multi-objective optimization.

**Result:** J6 enables effective conflict-aware prompt optimization, allowing for better handling of geometric structures in multi-objective neural tuning.

**Limitations:** 

**Conclusion:** The proposed J6 method not only enhances optimization in LLM adaptation but also provides interpretability and opens new avenues for structured reasoning in neural tuning.

**Abstract:** In large language model (LLM) adaptation, balancing multiple optimization objectives such as improving factuality (heat) and increasing confidence (via low entropy) poses a fundamental challenge, especially when prompt parameters (e.g., hidden-layer insertions h and embedding modifications w) interact in non-trivial ways. Existing multi-objective optimization strategies often rely on scalar gradient aggregation, ignoring the deeper geometric structure between objectives and parameters. We propose J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components. This decomposition enables both hard decision-making (e.g., choosing the dominant update direction via argmax) and soft strategies (e.g., attention-style weighting via softmax over J6), forming a dynamic update framework that adapts to local conflict and synergy. Moreover, the interpretable structure of J6 provides insight into parameter attribution, task interference, and geometry-aligned adaptation. Our work introduces a principled and extensible mechanism for conflict-aware prompt optimization, and opens a new avenue for incorporating structured Jacobian reasoning into multi-objective neural tuning.

</details>


### [42] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)

*Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evaluation, Machine Learning, Performance Measurement, Benchmarking

**Relevance Score:** 8

**TL;DR:** The paper introduces the Structured Transition Evaluation Method (STEM) for evaluating large language models (LLMs) in a lightweight and interpretable manner, highlighting its effectiveness in estimating model capabilities through significant transition samples.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges in evaluating LLMs due to rapid advancements, overfitting to benchmarks, and high evaluation costs, making it difficult to differentiate model performance meaningfully.

**Method:** The proposed method, STEM, identifies significant transition samples among LLMs of the same architecture but varying parameter scales to estimate the relative capabilities of unknown models efficiently.

**Key Contributions:**

	1. Introduction of the Structured Transition Evaluation Method (STEM) for LLM assessment
	2. Identification of significant transition samples for efficient performance estimation
	3. Demonstration of STEM's effectiveness across diverse benchmarks.

**Result:** Experimental results demonstrate that STEM reliably captures performance trends and aligns with the ground-truth rankings of model capabilities on diverse benchmarks.

**Limitations:** 

**Conclusion:** STEM is presented as a practical and scalable method for architecture-agnostic evaluation of LLMs, enhancing the evaluation process amidst growing model complexity.

**Abstract:** Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [43] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)

*Ziqian Bi, Lu Chen, Junhao Song, Hongying Luo, Enze Ge, Junmin Huang, Tianyang Wang, Keyu Chen, Chia Xin Liang, Zihan Wei, Huafeng Liu, Chunjie Tian, Jibin Guan, Joe Yeong, Yongzhi Xu, Peng Wang, Junfeng Hao*

**Main category:** cs.CL

**Keywords:** thinking budget, medical reasoning, scaling laws, AI systems, resource allocation

**Relevance Score:** 9

**TL;DR:** This study evaluates thinking budget mechanisms in medical reasoning, revealing scaling laws between computational resources and reasoning quality across model families Qwen3 and DeepSeek-R1.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how computational resources influence the quality of reasoning in medical AI systems, and to identify optimal resource allocation strategies for different clinical tasks.

**Method:** Comprehensive evaluation of Qwen3 and DeepSeek-R1 models across 15 medical datasets with varying thinking budgets, conducting controlled experiments on model performance at different token limits.

**Key Contributions:**

	1. Identification of efficiency regimes in medical reasoning
	2. Establishment of scaling laws for reasoning quality vs computational resources
	3. Insights into domain-specific reasoning requirements in medicine.

**Result:** Logarithmic scaling relationships were established, showing predictability in accuracy improvements based on model size and thinking budget, with distinct efficiency regimes identified for real-time applications and critical diagnostics.

**Limitations:** 

**Conclusion:** Thinking budget control is essential for optimizing AI systems in healthcare, allowing for efficient resource allocation while ensuring transparency in deployment.

**Abstract:** This study presents the first comprehensive evaluation of thinking budget mechanisms in medical reasoning tasks, revealing fundamental scaling laws between computational resources and reasoning quality. We systematically evaluated two major model families, Qwen3 (1.7B to 235B parameters) and DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning diverse specialties and difficulty levels. Through controlled experiments with thinking budgets ranging from zero to unlimited tokens, we establish logarithmic scaling relationships where accuracy improvements follow a predictable pattern with both thinking budget and model size. Our findings identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens) suitable for real-time applications, balanced (256 to 512 tokens) offering optimal cost-performance tradeoffs for routine clinical support, and high-accuracy (above 512 tokens) justified only for critical diagnostic tasks. Notably, smaller models demonstrate disproportionately larger benefits from extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger models, suggesting a complementary relationship where thinking budget provides greater relative benefits for capacity-constrained models. Domain-specific patterns emerge clearly, with neurology and gastroenterology requiring significantly deeper reasoning processes than cardiovascular or respiratory medicine. The consistency between Qwen3 native thinking budget API and our proposed truncation method for DeepSeek-R1 validates the generalizability of thinking budget concepts across architectures. These results establish thinking budget control as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment.

</details>


### [44] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)

*Stephen Meisenbacher, Alexandra Klymenko, Florian Matthes*

**Main category:** cs.CL

**Keywords:** privacy-preserving NLP, LLM-as-a-Judge, privacy evaluation, human-computer interaction, textual data

**Relevance Score:** 9

**TL;DR:** The paper explores using LLMs as privacy evaluators in NLP, demonstrating their potential to model human perspectives on privacy despite challenges in empirical measurement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate evaluation of privacy in NLP, which remains a significant challenge despite advancements in privacy-preserving techniques.

**Method:** Utilized a study involving 10 datasets, 13 LLMs, and 677 human participants to assess the capability of LLMs as a judge of privacy sensitivity in textual data.

**Key Contributions:**

	1. Proposing LLMs as evaluators for privacy sensitivity in textual data
	2. Empirical study comparing LLM evaluations to human perceptions of privacy
	3. Identifying challenges in measuring privacy and inter-human agreement rates

**Result:** LLMs can model a global human perspective on privacy, although empirical measurement of privacy remains difficult, with low inter-human agreement on privacy sensitivity.

**Limitations:** General low inter-human agreement rates indicate the subjectivity of privacy measurement.

**Conclusion:** The findings support the potential of LLMs as effective privacy evaluators, while also highlighting the complexities of quantifying privacy.

**Abstract:** Despite advances in the field of privacy-preserving Natural Language Processing (NLP), a significant challenge remains the accurate evaluation of privacy. As a potential solution, using LLMs as a privacy evaluator presents a promising approach $\unicode{x2013}$ a strategy inspired by its success in other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$ paradigm has achieved impressive results on a variety of natural language evaluation tasks, demonstrating high agreement rates with human annotators. Recognizing that privacy is both subjective and difficult to define, we investigate whether LLM-as-a-Judge can also be leveraged to evaluate the privacy sensitivity of textual data. Furthermore, we measure how closely LLM evaluations align with human perceptions of privacy in text. Resulting from a study involving 10 datasets, 13 LLMs, and 677 human survey participants, we confirm that privacy is indeed a difficult concept to measure empirically, exhibited by generally low inter-human agreement rates. Nevertheless, we find that LLMs can accurately model a global human privacy perspective, and through an analysis of human and LLM reasoning patterns, we discuss the merits and limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our findings pave the way for exploring the feasibility of LLMs as privacy evaluators, addressing a core challenge in solving pressing privacy issues with innovative technical solutions.

</details>


### [45] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)

*Abdelhamid Haouhat, Slimane Bellaouar, Attia Nehar, Hadda Cherroun, Ahmed Abdelali*

**Main category:** cs.CL

**Keywords:** Multimodal Machine Learning, Arabic language, Survey, Taxonomy, Research gaps

**Relevance Score:** 3

**TL;DR:** This paper provides a comprehensive survey of Arabic multimodal machine learning (MML), categorizing existing research into datasets, applications, approaches, and challenges, and highlighting research gaps.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the recent developments and state of Arabic MML given its maturity level, and to organize the knowledge in a structured way.

**Method:** The authors conducted a comprehensive survey and created a novel taxonomy that categorizes Arabic MML research into four key topics.

**Key Contributions:**

	1. Creation of a novel taxonomy for Arabic MML research
	2. Identification of critical research gaps
	3. Structured overview of the current state of Arabic MML

**Result:** The survey identifies the current state of Arabic MML and highlights unexplored areas and critical research gaps.

**Limitations:** 

**Conclusion:** The structured overview offered by the survey aims to empower future research in Arabic MML by addressing identified opportunities and challenges.

**Abstract:** Multimodal Machine Learning (MML) aims to integrate and analyze information from diverse modalities, such as text, audio, and visuals, enabling machines to address complex tasks like sentiment analysis, emotion recognition, and multimedia retrieval. Recently, Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey. This paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. Our taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges. By providing a structured overview, this survey offers insights into the current state of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field.

</details>


### [46] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)

*Wuttikorn Ponwitayarat, Raymond Ng, Jann Railey Montalan, Thura Aung, Jian Gang Ngui, Yosephine Susanto, William Tjhi, Panuthep Tasawong, Erik Cambria, Ekapol Chuangsuwanich, Sarana Nutanong, Peerat Limkonchotiwat*

**Main category:** cs.CL

**Keywords:** sentence embeddings, Southeast Asia, language benchmark, semantic search, multilingual NLP

**Relevance Score:** 8

**TL;DR:** Introducing SEA-BED, the first large-scale sentence embedding benchmark for Southeast Asia with 169 datasets across 9 tasks and 10 languages, highlighting challenges and performance gaps in SEA languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The Southeast Asia region lacks a region-specific embedding benchmark for sentence embeddings despite having a large population and unique linguistic properties.

**Method:** We constructed the SEA-BED benchmark with 169 datasets, evaluating 17 embedding models across six studies to analyze task and language challenges and translation effects.

**Key Contributions:**

	1. The creation of the SEA-BED benchmark for Southeast Asia.
	2. Evaluation of 17 embedding models revealing unique performance challenges in SEA languages.
	3. Highlighting the critical role of human-generated datasets for accurate language representation.

**Result:** The evaluation shows significant ranking shifts and inconsistent performance among models for SEA languages, emphasizing the need for human-curated datasets.

**Limitations:** The benchmark may not cover all linguistic nuances of SEA languages and relies heavily on human curation which may vary in quality.

**Conclusion:** The findings reveal the importance of region-specific benchmarks and human-generated datasets, particularly for low-resource languages like Burmese.

**Abstract:** Sentence embeddings are essential for NLP tasks such as semantic search, re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB broaden coverage, Southeast Asia (SEA) datasets are scarce and often machine-translated, missing native linguistic properties. With nearly 700 million speakers, the SEA region lacks a region-specific embedding benchmark. We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. We address three research questions: (1) which SEA languages and tasks are challenging, (2) whether SEA languages show unique performance gaps globally, and (3) how human vs. machine translations affect evaluation. We evaluate 17 embedding models across six studies, analyzing task and language challenges, cross-benchmark comparisons, and translation trade-offs. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.

</details>


### [47] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)

*Ankita Pasad*

**Main category:** cs.CL

**Keywords:** Speech Foundation Models, Spoken Language Understanding, Named Entity Recognition

**Relevance Score:** 8

**TL;DR:** This thesis analyzes the knowledge encoded in Speech Foundation Models (SFMs) and their performance on spoken language understanding tasks, contributing new datasets and methods to improve understanding and application in the field.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The growth of Speech Foundation Models (SFMs) necessitates a deeper understanding of the knowledge they encode to better apply them in speech-processing tasks.

**Method:** A lightweight analysis framework using statistical tools and training-free tasks was developed to investigate the acoustic and linguistic knowledge in SFM layers. A comparative study across multiple SFMs was conducted, along with the development of tasks for spoken named entity recognition and named entity localization in SLU.

**Key Contributions:**

	1. Development of a lightweight analysis framework for SFMs
	2. Introduction of new SLU tasks: spoken named entity recognition and localization
	3. Comparative evaluation of E2E SLU models against traditional approaches

**Result:** End-to-end models utilizing SFMs for spoken language understanding outperformed traditional cascaded approaches, demonstrating the potential of leveraging SFMs beyond speech recognition.

**Limitations:** 

**Conclusion:** This thesis addressed gaps in understanding SFMs and provided new tools and datasets, paving the way for improved model design and usage in the field of spoken language understanding.

**Abstract:** Speech foundation models (SFMs) are designed to serve as general-purpose representations for a wide range of speech-processing tasks. The last five years have seen an influx of increasingly successful self-supervised and supervised pre-trained models with impressive performance on various downstream tasks.   Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind. This thesis presents a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers. We conduct a comparative study across multiple SFMs and statistical tools. Our study also shows that the analytical insights have concrete implications for downstream task performance.   The effectiveness of an SFM is ultimately determined by its performance on speech applications. Yet it remains unclear whether the benefits extend to spoken language understanding (SLU) tasks that require a deeper understanding than widely studied ones, such as speech recognition. The limited exploration of SLU is primarily due to a lack of relevant datasets. To alleviate that, this thesis contributes tasks, specifically spoken named entity recognition (NER) and named entity localization (NEL), to the Spoken Language Understanding Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches. Further, we evaluate E2E SLU models across SFMs and adaptation strategies to assess the impact on task performance.   Collectively, this thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption.

</details>


### [48] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)

*Zheye Deng, Chunkit Chan, Tianshi Zheng, Wei Fan, Weiqi Wang, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Text-to-structure, AI systems, Evaluation framework

**Relevance Score:** 7

**TL;DR:** This paper systematically reviews methodologies and challenges in converting unstructured text into structured formats, evaluates existing datasets and metrics, and proposes a universal evaluation framework for structured outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for converting unstructured text into structured formats to support advanced applications in AI, yet existing research lacks a comprehensive overview.

**Method:** The paper employs a systematic review methodology to examine text-to-structure conversion techniques, datasets, and evaluation metrics in the context of AI systems.

**Key Contributions:**

	1. Systematic review of text-to-structure techniques and methodologies
	2. Evaluation of datasets and assessment criteria
	3. Proposal of a universal evaluation framework for structured outputs

**Result:** The review identifies key challenges in current methodologies and datasets, providing insights into gaps in research and practical applications.

**Limitations:** The review is limited to existing research and may not cover all emerging techniques and datasets in the rapidly evolving field.

**Conclusion:** Text-to-structure conversion is essential for the evolution of next-generation AI systems, and a universal evaluation framework is proposed to improve structured output assessments.

**Abstract:** The evolution of AI systems toward agentic operation and context-aware retrieval necessitates transforming unstructured text into structured formats like tables, knowledge graphs, and charts. While such conversions enable critical applications from summarization to data mining, current research lacks a comprehensive synthesis of methodologies, datasets, and metrics. This systematic review examines text-to-structure techniques and the encountered challenges, evaluates current datasets and assessment criteria, and outlines potential directions for future research. We also introduce a universal evaluation framework for structured outputs, establishing text-to-structure as foundational infrastructure for next-generation AI systems.

</details>


### [49] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)

*Xinda Jia, Jinpeng Li, Zezhong Wang, Jingjing Li, Xingshan Zeng, Yasheng Wang, Weinan Zhang, Yong Yu, Weiwen Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning strategies, adaptive reasoning, cognitive psychology, taxonomy

**Relevance Score:** 9

**TL;DR:** This paper proposes a taxonomy for LLM reasoning strategies inspired by cognitive psychology and categorizes methods for adaptive reasoning in real-world tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need for large language models to effectively adapt their reasoning strategies based on the demands of diverse real-world tasks.

**Method:** The authors propose a taxonomy of LLM reasoning strategies divided into a fast/slow boundary (intuitive vs. deliberative) and an internal/external boundary (model parameters vs. external tools). They systematically survey and categorize methods for adaptive reasoning.

**Key Contributions:**

	1. Proposed a novel taxonomy for LLM reasoning strategies.
	2. Categorized recent work on adaptive reasoning.
	3. Highlighted future directions for improving LLM reasoning efficiency.

**Result:** They identify key decision factors influencing reasoning strategies and survey recent work in adaptive reasoning for LLMs.

**Limitations:** 

**Conclusion:** The paper concludes with discussions on open challenges and future research directions towards more adaptive and reliable LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable progress in reasoning across diverse domains. However, effective reasoning in real-world tasks requires adapting the reasoning strategy to the demands of the problem, ranging from fast, intuitive responses to deliberate, step-by-step reasoning and tool-augmented thinking. Drawing inspiration from cognitive psychology, we propose a novel taxonomy of LLM reasoning strategies along two knowledge boundaries: a fast/slow boundary separating intuitive from deliberative processes, and an internal/external boundary distinguishing reasoning grounded in the model's parameters from reasoning augmented by external tools. We systematically survey recent work on adaptive reasoning in LLMs and categorize methods based on key decision factors. We conclude by highlighting open challenges and future directions toward more adaptive, efficient, and reliable LLMs.

</details>


### [50] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)

*Elon Ezra, Ariel Weizman, Amos Azaria*

**Main category:** cs.CL

**Keywords:** large language models, self-execution benchmark, output prediction, model limitations, self-awareness

**Relevance Score:** 8

**TL;DR:** This paper introduces the Self-Execution Benchmark, which evaluates LLMs on their ability to predict aspects of their own responses, revealing significant limitations in their self-awareness and reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore a novel evaluation method for large language models (LLMs) by assessing their ability to predict characteristics of their own outputs.

**Method:** The paper introduces a Self-Execution Benchmark that measures an LLM's ability to anticipate properties of its output, such as difficulty or likely associations.

**Key Contributions:**

	1. Introduction of the Self-Execution Benchmark for evaluating LLMs.
	2. Empirical evidence of poor LLM performance on self-prediction tasks.
	3. Insights into the limitations of LLM reasoning about their responses.

**Result:** The experiments demonstrate that LLMs generally perform poorly on this benchmark, and performance does not consistently improve with larger model size or capability.

**Limitations:** Limited to the tasks defined in the Self-Execution Benchmark; results may not generalize to all forms of reasoning or other model architectures.

**Conclusion:** The findings indicate a fundamental limitation in LLMs' representation and reasoning regarding their own behavior.

**Abstract:** Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.

</details>


### [51] [Legal$Î$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)

*Xin Dai, Buqiang Xu, Zhenghao Liu, Yukun Yan, Huiyuan Xie, Xiaoyuan Yi, Shuo Wang, Ge Yu*

**Main category:** cs.CL

**Keywords:** Legal Artificial Intelligence, Large Language Models, Reinforcement Learning, Legal Reasoning, Interpretability

**Relevance Score:** 4

**TL;DR:** LegalAI has made strides in automating judicial decision-making, but current LLMs struggle with reliable reasoning. The proposed framework, LegalÎ, enhances legal reasoning through a dual-mode reinforcement learning approach.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability and interpretability of legal reasoning in LLMs, addressing the challenge of generating explicit multi-step reasoning.

**Method:** LegalÎ uses a dual-mode input setup during training to maximize the information gain between direct answers and reasoning-augmented responses, enhancing reasoning patterns through a two-stage approach.

**Key Contributions:**

	1. Introduction of LegalÎ framework for legal reasoning
	2. Use of dual-mode input in reinforcement learning
	3. Improvement in accuracy and interpretability of legal judgments

**Result:** LegalÎ outperforms strong baselines in legal reasoning tasks, achieving improved accuracy and interpretability without needing labeled preference data.

**Limitations:** 

**Conclusion:** LegalÎ enhances the robustness and trustworthiness of legal judgments, demonstrating the effectiveness of reinforcement learning in legal AI applications.

**Abstract:** Legal Artificial Intelligence (LegalAI) has achieved notable advances in automating judicial decision-making with the support of Large Language Models (LLMs). However, existing legal LLMs still struggle to generate reliable and interpretable reasoning processes. They often default to fast-thinking behavior by producing direct answers without explicit multi-step reasoning, limiting their effectiveness in complex legal scenarios that demand rigorous justification. To address this challenge, we propose Legal$\Delta$, a reinforcement learning framework designed to enhance legal reasoning through chain-of-thought guided information gain. During training, Legal$\Delta$ employs a dual-mode input setup-comprising direct answer and reasoning-augmented modes-and maximizes the information gain between them. This encourages the model to acquire meaningful reasoning patterns rather than generating superficial or redundant explanations. Legal$\Delta$ follows a two-stage approach: (1) distilling latent reasoning capabilities from a powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning quality via differential comparisons, combined with a multidimensional reward mechanism that assesses both structural coherence and legal-domain specificity. Experimental results on multiple legal reasoning tasks demonstrate that Legal$\Delta$ outperforms strong baselines in both accuracy and interpretability. It consistently produces more robust and trustworthy legal judgments without relying on labeled preference data. All code and data will be released at https://github.com/NEUIR/LegalDelta.

</details>


### [52] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)

*Ziyang Chen, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Jichao Li, Shuaiqiang Wang, Baotian Hu, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Chinese question answering, temporal reasoning, RAG systems, benchmark dataset, evaluation

**Relevance Score:** 5

**TL;DR:** ChronoQA is a benchmark dataset for Chinese question answering focusing on temporal reasoning for Retrieval-Augmented Generation (RAG) systems, featuring over 300,000 news articles and 5,176 questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate temporal reasoning capabilities in RAG systems and provide a comprehensive resource for structured evaluation of temporal tasks in question answering.

**Method:** ChronoQA was constructed from 300,000 news articles, containing structured annotations and validated through rule-based, LLM-based, and human evaluations.

**Key Contributions:**

	1. Large-scale dataset for temporal reasoning in question answering.
	2. Comprehensive structural annotations with multi-stage validation.
	3. Dynamic resource for advancing time-sensitive RAG systems.

**Result:** The dataset includes 5,176 questions that address various temporal reasoning types and supports both single and multi-document scenarios, reflecting real-world requirements.

**Limitations:** 

**Conclusion:** ChronoQA serves as a scalable resource for evaluating time-sensitive retrieval-augmented question answering systems, enhancing the development of these technologies.

**Abstract:** We introduce ChronoQA, a large-scale benchmark dataset for Chinese question answering, specifically designed to evaluate temporal reasoning in Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over 300,000 news articles published between 2019 and 2024, and contains 5,176 high-quality questions covering absolute, aggregate, and relative temporal types with both explicit and implicit time expressions. The dataset supports both single- and multi-document scenarios, reflecting the real-world requirements for temporal alignment and logical consistency. ChronoQA features comprehensive structural annotations and has undergone multi-stage validation, including rule-based, LLM-based, and human evaluation, to ensure data quality. By providing a dynamic, reliable, and scalable resource, ChronoQA enables structured evaluation across a wide range of temporal tasks, and serves as a robust benchmark for advancing time-sensitive retrieval-augmented question answering systems.

</details>


### [53] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)

*Qinghua Wang, Xu Zhang, Lingyan Yang, Rui Shao, Bonan Wang, Fang Wang, Cunquan Qu*

**Main category:** cs.CL

**Keywords:** probation prediction, legal logic, deep learning, judicial decision-making, machine learning

**Relevance Score:** 2

**TL;DR:** This paper proposes a novel approach to probation prediction by integrating legal logic into deep learning models, improving upon existing data-driven methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The current Intelligent Judicial Assistant System lacks methods for effective probation prediction and understanding factors influencing probation eligibility.

**Method:** The authors construct a specialized probation dataset and design the Multi-Task Dual-Theory Probation Prediction Model (MT-DT) that incorporates legal logic.

**Key Contributions:**

	1. Integration of legal logic in deep learning for probation prediction
	2. Development of a specialized probation dataset
	3. Novel Multi-Task Dual-Theory Probation Prediction Model (MT-DT)

**Result:** Experiments show that the MT-DT model outperforms baseline models in predicting probation eligibility.

**Limitations:** 

**Conclusion:** The integration of legal logic into the prediction models improves accuracy and addresses gaps in existing research.

**Abstract:** Probation is a crucial institution in modern criminal law, embodying the principles of fairness and justice while contributing to the harmonious development of society. Despite its importance, the current Intelligent Judicial Assistant System (IJAS) lacks dedicated methods for probation prediction, and research on the underlying factors influencing probation eligibility remains limited. In addition, probation eligibility requires a comprehensive analysis of both criminal circumstances and remorse. Much of the existing research in IJAS relies primarily on data-driven methodologies, which often overlooks the legal logic underpinning judicial decision-making. To address this gap, we propose a novel approach that integrates legal logic into deep learning models for probation prediction, implemented in three distinct stages. First, we construct a specialized probation dataset that includes fact descriptions and probation legal elements (PLEs). Second, we design a distinct probation prediction model named the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), which is grounded in the legal logic of probation and the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the probation dataset demonstrate that the MT-DT model outperforms baseline models, and an analysis of the underlying legal logic further validates the effectiveness of the proposed approach.

</details>


### [54] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)

*Tomer Krichli, Bhiksha Raj, Joseph Keshet*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Streaming Model, Transformer, Low-Latency

**Relevance Score:** 6

**TL;DR:** This paper proposes a method for transforming transformer encoder-decoder models into efficient low-latency streaming models for Automatic Speech Recognition (ASR), achieving better performance in real-time transcription.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing ASR models like OpenAI Whisper and NVIDIA Canary excel in offline transcription but struggle with streaming due to architectural limitations.

**Method:** The authors modify a non-causal encoder into a causal encoder through fine-tuning with Low-Rank Adaptation (LoRA) and a weakly aligned dataset, and introduce a new inference mechanism for greedy and beam-search decoding.

**Key Contributions:**

	1. Transformation of encoder-decoder models for low-latency streaming ASR
	2. Introduction of a novel fine-tuning method using LoRA
	3. Development of an efficient inference mechanism for real-time transcription.

**Result:** The fine-tuned model demonstrates improved performance over non-fine-tuned streaming approaches with low-latency chunk sizes (less than 300 msec), alongside a simpler method for extracting word-level timestamps.

**Limitations:** 

**Conclusion:** The proposed approach offers a significant improvement in streaming ASR, making real-time transcription more accurate and feasible while releasing supporting code and models for further research.

**Abstract:** Automatic Speech Recognition (ASR) has seen remarkable progress, with models like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA) performance in offline transcription. However, these models are not designed for streaming (online or real-time) transcription, due to limitations in their architecture and training methodology. We propose a method to turn the transformer encoder-decoder model into a low-latency streaming model that is careless about future context. We present an analysis explaining why it is not straightforward to convert an encoder-decoder transformer to a low-latency streaming model. Our proposed method modifies the existing (non-causal) encoder to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated inference mechanism that utilizes the fine-tune causal encoder and decoder to yield greedy and beam-search decoding, and is shown to be locally optimal. Experiments on low-latency chunk sizes (less than 300 msec) show that our fine-tuned model outperforms existing non-fine-tuned streaming approaches in most cases, while using a lower complexity. Additionally, we observe that our training process yields better alignment, enabling a simple method for extracting word-level timestamps. We release our training and inference code, along with the fine-tuned models, to support further research and development in streaming ASR.

</details>


### [55] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)

*Eviatar Nachshoni, Arie Cattan, Shmuel Amar, Ori Shapira, Ido Dagan*

**Main category:** cs.CL

**Keywords:** Multi-Answer Question Answering, Language Models, Benchmark, Conflict-Detection, Fact-Checking

**Relevance Score:** 8

**TL;DR:** This paper addresses the challenges in Multi-Answer Question Answering (MAQA), introducing a new benchmark and methodology to evaluate models on conflict-aware tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Multi-Answer Question Answering (MAQA) by recognizing both valid answers and conflicting answer pairs, which traditional QA methods overlook.

**Method:** The study extends the MAQA setting by leveraging fact-checking datasets to create NATCONFQA, a benchmark focusing on realistic and conflict-aware question answering tasks.

**Key Contributions:**

	1. Introduction of NATCONFQA benchmark for conflict-aware MAQA
	2. Development of a novel methodology for dataset construction
	3. Evaluation of LLMs revealing their weaknesses in conflict resolution

**Result:** Evaluation of eight LLMs on NATCONFQA shows their fragility in managing various conflicts and reveals ineffective strategies for conflict resolution.

**Limitations:** The benchmark relies on existing fact-checking datasets, which may have their own limitations.

**Conclusion:** The introduction of NATCONFQA provides a necessary framework for advancing research on conflict-aware MAQA and highlights the limitations of existing LLMs in handling such scenarios.

**Abstract:** Large Language Models (LLMs) have demonstrated strong performance in question answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a question may have several valid answers, remains challenging. Traditional QA settings often assume consistency across evidences, but MAQA can involve conflicting answers. Constructing datasets that reflect such conflicts is costly and labor-intensive, while existing benchmarks often rely on synthetic data, restrict the task to yes/no questions, or apply unverified automated annotation. To advance research in this area, we extend the conflict-aware MAQA setting to require models not only to identify all valid answers, but also to detect specific conflicting answer pairs, if any. To support this task, we introduce a novel cost-effective methodology for leveraging fact-checking datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate eight high-end LLMs on NATCONFQA, revealing their fragility in handling various types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [56] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)

*Yuanfeng Xu, Zehui Dai, Jian Liang, Jiapeng Guan, Guangrun Wang, Liang Lin, Xiaohui Lv*

**Main category:** cs.CL

**Keywords:** Small Language Models, Reinforcement Learning, Reasoning

**Relevance Score:** 7

**TL;DR:** ReaLM is a framework to enhance Small Language Models' reasoning, autonomy, and generalization in vertical domains through novel training strategies.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of Small Language Models in reasoning capability, autonomy, and generalization due to their inherent biases and over-reliance on external signals.

**Method:** The framework includes Multi-Route Process Verification (MRPV) to contrast reasoning paths and a training strategy called Enabling Autonomy via Asymptotic Induction (EAAI) to reduce external guidance.

**Key Contributions:**

	1. Introduction of the ReaLM framework for Small Language Models.
	2. Development of Multi-Route Process Verification for contrasting reasoning paths.
	3. Implementation of Enabling Autonomy via Asymptotic Induction to improve model autonomy.

**Result:** ReaLM shows significant improvements in Small Language Models' performance on both vertical and general reasoning tasks, demonstrating enhanced reasoning capability, autonomy, and generalization.

**Limitations:** 

**Conclusion:** The proposed methods in ReaLM provide a structured approach to improve SLMs' shortfalls and can be beneficial in applications requiring robust reasoning.

**Abstract:** Small Language Models (SLMs) are a cost-effective alternative to Large Language Models (LLMs), but often struggle with complex reasoning due to their limited capacity and a tendency to produce mistakes or inconsistent answers during multi-step reasoning. Existing efforts have improved SLM performance, but typically at the cost of one or more of three key aspects: (1) reasoning capability, due to biased supervision that filters out negative reasoning paths and limits learning from errors; (2) autonomy, due to over-reliance on externally generated reasoning signals; and (3) generalization, which suffers when models overfit to teacher-specific patterns. In this paper, we introduce ReaLM, a reinforcement learning framework for robust and self-sufficient reasoning in vertical domains. To enhance reasoning capability, we propose Multi-Route Process Verification (MRPV), which contrasts both positive and negative reasoning paths to extract decisive patterns. To reduce reliance on external guidance and improve autonomy, we introduce Enabling Autonomy via Asymptotic Induction (EAAI), a training strategy that gradually fades external signals. To improve generalization, we apply guided chain-of-thought distillation to encode domain-specific rules and expert knowledge into SLM parameters, making them part of what the model has learned. Extensive experiments on both vertical and general reasoning tasks demonstrate that ReaLM significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [57] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)

*Duzhen Zhang, Zixiao Wang, Zhong-Zhi Li, Yahan Yu, Shuncheng Jia, Jiahua Dong, Haotian Xu, Xing Wu, Yingying Zhang, Tielin Zhang, Jie Yang, Xiuying Chen, Le Song*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Medical Literature, Large Language Models

**Relevance Score:** 9

**TL;DR:** MedKGent is a framework for constructing temporally evolving medical knowledge graphs (KGs) using LLMs, addressing limitations in current methods for biomedical literature integration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid growth of medical literature presents challenges in structuring and integrating knowledge, which Knowledge Graphs can help address, yet current methods are limited by their approaches.

**Method:** MedKGent uses a framework with two specialized agents: the Extractor Agent identifies knowledge triples and assigns confidence scores, while the Constructor Agent integrates these into an evolving KG, using a daily time series of PubMed abstracts.

**Key Contributions:**

	1. Introduction of the MedKGent framework for dynamic KG construction
	2. Improvement in biomedical knowledge retrieval using LLM-powered agents
	3. Demonstrated application in literature-based drug repurposing

**Result:** The constructed KG encompasses 156,275 entities and 2,971,384 relational triples with an accuracy nearing 90%.

**Limitations:** 

**Conclusion:** MedKGent demonstrates significant improvements in medical question answering and highlights the KG's value in drug repurposing through causal inference.

**Abstract:** The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.

</details>


### [58] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)

*Zilong Bai, Zihan Xu, Cong Sun, Chengxi Zang, H. Timothy Bunnell, Catherine Sinfield, Jacqueline Rutter, Aaron Thomas Martinez, L. Charles Bailey, Mark Weiner, Thomas R. Campion, Thomas Carton, Christopher B. Forrest, Rainu Kaushal, Fei Wang, Yifan Peng*

**Main category:** cs.CL

**Keywords:** PASC, NLP, health informatics, BERT, named entity recognition

**Relevance Score:** 9

**TL;DR:** The paper presents a hybrid NLP pipeline for diagnosing Post-Acute Sequelae of COVID-19 (PASC) by integrating named entity recognition and BERT-based assertion detection, achieving high accuracy in symptom extraction from clinical notes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Diagnosing PASC is challenging due to its evolving symptoms that manifest over varying time intervals, necessitating improved methods for extraction from clinical notes.

**Method:** A hybrid natural language processing pipeline integrating rule-based named entity recognition with BERT-based assertion detection modules was developed. A comprehensive lexicon for PASC was created with clinical specialists' input. The approach was evaluated using 160 intake progress notes for model development and 47,654 notes for a population-level prevalence study.

**Key Contributions:**

	1. Development of a hybrid NLP pipeline for PASC symptom extraction.
	2. Creation of a comprehensive PASC lexicon in collaboration with clinical experts.
	3. High validation scores indicate the reliability of the approach.

**Result:** The model achieved an average F1 score of 0.82 in internal validation and 0.76 in external validation for assertion detection. The average processing time for each note was $2.448\pm 0.812$ seconds, with high correlation coefficients for detection accuracy.

**Limitations:** 

**Conclusion:** The hybrid NLP pipeline demonstrates effectiveness and efficiency in improving the diagnosis of PASC, indicating its potential for practical application in health informatics.

**Abstract:** Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC) remains challenging due to its myriad symptoms that evolve over long- and variable-time intervals. To address this issue, we developed a hybrid natural language processing pipeline that integrates rule-based named entity recognition with BERT-based assertion detection modules for PASC-symptom extraction and assertion detection from clinical notes. We developed a comprehensive PASC lexicon with clinical specialists. From 11 health systems of the RECOVER initiative network across the U.S., we curated 160 intake progress notes for model development and evaluation, and collected 47,654 progress notes for a population-level prevalence study. We achieved an average F1 score of 0.82 in one-site internal validation and 0.76 in 10-site external validation for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$ seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These demonstrate the effectiveness and efficiency of our models and their potential for improving PASC diagnosis.

</details>


### [59] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)

*Zhuorui Liu, Chen Zhang, Dawei Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, Key-Value Cache, Attention Heads, Performance Optimization, Latent Memory Consumption

**Relevance Score:** 9

**TL;DR:** This paper proposes ZigzagAttention, an approach to improve long-context handling in large language models by optimizing the memory footprint of the KV cache and reducing latency related to attention heads.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in deploying large language models with long context due to increased KV cache memory consumption and latency from managing retrieval and streaming heads within attention layers.

**Method:** The authors designed a criterion to categorize attention heads into retrieval and streaming heads exclusively within one layer to minimize latency while preserving performance.

**Key Contributions:**

	1. Introduction of ZigzagAttention to optimize KV cache usage in LLMs
	2. Improved identification process for retrieval vs. streaming heads
	3. Reduction of latency without significant performance degradation

**Result:** ZigzagAttention reduces overall latency and maintains competitive performance among various baselines, achieving improved efficiency in memory usage during attention processes.

**Limitations:** 

**Conclusion:** This approach successfully identifies attention heads to minimize latency effects during deployment of large language models and offers performance comparable to existing methods.

**Abstract:** With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.

</details>


### [60] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)

*Emanuel Z. Fenech-Borg, Tilen P. Meznaric-Kos, Milica D. Lekovic-Bojovic, Arni J. Hentze-Djurhuus*

**Main category:** cs.CL

**Keywords:** Cultural Gene, Large Language Models, Cultural Probe Dataset, Individualism-Collectivism, Power Distance

**Relevance Score:** 8

**TL;DR:** This paper explores the cultural and ethical assumptions of large language models (LLMs) through the introduction of a Cultural Probe Dataset (CPD) and compares Western-centric and Eastern-centric models using cross-cultural dimensions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the cultural values that large language models inherit from their training data and to promote culturally aware evaluation and deployment.

**Method:** The authors introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting the cross-cultural dimensions of Individualism-Collectivism (IDV) and Power Distance (PDI). They compare responses from the Western-centric model GPT-4 and the Eastern-centric model ERNIE Bot, employing human annotations and computing a Cultural Alignment Index (CAI) against Hofstede's national scores.

**Key Contributions:**

	1. Introduction of the Cultural Probe Dataset (CPD) for exploring cultural dimensions in LLMs.
	2. Comparison of cross-cultural orientations in GPT-4 versus ERNIE Bot using statistical analysis.
	3. Development of the Cultural Alignment Index (CAI) to assess alignment with Hofstede's cultural dimensions.

**Result:** Significant divergence was found in the cultural orientations of GPT-4 and ERNIE Bot, with GPT-4 exhibiting individualistic and low-power-distance tendencies while ERNIE Bot displayed collectivistic and higher-power-distance tendencies. Statistical significance was noted with p < 0.001.

**Limitations:** 

**Conclusion:** The findings indicate that LLMs reflect the cultural biases present in their training data, underscoring the importance of culturally aware evaluation to mitigate the risks of algorithmic cultural hegemony.

**Abstract:** Large language models (LLMs) are deployed globally, yet their underlying cultural and ethical assumptions remain underexplored. We propose the notion of a "cultural gene" -- a systematic value orientation that LLMs inherit from their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting two classic cross-cultural dimensions: Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized zero-shot prompts, we compare a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot). Human annotation shows significant and consistent divergence across both dimensions. GPT-4 exhibits individualistic and low-power-distance tendencies (IDV score approx 1.21; PDI score approx -1.05), while ERNIE Bot shows collectivistic and higher-power-distance tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically significant (p < 0.001). We further compute a Cultural Alignment Index (CAI) against Hofstede's national scores and find GPT-4 aligns more closely with the USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative analyses of dilemma resolution and authority-related judgments illustrate how these orientations surface in reasoning. Our results support the view that LLMs function as statistical mirrors of their cultural corpora and motivate culturally aware evaluation and deployment to avoid algorithmic cultural hegemony.

</details>


### [61] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)

*Yeongwoo Song, Jaeyong Bae, Dong-Kyum Kim, Hawoong Jeong*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, physics, sparse autoencoders, dynamics forecasting

**Relevance Score:** 9

**TL;DR:** This paper investigates the in-context learning abilities of large language models (LLMs) through the study of physics-based tasks, revealing how LLMs can learn and reason about physics from contextual data.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms that enable large language models to exhibit in-context learning (ICL) across diverse tasks, specifically in the realm of physics.

**Method:** The study employs a dynamics forecasting task within physical systems, analyzing LLMs' performance as input context length increases and using sparse autoencoders to evaluate model activations.

**Key Contributions:**

	1. Mechanistic investigation of LLMs' reasoning abilities in physics
	2. Demonstration of improved performance with longer input contexts
	3. Correlation of model activations with fundamental physical concepts

**Result:** The performance of LLMs in dynamics forecasting improved with longer input contexts, and analysis showed that features captured by sparse autoencoders correlate with key physical variables like energy.

**Limitations:** 

**Conclusion:** The findings reveal that meaningful physical concepts are encoded within LLMs during in-context learning, enhancing our understanding of their learning capabilities.

**Abstract:** Large language models (LLMs) exhibit impressive in-context learning (ICL) abilities, enabling them to solve wide range of tasks via textual prompts alone. As these capabilities advance, the range of applicable domains continues to expand significantly. However, identifying the precise mechanisms or internal structures within LLMs that allow successful ICL across diverse, distinct classes of tasks remains elusive. Physics-based tasks offer a promising testbed for probing this challenge. Unlike synthetic sequences such as basic arithmetic or symbolic equations, physical systems provide experimentally controllable, real-world data based on structured dynamics grounded in fundamental principles. This makes them particularly suitable for studying the emergent reasoning behaviors of LLMs in a realistic yet tractable setting. Here, we mechanistically investigate the ICL ability of LLMs, especially focusing on their ability to reason about physics. Using a dynamics forecasting task in physical systems as a proxy, we evaluate whether LLMs can learn physics in context. We first show that the performance of dynamics forecasting in context improves with longer input contexts. To uncover how such capability emerges in LLMs, we analyze the model's residual stream activations using sparse autoencoders (SAEs). Our experiments reveal that the features captured by SAEs correlate with key physical variables, such as energy. These findings demonstrate that meaningful physical concepts are encoded within LLMs during in-context learning. In sum, our work provides a novel case study that broadens our understanding of how LLMs learn in context.

</details>


### [62] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)

*Ruirui Gao, Emily Johnson, Bowen Tan, Yanfei Qian*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, preference optimization, multimodal instruction following

**Relevance Score:** 8

**TL;DR:** Proposes M3PO, a data-efficient method for optimizing Large Vision-Language Models (LVLMs) in visual instruction following by selecting high-value preference samples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the high cost and inconsistency of human annotation in fine-tuning LVLMs, particularly for multimodal instruction following tasks.

**Method:** M3PO uses a selection mechanism combining Multimodal Alignment Score and Self-Consistency/Confidence to identify valuable preference sample pairs for Direct Preference Optimization.

**Key Contributions:**

	1. Introduction of M3PO for efficient preference optimization
	2. Use of M3P-Score for identifying valuable sample pairs
	3. Demonstrated superior performance over existing methods.

**Result:** M3PO outperforms traditional fine-tuning methods and existing preference optimization techniques on multiple multimodal instruction following benchmarks.

**Limitations:** 

**Conclusion:** M3PO enhances the efficiency of fine-tuning LVLMs by utilizing high-quality preference pairs for better instruction following performance.

**Abstract:** Large Vision-Language Models (LVLMs) hold immense potential for complex multimodal instruction following, yet their development is often hindered by the high cost and inconsistency of human annotation required for effective fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT) and existing preference optimization methods like RLHF and DPO frequently struggle to efficiently leverage the model's own generation space to identify highly informative "hard negative" samples. To address these challenges, we propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and data-efficient method designed to enhance LVLMs' capabilities in visual instruction following. M3PO intelligently selects the most "learning-valuable" preference sample pairs from a diverse pool of LVLM-generated candidates. This selection is driven by a sophisticated mechanism that integrates two crucial signals: a Multimodal Alignment Score (MAS) to assess external quality and the model's Self-Consistency / Confidence (log-probability) to gauge internal belief. These are combined into a novel M3P-Score, which specifically identifies preferred responses and challenging dispreferred responses that the model might confidently generate despite being incorrect. These high-quality preference pairs are then used for efficient Direct Preference Optimization (DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our extensive experiments demonstrate that M3PO consistently outperforms strong baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a comprehensive suite of multimodal instruction following benchmarks (MME-Bench, POPE, IFT, Human Pref. Score).

</details>


### [63] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)

*Alham Fikri Aji, Trevor Cohn*

**Main category:** cs.CL

**Keywords:** NLP, Low-resource languages, Benchmark, LLM, Indonesian

**Relevance Score:** 7

**TL;DR:** Introducing LoraxBench, a benchmark for evaluating NLP on Indonesia's low-resource languages across six tasks, revealing performance discrepancies among languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Indonesia, with 700 languages, has lagged in NLP progress, necessitating a benchmark for its low-resource languages.

**Method:** Developed LoraxBench as a benchmark covering reading comprehension, QA, language inference, causal reasoning, translation, and cultural QA across 20 languages.

**Key Contributions:**

	1. Introduction of LoraxBench for low-resource Indonesian languages
	2. Evaluation of multilingual models on diverse NLP tasks
	3. Analysis of the impact of language register on model performance

**Result:** Evaluation of multilingual and region-focused LLMs highlighted challenges in NLP for low-resource Indonesian languages and performance discrepancies based on language and register.

**Limitations:** Focused on low-resource languages; results may not generalize to other contexts outside Indonesia.

**Conclusion:** The benchmark emphasizes the need for targeted NLP approaches for low-resource languages and shows the impact of language politeness registers on model performance.

**Abstract:** As one of the world's most populous countries, with 700 languages spoken, Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Our dataset covers 20 languages, with the addition of two formality registers for three languages. We evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging. We note a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, we show that a change in register affects model performance, especially with registers not commonly found in social media, such as high-level politeness `Krama' Javanese.

</details>


### [64] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)

*Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song*

**Main category:** cs.CL

**Keywords:** OpenAI, GPT-OSS, large language models, code generation, multilingual understanding

**Relevance Score:** 9

**TL;DR:** A comparison of OpenAI's GPT-OSS models with contemporary open-source LLMs reveals that the 20B variant often outperforms the 120B model while consuming less memory and energy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the performance of OpenAI's latest large language models against other contemporary models and to understand the efficiency and effectiveness of scaling in sparse architectures.

**Method:** The models were tested using standardised inference settings across ten benchmarks, employing McNemars test and effect size analysis for statistical validation.

**Key Contributions:**

	1. Evaluation of new GPT-OSS models
	2. Insights into performance of sparse architecture scalability
	3. Comparison with contemporary open-source models

**Result:** GPT-OSS 20B consistently outperformed GPT-OSS 120B on several benchmarks like HumanEval and MMLU, with both models showing mid-tier performance overall, strong in code generation but weak in multilingual tasks.

**Limitations:** Performance weaknesses noted in multilingual tasks.

**Conclusion:** Scaling in sparse architectures may not produce proportional performance gains, suggesting a need for further optimisation strategies and informing model selection for future deployments.

**Abstract:** In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments.

</details>


### [65] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)

*Xiaomeng Zhu, R. Thomas McCoy, Robert Frank*

**Main category:** cs.CL

**Keywords:** Syntactic bootstrapping, Verbs, Large language models, RoBERTa, GPT-2

**Relevance Score:** 7

**TL;DR:** The paper investigates whether large language models, like RoBERTa and GPT-2, learn verb meanings through syntactic cues similar to children, finding that the degradation of verb representation is greater when syntactic cues are removed rather than co-occurrence information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore if large language models (LLMs) exhibit syntactic bootstrapping in verb meaning acquisition akin to children's language learning.

**Method:** Trained RoBERTa and GPT-2 on perturbed datasets with abbated syntactic information and measured the impact on verb and noun representations.

**Key Contributions:**

	1. Demonstrated the influence of syntactic cues on verb representation in LLMs.
	2. Highlighted differences in learning impacts between mental and physical verbs compared to nouns.
	3. Provided a framework for exploring developmental language hypotheses using LLMs.

**Result:** Models' verb representations were significantly degraded when syntactic information was ablated, particularly for mental verbs compared to physical verbs; noun representation was more affected by co-occurrence distortion.

**Limitations:** 

**Conclusion:** The findings support the hypothesis that syntactic bootstrapping plays a crucial role in verb learning and demonstrate a method for testing developmental hypotheses in LLMs.

**Abstract:** Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use the syntactic environments in which a verb occurs to learn its meaning. In this paper, we examine whether large language models exhibit a similar behavior. We do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic information is ablated. Our results show that models' verb representation degrades more when syntactic cues are removed than when co-occurrence information is removed. Furthermore, the representation of mental verbs, for which syntactic bootstrapping has been shown to be particularly crucial in human verb learning, is more negatively impacted in such training regimes than physical verbs. In contrast, models' representation of nouns is affected more when co-occurrences are distorted than when syntax is distorted. In addition to reinforcing the important role of syntactic bootstrapping in verb learning, our results demonstrated the viability of testing developmental hypotheses on a larger scale through manipulating the learning environments of large language models.

</details>


### [66] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)

*Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Causal Reasoning, Directed Acyclic Graph, Hallucinations, Supervised Fine-Tuning

**Relevance Score:** 9

**TL;DR:** The paper introduces CDCR-SFT, a framework for enhancing causal reasoning in LLMs by constructing directed acyclic graphs (DAGs) to reduce hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often produce logically inconsistent outputs due to hallucinations, yet previous reasoning methods fail to model causal relationships effectively.

**Method:** A supervised fine-tuning framework (CDCR-SFT) is proposed, which trains LLMs to construct variable-level DAGs and reason over them.

**Key Contributions:**

	1. Introduction of a novel framework for causal DAG construction in LLMs
	2. Creation of the CausalDR dataset with 25,368 samples for training and evaluation
	3. Significant improvements in causal reasoning accuracy and reduction in hallucinations

**Result:** CDCR-SFT achieves state-of-the-art accuracy of 95.33% on the CLADDER benchmark and reduces hallucinations by 10% on HaluEval.

**Limitations:** 

**Conclusion:** Modeling explicit causal structures in LLMs alleviates logical inconsistencies in their outputs.

**Abstract:** Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.

</details>


### [67] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)

*Seonglae Cho, Zekun Wu, Adriano Koshiyama*

**Main category:** cs.CL

**Keywords:** Sparse Autoencoders, feature selection, language models, inference-time, steering tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces CorrSteer, a method for selecting features from Sparse Autoencoders (SAEs) using inference-time activations to improve downstream steering tasks in language models.

**Read time:** 42 min

<details>
  <summary>Details</summary>

**Motivation:** The limitations of Sparse Autoencoders in effectively steering large language models due to the need for contrastive datasets and significant storage of activation data.

**Method:** CorrSteer correlates sample correctness with SAE activations at inference time to select features, automating the selection process based on average activations.

**Key Contributions:**

	1. Introduction of CorrSteer for feature selection
	2. Improvement in performance metrics for QA, bias mitigation, and reasoning
	3. Automated extraction of relevant features using inference-time activations

**Result:** The method improves task performance, notably achieving a +4.1% enhancement in MMLU performance and a +22.9% boost in HarmBench results while using only 4000 samples, showcasing effective feature selection aligned with task requirements.

**Limitations:** 

**Conclusion:** CorrSteer establishes correlation-based feature selection as a scalable and effective strategy for enhancing the performance of large language models in various tasks.

**Abstract:** Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.

</details>


### [68] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)

*Yu-Hsuan Fang, Tien-Hong Lo, Yao-Ting Sung, Berlin Chen*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Automated Speaking Assessment, Speech-First Multimodal Training

**Relevance Score:** 8

**TL;DR:** The paper explores the use of Multimodal Large Language Models for comprehensive Automated Speaking Assessment, addressing inherent limitations of traditional systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Automated Speaking Assessment by overcoming limitations of existing text-based and audio-based methods.

**Method:** The paper introduces Speech-First Multimodal Training (SFMT) that utilizes curriculum learning to enhance training for MLLM in ASA.

**Key Contributions:**

	1. First systematic study of MLLM in ASA
	2. Introduction of Speech-First Multimodal Training (SFMT)
	3. Demonstrated improvements in assessment performance using MLLM.

**Result:** Experiments show that MLLM improves holistic assessment performance from a PCC value of 0.783 to 0.846, with SFMT yielding a 4% accuracy improvement in delivery assessment.

**Limitations:** Challenges in evaluating the delivery aspect still require specialized training strategies.

**Conclusion:** MLLM combined with SFMT presents a significant advancement in Automated Speaking Assessment, particularly for evaluating delivery aspects.

**Abstract:** Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4% over conventional training approaches, which also paves a new avenue for ASA.

</details>


### [69] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)

*Maitreyi Chatterjee, Devansh Agarwal*

**Main category:** cs.CL

**Keywords:** Large Language Models, Memory Architecture, Human-Computer Interaction, Natural Language Processing, Dialogue Systems

**Relevance Score:** 9

**TL;DR:** The paper introduces Semantic Anchoring, a memory architecture that improves long-term dialogue interactions in LLMs by integrating explicit linguistic structures into vector storage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current RAG systems store dialogue history ineffectively, limiting LLMs' performance in multi-session interactions due to their lack of memory persistence and neglect of linguistic structures.

**Method:** The proposed approach combines dependency parsing, discourse relation tagging, and coreference resolution to enhance vector-based memory storage with structured entries that include explicit linguistic cues.

**Key Contributions:**

	1. Introduction of Semantic Anchoring as a hybrid memory architecture for LLMs
	2. Improved factual recall and coherence in dialogue through enhanced memory structures
	3. Experimental evidence supporting the effectiveness of the proposed method

**Result:** Experiments show that Semantic Anchoring improves factual recall and discourse coherence by up to 18% compared to standard RAG systems.

**Limitations:** 

**Conclusion:** Semantic Anchoring enhances LLMs' ability to manage long-term dialogue by adding linguistic context to memory storage, demonstrating potential for improved human-computer interactions.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive fluency and task competence in conversational settings. However, their effectiveness in multi-session and long-term interactions is hindered by limited memory persistence. Typical retrieval-augmented generation (RAG) systems store dialogue history as dense vectors, which capture semantic similarity but neglect finer linguistic structures such as syntactic dependencies, discourse relations, and coreference links. We propose Semantic Anchoring, a hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues to improve recall of nuanced, context-rich exchanges. Our approach combines dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries. Experiments on adapted long-term dialogue datasets show that semantic anchoring improves factual recall and discourse coherence by up to 18% over strong RAG baselines. We further conduct ablation studies, human evaluations, and error analysis to assess robustness and interpretability.

</details>


### [70] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)

*Yiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng Ye, Lei Bai, Shuyue Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Performance Efficiency, Test-Time Routing, Dynamic Query Assignment, Pareto Frontier

**Relevance Score:** 8

**TL;DR:** Avengers-Pro is a test-time routing framework that dynamically assigns queries to LLMs with varying efficiency and capacity, achieving state-of-the-art results in balancing performance and cost.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the central challenge of balancing performance and efficiency in large language model advancement.

**Method:** Avengers-Pro dynamically routes queries to the most suitable model based on clustered incoming queries and a performance-efficiency score.

**Key Contributions:**

	1. Development of a unified test-time routing framework for LLMs
	2. Achieving state-of-the-art performance with cost efficiency
	3. Establishing a Pareto frontier for performance-cost trade-offs

**Result:** Achieves state-of-the-art results across 6 benchmarks and 8 models, exceeding the accuracy of the strongest single model by +7% while providing significant cost savings.

**Limitations:** 

**Conclusion:** Avengers-Pro consistently yields the best accuracy for a given cost and the lowest cost for a given accuracy, establishing a Pareto frontier.

**Abstract:** Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [71] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)

*Chi Wang, Min Gao, Zongwei Wang, Junwei Yin, Kai Shu, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** fake news detection, language models, linguistic fingerprints

**Relevance Score:** 8

**TL;DR:** This paper presents Linguistic Fingerprints Extraction (LIFE), a novel method for detecting fake news generated by large language models (LLMs) based on linguistic features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large language models has led to easy generation of fake news, highlighting the urgent need for effective detection methods.

**Method:** The proposed method, LIFE, analyzes distributional divergence to extract linguistic fingerprints from LLM-generated content, identifying distinct probability shifts when prompted maliciously.

**Key Contributions:**

	1. Introduction of Linguistic Fingerprints Extraction (LIFE) for fake news detection
	2. Utilization of distributional divergence analysis to identify linguistic patterns
	3. Demonstrated state-of-the-art detection performance for LLM-generated and human-written fake news

**Result:** LIFE demonstrates state-of-the-art performance in detecting LLM-generated fake news while also performing well on human-written fake news.

**Limitations:** 

**Conclusion:** The study emphasizes the potential of linguistic fingerprints in enhancing the reliability of fake news detection and provides accessible code and data for implementation.

**Abstract:** With the rapid development of large language models, the generation of fake news has become increasingly effortless, posing a growing societal threat and underscoring the urgent need for reliable detection methods. Early efforts to identify LLM-generated fake news have predominantly focused on the textual content itself; however, because much of that content may appear coherent and factually consistent, the subtle traces of falsification are often difficult to uncover. Through distributional divergence analysis, we uncover prompt-induced linguistic fingerprints: statistically distinct probability shifts between LLM-generated real and fake news when maliciously prompted. Based on this insight, we propose a novel method named Linguistic Fingerprints Extraction (LIFE). By reconstructing word-level probability distributions, LIFE can find discriminative patterns that facilitate the detection of LLM-generated fake news. To further amplify these fingerprint patterns, we also leverage key-fragment techniques that accentuate subtle linguistic differences, thereby improving detection reliability. Our experiments show that LIFE achieves state-of-the-art performance in LLM-generated fake news and maintains high performance in human-written fake news. The code and data are available at https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [72] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)

*Tanay Nagar, Grigorii Khvatskii, Anna Sokol, Nitesh V. Chawla*

**Main category:** cs.CL

**Keywords:** Low-resource languages, Common Sense Reasoning, Code-switching, Language models, Multilingual communication

**Relevance Score:** 8

**TL;DR:** This paper proposes a method to enhance LLM performance in low-resource languages through fine-tuning on synthetic code-switched datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the disparity in LLM performance between high-resource and low-resource languages in Common Sense Reasoning tasks, ensuring fairness in multilingual communication.

**Method:** The authors fine-tune an LLM on synthetic code-switched text generated through controlled language-mixing techniques.

**Key Contributions:**

	1. Introduction of a method to improve LLMs for low-resource languages
	2. Creation of a synthetic code-switched dataset derived from CommonSenseQA
	3. Demonstration of improved CSR task performance across language resources

**Result:** Fine-tuning LLMs on synthetic code-switched datasets significantly improves performance in low-resource language models while maintaining or boosting performance in high-resource languages.

**Limitations:** 

**Conclusion:** This approach not only enhances LRL performance but also contributes a new dataset for further research in multilingual LLM applications.

**Abstract:** Cutting-edge LLMs have emerged as powerful tools for multilingual communication and understanding. However, LLMs perform worse in Common Sense Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi or Swahili compared to high-resource languages (HRLs) like English. Equalizing this inconsistent access to quality LLM outputs is crucial to ensure fairness for speakers of LRLs and across diverse linguistic communities. In this paper, we propose an approach to bridge this gap in LLM performance. Our approach involves fine-tuning an LLM on synthetic code-switched text generated using controlled language-mixing methods. We empirically demonstrate that fine-tuning LLMs on synthetic code-switched datasets leads to substantial improvements in LRL model performance while preserving or enhancing performance in HRLs. Additionally, we present a new dataset of synthetic code-switched text derived from the CommonSenseQA dataset, featuring three distinct language ratio configurations.

</details>


### [73] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)

*Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy*

**Main category:** cs.CL

**Keywords:** Large Language Models, Misery scoring, Affective computing, Gamified evaluation

**Relevance Score:** 9

**TL;DR:** This study explores using LLMs for predicting misery scores from text, employing various prompting strategies and introducing a gamified evaluation framework.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To predict human-perceived misery scores from natural language descriptions using LLMs.

**Method:** The study frames the prediction task as regression and evaluates prompting strategies such as zero-shot, few-shot, and a novel gamified framework called the 'Misery Game Show'.

**Key Contributions:**

	1. Introduction of the 'Misery Game Show' for evaluating LLMs
	2. Demonstration of few-shot prompting advantages over zero-shot
	3. Expansion of LLM applications in emotional reasoning beyond simple regression.

**Result:** Few-shot prompting approaches significantly outperform zero-shot baselines, demonstrating the effectiveness of contextual examples in predicting misery.

**Limitations:** 

**Conclusion:** The 'Misery Game Show' framework not only evaluates predictive accuracy but also assesses LLMs' adaptability through corrective feedback, indicating their potential in dynamic emotional reasoning tasks.

**Abstract:** This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [74] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)

*Xingshan Zeng, Weiwen Liu, Lingzhi Wang, Liangyou Li, Fei Mi, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, agentic task-solving, data generation

**Relevance Score:** 8

**TL;DR:** The paper introduces ToolACE-MT, a Non-Autoregressive Iterative Generation framework for high-quality multi-turn agentic dialogues, addressing limitations of existing simulation-based data generation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of agentic task-solving with LLMs, which currently relies on costly autoregressive interactions.

**Method:** The ToolACE-MT framework constructs dialogues through three stages: coarse-grained initialization, iterative refinement, and offline verification.

**Key Contributions:**

	1. Introduction of ToolACE-MT framework
	2. Three-stage dialogue construction process
	3. Enhanced efficiency and effectiveness in data generation

**Result:** Experiments show that ToolACE-MT generates efficient, effective, and generalizable agentic data, enhancing data construction in tool-augmented LLM scenarios.

**Limitations:** 

**Conclusion:** ToolACE-MT offers a novel approach for generating high-quality multi-turn dialogues, overcoming limitations of previous methods.

**Abstract:** Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.

</details>


### [75] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)

*Weize Liu, Yongchi Zhao, Yijia Luo, Mingyu Xu, Jiaheng Liu, Yanan Li, Xiguo Hu, Yuchi Xu, Wenbo Su, Bo Zheng*

**Main category:** cs.CL

**Keywords:** large language models, reasoning dataset, multidisciplinary, design logic, synthesis pipeline

**Relevance Score:** 8

**TL;DR:** The paper presents DESIGNER, a data synthesis pipeline for generating multidisciplinary reasoning questions using large language models (LLMs) and extensive raw documents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reasoning datasets lack depth and breadth, leading to insufficient complex, multi-step reasoning capabilities in LLMs across diverse disciplines.

**Method:** DEISGNER synthesizes reasoning questions by leveraging LLMs to reverse-engineer over 120,000 design logics from existing questions and matching them with source materials from books and web documents.

**Key Contributions:**

	1. Introduction of DESIGNER for data synthesis of complex reasoning questions.
	2. Creation of two extensive reasoning datasets (DLR-Book and DLR-Web) with high difficulty and diversity.
	3. Demonstration of improved reasoning performance in LLMs trained on the new datasets.

**Result:** Two large-scale datasets, DLR-Book (3.04 million questions) and DLR-Web (1.66 million questions), were synthesized, showing significantly greater difficulty and diversity than existing datasets, validated by strong SFT performance of LLMs.

**Limitations:** The methodology relies on the quality of the source documents and may not cover all potential domains adequately.

**Conclusion:** The proposed datasets enhance the reasoning capabilities of LLMs in multidisciplinary contexts by providing more challenging and diverse questions for model training.

**Abstract:** Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (book corpus and web corpus) to generate multidisciplinary challenging questions. A core innovation of our approach is the introduction of a Design Logic concept, which mimics the question-creation process of human educators. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with disciplinary source materials, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Based on this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book), containing 3.04 million challenging questions synthesized from the book corpus, and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging questions from the web corpus. Our data analysis demonstrates that the questions synthesized by our method exhibit substantially greater difficulty and diversity than those in the baseline datasets. We validate the effectiveness of these datasets by conducting SFT experiments on the Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset significantly outperforms existing multidisciplinary datasets of the same volume. Training with the full datasets further enables the models to surpass the multidisciplinary reasoning performance of the official Qwen3-8B and Qwen3-4B models.

</details>


### [76] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)

*Zhiyuan Ning, Tianle Gu, Jiaxin Song, Shixin Hong, Lingyu Li, Huacan Liu, Jie Li, Yixu Wang, Meng Lingyu, Yan Teng, Yingchun Wang*

**Main category:** cs.CL

**Keywords:** multilingual safety, large language models, safety evaluation

**Relevance Score:** 9

**TL;DR:** Introduction of LinguaSafe, a comprehensive multilingual safety benchmark for evaluating large language models (LLMs).

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a rigorous approach to ensure the safety of LLMs across diverse linguistic and cultural contexts, addressing the gaps in existing multilingual evaluations.

**Method:** Development of the LinguaSafe dataset, which includes 45k entries in 12 languages, using translated, transcreated, and natively-sourced data, along with a multidimensional evaluation framework.

**Key Contributions:**

	1. LinguaSafe dataset with extensive multilingual entries
	2. Multidimensional evaluation framework for LLM safety
	3. Public release of dataset and evaluation code

**Result:** Safety and helpfulness evaluations show significant variation across different domains and languages, indicating the importance of tailored safety assessments for LLMs.

**Limitations:** 

**Conclusion:** LinguaSafe fills a critical gap in the multilingual safety evaluation of LLMs and encourages further research by providing a comprehensive suite of metrics and publicly released data.

**Abstract:** The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.

</details>


### [77] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)

*Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Penge*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Natural Language Processing, Large Language Models

**Relevance Score:** 9

**TL;DR:** CRED-SQL is a framework that improves Text-to-SQL systems for large-scale databases by integrating cluster retrieval and an Execution Description Language to mitigate semantic mismatches between natural language questions and SQL queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of semantic mismatch between natural language questions and corresponding SQL queries in large-scale databases, which reduces accuracy.

**Method:** CRED-SQL employs cluster-based schema retrieval and introduces an Execution Description Language (EDL) to facilitate the transition from natural language to SQL, decomposing the process into Text-to-EDL and EDL-to-SQL stages.

**Key Contributions:**

	1. Introduction of the CRED-SQL framework for improved Text-to-SQL accuracy.
	2. Development of cluster-based schema retrieval to alleviate semantic mismatch.
	3. Proposal of the Execution Description Language (EDL) to create an intermediate representation.

**Result:** CRED-SQL achieves state-of-the-art performance on large-scale cross-domain benchmarks, significantly improving the accuracy of Text-to-SQL systems.

**Limitations:** 

**Conclusion:** The framework effectively addresses the semantic mismatch problem, demonstrating effectiveness and scalability in performance.

**Abstract:** Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [78] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)

*Javier Garcia Gilabert, Xixian Liao, Severino Da Dalt, Ella Bohman, Audrey Mash, Francesca De Luca Fornaciari, Irene Baucells, Joan Llop, Miguel Claramunt Argote, Carlos Escolano, Maite Melero*

**Main category:** cs.CL

**Keywords:** SALAMANDRATA, LLM, translation, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** The paper introduces the SALAMANDRATA family of LLMs aimed at improving translation tasks for 38 European languages, showcasing enhancements over previous models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind developing SALAMANDRATA was to improve translation performance across a wide range of European languages, addressing limitations in existing models.

**Method:** The models were trained using a two-step approach: initial continual pre-training on parallel data followed by supervised fine-tuning on high-quality instructions. The BSC submission to the WMT25 task utilized the 7B variant with specific adaptations for vocabulary and additional non-European languages.

**Key Contributions:**

	1. Introduction of SALAMANDRATA models with improved translation capabilities
	2. Application of quality-aware decoding strategies for enhanced performance
	3. Public release of multiple model variants on Hugging Face

**Result:** The SALAMANDRATA models, specifically the 7B variant, demonstrated strong translation performance in trials, benefiting from advanced decoding strategies like Minimum Bayes Risk Decoding and Tuned Re-ranking.

**Limitations:** 

**Conclusion:** The models are publicly released on Hugging Face, providing access to both the 2B and 7B versions, alongside the SALAMANDRATA-V2 model for further enhancements in translation tasks.

**Abstract:** In this paper, we present the SALAMANDRATA family of models, an improved iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically trained to achieve strong performance in translation-related tasks for 38 European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For both versions, we applied the same training recipe with a first step of continual pre-training on parallel data, and a second step of supervised fine-tuning on high-quality instructions. The BSC submission to the WMT25 General Machine Translation shared task is based on the 7B variant of SALAMANDRATA. We first adapted the model vocabulary to support the additional non-European languages included in the task. This was followed by a second phase of continual pre-training and supervised fine-tuning, carefully designed to optimize performance across all translation directions for this year's shared task. For decoding, we employed two quality-aware strategies: Minimum Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA, along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [79] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)

*Zhe Chen, Yusheng Liao, Shuyang Jiang, Zhiyuan Zhu, Haolin Li, Yanfeng Wang, Yu Wang*

**Main category:** cs.CL

**Keywords:** medical large vision-language models, heterogeneous knowledge, retrieval-augmented generation, clinical applications, multimodal integration

**Relevance Score:** 9

**TL;DR:** The paper presents HeteroRAG, a framework that improves medical large vision-language models by integrating diverse knowledge sources for enhanced retrieval and factual accuracy in clinical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address factual inaccuracies and unreliable outputs in medical large vision-language models (Med-LVLMs), which pose risks in real-world diagnostics.

**Method:** The authors constructed MedAtlas, a comprehensive repository of multimodal reports and text corpora, and introduced HeteroRAG, which utilizes Modality-specific CLIPs and a Multi-corpora Query Generator to improve retrieval processes and knowledge alignment.

**Key Contributions:**

	1. Development of MedAtlas, a multimodal report repository and diverse text corpus.
	2. Introduction of HeteroRAG framework for enhanced retrieval and knowledge alignment.
	3. Implementation of Modality-specific CLIPs and Multi-corpora Query Generator for effective query construction.

**Result:** HeteroRAG significantly enhances the performance of Med-LVLMs, achieving state-of-the-art results across 12 datasets and 3 modalities, improving factual accuracy and reliability.

**Limitations:** 

**Conclusion:** The proposed approach effectively aids medical decision-making by bridging gaps in knowledge retrieval and aligning multimodal information, resulting in improved clinical outcomes.

**Abstract:** Medical large vision-language Models (Med-LVLMs) have shown promise in clinical applications but suffer from factual inaccuracies and unreliable outputs, posing risks in real-world diagnostics. While retrieval-augmented generation has emerged as a potential solution, current medical multimodal RAG systems are unable to perform effective retrieval across heterogeneous sources. The irrelevance of retrieved reports affects the factuality of analysis, while insufficient knowledge affects the credibility of clinical decision-making. To bridge the gap, we construct MedAtlas, which includes extensive multimodal report repositories and diverse text corpora. Based on it, we present HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous knowledge sources. The framework introduces Modality-specific CLIPs for effective report retrieval and a Multi-corpora Query Generator for dynamically constructing queries for diverse corpora. Incorporating knowledge from such multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge Preference Tuning to achieve cross-modality and multi-source knowledge alignment. Extensive experiments across 12 datasets and 3 modalities demonstrate that the proposed HeteroRAG achieves state-of-the-art performance in most medical vision language benchmarks, significantly improving factual accuracy and reliability of Med-LVLMs.

</details>


### [80] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)

*Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Retrieval-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper presents Atomic Thought and Atom-Searcher, a new RL framework for LLMs that enhances reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations of LLMs in complex reasoning tasks and the inefficacies of current RL approaches in fine-grained reasoning.

**Method:** The authors propose a decomposition of reasoning tasks into Atomic Thoughts, supervised by RRMs. Atom-Searcher integrates this framework with a curriculum-inspired reward schedule.

**Key Contributions:**

	1. Introduction of Atomic Thought paradigm for LLMs
	2. Development of Atom-Searcher, a RL framework for fine-grained reasoning
	3. Empirical validation showing improved reasoning capabilities over existing models

**Result:** Experiments demonstrate that Atom-Searcher achieves consistent improvements across multiple benchmarks in terms of reasoning capability and computational efficiency.

**Limitations:** 

**Conclusion:** The proposed methods enhance the interpretability and effectiveness of LLMs in reasoning tasks by providing structured guidance and improving training efficiency.

**Abstract:** Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [81] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)

*Ahmed Elshabrawy, Hour Kaing, Haiyue Song, Alham Fikri Aji, Hideki Tanaka, Masao Utiyama, Raj Dabre*

**Main category:** cs.CL

**Keywords:** language models, generative modeling, dialectal MT, variational probing, Arabic dialects

**Relevance Score:** 8

**TL;DR:** This paper challenges the assumption that aligning low-resource language models with high-resource varieties improves their performance. It introduces an online variational probing framework to decouple dialects from dominant varieties, improving generative modeling in low-resource dialects of Arabic.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the misconception that alignment with high-resource languages always benefits low-resource varieties, exemplified through Arabic dialects and Modern Standard Arabic.

**Method:** An online variational probing framework is used to estimate and decouple the internal representations of large language models from dominant varieties during fine-tuning.

**Key Contributions:**

	1. Introduction of an online variational probing framework to decouple dialectal representations.
	2. Causal study demonstrating the negative impact of representational entanglement with high-resource languages.
	3. Tools for improving generative modeling in multilingual contexts.

**Result:** Interventions across 25 Arabic dialects led to an average generation quality improvement of +2.0 and up to +4.9 chrF++, while also indicating tradeoffs in performance on standard languages.

**Limitations:** Tradeoffs exist in performance on standard languages due to the focus on low-resource dialects.

**Conclusion:** The paper provides causal evidence that high-resource language dominance can hinder generative capacities in low-resource varieties, offering a unified approach to improve generative modeling methodologies.

**Abstract:** Alignment with high-resource standard languages is often assumed to aid the modeling of related low-resource varieties. We challenge this assumption by demonstrating that excessive representational entanglement with a dominant variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects, can actively hinder generative modeling. We present the first comprehensive causal study of this phenomenon by analyzing and directly intervening in the internal representation geometry of large language models (LLMs). Our key contribution is an online variational probing framework that continuously estimates the subspace of the standard variety during fine-tuning, enabling projection-based decoupling from this space. While our study uses Arabic as a case due to its unusually rich parallel resources across 25 dialects, the broader motivation is methodological: dialectal MT serves as a controlled proxy for generative tasks where comparable multi-variety corpora are unavailable. Across 25 dialects, our intervention improves generation quality by up to +4.9 chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured tradeoff in standard-language performance. These results provide causal evidence that subspace dominance by high-resource varieties can restrict generative capacity for related varieties. More generally, we unify geometric and information-theoretic probing with subspace-level causal interventions, offering practical tools for improving generative modeling in closely related language families and, more broadly, for controlling representational allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [82] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)

*Jeongwoo Kang, Maria Boritchev, Maximin Coavoux*

**Main category:** cs.CL

**Keywords:** French, semantic corpus, Abstract Meaning Representation, dialogue annotation, natural language processing

**Relevance Score:** 4

**TL;DR:** This paper presents a French semantic corpus annotated in Abstract Meaning Representation (AMR) for spontaneous dialogues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance AMR's representation of spontaneous French dialogue, addressing its current limitations.

**Method:** The authors annotate the DinG corpus of spontaneous French dialogues and provide extended guidelines for consistent annotation.

**Key Contributions:**

	1. Development of a French semantic corpus using AMR
	2. Extension of AMR framework for spontaneous speech
	3. Provision of annotation guidelines for consistent corpus creation

**Result:** The released corpus supports the training and evaluation of an AMR parser, which aids in initial annotation processes.

**Limitations:** 

**Conclusion:** The developed corpus contributes to the creation of semantic resources for French dialogue analysis.

**Abstract:** We present our work to build a French semantic corpus by annotating French dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate the DinG corpus, consisting of transcripts of spontaneous French dialogues recorded during the board game Catan. As AMR has insufficient coverage of the dynamics of spontaneous speech, we extend the framework to better represent spontaneous speech and sentence structures specific to French. Additionally, to support consistent annotation, we provide an annotation guideline detailing these extensions. We publish our corpus under a free license (CC-SA-BY). We also train and evaluate an AMR parser on our data. This model can be used as an assistance annotation tool to provide initial annotations that can be refined by human annotators. Our work contributes to the development of semantic resources for French dialogue.

</details>


### [83] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)

*Raneem Alharthi, Rajwa Alharthi, Aiqi Jiang, Arkaitz Zubiaga*

**Main category:** cs.CL

**Keywords:** abusive language detection, contextual features, social media, machine learning, conversational exchanges

**Relevance Score:** 7

**TL;DR:** This study examines the impact of contextual features from parent tweets on the detection of abusive language in replies, revealing significant performance improvements when context is considered.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of existing abusive language detection models, which largely ignore contextual information from prior posts, aiming to enhance detection accuracy in social media conversations.

**Method:** The research analyzes conversational exchanges by comparing models that utilize features from both parent tweets and reply tweets versus those that rely solely on reply tweet features. Four different classification models are tested on a dataset of labeled parent-reply tweet pairs.

**Key Contributions:**

	1. Introduces a context-aware approach to abusive language detection in social media.
	2. Demonstrates significant performance improvements by incorporating parent tweet context.
	3. Identifies the superiority of content-based features over account-based features in classification tasks.

**Result:** The findings indicate that incorporating contextual features leads to substantial improvements in abusive language detection performance, particularly through the use of a diverse range of content-based features.

**Limitations:** The study is limited to specific datasets and social media platforms, and results may not generalize universally across all types of conversations or languages.

**Conclusion:** The study underscores the importance of leveraging context in abusive language detection, suggesting that combined content-based features yield better performance than selective feature use.

**Abstract:** Abusive language detection has become an increasingly important task as a means to tackle this type of harmful content in social media. There has been a substantial body of research developing models for determining if a social media post is abusive or not; however, this research has primarily focused on exploiting social media posts individually, overlooking additional context that can be derived from surrounding posts. In this study, we look at conversational exchanges, where a user replies to an earlier post by another user (the parent tweet). We ask: does leveraging context from the parent tweet help determine if a reply post is abusive or not, and what are the features that contribute the most? We study a range of content-based and account-based features derived from the context, and compare this to the more widely studied approach of only looking at the features from the reply tweet. For a more generalizable study, we test four different classification models on a dataset made of conversational exchanges (parent-reply tweet pairs) with replies labeled as abusive or not. Our experiments show that incorporating contextual features leads to substantial improvements compared to the use of features derived from the reply tweet only, confirming the importance of leveraging context. We observe that, among the features under study, it is especially the content-based features (what is being posted) that contribute to the classification performance rather than account-based features (who is posting it). While using content-based features, it is best to combine a range of different features to ensure improved performance over being more selective and using fewer features. Our study provides insights into the development of contextualized abusive language detection models in realistic settings involving conversations.

</details>


### [84] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)

*Jan Maliszewski*

**Main category:** cs.CL

**Keywords:** stylometry, text analysis, medieval literature, authorship attribution, OCR

**Relevance Score:** 2

**TL;DR:** This paper applies stylometric techniques to analyze Stephen Langton's Quaestiones Theologiae, revealing editorial layers and validating hypotheses about its formation through the use of HTR and stylometric analysis.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover layers of editorial work in the literary production based on oral teaching records and validate hypotheses about the formation of Stephen Langton's Quaestiones Theologiae.

**Method:** The study applies stylometric techniques, including frequency analysis of words, POS tagging, and pseudo-affix examination, alongside an HTR pipeline to analyze stylometric data from the text.

**Key Contributions:**

	1. Application of stylometric techniques to historical texts.
	2. Validation of hypotheses regarding the formation of medieval literary collections.
	3. Methodological comparison between manual and automatic data extraction.

**Result:** The study aims to directly compare performance on manually composed and automatically extracted data, testing the efficacy of transformer-based OCR in the context of scholastic Latin corpora.

**Limitations:** 

**Conclusion:** If successful, the study will provide a reusable template for analyzing collaborative literary production from medieval universities, enhancing computational research methodologies.

**Abstract:** While the indirect evidence suggests that already in the early scholastic period the literary production based on records of oral teaching (so-called reportationes) was not uncommon, there are very few sources commenting on the practice. This paper details the design of a study applying stylometric techniques of authorship attribution to a collection developed from reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover layers of editorial work and thus validate some hypotheses regarding the collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I discuss the implementation of an HTR pipeline and stylometric analysis based on the most frequent words, POS tags, and pseudo-affixes. The proposed study will offer two methodological gains relevant to computational research on the scholastic tradition: it will directly compare performance on manually composed and automatically extracted data, and it will test the validity of transformer-based OCR and automated transcription alignment for workflows applied to scholastic Latin corpora. If successful, this study will provide an easily reusable template for the exploratory analysis of collaborative literary production stemming from medieval universities.

</details>


### [85] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)

*Jumbly Grindrod, Peter Grindrod*

**Main category:** cs.CL

**Keywords:** transformer models, semantic representation, RoBERTa, token embedding, psycholinguistics

**Relevance Score:** 8

**TL;DR:** This paper studies the representation of word meanings in transformer language models, particularly focusing on RoBERTa-base, and finds that semantic information is well encoded in its token embedding space.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how transformer models represent word meanings and whether they maintain a lexical store of semantic information.

**Method:** Extracted the token embedding space of RoBERTa-base and clustered it into 200 groups using k-means clustering; manually inspected clusters for semantic sensitivity and tested against psycholinguistic measures.

**Key Contributions:**

	1. Demonstrated a successful methodology for analyzing semantic representation in LLMs
	2. Provided evidence against meaning eliminativist theories in the context of transformer models
	3. Identified the linkage between embedding clusters and psycholinguistic measures.

**Result:** Clusters contained a wide variety of semantic information, indicating transformer models encode meaningful similarities between words.

**Limitations:** 

**Conclusion:** The findings challenge certain hypotheses suggesting that transformer models do not effectively represent semantic information.

**Abstract:** We investigate how word meanings are represented in the transformer language models. Specifically, we focus on whether transformer models employ something analogous to a lexical store - where each word has an entry that contains semantic information. To do this, we extracted the token embedding space of RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we then manually inspected the resultant clusters to consider whether they are sensitive to semantic information. In our second study, we tested whether the clusters are sensitive to five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition. Overall, our findings were very positive - there is a wide variety of semantic information encoded within the token embedding space. This serves to rule out certain "meaning eliminativist" hypotheses about how transformer LLMs process semantic information.

</details>


### [86] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)

*Yilin Geng, Shujing Wang, Chuan Wang, Keqing He, Yanfei Lv, Ying Wang, Zaiwen Feng, Xiaoying Bai*

**Main category:** cs.CL

**Keywords:** semantic table annotation, column type annotation, cell entity annotation, large language models, prompt engineering

**Relevance Score:** 7

**TL;DR:** This paper presents an LLM-based agent for the Semantic Table Annotation (STA) task, addressing challenges in Column Type Annotation (CTA) and Cell Entity Annotation (CEA) with tailored prompts and external tools, significantly improving annotation accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The STA task is crucial for semantic applications but faces challenges like semantic loss and strict ontological requirements that lower annotation accuracy.

**Method:** An LLM-based agent is designed with five external tools that use tailored prompts based on the ReAct framework, allowing dynamic selection of annotation strategies based on table characteristics.

**Key Contributions:**

	1. Introduction of an LLM-based agent for STA tasks with dynamic strategy selection.
	2. Implementation of tailored prompt engineering based on the ReAct framework.
	3. Empirical results demonstrating significant improvements over current methods in efficiency and accuracy.

**Result:** The proposed method outperforms existing annotation approaches across multiple metrics and achieves a 70% reduction in time costs and a 60% reduction in LLM token usage.

**Limitations:** Does not address the generalization of the approach to all possible table formats or external data sources.

**Conclusion:** The study presents an efficient and cost-effective solution for the STA task, enhancing annotation accuracy while reducing resource usage.

**Abstract:** The Semantic Table Annotation (STA) task, which includes Column Type Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to ontology entities and plays important roles in various semantic applications. However, complex tables often pose challenges such as semantic loss of column names or cell values, strict ontological hierarchy requirements, homonyms, spelling errors, and abbreviations, which hinder annotation accuracy. To address these issues, this paper proposes an LLM-based agent approach for CTA and CEA. We design and implement five external tools with tailored prompts based on the ReAct framework, enabling the STA agent to dynamically select suitable annotation strategies depending on table characteristics. Experiments are conducted on the Tough Tables and BiodivTab datasets from the SemTab challenge, which contain the aforementioned challenges. Our method outperforms existing approaches across various metrics. Furthermore, by leveraging Levenshtein distance to reduce redundant annotations, we achieve a 70% reduction in time costs and a 60% reduction in LLM token usage, providing an efficient and cost-effective solution for STA.

</details>


### [87] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)

*Jinyi Han, Xinyi Wang, Haiquan Zhao, Tingyun li, Zishang Jiang, Sihang Jiang, Jiaqing Liang, Xin Lin, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** self-refinement, large language models, contextual generation, dynamic refinement, machine learning

**Relevance Score:** 9

**TL;DR:** ProActive Self-Refinement (PASR) improves LLM output through iterative refinement during generation based on internal state and context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing self-refinement methods that use a reactive process with a fixed number of iterations, making refinement timing and content difficult to optimize.

**Method:** ProActive Self-Refinement (PASR) allows LLMs to refine outputs dynamically during the generation process, deciding when and how to refine based on context.

**Key Contributions:**

	1. Introduction of ProActive Self-Refinement (PASR) methodology
	2. Demonstrated significant reductions in token consumption and improvements in accuracy
	3. Open-source code and baseline availability for further research

**Result:** PASR outperforms standard generation methods, reducing average token consumption by 41.6% and improving accuracy by 8.2% on Qwen3-8B across 10 tasks.

**Limitations:** 

**Conclusion:** PASR effectively enhances problem-solving performance in LLMs by allowing dynamic output refinement during generation.

**Abstract:** Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.

</details>


### [88] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)

*Tianyue Ou, Saujas Vaduguru, Daniel Fried*

**Main category:** cs.CL

**Keywords:** multi-agent systems, large language models, travel planning, information sharing, orchestration

**Relevance Score:** 7

**TL;DR:** The study presents an LLM-based multi-agent system for travel planning, improving task performance through structured information sharing and orchestration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of long-horizon, multi-constraint planning tasks that require detailed information and coordination among agents.

**Method:** An LLM-based multi-agent system was constructed and evaluated using a notebook for information sharing and an orchestrator for improving coordination among agents during free form conversation.

**Key Contributions:**

	1. Introduction of a notebook for information sharing among LLM agents
	2. Development of an orchestrator agent for improved coordination
	3. Demonstration of a significant performance increase in long-horizon planning tasks

**Result:** The implementation of the notebook reduced errors from hallucinated details by 18%, and the orchestrator helped direct the MAS to further reduce errors by 13.5%, achieving a final pass rate of 25% on the TravelPlanner benchmark, significantly outperforming the single-agent baseline.

**Limitations:** 

**Conclusion:** Structured information sharing and reflective orchestration are crucial for enhancing the performance of multi-agent systems in complex planning tasks with LLMs.

**Abstract:** Multi-agent systems (MASs) have pushed the boundaries of large language model (LLM) agents in domains such as web research and software engineering. However, long-horizon, multi-constraint planning tasks involve conditioning on detailed information and satisfying complex interdependent constraints, which can pose a challenge for these systems. In this study, we construct an LLM-based MAS for a travel planning task which is representative of these challenges. We evaluate the impact of a notebook to facilitate information sharing, and evaluate an orchestrator agent to improve coordination in free form conversation between agents. We find that the notebook reduces errors due to hallucinated details by 18%, while an orchestrator directs the MAS to focus on and further reduce errors by up to 13.5% within focused sub-areas. Combining both mechanisms achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute improvement over the single-agent baseline's 7.5% pass rate. These results highlight the potential of structured information sharing and reflective orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [89] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)

*Ralph Peeters, Aaron Steiner, Luca Schwarz, Julian Yuya Caspary, Christian Bizer*

**Main category:** cs.CL

**Keywords:** LLM, web agents, e-commerce, benchmark, comparison-shopping

**Relevance Score:** 8

**TL;DR:** WebMall is a benchmark for evaluating LLM-based web agents on multi-shop comparison-shopping tasks, featuring 91 tasks across four simulated online shops.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate long-running web tasks in e-commerce, such as comparison shopping, using LLM-based web agents.

**Method:** Introduction of WebMall, which includes four simulated online shops and 91 cross-shop tasks for evaluating web agents in comparison-shopping.

**Key Contributions:**

	1. Introduction of a new benchmark for comparison-shopping tasks (WebMall)
	2. Evaluation of multiple baseline agents and their performance
	3. Contribution of authentic product offers from real-world shops

**Result:** Baseline agents demonstrated completion rates of 75% and 53%, and F1 scores of 87% and 63% for basic and advanced tasks, respectively.

**Limitations:** 

**Conclusion:** WebMall enables research on web agents, enhancing navigation, reasoning, and efficiency in e-commerce, and is publicly accessible for further study.

**Abstract:** LLM-based web agents have the potential to automate long-running web tasks, such as finding offers for specific products in multiple online shops and subsequently ordering the cheapest products that meet the users needs. This paper introduces WebMall, a multi-shop online shopping benchmark for evaluating the effectiveness and efficiency of web agents for comparison-shopping. WebMall consists of four simulated online shops populated with authentic product offers sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These tasks include basic tasks such as finding specific products in multiple shops, performing price comparisons, adding items to the shopping cart, and completing checkout. Advanced tasks involve searching for products based on vague requirements, identifying suitable substitutes, and finding compatible products. Compared to existing e-commerce benchmarks, such as WebShop or ShoppingBench, WebMall introduces comparison-shopping tasks across multiple shops. Furthermore, the product offers are more heterogeneous, as they originate from hundreds of distinct real-world shops. The tasks in WebMall require longer interaction trajectories than those in WebShop, while remaining representative of real-world shopping behaviors. We evaluate eight baseline agents on WebMall, varying in observation modality, memory utilization, and underlying large language model (GPT 4.1 and Claude Sonnet 4). The best-performing configurations achieve completion rates of 75% and 53%, and F1 scores of 87% and 63%, on the basic and advanced task sets, respectively. WebMall is publicly released to facilitate research on web agents and to promote advancements in navigation, reasoning, and efficiency within e-commerce scenarios.

</details>


### [90] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)

*Zhu Li, Yuqing Zhang, Xiyuan Gao, Devraj Raghuvanshi, Nagendra Kumar, Shekhar Nayak, Matt Coler*

**Main category:** cs.CL

**Keywords:** sarcastic speech synthesis, human-computer interaction, transfer learning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach for synthesizing sarcastic speech by integrating a bi-modal sarcasm detection model into the TTS training process, improving the model's sarcasm awareness through a two-stage fine-tuning process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing natural interactions in applications like entertainment and human-computer interaction through effective sarcastic speech synthesis.

**Method:** The approach incorporates feedback loss from a bi-modal sarcasm detection model into the TTS training process and uses transfer learning with two stages of fine-tuning a pre-trained speech synthesis model.

**Key Contributions:**

	1. Integration of feedback loss from sarcasm detection into TTS training.
	2. Two-stage fine-tuning process for sarcasm synthesis.
	3. Improvement of quality and naturalness in synthesized sarcastic speech.

**Result:** The proposed methods improve the quality, naturalness, and sarcasm-awareness of synthesized speech based on objective and subjective evaluations.

**Limitations:** 

**Conclusion:** By refining a speech synthesis model in a two-stage process, the study successfully enhances its ability to generate sarcasm-aware speech, addressing challenges related to sarcastic speech synthesis.

**Abstract:** Sarcastic speech synthesis, which involves generating speech that effectively conveys sarcasm, is essential for enhancing natural interactions in applications such as entertainment and human-computer interaction. However, synthesizing sarcastic speech remains a challenge due to the nuanced prosody that characterizes sarcasm, as well as the limited availability of annotated sarcastic speech data. To address these challenges, this study introduces a novel approach that integrates feedback loss from a bi-modal sarcasm detection model into the TTS training process, enhancing the model's ability to capture and convey sarcasm. In addition, by leveraging transfer learning, a speech synthesis model pre-trained on read speech undergoes a two-stage fine-tuning process. First, it is fine-tuned on a diverse dataset encompassing various speech styles, including sarcastic speech. In the second stage, the model is further refined using a dataset focused specifically on sarcastic speech, enhancing its ability to generate sarcasm-aware speech. Objective and subjective evaluations demonstrate that our proposed methods improve the quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [91] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)

*Xinhe Li, Jiajun Liu, Peng Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Small Language Models, mathematical reasoning, multi-LoRA Interaction, knowledge generation

**Relevance Score:** 9

**TL;DR:** The paper introduces LoRID, a novel method enhancing mathematical reasoning in Small Language Models (SLMs) by utilizing a two-mode reasoning approach inspired by human thinking processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs excel in mathematical reasoning but are resource-heavy. The paper aims to improve SLMs' reasoning capabilities by mimicking human thought processes.

**Method:** LoRID employs multi-LoRA Interaction: it generates datasets with LLMs, trains an Intuitive Reasoner for generating Chain-of-Thoughts, and develops a Knowledge Generator and Deep Reasoner for knowledge use and reasoning, incorporating mutual feedback to enhance consistency.

**Key Contributions:**

	1. Introduced LoRID for enhancing SLMs' reasoning abilities.
	2. Utilized a two-mode reasoning approach analogous to human thinking.
	3. Achieved state-of-the-art results on the GSM8K dataset.

**Result:** LoRID achieves state-of-the-art performance on the GSM8K dataset, outperforming the nearest competitor by notable accuracy improvements across various models.

**Limitations:** 

**Conclusion:** The method effectively enhances the reasoning abilities of SLMs through its structured approach to mimicking human thinking.

**Abstract:** Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [92] [BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans DeÄerlendirmesi, Zorluklar ve Ä°yileÅtirme FÄ±rsatlarÄ±](https://arxiv.org/abs/2508.13044)

*M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼Å, Banu Diri, SavaÅ YÄ±ldÄ±rÄ±m, Ãner AytaÅ*

**Main category:** cs.CL

**Keywords:** Turkish NLP, Language models, TR-MMLU

**Relevance Score:** 4

**TL;DR:** Introduction of the Turkish MMLU benchmark for evaluating LLMs in Turkish.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of evaluating language models in resource-limited languages, specifically Turkish.

**Method:** Development of a benchmark called TR-MMLU featuring 6,200 multiple-choice questions across 62 sections of the Turkish education system.

**Key Contributions:**

	1. Introduction of TR-MMLU, a new evaluation benchmark for Turkish
	2. Curated dataset of 6,200 questions for assessing LLMs
	3. Highlights the current capabilities and limitations of LLMs in Turkish

**Result:** State-of-the-art LLMs were evaluated on TR-MMLU, revealing areas that need improvement in model design.

**Limitations:** 

**Conclusion:** TR-MMLU establishes a new standard in Turkish NLP research, paving the way for future innovations.

**Abstract:** Language models have made significant advancements in understanding and generating human language, achieving remarkable success in various applications. However, evaluating these models remains a challenge, particularly for resource-limited languages like Turkish. To address this issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system. This benchmark provides a standard framework for Turkish NLP research, enabling detailed analyses of LLMs' capabilities in processing Turkish text. In this study, we evaluated state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model design. TR-MMLU sets a new standard for advancing Turkish NLP research and inspiring future innovations.

</details>


### [93] [DoÄal Dil Ä°Ålemede Tokenizasyon StandartlarÄ± ve ÃlÃ§Ã¼mÃ¼: TÃ¼rkÃ§e Ãzerinden BÃ¼yÃ¼k Dil Modellerinin KarÅÄ±laÅtÄ±rmalÄ± Analizi](https://arxiv.org/abs/2508.13058)

*M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼Å, Sercan KarakaÅ, Banu Diri, SavaÅ YÄ±ldÄ±rÄ±m*

**Main category:** cs.CL

**Keywords:** Tokenization, Natural Language Processing, Morphologically-rich languages, Turkish, Language-specific metrics

**Relevance Score:** 9

**TL;DR:** The paper presents a novel evaluation framework for assessing tokenization effectiveness in morphologically-rich languages, focused on Turkish, revealing that language-specific tokenization significantly affects NLP model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of tokenization in morphologically-rich and low-resource languages, which are crucial for the performance of language models.

**Method:** An evaluation framework was developed using the Turkish MMLU dataset, analyzing tokenizers based on vocabulary size, token count, processing time, language-specific token percentages, and token purity.

**Key Contributions:**

	1. Proposed new evaluation metrics for tokenization effectiveness in Turkish and similar languages.
	2. Demonstrated the significant impact of language-specific tokenization on NLP model performance.
	3. Established standards for tokenization in morphologically-rich languages.

**Result:** The study found that language-specific token percentages correlate more strongly with downstream performance than token purity, and increasing model parameters does not necessarily improve linguistic performance.

**Limitations:** 

**Conclusion:** Tailored, language-specific tokenization methods are essential for enhancing the performance of language models on morphologically complex languages.

**Abstract:** Tokenization is a fundamental preprocessing step in Natural Language Processing (NLP), significantly impacting the capability of large language models (LLMs) to capture linguistic and semantic nuances. This study introduces a novel evaluation framework addressing tokenization challenges specific to morphologically-rich and low-resource languages such as Turkish. Utilizing the Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from the Turkish education system, we assessed tokenizers based on vocabulary size, token count, processing time, language-specific token percentages (\%TR), and token purity (\%Pure). These newly proposed metrics measure how effectively tokenizers preserve linguistic structures. Our analysis reveals that language-specific token percentages exhibit a stronger correlation with downstream performance (e.g., MMLU scores) than token purity. Furthermore, increasing model parameters alone does not necessarily enhance linguistic performance, underscoring the importance of tailored, language-specific tokenization methods. The proposed framework establishes robust and practical tokenization standards for morphologically complex languages.

</details>


### [94] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)

*John Alderete, Macarious Kin Fung Hui, Aanchan Mohan*

**Main category:** cs.CL

**Keywords:** Speech Recognition, Speech Errors, Linguistic Annotation, ASR Evaluation, WhisperX

**Relevance Score:** 4

**TL;DR:** The SFUSED database is a resource for testing speech recognition models, comprising annotated speech errors that help evaluate ASR systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a public database for linguistic and psycholinguistic research that also serves as a tool for assessing speech recognition models.

**Method:** The study involves evaluating WhisperX's transcription accuracy using 5,300 documented speech errors from the SFUSED database.

**Key Contributions:**

	1. Introduction of the SFUSED database as a research tool
	2. Systematic annotation scheme for speech errors
	3. Demonstration of the database's utility in assessing ASR performance

**Result:** The evaluation demonstrates the effectiveness of the SFUSED database as a diagnostic tool for assessing ASR system performance.

**Limitations:** 

**Conclusion:** The SFUSED database's annotations are beneficial for evaluating speech recognition models and understanding speech errors.

**Abstract:** The Simon Fraser University Speech Error Database (SFUSED) is a public data collection developed for linguistic and psycholinguistic research. Here we demonstrate how its design and annotations can be used to test and evaluate speech recognition models. The database comprises systematically annotated speech errors from spontaneous English speech, with each error tagged for intended and actual error productions. The annotation schema incorporates multiple classificatory dimensions that are of some value to model assessment, including linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and both word-level and syllable-level error positioning. To assess the value of these classificatory variables, we evaluated the transcription accuracy of WhisperX across 5,300 documented word and phonological errors. This analysis demonstrates the atabase's effectiveness as a diagnostic tool for ASR system performance.

</details>


### [95] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)

*Long Ma, Fangwei Zhong, Yizhou Wang*

**Main category:** cs.CL

**Keywords:** Causal Language Models, Reinforcement Learning, Token Generation

**Relevance Score:** 8

**TL;DR:** This paper introduces Reinforced Context Order Recovery (ReCOR), a framework using reinforcement learning to enhance token generation order in language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current causal and diffusion models struggle with token generation that requires adaptive ordering, which can hinder performance on complex tasks.

**Method:** ReCOR employs reinforcement learning to extract optimal token generation orders from text data, utilizing self-supervised token prediction statistics.

**Key Contributions:**

	1. Introduction of a novel framework (ReCOR) for adaptive token generation order
	2. Utilization of reinforcement learning without requiring annotated data
	3. Demonstrated superior performance on challenging datasets compared to baseline models

**Result:** Experiments show ReCOR outperforms baseline models on reasoning and planning tasks, even exceeding models trained with true token orders.

**Limitations:** 

**Conclusion:** ReCOR presents a significant advancement in token generation methodologies for language models, making them more effective in complex problem-solving scenarios.

**Abstract:** Modern causal language models, followed by rapid developments in discrete diffusion models, can now produce a wide variety of interesting and useful content. However, these families of models are predominantly trained to output tokens with a fixed (left-to-right) or random order, which may deviate from the logical order in which tokens are generated originally. In this paper, we observe that current causal and diffusion models encounter difficulties in problems that require adaptive token generation orders to solve tractably, which we characterize with the $\mathcal{V}$-information framework. Motivated by this, we propose Reinforced Context Order Recovery (ReCOR), a reinforcement-learning-based framework to extract adaptive, data-dependent token generation orders from text data without annotations. Self-supervised by token prediction statistics, ReCOR estimates the hardness of predicting every unfilled token and adaptively selects the next token during both training and inference. Experiments on challenging reasoning and planning datasets demonstrate the superior performance of ReCOR compared with baselines, sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [96] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)

*DayyÃ¡n O'Brien, Bhavitvya Malik, Ona de Gibert, Pinzhen Chen, Barry Haddow, JÃ¶rg Tiedemann*

**Main category:** cs.CL

**Keywords:** document-level translation, multilingual dataset, long-context modeling

**Relevance Score:** 8

**TL;DR:** DocHPLT is the largest publicly available dataset for document-level machine translation, containing 124 million aligned document pairs across 50 languages, aimed at enhancing multilingual translation and long-context modeling.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of document-level translation resources for many languages, particularly under-resourced ones, and to improve long-context modeling capabilities.

**Method:** Modified a web extraction pipeline to create a dataset that preserves complete document integrity with 124 million document pairs and conducted experiments to identify optimal training strategies for document-level translation.

**Key Contributions:**

	1. Creation of DocHPLT, a large-scale document-level translation dataset
	2. Demonstrated performance improvements of LLMs for under-resourced languages
	3. Open-sourcing the dataset for community use

**Result:** LLMs fine-tuned on the DocHPLT dataset significantly outperform baseline models, especially for under-resourced languages.

**Limitations:** 

**Conclusion:** This dataset offers valuable resources for advancing multilingual document-level translation and is open-sourced under a permissive license.

**Abstract:** Existing document-level machine translation resources are only available for a handful of languages, mostly high-resourced ones. To facilitate the training and evaluation of document-level translation and, more broadly, long-context modeling for global communities, we create DocHPLT, the largest publicly available document-level translation dataset to date. It contains 124 million aligned document pairs across 50 languages paired with English, comprising 4.26 billion sentences, with further possibility to provide 2500 bonus pairs not involving English. Unlike previous reconstruction-based approaches that piece together documents from sentence-level data, we modify an existing web extraction pipeline to preserve complete document integrity from the source, retaining all content including unaligned portions. After our preliminary experiments identify the optimal training context strategy for document-level translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially outperform off-the-shelf instruction-tuned baselines, with particularly dramatic improvements for under-resourced languages. We open-source the dataset under a permissive license, providing essential infrastructure for advancing multilingual document-level translation.

</details>


### [97] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)

*Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, legal domain, open-source, query translation, RAG systems

**Relevance Score:** 8

**TL;DR:** This paper presents an enhanced Retrieval-Augmented Generation (RAG) pipeline for legal research that features a context-aware query translator, improved retrieval strategies, and a comprehensive evaluation framework for better performance and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of large language model outputs in the legal domain by mitigating hallucinations through grounding in cited sources.

**Method:** An end-to-end RAG pipeline with a context-aware query translator, open-source retrieval strategies utilizing SBERT and GTE embeddings, and a comprehensive evaluation framework combining RAGAS, BERTScore-F1, and ROUGE-Recall.

**Key Contributions:**

	1. Context-aware query translator
	2. Cost-efficient open-source retrieval strategies
	3. Comprehensive evaluation framework

**Result:** Substantial improvements in retrieval performance metrics (Recall@K by 30-95% and Precision@K by ~2.5x for K>4) were achieved, alongside more faithful and contextually relevant answers compared to baseline methods.

**Limitations:** 

**Conclusion:** Carefully designed open-source pipelines can outperform proprietary ones in retrieval quality, highlighting the value of task-aware tuning for legal research assistance.

**Abstract:** Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding large language model outputs in cited sources, a capability that is especially critical in the legal domain. We present an end-to-end RAG pipeline that revisits and extends the LegalBenchRAG baseline with three targeted enhancements: (i) a context-aware query translator that disentangles document references from natural-language questions and adapts retrieval depth and response style based on expertise and specificity, (ii) open-source retrieval strategies using SBERT and GTE embeddings that achieve substantial performance gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for $K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic alignment and faithfulness across models and prompt designs. Our results show that carefully designed open-source pipelines can rival or outperform proprietary approaches in retrieval quality, while a custom legal-grounded prompt consistently produces more faithful and contextually relevant answers than baseline prompting. Taken together, these contributions demonstrate the potential of task-aware, component-level tuning to deliver legally grounded, reproducible, and cost-effective RAG systems for legal research assistance.

</details>


### [98] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)

*Zefang Liu, Arman Anwar*

**Main category:** cs.CL

**Keywords:** incident response, large language models, retrieval-augmented generation, cybersecurity, multi-agent systems

**Relevance Score:** 6

**TL;DR:** AutoBnB-RAG enhances incident response simulations using retrieval-augmented generation to improve decision-making in cyber threat scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in incident response due to their lack of external knowledge access.

**Method:** The AutoBnB-RAG framework incorporates retrieval-augmented generation into multi-agent simulations of incident response, testing across various team structures and configurations.

**Key Contributions:**

	1. Introduction of AutoBnB-RAG framework
	2. Utilization of retrieval settings with curated documentation and incident reports
	3. Evaluation across multiple team configurations for critical reasoning

**Result:** Retrieval augmentation significantly improves decision quality and success rates in simulating real-world cyber incidents and multi-stage attacks.

**Limitations:** 

**Conclusion:** Integrating retrieval mechanisms into LLM-based systems enhances their effectiveness in cybersecurity decision-making.

**Abstract:** Incident response (IR) requires fast, coordinated, and well-informed decision-making to contain and mitigate cyber threats. While large language models (LLMs) have shown promise as autonomous agents in simulated IR settings, their reasoning is often limited by a lack of access to external knowledge. In this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that incorporates retrieval-augmented generation (RAG) into multi-agent incident response simulations. Built on the Backdoors & Breaches (B&B) tabletop game environment, AutoBnB-RAG enables agents to issue retrieval queries and incorporate external evidence during collaborative investigations. We introduce two retrieval settings: one grounded in curated technical documentation (RAG-Wiki), and another using narrative-style incident reports (RAG-News). We evaluate performance across eight team structures, including newly introduced argumentative configurations designed to promote critical reasoning. To validate practical utility, we also simulate real-world cyber incidents based on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct complex multi-stage attacks. Our results show that retrieval augmentation improves decision quality and success rates across diverse organizational models. This work demonstrates the value of integrating retrieval mechanisms into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [99] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)

*Kawin Mayilvaghanan, Siddhant Gupta, Ayush Kumar*

**Main category:** cs.CL

**Keywords:** Abstractive Summarization, Operational Bias, Large Language Models, Bias Quantification, Contact Centers

**Relevance Score:** 8

**TL;DR:** This paper introduces BlindSpot, a framework to identify and quantify operational biases in LLM-generated summaries of contact center call transcripts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and quantify the previously unexplored operational biases in contact center operations related to LLM-generated summaries.

**Method:** BlindSpot uses a taxonomy of 15 operational bias dimensions and employs an LLM as a zero-shot classifier to quantify biases via two metrics: Fidelity Gap and Coverage.

**Key Contributions:**

	1. Introduction of the BlindSpot framework for operational bias analysis
	2. Development of metrics for bias quantification (Fidelity Gap and Coverage)
	3. Empirical findings demonstrating systemic biases across LLMs.

**Result:** An empirical study with 2500 call transcripts showed that systemic biases are present in summaries generated by various LLMs, regardless of model size or type.

**Limitations:** The scope is limited to operational biases and may not encompass all potential biases in LLM applications.

**Conclusion:** Operational biases in generated summaries highlight the need for better understanding and improvement of LLMs in contact centers.

**Abstract:** Abstractive summarization is a core application in contact centers, where Large Language Models (LLMs) generate millions of summaries of call transcripts daily. Despite their apparent quality, it remains unclear whether LLMs systematically under- or over-attend to specific aspects of the transcript, potentially introducing biases in the generated summary. While prior work has examined social and positional biases, the specific forms of bias pertinent to contact center operations - which we term Operational Bias - have remained unexplored. To address this gap, we introduce BlindSpot, a framework built upon a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic) for the identification and quantification of these biases. BlindSpot leverages an LLM as a zero-shot classifier to derive categorical distributions for each bias dimension in a pair of transcript and its summary. The bias is then quantified using two metrics: Fidelity Gap (the JS Divergence between distributions) and Coverage (the percentage of source labels omitted). Using BlindSpot, we conducted an empirical study with 2500 real call transcripts and their summaries generated by 20 LLMs of varying scales and families (e.g., GPT, Llama, Claude). Our analysis reveals that biases are systemic and present across all evaluated models, regardless of size or family.

</details>


### [100] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)

*Kareem Elozeiri, Mervat Abassy, Preslav Nakov, Yuxia Wang*

**Main category:** cs.CL

**Keywords:** Arabic, commonsense reasoning, Graph Convolutional Networks, natural language understanding, dialects

**Relevance Score:** 4

**TL;DR:** This paper introduces MuDRiC, the first multi-dialect Arabic commonsense reasoning dataset, and a method using Graph Convolutional Networks for improved commonsense validation in Arabic.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance commonsense validation in Arabic, addressing the underrepresentation of dialects in existing resources, which primarily focus on Modern Standard Arabic.

**Method:** The authors introduce a novel method that adapts Graph Convolutional Networks (GCNs) to improve the modeling of semantic relationships for commonsense reasoning in Arabic.

**Key Contributions:**

	1. Introduction of MuDRiC, the first multi-dialect Arabic commonsense reasoning dataset.
	2. Adaptation of Graph Convolutional Networks for commonsense reasoning in Arabic.

**Result:** Experimental results show that the proposed method significantly improves performance in commonsense validation tasks for Arabic compared to existing approaches.

**Limitations:** 

**Conclusion:** The work provides a foundational dataset and a novel method for commonsense reasoning in Arabic, contributing to enhanced natural language understanding in the language.

**Abstract:** Commonsense validation evaluates whether a sentence aligns with everyday human understanding, a critical capability for developing robust natural language understanding systems. While substantial progress has been made in English, the task remains underexplored in Arabic, particularly given its rich linguistic diversity. Existing Arabic resources have primarily focused on Modern Standard Arabic (MSA), leaving regional dialects underrepresented despite their prevalence in spoken contexts. To bridge this gap, we present two key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense dataset incorporating multiple dialects, and (ii) a novel method adapting Graph Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances semantic relationship modeling for improved commonsense validation. Our experimental results demonstrate that this approach achieves superior performance in Arabic commonsense validation. Our work enhances Arabic natural language understanding by providing both a foundational dataset and a novel method for handling its complex variations. To the best of our knowledge, we release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [101] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)

*Dara Bahri, John Wieting*

**Main category:** cs.CL

**Keywords:** watermarking, large language models, detection, hybrid schemes, entropy

**Relevance Score:** 8

**TL;DR:** The paper explores hybrid watermark detection schemes to improve the identification of large language model generations, overcoming entropy limitations in watermarking alone.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in detecting generations of large language models (LLMs) using watermarking techniques, especially under conditions of limited entropy.

**Method:** The authors investigate various hybrid schemes that integrate watermark detectors with non-watermark ones to assess performance improvements.

**Key Contributions:**

	1. Investigation of hybrid detection schemes combining watermark and non-watermark techniques.
	2. Empirical validation showing performance improvements under various conditions. 
	3. Highlighting the limitations of traditional watermarking related to entropy.
	4. Proposing a novel approach to improve LLM generation detection efficiency.

**Result:** The hybrid schemes demonstrated performance gains over individual classes of detectors across a wide range of experimental setups.

**Limitations:** The study's effectiveness may vary based on specific conditions and the nature of the input prompts used.

**Conclusion:** Combining watermarking and non-watermarking detection methods is a promising approach to enhancing the detection of LLM generations.

**Abstract:** Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.

</details>


### [102] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)

*Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, Swarnadeep Saha*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Cognitive Performance, Machine Learning, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** The paper presents OptimalThinkingBench, a benchmark to evaluate and improve the balance of overthinking and underthinking in large language models (LLMs).

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of LLMs varies significantly between thinking and non-thinking variants, leading to inefficiencies in their application for complex and simple tasks.

**Method:** The benchmark includes two components: OverthinkingBench for simple queries in various domains and UnderthinkingBench with challenging reasoning tasks. It uses thinking-adjusted accuracy metrics for evaluation of LLMs.

**Key Contributions:**

	1. Introduction of OptimalThinkingBench for joint evaluation of LLMs' thinking capacities.
	2. Discovery that current models fail to balance overthinking and underthinking effectively.
	3. Identification of the need for unified models that perform well across various tasks.

**Result:** Evaluation of 33 models shows that none can optimally balance thinking and efficiency, with thinking models often overthinking and non-thinking models underthinking, leading to performance issues.

**Limitations:** Limited to 33 models; identified issues in creating a universally optimal thinking model.

**Conclusion:** The study emphasizes the necessity for better-designed models that can achieve optimal thinking rather than just segregating models based on their thinking capabilities.

**Abstract:** Thinking LLMs solve complex tasks at the expense of increased compute and overthinking on simpler problems, while non-thinking LLMs are faster and cheaper but underthink on harder reasoning problems. This has led to the development of separate thinking and non-thinking LLM variants, leaving the onus of selecting the optimal model for each query on the end user. In this work, we introduce OptimalThinkingBench, a unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench, featuring simple queries in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we perform extensive evaluation of 33 different thinking and non-thinking models and show that no model is able to optimally think on our benchmark. Thinking models often overthink for hundreds of tokens on the simplest user queries without improving performance. In contrast, large non-thinking models underthink, often falling short of much smaller thinking models. We further explore several methods to encourage optimal thinking, but find that these approaches often improve on one sub-benchmark at the expense of the other, highlighting the need for better unified and optimal models in the future.

</details>


### [103] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)

*David Heineman, Valentin Hofmann, Ian Magnusson, Yuling Gu, Noah A. Smith, Hannaneh Hajishirzi, Kyle Lo, Jesse Dodge*

**Main category:** cs.CL

**Keywords:** large language models, evaluation benchmarks, signal-to-noise ratio

**Relevance Score:** 8

**TL;DR:** This paper examines the reliability of evaluation benchmarks for large language models and introduces metrics to improve their design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The cost of developing large language models necessitates reliable benchmarks to make informed decisions based on small experiments.

**Method:** The paper analyzes the signal-to-noise ratio of benchmarks, introduces new metrics for evaluating benchmarks, and tests interventions to improve signal and reduce noise across 30 benchmarks and 375 language models.

**Key Contributions:**

	1. Introduction of signal and noise metrics for evaluating benchmarks
	2. Recommendations for improving benchmark design
	3. Creation of a new publicly available dataset of 900K benchmark results

**Result:** Benchmarks with a better signal-to-noise ratio are shown to be more reliable, resulting in decreased prediction error in scaling laws and more accurate evaluations.

**Limitations:** 

**Conclusion:** Creating benchmarks with high signal and low noise is essential for better decision-making in model development.

**Abstract:** Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.

</details>


### [104] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)

*Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong*

**Main category:** cs.CL

**Keywords:** large language models, detection methods, neural representations

**Relevance Score:** 9

**TL;DR:** Proposes RepreGuard, a method for detecting LLM-generated texts using internal representations for improved robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of existing detection methods for large language model output, specifically in out-of-distribution scenarios.

**Method:** Utilizes a surrogate model to collect and analyze representations of LLM-generated texts and human-written texts, extracting distinct activation features for improved classification.

**Key Contributions:**

	1. Introduction of RepreGuard, a new detection method leveraging LLM internal representations.
	2. Demonstration of significant differences in neural activation patterns between LGTs and HWTs.
	3. Proven robust performance across various conditions with high AUROC scores.

**Result:** RepreGuard achieves an average AUROC of 94.92% across both in-distribution and out-of-distribution scenarios, demonstrating resilience to varying text sizes and attacks.

**Limitations:** 

**Conclusion:** RepreGuard presents a robust and effective approach for distinguishing between LLM-generated and human-written texts, outperforming existing methodologies.

**Abstract:** Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard

</details>


### [105] [Large language models can replicate cross-cultural differences in personality](https://arxiv.org/abs/2310.10679)

*PaweÅ Niszczota, Mateusz Janczak, MichaÅ Misiak*

**Main category:** cs.CL

**Keywords:** GPT-4, cross-cultural psychology, Big Five personality, LLMs, Ten-Item Personality Inventory

**Relevance Score:** 7

**TL;DR:** The paper explores whether GPT-4 can accurately replicate cross-cultural personality differences between US and South Korean individuals using the Big Five personality traits measured by a Ten-Item Inventory.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ability of large language models (LLMs) like GPT-4 in replicating established psychological findings regarding cross-cultural personality differences.

**Method:** A large-scale experiment involving 8000 participants where personality traits were assessed using the Ten-Item Personality Inventory, with variables manipulated including cultural context (US vs. South Korea), language (English vs. Korean), and language model type (GPT-4 vs GPT-3.5).

**Key Contributions:**

	1. Demonstrated GPT-4's capability to replicate cross-cultural personality traits.
	2. Identified the biases and limitations in the model's output compared to human data.
	3. Provided preliminary evidence for the utility of LLMs in psychological research.

**Result:** GPT-4 successfully replicated the cross-cultural personality differences; however, mean ratings showed an upward bias and lower variation compared to human samples, alongside reduced structural validity.

**Limitations:** The model exhibited biases in mean ratings and lower structural validity when compared to human assessments.

**Conclusion:** LLMs like GPT-4 can serve as useful tools for cross-cultural research, despite certain biases and limitations in replicating human personality assessments.

**Abstract:** We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.

</details>


### [106] [MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation](https://arxiv.org/abs/2405.11430)

*Jianbo Dai, Jianqiao Lu, Yunlong Feng, Guangtao Zeng, Rongju Ruan, Ming Cheng, Dong Huang, Haochen Tan, Zhijiang Guo*

**Main category:** cs.CL

**Keywords:** large language models, code generation, MHPP, HumanEval, evaluation benchmark

**Relevance Score:** 9

**TL;DR:** This paper introduces the Mostly Hard Python Problems (MHPP) dataset to better assess function-level code generation capabilities of large language models (LLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for evaluating LLMs in code generation are inadequate due to issues in quality, difficulty, and granularity.

**Method:** The study introduces a new dataset, MHPP, consisting of 210 human-curated problems focusing on natural language and code reasoning to evaluate LLM capabilities.

**Key Contributions:**

	1. Introduction of the MHPP dataset for evaluating LLMs' code generation capabilities.
	2. Identification of limitations in existing benchmarks like HumanEval and MBPP.
	3. Initial evaluation results demonstrating varying performances of LLMs across different datasets.

**Result:** Initial evaluations showed that many high-performing models on existing benchmarks like HumanEval did not perform as well on MHPP, revealing previously unknown limitations.

**Limitations:** The dataset may require further validation and does not cover all potential code generation scenarios.

**Conclusion:** The MHPP dataset can enhance understanding of LLM capabilities and limitations in code generation.

**Abstract:** Recent advancements in large language models (LLMs) have greatly improved code generation, specifically at the function level. For instance, GPT-4o has achieved a 91.0\% pass rate on HumanEval. However, this draws into question the adequacy of existing benchmarks in thoroughly assessing function-level code generation capabilities. Our study analyzed two common benchmarks, HumanEval and MBPP, and found that these might not thoroughly evaluate LLMs' code generation capacities due to limitations in quality, difficulty, and granularity. To resolve this, we introduce the Mostly Hard Python Problems (MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on the combination of natural language and code reasoning, MHPP gauges LLMs' abilities to comprehend specifications and restrictions, engage in multi-step reasoning, and apply coding knowledge effectively. Initial evaluations of 26 LLMs using MHPP showed many high-performing models on HumanEval failed to achieve similar success on MHPP. Moreover, MHPP highlighted various previously undiscovered limitations within various LLMs, leading us to believe that it could pave the way for a better understanding of LLMs' capabilities and limitations. MHPP, evaluation pipeline, and leaderboard can be found in https://github.com/SparksofAGI/MHPP.

</details>


### [107] [FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models](https://arxiv.org/abs/2406.05328)

*Yanling Wang, Haoyang Li, Hao Zou, Jing Zhang, Xinlei He, Qi Li, Ke Xu*

**Main category:** cs.CL

**Keywords:** large language models, non-factuality prediction, Factuality Lens, NLP, transferability

**Relevance Score:** 9

**TL;DR:** The paper introduces Factuality Lens (FacLens), a lightweight model designed to predict non-factual responses in large language models, enhancing efficiency and transferability across models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of non-factual responses in fact-seeking question answering by predicting non-factuality prior to response generation.

**Method:** The study proposes FacLens, which probes hidden representations of fact-seeking questions to predict non-factual responses.

**Key Contributions:**

	1. Introduction of Factuality Lens (FacLens) for non-factuality prediction
	2. Demonstration of hidden representations' transferability across LLMs
	3. Improved efficiency and effectiveness in predicting non-factual responses.

**Result:** FacLens demonstrates superior effectiveness and efficiency in predicting non-factual responses compared to previous methods.

**Limitations:** 

**Conclusion:** The findings indicate that FacLens is a promising tool for improving the reliability of LLMs in generating factual responses and can easily adapt across different LLMs.

**Abstract:** Despite advancements in large language models (LLMs), non-factual responses still persist in fact-seeking question answering. Unlike extensive studies on post-hoc detection of these responses, this work studies non-factuality prediction (NFP), predicting whether an LLM will generate a non-factual response prior to the response generation. Previous NFP methods have shown LLMs' awareness of their knowledge, but they face challenges in terms of efficiency and transferability. In this work, we propose a lightweight model named Factuality Lens (FacLens), which effectively probes hidden representations of fact-seeking questions for the NFP task. Moreover, we discover that hidden question representations sourced from different LLMs exhibit similar NFP patterns, enabling the transferability of FacLens across different LLMs to reduce development costs. Extensive experiments highlight FacLens's superiority in both effectiveness and efficiency.

</details>


### [108] [S2Cap: A Benchmark and a Baseline for Singing Style Captioning](https://arxiv.org/abs/2409.09866)

*Hyunjong Ok, Jaeho Lee*

**Main category:** cs.CL

**Keywords:** Singing Voices, Audio Datasets, Style Captioning

**Relevance Score:** 4

**TL;DR:** This paper introduces S2Cap, a new dataset for singing voices that enhances vocal style captioning by capturing diverse attributes and features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current audio-text datasets for singing voices lack comprehensive acoustic features and varied attributes, limiting their effectiveness in tasks like style captioning.

**Method:** We define the singing style captioning task and present the S2Cap dataset, which includes detailed descriptions of singing voices across various dimensions, followed by the development of a baseline algorithm for captioning.

**Key Contributions:**

	1. Introduction of the S2Cap dataset for singing voices
	2. Formal definition of the singing style captioning task
	3. Development of a baseline algorithm for style captioning

**Result:** S2Cap provides a richer dataset for singing voices, enabling improved performance in style captioning tasks compared to existing datasets.

**Limitations:** 

**Conclusion:** The introduction of the S2Cap dataset significantly enhances the analysis and understanding of singing voices for machine learning applications in audio tasks, especially style captioning.

**Abstract:** Singing voices contain much richer information than common voices, including varied vocal and acoustic properties. However, current open-source audio-text datasets for singing voices capture only a narrow range of attributes and lack acoustic features, leading to limited utility towards downstream tasks, such as style captioning. To fill this gap, we formally define the singing style captioning task and present S2Cap, a dataset of singing voices with detailed descriptions covering diverse vocal, acoustic, and demographic characteristics. Using this dataset, we develop an efficient and straightforward baseline algorithm for singing style captioning. The dataset is available at https://zenodo.org/records/15673764.

</details>


### [109] [Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming](https://arxiv.org/abs/2409.11041)

*Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen*

**Main category:** cs.CL

**Keywords:** Collaborative Robots, Large Language Models, Conversational Code Generation, Assembly Tasks, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper investigates the use of Large Language Models (LLMs) for conversational code generation to aid collaborative robots (cobots) in repetitive assembly tasks, aiming to improve interaction through natural language instructions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The traditional programming of collaborative robots is limited in expressivity and adaptability, which hinders their usability in dynamic environments like household settings.

**Method:** The authors propose a repetitive assembly task (RATS) as a framework for simulating industrial assembly scenarios, where programmers use natural language to instruct cobots. They create a dataset with paired target structures, example instructions, and evaluate LLMs in generating code from these instructions.

**Key Contributions:**

	1. Introduces RATS for evaluating cobot programming via natural language
	2. Demonstrates LLM capabilities in generating first-order and higher-order code
	3. Establishes a dataset for training and evaluating conversational code generation for assembly tasks

**Result:** LLMs demonstrated the ability to generate accurate first-order code (instruction sequences) but struggled with higher-order code generation (such as functions and loops).

**Limitations:** LLMs have difficulty generating higher-order code and need further improvement in context understanding for complex tasks.

**Conclusion:** The research highlights the potential and current limitations of LLMs in assisting cobots with code generation through natural language, providing a basis for future improvements.

**Abstract:** While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).

</details>


### [110] [LLMs Are In-Context Bandit Reinforcement Learners](https://arxiv.org/abs/2410.05362)

*Giovanni Monea, Antoine Bosselut, KiantÃ© Brantley, Yoav Artzi*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-context Learning, Reinforcement Learning, Contextual Bandits, Classification Tasks

**Relevance Score:** 8

**TL;DR:** This paper explores in-context reinforcement learning (ICRL) in large language models (LLMs), showing their effectiveness and limitations over various classification tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models can learn from external rewards in a contextual bandit framework instead of relying solely on supervised data.

**Method:** The authors conducted experiments using LLMs ranging from 500M to 70B parameters on challenging classification tasks, focusing on in-context learning with external rewards.

**Key Contributions:**

	1. Introduction of the contextual bandit approach to in-context learning in LLMs
	2. Detailed examination of learning stability and scaling trends in LLMs
	3. Empirical results showing LLM performance with semantic and abstract labels

**Result:** LLMs demonstrated effective in-context reinforcement learning capabilities while revealing instability and limitations in reasoning about errors during the learning process.

**Limitations:** The process exhibited instability and limited implicit reasoning about errors.

**Conclusion:** The study highlights the potential of ICRL in LLMs but also points out critical challenges that need addressing, particularly regarding the model's implicit reasoning capabilities.

**Abstract:** Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.

</details>


### [111] [StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning](https://arxiv.org/abs/2410.07745)

*Yuanqing Yu, Zhefan Wang, Weizhi Ma, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, tool learning, reinforcement learning

**Relevance Score:** 9

**TL;DR:** Proposes StepTool, a reinforcement learning framework to improve large language models' (LLMs) ability to use external tools through dynamic decision-making and fine-grained reward shaping.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with multi-step tool learning, which requires effective decision-making rather than just text generation.

**Method:** Introduces StepTool, which includes Step-grained Reward Shaping and Step-grained Optimization to enhance decision-making in tool use through reinforcement learning.

**Key Contributions:**

	1. Introduction of StepTool as a novel reinforcement learning framework for LLMs.
	2. Step-grained Reward Shaping to assess tool interaction effectiveness.
	3. Step-grained Optimization to improve policy across decision steps.

**Result:** StepTool significantly outperforms traditional fine-tuning and reinforcement learning methods on benchmarks, improving task pass rates and tool recall.

**Limitations:** 

**Conclusion:** Fine-grained decision modeling is crucial for tool learning in LLMs, with StepTool providing a robust and general approach to enhance multi-step tool use.

**Abstract:** Despite their powerful text generation capabilities, large language models (LLMs) still struggle to effectively utilize external tools to solve complex tasks, a challenge known as tool learning. Existing methods primarily rely on supervised fine-tuning, treating tool learning as a text generation problem while overlooking the decision-making complexities inherent in multi-step contexts. In this work, we propose modeling tool learning as a dynamic decision-making process and introduce StepTool, a novel step-grained reinforcement learning framework that enhances LLMs' capabilities in multi-step tool use. StepTool comprises two key components: Step-grained Reward Shaping, which assigns rewards to each tool interaction based on its invocation success and contribution to task completion; and Step-grained Optimization, which applies policy gradient methods to optimize the model across multiple decision steps. Extensive experiments across diverse benchmarks show that StepTool consistently outperforms both SFT-based and RL-based baselines in terms of task Pass Rate and Recall of relevant tools. Furthermore, our analysis suggests that StepTool helps models discover new tool-use strategies rather than merely re-weighting prior knowledge. These results highlight the importance of fine-grained decision modeling in tool learning and establish StepTool as a general and robust solution for enhancing multi-step tool use in LLMs. Code and data are available at https://github.com/yuyq18/StepTool.

</details>


### [112] [NormXLogit: The Head-on-Top Never Lies](https://arxiv.org/abs/2411.16252)

*Sina Abbasi, Mohammad Reza Modarres, Mohammad Taher Pilehvar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Interpretability, Token Importance

**Relevance Score:** 9

**TL;DR:** This paper proposes NormXLogit, a model-agnostic method for assessing token importance in large language models (LLMs) to improve interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a model-agnostic approach for interpretability in LLMs that overcomes the limitations of existing complex, model-specific methods.

**Method:** NormXLogit assesses the significance of individual input tokens by using the norms of word embeddings during LLM pre-training and analyzing the relationship between token importance and final predictions.

**Key Contributions:**

	1. Introduction of NormXLogit for token importance assessment
	2. Demonstration of relationship between token representation and model predictions
	3. Evidence of improved interpretability and computational efficiency over existing methods

**Result:** NormXLogit outperforms existing gradient-based methods in terms of faithfulness and provides competitive performance compared to architecture-specific techniques.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances the interpretability of LLMs while being computationally efficient and easier to apply universally across different models.

**Abstract:** With new large language models (LLMs) emerging frequently, it is important to consider the potential value of model-agnostic approaches that can provide interpretability across a variety of architectures. While recent advances in LLM interpretability show promise, many rely on complex, model-specific methods with high computational costs. To address these limitations, we propose NormXLogit, a novel technique for assessing the significance of individual input tokens. This method operates based on the input and output representations associated with each token. First, we demonstrate that during the pre-training of LLMs, the norms of word embeddings effectively capture token importance. Second, we reveal a significant relationship between a token's importance and the extent to which its representation can resemble the model's final prediction. Extensive analyses reveal that our approach outperforms existing gradient-based methods in terms of faithfulness and offers competitive performance in layer-wise explanations compared to leading architecture-specific techniques.

</details>


### [113] [Idiom Detection in Sorani Kurdish Texts](https://arxiv.org/abs/2501.14528)

*Skala Kamaran Omer, Hossein Hassani*

**Main category:** cs.CL

**Keywords:** Idiom Detection, Natural Language Processing, Sorani Kurdish, Deep Learning, Transformer Models

**Relevance Score:** 3

**TL;DR:** This study focuses on idiom detection in Sorani Kurdish using deep learning, resulting in a new dataset and superior performance by Transformer models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant research gap in idiom detection for the Sorani Kurdish language, which is crucial for applications like machine translation and sentiment analysis.

**Method:** The study approached idiom detection as a text classification task utilizing deep learning techniques, specifically developing three models: KuBERT-based transformer, RCNN, and BiLSTM with an attention mechanism.

**Key Contributions:**

	1. Development of a dataset for Sorani Kurdish idioms
	2. Evaluation of three deep learning models for idiom detection
	3. Demonstration of high accuracy using Transformer-based architecture

**Result:** The experiments demonstrated that the fine-tuned KuBERT transformer model achieved nearly 99% accuracy, outperforming the RCNN model at 96.5% and the BiLSTM model at 80%.

**Limitations:** 

**Conclusion:** This research contributes a dataset of 10,580 sentences with 101 idioms and demonstrates the effectiveness of Transformer architectures in low-resource languages.

**Abstract:** Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.

</details>


### [114] [2SSP: A Two-Stage Framework for Structured Pruning of LLMs](https://arxiv.org/abs/2501.17771)

*Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca*

**Main category:** cs.CL

**Keywords:** structured pruning, large language models, width pruning, depth pruning, sparsity

**Relevance Score:** 8

**TL;DR:** The paper introduces a Two-Stage framework for Structured Pruning (2SSP) of Large Language Models, combining Width and Depth Pruning strategies to enhance model efficiency while preserving performance metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of Large Language Models (LLMs) through structured pruning while maintaining performance on language tasks.

**Method:** The proposed method consists of two stages: Width Pruning, which removes entire neurons based on their importance score, and Depth Pruning, which iteratively removes entire Attention submodules with minimal impact on a metric of interest like perplexity.

**Key Contributions:**

	1. Introduction of a Two-Stage pruning framework for LLMs
	2. Balanced sparsity mechanism across Width and Depth Pruning
	3. Performance improvements over existing pruning methods in multiple tasks

**Result:** 2SSP consistently outperforms five state-of-the-art competitors across three language modeling datasets and six downstream tasks, achieving substantial improvements in pruning time.

**Limitations:** 

**Conclusion:** The 2SSP framework effectively balances sparsity across its two stages, leading to improved efficiency in LLMs without sacrificing performance.

**Abstract:** We propose a novel Two-Stage framework for Structured Pruning (\textsc{2SSP}) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron on the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test \textsc{2SSP} on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at https://github.com/FabrizioSandri/2SSP.

</details>


### [115] [VisualSpeech: Enhancing Prosody Modeling in TTS Using Video](https://arxiv.org/abs/2501.19258)

*Shumin Que, Anton Ragni*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, prosody prediction, visual information, synthesis, speech expressiveness

**Relevance Score:** 4

**TL;DR:** This paper presents VisualSpeech, a model that integrates visual context to improve prosody prediction in Text-to-Speech synthesis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance prosody prediction in TTS by utilizing visual context, which is often available but underutilized.

**Method:** A novel model called VisualSpeech is proposed, which combines visual and textual information to refine prosody generation in TTS.

**Key Contributions:**

	1. Introduction of VisualSpeech model for TTS
	2. Demonstration of improved prosody generation through visual integration
	3. Presentation of audio samples showcasing enhanced speech expressiveness

**Result:** Empirical results show that integrating visual features leads to improved prosodic modeling and enhanced expressiveness of synthesized speech.

**Limitations:** 

**Conclusion:** The findings suggest that visual context is a valuable asset in generating more natural and expressive speech outputs in TTS systems.

**Abstract:** Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech. Audio samples are available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.

</details>


### [116] [Dealing with Annotator Disagreement in Hate Speech Classification](https://arxiv.org/abs/2502.08266)

*Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu*

**Main category:** cs.CL

**Keywords:** Hate Speech Detection, Machine Learning, Natural Language Processing, Social Media, Annotation

**Relevance Score:** 4

**TL;DR:** This paper focuses on hate speech detection in social media, specifically Turkish tweets, and addresses the challenge of annotator disagreement in labeling data for machine learning models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Hate speech detection is essential to mitigate the impact of harmful content spreading on social media.

**Method:** The paper evaluates various automatic approaches for aggregating multiple annotations to improve hate speech classification.

**Key Contributions:**

	1. Examined automatic aggregation methods for multiple annotations.
	2. Provided benchmark results for hate speech detection in Turkish tweets.
	3. Highlighted the significance of addressing annotator disagreement.

**Result:** State-of-the-art benchmark results were achieved for detecting and understanding hate speech in online discourse, emphasizing the importance of handling annotator disagreement.

**Limitations:** 

**Conclusion:** The findings underline the need for effective strategies in managing annotator disagreement to enhance model performance for hate speech detection.

**Abstract:** Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is essential for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate various automatic approaches for aggregating multiple annotations, in the context of hate speech classification in Turkish tweets. Our work highlights the importance of the problem and provides state-of-the-art benchmark results for the detection and understanding of hate speech in online discourse.

</details>


### [117] [LIDDIA: Language-based Intelligent Drug Discovery Agent](https://arxiv.org/abs/2502.13959)

*Reza Averly, Frazier N. Baker, Ian A. Watson, Xia Ning*

**Main category:** cs.CL

**Keywords:** drug discovery, AI, large language models, autonomous agents, health informatics

**Relevance Score:** 8

**TL;DR:** LIDDIA is an AI-based autonomous agent that intelligently navigates the drug discovery process, generating pharmaceutical-grade molecules and identifying promising candidates for cancer treatments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lengthy and complex challenges of drug discovery, which heavily rely on human expertise, and to create an intelligent agent that can streamline the process.

**Method:** Development of LIDDIA, an autonomous agent that uses large language models to generate and evaluate potential drug candidates in silico, focusing on balancing exploration and exploitation in chemical space.

**Key Contributions:**

	1. Introduction of LIDDIA, an AI-driven agent for drug discovery
	2. Capability to generate viable drug candidates for over 70% of targets
	3. Identification of a novel drug candidate for cancer treatments, showcasing its application in health informatics

**Result:** LIDDIA successfully generated molecules for over 70% of targeted clinical trials and identified a novel candidate for a significant cancer target, showcasing its effectiveness in drug discovery.

**Limitations:** 

**Conclusion:** LIDDIA demonstrates the potential of AI in transforming the drug discovery landscape, offering a scalable and cost-effective solution.

**Abstract:** Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDIA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDIA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDIA , demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it identifies one promising novel candidate on AR/NR3C4, a critical target for both prostate and breast cancers. Code and dataset are available at https://github.com/ninglab/LIDDiA

</details>


### [118] [An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data](https://arxiv.org/abs/2503.07303)

*Gideon Yoffe, Yair Segev, Barak Sober*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Text Analysis, Information Theory, Cultural Context

**Relevance Score:** 2

**TL;DR:** This study develops an information-theoretic algorithm to identify formulaic clusters in texts, particularly multi-author documents like the Hebrew Bible, through the analysis of recurring patterns and self-information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding structural and stylistic patterns in texts provides insights into their origins, purpose, and transmission, especially in complex multi-author documents.

**Method:** An information-theoretic algorithm is developed that utilizes weighted self-information distributions to identify structured patterns in texts, extending classical self-information measures with a continuous formulation.

**Key Contributions:**

	1. Development of a novel algorithm for identifying formulaic clusters in literary texts
	2. Application of the algorithm to the Hebrew Bible to isolate stylistic layers
	3. Extension of classical self-information measures to improve pattern detection in high-dimensional texts.

**Result:** The algorithm successfully isolates stylistic layers in hypothesized authorial divisions of the Hebrew Bible, providing a quantitative framework for analyzing compositional patterns.

**Limitations:** 

**Conclusion:** This method enhances text analysis by offering deeper insights into the literary and cultural evolution of documents shaped by complex authorship and editorial processes.

**Abstract:** Texts, whether literary or historical, exhibit structural and stylistic patterns shaped by their purpose, authorship, and cultural context. Formulaic texts, characterized by repetition and constrained expression, tend to have lower variability in self-information compared to more dynamic compositions. Identifying such patterns in historical documents, particularly multi-author texts like the Hebrew Bible provides insights into their origins, purpose, and transmission.   This study aims to identify formulaic clusters -- sections exhibiting systematic repetition and structural constraints -- by analyzing recurring phrases, syntactic structures, and stylistic markers. However, distinguishing formulaic from non-formulaic elements in an unsupervised manner presents a computational challenge, especially in high-dimensional textual spaces where patterns must be inferred without predefined labels.   To address this, we develop an information-theoretic algorithm leveraging weighted self-information distributions to detect structured patterns in text, unlike covariance-based methods, which become unstable in small-sample, high-dimensional settings, our approach directly models variations in self-information to identify formulaicity. By extending classical discrete self-information measures with a continuous formulation based on differential self-information, our method remains applicable across different types of textual representations, including neural embeddings under Gaussian priors.   Applied to hypothesized authorial divisions in the Hebrew Bible, our approach successfully isolates stylistic layers, providing a quantitative framework for textual stratification. This method enhances our ability to analyze compositional patterns, offering deeper insights into the literary and cultural evolution of texts shaped by complex authorship and editorial processes.

</details>


### [119] [More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models](https://arxiv.org/abs/2503.15904)

*Evan Chen, Run-Jun Zhan, Yan-Bai Lin, Hung-Hsuan Chen*

**Main category:** cs.CL

**Keywords:** Language Models, Gender Bias, Human Feedback, Natural Language Processing, Fairness

**Relevance Score:** 9

**TL;DR:** This study evaluates gender biases in Large Language Models (LLMs) through storytelling, revealing overrepresentation of female characters and alignment with human stereotypes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about gender biases in LLMs and their implications for fairness in AI applications.

**Method:** A novel evaluation framework using free-form storytelling to analyze gender representation in ten prominent LLMs.

**Key Contributions:**

	1. Introduction of a novel framework for evaluating gender biases in LLMs
	2. Empirical analysis of gender representation in ten LLMs
	3. Release of prompts and generated content for further research

**Result:** The analysis showed a consistent overrepresentation of female characters in various occupations, influenced by supervised fine-tuning and reinforcement learning, aligning more with stereotypes than real-world data.

**Limitations:** Focused solely on gender biases; other social biases may not be addressed.

**Conclusion:** The findings stress the need for balanced mitigation measures in LLMs to combat existing biases and avoid creating new ones.

**Abstract:** Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases. This study introduces a novel evaluation framework to uncover gender biases in LLMs: using free-form storytelling to surface biases embedded within the models. A systematic analysis of ten prominent LLMs shows a consistent pattern of overrepresenting female characters across occupations, likely due to supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Paradoxically, despite this overrepresentation, the occupational gender distributions produced by these LLMs align more closely with human stereotypes than with real-world labor data. This highlights the challenge and importance of implementing balanced mitigation measures to promote fairness and prevent the establishment of potentially new biases. We release the prompts and LLM-generated stories at GitHub.

</details>


### [120] [TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection](https://arxiv.org/abs/2503.24115)

*Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang*

**Main category:** cs.CL

**Keywords:** telecom fraud, multimodal dataset, machine learning, LLM, audio-text analysis

**Relevance Score:** 6

**TL;DR:** The paper introduces TeleAntiFraud-28k, an open-source audio-text dataset for telecom fraud detection, and TeleAntiFraud-Bench, a benchmark for evaluating models on this dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the significant challenges in telecom fraud detection arising from insufficient multimodal training data that combines audio and text analysis.

**Method:** The dataset is generated using privacy-preserved text created from ASR-transcribed recordings, enhanced by LLM-based sampling, and includes adversarially synthesized scenarios.

**Key Contributions:**

	1. Introduction of TeleAntiFraud-28k audio-text dataset for telecom fraud detection
	2. Development of TeleAntiFraud-Bench for standardized model evaluation
	3. Production-optimized supervised fine-tuning model for hybrid data

**Result:** The constructed dataset comprises 28,511 speech-text pairs and supports three main tasks: scenario classification, fraud detection, and classification of fraud types; as well as an evaluation benchmark for testing models.

**Limitations:** 

**Conclusion:** This work establishes a critical foundation for multimodal anti-fraud research, addressing issues of data privacy and diversity while providing tools for community expansion.

**Abstract:** The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.

</details>


### [121] [SpectR: Dynamically Composing LM Experts with Spectral Routing](https://arxiv.org/abs/2504.03454)

*William Fleshman, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** language models, expert models, dynamic composition, inference, routing accuracy

**Relevance Score:** 8

**TL;DR:** SPECTR is a method for dynamically composing expert language models at inference time without additional training, improving routing accuracy and task performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of training large, general-purpose language models and presents an alternative in the form of specialized expert models fine-tuned for specific tasks or domains.

**Method:** SPECTR dynamically composes expert models at each time step during inference, allowing token- and layer-wise model combinations without requiring additional training.

**Key Contributions:**

	1. Introduction of the SPECTR method for dynamic expert model composition
	2. Demonstration of improved routing accuracy
	3. Showcased task performance increase in expert domains

**Result:** Experimental results show that SPECTR improves routing accuracy over other training-free methods and enhances task performance across various expert domains.

**Limitations:** 

**Conclusion:** SPECTR offers a flexible and effective approach for leveraging specialized expert models in real-world applications, improving performance without extra training.

**Abstract:** Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.

</details>


### [122] [Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models](https://arxiv.org/abs/2504.04823)

*Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou*

**Main category:** cs.CL

**Keywords:** Quantization, Reasoning Models, Language Models

**Relevance Score:** 6

**TL;DR:** The paper examines the effects of quantization on reasoning language models, focusing on performance across various benchmarks while managing inference costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of quantization on reasoning models, as existing studies have primarily focused on large language models without addressing reasoning efficiency.

**Method:** The study evaluates several quantized models, applying weight, KV cache, and activation quantization techniques at different bit-widths, and assesses their performance on mathematical, scientific, and programming reasoning tasks.

**Key Contributions:**

	1. First systematic study on quantized reasoning models
	2. Explication of performance determinants
	3. Open-sourced models for community use

**Result:** Lossless quantization is achievable with specific configurations, but lower bit-widths pose accuracy risks; model size and task difficulty are influential factors in performance outcomes.

**Limitations:** The study is limited to specific model families and tasks; further research could explore broader applications and other model architectures.

**Conclusion:** Quantized reasoning models can maintain performance with certain optimization strategies, and all models tested are available as open-source.

**Abstract:** Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.

</details>


### [123] [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)

*Benjamin Lipkin, Benjamin LeBrun, Jacob Hoover Vigly, JoÃ£o Loula, David R. MacIver, Li Du, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Timothy J. O'Donnell, Alexander K. Lew, Tim Vieira*

**Main category:** cs.CL

**Keywords:** constrained decoding, adaptive rejection sampling, importance weights

**Relevance Score:** 9

**TL;DR:** This paper introduces a new adaptive rejection sampling algorithm for generating constrained outputs from language models, significantly improving runtime and performance by reducing constraint evaluations and enabling better estimation of importance weights.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to efficiently generate outputs from language models under constraints without compromising performance due to the high cost of evaluating large vocabularies at each step.

**Method:** An adaptive rejection sampling algorithm that reduces the number of required constraint evaluations while producing unbiased estimates of importance weights, which can correct for local constraint enforcement's myopic behavior.

**Key Contributions:**

	1. Introduction of an adaptive rejection sampling algorithm
	2. Ability to produce low-variance, unbiased estimates of importance weights
	3. Demonstrated superior performance and broader applicability across multiple domains

**Result:** The proposed algorithm demonstrated superior performance compared to state-of-the-art baselines across various domains, including text-to-SQL and molecular synthesis, while enhancing runtime efficiency.

**Limitations:** 

**Conclusion:** The new method offers a more efficient means of generating constrained outputs from language models, catering to a wider range of constraints and promoting overall runtime and performance improvements.

**Abstract:** The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.

</details>


### [124] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)

*Manya Wadhwa, Zayne Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, Greg Durrett*

**Main category:** cs.CL

**Keywords:** language models, evaluation criteria, human-computer interaction, LLM, structured writing

**Relevance Score:** 9

**TL;DR:** EvalAgent is a framework designed to uncover nuanced and task-specific evaluation criteria for language model outputs by mining expert guidance and proposing criteria that enhance response quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation process of language model outputs on structured writing tasks by identifying implicit, task-specific criteria that go beyond basic requirements.

**Method:** EvalAgent mines expert-authored online guidance to propose diverse evaluation criteria that are grounded in reliable external sources, improving the specificity and quality of evaluation.

**Key Contributions:**

	1. Introduction of EvalAgent framework for automatic uncovering of evaluation criteria
	2. Demonstration that EvalAgent criteria improve upon initial responses
	3. Evidence that combined criteria yield more valuable evaluations from human perspectives.

**Result:** EvalAgent-generated criteria are often implicit yet specific, revealing additional human-valued criteria in model responses. Combining these with LLM-generated criteria yields better evaluations than using LLMs alone.

**Limitations:** 

**Conclusion:** The implementation of EvalAgent can significantly improve the quality of language model outputs by focusing on nuanced and actionable evaluation criteria.

**Abstract:** Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like "Help me draft an academic talk on coffee intake vs research productivity", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.

</details>


### [125] [Translation in the Wild](https://arxiv.org/abs/2505.23548)

*Yuri Balashov*

**Main category:** cs.CL

**Keywords:** Large Language Models, translation, instruction tuning, bilingualism, deep learning

**Relevance Score:** 8

**TL;DR:** This paper explores the translation capabilities of Large Language Models (LLMs), proposing that their abilities stem from incidental bilingualism in training data and instruction tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the underlying reasons behind the translation abilities of LLMs, which were not specifically trained for translation tasks.

**Method:** The author reflects on existing studies and user experiences related to LLMs' translation performance, proposing the 'duality' hypothesis regarding pre-training data.

**Key Contributions:**

	1. Proposition of the 'duality' hypothesis regarding LLMs' pre-training data and its effects on translation ability.
	2. Reflections on the role of instruction tuning in enhancing LLM translation capabilities.
	3. Discussion on empirical testing of the duality hypothesis and its implications for translation theory.

**Result:** LLMs exhibit competitive translation performance despite not being trained on translation objectives, attributing this to their utilization of diverse linguistic information from varying contexts.

**Limitations:** 

**Conclusion:** The translation abilities of LLMs may redefine our understanding of translation processes, both human and machine, underpinned by deep learning techniques.

**Abstract:** Large Language Models (LLMs) excel in translation among other things, demonstrating competitive performance for many language pairs in zero- and few-shot settings. But unlike dedicated neural machine translation models, LLMs are not trained on any translation-related objective. What explains their remarkable translation abilities? Are these abilities grounded in "incidental bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning contribute to it? Are LLMs capable of aligning and leveraging semantically identical or similar monolingual contents from different corners of the internet that are unlikely to fit in a single context window? I offer some reflections on this topic, informed by recent studies and growing user experience. My working hypothesis is that LLMs' translation abilities originate in two different types of pre-training data that may be internalized by the models in different ways. I discuss the prospects for testing the "duality" hypothesis empirically and its implications for reconceptualizing translation, human and machine, in the age of deep learning.

</details>
