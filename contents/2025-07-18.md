# 2025-07-18

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 53]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] ["How to Explore Biases in Speech Emotion AI with Users?" A Speech-Emotion-Acting Study Exploring Age and Language Biases](https://arxiv.org/abs/2507.12580)

*Josephine Beatrice Skovbo Borre, Malene Gorm Wold, Sara Kj√¶r Rasmussen, Ilhan Aslan*

**Main category:** cs.HC

**Keywords:** Speech Emotion Recognition, Human-Computer Interaction, Age and Language

**Relevance Score:** 7

**TL;DR:** This study investigates how age and language affect vocal emotional expression in speech emotion recognition (SER) across Teenagers and Adults 55+, revealing no significant differences in SER model interpretations despite limitations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the interaction between age, language, and deliberate vocal expression of emotion in speech emotion recognition (SER), particularly focusing on underexplored groups.

**Method:** Developed a custom user interface combined with a backend for real-time SER prediction to evaluate intentional emotional speech in different age groups and languages (Danish and English).

**Key Contributions:**

	1. Provides new insights into user groups often overlooked in SER studies.
	2. Develops a novel experimental paradigm for evaluating SER across age and language.
	3. Recommends a shift towards inclusive SER models that consider human emotional intent.

**Result:** No significant differences in SER interpretations across age or language groups were found, suggesting robustness of SER models despite some limitations in high-arousal emotion recognition.

**Limitations:** Some reliance on self-managed voice recordings and inconsistent task execution.

**Conclusion:** Highlights the importance of moving towards more human-centered SER models that account for affective misalignment between human intent and machine interpretation.

**Abstract:** This study explores how age and language shape the deliberate vocal expression of emotion, addressing underexplored user groups, Teenagers (N = 12) and Adults 55+ (N = 12), within speech emotion recognition (SER). While most SER systems are trained on spontaneous, monolingual English data, our research evaluates how such models interpret intentionally performed emotional speech across age groups and languages (Danish and English). To support this, we developed a novel experimental paradigm combining a custom user interface with a backend for real-time SER prediction and data logging. Participants were prompted to hit visual targets in valence-arousal space by deliberately expressing four emotion targets. While limitations include some reliance on self-managed voice recordings and inconsistent task execution, the results suggest contrary to expectations, no significant differences between language or age groups, and a degree of cross-linguistic and age robustness in model interpretation. Though some limitations in high-arousal emotion recognition were evident. Our qualitative findings highlight the need to move beyond system-centered accuracy metrics and embrace more inclusive, human-centered SER models. By framing emotional expression as a goal-directed act and logging the real-time gap between human intent and machine interpretation, we expose the risks of affective misalignment.

</details>


### [2] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)

*Kuangshi Ai, Kaiyuan Tang, Chaoli Wang*

**Main category:** cs.HC

**Keywords:** Volume Visualization, Natural Language Interaction, Semantic Segmentation

**Relevance Score:** 6

**TL;DR:** NLI4VolVis is an interactive system that allows users to explore and edit volumetric scenes using natural language, improving usability and accessibility in volume visualization.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional volume visualization methods are limited by rigid transfer functions and high computational costs, which hinder user interaction and efficiency.

**Method:** NLI4VolVis employs multi-view semantic segmentation and vision-language models to analyze and interpret user intents, utilizing a multi-agent large language model architecture with function-calling tools for interactive visualization tasks.

**Key Contributions:**

	1. Interactive volumetric visualization using natural language
	2. Integration of vision-language models with semantic segmentation
	3. Real-time scene editing and object querying capabilities

**Result:** The system facilitates open-vocabulary object querying, real-time scene editing, and best-view selection, demonstrating improved user accessibility and efficiency through case studies and user testing.

**Limitations:** 

**Conclusion:** NLI4VolVis significantly enhances the interactive capabilities of volumetric data visualization while being accessible to non-expert users.

**Abstract:** Traditional volume visualization (VolVis) methods, like direct volume rendering, suffer from rigid transfer function designs and high computational costs. Although novel view synthesis approaches enhance rendering efficiency, they require additional learning effort for non-experts and lack support for semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an interactive system that enables users to explore, query, and edit volumetric scenes using natural language. NLI4VolVis integrates multi-view semantic segmentation and vision-language models to extract and understand semantic components in a scene. We introduce a multi-agent large language model architecture equipped with extensive function-calling tools to interpret user intents and execute visualization tasks. The agents leverage external tools and declarative VolVis commands to interact with the VolVis engine powered by 3D editable Gaussians, enabling open-vocabulary object querying, real-time scene editing, best-view selection, and 2D stylization. We validate our system through case studies and a user study, highlighting its improved accessibility and usability in volumetric data exploration. We strongly recommend readers check our case studies, demo video, and source code at https://nli4volvis.github.io/.

</details>


### [3] [Design Patterns of Human-AI Interfaces in Healthcare](https://arxiv.org/abs/2507.12721)

*Rui Sheng, Chuhan Shi, Sobhan Lotfi, Shiyi Liu, Adam Perer, Huamin Qu, Furui Cheng*

**Main category:** cs.HC

**Keywords:** Human-AI interfaces, Healthcare, Design patterns, User experience, HCI

**Relevance Score:** 9

**TL;DR:** The paper provides guidance for designing human-AI interfaces in healthcare by summarizing 12 design patterns, based on interviews and workshops with healthcare professionals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the design of human-AI interfaces in the healthcare domain, which is essential for effective human-AI collaboration.

**Method:** Interviews with 12 healthcare professionals and workshops with 14 participants to evaluate and explore usage scenarios for 12 identified design patterns.

**Key Contributions:**

	1. Systematic guidance for designing human-AI interfaces in healthcare.
	2. Identification and evaluation of 12 key design patterns.
	3. Insights from healthcare professionals on usage scenarios and considerations.

**Result:** The study identified and summarized 12 design patterns that enhance human-AI interactions in healthcare, providing a systematic framework for interface designers.

**Limitations:** The generalizability of the design patterns beyond healthcare has not been extensively tested yet.

**Conclusion:** The design patterns are potentially generalizable to other application domains, and the work highlights limitations and future research directions.

**Abstract:** Human-AI interfaces play a crucial role in advancing practices and research within the healthcare domain. However, designing such interfaces presents a substantial challenge for designers. In this paper, we propose systematic guidance for designing human-AI interfaces in typical healthcare scenarios by summarizing the design patterns for presenting and interacting with common information entities. To deepen our understanding of these 12 design patterns, we interviewed 12 healthcare professionals to explore potential usage scenarios and important considerations. Furthermore, we conducted workshops with 14 participants recruited online to evaluate our design patterns. Finally, we discussed the generalizability of the design patterns to other application domains, the limitations, and the future work.

</details>


### [4] [An Age-based Study into Interactive Narrative Visualization Engagement](https://arxiv.org/abs/2507.12734)

*Nina Errey, Yi Chen, Yu Dong, Quang Vinh Nguyen, Xiaoru Yuan, Tuck Wah Leong, Christy Jie Liang*

**Main category:** cs.HC

**Keywords:** interactive visualization, audience engagement, age differences, inclusive design, data storytelling

**Relevance Score:** 7

**TL;DR:** Age impacts engagement in interactive narrative visualization, with younger audiences showing higher engagement levels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how audience age affects engagement in interactive narrative visualization and address this gap in literature.

**Method:** An empirical experiment using an established visualization engagement questionnaire to compare engagement scores across different age cohorts.

**Key Contributions:**

	1. Empirical evidence of age-related engagement differences in narrative visualizations.
	2. Qualitative insights into audience interactions with narrative patterns.
	3. Practical recommendations for inclusive design in interactive visualizations.

**Result:** Younger audiences showed higher engagement scores than older ones, and qualitative feedback indicated a better understanding of narrative patterns among younger participants.

**Limitations:** The study may not account for other demographic factors that could influence engagement.

**Conclusion:** The paper recommends inclusive design strategies for interactive narrative visualizations tailored to different age groups.

**Abstract:** Research has shown that an audiences' age impacts their engagement in digital media. Interactive narrative visualization is an increasingly popular form of digital media that combines data visualization and storytelling to convey important information. However, audience age is often overlooked by interactive narrative visualization authors. Using an established visualization engagement questionnaire, we ran an empirical experiment where we compared end-user engagement to audience age. We found a small difference in engagement scores where older age cohorts were less engaged than the youngest age cohort. Our qualitative analysis revealed that the terminology and overall understanding of interactive narrative patterns integrated into narrative visualization was more apparent in the feedback from younger age cohorts relative to the older age cohorts. We conclude this paper with a series of recommendations for authors of interactive narrative visualization on how to design inclusively for audiences according to their age.

</details>


### [5] [Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land](https://arxiv.org/abs/2507.12741)

*Lotfi El Hafi, Kazuma Onishi, Shoichi Hasegawa, Akira Oyama, Tomochika Ishikawa, Masashi Osada, Carl Tornberg, Ryoma Kado, Kento Murata, Saki Hashimoto, Sebastian Carrera Villalobos, Akira Taniguchi, Gustavo Alfonso Garcia Ricardez, Yoshinobu Hagiwara, Tatsuya Aoki, Kensuke Iwata, Takato Horii, Yukiko Horikawa, Takahiro Miyashita, Tadahiro Taniguchi, Hiroshi Ishiguro*

**Main category:** cs.HC

**Keywords:** cybernetic avatars, human-robot interaction, autonomous systems, public perception, robotic assistance

**Relevance Score:** 7

**TL;DR:** The study assesses public perceptions of fully autonomous cybernetic avatars (CAs) for physical support, highlighting interest and concerns regarding task reliability based on survey responses from a large-scale demonstration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate public perception and potential social impacts of fully autonomous cybernetic avatars (CAs) in addressing physical limitations in daily life.

**Method:** Conducted a large-scale demonstration and survey during Avatar Land in Osaka, Japan, with 2,285 visitors engaging with various CAs, alongside a focused survey for 333 participants who interacted with fully autonomous CAs.

**Key Contributions:**

	1. Large-scale evaluation of public perceptions towards fully autonomous cybernetic avatars.
	2. Insights on reliability concerns versus other factors like cost and interaction quality.
	3. Data-driven understanding of societal impacts from integrating CAs in everyday tasks.

**Result:** Survey results show high interest in CAs for physical support, but significant concerns regarding reliability of task execution were noted, while cost and human-like interaction were less concerning.

**Limitations:** Limited to survey responses from a specific event and location, which may not generalize to broader populations.

**Conclusion:** The findings demonstrate a positive inclination towards integrating fully autonomous CAs in daily life, though addressing reliability is crucial for broader acceptance.

**Abstract:** Cybernetic avatars (CAs) are key components of an avatar-symbiotic society, enabling individuals to overcome physical limitations through virtual agents and robotic assistants. While semi-autonomous CAs intermittently require human teleoperation and supervision, the deployment of fully autonomous CAs remains a challenge. This study evaluates public perception and potential social impacts of fully autonomous CAs for physical support in daily life. To this end, we conducted a large-scale demonstration and survey during Avatar Land, a 19-day public event in Osaka, Japan, where fully autonomous robotic CAs, alongside semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we analyzed responses from 2,285 visitors who engaged with various CAs, including a subset of 333 participants who interacted with fully autonomous CAs and shared their perceptions and concerns through a survey questionnaire. The survey results indicate interest in CAs for physical support in daily life and at work. However, concerns were raised regarding task execution reliability. In contrast, cost and human-like interaction were not dominant concerns. Project page: https://lotfielhafi.github.io/FACA-Survey/.

</details>


### [6] [PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts](https://arxiv.org/abs/2507.12749)

*Xumeng Wang, Xiangxuan Zhang, Zhiqi Gao, Shuangcheng Jiao, Yuxin Ma*

**Main category:** cs.HC

**Keywords:** perception simulation, chart design, visualization, PatternSight, human perception

**Relevance Score:** 4

**TL;DR:** The paper introduces a perception simulation model that helps chart authors evaluate the effectiveness of their chart designs by predicting perceptual patterns that viewers notice, and presents a prototype interface called PatternSight to assist in optimizing chart designs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Chart authors may struggle to design effective visual representations without understanding perceptual theories, leading to ineffective communication of data patterns.

**Method:** The paper proposes a perception simulation model that integrates perceptual theory with visual feature extraction to predict graphical patterns noticed by viewers. This model is implemented in a prototype interface named PatternSight.

**Key Contributions:**

	1. Development of a perception simulation model for chart design
	2. Creation of the PatternSight interface for evaluating chart effectiveness
	3. Validation through human perceptual experiments showing model accuracy

**Result:** Human perceptual results demonstrate that the model successfully simulates the grouping behaviors of chart viewers and encapsulates diverse perceptual outcomes. User experiments show that PatternSight effectively aids authors in improving chart designs to better represent data patterns.

**Limitations:** The model may not cover all individual differences in perception and may require additional testing across diverse datasets.

**Conclusion:** The integration of perceptual theory into chart design via the PatternSight interface provides a valuable tool for authors to create more effective charts that communicate data patterns clearly.

**Abstract:** The boom in visualization generation tools has significantly lowered the threshold for chart authoring. Nevertheless, chart authors with an insufficient understanding of perceptual theories may encounter difficulties in evaluating the effectiveness of chart representations, thereby struggling to identify the appropriate chart design to convey the intended data patterns. To address this issue, we propose a perception simulation model that can assess the perceptual effectiveness of charts by predicting graphical patterns that chart viewers are likely to notice. The perception simulation model integrates perceptual theory into visual feature extraction of chart elements to provide interpretable model outcomes. Human perceptual results proved that the outcome of our model can simulate the perceptual grouping behaviors of most chart viewers and cover diverse perceptual results. We also embed the model into a prototype interface called PatternSight to facilitate chart authors in assessing whether the chart design can satisfy their pattern representation requirements as expected and determining feasible improvements of visual design. According to the results of a user experiment, PatternSight can effectively assist chart authors in optimizing chart design for representing data patterns.

</details>


### [7] [Autonomy for Older Adult-Agent Interaction](https://arxiv.org/abs/2507.12767)

*Jiaxin An*

**Main category:** cs.HC

**Keywords:** AI agents, older adults, autonomy, caregiving, ethical implications

**Relevance Score:** 7

**TL;DR:** The paper explores AI-powered agents in caregiving for older adults and their alignment with autonomy preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in ensuring AI agents support older adults' autonomy in caregiving as the population ages.

**Method:** The paper examines four dimensions of autonomy: decision-making, goal-oriented, control, and social responsibility, and proposes research directions for enhancing agent autonomy.

**Key Contributions:**

	1. Proposes a framework for understanding autonomy in AI agents for older adults
	2. Identifies key dimensions of autonomy relevant to caregiving
	3. Suggests future research directions for operationalizing autonomy measures.

**Result:** Identified the need for new measures and frameworks to better align AI agents with the autonomy of older adults in caregiving roles.

**Limitations:** 

**Conclusion:** The paper highlights the necessity of addressing ethical and operational aspects of agent autonomy in communal caregiving settings.

**Abstract:** As the global population ages, artificial intelligence (AI)-powered agents have emerged as potential tools to support older adults' caregiving. Prior research has explored agent autonomy by identifying key interaction stages in task processes and defining the agent's role at each stage. However, ensuring that agents align with older adults' autonomy preferences remains a critical challenge. Drawing on interdisciplinary conceptualizations of autonomy, this paper examines four key dimensions of autonomy for older adults: decision-making autonomy, goal-oriented autonomy, control autonomy, and social responsibility autonomy. This paper then proposes the following research directions: (1) Addressing social responsibility autonomy, which concerns the ethical and social implications of agent use in communal settings; (2) Operationalizing agent autonomy from the task perspective; and (3) Developing autonomy measures.

</details>


### [8] [Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication](https://arxiv.org/abs/2507.13052)

*Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Robotics, Ultrasound, Virtual Agent

**Relevance Score:** 9

**TL;DR:** This paper introduces an intelligent virtual sonographer (IVS) that enhances physician-robot-patient interactions during robotic ultrasound procedures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the communication gap between physicians, robotic systems, and patients in robotic ultrasound applications.

**Method:** The study develops a conversational virtual agent within Extended Reality (XR) that employs LLM-powered dialogue, including speech-to-text and text-to-speech capabilities, to facilitate real-time communication and robotic control.

**Key Contributions:**

	1. Development of an intelligent virtual sonographer for robotic ultrasound
	2. Integration of LLM-powered dialogue with robotic control
	3. Improvement in physician-robot-patient communication dynamics

**Result:** The system improves the efficiency and accessibility of robotic ultrasound acquisition, leading to better patient experiences and increased trust in physician-robot interactions.

**Limitations:** 

**Conclusion:** The introduction of an IVS paves the way for more effective communication in medical procedures involving robotics, enhancing both usability for physicians and reassurance for patients.

**Abstract:** The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound.

</details>


### [9] ["What do you expect? You're part of the internet": Analyzing Celebrities' Experiences as Usees of Deepfake Technology](https://arxiv.org/abs/2507.13065)

*John Twomey, Sarah Foley, Sarah Robinson, Michael Quayle, Matthew Peter Aylett, Conor Linehan, Gillian Murphy*

**Main category:** cs.HC

**Keywords:** deepfake, non-consensual, synthetic imagery, human-computer interaction, celebrity

**Relevance Score:** 6

**TL;DR:** The paper explores the experiences of celebrity victims of non-consensual synthetic intimate imagery (NSII) and identifies social and infrastructural barriers to recourse.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the harms faced by celebrities targeted by deepfakes and the challenges in seeking recourse.

**Method:** Critical Discursive Psychological analysis of public statements by eight celebrity women and one non-binary individual who were targeted by NSII.

**Key Contributions:**

	1. Adoption of Baumers concept of Usees to analyze celebrity experiences with deepfakes
	2. Identification of infrastructural and social obstacles to recourse
	3. Implications for HCI in improving responses to NSII

**Result:** Celebrities articulated the distress caused by non-consensual deepfakes and identified social/infrastructural factors that hinder their activism and recourse efforts.

**Limitations:** 

**Conclusion:** The study highlights the need for improved HCI interventions and challenges false online beliefs that facilitate deepfake abuse.

**Abstract:** Deepfake technology is often used to create non-consensual synthetic intimate imagery (NSII), mainly of celebrity women. Through Critical Discursive Psychological analysis we ask; i) how celebrities construct being targeted by deepfakes and ii) how they navigate infrastructural and social obstacles when seeking recourse. In this paper, we adopt Baumers concept of Usees (stakeholders who are non-consenting, unaware and directly targeted by technology), to understand public statements made by eight celebrity women and one non-binary individual targeted with NSII. Celebrities describe harms of being non-consensually targeted by deepfakes and the distress of becoming aware of these videos. They describe various infrastructural/social factors (e.g. blaming/ silencing narratives and the industry behind deepfake abuse) which hinder activism and recourse. This work has implications in recognizing the roles of various stakeholders in the infrastructures underlying deepfake abuse and the potential of human-computer interaction to improve existing recourses for NSII. We also contribute to understanding how false beliefs online facilitate deepfake abuse. Future work should involve interventions which challenge the values and false beliefs which motivate NSII creation/dissemination.

</details>


### [10] [On tangible user interfaces, humans and spatiality](https://arxiv.org/abs/2507.13167)

*Ehud Sharlin, Benjamin Watson, Yoshifumi Kitamura, Fumio Kishino, Yuichi Itoh*

**Main category:** cs.HC

**Keywords:** Tangible User Interfaces, Spatiality, Human-Object Interaction, Heuristics, Application Design

**Relevance Score:** 4

**TL;DR:** This paper explores the role of spatiality in the design of tangible user interfaces (TUIs) and provides heuristics for their successful application.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how effective tangible user interfaces can be when they leverage human spatial skills with physical objects.

**Method:** The authors analyze past research on human-object interaction to develop heuristics related to spatiality in TUI design.

**Key Contributions:**

	1. Development of heuristics for TUI design based on spatiality.
	2. Classification of spatial TUIs that focus on shape, space, and structure.
	3. Analysis of existing spatial TUIs with respect to the proposed heuristics.

**Result:** The study identifies key heuristics for integrating spatiality into TUI applications, supported by an analysis of existing spatial TUIs.

**Limitations:** 

**Conclusion:** Incorporating spatiality into the design of TUIs can enhance their effectiveness and user experience.

**Abstract:** Like the prehistoric twig and stone, tangible user interfaces (TUIs) are objects manipulated by humans. TUI success will depend on how well they exploit spatiality, the intuitive spatial skills humans have with the objects they use. In this paper we carefully examine the relationship between humans and physical objects, and related previous research. From this examination we distill a set of observations, and turn these into heuristics for incorporation of spatiality into TUI application design, a cornerstone for their success. Following this line of thought, we identify spatial TUIs, the subset of TUIs that mediate interaction with shape, space and structure. We then examine several existing spatial TUIs using our heuristics.

</details>


### [11] [Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item](https://arxiv.org/abs/2507.13235)

*Minghao Cai, Guher Gorgun, Carrie Demmans Epp*

**Main category:** cs.HC

**Keywords:** Cognitive Load, Item Difficulty, Educational Technology, Learning Games, Item-Response Theory

**Relevance Score:** 4

**TL;DR:** This study explores using item difficulty parameters as a proxy for measuring cognitive load in online learning environments, suggesting that item difficulty can represent intrinsic cognitive load.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To find an objective method for measuring cognitive load in educational tasks, as traditional self-report measures are criticized for their subjectivity.

**Method:** The study investigated item difficulty parameters derived from item-response theory to assess their consistency with cognitive load theories in a learning platform.

**Key Contributions:**

	1. Proposes a novel method for measuring cognitive load using item difficulty parameters.
	2. Demonstrates the connection between item difficulty and cognitive load theories.
	3. Suggests practical implications for designing online educational tools.

**Result:** Item difficulty values aligned with models of intrinsic and extraneous load, indicating their potential as proxies for cognitive load in learning games.

**Limitations:** The study focuses on item difficulty in a specific online learning platform, limiting generalizability across different educational contexts.

**Conclusion:** Using item difficulty parameters can effectively represent intrinsic cognitive load, offering a more objective measurement approach.

**Abstract:** Cognitive load is key to ensuring an optimal learning experience. However, measuring the cognitive load of educational tasks typically relies on self-report measures which has been criticized by researchers for being subjective. In this study, we investigated the feasibility of using item difficulty parameters as a proxy for measuring cognitive load in an online learning platform. Difficulty values that were derived using item-response theory were consistent with theories of how intrinsic and extraneous load contribute to cognitive load. This finding suggests that we can use item difficulty to represent intrinsic load when modelling cognitive load in learning games.

</details>


### [12] [RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality](https://arxiv.org/abs/2507.13247)

*Ruohao Li, Jiawei Li, Jia Sun, Zhiqing Wu, Zisu Li, Ziyan Wang, Ge Lin Kan, Mingming Fan*

**Main category:** cs.HC

**Keywords:** virtual reality, reminiscence, AI, older adults, user study

**Relevance Score:** 7

**TL;DR:** RemVerse is an AI-powered VR prototype designed to aid reminiscence activities for older adults by using generative models and interactive dialogues to enhance memory recall and engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of facilitating effective reminiscence activities for older adults who are losing familiar environments due to urbanization.

**Method:** Developed an AI-empowered VR prototype, RemVerse, that incorporates generative models and an AI agent to enhance reminiscence by providing visual cues and interactive conversations.

**Key Contributions:**

	1. Development of an AI-enhanced VR prototype for reminiscence activities
	2. Insights from a user study demonstrating effectiveness for older adults
	3. Design implications for improving accessibility and engagement in reminiscence activities

**Result:** The user study with 14 older adults indicated that RemVerse effectively supported reminiscence activities by enhancing memory recall, engagement, and autonomy.

**Limitations:** 

**Conclusion:** Design implications were proposed to enhance accessibility and engagement in AI-assisted VR reminiscence activities for older adults.

**Abstract:** Reminiscence activities, which involve recalling and sharing past experiences, have proven beneficial for improving cognitive function, mood, and overall well-being. However, urbanization has led to the disappearance of familiar environments, removing visual and audio cues for effective reminiscence. While old photos can serve as visual cues to aid reminiscence, it is challenging for people to reconstruct the reminisced content and environment that are not in the photos. Virtual reality (VR) and artificial intelligence (AI) offer the ability to reconstruct an immersive environment with dynamic content and to converse with people to help them gradually reminisce. We designed RemVerse, an AI-empowered VR prototype aimed to support reminiscence activities. Integrating generative models and AI agent into a VR environment, RemVerse helps older adults reminisce with AI-generated visual cues and interactive dialogues. Our user study with 14 older adults showed that RemVerse effectively supported reminiscence activities by triggering, concretizing, and deepening personal memories, while fostering increased engagement and autonomy among older adults. Based on our findings, we proposed design implications to make reminiscence activities in AI-assisted VR more accessible and engaging for older adults.

</details>


### [13] [FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD](https://arxiv.org/abs/2507.13309)

*Hanxiu 'Hazel' Zhu, Ruijia Chen, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** ADHD, video customization, viewability, distractions, interface design

**Relevance Score:** 7

**TL;DR:** FocusView is a video customization interface designed for individuals with ADHD that helps improve video viewability by reducing distractions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Individuals with ADHD struggle with attention challenges when watching informational videos due to distractions caused by dynamic video elements.

**Method:** FocusView was evaluated with 12 participants with ADHD to assess its effectiveness in improving video viewability and understanding distractions.

**Key Contributions:**

	1. Introduction of FocusView for ADHD individuals
	2. Significant improvement in video viewability
	3. Identification of unique ADHD-relevant customization needs

**Result:** The study found that FocusView significantly improved video viewability by reducing distractions, revealing diverse perceptions of video elements among participants.

**Limitations:** Study limited to 12 participants; results may not generalize to all ADHD individuals.

**Conclusion:** The findings highlight the need for tailored design considerations in video customization systems for individuals with ADHD.

**Abstract:** While videos have become increasingly prevalent in delivering information across different educational and professional contexts, individuals with ADHD often face attention challenges when watching informational videos due to the dynamic, multimodal, yet potentially distracting video elements. To understand and address this critical challenge, we designed \textit{FocusView}, a video customization interface that allows viewers with ADHD to customize informational videos from different aspects. We evaluated FocusView with 12 participants with ADHD and found that FocusView significantly improved the viewability of videos by reducing distractions. Through the study, we uncovered participants' diverse perceptions of video distractions (e.g., background music as a distraction vs. stimulation boost) and their customization preferences, highlighting unique ADHD-relevant needs in designing video customization interfaces (e.g., reducing the number of options to avoid distraction caused by customization itself). We further derived design considerations for future video customization systems for the ADHD community.

</details>


### [14] [A Design Space for Multiscale Visualization](https://arxiv.org/abs/2404.01485)

*Mara Solen, Matt Oddo, Tamara Munzner*

**Main category:** cs.HC

**Keywords:** multiscale visualizations, design space, visualization strategies, data visualization, HCI

**Relevance Score:** 4

**TL;DR:** This paper presents a design space for creating multiscale visualizations, categorizing existing approaches, and identifying potential improvements through static and generative analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of designing effective multiscale visualizations with a large scale ratio, harnessing a structured design framework.

**Method:** A design space comprising three main dimensions and eight subdimensions was developed and used to categorize 52 different visualization examples from practitioners and academics, leading to the identification of strategies and gaps.

**Key Contributions:**

	1. Introduction of a structured design space for multiscale visualizations.
	2. Categorization of 52 existing designs into four strategic approaches.
	3. Identification of design gaps and missed opportunities in current multiscale visualization methods.

**Result:** The analysis partitioned the examples into four high-level strategies for multiscale visualization design and revealed missed opportunities for improving existing designs based on the design space.

**Limitations:** 

**Conclusion:** The study discusses patterns in dimension choices and strategies employed in various visualization contexts, highlighting the importance of a structured approach to support better design decisions.

**Abstract:** Designing multiscale visualizations, particularly when the ratio between the largest scale and the smallest item is large, can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with multiple scales. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 52 examples, created by a mix of academics and practitioners. We demonstrate descriptive power by analyzing and partitioning these examples into four high-level strategies for designing multiscale visualizations, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. We discuss patterns in the use of different dimension and strategy choices in the different visualization contexts of analysis and presentation.   Supplemental materials: https://osf.io/wbrdm/   Design space website: https://marasolen.github.io/multiscale-vis-ds/

</details>


### [15] [Characterizing Collective Efforts in Content Sharing and Quality Control for ADHD-relevant Content on Video-sharing Platforms](https://arxiv.org/abs/2501.13020)

*Hanxiu 'Hazel' Zhu, Avanthika Senthil Kumar, Sihang Zhao, Ru Wang, Xin Tong, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** ADHD, Video-sharing platforms, Accessibility, Content quality, Human-Computer Interaction

**Relevance Score:** 6

**TL;DR:** This study analyzes ADHD-relevant videos on VSPs, identifying quality and accessibility issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research on video content quality and accessibility issues for users with ADHD on video-sharing platforms.

**Method:** Systematic collection and analysis of 373 ADHD-relevant videos and comments from YouTube and TikTok using mixed methods.

**Key Contributions:**

	1. Systematic analysis of ADHD-related video content on VSPs
	2. Identification of key quality and accessibility challenges
	3. Actionable design implications for improving VSPs for ADHD users

**Result:** Identified characteristics of ADHD-relevant videos and collective creator-viewer efforts for video quality control.

**Limitations:** 

**Conclusion:** Developed actionable design implications for VSPs to enhance reliability and ADHD-friendliness of content.

**Abstract:** Video-sharing platforms (VSPs) have become increasingly important for individuals with ADHD to recognize symptoms, acquire knowledge, and receive support. While videos offer rich information and high engagement, they also present unique challenges, such as information quality and accessibility issues to users with ADHD. However, little work has thoroughly examined the video content quality and accessibility issues, the impact, and the control strategies in the ADHD community. We fill this gap by systematically collecting 373 ADHD-relevant videos with comments from YouTube and TikTok and analyzing the data with a mixed method. Our study identified the characteristics of ADHD-relevant videos on VSPs (e.g., creator types, video presentation forms, quality issues) and revealed the collective efforts of creators and viewers in video quality control, such as authority building, collective quality checking, and accessibility improvement. We further derive actionable design implications for VSPs to offer more reliable and ADHD-friendly contents.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)

*Lionel Wong, Katherine M. Collins, Lance Ying, Cedegao E. Zhang, Adrian Weller, Tobias Gersternberg, Timothy O'Donnell, Alexander K. Lew, Jacob D. Andreas, Joshua B. Tenenbaum, Tyler Brooke-Wilson*

**Main category:** cs.CL

**Keywords:** Model Synthesis Architecture, human reasoning, language models, open-ended reasoning, probabilistic programming

**Relevance Score:** 7

**TL;DR:** This paper proposes the Model Synthesis Architecture (MSA) as a computational implementation of how humans use combined distributed and symbolic representations to reason in novel situations. It evaluates MSA using a unique dataset based on sports vignettes, demonstrating its superior performance over standard language models in capturing human-like reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how people draw on global background knowledge and reason coherently in novel situations.

**Method:** The paper introduces the Model Synthesis Architecture (MSA) that combines language models for global relevance-based retrieval and probabilistic programs for coherent world model synthesis. It evaluates this model against human judgments using a novel reasoning dataset focusing on sports scenarios.

**Key Contributions:**

	1. Introduction of the Model Synthesis Architecture (MSA) for reasoning
	2. Evaluation against a novel reasoning dataset related to sports
	3. Demonstration of MSA's effectiveness over conventional language models

**Result:** The MSA captures human judgments more effectively than language model-only baselines, indicating that it can mirror human reasoning capabilities in open-ended scenarios.

**Limitations:** 

**Conclusion:** The MSA approach offers insights into replicating human reasoning in complex domains, suggesting a feasible computational framework for better understanding human cognitive processes.

**Abstract:** When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.

</details>


### [17] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)

*Michael A. Lepori, Jennifer Hu, Ishita Dasgupta, Roma Patel, Thomas Serre, Ellie Pavlick*

**Main category:** cs.CL

**Keywords:** language models, modal categorization, mechanistic interpretability, human categorization, NLP

**Relevance Score:** 8

**TL;DR:** This paper explores how language models discern modal categories of sentences and identifies reliable linear representations, termed modal difference vectors, that demonstrate consistent patterns as model competence increases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses concerns regarding language models' ability to categorize sentences based on modality, which has implications for their application in various tasks such as question answering and narrative construction.

**Method:** The authors identify and analyze modal difference vectors in language model activations and correlate these with human categorization behaviors through interpretability techniques.

**Key Contributions:**

	1. Identification of modal difference vectors in language models
	2. Demonstration of consistent emergence of modal categorization as models improve
	3. Correlation of model activations with human categorization behavior

**Result:** The findings reveal that language models have more reliable modalities than previously reported and that the emergence of modal difference vectors correlates with model competence, effectively modeling human categorization behaviors.

**Limitations:** 

**Conclusion:** The insights drawn from the analysis of modal difference vectors could enhance our understanding of both language models and human modal categorization, with potential applications in improving language model performance.

**Abstract:** Language models (LMs) are used for a diverse range of tasks, from question answering to writing fantastical stories. In order to reliably accomplish these tasks, LMs must be able to discern the modal category of a sentence (i.e., whether it describes something that is possible, impossible, completely nonsensical, etc.). However, recent studies have called into question the ability of LMs to categorize sentences according to modality (Michaelov et al., 2025; Kauf et al., 2023). In this work, we identify linear representations that discriminate between modal categories within a variety of LMs, or modal difference vectors. Analysis of modal difference vectors reveals that LMs have access to more reliable modal categorization judgments than previously reported. Furthermore, we find that modal difference vectors emerge in a consistent order as models become more competent (i.e., through training steps, layers, and parameter count). Notably, we find that modal difference vectors identified within LM activations can be used to model fine-grained human categorization behavior. This potentially provides a novel view into how human participants distinguish between modal categories, which we explore by correlating projections along modal difference vectors with human participants' ratings of interpretable features. In summary, we derive new insights into LM modal categorization using techniques from mechanistic interpretability, with the potential to inform our understanding of modal categorization in humans.

</details>


### [18] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)

*Abu-Viskhan A. Umishov, Vladislav A. Grigorian*

**Main category:** cs.CL

**Keywords:** Chechen language, translation model, multilingual translation, NLLB-200, language resources

**Relevance Score:** 4

**TL;DR:** This paper presents an open-source translation model for the Chechen language and Russian, including a training dataset and evaluation metrics.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To create a translation model for the Chechen language, which is under-resourced, and to support multilingual capabilities in the NLLB-200 system.

**Method:** The authors developed a translation model and fine-tuned it using collected parallel datasets for Chechen and Russian.

**Key Contributions:**

	1. First open-source translation model for the Chechen language and Russian.
	2. Creation and provision of a dataset for model training and evaluation.
	3. Release of parallel corpora and a multilingual sentence encoder for Chechen.

**Result:** The model achieved BLEU scores of 8.34 (Russian to Chechen) and 20.89 (Chechen to Russian), indicating its efficacy in translation tasks.

**Limitations:** The low BLEU scores suggest room for improvement in translation quality.

**Conclusion:** The paper highlights the need for multilingual translation systems and provides resources for further research on the Chechen language.

**Abstract:** We introduce the first open-source model for translation between the vulnerable Chechen language and Russian, and the dataset collected to train and evaluate it. We explore fine-tuning capabilities for including a new language into a large language model system for multilingual translation NLLB-200. The BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for translation from Russian to Chechen and reverse direction, respectively. The release of the translation models is accompanied by the distribution of parallel words, phrases and sentences corpora and multilingual sentence encoder adapted to the Chechen language.

</details>


### [19] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)

*Arthur J. Funnell, Panayiotis Petousis, Fabrice Harel-Canada, Ruby Romero, Alex A. T. Bui, Adam Koncsol, Hritika Chaturvedi, Chelsea Shover, David Goodman-Meza*

**Main category:** cs.CL

**Keywords:** NLP, overdose surveillance, BioClinicalBERT, drug-related deaths, health informatics

**Relevance Score:** 9

**TL;DR:** This paper explores the use of NLP models, specifically fine-tuned BioClinicalBERT, to automate the classification of drug-related deaths from unstructured coroner reports, addressing issues related to traditional ICD-10 coding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the timeliness and accuracy of overdose surveillance, especially given the increasing drug-related deaths linked to fentanyl in the US.

**Method:** The study employed a dataset of 35,433 death records for training and internal testing of various NLP approaches, including traditional classifiers and fine-tuned models like BioClinicalBERT. External validation used a separate dataset of 3,335 records.

**Key Contributions:**

	1. Demonstration of NLP's effectiveness in interpreting unstructured coroner reports for overdose classification
	2. Creation of a novel dataset for external validation of NLP models
	3. Comparison of various NLP approaches, highlighting the superiority of fine-tuned clinical models

**Result:** Fine-tuned BioClinicalBERT models achieved exceptional performance on both internal (F1 >= 0.998) and external validation (F1 = 0.966), outperforming traditional machine learning methods and other large language models.

**Limitations:** The study primarily focuses on specific NLP models and may not generalize to all forms of death reporting or overdose data classification.

**Conclusion:** NLP models, particularly BioClinicalBERT, provide a highly accurate and scalable method for classifying overdose deaths from free-text reports, enhancing surveillance workflows and enabling faster detection of substance use trends.

**Abstract:** The rising rate of drug-related deaths in the United States, largely driven by fentanyl, requires timely and accurate surveillance. However, critical overdose data are often buried in free-text coroner reports, leading to delays and information loss when coded into ICD (International Classification of Disease)-10 classifications. Natural language processing (NLP) models may automate and enhance overdose surveillance, but prior applications have been limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in 2020 was used for model training and internal testing. External validation was conducted using a novel separate dataset of 3,335 records from 2023-2024. Multiple NLP approaches were evaluated for classifying specific drug involvement from unstructured death certificate text. These included traditional single- and multi-label classifiers, as well as fine-tuned encoder-only language models such as Bidirectional Encoder Representations from Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large language models such as Qwen 3 and Llama 3. Model performance was assessed using macro-averaged F1 scores, and 95% confidence intervals were calculated to quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect performance, with macro F1 scores >=0.998 on the internal test set. External validation confirmed robustness (macro F1=0.966), outperforming conventional machine learning, general-domain BERT models, and various decoder-only large language models. NLP models, particularly fine-tuned clinical variants like BioClinicalBERT, offer a highly accurate and scalable solution for overdose death classification from free-text reports. These methods can significantly accelerate surveillance workflows, overcoming the limitations of manual ICD-10 coding and supporting near real-time detection of emerging substance use trends.

</details>


### [20] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)

*S M Rafiuddin, Sadia Kamal, Mohammed Rakib, Arunkumar Bagavathi, Atriya Sen*

**Main category:** cs.CL

**Keywords:** Multimodal Sentiment Analysis, Aspect Extraction, Adaptive Attention

**Relevance Score:** 7

**TL;DR:** AdaptiSent is a new framework for Multimodal Aspect-Based Sentiment Analysis that enhances sentiment classification by using adaptive cross-modal attention mechanisms and demonstrates superior performance on Twitter datasets.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve sentiment classification and aspect term extraction from multimodal data, particularly integrating text and images.

**Method:** The framework employs adaptive cross-modal attention with dynamic modality weighting and context-adaptive attention mechanisms to enhance sentiment and aspect extraction.

**Key Contributions:**

	1. Introduction of adaptive cross-modal attention mechanisms for sentiment analysis.
	2. Dynamic modality weighting that improves aspect extraction.
	3. Enhanced performance on nuanced sentiment analysis tasks over existing models.

**Result:** AdaptiSent outperformed traditional and other multimodal methods in precision, recall, and F1 score on standard Twitter datasets, particularly excelling at identifying complex inter-modal relationships.

**Limitations:** 

**Conclusion:** The model sets a new standard in MABSA, showing significant improvements in analyzing multimodal information.

**Abstract:** We introduce AdaptiSent, a new framework for Multimodal Aspect-Based Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms to improve sentiment classification and aspect term extraction from both text and images. Our model integrates dynamic modality weighting and context-adaptive attention, enhancing the extraction of sentiment and aspect-related information by focusing on how textual cues and visual context interact. We tested our approach against several baselines, including traditional text-based models and other multimodal methods. Results from standard Twitter datasets show that AdaptiSent surpasses existing models in precision, recall, and F1 score, and is particularly effective in identifying nuanced inter-modal relationships that are crucial for accurate sentiment and aspect term extraction. This effectiveness comes from the model's ability to adjust its focus dynamically based on the context's relevance, improving the depth and accuracy of sentiment analysis across various multimodal data sets. AdaptiSent sets a new standard for MABSA, significantly outperforming current methods, especially in understanding complex multimodal information.

</details>


### [21] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)

*Potsawee Manakul, Woody Haosheng Gan, Michael J. Ryan, Ali Sartaz Khan, Warit Sirichotedumrong, Kunat Pipatanakul, William Held, Diyi Yang*

**Main category:** cs.CL

**Keywords:** Audio Evaluation, Large Audio Model, Human Preference Simulation, Speech Assessment, Prompt Engineering

**Relevance Score:** 6

**TL;DR:** This paper introduces AudioJudge, a Large Audio Model-based system for unified audio evaluation, addressing limitations in current speech evaluation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for specialized systems in speech evaluation and the poor correlation between automatic methods and human preferences motivates the investigation of AudioJudge as a unified evaluation framework.

**Method:** A systematic study of AudioJudge is conducted, focusing on audio characteristic detection tasks such as pronunciation and speech quality, using various prompt engineering strategies and a multi-aspect ensemble approach for comprehensive evaluation.

**Key Contributions:**

	1. Introduction of AudioJudge for unified audio evaluation
	2. Successful implementation of multi-aspect ensemble approach
	3. Significant correlation with human preferences in speech assessment.

**Result:** AudioJudge achieves a Spearman correlation of up to 0.91 with human preferences on benchmark tasks, indicating significant improvement in performance and versatility across multiple audio characteristics.

**Limitations:** LAMs show significant verbosity and positional biases that need careful mitigation.

**Conclusion:** The proposed method provides an effective solution for audio evaluation but highlights the need to address verbosity and positional biases in LAM models.

**Abstract:** Current speech evaluation suffers from two critical limitations: the need and difficulty of designing specialized systems targeting individual audio characteristics, and poor correlation between automatic evaluation methods and human preferences. This work presents a systematic study of Large Audio Model (LAM) as a Judge, AudioJudge, investigating whether it can provide a unified evaluation framework that addresses both challenges. We systematically explore AudioJudge across audio characteristic detection tasks, including pronunciation, speaking rate, speaker identification and speech quality, and system-level human preference simulation for automated benchmarking. We investigate different prompt engineering strategies, finding that audio concatenation combined with in-context learning significantly improves performance across both audio characteristic detection and human preference simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to enable general-purpose multi-aspect audio evaluation. This method decomposes speech assessment into specialized judges for lexical content, speech quality, and paralinguistic features, achieving up to 0.91 Spearman correlation with human preferences on our system ranking benchmark. Robustness analysis reveals that while LAMs maintain strong performance under acoustic noise, they exhibit significant verbosity and positional biases that require careful mitigation.

</details>


### [22] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)

*Abraham Toluase Owodunni, Orevaoghene Ahia, Sachin Kumar*

**Main category:** cs.CL

**Keywords:** language models, adaptive tokenization, FLEXITOKENS

**Relevance Score:** 8

**TL;DR:** This paper introduces FLEXITOKENS, a byte-level language model with a learnable tokenizer that adapts to new data distributions, reducing token over-fragmentation and improving performance on various benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency of existing subword tokenizers that hinder the adaptation of language models to new or diverse data distributions.

**Method:** FLEXITOKENS employs a boundary predictor that learns to create variable-length segments from input byte sequences, enhancing flexibility without enforcing a fixed compression rate.

**Key Contributions:**

	1. Introduction of FLEXITOKENS for adaptive tokenization
	2. Demonstration of improved performance on multilingual tasks
	3. Reduction of token over-fragmentation through variable-length segment encoding

**Result:** FLEXITOKENS consistently reduces token over-fragmentation and shows up to 10% improvement in performance on multilingual benchmarks compared to traditional subword tokenizers.

**Limitations:** 

**Conclusion:** The approach demonstrates significant enhancements in adaptability of language models to diverse languages and tasks, promoting better performance and reduced fragmentation in tokenization.

**Abstract:** Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens

</details>


### [23] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)

*Richard Sproat, Tianyu Zhao, Llion Jones*

**Main category:** cs.CL

**Keywords:** Translation Evaluation, Machine Translation, Natural Language Processing, Language Models, Position Bias

**Relevance Score:** 7

**TL;DR:** TransEvalnia is a prompting-based translation evaluation system that provides finetuned scoring and ranking of translations, outperforming state-of-the-art methods on various language pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and granularity of translation evaluation using reasoning and fine-grained metrics.

**Method:** TransEvalnia employs prompt-based evaluation, utilizing a subset of the Multidimensional Quality Metrics and leveraging LLMs like Claude-3.5-Sonnet for assessing translations across multiple dimensions.

**Key Contributions:**

	1. Introduction of a novel prompting-based evaluation system for translations
	2. Demonstrated better or equivalent performance compared to the MT-Ranker
	3. Addressing position bias in translation evaluations.

**Result:** TransEvalnia matches or surpasses the performance of the MT-Ranker on English-Japanese and other WMT datasets, with high correlation of LLM scores to human assessments.

**Limitations:** Sensitivity to the order of presented translations.

**Conclusion:** The system is sensitive to translation presentation order, which can affect evaluations, and proposes solutions for this bias while releasing all data and code for public use.

**Abstract:** We present TransEvalnia, a prompting-based translation evaluation and ranking system that uses reasoning in performing its evaluations and ranking. This system presents fine-grained evaluations based on a subset of the Multidimensional Quality Metrics (https://themqm.org/), returns an assessment of which translation it deems the best, and provides numerical scores for the various dimensions and for the overall translation. We show that TransEvalnia performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al. 2024) on our own English-Japanese data as well as several language pairs from various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations returned are deemed highly acceptable to human raters, and that the scores assigned to the translations by Sonnet, as well as other LLMs, correlate well with scores assigned by the human raters. We also note the sensitivity of our system -- as well as MT-Ranker -- to the order in which the translations are presented, and we propose methods to address this position bias. All data, including the system's evaluation and reasoning, human assessments, as well as code is released.

</details>


### [24] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)

*Fuya Nakamori, Yin Jou Huang, Fei Cheng*

**Main category:** cs.CL

**Keywords:** Werewolf agents, strategy adaptation, game performance, contextual analysis, player interaction

**Relevance Score:** 4

**TL;DR:** This study introduces a method for Werewolf agents to dynamically adapt their strategies based on player attitudes and conversation context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the adaptability and performance of Werewolf agents in games by allowing them to switch strategies based on the situational context and player interactions.

**Method:** The proposed method involves the explicit selection of strategies for Werewolf agents that respond to the game context and perceived roles of other players, contrasting with previous approaches that utilized fixed or implicit strategies.

**Key Contributions:**

	1. Development of a dynamic strategy adaptation method for Werewolf agents
	2. Comparison with baseline agents leading to demonstrable performance improvements
	3. Insights into the impact of player attitudes on strategy selection

**Result:** Experimental comparisons demonstrate that strategy adaptation in Werewolf agents leads to improved performance compared to baseline agents with static strategies.

**Limitations:** 

**Conclusion:** The findings support the effectiveness of context-aware strategy selection for enhancing Werewolf agents' gameplay.

**Abstract:** This study proposes a method to improve the performance of Werewolf agents by switching between predefined strategies based on the attitudes of other players and the context of conversations. While prior works of Werewolf agents using prompt engineering have employed methods where effective strategies are implicitly defined, they cannot adapt to changing situations. In this research, we propose a method that explicitly selects an appropriate strategy based on the game context and the estimated roles of other players. We compare the strategy adaptation Werewolf agents with baseline agents using implicit or fixed strategies and verify the effectiveness of our proposed method.

</details>


### [25] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)

*Yunxiang Zhang, Muhammad Khalifa, Lechen Zhang, Xin Liu, Ayoung Lee, Xinliang Frederick Zhang, Farima Fatahi Bayat, Lu Wang*

**Main category:** cs.CL

**Keywords:** long reasoning, large reasoning models, logits arithmetic, preference optimization, thinking

**Relevance Score:** 7

**TL;DR:** This paper presents ThinkLogit, a decoding-time approach that enhances long reasoning abilities in large reasoning models without extensive training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if long reasoning behavior in large reasoning models can be elicited without additional training and to improve performance using a smaller guide model.

**Method:** The paper proposes a decoding-time approach called ThinkLogit that uses logits arithmetic to tune a larger model for long reasoning, guided by a substantially smaller model. It also introduces ThinkLogit-DPO, which trains the guider model using preference optimization over reasoning pairs.

**Key Contributions:**

	1. Introduction of ThinkLogit, a decoding-time approach for long reasoning
	2. Demonstration of improved performance via guided training with a smaller model
	3. Evidence that long reasoning skills can be transferred through reinforcement learning

**Result:** ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 scores of 26% and 29%, respectively, on four mathematical datasets, leveraging the smaller guider model for enhanced reasoning.

**Limitations:** 

**Conclusion:** The proposed methods demonstrate that it is possible to elicit long reasoning in large models efficiently with minimal training, providing significant performance improvements.

**Abstract:** Large reasoning models (LRMs) can do complex reasoning via long chain-of-thought (CoT) involving cognitive strategies such as backtracking and self-correction. Recent studies suggest that some models inherently possess these long reasoning abilities, which may be unlocked via extra training. Our work first investigates whether we can elicit such behavior without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for long reasoning using a substantially smaller model as guider. We then show that we can further boost performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model -- a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in pass@1 by 26% and 29%, respectively, over four mathematical datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model 21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills acquired through reinforcement learning, improving pass@1 by 13% relative compared to the Qwen2.5-32B base model. Our work presents a computationally-efficient method to elicit long reasoning in large models with minimal or no additional training.

</details>


### [26] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)

*Keli Zheng, Zerong Xie*

**Main category:** cs.CL

**Keywords:** language model, tokenization, byte-level, positional encodings, abstraction

**Relevance Score:** 7

**TL;DR:** This paper presents Synergy, a byte-level language model that improves tokenization efficiency and demonstrates better performance than existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a model that effectively bridges different levels of abstraction without relying heavily on traditional tokenization methods.

**Method:** The model is trained as a byte-level language model with a learned routing mechanism, evaluating performance against Llama3.

**Key Contributions:**

	1. Introduction of Synergy, a novel byte-level language model
	2. Demonstration of reduced concept tokens compared to BBPE
	3. Findings on position-independent concepts in higher abstraction layers

**Result:** Synergy produces fewer concept tokens than BBPE tokenizers while maintaining competitive performance; it shows advantages in specific model configurations.

**Limitations:** 

**Conclusion:** The research supports the viability of tokenizer-free architectures, which could lead to more robust AI pipelines.

**Abstract:** In this paper, we present Synergy, a language model that bridges different levels of abstraction in an end-to-end fashion through a learned routing mechanism. Focusing on low-level linguistic abstraction, we trained our model as a byte-level language model. Our model spontaneously learns to tokenize bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE) tokenizers while keeping comparable performance. By comparing with Llama3, we observed an advantage of Synergy under the same model scale and training dataset size. Further studies show that the middle part (the higher abstraction part) of our model performs better when positional encodings are removed, suggesting the emergence of position-independent concepts. These findings demonstrate the feasibility of tokenizer-free architectures, paving the way for more robust and flexible pipelines.

</details>


### [27] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)

*Thinh Hung Truong, Karin Verspoor, Trevor Cohn, Timothy Baldwin*

**Main category:** cs.CL

**Keywords:** negation, text encoders, contrastive learning, large language models, BERT

**Relevance Score:** 8

**TL;DR:** This paper proposes a strategy to enhance the negation robustness of text encoders through knowledge distillation from large language models, using contrastive learning to improve a BERT-based model's performance on negation tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequate handling of negation by current text encoders which affects the performance in various applications that rely on text embeddings.

**Method:** The authors use knowledge distillation from large language models to teach a BERT-based model diverse patterns of negation and hedging, followed by fine-tuning with a contrastive learning strategy.

**Key Contributions:**

	1. Proposed a novel distillation strategy for improving negation understanding in text encoders.
	2. Implemented a contrastive learning approach to fine-tune a BERT-based model.
	3. Showed adaptability of the method to large language models for better negation handling.

**Result:** The fine-tuned BERT-based model demonstrates significant improvements in understanding negation while maintaining strong performance on general text benchmarks.

**Limitations:** 

**Conclusion:** The proposed strategy successfully adapts to large language models as well, enhancing their performance in handling negation tasks.

**Abstract:** Despite rapid adoption of autoregressive large language models, smaller text encoders still play an important role in text understanding tasks that require rich contextualized representations. Negation is an important semantic function that is still not properly captured by such methods, affecting many downstream applications relying on text embeddings. We propose a strategy to improve negation robustness of text encoders, by distilling data from large language models using diverse patterns of negation and hedging. We adopt a standard contrastive learning strategy to finetune a strong BERT-based model, and observe large improvement in negation understanding capabilities while maintaining competitive performance on general benchmarks. In addition, we also show that our method can be adapted to LLMs, leading to improved performance on negation benchmarks.

</details>


### [28] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)

*Andrew Shin, Kunitake Kaneko*

**Main category:** cs.CL

**Keywords:** large language models, symbolic music, MIDI generation, genre classification, machine learning

**Relevance Score:** 5

**TL;DR:** This paper investigates how large language models (LLMs) represent and generate symbolic music data from textual descriptions, producing a dataset of MIDI files for musical classification and melody completion tasks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of large language models (LLMs) in modeling and generating symbolic music data and to understand their limitations in the musical context.

**Method:** The authors generated a dataset of MIDI files from textual prompts and trained neural networks on this LLM-generated dataset to perform genre classification and melody completion, comparing performance with established models.

**Key Contributions:**

	1. Creation of a new dataset of LLM-generated MIDI files
	2. Evaluation of neural networks trained on this dataset for music tasks
	3. Insights into LLMs‚Äô ability to model musical structures

**Result:** The findings indicate that while LLMs can infer basic musical structures and temporal relationships from text, they have limitations due to a lack of explicit musical training.

**Limitations:** LLMs lack explicit musical context, impacting their performance in more complex musical tasks.

**Conclusion:** The study enhances understanding of LLMs' generative capabilities in symbolic music, suggesting both potential and constraints in their musical encoding.

**Abstract:** Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.

</details>


### [29] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)

*Xi Ai, Mahardika Krisna Ihsani, Min-Yen Kan*

**Main category:** cs.CL

**Keywords:** cross-lingual consistency, multilingual models, factual knowledge, code-switching training, cross-lingual alignment

**Relevance Score:** 7

**TL;DR:** This paper analyzes cross-lingual consistency in knowledge transfer across languages, focusing on multilingual models' performance and their interpretability mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess cross-lingual transferability and maintain factual knowledge consistency across languages in multilingual models.

**Method:** The study examines code-mixed coreferential statements in multilingual contexts using interpretability approaches to analyze model behavior and consistency levels based on linguistic factors.

**Key Contributions:**

	1. Analysis of cross-lingual consistency for factual knowledge in multilingual models.
	2. Identification of bottlenecks in cross-lingual consistency related to linguistic factors.
	3. Evaluation of training strategies that improve both multilingual performance and cross-lingual consistency.

**Result:** Multilingual models show varying levels of cross-lingual consistency influenced by language families and specific bottlenecks; strategies like code-switching training and cross-lingual alignment showed promising improvements in consistency and performance.

**Limitations:** The study identifies that knowledge does not always equate to cross-lingual consistency.

**Conclusion:** Cross-lingual consistency is crucial for multilingual models, with specific training strategies proving effective in enhancing both performance and consistency across languages.

**Abstract:** Cross-lingual consistency should be considered to assess cross-lingual transferability, maintain the factuality of the model knowledge across languages, and preserve the parity of language model performance. We are thus interested in analyzing, evaluating, and interpreting cross-lingual consistency for factual knowledge. We examine code-mixed coreferential statements conveyed identical knowledge across languages to study cross-lingual knowledge consistency. We use some interpretability approaches to analyze the behavior of a model in cross-lingual contexts, discovering that multilingual models show different levels of consistency, subject to language families, linguistic factors, and a bottleneck in cross-lingual consistency on a particular layer. In addition, we evaluate common strategies aimed at improving multilingual performance to observe whether these strategies can improve knowledge consistency at the same time. While knowledge is not cross-lingual consistency in many cases, code-switching training and cross-lingual word alignment objectives show the most promising results, emphasizing the noteworthiness of cross-lingual alignment supervision and code-switching training for both multilingual performance and cross-lingual consistency enhancement.

</details>


### [30] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)

*Yihong Wang, Zhonglin Jiang, Ningyuan Xi, Yue Zhao, Qingqing Gu, Xiyuan Chen, Hao Wu, Sheng Xu, Hange Zhou, Yong Chen, Luo Ji*

**Main category:** cs.CL

**Keywords:** hierarchical decoder, language models, text classification, text generation, fine-tuning

**Relevance Score:** 7

**TL;DR:** This paper proposes a hierarchical decoder architecture for language models that decodes text using multiple layers simultaneously, leading to improved performance on various tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study is motivated by the desire to emulate human hierarchical thinking capabilities in language models.

**Method:** A pretrained language model is adapted by copying language heads from the last layer to selected intermediate layers which are then fine-tuned on different task inputs.

**Key Contributions:**

	1. Introduction of a hierarchical decoder architecture for language models
	2. Demonstration of improved performance on text classification and generation tasks
	3. Validation of meaningful outputs from intermediate layers

**Result:** The model exhibits meaningful output from intermediate layers and achieves state-of-the-art performance on tasks like hierarchical text classification and generation.

**Limitations:** 

**Conclusion:** The findings support the feasibility of developing a generalized hierarchical reasoner that could be pretrained from scratch.

**Abstract:** Decoder-only language models, such as GPT and LLaMA, generally decode on the last layer. Motivated by human's hierarchical thinking capability, we propose that a hierarchical decoder architecture could be built with different layers decoding texts simultaneously. Due to limited time and computationally resources, we choose to adapt a pretrained language model into this form of hierarchical decoder. Language heads of the last layer are copied to different selected intermediate layers, and fine-tuned with different task inputs. By thorough experiments, we validate that these selective intermediate layers could be adapted to speak meaningful and reasonable contents, and this paradigm of hierarchical decoder can obtain state-of-the-art performances on multiple tasks such as hierarchical text classification, classification-guided generation, and hierarchical text generation. This study suggests the possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [31] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)

*Maximiliano Hormaz√°bal Lagos, √Ålvaro Bueno S√°ez, H√©ctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro*

**Main category:** cs.CL

**Keywords:** LLM, Python code generation, tabular data, Spanish, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper discusses a method for answering questions about tables in Spanish using Python code generation with LLMs, achieving 85% accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of answering questions about tabular data in Spanish, enhancing the capabilities of LLMs in this domain.

**Method:** The solution involves analyzing table content, selecting relevant columns, generating natural language instructions, translating them to Python code, executing it, and managing errors, utilizing open-source LLMs and optimized prompts.

**Key Contributions:**

	1. Implementation of LLMs for code generation in answering tabular data questions in Spanish
	2. Achieved a high accuracy score of 85%
	3. Developed a multi-step process for table data interaction using LLMs

**Result:** The proposed method achieved an accuracy score of 85% in answering questions about tables in Spanish.

**Limitations:** 

**Conclusion:** The approach demonstrates the viability of using LLMs for effectively interacting with tabular data and suggests further exploration in contextual understanding of such data.

**Abstract:** This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in Spanish). Our solution obtains answers to the questions by implementing Python code generation with LLMs that is used to filter and process the table. This solution evolves from the MRT implementation for the Semeval 2025 related task. The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions. These steps use open-source LLMs and fine-grained optimized prompts for each step. With this approach, we achieved an accuracy score of 85\% in the task.

</details>


### [32] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)

*Quentin Goux, Nadira Lammari*

**Main category:** cs.CL

**Keywords:** cybersecurity, attack scenarios, UML, automation, training

**Relevance Score:** 4

**TL;DR:** This paper presents a formal model for cybersecurity automation focused on attack scenarios, aiding in attack simulation and training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Organizations need to automate cybersecurity processes due to a constantly evolving threat landscape.

**Method:** The paper introduces a UML-based formal model that describes attack contexts and scenarios.

**Key Contributions:**

	1. A formal UML class model for attack scenarios
	2. Use of model for automating attack script generation
	3. Application of model in upstream attack analysis

**Result:** The proposed model facilitates upstream attack analysis and enables automatic generation of attack scripts for training.

**Limitations:** 

**Conclusion:** The novel model can enhance cybersecurity training and attack analysis processes effectively.

**Abstract:** Organizations face an ever-changing threat landscape. They must continuously dedicate significant efforts to protect their assets, making their adoption of increased cybersecurity automation inevitable. However, process automation requires formalization of input data. Through this paper, we address this need for processes that use attack scenarios as input. Among these processes, one can mention both the generation of scripts for attack simulation and training purposes, as well as the analysis of attacks. Therefore, the paper's main research contribution is a novel formal model that encompasses the attack's context description and its scenario. It is abstracted using UML class model. Once the description of our model done, we will show how it could serve an upstream attack analysis process. We will show also its use for an automatic generation of attack scripts in the context of cybersecurity training. These two uses cases constitute the second contribution of this present research work.

</details>


### [33] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)

*Marc Brinner, Sina Zarriess*

**Main category:** cs.CL

**Keywords:** semantic embeddings, contrastive learning, scientific texts

**Relevance Score:** 8

**TL;DR:** Introduction of SemCSE, an unsupervised method for learning semantic embeddings of scientific texts utilizing LLM-generated summaries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve upon traditional citation-based approaches by capturing the true semantic content of scientific texts through unsupervised learning methods.

**Method:** SemCSE employs contrastive learning on LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space.

**Key Contributions:**

	1. Introduction of a novel unsupervised method for semantic embeddings (SemCSE)
	2. Proposing a new benchmark for evaluating semantic understanding in scientific texts
	3. Achieving state-of-the-art performance on the SciRepEval benchmark for scientific text embeddings.

**Result:** SemCSE demonstrates stronger semantic separation in the embedding space and achieves state-of-the-art performance on the SciRepEval benchmark for scientific text embeddings.

**Limitations:** 

**Conclusion:** The study validates that a semantically focused training approach enhances the ability to encode the semantic content of scientific texts effectively.

**Abstract:** We introduce SemCSE, an unsupervised method for learning semantic embeddings of scientific texts. Building on recent advances in contrastive learning for text embeddings, our approach leverages LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space. This resulting objective ensures that the model captures the true semantic content of a text, in contrast to traditional citation-based approaches that do not necessarily reflect semantic similarity. To validate this, we propose a novel benchmark designed to assess a model's ability to understand and encode the semantic content of scientific texts, demonstrating that our method enforces a stronger semantic separation within the embedding space. Additionally, we evaluate SemCSE on the comprehensive SciRepEval benchmark for scientific text embeddings, where it achieves state-of-the-art performance among models of its size, thus highlighting the benefits of a semantically focused training approach.

</details>


### [34] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)

*Jaya Caporusso, Matthew Purver, Senja Pollak*

**Main category:** cs.CL

**Keywords:** NLP, Self-aspects, Mental health, Ontology, Large language models

**Relevance Score:** 7

**TL;DR:** The paper proposes a computational framework to identify Self-aspects in text, focusing on mental health and phenomenology using NLP techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The Self is a multifaceted construct that is underexplored in NLP, though it aligns with important psychological phenomena, especially in mental health contexts.

**Method:** Develop an ontology of Self-aspects and a gold-standard annotated dataset; evaluate models including discriminative models, generative large language models, and embedding-based retrieval approaches based on interpretability, accuracy, and efficiency.

**Key Contributions:**

	1. Development of an ontology of Self-aspects
	2. Creation of a gold-standard annotated dataset
	3. Application of advanced NLP models in mental health and phenomenology

**Result:** The framework aims to enhance understanding and analysis of Self-aspects in language, providing tools for mental health research.

**Limitations:** 

**Conclusion:** By systematically analyzing Self-aspects through NLP, this research could significantly impact mental health studies and phenomenological research.

**Abstract:** This Ph.D. proposal introduces a plan to develop a computational framework to identify Self-aspects in text. The Self is a multifaceted construct and it is reflected in language. While it is described across disciplines like cognitive science and phenomenology, it remains underexplored in natural language processing (NLP). Many of the aspects of the Self align with psychological and other well-researched phenomena (e.g., those related to mental health), highlighting the need for systematic NLP-based analysis. In line with this, we plan to introduce an ontology of Self-aspects and a gold-standard annotated dataset. Using this foundation, we will develop and evaluate conventional discriminative models, generative large language models, and embedding-based retrieval approaches against four main criteria: interpretability, ground-truth adherence, accuracy, and computational efficiency. Top-performing models will be applied in case studies in mental health and empirical phenomenology.

</details>


### [35] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)

*Hadi Mohammadi, Tina Shahedi, Pablo Mosteiro, Massimo Poesio, Ayoub Bagheri, Anastasia Giachanou*

**Main category:** cs.CL

**Keywords:** NLP, fairness, Generative AI, annotator demographics, sexism detection

**Relevance Score:** 6

**TL;DR:** The study explores the impact of annotator demographics on labeling decisions in NLP tasks, finding content to be more influential than demographics. It evaluates the effectiveness of using Generative AI with demographic personas in improving human alignment, concluding that content-driven strategies are more reliable for fairness in NLP systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the influence of annotator demographic features on labeling decisions in NLP tasks like sexism detection, and to enhance fairness in NLP systems.

**Method:** The study utilizes a Generalized Linear Mixed Model to quantify the influence of demographic factors versus text content on annotation variability. It also assesses Generative AI models with persona prompting and evaluates performance against human judgments using explainable AI techniques.

**Key Contributions:**

	1. Quantification of the impact of demographic features on NLP labeling decisions.
	2. Evaluation of Generative AI models in the context of demographic persona prompting.
	3. Recommendations for improved fairness focusing on content-driven approaches.

**Result:** Demographic features account for only 8% of the observed variance in labeling decisions, with tweet content being the primary factor. Guiding Generative AI models with demographic personas does not significantly enhance performance; in some cases, it degrades it.

**Limitations:** The findings may not generalize across all NLP tasks or datasets, and the effectiveness of annotator demographics may vary in other contexts.

**Conclusion:** Focusing on content-driven explanations and robust annotation protocols is advocated as a more reliable approach for achieving fairness in NLP systems than relying on persona simulation.

**Abstract:** Understanding the sources of variability in annotations is crucial for developing fair NLP systems, especially for tasks like sexism detection where demographic bias is a concern. This study investigates the extent to which annotator demographic features influence labeling decisions compared to text content. Using a Generalized Linear Mixed Model, we quantify this inf luence, finding that while statistically present, demographic factors account for a minor fraction ( 8%) of the observed variance, with tweet content being the dominant factor. We then assess the reliability of Generative AI (GenAI) models as annotators, specifically evaluating if guiding them with demographic personas improves alignment with human judgments. Our results indicate that simplistic persona prompting often fails to enhance, and sometimes degrades, performance compared to baseline models. Furthermore, explainable AI (XAI) techniques reveal that model predictions rely heavily on content-specific tokens related to sexism, rather than correlates of demographic characteristics. We argue that focusing on content-driven explanations and robust annotation protocols offers a more reliable path towards fairness than potentially persona simulation.

</details>


### [36] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)

*Emma Sharratt, Annelien Smith, Retief Louw, Daleen Klop, Febe de Wet, Herman Kamper*

**Main category:** cs.CL

**Keywords:** oral narratives, literacy development, machine learning, multilingual, early assessment

**Relevance Score:** 4

**TL;DR:** The study analyzes oral narratives from young children to identify predictors of literacy development using machine learning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore features of oral narratives in children needing intervention and their relationship to literacy development.

**Method:** Simple machine learning methods to analyze recorded stories from four- and five-year-old Afrikaans- and isiXhosa-speaking children.

**Key Contributions:**

	1. Analysis of narrative skills in different languages
	2. Identification of key features predicting literacy development
	3. Implications for early assessment strategies in multilingual settings

**Result:** Identified lexical diversity and mean utterance length as key indicators of typical development, while articulation rate was less informative. Specific verbs and auxiliaries linked to storytelling success were also found.

**Limitations:** 

**Conclusion:** The findings highlight language-specific and common predictors of narrative proficiency, which could inform early assessment in multilingual contexts.

**Abstract:** Oral narrative skills are strong predictors of later literacy development. This study examines the features of oral narratives from children who were identified by experts as requiring intervention. Using simple machine learning methods, we analyse recorded stories from four- and five-year-old Afrikaans- and isiXhosa-speaking children. Consistent with prior research, we identify lexical diversity (unique words) and length-based features (mean utterance length) as indicators of typical development, but features like articulation rate prove less informative. Despite cross-linguistic variation in part-of-speech patterns, the use of specific verbs and auxiliaries associated with goal-directed storytelling is correlated with a reduced likelihood of requiring intervention. Our analysis of two linguistically distinct languages reveals both language-specific and shared predictors of narrative proficiency, with implications for early assessment in multilingual contexts.

</details>


### [37] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)

*Jisoo Lee, Raeyoung Chang, Dongwook Kwon, Harmanpreet Singh, Nikhil Verma*

**Main category:** cs.CL

**Keywords:** multi-agent systems, collaborative reasoning, evaluation metrics

**Relevance Score:** 8

**TL;DR:** Introduces GEMMAS, a graph-based framework for evaluating multi-agent systems in collaborative reasoning tasks, focusing on the quality of agent communication and coordination.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluations of multi-agent systems overlook the quality of communication and coordination, which can lead to inefficiencies and higher computational costs.

**Method:** GEMMAS models agent interactions as a directed acyclic graph and introduces two metrics: Information Diversity Score (IDS) and Unnecessary Path Ratio (UPR) for assessing collaboration quality.

**Key Contributions:**

	1. Introduction of GEMMAS framework for evaluating collaborative AI systems
	2. Development of IDS and UPR metrics for assessing collaboration quality
	3. Demonstration of substantial variations in multi-agent collaboration beyond outcome metrics

**Result:** Evaluation of GEMMAS across five benchmarks reveals significant differences in internal collaboration, with systems showing small accuracy differences exhibiting substantial variation in IDS and UPR metrics.

**Limitations:** 

**Conclusion:** Outcome-only metrics are insufficient for evaluating multi-agent performance; process-level diagnostics are crucial for designing interpretable and resource-efficient AI systems.

**Abstract:** Multi-agent systems built on language models have shown strong performance on collaborative reasoning tasks. However, existing evaluations focus only on the correctness of the final output, overlooking how inefficient communication and poor coordination contribute to redundant reasoning and higher computational costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes the internal collaboration process by modeling agent interactions as a directed acyclic graph. To capture collaboration quality, we propose two process-level metrics: Information Diversity Score (IDS) to measure semantic variation in inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant reasoning paths. We evaluate GEMMAS across five benchmarks and highlight results on GSM8K, where systems with only a 2.1% difference in accuracy differ by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal collaboration. These findings demonstrate that outcome-only metrics are insufficient for evaluating multi-agent performance and highlight the importance of process-level diagnostics in designing more interpretable and resource-efficient collaborative AI systems.

</details>


### [38] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)

*R. Louw, E. Sharratt, F. de Wet, C. Jacobs, A. Smith, H. Kamper*

**Main category:** cs.CL

**Keywords:** automated assessment, natural language processing, preschool education, machine learning, large language models

**Relevance Score:** 7

**TL;DR:** The paper presents a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa, utilizing machine learning and large language models for enhanced comprehension evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To aid preschool teachers in identifying students needing intervention in narrative and comprehension skills more effectively.

**Method:** The system employs automatic speech recognition and a machine learning scoring model, comparing a linear model to a large language model (LLM) for prediction accuracy.

**Key Contributions:**

	1. Development of an automatic assessment system for preschool children's narratives.
	2. Comparison of linear and LLM scoring models for educational purposes.
	3. Demonstration of LLM's effectiveness in identifying children requiring support.

**Result:** The LLM-based system generally outperforms the linear model, achieving performance comparable to human experts in identifying at-risk children.

**Limitations:** 

**Conclusion:** The proposed system lays the groundwork for automatic oral assessments, enabling teachers to provide personalized support more effectively.

**Abstract:** Developing narrative and comprehension skills in early childhood is critical for later literacy. However, teachers in large preschool classrooms struggle to accurately identify students who require intervention. We present a system for automatically assessing oral narratives of preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition followed by a machine learning scoring model to predict narrative and comprehension scores. For scoring predicted transcripts, we compare a linear model to a large language model (LLM). The LLM-based system outperforms the linear model in most cases, but the linear system is competitive despite its simplicity. The LLM-based system is comparable to a human expert in flagging children who require intervention. We lay the foundation for automatic oral assessments in classrooms, giving teachers extra capacity to focus on personalised support for children's learning.

</details>


### [39] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)

*Xinyu Tang, Zhihao Lv, Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, Jun Zhou*

**Main category:** cs.CL

**Keywords:** large language models, cross-task transfer, latent space steering, machine learning, low-resource tasks

**Relevance Score:** 9

**TL;DR:** The paper presents CAST, a novel framework for cross-task knowledge transfer in LLMs by manipulating activation states, which improves performance in data-scarce scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models (LLMs) in unseen tasks, especially in data-scarce environments, by exploring latent space steering for effective knowledge transfer.

**Method:** The proposed CAST framework selects influential samples from high-resource tasks and manipulates their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks without requiring parameter updates or input expansion.

**Key Contributions:**

	1. Development of a novel Cross-task Activation Steering Transfer framework (CAST)
	2. Analysis of activation patterns in the latent space of LLMs for robust transfer
	3. Demonstration of improved performance in data-scarce scenarios with lower computational overhead.

**Result:** CAST outperforms competitive baselines in cross-domain and cross-lingual transfer scenarios, demonstrating superior scalability and lower computational costs.

**Limitations:** 

**Conclusion:** The results support the feasibility of cross-task transfer using activation steering, paving the way for more efficient LLM applications in low-resource settings.

**Abstract:** Large language models (LLMs) have shown impressive abilities in leveraging pretrained knowledge through prompting, but they often struggle with unseen tasks, particularly in data-scarce scenarios. While cross-task in-context learning offers a direct solution for transferring knowledge across tasks, it still faces critical challenges in terms of robustness, scalability, and efficiency. In this paper, we investigate whether cross-task transfer can be achieved via latent space steering without parameter updates or input expansion. Through an analysis of activation patterns in the latent space of LLMs, we observe that the enhanced activations induced by in-context examples have consistent patterns across different tasks. Inspired by these findings, we propose CAST, a novel Cross-task Activation Steering Transfer framework that enables effective transfer by manipulating the model's internal activation states. Our approach first selects influential and diverse samples from high-resource tasks, then utilizes their contrastive representation-enhanced activations to adapt LLMs to low-resource tasks. Extensive experiments across both cross-domain and cross-lingual transfer settings show that our method outperforms competitive baselines and demonstrates superior scalability and lower computational costs.

</details>


### [40] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)

*Ashray Gupta, Rohan Joseph, Sunny Rai*

**Main category:** cs.CL

**Keywords:** Hindi Analogy Test Set, Large Language Models, Reasoning, Cognitive Theories, Multilingual Evaluation

**Relevance Score:** 7

**TL;DR:** Introduction of a new Hindi Analogy Test Set (HATS) to evaluate reasoning capabilities of large language models (LLMs) in Hindi.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in studying LLM reasoning abilities in Indic languages and to provide a resource for evaluating models' performance in Hindi.

**Method:** The authors developed a Hindi Analogy Test Set (HATS) comprising 405 multiple-choice questions from Indian government exams and employed various prompting strategies along with a grounded Chain of Thought approach.

**Key Contributions:**

	1. Creation of the Hindi Analogy Test Set (HATS) with 405 questions.
	2. Benchmarking state-of-the-art multilingual LLMs in Hindi.
	3. Introduction of a grounded Chain of Thought approach for improved reasoning ability.

**Result:** LLMs performed best with English prompts regardless of the prompting strategy, highlighting the challenges faced when using Hindi prompts.

**Limitations:** Models predominantly perform better with English prompts, indicating a potential area for improvement in multilingual capabilities.

**Conclusion:** The introduction of HATS provides a necessary resource for evaluating reasoning capabilities of LLMs in Hindi, revealing a performance gap when compared to English prompts.

**Abstract:** Analogies test a model's ability to infer implicit relationships between concepts, making them a key benchmark for evaluating reasoning capabilities. While large language models (LLMs) are widely evaluated for reasoning in English, their abilities in Indic languages remain understudied, limiting our understanding of whether these models generalize across languages. To address this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405 multiple-choice questions sourced from Indian government exams. We benchmark state-of-the-art multilingual LLMs using various prompting strategies and introduce a grounded Chain of Thought approach that leverages cognitive theories of analogical reasoning. This approach improves model performance on Hindi analogy questions. Our experiments show that models perform best with English prompts, irrespective of the prompting strategy. Our test set addresses the lack of a critical resource to evaluate LLM reasoning capabilities in Hindi.

</details>


### [41] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)

*Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, Shumin Deng*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, safety, AutoSteer, adversarial inputs, inference

**Relevance Score:** 8

**TL;DR:** AutoSteer is a new intervention technology for improving safety in Multimodal Large Language Models (MLLMs) during inference, using a Safety Awareness Score, adaptive safety prober, and a Refusal Head to handle safety risks without model fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recent advancements in MLLMs raise safety concerns with adversarial inputs; a need for a safety-improving method during inference is crucial.

**Method:** Develops AutoSteer, a modular system consisting of a Safety Awareness Score, an adaptive safety prober, and a Refusal Head for managing safety risks during language model output generation.

**Key Contributions:**

	1. Introduction of AutoSteer for adaptive interventions in MLLMs.
	2. Development of a Safety Awareness Score for identifying safety-relevant distinctions.
	3. Demonstrated reduction of attack success rates while preserving model capabilities.

**Result:** AutoSteer significantly lowers the Attack Success Rate for various adversarial inputs while maintaining the general capabilities of MLLMs, as demonstrated through experiments on diverse safety benchmarks.

**Limitations:** 

**Conclusion:** AutoSteer provides a practical and interpretable framework for enhancing the safety of multimodal AI systems without the need for model retraining.

**Abstract:** Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.

</details>


### [42] [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)

*Jiazheng Li, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Hongzhou Lin, Yi Wu, Jingzhao Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Multi-step reasoning

**Relevance Score:** 8

**TL;DR:** The paper presents QuestA, a reinforcement learning strategy that improves multi-step reasoning in large language models through question augmentation.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of reinforcement learning in enhancing multi-step reasoning capabilities in large language models, particularly on challenging problems.

**Method:** The proposed method introduces partial solutions during the training phase to reduce problem difficulty and enhance learning signals in reinforcement learning.

**Key Contributions:**

	1. Introduction of the QuestA method for question augmentation during RL training
	2. Demonstration of significant performance improvements on math reasoning benchmarks
	3. Theoretical explanations supporting increased sample efficiency

**Result:** QuestA significantly boosts performance in mathematical reasoning tasks, achieving new state-of-the-art results, with notable gains in metrics like pass@1 and pass@k on multiple benchmarks.

**Limitations:** 

**Conclusion:** QuestA offers a practical approach to enhance reasoning capabilities in LLMs using reinforcement learning, demonstrating improved sample efficiency and effectiveness on difficult tasks.

**Abstract:** Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.

</details>


### [43] [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)

*Luis Gasco, Hermenegildo Fabregat, Laura Garc√≠a-Sardi√±a, Paula Estrella, Daniel Deniz, Alvaro Rodrigo, Rabih Zbib*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Human Capital Management, Skill Prediction, Job Title Matching, Fairness in AI

**Relevance Score:** 7

**TL;DR:** TalentCLEF 2025 presents an evaluation campaign for skill and job title intelligence, focusing on multilingual job title matching and job title-based skill prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is an increasing need for reliable and fair language models in Human Capital Management to improve talent acquisition and workforce planning, but existing benchmarks and models have been lacking in this domain.

**Method:** The evaluation consists of two tasks: multilingual job title matching across several languages and skill prediction based on job titles, with datasets built from real job applications.

**Key Contributions:**

	1. First evaluation campaign for skill and job title intelligence.
	2. Two distinct tasks: multilingual job title matching and skill prediction.
	3. Public benchmark promoting fair and reliable language models.

**Result:** The campaign attracted 76 teams and produced over 280 submissions, revealing that training strategies significantly impact performance beyond just model size.

**Limitations:** The study may be limited to specific languages and job titles, potentially reducing its applicability to other contexts.

**Conclusion:** TalentCLEF provides the first public benchmark for skill and job title intelligence, fostering the development of effective language technologies for labor markets.

**Abstract:** Advances in natural language processing and large language models are driving a major transformation in Human Capital Management, with a growing interest in building smart systems based on language technologies for talent acquisition, upskilling strategies, and workforce planning. However, the adoption and progress of these technologies critically depend on the development of reliable and fair models, properly evaluated on public data and open benchmarks, which have so far been unavailable in this domain.   To address this gap, we present TalentCLEF 2025, the first evaluation campaign focused on skill and job title intelligence. The lab consists of two tasks: Task A - Multilingual Job Title Matching, covering English, Spanish, German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English. Both corpora were built from real job applications, carefully anonymized, and manually annotated to reflect the complexity and diversity of real-world labor market data, including linguistic variability and gender-marked expressions.   The evaluations included monolingual and cross-lingual scenarios and covered the evaluation of gender bias.   TalentCLEF attracted 76 registered teams with more than 280 submissions. Most systems relied on information retrieval techniques built with multilingual encoder-based models fine-tuned with contrastive learning, and several of them incorporated large language models for data augmentation or re-ranking. The results show that the training strategies have a larger effect than the size of the model alone. TalentCLEF provides the first public benchmark in this field and encourages the development of robust, fair, and transferable language technologies for the labor market.

</details>


### [44] [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)

*Wang Xi, Quan Shi, Tian Yu, Yujie Peng, Jiayi Sun, Mengxing Ren, Zenghui Ding, Ningguang Yao*

**Main category:** cs.CL

**Keywords:** media presentations, narrative planning, layout generation, quality optimization, automated evaluation

**Relevance Score:** 4

**TL;DR:** The paper presents RCPS, a framework for automated media presentation generation that enhances quality through narrative planning, layout generation, and optimization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automate the generation of high-quality media presentations to meet professional standards, addressing challenges in content extraction and layout optimization.

**Method:** RCPS integrates three components: Deep Structured Narrative Planning, Adaptive Layout Generation, and an Iterative Optimization Loop, along with PREVAL for evaluating presentation quality.

**Key Contributions:**

	1. Introduction of RCPS framework for media presentation generation
	2. Development of PREVAL for presentation quality assessment
	3. Demonstrated superior performance over baseline methods

**Result:** RCPS significantly outperforms existing methods in all quality dimensions, yielding presentations that align closely with human expert standards.

**Limitations:** The study focuses on specific quality dimensions and may not cover broader contextual factors influencing presentation effectiveness.

**Conclusion:** The framework and evaluation tool improve the process of generating and assessing media presentations, validating the potential for automation in this field.

**Abstract:** Automated generation of high-quality media presentations is challenging, requiring robust content extraction, narrative planning, visual design, and overall quality optimization. Existing methods often produce presentations with logical inconsistencies and suboptimal layouts, thereby struggling to meet professional standards. To address these challenges, we introduce RCPS (Reflective Coherent Presentation Synthesis), a novel framework integrating three key components: (1) Deep Structured Narrative Planning; (2) Adaptive Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose PREVAL, a preference-based evaluation framework employing rationale-enhanced multi-dimensional models to assess presentation quality across Content, Coherence, and Design. Experimental results demonstrate that RCPS significantly outperforms baseline methods across all quality dimensions, producing presentations that closely approximate human expert standards. PREVAL shows strong correlation with human judgments, validating it as a reliable automated tool for assessing presentation quality.

</details>


### [45] [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)

*Yilun Zhao, Weiyuan Chen, Zhijian Xu, Manasi Patwardhan, Yixin Liu, Chengye Wang, Lovekesh Vig, Arman Cohan*

**Main category:** cs.CL

**Keywords:** ablation studies, large language models, benchmarking, evaluation methods, NLP

**Relevance Score:** 9

**TL;DR:** AbGen is a benchmark for assessing LLMs in designing ablation studies for scientific research, revealing gaps in performance compared to human experts and the limitations of automated evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well LLMs can design ablation studies, which are crucial for scientific research, and to highlight existing performance gaps and evaluation challenges.

**Method:** AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers, where LLMs generate detailed ablation study designs based on research context.

**Key Contributions:**

	1. Introduction of AbGen benchmark for ablation study design
	2. Identification of performance gaps between LLMs and human experts
	3. Creation of AbGen-Eval for reliable evaluation of LLMs

**Result:** Evaluation of models like DeepSeek-R1-0528 and o4-mini shows a performance gap compared to human experts; automated evaluation methods were found unreliable.

**Limitations:** Current automated evaluation methods do not reliably measure LLM performance compared to human assessment.

**Conclusion:** Developing AbGen-Eval for better assessment of automated evaluation systems is crucial for advancing LLM performance on scientific tasks.

**Abstract:** We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.

</details>


### [46] [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)

*Guimin Hu, Daniel Hershcovich, Hasti Seifi*

**Main category:** cs.CL

**Keywords:** haptics, dataset, vibration signals, machine learning, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces HapticCap, a multimodal dataset for matching user descriptions to vibration haptic signals, addressing the challenges of limited existing datasets and models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The objective is to improve the design of haptic signals that resonate with users by providing a structured dataset that connects user descriptions to haptic vibration signals.

**Method:** The authors created HapticCap, a dataset with 92,070 haptic-text pairs, and proposed a haptic-caption retrieval task using a supervised contrastive learning framework.

**Key Contributions:**

	1. Introduction of the first fully human-annotated haptic-captioned dataset (HapticCap) with 92,070 pairs.
	2. Development of a haptic-caption retrieval task, addressing the challenges of relating descriptions to vibration signals.
	3. Highlighting the effectiveness of T5 and AST models in improving task performance.

**Result:** The study demonstrated that combining a language model (T5) with an audio model (AST) achieved the best results in the haptic-caption retrieval task, particularly when models were trained separately for different description categories.

**Limitations:** The dataset may not cover the full range of human sensory experiences with haptics, and the model performance might vary with different user demographics.

**Conclusion:** HapticCap serves as a significant resource to better understand and design haptic feedback by connecting textual descriptions to haptic signals.

**Abstract:** Haptic signals, from smartphone vibrations to virtual reality touch feedback, can effectively convey information and enhance realism, but designing signals that resonate meaningfully with users is challenging. To facilitate this, we introduce a multimodal dataset and task, of matching user descriptions to vibration haptic signals, and highlight two primary challenges: (1) lack of large haptic vibration datasets annotated with textual descriptions as collecting haptic descriptions is time-consuming, and (2) limited capability of existing tasks and models to describe vibration signals in text. To advance this area, we create HapticCap, the first fully human-annotated haptic-captioned dataset, containing 92,070 haptic-text pairs for user descriptions of sensory, emotional, and associative attributes of vibrations. Based on HapticCap, we propose the haptic-caption retrieval task and present the results of this task from a supervised contrastive learning framework that brings together text representations within specific categories and vibrations. Overall, the combination of language model T5 and audio model AST yields the best performance in the haptic-caption retrieval task, especially when separately trained for each description category.

</details>


### [47] [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)

*Amrit Poudel, Tim Weninger*

**Main category:** cs.CL

**Keywords:** search engines, search bias, ideological polarization, public discourse, information access

**Relevance Score:** 4

**TL;DR:** This study examines how search engines and ideologically-motivated user queries contribute to bias in search results, revealing that both factors amplify specific narratives and reinforce ideological divides.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how search engines and user ideologies contribute to bias in the portrayal of political and social topics.

**Method:** Analysis of outputs from major search engines using a dataset focused on political and social topics.

**Key Contributions:**

	1. Demonstrates the link between ideologically-driven queries and search bias.
	2. Highlights differences in content prioritization across various search engines.
	3. Sheds light on the role of search engines in public discourse and polarization.

**Result:** Search engines reflect biases in content prioritization and ideologically-driven user queries exacerbate these biases, leading to narrative amplification and differing sources across engines.

**Limitations:** 

**Conclusion:** Search engines significantly shape public perceptions and contribute to information polarization by reinforcing ideological divides.

**Abstract:** Search engines play a crucial role in shaping public discourse by influencing how information is accessed and framed. While prior research has extensively examined various dimensions of search bias -- such as content prioritization, indexical bias, political polarization, and sources of bias -- an important question remains underexplored: how do search engines and ideologically-motivated user queries contribute to bias in search results. This study analyzes the outputs of major search engines using a dataset of political and social topics. The findings reveal that search engines not only prioritize content in ways that reflect underlying biases but also that ideologically-driven user queries exacerbate these biases, resulting in the amplification of specific narratives. Moreover, significant differences were observed across search engines in terms of the sources they prioritize. These results suggest that search engines may play a pivotal role in shaping public perceptions by reinforcing ideological divides, thereby contributing to the broader issue of information polarization.

</details>


### [48] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)

*Yulu Qin, Dheeraj Varghese, Adam Dahlgren Lindstr√∂m, Lucia Donatelli, Kanishka Misra, Najoung Kim*

**Main category:** cs.CL

**Keywords:** vision-and-language, language models, taxonomic knowledge, question-answering, linguistic representation

**Relevance Score:** 7

**TL;DR:** This paper investigates the impact of vision-and-language (VL) training on language models' linguistic representations, particularly in taxonomic organization of lexical-conceptual knowledge.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether VL training leads to significant changes in the linguistic representations of language models, particularly in the context of taxonomic organization of concepts.

**Method:** The authors compare text-only language models with their VL-trained counterparts using a text-only question-answering task that assesses taxonomic understanding. They perform targeted behavioral and representational analyses to investigate differences.

**Key Contributions:**

	1. Demonstrated the superiority of VL models in text-only taxonomic understanding tasks.
	2. Revealed that while taxonomic knowledge remains consistent, the representation and deployment of this knowledge improve with VL training.
	3. Provided evidence that VL training affects how language models represent questions based on taxonomic relations.

**Result:** VL-trained models outperform text-only models on a task requiring taxonomic knowledge, highlighting improved deployment of lexical-conceptual knowledge in VL models, though their underlying taxonomic knowledge remains similar.

**Limitations:** 

**Conclusion:** VL training enhances the ability of models to utilize their knowledge in task-specific contexts but does not fundamentally change the nature of their taxonomic knowledge.

**Abstract:** Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.

</details>


### [49] [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)

*Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen*

**Main category:** cs.CL

**Keywords:** Turing Machine, Length Generalization, Imitation Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces Turing MAchine Imitation Learning (TAIL) to enhance the length generalization ability of large language models (LLMs) by imitating Turing Machine processes, significantly improving performance on reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of length generalization in Transformer-based LLMs, which struggle with longer sequences than those observed during training.

**Method:** TAIL synthesizes chain-of-thought data that imitates the execution of a Turing Machine by expanding reasoning steps into atomic states and employing explicit memory fetch mechanisms.

**Key Contributions:**

	1. Introduction of Turing MAchine Imitation Learning (TAIL)
	2. Demonstration of improved length generalization in LLMs
	3. Creation of a synthetic dataset covering various algorithms and tasks

**Result:** TAIL improves the performance of Qwen2.5-7B on various tasks, surpassing previous methods and demonstrating significant length generalization capabilities using synthetic data.

**Limitations:** 

**Conclusion:** This work highlights the importance of Turing Machine concepts for enhancing LLM reasoning, and provides a promising direction for future research in synthetic data learning.

**Abstract:** Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.

</details>


### [50] [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)

*Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Context Engineering, Information Retrieval

**Relevance Score:** 9

**TL;DR:** This survey introduces Context Engineering, a discipline focused on optimizing contextual information for Large Language Models (LLMs), analyzing foundational components and system implementations, and revealing gaps in current model capabilities.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a formal framework for improving the performance of LLMs through systematic optimization of contextual information during inference.

**Method:** The paper presents a comprehensive taxonomy of Context Engineering, examining components like context retrieval, processing, and management, and their architectural integration into systems such as retrieval-augmented generation (RAG) and multi-agent systems.

**Key Contributions:**

	1. Introduction of the Context Engineering discipline
	2. Comprehensive taxonomy of context components
	3. Identification of critical research gaps in LLM performance

**Result:** The survey analyzes over 1300 research papers and identifies a critical research gap: current LLMs show advanced understanding but struggle with generating sophisticated long-form outputs.

**Limitations:** 

**Conclusion:** A unified framework for advancing context-aware AI is provided, highlighting the need for further research to address the identified performance gap in LLMs.

**Abstract:** The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.

</details>


### [51] [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)

*Tyler Loakman, William Thorne, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** humour, Large Language Models, dataset, jokes, computational humour

**Relevance Score:** 7

**TL;DR:** This paper investigates the ability of Large Language Models (LLMs) to explain different types of humour, focusing on a dataset of 600 jokes that includes various forms beyond simple puns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs can explain humour effectively across different types of jokes, moving beyond the focus on short pun-based jokes.

**Method:** The authors curate a dataset of 600 jokes categorized into four types, including puns and topical humour, and manually write high-quality explanations for each joke. They assess the zero-shot capabilities of various LLMs in explaining these jokes.

**Key Contributions:**

	1. Curated a diverse dataset of jokes with comprehensive explanations.
	2. Identified limitations in LLMs' abilities to explain complex humour forms.
	3. Highlighted the need for broader investigation in computational humour beyond simple jokes.

**Result:** The study finds that none of the tested models, including those designed for reasoning, can consistently provide adequate explanations for all joke types, revealing limitations in their understanding of humour.

**Limitations:** The dataset is limited to a specific set of joke types and may not represent the entirety of humour forms.

**Conclusion:** The findings indicate a significant research gap in computational humour, particularly in models' abilities to handle complex jokes that require real-world knowledge.

**Abstract:** Humour, as a complex language form, is derived from myriad aspects of life, whilst existing work on computational humour has focussed almost exclusively on short pun-based jokes. In this work, we investigate whether the ability of Large Language Models (LLMs) to explain humour depends on the particular humour form. We compare models on simple puns and more complex topical humour that requires knowledge of real-world entities and events. In doing so, we curate a dataset of 600 jokes split across 4 joke types and manually write high-quality explanations. These jokes include heterographic and homographic puns, contemporary internet humour, and topical jokes, where understanding relies on reasoning beyond "common sense", rooted instead in world knowledge regarding news events and pop culture. Using this dataset, we compare the zero-shot abilities of a range of LLMs to accurately and comprehensively explain jokes of different types, identifying key research gaps in the task of humour explanation. We find that none of the tested models (inc. reasoning models) are capable of reliably generating adequate explanations of all joke types, further highlighting the narrow focus of most works in computational humour on overly simple joke forms.

</details>


### [52] [A Logically Consistent Chain-of-Thought Approach for Stance Detection](https://arxiv.org/abs/2312.16054)

*Bowen Zhang, Daijun Ding, Liwen Jing, Hu Huang*

**Main category:** cs.CL

**Keywords:** Zero-shot stance detection, Logical consistency, Chain-of-Thought, AI, Machine Learning

**Relevance Score:** 7

**TL;DR:** Introducing Logically Consistent Chain-of-Thought (LC-CoT) for zero-shot stance detection, enhancing knowledge extraction and logical consistency in predictions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve stance detection in zero-shot settings by addressing knowledge-task disconnect and enhancing logical consistency in predictions.

**Method:** LC-CoT employs a three-step process: assessing the need for external knowledge, retrieving knowledge via API calls for processing by a separate LLM, and guiding the LLM with a manual exemplar using an if-then logical structure.

**Key Contributions:**

	1. Introduction of LC-CoT for zero-shot stance detection
	2. Improved logical consistency in predictions using an if-then approach
	3. Elimination of the need for labeled data in stance detection tasks

**Result:** The proposed method outperforms traditional supervised methods in stance detection without needing labeled data.

**Limitations:** 

**Conclusion:** LC-CoT effectively enhances the model's capabilities in stance detection tasks by ensuring relevant and logically sound knowledge extraction.

**Abstract:** Zero-shot stance detection (ZSSD) aims to detect stances toward unseen targets. Incorporating background knowledge to enhance transferability between seen and unseen targets constitutes the primary approach of ZSSD. However, these methods often struggle with a knowledge-task disconnect and lack logical consistency in their predictions. To address these issues, we introduce a novel approach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which improves stance detection by ensuring relevant and logically sound knowledge extraction. LC-CoT employs a three-step process. Initially, it assesses whether supplementary external knowledge is necessary. Subsequently, it uses API calls to retrieve this knowledge, which can be processed by a separate LLM. Finally, a manual exemplar guides the LLM to infer stance categories, using an if-then logical structure to maintain relevance and logical coherence. This structured approach to eliciting background knowledge enhances the model's capability, outperforming traditional supervised methods without relying on labeled data.

</details>


### [53] [Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2402.13722)

*S M Rafiuddin, Mohammed Rakib, Sadia Kamal, Arunkumar Bagavathi*

**Main category:** cs.CL

**Keywords:** Aspect-Based Sentiment Analysis, Adaptive Masking, Natural Language Processing, Contextual Relevance

**Relevance Score:** 7

**TL;DR:** This paper presents adaptive masking methods to improve Aspect-Based Sentiment Analysis (ABSA) by addressing challenges in context adaptation and relevance of words in complex sentences.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance the accuracy of Aspect-Based Sentiment Analysis by addressing limitations in current methods related to static hyperparameters and context adaptation.

**Method:** Proposes adaptive masking methods that dynamically remove irrelevant tokens based on context to improve Aspect Term Extraction and Aspect Sentiment Classification in ABSA tasks.

**Key Contributions:**

	1. Introduction of adaptive masking methods for ABSA
	2. Demonstrated performance improvements in accuracy and F1 scores
	3. Qualitative analysis showcasing the effectiveness of proposed methods

**Result:** The proposed methods outperform baseline methods in accuracy and F1 scores across four benchmark online review datasets, demonstrating better handling of complex sentences.

**Limitations:** 

**Conclusion:** Adaptive masking can significantly improve ABSA effectiveness and can be extended with multiple adaptations, showing promising results in both quantitative and qualitative analyses.

**Abstract:** Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments. In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets. Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction.

</details>


### [54] [On the Limitations of Large Language Models (LLMs): False Attribution](https://arxiv.org/abs/2404.04631)

*Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney*

**Main category:** cs.CL

**Keywords:** hallucination, language models, author attribution, NLP, evaluation metrics

**Relevance Score:** 8

**TL;DR:** This paper introduces the Simple Hallucination Index (SHI) as a new metric to measure hallucinations in LLMs, evaluates three state-of-the-art LLMs for author attribution, and finds that prediction accuracy negatively correlates with hallucination levels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models (LLMs) in terms of false attribution and to propose a new metric for measuring hallucinations.

**Method:** Evaluated three LLMs‚ÄîGemma-7B, Mixtral 8x7B, and LLaMA-2-13B‚Äîon their ability to perform automatic author attribution in a zero-shot setting using divided chunks of popular books, assessing accuracy and new hallucination metric metrics.

**Key Contributions:**

	1. Introduction of the Simple Hallucination Index (SHI) metric
	2. Empirical evaluation of author attribution accuracy for LLMs
	3. Public release of annotated data for further research

**Result:** Mixtral 8x7B achieved the highest prediction accuracy and lowest SHI, but also had the highest hallucination level for three books. Strong negative correlations between accuracy and SHI were reported, indicating the new metric's potential for broader application.

**Limitations:** The dependence on specific text chunks and the inherent challenges in author attribution tasks.

**Conclusion:** The results suggest that the SHI can be a reliable metric for evaluating hallucinations in LLMs and that prediction accuracies relate to the frequency of Wikipedia mentions, leading to public release of data and code for reproducibility.

**Abstract:** In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.

</details>


### [55] [DeFine: Decision-Making with Analogical Reasoning over Factor Profiles](https://arxiv.org/abs/2410.01772)

*Yebowen Hu, Xiaoyang Wang, Wenlin Yao, Yiming Lu, Daoan Zhang, Hassan Foroosh, Dong Yu, Fei Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Decision Making, Uncertainty, Analogical Reasoning, Financial Deliberation

**Relevance Score:** 8

**TL;DR:** The paper presents DeFine, a modular framework for LLMs to manage uncertainty in complex decision-making scenarios by constructing probabilistic factor profiles and leveraging past analogical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLM decision-making by systematically incorporating uncertainty from speech transcripts addressing complex scenarios.

**Method:** The DeFine framework builds probabilistic factor profiles from complex scenarios and integrates these profiles with analogical reasoning based on similar past experiences.

**Key Contributions:**

	1. Introduction of the DeFine framework for LLMs
	2. Probabilistic factor profiles for handling uncertainty
	3. Integration of analogical reasoning for improved decision-making

**Result:** DeFine improves LLMs' ability to make informed decisions under uncertainty, particularly relevant in consulting and financial contexts.

**Limitations:** 

**Conclusion:** The separation of uncertainty quantification and its incorporation into decision-making enhances the LLMs' performance in complex scenarios.

**Abstract:** LLMs are ideal for decision-making thanks to their ability to reason over long contexts. However, challenges arise when processing speech transcripts that describe complex scenarios, as they are verbose and include repetition, hedging, and vagueness. E.g., during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce \textsc{DeFine}, a modular framework that constructs probabilistic factor profiles from complex scenarios. It then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in new situations. Our framework separates the tasks of quantifying uncertainty and incorporating it into LLM decision-making. This approach is particularly useful in areas such as consulting and financial deliberation, where making decisions under uncertainty is vital.

</details>


### [56] [Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information](https://arxiv.org/abs/2410.12774)

*Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova*

**Main category:** cs.CL

**Keywords:** multi-task learning, task grouping, pointwise V-usable information, PVI, NLP

**Relevance Score:** 8

**TL;DR:** This paper proposes a metric for grouping tasks in multi-task learning based on pointwise V-usable information (PVI) to enhance model performance by maximizing learned information utility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multi-task learning by identifying effective task groupings that avoid negative transfer, which can occur with random or naive task associations.

**Method:** The authors introduce the PVI metric to quantify task relatedness based on task difficulty and perform experiments on 15 NLP datasets across various domains to assess the effectiveness of their grouping method.

**Key Contributions:**

	1. Introduction of PVI as a metric for task relatedness in multi-task learning.
	2. Empirical validation of PVI-based task grouping on various NLP datasets.
	3. Demonstration of competitive performance with reduced parameter count in joint learning settings.

**Result:** Joint learners using task groupings based on similar PVI estimates achieved competitive performance with fewer parameters, consistently across general, biomedical, and clinical domains.

**Limitations:** The choice of datasets may limit generalizability; results could vary with different task domains or learning frameworks.

**Conclusion:** Grouping tasks with similar PVI estimates leads to improved multi-task learning outcomes, suggesting the PVI metric is effective for identifying beneficial task associations.

**Abstract:** The success of multi-task learning can depend heavily on which tasks are grouped together. Naively grouping all tasks or a random set of tasks can result in negative transfer, with the multi-task models performing worse than single-task models. Though many efforts have been made to identify task groupings and to measure the relatedness among different tasks, it remains a challenging research topic to define a metric to identify the best task grouping out of a pool of many potential task combinations. We propose a metric of task relatedness based on task difficulty measured by pointwise V-usable information (PVI). PVI is a recently proposed metric to estimate how much usable information a dataset contains given a model. We hypothesize that tasks with not statistically different PVI estimates are similar enough to benefit from the joint learning process. We conduct comprehensive experiments to evaluate the feasibility of this metric for task grouping on 15 NLP datasets in the general, biomedical, and clinical domains. We compare the results of the joint learners against single learners, existing baseline methods, and recent large language models, including Llama 2 and GPT-4. The results show that by grouping tasks with similar PVI estimates, the joint learners yielded competitive results with fewer total parameters, with consistent performance across domains.

</details>


### [57] [SCULPT: Systematic Tuning of Long Prompts](https://arxiv.org/abs/2410.20788)

*Shanu Kumar, Akhila Yesantarao Venkata, Shubhanshu Khandelwal, Bishal Santra, Parag Agrawal, Manish Gupta*

**Main category:** cs.CL

**Keywords:** prompt optimization, large language models, SCULPT, hierarchical refinement, Critic-Actor framework

**Relevance Score:** 8

**TL;DR:** SCULPT is a framework for optimizing long prompts for LLMs using a hierarchical tree refinement approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing prompt optimization methods are ineffective for long prompts, risking information loss and being sensitive to perturbations.

**Method:** SCULPT treats prompt optimization as a hierarchical tree refinement problem, using a Critic-Actor framework to generate reflections and apply actions for prompt refinement.

**Key Contributions:**

	1. Introduces a hierarchical tree structure for prompt representation.
	2. Implements a novel Critic-Actor framework for prompt refinement.
	3. Achieves better performance on long prompts compared to prior methods.

**Result:** SCULPT demonstrates improved effectiveness on long prompts and robustness against adversarial perturbations, outperforming existing methods by ensuring essential task information preservation.

**Limitations:** 

**Conclusion:** SCULPT's approach results in more stable and interpretable prompt modifications, enhancing generalization across tasks when working with LLMs.

**Abstract:** Prompt optimization is essential for effective utilization of large language models (LLMs) across diverse tasks. While existing optimization methods are effective in optimizing short prompts, they struggle with longer, more complex ones, often risking information loss and being sensitive to small perturbations. To address these challenges, we propose SCULPT (Systematic Tuning of Long Prompts), a framework that treats prompt optimization as a hierarchical tree refinement problem. SCULPT represents prompts as tree structures, enabling targeted modifications while preserving contextual integrity. It employs a Critic-Actor framework that generates reflections and applies actions to refine the prompt. Evaluations demonstrate SCULPT's effectiveness on long prompts, its robustness to adversarial perturbations, and its ability to generate high-performing prompts even without any initial human-written prompt. Compared to existing state of the art methods, SCULPT consistently improves LLM performance by preserving essential task information while applying structured refinements. Both qualitative and quantitative analyses show that SCULPT produces more stable and interpretable prompt modifications, ensuring better generalization across tasks.

</details>


### [58] [IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization](https://arxiv.org/abs/2411.06208)

*Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** large language models, instruction following, TRACE benchmark, IOPO alignment, preference optimization

**Relevance Score:** 8

**TL;DR:** This paper introduces TRACE, a benchmark for evaluating complex instruction-following abilities in LLMs, and the IOPO alignment method for enhancing instruction adherence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in the ability of large language models to follow complex instructions due to limited evaluation data and lack of dedicated algorithms.

**Method:** TRACE benchmark with 120K training and 1K evaluation data; IOPO alignment method considering input-output preference pairs to enhance model performance on instructions.

**Key Contributions:**

	1. Introduction of TRACE benchmark for complex instruction evaluation
	2. Development of IOPO alignment method for better instruction adherence
	3. Demonstrated effectiveness through extensive experiments

**Result:** IOPO demonstrates significant improvements in instruction following, with up to 8.15% and 6.29% gains on in-domain and out-of-domain datasets, respectively.

**Limitations:** 

**Conclusion:** The proposed methods effectively enhance the ability of LLMs to follow complex instructions, confirming their utility through empirical results.

**Abstract:** In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.

</details>


### [59] [Multi-task retriever fine-tuning for domain-specific and efficient RAG](https://arxiv.org/abs/2501.04652)

*Patrice B√©chard, Orlando Marquez Ayala*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, instruction fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper presents a method to instruction fine-tune a retriever encoder for Retrieval-Augmented Generation (RAG) applications, enabling scalability and efficiency by serving multiple domain-specific tasks with a single encoder.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges in deploying RAG applications, particularly the computational expense of fine-tuning LLMs, and the need for a single retriever to accommodate various domain-specific tasks.

**Method:** The methodology involves instruction fine-tuning a small retriever encoder on diverse domain-specific tasks to enhance the quality of data retrieved for LLM input.

**Key Contributions:**

	1. Instruction fine-tuning approach for retriever encoder
	2. One encoder serving multiple domain-specific use cases
	3. Demonstrates generalization to out-of-domain tasks

**Result:** The study demonstrates that the instruction fine-tuned encoder generalizes well to out-of-domain scenarios and is effective in unseen retrieval tasks in real-world enterprise applications.

**Limitations:** 

**Conclusion:** The paper concludes that a unified retriever can reduce costs and improve speed while maintaining performance across different tasks.

**Abstract:** Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.

</details>


### [60] [Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation](https://arxiv.org/abs/2502.01491)

*Verna Dankers, Vikas Raunak*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, knowledge distillation, memorization, hallucination, Adaptive-SeqKD

**Relevance Score:** 7

**TL;DR:** This paper investigates how knowledge is transferred in sequence-level knowledge distillation for Neural Machine Translation, noting that student models inherit memorization from teacher models, leading to increased hallucinations and recommending a new method to mitigate these issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how instance-level memorization in teacher models affects student models during knowledge distillation in Neural Machine Translation.

**Method:** The authors analyze memorization and hallucination rates in student models trained under sequence-level knowledge distillation and introduce a modified approach called Adaptive-SeqKD to reduce these effects.

**Key Contributions:**

	1. Inventory of memorization and hallucination rates in student models
	2. Characterization of student behavior on specific training data subgroups
	3. Introduction of Adaptive-SeqKD to reduce memorization effects

**Result:** Students show 3.4% more exact matches and 57% higher extractive memorization than baseline models, with increased hallucination rates, particularly in low-quality data subgroups.

**Limitations:** The paper includes an analysis of limitations and potential biases in the study and the proposed method.

**Conclusion:** Caution is advised when applying sequence-level knowledge distillation, as students inherit both benefits and weaknesses from their teachers, necessitating careful monitoring.

**Abstract:** In this work, we explore how instance-level memorization in the teacher Neural Machine Translation (NMT) model gets inherited by the student model in sequence-level knowledge distillation (SeqKD). We find that despite not directly seeing the original training data, students memorize more than baseline models (models of the same size, trained on the original data) -- 3.4% for exact matches and 57% for extractive memorization -- and show increased hallucination rates. Further, under this SeqKD setting, we also characterize how students behave on specific training data subgroups, such as subgroups with low quality and specific counterfactual memorization (CM) scores, and find that students exhibit amplified denoising on low-quality subgroups. Finally, we propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD to reduce memorization and hallucinations. Overall, we recommend caution when applying SeqKD: students inherit both their teachers' superior performance and their fault modes, thereby requiring active monitoring.

</details>


### [61] [MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment](https://arxiv.org/abs/2502.18699)

*Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Preference Optimization, Large Language Models, Computational Efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents Mixing Preference Optimization (MPO) for combining single-objective policies to improve reinforcement learning from human feedback and reduce costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on singular reward models in Reinforcement Learning from Human Feedback (RLHF) fails to capture diverse human preferences, leading to costly and unstable training processes.

**Method:** Mixing Preference Optimization (MPO) is proposed as a post-processing framework that log-linearly combines existing policies using batch stochastic mirror descent to create a unified policy from multiple single-objective policies.

**Key Contributions:**

	1. Introduction of Mixing Preference Optimization (MPO) framework
	2. Improved performance on aligning large language models with diverse human preferences
	3. Significantly reduced computational costs compared to existing models

**Result:** Empirical results show that MPO achieves balanced performance across varied human preferences while outperforming or matching existing models with lower computational costs.

**Limitations:** 

**Conclusion:** MPO provides a viable alternative to multi-objective approaches in RLHF by effectively aggregating diverse human feedback into a single objective framework.

**Abstract:** Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.

</details>


### [62] [OASIS: Order-Augmented Strategy for Improved Code Search](https://arxiv.org/abs/2503.08161)

*Zuchen Gao, Zizheng Zhan, Xianming Li, Erxin Yu, Ziqi Zhan, Haotian Zhang, Bin Chen, Yuqun Zhang, Jing Li*

**Main category:** cs.CL

**Keywords:** code embeddings, large language models, code search, order-based similarity, machine learning

**Relevance Score:** 8

**TL;DR:** The OASIS model improves code embedding training for code search by using order-based similarity labels to capture subtle differences among negative pairs, outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance code embeddings for better performance in code-related LLM applications like code search by addressing the limitations of existing training methods which focus predominantly on positive-negative pair comparisons.

**Method:** The proposed approach, OASIS, uses order-based similarity labels during training to help models recognize subtle semantic differences among negative pairs, enhancing the training of code embeddings.

**Key Contributions:**

	1. Introduction of order-augmented strategy for code embeddings
	2. Demonstration of improved performance in code search
	3. Highlighting the significance of subtle semantic differences in negative pair training

**Result:** Extensive benchmarking shows that OASIS significantly outperforms previous state-of-the-art models by effectively leveraging subtle semantic distinctions in negative pairs.

**Limitations:** 

**Conclusion:** The approach highlights the importance of considering subtle differences among negative pairs, improving the effectiveness of code embedding models in code search applications.

**Abstract:** Code embeddings capture the semantic representations of code and are crucial for various code-related large language model (LLM) applications, such as code search. Previous training primarily relies on optimizing the InfoNCE loss by comparing positive natural language (NL)-code pairs with in-batch negatives. However, due to the sparse nature of code contexts, training solely by comparing the major differences between positive and negative pairs may fail to capture deeper semantic nuances. To address this issue, we propose a novel order-augmented strategy for improved code search (OASIS). It leverages order-based similarity labels to train models to capture subtle differences in similarity among negative pairs. Extensive benchmark evaluations demonstrate that our OASIS model significantly outperforms previous state-of-the-art models focusing solely on major positive-negative differences. It underscores the value of exploiting subtle differences among negative pairs with order labels for effective code embedding training.

</details>


### [63] [Synthesizing Privacy-Preserving Text Data via Finetuning without Finetuning Billion-Scale LLMs](https://arxiv.org/abs/2503.12347)

*Bowen Tan, Zheng Xu, Eric Xing, Zhiting Hu, Shanshan Wu*

**Main category:** cs.CL

**Keywords:** synthetic data, differential privacy, data generation, large language models, clustering

**Relevance Score:** 9

**TL;DR:** CTCL is a novel framework for generating privacy-preserving synthetic data that overcomes limitations of existing methods by using a lightweight conditional generator and clustering-based topic model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Synthetic data generation is crucial for training models while ensuring data privacy, particularly with increasing restrictions around data usage. Existing methods face challenges due to resource constraints and reliance on manual prompts.

**Method:** CTCL employs a lightweight 140M conditional generator pre-trained on public data and a clustering-based topic model. The generator undergoes DP finetuning on private data, and the topic model creates a DP histogram for effective sampling of synthetic data.

**Key Contributions:**

	1. Introduction of CTCL framework for synthetic data generation
	2. Utilization of a lightweight conditional generator
	3. Development of a clustering-based topic model for DP histogram representation

**Result:** Evaluation shows that CTCL effectively generates high-quality synthetic data across five diverse domains, maintaining strong privacy while being scalable and requiring less extensive resources than prior methods.

**Limitations:** Limited to scenarios where strong privacy is paramount and may require further validation in broader applications.

**Conclusion:** CTCL provides a robust solution for synthetic data generation with better controllability and reduced dependency on large resources or manual prompt engineering, proving its effectiveness in data privacy applications.

**Abstract:** Synthetic data offers a promising path to train models while preserving data privacy. Differentially private (DP) finetuning of large language models (LLMs) as data generator is effective, but is impractical when computation resources are limited. Meanwhile, prompt-based methods such as private evolution depend heavily on the manual prompts, and ineffectively use private information in their iterative data selection process. To overcome these limitations, we propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel framework for generating privacy-preserving synthetic data without extensive prompt engineering or billion-scale LLM finetuning. CTCL pretrains a lightweight 140M conditional generator and a clustering-based topic model on large-scale public data. To further adapt to the private domain, the generator is DP finetuned on private data for fine-grained textual information, while the topic model extracts a DP histogram representing distributional information. The DP generator then samples according to the DP histogram to synthesize a desired number of data examples. Evaluation across five diverse domains demonstrates the effectiveness of our framework, particularly in the strong privacy regime. Systematic ablation validates the design of each framework component and highlights the scalability of our approach.

</details>


### [64] [A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models](https://arxiv.org/abs/2503.12989)

*Palakorn Achananuparp, Ee-Peng Lim, Yao Lu*

**Main category:** cs.CL

**Keywords:** occupation classification, large language models, taxonomy-guided reasoning, inferencing, retrieval

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for job data annotation using large language models (LLMs) that integrates taxonomy-guided reasoning to improve occupational classification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for standardized occupation classification in labor market analysis and the limitations of manual annotations.

**Method:** A multi-stage framework involving inference, retrieval, and reranking, enhanced with taxonomy-guided reasoning examples.

**Key Contributions:**

	1. A multi-stage approach to improve occupation classification
	2. Integration of taxonomy-guided reasoning examples
	3. Cost-effective solution compared to leading LLMs

**Result:** The proposed framework shows improved performance in occupation and skill classification tasks while significantly reducing computational costs compared to models like GPT-4o.

**Limitations:** Smaller models struggle more than larger ones with taxonomic understanding.

**Conclusion:** The framework provides a practical and scalable solution for occupation classification using LLMs.

**Abstract:** Automatically annotating job data with standardized occupations from taxonomies, known as occupation classification, is crucial for labor market analysis. However, this task is often hindered by data scarcity and the challenges of manual annotations. While large language models (LLMs) hold promise due to their extensive world knowledge and in-context learning capabilities, their effectiveness depends on their knowledge of occupational taxonomies, which remains unclear. In this study, we assess the ability of LLMs to generate precise taxonomic entities from taxonomy, highlighting their limitations, especially for smaller models. To address these challenges, we propose a multi-stage framework consisting of inference, retrieval, and reranking stages, which integrates taxonomy-guided reasoning examples to enhance performance by aligning outputs with taxonomic knowledge. Evaluations on a large-scale dataset show that our framework not only enhances occupation and skill classification tasks, but also provides a cost-effective alternative to frontier models like GPT-4o, significantly reducing computational costs while maintaining strong performance. This makes it a practical and scalable solution for occupation classification and related tasks across LLMs.

</details>


### [65] [CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings](https://arxiv.org/abs/2503.13733)

*Daniil Orel, Dilshod Azizov, Preslav Nakov*

**Main category:** cs.CL

**Keywords:** code generation, large language models, authorship detection, programming ethics, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for detecting whether code is written by humans or generated by large language models (LLMs), addressing the challenges of code generation and integrity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of LLMs in programming raises concerns about programming skills, ethics, and assessment integrity, necessitating effective detection of LLM-generated code to uphold accountability.

**Method:** The authors developed a framework that utilizes a large-scale dataset from various platforms and code generators. The framework employs rigorous data quality checks, feature engineering, and comparative analysis of traditional machine learning models, pre-trained language models, and LLMs for code detection.

**Key Contributions:**

	1. A novel multi-language code detection framework
	2. Extensive evaluation on out-of-domain scenarios
	3. Establishing new benchmarks for detecting LLM-generated code

**Result:** The framework successfully distinguishes between human and LLM-generated code across multiple programming languages and sets a new benchmark for code generation detection.

**Limitations:** 

**Conclusion:** This research highlights the importance of accurately identifying code authorship in the context of LLM advancements and demonstrates the effectiveness of the proposed framework.

**Abstract:** Large language models (LLMs) have revolutionized code generation, automating programming with remarkable efficiency. However, these advancements challenge programming skills, ethics, and assessment integrity, making the detection of LLM-generated code essential for maintaining accountability and standards. While, there has been some research on this problem, it generally lacks domain coverage and robustness, and only covers a small number of programming languages. To this end, we propose a framework capable of distinguishing between human- and LLM-written code across multiple programming languages, code generators, and domains. We use a large-scale dataset from renowned platforms and LLM-based code generators, alongside applying rigorous data quality checks, feature engineering, and comparative analysis using evaluation of traditional machine learning models, pre-trained language models (PLMs), and LLMs for code detection. We perform an evaluation on out-of-domain scenarios, such as detecting the authorship and hybrid authorship of generated code and generalizing to unseen models, domains, and programming languages. Moreover, our extensive experiments show that our framework effectively distinguishes human- from LLM-written code and sets a new benchmark for this task.

</details>


### [66] [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)

*Grace Byun, Shinsun Lee, Nayoung Choi, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Secure LLM, Enterprise AI, Human-Computer Interaction, Knowledge Retrieval

**Relevance Score:** 8

**TL;DR:** The SecMulti-RAG framework improves enterprise retrieval-augmented generation by combining internal documents, expert knowledge, and safe external LLM-generated knowledge, enhancing response quality while mitigating data security risks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Challenges in existing RAG systems for enterprises due to limited retrieval scope and data security risks.

**Method:** The SecMulti-RAG framework retrieves from internal documents, pre-generated expert knowledge, and on-demand external LLM-generated knowledge while employing a filtering mechanism for security.

**Key Contributions:**

	1. Introduction of the Secure Multifaceted-RAG (SecMulti-RAG) framework
	2. Use of a filtering mechanism for safe external LLM utilization
	3. Significant performance improvement in report generation tasks

**Result:** SecMulti-RAG achieves 79.3 to 91.9 percent win rates in correctness, richness, and helpfulness, along with human evaluations showing 56.3 to 70.4 percent improvements over traditional RAG.

**Limitations:** 

**Conclusion:** SecMulti-RAG offers a practical and secure solution for enterprise RAG challenges.

**Abstract:** Existing Retrieval-Augmented Generation (RAG) systems face challenges in enterprise settings due to limited retrieval scope and data security risks. When relevant internal documents are unavailable, the system struggles to generate accurate and complete responses. Additionally, using closed-source Large Language Models (LLMs) raises concerns about exposing proprietary information. To address these issues, we propose the Secure Multifaceted-RAG (SecMulti-RAG) framework, which retrieves not only from internal documents but also from two supplementary sources: pre-generated expert knowledge for anticipated queries and on-demand external LLM-generated knowledge. To mitigate security risks, we adopt a local open-source generator and selectively utilize external LLMs only when prompts are deemed safe by a filtering mechanism. This approach enhances completeness, prevents data leakage, and reduces costs. In our evaluation on a report generation task in the automotive industry, SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9 percent win rates across correctness, richness, and helpfulness in LLM-based evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights SecMulti-RAG as a practical and secure solution for enterprise RAG.

</details>


### [67] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)

*Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

**Main category:** cs.CL

**Keywords:** Multi-modal, Large Language Models, Dialogue Systems, Context Modeling, Dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces ContextQFormer, a new approach for enhancing multi-turn interactions in multi-modal large language models, and presents the TMDialog dataset for future research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the capabilities of open-source multi-modal models in multi-turn interaction, particularly for long contexts, which existing models struggle with.

**Method:** The authors develop a context modeling module called ContextQFormer that uses a memory block to enhance the representation of contextual information. They also create TMDialog, a new dataset for pre-training and evaluation, which features longer conversations for more effective dialogue modeling.

**Key Contributions:**

	1. Introduction of ContextQFormer for better context modeling in multi-modal dialogues
	2. Creation of the TMDialog dataset for enhanced multi-turn dialogue capabilities
	3. Demonstration of improved interaction rates over existing models.

**Result:** ContextQFormer demonstrates an improvement of 2%-4% in available rate compared to three baseline models when evaluated on the TMDialog dataset.

**Limitations:** 

**Conclusion:** ContextQFormer improves multi-turn multi-modal dialogue capabilities, and the TMDialog dataset will support further research in this area.

**Abstract:** Multi-modal large language models have demonstrated remarkable zero-shot abilities and powerful image-understanding capabilities. However, the existing open-source multi-modal models suffer from the weak capability of multi-turn interaction, especially for long contexts. To address the issue, we first introduce a context modeling module, termed ContextQFormer, which utilizes a memory block to enhance the presentation of contextual information. Furthermore, to facilitate further research, we carefully build a new multi-turn multi-modal dialogue dataset (TMDialog) for pre-training, instruction-tuning, and evaluation, which will be open-sourced lately. Compared with other multi-modal dialogue datasets, TMDialog contains longer conversations, which supports the research of multi-turn multi-modal dialogue. In addition, ContextQFormer is compared with three baselines on TMDialog and experimental results illustrate that ContextQFormer achieves an improvement of 2%-4% in available rate over baselines.

</details>


### [68] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)

*Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Generation, Reinforcement Learning, API Changes, Version Migration

**Relevance Score:** 9

**TL;DR:** ReCode is a framework enhancing LLMs' code generation in dynamic API scenarios through reinforcement learning and version migration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of LLMs in adapting to updates in external library APIs, which affect reliable code generation.

**Method:** A dataset of 2,000 entries is created for training LLMs on version migration, using a modified string similarity metric for reinforcement learning rewards.

**Key Contributions:**

	1. Introduction of ReCode framework for code update adaptation.
	2. Development of a dataset for training LLMs on version migration.
	3. Creation of a modified string similarity metric for code evaluation.

**Result:** ReCode improves LLMs' performance significantly in dynamic API environments, particularly in the CodeUpdateArena task, with less impact on general code abilities compared to supervised fine-tuning.

**Limitations:** Work in progress; practical limitations of deployment not addressed.

**Conclusion:** ReCode shows consistent improvements across various LLMs and reinforcement learning algorithms, with Qwen2.5-Coder-7B outperforming larger models.

**Abstract:** Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.

</details>
