# 2025-05-05

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 35]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users](https://arxiv.org/abs/2505.00821)

*Jeffrey Basoah, Jay L. Cunningham, Erica Adams, Alisha Bose, Aditi Jain, Kaustubh Yadav, Zhengyang Yang, Katharina Reinecke, Daniela Rosner*

**Main category:** cs.HC

**Keywords:** AI-supported writing technologies, racial biases, African American Vernacular English

**Relevance Score:** 8

**TL;DR:** This paper investigates how Black American users perceive AI-supported writing technologies, highlighting issues of racial bias and cultural misalignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand perceptions of AI-supported writing technologies among Black American users in light of racial biases present in language models.

**Method:** Interviews and observational user studies conducted with 13 Black American users of AISWT.

**Key Contributions:**

	1. Explores user perceptions of AISWT among Black American users.
	2. Identifies cultural and linguistic biases in AISWT.
	3. Discusses the social implications of linguistic corrections in AISWT.

**Result:** Participants experienced a tradeoff between the benefits of AISWT for writing enhancement and feeling that these tools did not cater to their cultural language use.

**Limitations:** Study limited to 13 participants, which may not represent the wider Black American community.

**Conclusion:** The findings reflect the tension between the failures of AISWT to recognize Black American culture and language and the attempts of these tools to authentically represent it.

**Abstract:** AI-supported writing technologies (AISWT) that provide grammatical suggestions, autocomplete sentences, or generate and rewrite text are now a regular feature integrated into many people's workflows. However, little is known about how people perceive the suggestions these tools provide. In this paper, we investigate how Black American users perceive AISWT, motivated by prior findings in natural language processing that highlight how the underlying large language models can contain racial biases. Using interviews and observational user studies with 13 Black American users of AISWT, we found a strong tradeoff between the perceived benefits of using AISWT to enhance their writing style and feeling like "it wasn't built for us". Specifically, participants reported AISWT's failure to recognize commonly used names and expressions in African American Vernacular English, experiencing its corrections as hurtful and alienating and fearing it might further minoritize their culture. We end with a reflection on the tension between AISWT that fail to include Black American culture and language, and AISWT that attempt to mimic it, with attention to accuracy, authenticity, and the production of social difference.

</details>


### [2] [Beyond the Mirror: Personal Analytics through Visual Juxtaposition with Other People's Data](https://arxiv.org/abs/2505.00855)

*Sungbok Shin, Sunghyo Chung, Hyeon Jeon, Hyunwook Lee, Minje Choi, Taehun Kim, Jaehoon Choi, Sungahn Ko, Jaegul Choo*

**Main category:** cs.HC

**Keywords:** visual analytics, self-tracking, behavioral insights, comparative analysis, domain-specific interpretation

**Relevance Score:** 7

**TL;DR:** The paper presents CALTREND, a visual analytics system for comparing individual anonymized online schedule logs with those of others to enhance behavioral insights through comparative views.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve insights from self-tracking applications by juxtaposing individual data with that of others for broader interpretation.

**Method:** Developed a visual analytics system named CALTREND that compares anonymized online schedule logs among individuals and conducted a study with domain experts.

**Key Contributions:**

	1. Introduction of CALTREND, a visual analytics tool for comparative analysis of schedule logs.
	2. Demonstrated the value of comparative perspectives in self-tracking data interpretation.
	3. Provided insights from domain experts on individual behavior characterization.

**Result:** The study revealed that comparative views allowed for enriched characterization of individuals based on domain-specific interpretations from the experts.

**Limitations:** Study based on a small sample of experts; findings may not generalize across all domains or populations.

**Conclusion:** Juxtaposing personal data with others can lead to varied interpretations influenced by domain-specific mental models, enhancing the understanding of individual behavior.

**Abstract:** An individual's data can reveal facets of behavior and identity, but its interpretation is context dependent. We can easily identify various self-tracking applications that help people reflect on their lives. However, self-tracking confined to one person's data source may fall short in terms of objectiveness, and insights coming from various perspectives. To address this, we examine how those interpretations about a person's data can be augmented when the data are juxtaposed with that of others using anonymized online calendar logs from a schedule management app. We develop CALTREND, a visual analytics system that compares an individuals anonymized online schedule logs with using those from other people. Using CALTREND as a probe, we conduct a study with two domain experts, one in information technology and one in Korean herbal medicine. We report our observations on how comparative views help enrich the characterization of an individual based on the experts' comments. We find that juxtaposing personal data with others' can potentially lead to diverse interpretations of one dataset shaped by domain-specific mental models.

</details>


### [3] [Inattentional Blindness with Augmented Reality HUDS: An On-road Study](https://arxiv.org/abs/2505.00879)

*Nayara de Oliveira Faria, Joseph L. Gabbard*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Inattentional Blindness, Driver Attention, Head-Up Displays, User Study

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of augmented reality (AR) tasks on driver attention and stimulus detection in vehicles, highlighting the phenomenon of inattentional blindness induced by AR displays.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the design and evaluation of AR interfaces in head-up displays (HUDs) for vehicle safety, particularly in relation to inattentional blindness during AR tasks.

**Method:** Conducted an on-road user study to analyze how the visual demand of AR tasks affects drivers' detection of environmental stimuli.

**Key Contributions:**

	1. Investigation of inattentional blindness in AR HUDs
	2. Empirical data on how visual demand affects driver attention
	3. Framework proposal for AR HUD safety evaluation

**Result:** Drivers' ability to detect stimuli decreased as the visual demand of AR tasks increased, with inattentional blindness being more pronounced in the central field of view.

**Limitations:** 

**Conclusion:** The study suggests a need for a safety-centric evaluation framework for AR HUDs, given the risks posed by inattentional blindness.

**Abstract:** As the integration of augmented reality (AR) technology in head-up displays (HUDs) becomes more prevalent in vehicles, it is crucial to understand how to design and evaluate AR interfaces to ensure safety. With new AR displays capable of rendering images with larger field of views and at varying depths, the visual and cognitive separation between graphical and real-world visual stimuli will be increasingly more difficult to quantify as will drivers' ability to efficiently allocate visual attention between the two sets of stimuli. In this study, we present a user study that serves as a crucial first step in gaining insight into inattentional blindness while using AR in surface transportation, where understanding is currently limited. Our primary goal is to investigate how the visual demand of AR tasks influences drivers' ability to detect stimuli, and whether the nature of the stimuli itself plays a role in this effect. To address these questions, we designed an on-road user study aimed at producing a more realistic and ecologically valid understanding of the phenomenon.   Our results show that drivers' ability to timely detect stimuli in the environment decreased as the AR task visual demand increased demonstrated by both detection performance and inattentional blindness metrics. Further, inattentional blindness caused by AR displays appears to be more prevalent within drivers' central field of view. We conclude by discussing implications towards a safety-centric evaluation framework for AR HUDs.

</details>


### [4] [Co-Designing a Knowledge Graph Navigation Interface: A Participatory Approach](https://arxiv.org/abs/2505.00907)

*Stanislava Gardasevic, Manika Lamba, Jasmine S. Malone*

**Main category:** cs.HC

**Keywords:** Knowledge Graphs, User-Centric Design, Participatory Design, Visualization Techniques, Interface Guidelines

**Relevance Score:** 6

**TL;DR:** This paper discusses the design of user-centric interfaces for multilayered knowledge graphs based on user insights from workshops.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective navigation and visualization of multilayered knowledge graphs in information systems design.

**Method:** The authors conducted a participatory workshop with doctoral students to gather insights and developed interface guidelines from these findings.

**Key Contributions:**

	1. User-driven interface guidelines for knowledge graph navigation.
	2. Demonstration of participatory iterative design's effectiveness in interface development.
	3. Framework for combining user insights with visualization techniques.

**Result:** The findings indicate that a participatory iterative design approach aids designers in creating innovative, user-centric interfaces for knowledge graphs.

**Limitations:** 

**Conclusion:** By merging user-driven requirements with visualization techniques, the paper provides a framework for enhancing the development of knowledge-graph navigation tools.

**Abstract:** Navigating and visualizing multilayered knowledge graphs remains a challenging, unresolved problem in information systems design. Building on our earlier study, which engaged end users in both the design and population of a domain-specific knowledge graph, we now focus on translating their insights into actionable interface guidelines. In this paper, we synthesize recommendations drawn from a participatory workshop with doctoral students. We then demonstrate how these recommendations inform the design of a prototype interface. Finally, we found that a participatory iterative design approach can help designers in decision making, leading to interfaces that are both innovative and user-centric. By combining user-driven requirements with proven visualization techniques, this paper presents a coherent framework for guiding future development of knowledge-graph navigation tools.

</details>


### [5] [SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning](https://arxiv.org/abs/2505.00945)

*Xiaoshan Huang, Jie Gao, Haolun Wu*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Computer Interaction, Collaborative Learning, Medical Education, Decision-Making

**Relevance Score:** 9

**TL;DR:** This paper presents SSRLBot, an LLM-based chatbot aimed at enhancing collaborative decision-making in medical diagnostic teamwork through the application of the Socially Shared Regulation of Learning (SSRL) framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in utilizing LLMs for multi-professional decision-making, particularly in teamwork contexts, and to improve collaborative learning outcomes in medicine.

**Method:** The study develops SSRLBot, a chatbot that analyzes team conversations using SSRL principles to enhance collaborative learning and decision-making. It includes functions for summarizing dialogues, assessing SSRL skills, and evaluating diagnostic outcomes.

**Key Contributions:**

	1. Development of SSRLBot as a novel tool for teamwork
	2. Integration of SSRL framework with LLM functionalities
	3. Enhanced understanding of team dynamics in medical diagnostics

**Result:** SSRLBot was compared with established models like Gemini-1.5 and GPT-3.5, demonstrating superior alignment with SSRL theory and providing detailed evaluations that improve team collaboration.

**Limitations:** 

**Conclusion:** SSRLBot represents a significant advancement in enhancing team-based decision-making in high-stakes environments, such as medical education, by integrating SSRL principles with LLM capabilities.

**Abstract:** Large language model (LLM)-based agents are increasingly used to support human experts by streamlining complex tasks and offering actionable insights. However, their application in multi-professional decision-making, particularly in teamwork contexts, remains underexplored. This design-based study addresses that gap by developing LLM functions to enhance collaboration, grounded in the Socially Shared Regulation of Learning (SSRL) framework and applied to medical diagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational, and emotional processes in shared learning, focusing on how teams manage these processes to improve decision-making. This paper introduces SSRLBot, a prototype chatbot designed to help team members reflect on both their diagnostic performance and key SSRL skills. Its core functions include summarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic outcomes, annotating SSRL markers in conversation, assessing their impact on performance, and identifying interpersonal regulatory dynamics. We compare SSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a case study. SSRLBot demonstrates stronger alignment with SSRL theory, offering detailed evaluations that link behaviors to regulatory dimensions and suggesting improvements for collaboration. By integrating SSRL theory with LLM capabilities, SSRLBot contributes a novel tool for enhancing team-based decision-making and collaborative learning in high-stakes environments, such as medical education.

</details>


### [6] [What Makes Teamwork Work? A Multimodal Case Study on Emotions and Diagnostic Expertise in an Intelligent Tutoring System](https://arxiv.org/abs/2505.00948)

*Xiaoshan Huang, Haolun Wu, Xue Liu, Susanne P. Lajoie*

**Main category:** cs.HC

**Keywords:** medical teamwork, decision-making, emotional dynamics, Intelligent Tutoring System, multimodal data analysis

**Relevance Score:** 8

**TL;DR:** This study analyzes the role of emotions and professional skills in medical teamwork through case studies within an Intelligent Tutoring System, focusing on their impact on diagnostic efficiency and team dynamics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the interplay between emotions and skills in medical decision-making is critical for improving group collaboration and diagnostic accuracy.

**Method:** The study employs multimodal data analysis comparing verbal and physiological data from high-performing and low-performing medical teams, along with retrospective experiences.

**Key Contributions:**

	1. Identification of emotional climate patterns in medical teams.
	2. Analysis of how emotion-driven dialogue impacts diagnostic efficiency.
	3. Insights into harmonizing emotional dynamics with decision-making strategies.

**Result:** Key findings reveal that social-motivational interactions significantly enhance the team emotional climate, which in turn positively affects diagnostic accuracy and team effectiveness.

**Limitations:** 

**Conclusion:** Optimizing group collaboration in medical contexts can be achieved by focusing on emotional dynamics and adaptive strategies, improving decision-making and diagnostic outcomes.

**Abstract:** Teamwork is pivotal in medical teamwork when professionals with diverse skills and emotional states collaborate to make critical decisions. This case study examines the interplay between emotions and professional skills in group decision-making during collaborative medical diagnosis within an Intelligent Tutoring System (ITS). By comparing verbal and physiological data between high-performing and low-performing teams of medical professionals working on a patient case within the ITS, alongside individuals' retrospective collaboration experiences, we employ multimodal data analysis to identify patterns in team emotional climate and their impact on diagnostic efficiency. Specifically, we investigate how emotion-driven dialogue and professional expertise influence both the information-seeking process and the final diagnostic decisions. Grounded in the socially shared regulation of learning framework and utilizing sentiment analysis, we found that social-motivational interactions are key drivers of a positive team emotional climate. Furthermore, through content analysis of dialogue and physiological signals to pinpoint emotional fluctuations, we identify episodes where knowledge exchange and skill acquisition are most likely to occur. Our findings offer valuable insights into optimizing group collaboration in medical contexts by harmonizing emotional dynamics with adaptive strategies for effective decision-making, ultimately enhancing diagnostic accuracy and teamwork effectiveness.

</details>


### [7] [Audio Personas: Augmenting Social Perception via Body-Anchored Audio Cues](https://arxiv.org/abs/2505.00956)

*Yujie Tao, Libby Ye, Jeremy N. Bailenson, Sean Follmer*

**Main category:** cs.HC

**Keywords:** Audio Personas, Augmented Reality, Social Impressions, Human-Computer Interaction, Emotion Expression

**Relevance Score:** 8

**TL;DR:** This paper introduces Audio Personas, a method for users to express themselves through sound in augmented reality, impacting social perceptions in face-to-face interactions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Audio Personas is to enhance interpersonal communication by allowing individuals to use sound to reflect their emotions and personality, similar to how they use clothing and other personal items for self-expression.

**Method:** The authors implemented a headphone-based prototype that allows for multi-user tracking and audio streaming, followed by formative studies and a preregistered in-lab study with 64 participants.

**Key Contributions:**

	1. Introduction of the concept of Audio Personas for self-expression in social settings.
	2. Development of a headphone-based prototype facilitating multi-user sound interactions.
	3. Demonstration through studies that audio personas can significantly affect social impression formation.

**Result:** The study found that audio personas significantly influenced social impressions, with positive audio personas leading to higher ratings of social attractiveness, likability, and lower perceptions of threat compared to negative personas.

**Limitations:** Potential limitations include the specificity of the study sample and environmental factors that may affect audio perception in real-world settings.

**Conclusion:** The findings highlight the potential of Audio Personas in audio augmented reality to enrich social interactions by allowing users to communicate their emotions and personality through sound.

**Abstract:** We introduce Audio Personas, enabling users to "decorate" themselves with body-anchored sounds in audio augmented reality. Like outfits, makeup, and fragrances, audio personas offer an alternative yet dynamic channel to augment face-to-face interactions. For instance, one can set their audio persona as rain sounds to reflect a bad mood, bee sounds to establish personal boundaries, or a playful "woosh" sound to mimic passing by someone like a breeze. To instantiate the concept, we implemented a headphone-based prototype with multi-user tracking and audio streaming. Our formative study with designers revealed that audio personas were preferred in public and semi-public-private spaces for managing social impressions (e.g., personality) and signaling current states (e.g., emotions). Our preregistered in-lab study with 64 participants showed that audio personas influenced how participants formed impressions. Individuals with positive audio personas were rated as more socially attractive, more likable, and less threatening than those with negative audio personas.

</details>


### [8] [Destructive Interference: Encoding Loss in the Overlap](https://arxiv.org/abs/2505.00987)

*Nik Aberle*

**Main category:** cs.HC

**Keywords:** data visualization, mass shootings, interactive installation, sculpture, statistics

**Relevance Score:** 4

**TL;DR:** Destructive Interference is a data visualization installation that represents mass shooting casualties in 2024 through an interactive sculpture involving interlocking rings that encode violence statistics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to visualize the overwhelming statistics of mass shootings in a manner that is physically engaging and emotionally impactful.

**Method:** The installation features a series of interlocking ring sculptures, one for each month of 2024, designed to reflect the level of violence through varying heights and encoded data on casualties and shootings.

**Key Contributions:**

	1. Parametric design of interactive sculptures for data visualization.
	2. Physical representation of complex statistics related to mass shootings.
	3. Engagement of viewers through immersive visual and physical experience.

**Result:** The resulting sculpture physically portrays the complexity of mass shooting data, with rotating cylinders that cast overlapping shadows symbolizing the deaths and injuries.

**Limitations:** 

**Conclusion:** This installation invites reflection and discussion about the ongoing crisis of gun violence by making the statistics tangible and visually compelling.

**Abstract:** Destructive Interference is a data visualization installation that representing the deaths and injuries caused by mass shootings in 2024 in the United States. I parametrically designed and fabricated an interlocking ring sculpture for each month of 2024; where the overall height corresponds to the level of violence in that month. Taller forms mark the deadliest months, while shorter ones reflect fewer casualties. Each inner ring encodes the number of people killed or injured, and each outer ring encodes the number of shootings and the number of days without them. The interlocking cylinders are powered via a motor to rotate, and lit from within. As the cylinders rotate, they cast overlapping shadows that represent those killed or injured by mass shootings. The goal of this work is to visualize otherwise overwhelming and disparate statistics in a way that is both physically present and emotionally resonant. By inviting viewers to step into and engage with these shadows, the piece creates space for reflection, conversation, and confrontation with the scale of this ongoing crisis.

</details>


### [9] [Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability](https://arxiv.org/abs/2505.01000)

*Jaeyoon Song, Zahra Ashktorab, Thomas W. Malone*

**Main category:** cs.HC

**Keywords:** adaptive scheduling, large language models, user preferences, cognitive load, decision making

**Relevance Score:** 8

**TL;DR:** Togedule is an adaptive scheduling tool that leverages large language models to tailor scheduling options based on user inputs, enhancing decision-making for both attendees and organizers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing scheduling tools are static and do not account for attendees' preferences and inputs, leading to inefficient decision-making.

**Method:** We developed Togedule, an adaptive scheduling tool, and conducted a formative study followed by two controlled experiments to assess its impact on scheduling efficiency.

**Key Contributions:**

	1. Introduction of an adaptive scheduling tool using large language models
	2. Empirical validation through controlled experiments
	3. Reduced cognitive load and improved decision-making efficiency

**Result:** Togedule significantly reduces cognitive load for attendees and enhances the quality and speed of decisions made by organizers in comparison to traditional scheduling methods.

**Limitations:** The study involved a limited sample size, which may impact the generalizability of the findings.

**Conclusion:** Togedule provides a responsive alternative to static scheduling tools, improving the overall scheduling experience for users.

**Abstract:** Scheduling is a perennial-and often challenging-problem for many groups. Existing tools are mostly static, showing an identical set of choices to everyone, regardless of the current status of attendees' inputs and preferences. In this paper, we propose Togedule, an adaptive scheduling tool that uses large language models to dynamically adjust the pool of choices and their presentation format. With the initial prototype, we conducted a formative study (N=10) and identified the potential benefits and risks of such an adaptive scheduling tool. Then, after enhancing the system, we conducted two controlled experiments, one each for attendees and organizers (total N=66). For each experiment, we compared scheduling with verbal messages, shared calendars, or Togedule. Results show that Togedule significantly reduces the cognitive load of attendees indicating their availability and improves the speed and quality of the decisions made by organizers.

</details>


### [10] [Barriers to Employment: The Deaf Multimedia Authoring Tax](https://arxiv.org/abs/2505.01030)

*C. Vogler, A. Glasser, R. Kushalnagar, M. Seita, M. Arroyo Chavez, K. Delk, P. DeVries, M. Feanny, B. Thompson, J. Waller*

**Main category:** cs.HC

**Keywords:** accessibility, deaf, hard of hearing, multimedia content creation, employment

**Relevance Score:** 3

**TL;DR:** The paper discusses the challenges faced by deaf and hard of hearing individuals in creating accessible multimedia content, highlighting the barriers in the content creation process and offering recommendations for improvement.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the accessibility challenges that deaf and hard of hearing individuals encounter when creating multimedia content, which affects their employment opportunities.

**Method:** The authors provide real-world examples of the challenges faced by deaf creators and propose guidance and recommendations for making the content creation process more accessible.

**Key Contributions:**

	1. Identification of barriers in the process of multimedia content creation for deaf individuals.
	2. Introduction of the 'deaf content creation tax' concept.
	3. Recommendations for making content creation more accessible.

**Result:** The paper highlights the concept of a 'deaf content creation tax,' demonstrating the additional time and resources required to produce content that is equivalent to that of non-disabled individuals.

**Limitations:** 

**Conclusion:** Improving accessibility in content creation for the deaf and hard of hearing can reduce barriers to employment and improve their ability to create multimedia content.

**Abstract:** This paper describes the challenges that deaf and hard of hearing people face with creating accessible multimedia content, such as portfolios, instructional videos and video presentations. Unlike content consumption, the process of content creation itself remains highly inaccessible, creating barriers to employment in all stages of recruiting, hiring, and carrying out assigned job duties. Overcoming these barriers incurs a "deaf content creation tax" that translates into requiring significant additional time and resources to produce content equivalent to what a non-disabled person would produce. We highlight this process and associated challenges through real-world examples experienced by the authors, and provide guidance and recommendations for addressing them.

</details>


### [11] [Closing the Loop: A Systematic Review of Experience-Driven Game Adaptation](https://arxiv.org/abs/2505.01351)

*Phil Lopes, Nuno Fachada, Maria Fonseca*

**Main category:** cs.HC

**Keywords:** adaptive systems, game experience, player sensing, emotional inference, machine learning

**Relevance Score:** 5

**TL;DR:** This review analyzes adaptive game systems, focusing on integrating player sensing, modeling, and content adaptation for enhanced player experiences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in integrating content personalization and player experience modeling into operational adaptive gameplay systems.

**Method:** A systematic review of 17 empirical studies following PRISMA guidelines, examining the player sensing modalities, modeling methods, and their application in adaptive gameplay.

**Key Contributions:**

	1. Systematic review of adaptive game systems
	2. Highlighting underutilized sensing modalities for emotional inference
	3. Identifying knowledge-based methods as dominant in modeling and adaptation

**Result:** Identified the prevalence of game telemetry for sensing, the dominance of knowledge-based methods for modeling, and noted the neglect of emotionally adaptive systems despite their potential.

**Limitations:** Focus primarily on performance over emotional adaptation; affective states like stress and anxiety largely ignored.

**Conclusion:** Advancing emotionally responsive game systems through underutilized sensing modalities can increase immersion and support emotional regulation in gameplay.

**Abstract:** Adaptive game systems aim to enrich player experiences by dynamically adjusting game content in response to user data. While extensive research has addressed content personalization and player experience modeling, the integration of these components into fully operational adaptive gameplay systems remains limited. This systematic review, conducted in accordance with PRISMA guidelines, analyzes 17 empirical studies published between January 2015 and May 2024, identifying and analyzing approaches that implement the complete experience-driven loop -- including player sensing, modeling, and content adaptation. Game telemetry remains the most prevalent sensing modality, although other non-invasive methods suitable for affective modeling -- such as facial expression analysis (FEA) and peripheral interaction data -- remain underutilized despite their potential for real-time emotional inference. Knowledge-based methods, such as rule-based systems and heuristics, dominate modeling and adaptation due to their interpretability and low resource demands, whereas machine learning approaches face challenges related to data availability and transparency. Despite their relevance to immersive and therapeutic experiences, affective states such as stress and anxiety remain largely ignored, as systems continue to favor performance over emotion-sensitive adaptation. These findings highlight a crucial research direction: advancing emotionally responsive game systems that move beyond performance optimization by incorporating underutilized sensing modalities -- such as FEA and peripheral interaction -- to enable real-time affect-driven personalization. Advancing in this direction holds strong potential to increase immersion, personalize gameplay, and support affect regulation across entertainment and therapeutic contexts.

</details>


### [12] [Group Gaze-Sharing with Projection Displays](https://arxiv.org/abs/2505.01413)

*Maurice Koch, Tobias Rau, Vladimir Mikheev, Seyda Öney, Michael Becher, Xiangyu Wang, Nelusa Pathmanathan, Patrick Gralka, Daniel Weiskopf, Kuno Kurzhals*

**Main category:** cs.HC

**Keywords:** gaze communication, group collaboration, projection mapping, tabletop interaction, eye tracking

**Relevance Score:** 8

**TL;DR:** This paper presents a technique for communicating gaze between groups in tabletop collaborative settings using projection mapping.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding and interaction in group collaborations by enhancing the communication of visual attention through shared gaze.

**Method:** The authors use projection mapping to unify gaze data from multiple participants and display it in a common visualization space on a tabletop.

**Key Contributions:**

	1. Development of a projection mapping technique for gaze communication in group settings
	2. Application of the technique in a tabletop collaborative puzzle-solving task
	3. Improved visualization of shared gaze to enhance group interaction

**Result:** The technique was showcased through a collaborative puzzle-solving task, which visualized shared attention on puzzle pieces and provided hints for solving the task.

**Limitations:** The technique may be limited by the accuracy of gaze detection and the physical arrangement of participants around the table.

**Conclusion:** The approach successfully lets groups communicate their visual attention more effectively, enhancing collaboration dynamics.

**Abstract:** The eyes play an important role in human collaboration. Mutual and shared gaze help communicate visual attention to each other or to a specific object of interest. Shared gaze was typically investigated for pair collaborations in remote settings and with people in virtual and augmented reality. With our work, we expand this line of research by a new technique to communicate gaze between groups in tabletop workshop scenarios. To achieve this communication, we use an approach based on projection mapping to unify gaze data from multiple participants into a common visualization space on a tabletop. We showcase our approach with a collaborative puzzle-solving task that displays shared visual attention on individual pieces and provides hints to solve the problem at hand.

</details>


### [13] [Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks](https://arxiv.org/abs/2402.08978)

*Wong Kam-Kwai, Yan Luo, Xuanwu Yue, Wei Chen, Huamin Qu*

**Main category:** cs.HC

**Keywords:** financial cluster analysis, visual analytics, business correlations

**Relevance Score:** 4

**TL;DR:** Prismatic is a visual analytics system designed for financial cluster analysis, combining quantitative performance and qualitative business knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better tools to simplify the analytical task of financial cluster analysis due to challenges like pairwise comparisons and dynamic correlations.

**Method:** Prismatic integrates quantitative historical data analysis with qualitative insights from business relationships, employing three processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation.

**Key Contributions:**

	1. Introduces a novel visual analytics system for financial clustering.
	2. Integrates quantitative and qualitative analyses seamlessly.
	3. Provides a multi-view approach for enhanced interpretation of business correlations.

**Result:** The system demonstrates effective clustering of correlated businesses, supported by case studies on concept stocks and expert interviews.

**Limitations:** 

**Conclusion:** Prismatic enhances the understanding of business correlations through a multi-view approach, combining data-driven and knowledge-driven insights.

**Abstract:** Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.

</details>


### [14] [Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile](https://arxiv.org/abs/2411.11382)

*Mudassir Ibrahim Awan, Ahsan Raza, Waseem Hassan, Ki-Uk Kyung, Seokhee Jeon*

**Main category:** cs.HC

**Keywords:** Haptic Affection, Deep Learning, Automotive Industry, User Experience, Force Profiles

**Relevance Score:** 4

**TL;DR:** This study explores how to predict the affective qualities of car doors using deep learning models based on force profiles during opening actions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the significance of haptic affection in enhancing user experience and customer satisfaction in the automotive sector.

**Method:** A deep learning model analyzes force and torque profiles of car doors, correlating them with user-defined adjective ratings to predict tactile feedback.

**Key Contributions:**

	1. Development of a deep learning model for predicting haptic affection
	2. Use of force profile data correlated with user adjective ratings
	3. Demonstration of high prediction accuracy in a diverse dataset

**Result:** The model demonstrates high accuracy in predicting affective properties from the force profiles, suggesting its efficiency in application for haptic affection and design optimization.

**Limitations:** 

**Conclusion:** The findings indicate that the model can significantly contribute to design processes in automotive development by enhancing user experience related to tactile interaction.

**Abstract:** Haptic affection plays a crucial role in user experience, particularly in the automotive industry where the tactile quality of components can influence customer satisfaction. This study aims to accurately predict the affective property of a car door by only watching the force or torque profile of it when opening. To this end, a deep learning model is designed to capture the underlying relationships between force profiles and user-defined adjective ratings, providing insights into the door-opening experience. The dataset employed in this research includes force profiles and user adjective ratings collected from six distinct car models, reflecting a diverse set of door-opening characteristics and tactile feedback. The model's performance is assessed using Leave-One-Out Cross-Validation, a method that measures its generalization capability on unseen data. The results demonstrate that the proposed model achieves a high level of prediction accuracy, indicating its potential in various applications related to haptic affection and design optimization in the automotive industry.

</details>


### [15] [Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention](https://arxiv.org/abs/2502.19877)

*Weiyan Shi, Viet Hai Le, Kenny Tsu Wei Choo*

**Main category:** cs.HC

**Keywords:** joint attention, Multimodal Large Language Models, parent-child interaction, speech-language development, multimodal reasoning

**Relevance Score:** 8

**TL;DR:** This study assesses the performance of Multimodal Large Language Models (MLLMs) in understanding joint attention during parent-child interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the capabilities of MLLMs in analyzing joint attention, which is vital for early speech-language development and effective parent-child communication.

**Method:** Analyzed 26 annotated parent-child interaction videos using insights from speech-language pathologists, focusing on segments with strong and poor joint attention.

**Key Contributions:**

	1. First comprehensive evaluation of MLLMs on joint attention detection.
	2. Identification of specific shortcomings in MLLM comprehension related to eye contact.
	3. Recommendations for enhancing MLLM capabilities in multimodal reasoning relevant to caregiver interactions.

**Result:** Current MLLMs showed difficulty in accurately interpreting joint attention, particularly regarding child-initiated eye contact.

**Limitations:** Limited sample size of annotated videos; Findings may not generalize across broader contexts.

**Conclusion:** Improving MLLMs' understanding of nuanced eye contact is crucial for advancing their application in parent-child interaction analysis.

**Abstract:** Joint attention is a critical component of early speech-language development and a key indicator of effective parent-child interaction. However, research on detecting and analysing joint attention remains limited, particularly for Multimodal Large Language Models (MLLMs). This study evaluates MLLMs' ability to comprehend joint attention by analysing 26 parent-child interaction videos annotated by two speech-language pathologists. These annotations identify strong and poor joint attention segments, serving as benchmarks for evaluating the models' interpretive capabilities. Our findings reveal that current MLLMs struggle to accurately interpret joint attention due to a lack of nuanced understanding of child-initiated eye contact, a crucial component of joint attention dynamics. This study highlights the importance of incorporating detailed eye contact to enhance MLLMs' multimodal reasoning. Addressing these gaps is essential for future research to advance the use of MLLMs in analysing and supporting parent-child interactions.

</details>


### [16] [WiCross: Indoor Human Zone-Crossing Detection Using Commodity WiFi Devices](https://arxiv.org/abs/2503.20331)

*Weiyan Shi, Xuanzhi Wang, Kai Niu, Leye Wang, Daqing Zhang*

**Main category:** cs.HC

**Keywords:** WiFi, zone crossing detection, smart homes, turn-back behavior, phase statistics

**Relevance Score:** 6

**TL;DR:** WiCross is a novel approach using WiFi signals to accurately detect zone crossings by distinguishing between crossing and turn-back behaviors, achieving over 95% accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate zone crossing detection in smart homes for applications such as security and people counting, addressing limitations of current infrared methods.

**Method:** WiCross analyzes the phase statistics pattern of WiFi signals to differentiate between crossing and turn-back behaviors, utilizing commercial WiFi devices.

**Key Contributions:**

	1. Proposes a new method for detecting zone crossings using WiFi signals.
	2. Successfully distinguishes between crossing and turn-back behaviors, reducing false alarms.
	3. Demonstrates high accuracy and low false alarm rates in real-world experiments.

**Result:** WiCross achieved over 95% accuracy in detecting zone crossings with a false alarm rate of less than 5% through extensive experimentation.

**Limitations:** 

**Conclusion:** WiCross offers a robust solution for zone crossing detection in smart home environments by leveraging existing WiFi infrastructure.

**Abstract:** Detecting whether a target crosses the given zone (e.g., a door) can enable various practical applications in smart homes, including intelligent security and people counting. The traditional infrared-based approach only covers a line and can be easily cracked. In contrast, reusing the ubiquitous WiFi devices deployed in homes has the potential to cover a larger area of interest as WiFi signals are scattered throughout the entire space. By detecting the walking direction (i.e., approaching and moving away) with WiFi signal strength change, existing work can identify the behavior of crossing between WiFi transceiver pair. However, this method mistakenly classifies the turn-back behavior as crossing behavior, resulting in a high false alarm rate. In this paper, we propose WiCross, which can accurately distinguish the turn-back behavior with the phase statistics pattern of WiFi signals and thus robustly identify whether the target crosses the area between the WiFi transceiver pair. We implement WiCross with commercial WiFi devices and extensive experiments demonstrate that WiCross can achieve an accuracy higher than 95\% with a false alarm rate of less than 5%.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)

*Bithiah Yuan*

**Main category:** cs.CL

**Keywords:** financial QA, BERT, information retrieval, machine learning, question answering

**Relevance Score:** 7

**TL;DR:** Proposal of a novel financial QA system using BERT to enhance decision-making for financial advisers by addressing data scarcity and language specificity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Emerging demand in the financial industry for automatic analysis of unstructured and structured data at scale to facilitate decision making for financial advisers.

**Method:** The QA system utilizes a transformer-based BERT model to perform financial non-factoid answer selection, framing this as a re-ranking problem involving an initial answer retrieval using BM25 followed by a BERT-based re-ranking for relevance.

**Key Contributions:**

	1. Introduction of FinBERT-QA for financial QA tasks
	2. Application of transfer learning and fine-tuning approaches on BERT for improved results
	3. Demonstrated significant performance improvement over the existing state-of-the-art QA systems in finance.

**Result:** FinBERT-QA, developed through further training and fine-tuning, achieved significant improvements on the FiQA dataset, with increases of 16% in MRR, 17% in NDCG, and 21% in Precision@1 compared to state-of-the-art results.

**Limitations:** 

**Conclusion:** The proposed model effectively enhances financial question answering by utilizing advancements in language modeling with BERT and tailored training strategies for better performance.

**Abstract:** Motivated by the emerging demand in the financial industry for the automatic analysis of unstructured and structured data at scale, Question Answering (QA) systems can provide lucrative and competitive advantages to companies by facilitating the decision making of financial advisers. Consequently, we propose a novel financial QA system using the transformer-based pre-trained BERT language model to address the limitations of data scarcity and language specificity in the financial domain. Our system focuses on financial non-factoid answer selection, which retrieves a set of passage-level texts and selects the most relevant as the answer. To increase efficiency, we formulate the answer selection task as a re-ranking problem, in which our system consists of an Answer Retriever using BM25, a simple information retrieval approach, to first return a list of candidate answers, and an Answer Re-ranker built with variants of pre-trained BERT language models to re-rank and select the most relevant answers. We investigate various learning, further pre-training, and fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a model built from applying the Transfer and Adapt further fine-tuning and pointwise learning approach, is the most effective, improving the state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on NDCG, and 21% on Precision@1.

</details>


### [18] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)

*Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** LLM-based agents, human-agent systems, survey, machine learning, autonomous agents

**Relevance Score:** 9

**TL;DR:** This paper surveys LLM-based human-agent systems (LLM-HAS) to address challenges in fully autonomous LLM-based agents by incorporating human feedback for improved reliability and safety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is to enhance the performance, reliability, and safety of LLM-based agents, which face significant challenges in real-world applications due to issues like hallucinations and task complexity.

**Method:** The paper presents a structured survey of LLM-HAS, clarifying fundamental concepts and systematically outlining core components such as environment profiling, human feedback, and communication methods.

**Key Contributions:**

	1. First comprehensive survey of LLM-based human-agent systems
	2. Clarifies core components shaping LLM-HAS
	3. Explores unique challenges and opportunities in the field

**Result:** The paper consolidates existing knowledge on LLM-HAS and reveals emerging applications, along with the unique challenges and opportunities these systems present.

**Limitations:** 

**Conclusion:** By providing a comprehensive overview of LLM-HAS, the paper aims to drive further research and innovation in the field.

**Abstract:** Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>


### [19] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)

*Alessandro Raganato, Rafael Peñaloza, Marco Viviani, Gabriella Pasi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Prompt Dependency, Benchmark Dataset, Geometric Figures

**Relevance Score:** 9

**TL;DR:** This paper analyzes the reasoning capabilities of Large Language Models (LLMs) using a new benchmark of simple geometric reasoning tasks to assess prompt dependency and performance variations across sizes of models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the reasoning competence of LLMs and investigate their prompt dependency through a structured benchmark.

**Method:** Introduces a new benchmark dataset with simple reasoning questions based on geometric figures; conducts empirical tests across 24 LLMs using zero-shot and few-shot prompting.

**Key Contributions:**

	1. Development of a benchmark dataset for LLM reasoning analysis
	2. Empirical analysis of reasoning across varying LLM sizes
	3. Insights into the effects of chain-of-thought prompting on model performance

**Result:** LLMs over 70 billion parameters show better performance in zero-shot settings, yet significant improvement potential exists; chain-of-thought prompting affects model performance variably.

**Limitations:** The reasoning tasks are limited to a specific domain (geometric figures) which may not fully represent broader reasoning capabilities.

**Conclusion:** Further research and model improvement are necessary to enhance reasoning capabilities, particularly in the context of appropriate prompting strategies.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.

</details>


### [20] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)

*Mario Sänger, Ulf Leser*

**Main category:** cs.CL

**Keywords:** relationship extraction, biomedical literature, pre-trained language models, hyperparameter optimization, contextual information

**Relevance Score:** 6

**TL;DR:** The paper evaluates the effectiveness of pre-trained language models (PLMs) enhanced with contextual information for automatic relationship extraction (RE) in biomedical literature.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Automatic relationship extraction from biomedical literature is essential for managing scientific knowledge, but variations in models and methodologies complicate direct comparisons of prior studies.

**Method:** The study evaluates three baseline PLMs across five datasets while applying hyperparameter optimization and augmenting the models with additional contextual data such as textual entity descriptions and relational information.

**Key Contributions:**

	1. Evaluation of PLMs with a uniform framework across diverse datasets
	2. Insights on the impact of hyperparameter tuning on model performance
	3. Demonstration of context data benefits for smaller PLMs in RE tasks

**Result:** The results demonstrate that while the choice of language model and hyperparameter optimization is crucial, adding context information provides only minor improvements overall but significant benefits for smaller PLMs.

**Limitations:** The improvement from context information was minor in general and may not translate to all contexts or models.

**Conclusion:** The study concludes that effective model selection and hyperparameter tuning are key to improving RE performance, particularly for smaller PLMs when enhanced with contextual data.

**Abstract:** Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent approach in RE. Several studies report improved performance when incorporating additional context information while fine-tuning PLMs for RE. However, variations in the PLMs applied, the databases used for augmentation, hyper-parameter optimization, and evaluation methods complicate direct comparisons between studies and raise questions about the generalizability of these findings. Our study addresses this research gap by evaluating PLMs enhanced with contextual information on five datasets spanning four relation scenarios within a consistent evaluation framework. We evaluate three baseline PLMs and first conduct extensive hyperparameter optimization. After selecting the top-performing model, we enhance it with additional data, including textual entity descriptions, relational information from knowledge graphs, and molecular structure encodings. Our findings illustrate the importance of i) the choice of the underlying language model and ii) a comprehensive hyperparameter optimization for achieving strong extraction performance. Although inclusion of context information yield only minor overall improvements, an ablation study reveals substantial benefits for smaller PLMs when such external data was included during fine-tuning.

</details>


### [21] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)

*Timur Jaganov, John Blake, Julián Villegas, Nicholas Carr*

**Main category:** cs.CL

**Keywords:** Large Language Models, Dynamic Assessment, Grammatical Tutoring, DynaWrite, Language Learning

**Relevance Score:** 9

**TL;DR:** Study explores LLMs for scaling Dynamic Assessment in language learning using a modular app.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how LLMs can improve Dynamic Assessment (DA) in language education.

**Method:** Developed DynaWrite, a microservices-based app, testing 21 LLMs with focus on GPT-4o and neural chat.

**Key Contributions:**

	1. Development of DynaWrite for grammatical tutoring
	2. Performance comparison of 21 LLMs for DA
	3. Demonstration of LLMs enhancing scalability in education

**Result:** GPT-4o and neural chat showed similar error identification; however, GPT-4o provided higher quality dynamic assessment.

**Limitations:** 

**Conclusion:** LLMs have the potential to scale up dynamic assessment, allowing it to be applied to larger groups beyond traditional classroom settings.

**Abstract:** This study investigates the potential for Large Language Models (LLMs) to scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first developed DynaWrite-a modular, microservices-based grammatical tutoring application which supports multiple LLMs to generate dynamic feedback to learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural chat to have the most potential to scale-up DA in the language learning classroom. Further testing of these two candidates found both models performed similarly in their ability to accurately identify grammatical errors in user sentences. However, GPT-4o consistently outperformed neural chat in the quality of its DA by generating clear, consistent, and progressively explicit hints. Real-time responsiveness and system stability were also confirmed through detailed performance testing, with GPT-4o exhibiting sufficient speed and stability. This study shows that LLMs can be used to scale-up dynamic assessment and thus enable dynamic assessment to be delivered to larger groups than possible in traditional teacher-learner settings.

</details>


### [22] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)

*Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung*

**Main category:** cs.CL

**Keywords:** Llama-Nemotron, Reasoning models, Open-source AI

**Relevance Score:** 8

**TL;DR:** Introduction of the Llama-Nemotron series of models enhancing reasoning capabilities with an open license.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an open-source family of models that excel in reasoning while maintaining high inference efficiency and memory usage for enterprise applications.

**Method:** Employs neural architecture search for accelerated inference, knowledge distillation, and reinforcement learning in a post-training phase focused on reasoning.

**Key Contributions:**

	1. Dynamic reasoning toggle for user flexibility between chat and reasoning modes
	2. Release of Llama-Nemotron models under NVIDIA Open Model License
	3. Provision of comprehensive post-training dataset and training codebases

**Result:** Llama-Nemotron models outperform state-of-the-art reasoning models with superior throughput and memory efficiency while allowing dynamic mode switching during inference.

**Limitations:** 

**Conclusion:** The introduction of Llama-Nemotron models fosters open research and development in reasoning-focused AI applications.

**Abstract:** We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>


### [23] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)

*Yingquan Chen, Qianmu Li, Xiaocong Wu, Huifeng Li, Qing Chang*

**Main category:** cs.CL

**Keywords:** steganography, text generation, embedding algorithms, XLNet, character-based diffusion

**Relevance Score:** 4

**TL;DR:** This paper introduces a novel embedding algorithm called the character-based diffusion embedding algorithm (CDEA) to improve the quality of generated steganographic text by effectively leveraging sensitive information properties and utilizing XLNet for better processing of long sequences.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of generating high-quality steganographic text is due to the limitations of existing models and embedding algorithms in effectively handling sensitive information's properties.

**Method:** The paper proposes CDEA, which enhances the selection frequency of high-probability candidate words while reducing low-probability candidates using statistical properties at the character level and power-law distribution methods. The XLNet model is also utilized for better transformation of sensitive information.

**Key Contributions:**

	1. Introduction of the character-based diffusion embedding algorithm (CDEA) for steganographic text generation.
	2. Demonstration of improved quality in generated text through the combination of CDEA and XLNet.
	3. Utilization of power-law distribution methods in candidate word selection.

**Result:** Experimental results show that the CDEA combined with XLNet significantly enhances the quality of steganographic text generation, especially in perceptual-imperceptibility aspects.

**Limitations:** 

**Conclusion:** CDEA provides a more effective approach to generating high-quality steganographic text by smartly leveraging the properties of sensitive information.

**Abstract:** Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.

</details>


### [24] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)

*Xuhui Jiang, Shengjie Ma, Chengjin Xu, Cehao Yang, Liyu Zhang, Jian Guo*

**Main category:** cs.CL

**Keywords:** Large Language Models, synthetic data generation, cross-document associations, knowledge acquisition, context graph

**Relevance Score:** 9

**TL;DR:** The paper presents Synthetic-on-Graph (SoG), a framework for generating synthetic data by leveraging cross-document knowledge associations, improving the efficiency and diversity of learning in Large Language Models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the data inefficiency in LLMs, particularly when working with small, specialized corpora containing limited data.

**Method:** SoG constructs a context graph from the original corpus, capturing cross-document associations and utilizing a graph walk strategy for knowledge-associated sampling. It also integrates Chain-of-Thought (CoT) and Contrastive Clarifying (CC) techniques to enhance reasoning and clarity.

**Key Contributions:**

	1. Introduction of Synthetic-on-Graph framework for synthetic data generation.
	2. Use of cross-document knowledge associations for improved data efficiency.
	3. Demonstrated superior performance in specific LLM tasks compared to SOTA methods.

**Result:** SoG demonstrates superior performance over state-of-the-art methods in a multi-hop document Q&A dataset and comparable results on reading comprehension tasks, indicating improved generalization capabilities.

**Limitations:** 

**Conclusion:** The proposed SoG framework enhances synthetic data generation and provides effective solutions for knowledge acquisition in LLMs, particularly in data-scarce domains.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.

</details>


### [25] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)

*Ayan Sengupta, Yash Goel, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** downscaling, large language models, sustainability, computational efficiency, deployment

**Relevance Score:** 8

**TL;DR:** This paper advocates for downscaling large language models (LLMs) instead of focusing solely on scaling laws, proposing a framework to maintain performance while reducing resource demands.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper challenges the prevalent reliance on neural scaling laws, highlighting the computational inefficiency and environmental impact of traditional scaling methods in LLM development.

**Method:** A holistic framework for downscaling LLMs is proposed, incorporating practical strategies to enhance efficiency and sustainability.

**Key Contributions:**

	1. Proposes a paradigm shift from scaling to downscaling in LLM development.
	2. Outlines practical strategies for efficient LLM deployment.
	3. Highlights the limitations of traditional scaling laws and their impacts.

**Result:** Downscaling strategies can maintain LLM performance while significantly reducing resource usage and making deployment more accessible.

**Limitations:** May not comprehensively address all performance aspects of LLMs compared to scaling.

**Conclusion:** The shift from scaling to downscaling can lead to a more sustainable approach in LLM development, addressing crucial environmental and efficiency concerns.

**Abstract:** We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.

</details>


### [26] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)

*Sijin Sun, Liangbin Zhao, Ming Deng, Xiuju Fu*

**Main category:** cs.CL

**Keywords:** Vessel Traffic Services, large LLM, Text-to-SQL, natural language processing, maritime safety

**Relevance Score:** 5

**TL;DR:** This paper introduces VTS-LLM Agent, a domain-adaptive large LLM designed to improve decision support in Vessel Traffic Services (VTS) by integrating multimodal data and enhancing human interaction through advanced natural language processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of spatiotemporal reasoning and human interaction in existing VTS systems, prompted by increasing traffic complexity and varied data sources.

**Method:** The authors propose a knowledge-augmented Text-to-SQL approach for risk-prone vessel identification, utilizing a curated benchmark dataset that includes a custom schema and diverse query styles. The framework enhances understanding through NER-based reasoning and agent knowledge injection.

**Key Contributions:**

	1. Introduction of the VTS-LLM Agent specifically for maritime traffic management.
	2. Development of a benchmark dataset tailored for the VTS domain.
	3. First empirical evidence that linguistic style variation affects Text-to-SQL performance.

**Result:** VTS-LLM outperforms general-purpose and SQL-specific models in various query styles, demonstrating improved decision support capabilities.

**Limitations:** 

**Conclusion:** The study provides foundational insights for developing natural language interfaces in VTS, highlighting the impact of linguistic styles on performance in Text-to-SQL tasks and exploiting LLMs for enhanced maritime traffic management.

**Abstract:** Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.

</details>


### [27] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)

*Sumit Mamtani, Maitreya Sonawane, Kanika Agarwal, Nishanth Sanjeev*

**Main category:** cs.CL

**Keywords:** Tokenization, Natural Language Processing, Sarcasm Detection, Token-Free Models, ByT5, CANINE

**Relevance Score:** 8

**TL;DR:** This paper evaluates token-free models, ByT5 and CANINE, for sarcasm detection, demonstrating their superiority over token-based models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Tokenization in NLP introduces challenges like vocabulary mismatch, leading to the exploration of token-free modeling alternatives.

**Method:** Two token-free models, ByT5 and CANINE, are fine-tuned and benchmarked on sarcasm detection tasks across social media and news headlines, compared against token-based baselines.

**Key Contributions:**

	1. Evaluation of token-free models for sarcasm detection
	2. Demonstration of improved performance over token-based models
	3. Insights into the effectiveness of models on noisy data

**Result:** ByT5-small and CANINE achieve state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of token-free models in handling tasks in noisy and informal contexts like social media.

**Abstract:** Tokenization is a foundational step in most natural language processing (NLP) pipelines, yet it introduces challenges such as vocabulary mismatch and out-of-vocabulary issues. Recent work has shown that models operating directly on raw text at the byte or character level can mitigate these limitations. In this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of sarcasm detection in both social media (Twitter) and non-social media (news headlines) domains. We fine-tune and benchmark these models against token-based baselines and state-of-the-art approaches. Our results show that ByT5-small and CANINE outperform token-based counterparts and achieve new state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively. These findings underscore the potential of token-free models for robust NLP in noisy and informal domains such as social media.

</details>


### [28] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)

*Jongwook Han, Dongmin Choi, Woojung Song, Eun-Ju Lee, Yohan Jo*

**Main category:** cs.CL

**Keywords:** Language Models, Benchmark, Human-Computer Interaction, Value Orientation, Bias

**Relevance Score:** 9

**TL;DR:** Introduction of the Value Portrait benchmark for assessing LLMs' value orientations based on real-life user interactions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To create a benchmark that addresses biases in current evaluations of language models and enhances ecological validity by reflecting real-world use cases.

**Method:** The benchmark comprises items rated by human subjects based on similarity to their values, allowing for correlations between user ratings and actual value scores.

**Key Contributions:**

	1. Introduction of a new benchmark for evaluating LLMs' values
	2. Focus on real-life user interactions to improve ecological validity
	3. Psychometric validation of assessment items based on human ratings

**Result:** Evaluation of 27 LLMs revealed a priority for Benevolence, Security, and Self-Direction values, with less focus on Tradition, Power, and Achievement, highlighting biases in LLM perceptions of demographic groups.

**Limitations:** Limited to the tested LLMs and may not generalize across all language models or contexts.

**Conclusion:** The Value Portrait benchmark provides a psychometrically validated framework for assessing LLMs, revealing their value orientations and biases in perspective.

**Abstract:** The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage and thus ecological validity. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 27 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.

</details>


### [29] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)

*Lui Yoshida*

**Main category:** cs.CL

**Keywords:** automated essay scoring, large language models, rubric detail, scoring accuracy, TOEFL11 dataset

**Relevance Score:** 7

**TL;DR:** This study evaluates the impact of rubric detail on the accuracy of automated essay scoring using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the necessity of detailed rubrics in automated essay scoring (AES) and their effect on scoring accuracy.

**Method:** Examined different levels of rubric detail (full, simplified, none) across four large language models using the TOEFL11 dataset.

**Key Contributions:**

	1. Demonstrated that simplified rubrics can maintain scoring accuracy in LLM-based AES
	2. Identified the token usage efficiency of simplified rubrics
	3. Highlighted the performance variability across different LLMs with rubric detail levels

**Result:** Three out of four models showed similar scoring accuracy with a simplified rubric, reducing token usage; one model performed worse with detailed rubrics.

**Limitations:** Results could vary with other datasets or different LLMs not tested in this study.

**Conclusion:** Simplified rubrics may suffice for most LLM-based AES applications, but model-specific evaluations are crucial due to varying performance.

**Abstract:** This study investigates the necessity and impact of a detailed rubric in automated essay scoring (AES) using large language models (LLMs). While using rubrics are standard in LLM-based AES, creating detailed rubrics requires substantial ef-fort and increases token usage. We examined how different levels of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11 dataset. Our experiments compared three conditions: a full rubric, a simplified rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5 Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of four models maintained similar scoring accuracy with the simplified rubric compared to the detailed one, while significantly reducing token usage. However, one model (Gemini 1.5 Flash) showed decreased performance with more detailed rubrics. The findings suggest that simplified rubrics may be sufficient for most LLM-based AES applications, offering a more efficient alternative without compromis-ing scoring accuracy. However, model-specific evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>


### [30] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)

*Yijie Jin, Junjie Peng, Xuanchao Lin, Haochen Yuan, Lan Wang, Cangzhi Zheng*

**Main category:** cs.CL

**Keywords:** Multimodal Sentiment Analysis, Multimodal Transformers, Efficiency Optimization

**Relevance Score:** 7

**TL;DR:** This paper introduces an efficient multimodal transformer model, GsiT, utilizing a novel Interlaced Mask mechanism to optimize multimodal sentiment analysis by reducing parameters while improving performance over traditional models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address efficiency concerns in existing multimodal transformers used for sentiment analysis.

**Method:** The authors propose the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT), leveraging a hierarchical modal-wise heterogeneous graph representation and an Interlaced Mask mechanism for optimized parameter sharing.

**Key Contributions:**

	1. Introduction of the Interlaced Mask mechanism for multimodal transformers.
	2. Proposition of GsiT as a significant improvement over traditional MulTs in terms of efficiency and performance.
	3. Integration of GsiT into existing state-of-the-art models showing notable enhancements.

**Result:** GsiT achieves significant performance improvements over traditional multimodal transformers with only one-third of the parameters and has been validated across multiple MSA datasets.

**Limitations:** 

**Conclusion:** The proposed GsiT model demonstrates that hierarchical modal-wise graphs can enhance efficiency in multimodal sentiment analysis without compromising performance.

**Abstract:** Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.

</details>


### [31] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)

*Murtadha Ahmed, Wenbo, Liu yunfeng*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-Context Learning, Attention Mechanism

**Relevance Score:** 8

**TL;DR:** Introducing MateICL, a method to improve in-context learning by managing attention dispersion in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the effectiveness of in-context learning in large language models by addressing the limitations of fixed position length constraints and attention dispersion.

**Method:** The proposed method splits the context into multiple windows processed separately and adds a layer to recalibrate attention weights, focusing on query tokens as the number of demonstrations increases.

**Key Contributions:**

	1. Introduction of MateICL to mitigate attention dispersion in LLMs
	2. Ability to effectively utilize larger contexts for ICL performance
	3. Performance improvement without reliance on external retrieval models

**Result:** MateICL demonstrates improved in-context learning performance compared to retrieval-based models without requiring an external retrieval system, even in resource-constrained settings.

**Limitations:** 

**Conclusion:** MateICL effectively supports larger contexts while maintaining attention efficiency, enhancing in-context learning capabilities of LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at https://github.com/amurtadha/MateICL.

</details>


### [32] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)

*Chebrolu Niranjan, Kokil Jaidka, Gerard Christopher Yeo*

**Main category:** cs.CL

**Keywords:** steering vectors, alignment mechanisms, language models, transformer interventions, context complexity

**Relevance Score:** 8

**TL;DR:** This paper evaluates the limitations of steering vectors in aligning language model behavior at inference, highlighting their effectiveness for specific tasks but questioning their general-purpose applicability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the limitations of steering vectors as methods for aligning language model behavior during inference.

**Method:** The study utilizes transformer hook interventions and antonym-based function vectors to evaluate how prompt structure and context complexity affect steering effectiveness.

**Key Contributions:**

	1. Evaluation of steering vectors for alignment in language models
	2. Methodological framework for assessing alignment mechanisms
	3. Insights into the limitations of steering vectors in complex scenarios

**Result:** The findings suggest that while steering vectors are effective for certain alignment tasks like value alignment, they lack robustness for general-purpose alignment in complex scenarios.

**Limitations:** Limited applicability of steering vectors for general-purpose alignment in complex scenarios.

**Conclusion:** The paper establishes a methodological framework aimed at guiding future research into the steering capabilities of reasoning models.

**Abstract:** Steering vectors are a promising approach to aligning language model behavior at inference time. In this paper, we propose a framework to assess the limitations of steering vectors as alignment mechanisms. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. Our findings indicate that steering vectors are promising for specific alignment tasks, such as value alignment, but may not provide a robust foundation for general-purpose alignment in LLMs, particularly in complex scenarios. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.

</details>


### [33] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)

*Mahdi Dhaini, Ege Erdogan, Nils Feldhus, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** explainability, fairness, machine learning, gender disparities, feature attribution

**Relevance Score:** 9

**TL;DR:** This paper addresses the gender disparity in the performance of explanation methods used in machine learning, highlighting issues in faithfulness, robustness, and complexity across multiple tasks and language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked aspect of fairness in explanation methods, especially concerning performance discrepancies across different gender subgroups.

**Method:** The authors evaluate widely used post-hoc feature attribution methods across three tasks and five language models, assessing their faithfulness, robustness, and complexity.

**Key Contributions:**

	1. Identification of gender disparity in post-hoc feature attribution methods
	2. Establishing importance of addressing explanation disparities in machine learning models
	3. Recommendations for incorporating fairness of explanations into regulatory frameworks.

**Result:** The study finds significant gender disparities in the explanation methods, which persist regardless of the models being trained on unbiased datasets.

**Limitations:** The study focuses only on gender disparities without exploring other demographic factors.

**Conclusion:** The findings suggest that fairness in explanations is critical to avoid biased outcomes in high-stakes contexts and should be part of regulatory frameworks alongside overall model fairness.

**Abstract:** While research on applications and evaluations of explanation methods continues to expand, fairness of the explanation methods concerning disparities in their performance across subgroups remains an often overlooked aspect. In this paper, we address this gap by showing that, across three tasks and five language models, widely used post-hoc feature attribution methods exhibit significant gender disparity with respect to their faithfulness, robustness, and complexity. These disparities persist even when the models are pre-trained or fine-tuned on particularly unbiased datasets, indicating that the disparities we observe are not merely consequences of biased training data. Our results highlight the importance of addressing disparities in explanations when developing and applying explainability methods, as these can lead to biased outcomes against certain subgroups, with particularly critical implications in high-stakes contexts. Furthermore, our findings underscore the importance of incorporating the fairness of explanations, alongside overall model fairness and explainability, as a requirement in regulatory frameworks.

</details>


### [34] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)

*Mahdi Dhaini, Kafaite Zahra Hussain, Efstratios Zaradoukas, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** explainability, NLP, feature attribution, XAI, EvalxNLP

**Relevance Score:** 9

**TL;DR:** EvalxNLP is a Python framework for benchmarking explainability methods for NLP models, integrating multiple techniques to enhance interpretability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for frameworks that help stakeholders select appropriate explanations tailored to their specific use cases in NLP.

**Method:** EvalxNLP integrates eight widely recognized explainability techniques, allowing users to generate and evaluate explanations based on properties like faithfulness, plausibility, and complexity. It also provides interactive LLM-based textual explanations.

**Key Contributions:**

	1. Development of EvalxNLP as a benchmarking tool for explainability methods in NLP
	2. Integration of interactive LLM-based explanations
	3. High user satisfaction demonstrated through human evaluation

**Result:** Human evaluations show high user satisfaction with EvalxNLP, indicating its effectiveness as a benchmarking framework for explanation methods across user groups.

**Limitations:** 

**Conclusion:** EvalxNLP is positioned to democratize explainability tools and support the advancement of XAI techniques in NLP.

**Abstract:** As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.

</details>


### [35] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)

*Wei Han, Hui Chen, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** multimodal learning, matching scores, recommendation systems, review helpfulness, computational efficiency

**Relevance Score:** 6

**TL;DR:** PREMISE is a new architecture for matching-based learning in multimodal fields, focusing on improving performance for the multimodal review helpfulness task.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of multimodal learning tasks while reducing computational costs, particularly for review helpfulness recommendations.

**Method:** The PREMISE architecture computes multi-scale and multi-field representations, filters out duplicated semantics, and generates matching scores as feature vectors for downstream recommendation tasks.

**Key Contributions:**

	1. Introduction of the PREMISE architecture for multimodal learning
	2. Improved performance on the multimodal review helpfulness task
	3. Reduced computational cost compared to traditional approaches

**Result:** PREMISE significantly outperforms state-of-the-art fusion-based methods in multimodal tasks with high context matching to target queries, while also requiring less computational resources.

**Limitations:** 

**Conclusion:** The PREMISE framework provides an effective alternative for multimodal representation learning, demonstrating improved efficiency and performance in real-world applications.

**Abstract:** We present PREMISE (PREdict with Matching ScorEs), a new architecture for the matching-based learning in the multimodal fields for the multimodal review helpfulness (MRHP) task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.

</details>


### [36] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)

*Xuan Li, Zhe Yin, Xiaodong Gu, Beijun Shen*

**Main category:** cs.CL

**Keywords:** privacy, LLMs, desensitization, anti-adversarial learning, NLP

**Relevance Score:** 9

**TL;DR:** The paper introduces PromptObfus, a method for desensitizing prompts to protect user privacy when using LLMs by perturbing sensitive words while maintaining task performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs raises privacy concerns as prompts may expose sensitive information, motivating the need for effective desensitization methods.

**Method:** PromptObfus employs 'anti-adversarial' learning, treating prompt desensitization as a masked language modeling task where privacy-sensitive terms in prompts are replaced by a [MASK] token. A desensitization model is trained to propose replacements for these tokens, guided by gradient feedback from a surrogate model.

**Key Contributions:**

	1. Introduction of PromptObfus for LLM prompt desensitization
	2. Application of anti-adversarial learning to protect privacy
	3. Demonstration of effectiveness through empirical results on NLP tasks

**Result:** The approach was tested on three NLP tasks, demonstrating that PromptObfus effectively prevents privacy inference while maintaining performance.

**Limitations:** The method may still require further validation across diverse NLP tasks and scenarios.

**Conclusion:** PromptObfus is a practical solution for preserving user privacy in LLM interactions, balancing desensitization with task performance.

**Abstract:** With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.

</details>


### [37] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)

*Svenja Kenneweg, Jörg Deigmöller, Julian Eggert, Philipp Cimiano*

**Main category:** cs.CL

**Keywords:** temporal adverbials, probabilistic distributions, factorized model

**Relevance Score:** 4

**TL;DR:** This paper presents a factorized model for vague temporal adverbials that captures their semantics as probabilistic distributions, offering improved simplicity and extendability compared to traditional models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To capture the nuanced semantics of vague temporal adverbials in a context-aware manner.

**Method:** The authors introduce a factorized model that composes vague adverbials with event-specific distributions, fitting parameters using native speaker judgments on the applicability of these adverbials to past events.

**Key Contributions:**

	1. Introduction of a factorized model for vague temporal adverbials
	2. Demonstration of improved simplicity and extendability compared to traditional models
	3. Parameter fitting using native speaker judgments for contextual relevance.

**Result:** The model aligns closely in predictive power with a non-factorized Gaussian model but is simpler and allows for better extendability.

**Limitations:** 

**Conclusion:** The factorized model is advantageous due to its simplicity and Occam's razor principles, despite similar predictive performance.

**Abstract:** Vague temporal adverbials, such as recently, just, and a long time ago, describe the temporal distance between a past event and the utterance time but leave the exact duration underspecified. In this paper, we introduce a factorized model that captures the semantics of these adverbials as probabilistic distributions. These distributions are composed with event-specific distributions to yield a contextualized meaning for an adverbial applied to a specific event. We fit the model's parameters using existing data capturing judgments of native speakers regarding the applicability of these vague temporal adverbials to events that took place a given time ago. Comparing our approach to a non-factorized model based on a single Gaussian distribution for each pair of event and temporal adverbial, we find that while both models have similar predictive power, our model is preferable in terms of Occam's razor, as it is simpler and has better extendability.

</details>


### [38] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)

*Shang Wang, Huanrong Tang, Jianquan Ouyang*

**Main category:** cs.CL

**Keywords:** neural architecture search, Transformer, multi-objective genetic algorithm

**Relevance Score:** 6

**TL;DR:** The paper introduces a neural architecture search method using Transformers to improve translation results through multi-objective genetic algorithms, incorporating perplexity as an evaluation metric.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To find better neural network structures for translation tasks beyond traditional BLEU score evaluations.

**Method:** The authors developed a method that searches cross multihead attention computation ways for varying encoder and decoder configurations and utilized a multi-objective genetic algorithm to iteratively enhance neural networks.

**Key Contributions:**

	1. Introduction of a novel neural architecture search method for Transformers
	2. Inclusion of perplexity as an auxiliary metric in model evaluation
	3. Demonstrated superiority of searched models over baseline translation models

**Result:** The neural network structures identified by the proposed algorithm outperformed all baseline models significantly.

**Limitations:** 

**Conclusion:** Incorporating perplexity as an auxiliary evaluation metric led to better model discovery compared to relying solely on BLEU scores.

**Abstract:** This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.

</details>


### [39] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)

*Sheikh Samit Muhaimin, Spyridon Mastorakis*

**Main category:** cs.CL

**Keywords:** Large Language Models, adversarial attacks, deployable defenses, natural language processing, machine learning

**Relevance Score:** 9

**TL;DR:** This study presents a defense framework for Large Language Models that enables them to independently recognize and filter adversarial inputs without the need for retraining. It uses prompt filtering and summarization of adversarial research literature to enhance LLMs' resistance against malicious inputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerability of Large Language Models to adversarial assaults and manipulative prompts, providing a defense mechanism that does not require computationally expensive retraining.

**Method:** The proposed framework consists of a prompt filtering module using NLP techniques like zero-shot classification, keyword analysis, and encoded content detection, along with a summarization module that provides context-aware knowledge about adversarial literature.

**Key Contributions:**

	1. Development of a framework that does not require retraining for LLM defense
	2. Integration of NLP techniques for prompt filtering
	3. Enhanced jailbreak resistance and refusal rate through contextual knowledge

**Result:** The integrated technique achieved a 98.71% success rate in identifying harmful patterns and manipulative language structures, significantly increasing the LLM's resistance to adversarial misuse without compromising response quality.

**Limitations:** 

**Conclusion:** This framework serves as an effective, low-cost alternative to traditional retraining-based defenses, showcasing how LLMs can self-defend against adversarial attacks.

**Abstract:** The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.

</details>


### [40] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)

*Svenja Kenneweg, Jörg Deigmöller, Philipp Cimiano, Julian Eggert*

**Main category:** cs.CL

**Keywords:** Temporal References, Natural Language Understanding, Question Answering, Large Language Models, Benchmark Dataset

**Relevance Score:** 8

**TL;DR:** Introduction of TRAVELER, a synthetic benchmark dataset for evaluating models on temporal reference resolution in question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited systematic evaluation of models' ability to resolve different types of temporal references in Natural Language Understanding.

**Method:** TRAVELER consists of a variety of questions with explicit, implicit, and vague temporal references evaluated through a Question Answering paradigm.

**Key Contributions:**

	1. Introduction of a novel synthetic benchmark dataset for temporal reference evaluation.
	2. Evaluation of LLM performance across different types of temporal references.
	3. Public availability of the benchmark for further research.

**Result:** Assessment of four state-of-the-art LLMs on 3,300 questions showed that performance deteriorates with larger event sets and less explicit temporal references, particularly for vague questions.

**Limitations:** Performance of models is notably low for vague temporal references and with increased event set sizes.

**Conclusion:** TRAVELER provides a valuable tool for evaluating temporal reference resolution in models, highlighting significant performance gaps that need to be addressed.

**Abstract:** Understanding and resolving temporal references is essential in Natural Language Understanding as we often refer to the past or future in daily communication. Although existing benchmarks address a system's ability to reason about and resolve temporal references, systematic evaluation of specific temporal references remains limited. Towards closing this gap, we introduce TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering paradigm and consists of questions involving temporal references with the corresponding correct answers. TRAVELER assesses models' abilities to resolve explicit, implicit relative to speech time, and vague temporal references. Beyond investigating the performance of state-of-the-art LLMs depending on the type of temporal reference, our benchmark also allows evaluation of performance in relation to the length of the set of events. For the category of vague temporal references, ground-truth answers were established via human surveys on Prolific, following a procedure similar to the one from Kenneweg et al. To demonstrate the benchmark's applicability, we evaluate four state-of-the-art LLMs using a question-answering task encompassing 3,300 questions. Our findings show that while the benchmarked LLMs can answer questions over event sets with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event set length and when temporal references get less explicit. Notably, the vague question category exhibits the lowest performance across all models.   The benchmark is publicly available at: https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>


### [41] [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359)

*Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang*

**Main category:** cs.CL

**Keywords:** scientific summarization, large language models, evaluation metrics

**Relevance Score:** 9

**TL;DR:** This paper assesses the use of pretrained LLMs for scientific summarization, introduces the Facet-aware Metric for evaluation, and curates a new dataset with facet-level annotations.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The inadequacy of traditional evaluation methods for scientific summarization and the need for effective metrics driven by LLMs.

**Method:** Introducing the Facet-aware Metric (FM) for evaluation of scientific summaries using LLMs and curating a Facet-based scientific summarization Dataset (FD).

**Key Contributions:**

	1. Introduction of the Facet-aware Metric (FM) for summarization evaluation
	2. Creation of the Facet-based summarization Dataset (FD)
	3. Demonstration that smaller models can effectively compete with LLMs in scientific contexts

**Result:** FM provides a more logical evaluation method for scientific summaries; fine-tuned smaller models can match the performance of LLMs in scientific contexts.

**Limitations:** Identifies limitations of LLMs in scientific domain in-context information learning.

**Conclusion:** While LLMs show limitations in in-context learning for scientific domains, the FM evaluation approach is promising for accurate summarization assessments.

**Abstract:** The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.

</details>


### [42] [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://arxiv.org/abs/2404.18624)

*Letitia Parcalabescu, Anette Frank*

**Main category:** cs.CL

**Keywords:** vision and language models, self-consistency, multimodal tasks, explanation generation, benchmarking

**Relevance Score:** 8

**TL;DR:** This paper investigates the reliance of vision and language model decoders on input modalities when generating answers and explanations, finding a notable difference in their usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how VLM decoders utilize vision and text inputs differently in generating answers versus explanations.

**Method:** The study extends existing unimodal tests to evaluate VLM decoders, measuring their self-consistency in post-hoc and CoT explanation settings.

**Key Contributions:**

	1. Investigating the modality reliance in VLM decoders for explanation vs. answer generation
	2. Benchmarking VLM decoders on the VALSE benchmark
	3. Highlighting the differences in self-consistency between VLMs and LLMs

**Result:** Most VLMs exhibit lower self-consistency than LLMs, with text contributions being more significant than image contributions across tasks; however, image contributions are more critical for explanation generation than for answer generation.

**Limitations:** The tested models still struggle with many phenomena as per the VALSE benchmark.

**Conclusion:** VLM decoders continue to face challenges with various phenomena as per the VALSE benchmark, indicating the need for further improvement in multimodal integration.

**Abstract:** Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.

</details>


### [43] [REFFLY: Melody-Constrained Lyrics Editing Model](https://arxiv.org/abs/2409.00292)

*Songyan Zhao, Bingxuan Li, Yufei Tian, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** melody-to-lyric generation, lyric revision, music technology, natural language generation, AI in music

**Relevance Score:** 4

**TL;DR:** REFFLY is a novel framework for editing and generating melody-aligned lyrics, allowing flexible inputs and outperforming existing models in various music-related tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing melody-to-lyric generation methods primarily create lyrics from scratch; revision offers a more flexible and practical alternative.

**Method:** REFFLY uses a curated dataset of melody-aligned lyrics to train a revision module that transforms plain text into lyrics, implementing training-free heuristics for semantic and musical consistency.

**Key Contributions:**

	1. Introduction of REFFLY, the first revision framework for melody-aligned lyrics
	2. Utilization of a curated dataset for training
	3. Implementation of training-free heuristics for better revision quality

**Result:** REFFLY outperforms strong baselines (e.g., Lyra and GPT-4) by 25% in both musicality and text quality across various tasks like lyrics generation and song translation.

**Limitations:** 

**Conclusion:** The results demonstrate the effectiveness of REFFLY in generating and editing lyrics that align with given melodies and hold semantic and musical integrity.

**Abstract:** Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision, editing plain text draft to fit it into the melody, offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the melody intact), or style transfer (adapting lyrics to different genres). This paper introduces REFFLY (REvision Framework For LYrics), the first revision framework for editing and generating melody-aligned lyrics. We train the lyric revision module using our curated synthesized melody-aligned lyrics dataset, enabling it to transform plain text into lyrics that align with a given melody. To further enhance the revision ability, we propose training-free heuristics aimed at preserving both semantic meaning and musical consistency throughout the editing process. Experimental results demonstrate the effectiveness of REFFLY across various tasks (e.g. lyrics generation, song translation), showing that our model outperforms strong baselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both musicality and text quality.

</details>


### [44] [Does Self-Attention Need Separate Weights in Transformers?](https://arxiv.org/abs/2412.00359)

*Md Kowsher, Nusrat Jahan Prottasha, Chun-Nam Yu, Ozlem Ozmen Garibay, Niloofar Yousefi*

**Main category:** cs.CL

**Keywords:** self-attention, BERT, parameter reduction, GLUE, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents a new shared weight self-attention-based BERT model that reduces parameter size and training time while improving prediction accuracy on small GLUE tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational complexity of self-attention and improve its application in sequential data processing.

**Method:** The proposed model uses a single weight matrix for Key, Value, and Query representations, significantly reducing parameter size and training time.

**Key Contributions:**

	1. Introduction of a shared weight self-attention mechanism for BERT
	2. Reduction of training parameters and time by over half
	3. Improved generalization on noisy and out-of-domain data

**Result:** Achieves a 66.53% reduction in parameter size within the attention block and improved accuracy on the GLUE dataset compared to standard models.

**Limitations:** 

**Conclusion:** The shared weight self-attention method enhances efficiency and prediction accuracy, offering a promising alternative to traditional self-attention mechanisms.

**Abstract:** The success of self-attention lies in its ability to capture long-range dependencies and enhance context understanding, but it is limited by its computational complexity and challenges in handling sequential data with inherent directionality. This work introduces a shared weight self-attention-based BERT model that only learns one weight matrix for (Key, Value, and Query) representations instead of three individual matrices for each of them. Our shared weight attention reduces the training parameter size by more than half and training time by around one-tenth. Furthermore, we demonstrate higher prediction accuracy on small tasks of GLUE over the BERT baseline and in particular a generalization power on noisy and out-of-domain data. Experimental results indicate that our shared self-attention method achieves a parameter size reduction of 66.53% in the attention block. In the GLUE dataset, the shared weight self-attention-based BERT model demonstrates accuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric, and pairwise attention-based BERT models, respectively. The model and source code are available at Anonymous.

</details>


### [45] [When Every Token Counts: Optimal Segmentation for Low-Resource Language Models](https://arxiv.org/abs/2412.06926)

*Bharath Raj, Garvit Suri, Vikrant Dewangan, Raghav Sonavane*

**Main category:** cs.CL

**Keywords:** tokenization, Byte-Pair Encoding, Natural Language Processing, multilingual applications, low-resource languages

**Relevance Score:** 8

**TL;DR:** The paper investigates the effectiveness of optimal Byte-Pair Encoding (BPE) configurations in tokenization for NLP, showcasing improvements in token count and model performance, especially for smaller models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the optimality of subword tokenization methods and their impact on model performance across scales and languages.

**Method:** Extensive experiments evaluating tokenization performance using various intrinsic and extrinsic tasks, including generation and classification.

**Key Contributions:**

	1. Demonstrated significant reductions in token count with optimal BPE configurations
	2. Provided insights into tokenization performance across multiple tasks
	3. Highlighted the value of compression-optimized methods for diverse language applications

**Result:** An optimal BPE configuration significantly reduces token count compared to greedy segmentation, leading to performance improvements, particularly in smaller models.

**Limitations:** 

**Conclusion:** Compression-optimized tokenization strategies show promise for enhancing NLP applications in multilingual and low-resource contexts, suggesting a new research direction.

**Abstract:** Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.

</details>


### [46] [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)

*Meihao Fan, Ju Fan, Nan Tang, Lei Cao, Guoliang Li, Xiaoyong Du*

**Main category:** cs.CL

**Keywords:** Tabular Question Answering, Data Preparation, Multi-Agent Framework, Natural Language Processing, Large Language Models

**Relevance Score:** 8

**TL;DR:** AutoPrep is an LLM-based multi-agent framework designed for question-aware data preparation in tabular question answering, improving accuracy and relevance in responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to efficiently extract meaningful insights from structured data via natural language questions, addressing the challenges of existing data preparation methods.

**Method:** The framework uses three components: Planner for logical planning, Programmer for translating plans into code, and Executor for executing table processing tasks, all enhanced by a novel reasoning mechanism.

**Key Contributions:**

	1. Introduction of AutoPrep as a multi-agent framework for data preparation
	2. Development of a Chain-of-Clauses reasoning mechanism
	3. Innovative tool-augmented code generation method

**Result:** AutoPrep demonstrates improved performance in question-aware data preparation tasks, leading to more accurate answers to natural language questions about tables.

**Limitations:** 

**Conclusion:** The study concludes that a multi-agent approach tailored to specific data prep tasks allows for better handling of question-aware scenarios.

**Abstract:** Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...

</details>


### [47] [ICLR: In-Context Learning of Representations](https://arxiv.org/abs/2501.00070)

*Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, Hidenori Tanaka*

**Main category:** cs.CL

**Keywords:** large language models, contextual semantics, graph tracing, semantic reorganization, in-context learning

**Relevance Score:** 9

**TL;DR:** This paper explores how large language models (LLMs) adapt their semantic representations when provided with context-specific examples, particularly in a graph tracing task, revealing a shift from pretraining semantics to context-adopted semantics as context size increases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The influence of pretraining data on LLM representations and the potential for these models to reorganize semantics based on in-context learning.

**Method:** A graph tracing task is introduced where nodes represent concepts learned during training, and connectivity is based on a defined structure. The study analyzes model representations as the context size varies and uses energy minimization analogies to explain the findings.

**Key Contributions:**

	1. Demonstrates the reorganization of LLM representations with increased context size.
	2. Establishes a link between semantic representations and graph topology through energy minimization.
	3. Provides insights into in-context learning and its effects on pretrained semantics.

**Result:** As context size scales, models demonstrate a significant re-organization of semantic representations from pretrained to context-specified structures. However, in cases with correlated reference concepts, the pretrained semantics still prevail.

**Limitations:** 

**Conclusion:** Scaling context size can lead to flexible re-organization of representations in LLMs, which may enable the discovery of new capabilities in how these models interpret and respond to context.

**Abstract:** Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy "graph tracing" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.

</details>


### [48] [Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention](https://arxiv.org/abs/2501.06382)

*Mumin Jia, Jairo Diaz-Rodriguez*

**Main category:** cs.CL

**Keywords:** self-attention, spontaneous thought, human cognition, LLM, Token Priority Graphs

**Relevance Score:** 7

**TL;DR:** The paper explores spontaneous topic changes in self-attention models, contrasting them with human cognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how self-attention models handle spontaneous topic changes in comparison to human thought, highlighting the lack of spontaneity in AI.

**Method:** The paper analyzes a simplified, single-layer self-attention model theoretically, defining topics via Token Priority Graphs (TPGs) and empirically validating the findings in modern LLMs.

**Key Contributions:**

	1. Characterization of spontaneous topic changes in self-attention architectures
	2. Empirical validation in modern LLMs
	3. Identification of conditions under which AI can exhibit topic shifts.

**Result:** The model maintains token priority orders, indicates that spontaneous changes occur under specific conditions, and shows that longer contexts reduce the likelihood of these changes, unlike human cognition.

**Limitations:** 

**Conclusion:** There is a fundamental disparity between human cognition and AI behavior in terms of spontaneous topic changes, with implications for understanding AI limitations.

**Abstract:** Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.

</details>


### [49] [TableMaster: A Recipe to Advance Table Understanding with Language Models](https://arxiv.org/abs/2501.19378)

*Lang Cao, Hanbing Liu*

**Main category:** cs.CL

**Keywords:** table understanding, language models, adaptive reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces TableMaster, a framework designed to enhance language models' understanding of tabular data by addressing key challenges in table semantics, reasoning, and numerical accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language models' (LMs) performance on table understanding, tackling inherent challenges in structured relational data.

**Method:** The authors propose TableMaster, which extracts relevant table content and uses adaptive reasoning to adjust between textual and symbolic reasoning based on the query.

**Key Contributions:**

	1. Introduction of TableMaster framework for table understanding.
	2. Adaptive reasoning approach for flexible processing of queries.
	3. Enhanced extraction and semantic context for tabular data.

**Result:** TableMaster achieves an accuracy of 78.13% on the WikiTQ dataset using GPT-4o-mini, outperforming existing methods.

**Limitations:** 

**Conclusion:** The framework effectively addresses the identified challenges in table understanding, proving beneficial for language models.

**Abstract:** Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.

</details>


### [50] [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/abs/2502.01976)

*Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao*

**Main category:** cs.CL

**Keywords:** large language models, inference efficiency, token-level routing, resource-constrained applications, collaborative inference

**Relevance Score:** 8

**TL;DR:** The CITER framework enhances inference efficiency in large language models by using a token-level routing strategy between small and large models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the high computational costs during inference of large language models, which limits their usability in resource-constrained settings.

**Method:** A novel collaborative inference framework called CITER, which implements token-level routing between small and large language models, routing non-critical tokens to small models and critical tokens to large models. Router training is formulated as policy optimization with rewards based on prediction quality and inference costs.

**Key Contributions:**

	1. Introduced a token-level routing strategy for efficient collaboration between small and large language models.
	2. Formulated router training as a policy optimization problem to balance prediction quality and cost efficiency.
	3. Developed a shortcut for reward evaluation that reduces the computational overhead of the routing process.

**Result:** CITER significantly lessens inference costs while maintaining high-quality generation across five benchmark datasets.

**Limitations:** 

**Conclusion:** CITER presents a viable solution for efficient language model deployment in real-time and resource-constrained applications.

**Abstract:** Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs \& LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.

</details>


### [51] [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)

*Sudesh Ramesh Bhagat, Ibne Farabi Shihab, Anuj Sharma*

**Main category:** cs.CL

**Keywords:** deep learning, expert agreement, LLM, crash analysis, NLP

**Relevance Score:** 9

**TL;DR:** This study examines the relationship between the accuracy of deep learning models and expert agreement in classifying crash narratives, finding that models with higher accuracy often have lower agreement with experts, while LLMs align more closely with expert judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of various deep learning models and LLMs in accurately classifying crash narratives and their alignment with expert assessments.

**Method:** Evaluated five deep learning models (including BERT variants and a zero-shot classifier) and extended the analysis to four large language models (GPT-4, LLaMA 3, Qwen, Claude) using Cohen's Kappa, PCA, and SHAP analysis to explore misclassifications and model-expert agreement.

**Key Contributions:**

	1. Demonstrated an inverse relationship between DL model accuracy and expert agreement.
	2. Highlighted the stronger alignment of LLMs with expert assessments.
	3. Proposed the need for incorporating expert agreement into model evaluation metrics.

**Result:** The study found an inverse relationship between model accuracy and expert agreement; LLMs showed greater alignment with expert classifications despite being less accurate than some DL models.

**Limitations:** The study primarily focuses on crash narratives and may not be generalizable to other safety-critical contexts.

**Conclusion:** Accuracy alone is not a sufficient metric for safety-critical NLP tasks, suggesting the incorporation of expert agreement into evaluation frameworks and highlighting the utility of LLMs as interpretable tools in crash analysis.

**Abstract:** This study investigates the relationship between deep learning (DL) model accuracy and expert agreement in classifying crash narratives. We evaluate five DL models -- including BERT variants, USE, and a zero-shot classifier -- against expert labels and narratives, and extend the analysis to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal an inverse relationship: models with higher technical accuracy often show lower agreement with human experts, while LLMs demonstrate stronger expert alignment despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis (PCA) to quantify and visualize model-expert agreement, and employ SHAP analysis to explain misclassifications. Results show that expert-aligned models rely more on contextual and temporal cues than location-specific keywords. These findings suggest that accuracy alone is insufficient for safety-critical NLP tasks. We argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of LLMs as interpretable tools in crash analysis pipelines.

</details>
