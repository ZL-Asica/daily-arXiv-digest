# 2025-07-14

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 13]

- [cs.CL](#cs.CL) [Total: 65]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study](https://arxiv.org/abs/2507.08002)

*Karisa Parkington, Bazen G. Teferra, Marianne Rouleau-Tang, Argyrios Perivolaris, Alice Rueda, Adam Dubrowski, Bill Kapralos, Reza Samavi, Andrew Greenshaw, Yanbo Zhang, Bo Cao, Yuqi Wu, Sirisha Rambhatla, Sridhar Krishnan, Venkat Bhat*

**Main category:** cs.HC

**Keywords:** thematic analysis, large language models, mental health, qualitative research, healthcare

**Relevance Score:** 9

**TL;DR:** This study compares LLM-based thematic analysis using GPT-4o with traditional human analysis in mental health interviews, focusing on efficiency and coding quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Thematic analysis is resource-intensive, limiting its use in large healthcare studies, prompting an exploration of LLM capabilities to automate this process.

**Method:** The study utilized OpenAI's GPT-4o model with the RISEN prompt engineering framework to analyze transcripts from a stress-reduction trial, comparing results to human analysis conducted via Dedoose.

**Key Contributions:**

	1. Comparison of LLMs and human analysis in thematic analysis
	2. Introduction of the RISEN prompt engineering framework
	3. Findings on coding efficiency and quality differences between LLMs and human researchers.

**Result:** LLMs achieved coding saturation with fewer transcripts than human analysis, and demonstrated strong inter-rater reliability; however, humans outperformed LLMs in thematic depth and inductive code creation.

**Limitations:** LLMs produced less detailed analysis compared to humans and applied fewer codes per excerpt.

**Conclusion:** LLM-based thematic analysis is a promising cost-effective alternative that can enhance qualitative research in mental healthcare, provided it is paired with human oversight to ensure depth and accuracy.

**Abstract:** Thematic analysis provides valuable insights into participants' experiences through coding and theme development, but its resource-intensive nature limits its use in large healthcare studies. Large language models (LLMs) can analyze text at scale and identify key content automatically, potentially addressing these challenges. However, their application in mental health interviews needs comparison with traditional human analysis. This study evaluates out-of-the-box and knowledge-base LLM-based thematic analysis against traditional methods using transcripts from a stress-reduction trial with healthcare workers. OpenAI's GPT-4o model was used along with the Role, Instructions, Steps, End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human analysis in Dedoose. Each approach developed codes, noted saturation points, applied codes to excerpts for a subset of participants (n = 20), and synthesized data into themes. Outputs and performance metrics were compared directly. LLMs using the RISEN framework developed deductive parent codes similar to human codes, but humans excelled in inductive child code development and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99). The out-of-the-box LLM identified a comparable number of excerpts to human researchers, showing strong inter-rater reliability (K = 0.84), though the knowledge-based LLM produced fewer excerpts. Human excerpts were longer and involved multiple codes per excerpt, while LLMs typically applied one code. Overall, LLM-based thematic analysis proved more cost-effective but lacked the depth of human analysis. LLMs can transform qualitative analysis in mental healthcare and clinical research when combined with human oversight to balance participant perspectives and research resources.

</details>


### [2] [A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages](https://arxiv.org/abs/2507.08003)

*Kayhan Latifzadeh, Jacek Gwizdka, Luis A. Leiva*

**Main category:** cs.HC

**Keywords:** user attention, SERPs, eye tracking, purchasing behavior, dataset

**Relevance Score:** 4

**TL;DR:** The paper introduces a comprehensive dataset for studying user attention and purchasing behavior on Search Engine Result Pages (SERPs), utilizing eye tracking for accurate data collection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of understanding user attention and purchasing behavior, overcoming limitations of previous research methods that relied on mouse movements and biased self-reports.

**Method:** The study employs eye tracking to gather objective data on user attention, resulting in a dataset of 2,776 queries from Google SERPs with various associated data types.

**Key Contributions:**

	1. Comprehensive dataset on user attention using eye tracking
	2. Includes various data types for in-depth analysis
	3. Baseline experiments to stimulate future research
	4. Addresses limitations of previous methodologies

**Result:** The dataset includes HTML files, rendered SERP screenshots, eye movement and mouse movement data, and bounding boxes for advertisements, along with baseline classification experiments.

**Limitations:** The study is limited to data collected from 47 participants, which may affect the generalizability of the findings.

**Conclusion:** The dataset provides a rich resource for future research on user interaction with SERPs and sets the stage for innovative classification tasks.

**Abstract:** We contribute a comprehensive dataset to study user attention and purchasing behavior on Search Engine Result Pages (SERPs). Previous work has relied on mouse movements as a low-cost large-scale behavioral proxy but also has relied on self-reported ground-truth labels, collected at post-task, which can be inaccurate and prone to biases. To address this limitation, we use an eye tracker to construct an objective ground-truth of continuous visual attention. Our dataset comprises 2,776 transactional queries on Google SERPs, collected from 47 participants, and includes: (1) HTML source files, with CSS and images; (2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data; (5) bounding boxes of direct display and organic advertisements; and (6) scripts for further preprocessing the data. In this paper we provide an overview of the dataset and baseline experiments (classification tasks) that can inspire researchers about the different possibilities for future work.

</details>


### [3] [SSSUMO: Real-Time Semi-Supervised Submovement Decomposition](https://arxiv.org/abs/2507.08028)

*Evgenii Rudakov, Jonathan Shock, Otto Lappi, Benjamin Ultan Cowley*

**Main category:** cs.HC

**Keywords:** Semi-supervised learning, Submovement decomposition, Human-computer interaction, Motor control, Deep learning

**Relevance Score:** 8

**TL;DR:** This paper presents SSSUMO, a semi-supervised deep learning model for accurate and fast submovement decomposition, enhancing motor control analysis and applicable in HCI and rehabilitation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in submovement analysis related to accuracy, cost, and the scarcity of labeled data.

**Method:** The authors utilize a semi-supervised learning framework that combines synthetic data generation with iterative adaptation to unlabeled human movement data, implemented in a fully convolutional architecture.

**Key Contributions:**

	1. Introduction of SSSUMO, a novel semi-supervised learning approach for submovement decomposition.
	2. Demonstrated real-time performance in processing human movement data.
	3. Availability of training and benchmarking source code along with pre-trained model weights.

**Result:** SSSUMO achieves state-of-the-art performance in accuracy and real-time processing speed on various human motion datasets, outperforming existing methods, especially in high-noise conditions.

**Limitations:** 

**Conclusion:** The findings suggest that SSSUMO can significantly advance applications in human-computer interaction, rehabilitation medicine, and motor control studies due to its enhanced performance.

**Abstract:** This paper introduces a SSSUMO, semi-supervised deep learning approach for submovement decomposition that achieves state-of-the-art accuracy and speed. While submovement analysis offers valuable insights into motor control, existing methods struggle with reconstruction accuracy, computational cost, and validation, due to the difficulty of obtaining hand-labeled data. We address these challenges using a semi-supervised learning framework. This framework learns from synthetic data, initially generated from minimum-jerk principles and then iteratively refined through adaptation to unlabeled human movement data. Our fully convolutional architecture with differentiable reconstruction significantly surpasses existing methods on both synthetic and diverse human motion datasets, demonstrating robustness even in high-noise conditions. Crucially, the model operates in real-time (less than a millisecond per input second), a substantial improvement over optimization-based techniques. This enhanced performance facilitates new applications in human-computer interaction, rehabilitation medicine, and motor control studies. We demonstrate the model's effectiveness across diverse human-performed tasks such as steering, rotation, pointing, object moving, handwriting, and mouse-controlled gaming, showing notable improvements particularly on challenging datasets where traditional methods largely fail. Training and benchmarking source code, along with pre-trained model weights, are made publicly available at https://github.com/dolphin-in-a-coma/sssumo.

</details>


### [4] [Pushing the Boundaries of Immersion and Storytelling: A Technical Review of Unreal Engine](https://arxiv.org/abs/2507.08142)

*Oleksandra Sobchyshak, Santiago Berrezueta-Guzman, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** Unreal Engine, virtual reality, immersive storytelling, healthcare, procedural content generation

**Relevance Score:** 4

**TL;DR:** This paper reviews Unreal Engine's innovations in immersive storytelling and virtual reality, highlighting its applications across various sectors, including healthcare and education.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an in-depth technical review of Unreal Engine and analyze its impact on immersive storytelling and environments.

**Method:** A technical review conducted through case studies and analysis of Unreal Engine's features and applications.

**Key Contributions:**

	1. Technical analysis of Unreal Engine's innovations
	2. Case studies demonstrating its applications
	3. Identification of challenges in adoption and ethics

**Result:** The paper highlights Unreal Engine's transformative impact across industries, demonstrating its capacity to merge storytelling with cutting-edge technologies.

**Limitations:** High hardware demands, limited accessibility, ethical concerns on over-immersion and data privacy.

**Conclusion:** Unreal Engine's ability to empower creators and redefine workflows positions it as pivotal in shaping the future of virtual reality and interactive media despite its challenges.

**Abstract:** Unreal Engine is a platform that has influenced immersive storytelling and virtual reality (VR) through its advanced features and diverse applications. This paper provides an in-depth technical review of Unreal Engine. It analyzes its key innovations in creating hyper-realistic environments and emotionally engaging narratives, with significant applications in gaming, virtual production, education, cultural preservation, and healthcare. The findings of this article highlight Unreal Engine's transformative impact across industries, demonstrating its ability to merge storytelling with cutting-edge technologies. Case studies illustrate how Unreal Engine facilitates seamless visuals, audio, and interactivity integration to create compelling experiences. Additionally, this study identifies Unreal Engine's versatility in applications ranging from procedural content generation and AI-driven workflows to smart city simulations and VR-based rehabilitation programs.   While Unreal Engine sets new benchmarks for visual fidelity and interactivity, this paper underscores critical challenges, including its high hardware demands, limited accessibility, and ethical concerns related to over-immersion and data privacy. Addressing these challenges through cloud-based rendering, inclusive design, and ethical practices is essential for broader adoption and sustainability. This review concludes that Unreal Engine is suitable for innovation and interdisciplinary collaboration. Its ability to empower creators, redefine workflows, and push the boundaries of immersive storytelling positions Unreal Engine as pivotal in shaping the future of virtual reality and interactive media.

</details>


### [5] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)

*Md. Saif Hassan Onim, Andrew M. Kiselica, Himanshu Thapliyal*

**Main category:** cs.HC

**Keywords:** Emotion Detection, Wearable Sensors, Older Adults, Machine Learning, Cognitive Health

**Relevance Score:** 8

**TL;DR:** This study investigates a non-intrusive method for emotion detection in older adults using wearable physiological sensors, achieving promising results that could assist in cognitive health monitoring.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding emotional well-being in older adults is important in healthcare settings, especially for those with cognitive impairments.

**Method:** The authors leveraged physiological signals obtained from wearable sensors (Empatica E4 and Shimmer3 GSR+) to identify emotional states, supplemented by facial expression analysis for validation.

**Key Contributions:**

	1. Development of a non-obtrusive emotion detection method using wearable sensors
	2. High accuracy in predicting emotional intensity solely from physiological signals
	3. Implications for cognitive health monitoring in sensitive populations.

**Result:** A regression model was developed that achieved a maximum r2 score of 0.782 and a minimum MSE of 0.0006, indicating promising accuracy in emotion prediction using physiological data alone.

**Limitations:** The study's dataset is limited to 40 older individuals, which may affect the generalizability of the findings.

**Conclusion:** The results demonstrate the potential for a privacy-preserving emotion recognition system, which is particularly beneficial for patients with Alzheimer’s or PTSD.

**Abstract:** Emotion detection in older adults is crucial for understanding their cognitive and emotional well-being, especially in hospital and assisted living environments. In this work, we investigate an edge-based, non-obtrusive approach to emotion identification that uses only physiological signals obtained via wearable sensors. Our dataset includes data from 40 older individuals. Emotional states were obtained using physiological signals from the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were recorded using camera-based emotion recognition with the iMotion's Facial Expression Analysis (FEA) module. The dataset also contains twelve emotion categories in terms of relative intensities. We aim to study how well emotion recognition can be accomplished using simply physiological sensor data, without the requirement for cameras or intrusive facial analysis. By leveraging classical machine learning models, we predict the intensity of emotional responses based on physiological signals. We achieved the highest 0.782 r2 score with the lowest 0.0006 MSE on the regression task. This method has significant implications for individuals with Alzheimer's Disease and Related Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder (PTSD) or other cognitive impairments. Our results across multiple classical regression models validate the feasibility of this method, paving the way for privacy-preserving and efficient emotion recognition systems in real-world settings.

</details>


### [6] [Uncanny or Not? Perceptions of AI-Generated Faces in Autism](https://arxiv.org/abs/2507.08230)

*Gabriella Waters*

**Main category:** cs.HC

**Keywords:** AI-generated faces, autism, uncanny valley, visual perception, inclusive AI

**Relevance Score:** 6

**TL;DR:** This study explores how autistic individuals perceive AI-generated faces, highlighting their unique discomfort levels compared to non-autistic individuals and discussing implications for AI system design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI-generated faces are perceived by autistic individuals and the implications for the design of inclusive AI systems.

**Method:** Qualitative analysis of discussions from the r/autism community on Reddit regarding AI-generated faces and the uncanny valley phenomenon.

**Key Contributions:**

	1. Insights into the uncanny valley effect in autistic perception.
	2. Emphasis on the need for inclusive AI system design based on user experiences.
	3. Contribution to the discourse on visual perception in autism.

**Result:** Findings indicate that autistic individuals may experience discomfort with AI-generated faces differently, reporting more discomfort with real human faces.

**Limitations:** The study relies on qualitative data from a specific community, which may not be representative of the broader autistic population.

**Conclusion:** Understanding these perceptions can guide the development of more inclusive and effective AI technologies for individuals with autism.

**Abstract:** As artificial intelligence (AI) systems become increasingly sophisticated at generating synthetic human faces, understanding how these images are perceived across diverse populations is important. This study investigates how autistic individuals/individuals with autism perceive AI-generated faces, focusing on the uncanny valley effect. Using a qualitative approach, we analyzed discussions from the r/autism community on Reddit to explore how autistic participants/participants with autism describe their experiences with AI-generated faces and the uncanny valley phenomenon. The findings suggest that autistic people/people with autism may experience the uncanny valley differently, often reporting stronger discomfort with real human faces than with artificial ones. This research contributes to our understanding of visual perception in autism and has implications for the development of inclusive AI systems and assistive technologies.

</details>


### [7] [Do Conversational Interfaces Limit Creativity? Exploring Visual Graph Systems for Creative Writing](https://arxiv.org/abs/2507.08260)

*Abhinav Sood, Maria Teresa Llano, Jon McCormack*

**Main category:** cs.HC

**Keywords:** generative AI, creativity, user interface, node-based systems, ideation

**Relevance Score:** 8

**TL;DR:** A graphical, node-based system for chaining generative AI models enhances creativity and ideation compared to linear chat interfaces.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of linear chat-based AI interactions and facilitate creative exploration in generative tasks.

**Method:** Develop a node-based system enabling users to visually chain generative AI models and create reusable, shareable templates.

**Key Contributions:**

	1. Novel node-based system for chaining generative AI models
	2. Visual aids for enhancing user ideation and creativity
	3. Reusable templates for different creative tasks

**Result:** User study shows the graph-based system supports better ideation and visualization of the writing process than traditional conversational interfaces.

**Limitations:** User interfaces with higher complexity may be challenging for some users to use effectively.

**Conclusion:** Complex user interfaces can benefit creativity by allowing more freedom and control, though they may also pose challenges for users not adept in using them.

**Abstract:** We present a graphical, node-based system through which users can visually chain generative AI models for creative tasks. Research in the area of chaining LLMs has found that while chaining provides transparency, controllability and guardrails to approach certain tasks, chaining with pre-defined LLM steps prevents free exploration. Using cognitive processes from creativity research as a basis, we create a system that addresses the inherent constraints of chat-based AI interactions. Specifically, our system aims to overcome the limiting linear structure that inhibits creative exploration and ideation. Further, our node-based approach enables the creation of reusable, shareable templates that can address different creative tasks. In a small-scale user study, we find that our graph-based system supports ideation and allows some users to better visualise and think through their writing process when compared to a similar conversational interface. We further discuss the weaknesses and limitations of our system, noting the benefits to creativity that user interfaces with higher complexity can provide for users who can effectively use them.

</details>


### [8] [Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance](https://arxiv.org/abs/2507.08624)

*Gábor Baranyi, Zsolt Csibi, Kristian Fenech, Áron Fóthi, Zsófia Gaál, Joul Skaf, András Lőrincz*

**Main category:** cs.HC

**Keywords:** Ambient Intelligence, Rehabilitation, AI, Vision-Language Models, Home Care

**Relevance Score:** 9

**TL;DR:** The AIRS framework integrates AI technologies for enhanced home rehabilitation, particularly after knee surgeries, using 3D reconstruction and vision-language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an advanced AI-based solution for home rehabilitation, addressing needs in post-surgery recovery and catering to diverse user capabilities.

**Method:** The AIRS framework utilizes Real-Time 3D Reconstruction, intelligent navigation, and Vision-Language Models to support machine-guided rehabilitation.

**Key Contributions:**

	1. Integration of RT-3DR for home rehabilitation
	2. Use of VLMs for personalized feedback
	3. Support for users with visual and hearing impairments

**Result:** AIRS was evaluated using 263 video recordings from post-total knee replacement rehabilitation, demonstrating effective exercise feedback and guidance mechanisms.

**Limitations:** 

**Conclusion:** AIRS offers an adaptable, privacy-conscious rehabilitation solution that supports diverse user needs and encourages exercise compliance.

**Abstract:** This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS) framework, an advanced artificial intelligence-based solution tailored for home rehabilitation environments. AIRS integrates cutting-edge technologies, including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and large Vision-Language Models (VLMs), to create a comprehensive system for machine-guided physical rehabilitation. The general AIRS framework is demonstrated in rehabilitation scenarios following total knee replacement (TKR), utilizing a database of 263 video recordings for evaluation. A smartphone is employed within AIRS to perform RT-3DR of living spaces and has a body-matched avatar to provide visual feedback about the excercise. This avatar is necessary in (a) optimizing exercise configurations, including camera placement, patient positioning, and initial poses, and (b) addressing privacy concerns and promoting compliance with the AI Act. The system guides users through the recording process to ensure the collection of properly recorded videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling direct comparisons between prerecorded clinical exercises and patient home recordings and (ii) VLM-generated feedback, providing detailed explanations and corrections for exercise errors. The framework also supports people with visual and hearing impairments. It also features a modular design that can be adapted to broader rehabilitation contexts. AIRS software components are available for further use and customization.

</details>


### [9] [Push or Light: Nudging Standing to Break Prolonged Sitting](https://arxiv.org/abs/2507.08659)

*Sohshi Yoshida, Ko Watanabe, Andreas Dengel, Shoya Ishimaru, Shingo Ata, Manato Fujimoto*

**Main category:** cs.HC

**Keywords:** nudging, behavior change, user context, standing breaks, health informatics

**Relevance Score:** 6

**TL;DR:** The study investigates the effectiveness of nudging strategies (push notifications vs. light dimming) to encourage standing up during different user task contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address health risks associated with prolonged sitting and evaluate different nudging strategies in promoting stand-up behavior.

**Method:** A mixed design experiment was conducted with 15 college students to compare the effects of no intervention, push notifications, and light dimming across three task contexts.

**Key Contributions:**

	1. Evaluation of multiple nudging strategies for stand-up behavior
	2. Impact of task context on effectiveness of interventions
	3. Insights into participant discomfort with nudging methods

**Result:** Dimming led to slightly more standing breaks than push notifications, but it caused discomfort for a higher percentage of participants. The effectiveness varied by task context: dimming was best during video calls and reading; notifications were better for computer work.

**Limitations:** Study limited to a small sample size and short duration of tasks.

**Conclusion:** Adaptive nudging systems should be developed to tailor strategies based on user context and preferences to effectively promote standing behavior.

**Abstract:** Prolonged sitting is a health risk leading to metabolic and cardiovascular diseases. To combat this, various "nudging" strategies encourage stand-ups. Behavior change triggers use explicit prompts such as smartphone push notifications or light controls. However, comparisons of the effects of such interactions, discomfort, and user context have not yet been performed. The present study evaluated these methods in a mixed design experiment with 15 college students. Three intervention methods (none, push notifications, and light dimming) and three user task contexts (computer work, video calls, and reading) were tested. The frequency of standing up and comfort were assessed after each ten-minute session. Results showed that dimming resulted in slightly more breaks (1.4 \pm 1.55) than push notification (1.2 \pm 1.08), but caused discomfort for 66.7% of participants, compared to 20% for notification. The results were influenced by task context. Dimming was most effective during video calls and reading, while push notifications were more effective during computer work. These findings suggest adaptive nudging systems should tailor interventions based on context and individual preferences.

</details>


### [10] [LIMITER: A Gamified Interface for Harnessing Just Intonation Systems](https://arxiv.org/abs/2507.08675)

*Antonis Christou*

**Main category:** cs.HC

**Keywords:** microtonality, digital instrument, gamification, music performance, creativity

**Relevance Score:** 2

**TL;DR:** LIMITER is a gamified digital musical instrument that simplifies the performance of microtonal music using a user-friendly interface.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To make microtonality in Western music more accessible and expressive through an engaging interface.

**Method:** Developed a digital instrument that combines color, geometric transformations, and game-like controls to facilitate the performance of microtonal sounds.

**Key Contributions:**

	1. Creation of an intuitive interface for microtonal sounds
	2. Integration of gamification principles into musical performance
	3. Preliminary evidence of creativity enhancement with the interface

**Result:** Preliminary evaluations suggest that the LIMITER interface enhances creativity in music performance.

**Limitations:** 

**Conclusion:** LIMITER offers a novel approach to performing microtonal music, potentially broadening its appeal and usability.

**Abstract:** This paper introduces LIMITER, a gamified digital musical instrument for harnessing and performing microtonal and justly intonated sounds. While microtonality in Western music remains a niche and esoteric system that can be difficult both to conceptualize and to perform with, LIMITER presents a novel, easy to pickup interface that utilizes color, geometric transformations, and game-like controls to create a simpler inlet into utilizing these sounds as a means of expression. We report on the background of the development of LIMITER, as well as explain the underlying musical and engineering systems that enable its function. Additionally, we offer a discussion and preliminary evaluation of the creativity-enhancing effects of the interface.

</details>


### [11] [EqualMotion: Accessible Motion Capture for the Creative Industries](https://arxiv.org/abs/2507.08744)

*Clarice Hilton, Kat Hawkins, Phill Tew, Freddie Collins, Seb Madgwick, Dominic Potts, Tom Mitchell*

**Main category:** cs.HC

**Keywords:** motion capture, disability-inclusive design, digital performance, wearable technology, accessibility

**Relevance Score:** 7

**TL;DR:** EqualMotion is a wearable motion capture system designed to include disabled practitioners by adopting a disability-centered co-design approach, enabling personalized calibration and supporting diverse body types.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the exclusion of disabled practitioners in motion capture technologies through normative assumptions in body modeling and representation.

**Method:** The system was developed using a body-agnostic design, allowing for personalized calibration and integration of mobility aids, assessed through ongoing case studies in dance and music.

**Key Contributions:**

	1. Development of a body-agnostic motion capture system
	2. Integration of personalized calibration for diverse users
	3. Collaboration with disabled researchers for inclusive design

**Result:** EqualMotion demonstrates increased accessibility in digital performance workflows by supporting a variety of body types and movement styles, allowing for equitable participation.

**Limitations:** 

**Conclusion:** The project aims to foster inclusive practices in creative industries by enabling disabled individuals to engage fully in motion capture and performance.

**Abstract:** Motion capture technologies are increasingly used in creative and performance contexts but often exclude disabled practitioners due to normative assumptions in body modeling, calibration, and avatar representation. EqualMotion introduces a body-agnostic, wearable motion capture system designed through a disability-centred co-design approach. By enabling personalised calibration, integrating mobility aids, and adopting an inclusive visual language, EqualMotion supports diverse body types and movement styles. The system is developed collaboratively with disabled researchers and creatives, aiming to foster equitable participation in digital performance and prototyping. This paper outlines the system's design principles and highlights ongoing case studies in dance and music to evaluate accessibility in real-world creative workflows.

</details>


### [12] [Human-AI Collaboration for Wearable Technology Component Standardization](https://arxiv.org/abs/2503.15488)

*Andrew M. Lydner*

**Main category:** cs.HC

**Keywords:** wearable technology, collaboration, artificial intelligence, multidisciplinary, innovation

**Relevance Score:** 6

**TL;DR:** The paper discusses the challenges in the wearable technology industry due to a lack of multidisciplinary collaboration and proposes the use of AI collaboration tools to enhance innovation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The wearable technology industry, despite its potential, is experiencing stagnation in innovation due to its multidisciplinary nature and a lack of collaborative expert knowledge.

**Method:** The paper explores the application of mental model development for AI tool usage among industry experts to foster collaboration in wearable technology innovation.

**Key Contributions:**

	1. Identifies the stagnation in wearable technology innovation due to limited expert collaboration.
	2. Proposes the use of AI collaboration tools to facilitate interdisciplinary engagement among industry professionals.
	3. Suggests mental model development as a strategy to improve AI tool utilization in the context of wearable technology.

**Result:** It highlights the necessity of establishing a collaborative foundation among experts to drive innovation in wearable technologies through AI tools.

**Limitations:** The paper may not address specific implementation strategies for AI tools or the challenges in ensuring their adoption by experts.

**Conclusion:** The involvement of intelligent systems is crucial for enhancing collaboration in the industry, which could lead to innovative solutions in wearable technology.

**Abstract:** Due to the multidisciplinary nature of wearable technology, the industry faces potential limitations in innovation. The wearable technology industry is still in its infancy and increased applicable use faces stagnation despite the plethora of technologies that have been largely wrist worn. This could be a result of the lack of multidisciplinary expert knowledge disseminating through the industry. Unlike other technologies which have standardizations and processes for how they are developed, wearable technologies exist in a realm of perpetual change as given the various materials and subcomponents that continue to be developed. It is essential that expert opinions form a collaborative foundation, and even more so that intelligent systems foster that collaboration. The caveat though, is likeliness of these artificial intelligence (AI) collaboration tools to be utilized by industry experts. Mental model development for AI tool usage could be applied to wearable technology innovation in this regard, thus the goal of this paper and focus of research.

</details>


### [13] [Conversational Self-Play for Discovering and Understanding Psychotherapy Approaches](https://arxiv.org/abs/2503.16521)

*Onno P Kampman, Michael Xing, Charmaine Lim, Ahmad Ishqi Jabir, Ryan Louie, Jimmy Lee, Robert JT Morris*

**Main category:** cs.HC

**Keywords:** conversational self-play, LLMs, psychotherapy, AI dialogue, therapeutic modalities

**Relevance Score:** 7

**TL;DR:** Explores conversational self-play with LLMs to analyze and evaluate therapeutic dialogues in psychotherapy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how AI-generated therapeutic dialogues align with established psychotherapy modalities and enhance the understanding of these approaches through scalable methods.

**Method:** Utilizes conversational self-play with large language models (LLMs) to generate and assess therapeutic conversations.

**Key Contributions:**

	1. Introduces conversational self-play as a method for evaluating psychotherapy dialogues
	2. Demonstrates the potential of AI in understanding therapeutic modalities
	3. Provides insights into the effectiveness of LLMs in generating human-like therapeutic conversations.

**Result:** Findings reveal insights into the alignment and effectiveness of AI-generated dialogues compared to traditional psychotherapy methods.

**Limitations:** The paper notes a need for improvement in writing clarity and the extraction of multiple techniques.

**Conclusion:** The study suggests that conversational self-play can be a valuable tool for analyzing psychotherapy approaches and understanding the nuances of therapeutic dialogue generation.

**Abstract:** This paper explores conversational self-play with LLMs as a scalable approach for analyzing and exploring psychotherapy approaches, evaluating how well AI-generated therapeutic dialogues align with established modalities.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [14] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)

*Atli Sigurgeirsson, Simon King*

**Main category:** cs.CL

**Keywords:** Text-To-Speech, Fine-tuning, Latent Features, Principal Component Analysis, Controllability

**Relevance Score:** 4

**TL;DR:** This paper presents a novel fine-tuning approach for a Prompt-based Text-To-Speech model, enhancing controllability by addressing issues of training constraints and output variance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve controllability in Prompt-based Text-To-Speech models by addressing the limitations of user control and output variability through novel fine-tuning techniques.

**Method:** The approach involves principal component analysis of thousands of synthesized samples to identify latent features that capture significant output variance, which are then used as additional labels for secondary fine-tuning.

**Key Contributions:**

	1. Development of a dual-fine tuning methodology in TTS models
	2. Introduction of latent features via PCA to improve control
	3. Evaluation of methods on expressive Icelandic speech datasets

**Result:** The method improved overall controllability in a model trained on an expressive Icelandic speech corpus, demonstrating effectiveness with both continuous and discrete features, especially in models lacking emotional disclosure.

**Limitations:** 

**Conclusion:** Introducing secondary fine-tuning using identified latent features can significantly enhance the controllability of speech synthesis models without emotional disclosure.

**Abstract:** A Prompt-based Text-To-Speech model allows a user to control different aspects of speech, such as speaking rate and perceived gender, through natural language instruction. Although user-friendly, such approaches are on one hand constrained: control is limited to acoustic features exposed to the model during training, and too flexible on the other: the same inputs yields uncontrollable variation that are reflected in the corpus statistics.   We investigate a novel fine-tuning regime to address both of these issues at the same time by exploiting the uncontrollable variance of the model. Through principal component analysis of thousands of synthesised samples, we determine latent features that account for the highest proportion of the output variance and incorporate them as new labels for secondary fine-tuning. We evaluate the proposed methods on two models trained on an expressive Icelandic speech corpus, one with emotional disclosure and one without. In the case of the model without emotional disclosure, the method yields both continuous and discrete features that improve overall controllability of the model.

</details>


### [15] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)

*K. Sahit Reddy, N. Ragavenderan, Vasanth K., Ganesh N. Naik, Vishalakshi Prabhu, Nagaraja G. S*

**Main category:** cs.CL

**Keywords:** Biomedical NLP, MedicalBERT, Transfer Learning, Pretrained Models, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** MedicalBERT is a pretrained BERT model optimized for biomedical literature, outperforming other models in various NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding and processing of biomedical literature which poses challenges for existing NLP models due to its domain-specific terminology.

**Method:** MedicalBERT was trained on a large biomedical dataset and fine-tuned for tasks such as named entity recognition, relation extraction, question answering, sentence similarity, and document classification.

**Key Contributions:**

	1. Proposed MedicalBERT, enhancing biomedical NLP with domain-specific vocabulary.
	2. Demonstrated superior performance of MedicalBERT over existing models in a variety of tasks.
	3. Showcased effective transfer learning for biomedical applications of NLP.

**Result:** MedicalBERT outperformed BioBERT, SciBERT, and ClinicalBERT on most benchmarks, surpassing general-purpose BERT by an average of 5.67% across all evaluated tasks.

**Limitations:** 

**Conclusion:** The study illustrates the effectiveness of transfer learning techniques in biomedical NLP, emphasizing the potential of pretrained models like BERT in capturing domain-specific information.

**Abstract:** Recent advances in natural language processing (NLP) have been driven bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel at understanding complex texts, but biomedical literature, withits domain-specific terminology, poses challenges that models likeWord2Vec and bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5, despite capturing context, fall short in tasks needingbidirectional understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a pretrained BERT model trained on a large biomedicaldataset and equipped with domain-specific vocabulary that enhances thecomprehension of biomedical terminology. MedicalBERT model is furtheroptimized and fine-tuned to address diverse tasks, including named entityrecognition, relation extraction, question answering, sentence similarity, anddocument classification. Performance metrics such as the F1-score,accuracy, and Pearson correlation are employed to showcase the efficiencyof our model in comparison to other BERT-based models such as BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost of the benchmarks, and surpasses the general-purpose BERT model by5.67% on average across all the tasks evaluated respectively. This work alsounderscores the potential of leveraging pretrained BERT models for medicalNLP tasks, demonstrating the effectiveness of transfer learning techniques incapturing domain-specific information.   (PDF) MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model. Available from: https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model [accessed Jul 06 2025].

</details>


### [16] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)

*Aldan Creo, Raul Castro Fernandez, Manuel Cebrian*

**Main category:** cs.CL

**Keywords:** large language models, jailbreaking, AI safety, complexity analysis, toxicity

**Relevance Score:** 8

**TL;DR:** The paper analyzes the complexity of jailbreak strategies in large language models (LLMs) through a mass-scale study of over 2 million conversations, revealing that jailbreak attempts are not more complex than normal conversations, indicating limits on attack sophistication and improvements in safety measures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the complexity and evolution of jailbreaking strategies is critical for AI safety as LLMs are increasingly deployed.

**Method:** The study conducted a mass-scale empirical analysis of jailbreak complexity using over 2 million conversations from various platforms and applied complexity metrics such as probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators.

**Key Contributions:**

	1. Mass-scale analysis of jailbreak complexity across diverse platforms
	2. Introduction of various metrics to measure complexity
	3. Insights into the evolution of AI safety mechanisms against jailbreaks

**Result:** Jailbreak attempts do not show significantly higher complexity than normal conversations, and there is a stable pattern in toxicity and complexity over time, indicating bounding constraints on both attack sophistication and safety evolution.

**Limitations:** Further research needed to explore potential future jailbreak strategies that exceed current complexity measures.

**Conclusion:** The findings suggest that the escalation of jailbreak complexity may be limited by human ingenuity and highlight the importance of cautious jailbreak disclosures to avoid enabling harm before defensive advancements can keep pace.

**Abstract:** As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.   We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.   Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.

</details>


### [17] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)

*Prudence Djagba, Chimezie A. Odinakachukwu*

**Main category:** cs.CL

**Keywords:** FinGPT, financial NLP, language model, benchmark, natural language processing

**Relevance Score:** 6

**TL;DR:** Evaluation of FinGPT on financial NLP tasks reveals strengths in classification but weaknesses in reasoning and generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities and limitations of FinGPT in real-world financial applications across various NLP tasks.

**Method:** The evaluation involved testing FinGPT on six NLP tasks using finance-specific datasets to measure its performance against benchmarks.

**Key Contributions:**

	1. Performance benchmark for FinGPT against GPT-4
	2. Identification of limitations in reasoning and generation tasks
	3. Framework for future enhancements in financial language models.

**Result:** FinGPT showed strong performance in sentiment analysis and text classification but significantly lagged in financial question answering and text summarization compared to GPT-4 and human benchmarks.

**Limitations:** Not comprehensive for all financial NLP tasks, particularly reasoning.

**Conclusion:** While effective for structured financial tasks, FinGPT is not a comprehensive solution, highlighting the need for further architectural improvements.

**Abstract:** This work evaluates FinGPT, a financial domain-specific language model, across six key natural language processing (NLP) tasks: Sentiment Analysis, Text Classification, Named Entity Recognition, Financial Question Answering, Text Summarization, and Stock Movement Prediction. The evaluation uses finance-specific datasets to assess FinGPT's capabilities and limitations in real-world financial applications. The results show that FinGPT performs strongly in classification tasks such as sentiment analysis and headline categorization, often achieving results comparable to GPT-4. However, its performance is significantly lower in tasks that involve reasoning and generation, such as financial question answering and summarization. Comparisons with GPT-4 and human benchmarks highlight notable performance gaps, particularly in numerical accuracy and complex reasoning. Overall, the findings indicate that while FinGPT is effective for certain structured financial tasks, it is not yet a comprehensive solution. This research provides a useful benchmark for future research and underscores the need for architectural improvements and domain-specific optimization in financial language models.

</details>


### [18] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)

*Pierre Beckmann, Matthieu Queloz*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, Large Language Models, machine understanding, cognitive architecture, AI

**Relevance Score:** 9

**TL;DR:** The paper synthesizes findings in mechanistic interpretability of Large Language Models (LLMs) and presents a novel framework for understanding machine cognition.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the notion that LLMs rely solely on superficial statistics and to explore the mechanisms behind their interpretability and understanding.

**Method:** The authors propose a theoretical framework comprising three tiers of machine understanding: conceptual, state-of-the-world, and principled understanding.

**Key Contributions:**

	1. Introduction of a three-tiered framework for machine understanding
	2. Synthesis of existing findings in mechanistic interpretability
	3. Exploration of the differences between LLM and human cognition

**Result:** The study argues that LLMs develop internal structures similar to human understanding, allowing them to see connections but differ fundamentally in cognitive architecture from humans.

**Limitations:** 

**Conclusion:** The authors advocate shifting focus from whether LLMs understand to understanding the nature of their cognitive processes and mechanisms.

**Abstract:** Recent findings in mechanistic interpretability (MI), the field probing the inner workings of Large Language Models (LLMs), challenge the view that these models rely solely on superficial statistics. Here, we offer an accessible synthesis of these findings that doubles as an introduction to MI, all while integrating these findings within a novel theoretical framework for thinking about machine understanding. We argue that LLMs develop internal structures that are functionally analogous to the kind of understanding that consists in seeing connections. To sharpen this idea, we propose a three-tiered conception of machine understanding. First, conceptual understanding emerges when a model forms "features" as directions in latent space, thereby learning the connections between diverse manifestations of something. Second, state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world. Third, principled understanding emerges when a model ceases to rely on a collection of memorized facts and discovers a "circuit" that connects these facts. However, we conclude by exploring the "parallel mechanisms" phenomenon, arguing that while LLMs exhibit forms of understanding, their cognitive architecture remains different from ours, and the debate should shift from whether LLMs understand to how their strange minds work.

</details>


### [19] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)

*Nikita Mounier, Parsa Idehpour*

**Main category:** cs.CL

**Keywords:** text generation, error correction, Process Reward Model, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** Proposes a framework called Review, Remask, Refine (R3) for improving text generation by correcting model errors through a structured approach using a Process Reward Model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of iterative text generation where models need to identify and rectify their own errors effectively.

**Method:** The R3 framework leverages a Process Reward Model (PRM) to review generated text blocks, applying a Remask strategy based on PRM scores, and focuses refinement efforts on sub-optimal segments.

**Key Contributions:**

	1. Introduction of the R3 framework for text generation error correction
	2. Utilization of a Process Reward Model for targeted remasking
	3. Improves end output quality with minimal adjustments to existing models.

**Result:** R3 enhances the quality of generated outputs by directing more attention to parts identified as problematic, leading to more accurate text generation.

**Limitations:** 

**Conclusion:** R3 is a practical and effective solution for optimizing pre-trained masked text diffusion models without the need for additional training.

**Abstract:** A key challenge for iterative text generation is enabling models to efficiently identify and correct their own errors. We propose Review, Remask, Refine (R3), a relatively simple yet elegant framework that requires no additional model training and can be applied to any pre-trained masked text diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is utilized for the Review of intermediate generated blocks. The framework then translates these PRM scores into a Remask strategy: the lower a block's PRM score, indicating potential mistakes, the greater the proportion of tokens within that block are remasked. Finally, the model is compelled to Refine these targeted segments, focusing its efforts more intensively on specific sub-optimal parts of past generations, leading to improved final output.

</details>


### [20] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)

*Aryan Varshney, Venkat Ram Reddy Ganuthula*

**Main category:** cs.CL

**Keywords:** large language models, resume screening, human evaluation, recruitment, adaptive behavior

**Relevance Score:** 9

**TL;DR:** This study analyzes the behavior of large language models (LLMs) in resume screening compared to human experts, revealing significant performance differences and insights into LLM adaptability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reliability of LLMs in screening resumes and their comparison with human recruitment experts in terms of consistent behavior versus random variation.

**Method:** Controlled datasets were used to test three LLMs (Claude, GPT, and Gemini) against human experts across various contexts and conditions, with statistical analysis to evaluate performance differences.

**Key Contributions:**

	1. Demonstrates significant performance differences between LLMs and human evaluators in resume screening tasks.
	2. Showcases the adaptive behavior of different LLMs in varying company contexts.
	3. Highlights the limitations of LLMs in mimicking human judgment in recruitment processes.

**Result:** Significant differences were found in LLM evaluations both among themselves and against human experts, particularly in response to the company context.

**Limitations:** Limited to specific LLMs tested and controlled settings; may not represent all LLMs or real-world scenarios.

**Conclusion:** LLMs can provide interpretable patterns in resume screening but exhibit substantial divergence from human judgment, which is crucial for understanding their role in automated hiring.

**Abstract:** This study investigates whether large language models (LLMs) exhibit consistent behavior (signal) or random variation (noise) when screening resumes against job descriptions, and how their performance compares to human experts. Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini) across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context) with identical and randomized resumes, benchmarked against three human recruitment experts. Analysis of variance revealed significant mean differences in four of eight LLM-only conditions and consistently significant differences between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts strongly to company context (p < 0.001), Gemini partially (p = 0.038 for Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly from human experts across contexts. Meta-cognition analysis highlighted adaptive weighting patterns that differ markedly from human evaluation approaches. Findings suggest LLMs offer interpretable patterns with detailed prompts but diverge substantially from human judgment, informing their deployment in automated hiring systems.

</details>


### [21] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)

*Zhibo Zhang, Yuxi Li, Kailong Wang, Shuai Yuan, Ling Shi, Haoyu Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, embedding space poisoning, toxicity attenuation, adversarial perturbation, safety alignment

**Relevance Score:** 7

**TL;DR:** This paper presents ETTA, a framework that targets and mitigates toxicity in the embedding space of LLMs, highlighting vulnerabilities in current safety alignment approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant security risks in LLMs due to embedding space poisoning and the insufficient understanding of embedding-level safety alignment.

**Method:** The proposed ETTA framework identifies toxicity-sensitive dimensions in the embedding space and applies linear transformations to attenuate these risks without needing model fine-tuning or training data.

**Key Contributions:**

	1. Introduction of ETTA framework for toxicity mitigation in LLMs
	2. Demonstrated high attack success rate of 88.61%
	3. Addressing the gap in understanding embedding-level safety alignment

**Result:** ETTA demonstrates an average attack success rate of 88.61% on five open-source LLMs, outperforming existing methods.

**Limitations:** 

**Conclusion:** The findings reveal critical vulnerabilities in current alignment strategies, emphasizing the importance of embedding-aware defenses in LLM applications.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success across domains such as healthcare, education, and cybersecurity. However, this openness also introduces significant security risks, particularly through embedding space poisoning, which is a subtle attack vector where adversaries manipulate the internal semantic representations of input data to bypass safety alignment mechanisms. While previous research has investigated universal perturbation methods, the dynamics of LLM safety alignment at the embedding level remain insufficiently understood. Consequently, more targeted and accurate adversarial perturbation techniques, which pose significant threats, have not been adequately studied.   In this work, we propose ETTA (Embedding Transformation Toxicity Attenuation), a novel framework that identifies and attenuates toxicity-sensitive dimensions in embedding space via linear transformations. ETTA bypasses model refusal behaviors while preserving linguistic coherence, without requiring model fine-tuning or access to training data. Evaluated on five representative open-source LLMs using the AdvBench benchmark, ETTA achieves a high average attack success rate of 88.61%, outperforming the best baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR on instruction-tuned defenses). These results highlight a critical vulnerability in current alignment strategies and underscore the need for embedding-aware defenses.

</details>


### [22] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)

*Li Li, Yongliang Wu, Jingze Zhu, Jiawei Peng, Jianfei Cai, Xu Yang*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Large Multimodal Models, Image Captioning, Attention Metrics, NLP

**Relevance Score:** 9

**TL;DR:** This paper investigates the In-Context Learning (ICL) capabilities of Large Multimodal Models (LMMs) on image captioning tasks through both external and internal analyses.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To explore demonstration configuration strategies for multimodal In-Context Learning (ICL) and analyze inference characteristics of Large Multimodal Models (LMMs).

**Method:** The study conducts external experiments examining shot number, image retrieval, and caption assignment in ICL, using multiple metrics for evaluation. Internally, attention characteristics of LMMs are analyzed with newly developed metrics for quantifying model behaviors.

**Key Contributions:**

	1. Investigated demonstration configuration strategies for multimodal ICL.
	2. Developed attention-based metrics for analyzing model behaviors.
	3. Revealed performance variations in LMMs based on pre-training data features.

**Result:** The research reveals how configuration strategies for In-Context Examples (ICEs) affect model performance and identifies typical patterns in LMM behaviors through dual analyses.

**Limitations:** 

**Conclusion:** Combining external and internal analyses to study LMMs enhances understanding of multimodal ICL, with proposed metrics applicable in wider research contexts.

**Abstract:** The evolution of large models has witnessed the emergence of In-Context Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous studies have demonstrated the effectiveness of ICL. Inspired by the success of Large Language Models (LLMs), researchers have developed Large Multimodal Models (LMMs) with ICL capabilities. However, explorations of demonstration configuration for multimodal ICL remain preliminary. Additionally, the controllability of In-Context Examples (ICEs) provides an efficient and cost-effective means to observe and analyze the inference characteristics of LMMs under varying inputs. This paper conducts a comprehensive external and internal investigation of multimodal in-context learning on the image captioning task. Externally, we explore demonstration configuration strategies through three dimensions: shot number, image retrieval, and caption assignment. We employ multiple metrics to systematically and thoroughly evaluate and summarize key findings. Internally, we analyze typical LMM attention characteristics and develop attention-based metrics to quantify model behaviors. We also conduct auxiliary experiments to explore the feasibility of attention-driven model acceleration and compression. We further compare performance variations between LMMs with identical model design and pretraining strategies and explain the differences from the angles of pre-training data features. Our study reveals both how ICEs configuration strategies impact model performance through external experiments and characteristic typical patterns through internal inspection, providing dual perspectives for understanding multimodal ICL in LMMs. Our method of combining external and internal analysis to investigate large models, along with our newly proposed metrics, can be applied to broader research areas.

</details>


### [23] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)

*Sonali Sharma, Ahmed M. Alaa, Roxana Daneshjou*

**Main category:** cs.CL

**Keywords:** Generative AI, Large Language Models, Vision-Language Models, Medical Disclaimers, Medical Imaging

**Relevance Score:** 9

**TL;DR:** This study analyzes the decline of medical disclaimers in LLM and VLM outputs from 2022 to 2025, highlighting the safety concerns surrounding AI interpretations of medical images.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety concerns arising from growing use of generative AI models in medical contexts, particularly the lack of disclaimers in AI-generated outputs.

**Method:** The study evaluated outputs from LLMs and VLMs across four types of medical images and questions, screening 2000 outputs for the presence of disclaimer phrases.

**Key Contributions:**

	1. Quantitative analysis of the decline in medical disclaimers in AI outputs over time.
	2. Discussion on the implications of declining disclaimer presence for patient safety in AI medical applications.
	3. Recommendations for implementing tailored disclaimers in medical AI contexts.

**Result:** The presence of medical disclaimers in AI outputs significantly decreased from 26.3% in 2022 to 0.97% in 2025 for LLMs, and from 19.6% in 2023 to 1.05% in 2025 for VLMs, indicating a concerning trend.

**Limitations:** The study focuses solely on certain model generations and does not evaluate the accuracy of the outputs themselves.

**Conclusion:** As AI models advance and their authority grows, it is essential to enforce disclaimers consciously tailored to the clinical context to ensure user safety.

**Abstract:** Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.

</details>


### [24] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)

*W. Russell Neuman, Chad Coleman, Ali Dasdan, Safinah Ali, Manan Shah, Kund Meghani*

**Main category:** cs.CL

**Keywords:** large language models, political orientation, liberal values, ethical discourse, training corpora

**Relevance Score:** 6

**TL;DR:** This paper investigates the political temperament of various large language models, revealing a consistent liberal orientation in their ethical and political responses, attributed to factors like training corpora and fine-tuning practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the underlying causes and implications of the observed liberal orientation in the responses generated by commercial large language models (LLMs).

**Method:** The study employs Moral Foundations Theory, multiple established political ideology scales, and a new index of current political controversies to analyze the political temperament of seven prominent LLMs.

**Key Contributions:**

	1. Systematic investigation of the political temperament of seven LLMs.
	2. Insights into the influence of training data and reinforcement learning on political bias.
	3. Clarification of the distinction between political bias and epistemic differences.

**Result:** The analysis shows strong prioritization of liberal-leaning values, particularly care and fairness, across most models, influenced by their training data and fine-tuning processes.

**Limitations:** The analysis is limited to seven prominent LLMs and may not generalize to all models.

**Conclusion:** The liberal tilt observed in LLMs is an emergent property rather than a programming error, suggesting that these models could reflect a moral stance that enhances democratic discourse.

**Abstract:** Recent studies have revealed a consistent liberal orientation in the ethical and political responses generated by most commercial large language models (LLMs), yet the underlying causes and resulting implications remain unclear. This paper systematically investigates the political temperament of seven prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity (Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes Moral Foundations Theory, a dozen established political ideology scales and a new index of current political controversies. We find strong and consistent prioritization of liberal-leaning values, particularly care and fairness, across most models. Further analysis attributes this trend to four overlapping factors: Liberal-leaning training corpora, reinforcement learning from human feedback (RLHF), the dominance of liberal frameworks in academic ethical discourse and safety-driven fine-tuning practices. We also distinguish between political "bias" and legitimate epistemic differences, cautioning against conflating the two. A comparison of base and fine-tuned model pairs reveals that fine-tuning generally increases liberal lean, an effect confirmed through both self-report and empirical testing. We argue that this "liberal tilt" is not a programming error or the personal preference of programmers but an emergent property of training on democratic rights-focused discourse. Finally, we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance philosophical aspiration, reflecting a moral stance unanchored to personal identity or interest. Rather than undermining democratic discourse, this pattern may offer a new lens through which to examine collective reasoning.

</details>


### [25] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)

*Ada Aka, Emil Palikot, Ali Ansari, Nima Yazdani*

**Main category:** cs.CL

**Keywords:** AI, recruitment, hiring efficiency, candidate selection, interview dynamics

**Relevance Score:** 4

**TL;DR:** This paper evaluates the impact of AI-assisted recruitment on hiring efficiency and candidate outcomes compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the empirical effects of AI on recruitment processes and hiring outcomes, particularly in tech job roles.

**Method:** A randomized assignment of 37,000 applicants to either a traditional recruitment process or an AI-assisted pipeline, followed by analysis of interview outcomes and subsequent job placements.

**Key Contributions:**

	1. Demonstrates the effectiveness of AI in improving hiring outcomes.
	2. Identifies biases in AI selection favoring younger and less experienced candidates.
	3. Provides insights into AI-driven interview dynamics and criteria.

**Result:** Candidates in the AI-assisted pipeline had a significantly higher pass rate in final interviews (54%) compared to traditional methods (34%), and a greater likelihood of finding new jobs (23% vs. 18%).

**Limitations:** The study focused only on junior-developer positions; broader applicability across other roles is uncertain.

**Conclusion:** AI technologies positively impact hiring effectiveness but tend to favor less experienced candidates, raising important considerations for selection criteria.

**Abstract:** Artificial intelligence (AI) is increasingly used in recruitment, yet empirical evidence quantifying its impact on hiring efficiency and candidate selection remains limited. We randomly assign 37,000 applicants for a junior-developer position to either a traditional recruitment process (resume screening followed by human selection) or an AI-assisted recruitment pipeline incorporating an initial AI-driven structured video interview before human evaluation. Candidates advancing from either track faced the same final-stage human interview, with interviewers blind to the earlier selection method. In the AI-assisted pipeline, 54% of candidates passed the final interview compared with 34% from the traditional pipeline, yielding an average treatment effect of 20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn profiles of top applicants from both groups and found that 18% (SE 1.1%) of applicants from the traditional track found new jobs compared with 23% (SE 2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the probability of finding new employment between groups. The AI system tended to select younger applicants with less experience and fewer advanced credentials. We analyze AI-generated interview transcripts to examine the selection criteria and conversational dynamics. Our findings contribute to understanding how AI technologies affect decision making in recruitment and talent acquisition while highlighting some of their potential implications.

</details>


### [26] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)

*Sonali Sharma, Ahmed M. Alaa, Roxana Daneshjou*

**Main category:** cs.CL

**Keywords:** Generative AI, Medical Disclaimers, Large Language Models, Vision-Language Models, Healthcare AI

**Relevance Score:** 9

**TL;DR:** This study analyzes the presence of medical disclaimers in outputs from LLM and VLM models interpreting medical images and answering clinical questions, finding a significant decrease in disclaimers from 2022 to 2025.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the critical role of medical disclaimers in AI models interpreting healthcare data and to ensure user awareness of the potential inaccuracies in AI outputs.

**Method:** The study screened outputs from various generative AI models using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions for the presence of disclaimer phrases from 2022 to 2025.

**Key Contributions:**

	1. Investigated the trend of medical disclaimers in AI outputs over a three-year period.
	2. Highlighted the critical need for safety measures in the deployment of AI models in healthcare applications.
	3. Provided insights into user safety and trust in AI-generated medical content.

**Result:** The study found that the presence of medical disclaimers in outputs dropped significantly: from 26.3% in 2022 to 0.97% in 2025 for LLMs, and from 19.6% in 2023 to 1.05% in 2025 for VLMs.

**Limitations:** The study is limited to the evaluation of specific medical images and may not represent all potential scenarios in healthcare settings.

**Conclusion:** As generative AI models become more authoritative, implementing disclaimers is essential to safeguard users and adapt to the clinical context.

**Abstract:** Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.

</details>


### [27] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)

*Hong Jia, Shiya Fu, Vassilis Kostakos, Feng Xia, Ting Dang*

**Main category:** cs.CL

**Keywords:** Small Language Models, Large Language Models, mental health understanding, few-shot learning, privacy-preserving AI

**Relevance Score:** 8

**TL;DR:** This paper evaluates the mental health understanding capabilities of Small Language Models (SLMs) compared to Large Language Models (LLMs) through systematic classification tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper investigates the potential of Small Language Models as privacy-preserving alternatives to Large Language Models in sensitive applications like mental health understanding.

**Method:** The study employs zero-shot and few-shot learning paradigms to benchmark five state-of-the-art SLMs against three established LLMs across six mental health understanding tasks.

**Key Contributions:**

	1. Evaluation of SLMs in mental health tasks
	2. Comparison of SLMs and LLMs on performance metrics
	3. Insights on few-shot learning impacts on SLM efficiency

**Result:** SLMs perform competitively with LLMs on binary classification tasks but show similar degradation on multi-class severity tasks; few-shot learning significantly enhances SLM performance.

**Limitations:** The study highlights that both SLMs and LLMs struggle with nuanced clinical understanding in multi-class tasks.

**Conclusion:** SLMs exhibit notable capabilities for mental health understanding and can serve as effective tools for analyzing sensitive online text data, particularly in scalable screening applications.

**Abstract:** The emergence of Small Language Models (SLMs) as privacy-preserving alternatives for sensitive applications raises a fundamental question about their inherent understanding capabilities compared to Large Language Models (LLMs). This paper investigates the mental health understanding capabilities of current SLMs through systematic evaluation across diverse classification tasks. Employing zero-shot and few-shot learning paradigms, we benchmark their performance against established LLM baselines to elucidate their relative strengths and limitations in this critical domain. We assess five state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding tasks. Our findings reveal that SLMs achieve mean performance within 2\% of LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot settings), demonstrating notable competence despite orders of magnitude fewer parameters. Both model categories experience similar degradation on multi-class severity tasks (a drop of over 30\%), suggesting that nuanced clinical understanding challenges transcend model scale. Few-shot prompting provides substantial improvements for SLMs (up to 14.6\%), while LLM gains are more variable. Our work highlights the potential of SLMs in mental health understanding, showing they can be effective privacy-preserving tools for analyzing sensitive online text data. In particular, their ability to quickly adapt and specialize with minimal data through few-shot learning positions them as promising candidates for scalable mental health screening tools.

</details>


### [28] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)

*Nripesh Niketan, Hadj Batatia*

**Main category:** cs.CL

**Keywords:** Large Language Models, Framework Integration, Educational Technology

**Relevance Score:** 9

**TL;DR:** The paper presents a framework for enhancing large language models (LLMs) by integrating external tools and APIs to provide relevant contextual information, significantly improving query performance in educational settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in providing accurate responses without relevant context and reduce hallucination in their outputs.

**Method:** Development of the Athena framework, which integrates external APIs for accessing additional information and computational capabilities, evaluated using datasets from the Multi-Modal Language Understanding (MMLU) collection.

**Key Contributions:**

	1. Introduction of the Athena framework for LLMs
	2. Demonstrated significant improvement over baseline models in educational contexts
	3. Evaluation with comprehensive datasets from MMLU

**Result:** Achieved 83% accuracy in mathematical reasoning and 88% in scientific reasoning, outperforming state-of-the-art models such as GPT-4o and LLaMA-Large.

**Limitations:** 

**Conclusion:** The framework shows promise for creating better computing ecosystems around LLMs to support various educational tasks and activities effectively.

**Abstract:** This paper deals with improving querying large language models (LLMs). It is well-known that without relevant contextual information, LLMs can provide poor quality responses or tend to hallucinate. Several initiatives have proposed integrating LLMs with external tools to provide them with up-to-date data to improve accuracy. In this paper, we propose a framework to integrate external tools to enhance the capabilities of LLMs in answering queries in educational settings. Precisely, we develop a framework that allows accessing external APIs to request additional relevant information. Integrated tools can also provide computational capabilities such as calculators or calendars. The proposed framework has been evaluated using datasets from the Multi-Modal Language Understanding (MMLU) collection. The data consists of questions on mathematical and scientific reasoning. Results compared to state-of-the-art language models show that the proposed approach significantly improves performance. Our Athena framework achieves 83% accuracy in mathematical reasoning and 88% in scientific reasoning, substantially outperforming all tested models including GPT-4o, LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline model (LLaMA-Large) achieving only 67% and 79% respectively. These promising results open the way to creating complex computing ecosystems around LLMs to make their use more natural to support various tasks and activities.

</details>


### [29] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)

*Deepali Mishra, Chaklam Silpasuwanchai, Ashutosh Modi, Madhumita Sushil, Sorayouth Chumnanvej*

**Main category:** cs.CL

**Keywords:** Medical Visual Question Answering, radiology, clinical integration

**Relevance Score:** 9

**TL;DR:** This study reviews 68 publications on Medical Visual Question Answering (MedVQA) and surveys 50 clinicians to assess its integration into clinical workflows, highlighting practical utility, challenges, and gaps.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and practical utility of MedVQA in clinical settings, assessing both literature and clinician perspectives.

**Method:** A systematic review of 68 publications and a survey of 50 clinicians using the Arksey and O'Malley scoping review framework.

**Key Contributions:**

	1. Identification of key concepts and research gaps in MedVQA for radiology workflows
	2. Insights from clinicians on the relevance and preferences for MedVQA systems
	3. Highlighting the importance of multi-view imaging and interactive dialogue systems in MedVQA

**Result:** The review found that 60% of QA pairs are non-diagnostic, and there is a significant disconnect between model capabilities and clinical needs, with only 29.8% of clinicians finding MedVQA highly useful.

**Limitations:** Challenges such as limited multimodal analysis, lack of patient context, and misaligned evaluation approaches need to be addressed for effective clinical integration.

**Conclusion:** For MedVQA to be effectively integrated into clinical workflows, it needs improvements in multimodal analysis, inclusion of patient context, and alignment of evaluation metrics with clinical needs.

**Abstract:** Medical Visual Question Answering (MedVQA) is a promising tool to assist radiologists by automating medical image interpretation through question answering. Despite advances in models and datasets, MedVQA's integration into clinical workflows remains limited. This study systematically reviews 68 publications (2018-2024) and surveys 50 clinicians from India and Thailand to examine MedVQA's practical utility, challenges, and gaps. Following the Arksey and O'Malley scoping review framework, we used a two-pronged approach: (1) reviewing studies to identify key concepts, advancements, and research gaps in radiology workflows, and (2) surveying clinicians to capture their perspectives on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs are non-diagnostic and lack clinical relevance. Most datasets and models do not support multi-view, multi-resolution imaging, EHR integration, or domain knowledge, features essential for clinical diagnosis. Furthermore, there is a clear mismatch between current evaluation metrics and clinical needs. The clinician survey confirms this disconnect: only 29.8% consider MedVQA systems highly useful. Key concerns include the absence of patient history or domain knowledge (87.2%), preference for manually curated datasets (51.1%), and the need for multi-view image support (78.7%). Additionally, 66% favor models focused on specific anatomical regions, and 89.4% prefer dialogue-based interactive systems. While MedVQA shows strong potential, challenges such as limited multimodal analysis, lack of patient context, and misaligned evaluation approaches must be addressed for effective clinical integration.

</details>


### [30] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)

*Matan Vetzler, Koren Lazar, Guy Uziel, Eran Hirsch, Ateret Anaby-Tavor, Leshem Choshen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Dataset, Plan Generation, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces CRISP, a dataset for generating high-level plans that improve reasoning in LLMs, demonstrating that fine-tuning on this dataset enhances plan generation significantly.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Advancements in LLMs have revealed insufficient reasoning capabilities, prompting the need for stronger methods in solving complex problems.

**Method:** The authors introduce the CRISP dataset for high-level planning and evaluate the performance of fine-tuning a small model on this dataset against larger models using few-shot prompting.

**Key Contributions:**

	1. Introduction of the CRISP dataset for multi-domain high-level plan generation.
	2. Demonstration of the effectiveness of fine-tuning a smaller model over larger ones in generating plans.
	3. Evidence of transferability in plan generation capabilities across different domains.

**Result:** Fine-tuning on the CRISP dataset allows a smaller model to generate higher-quality plans than larger models, outperforming traditional Chain-of-Thought reasoning.

**Limitations:** 

**Conclusion:** The study indicates that fine-tuning on one domain enhances the model's plan generation abilities across multiple domains, underscoring the importance of learned planning capabilities.

**Abstract:** Recent advancements in large language models (LLMs) underscore the need for stronger reasoning capabilities to solve complex problems effectively. While Chain-of-Thought (CoT) reasoning has been a step forward, it remains insufficient for many domains. A promising alternative is explicit high-level plan generation, but existing approaches largely assume that LLMs can produce effective plans through few-shot prompting alone, without additional training. In this work, we challenge this assumption and introduce CRISP (Complex Reasoning with Interpretable Step-based Plans), a multi-domain dataset of high-level plans for mathematical reasoning and code generation. The plans in CRISP are automatically generated and rigorously validated--both intrinsically, using an LLM as a judge, and extrinsically, by evaluating their impact on downstream task performance. We demonstrate that fine-tuning a small model on CRISP enables it to generate higher-quality plans than much larger models using few-shot prompting, while significantly outperforming Chain-of-Thought reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning on one domain improves plan generation in the other, highlighting the generalizability of learned planning capabilities.

</details>


### [31] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)

*Talor Abramovich, Gal Chechik*

**Main category:** cs.CL

**Keywords:** autonomous agents, language models, ablation experiments, empirical AI research, benchmark suite

**Relevance Score:** 6

**TL;DR:** Introduction of AblationBench, a benchmark for evaluating language model agents on ablation planning tasks in AI research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To support or automate parts of the research process using language model-based autonomous agents, specifically in designing ablation experiments.

**Method:** Two tasks are introduced: AuthorAblation for proposing ablation experiments and ReviewerAblation for identifying missing ablations in papers, both evaluated using LM-based judges.

**Key Contributions:**

	1. Introduction of AblationBench benchmark suite
	2. Development of two distinct ablation planning tasks
	3. Implementation of LM-based judges for automatic evaluation

**Result:** Experiments reveal that the best LMs are only able to identify 29% of the original ablations on average, indicating the tasks' difficulty.

**Limitations:** Current LMs struggle with identifying ablations, demonstrating that improvements are needed in their capabilities.

**Conclusion:** Chain-of-thought prompting outperforms existing agent-based approaches, highlighting limitations in current LMs for ablation tasks.

**Abstract:** Autonomous agents built on language models (LMs) are showing increasing popularity in many fields, including scientific research. AI co-scientists aim to support or automate parts of the research process using these agents. A key component of empirical AI research is the design of ablation experiments. To this end, we introduce AblationBench, a benchmark suite for evaluating agents on ablation planning tasks in empirical AI research. It includes two tasks: AuthorAblation, which helps authors propose ablation experiments based on a method section and contains 83 instances, and ReviewerAblation, which helps reviewers find missing ablations in a full paper and contains 350 instances. For both tasks, we develop LM-based judges that serve as an automatic evaluation framework. Our experiments with frontier LMs show that these tasks remain challenging, with the best-performing LM system identifying only 29% of the original ablations on average. Lastly, we analyze the limitations of current LMs on these tasks, and find that chain-of-thought prompting outperforms the currently existing agent-based approach.

</details>


### [32] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)

*Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Zibin Zheng*

**Main category:** cs.CL

**Keywords:** LLM inference, KV cache, attention similarity, multi-turn conversations, compression strategy

**Relevance Score:** 9

**TL;DR:** Krul is a system for efficient state restoration in multi-turn conversations using LLMs, dynamically optimizing KV cache compression based on attention similarity, achieving significant reductions in time and storage without degrading quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To solve the challenge of inefficient state restoration in multi-turn conversations with LLMs due to fixed compression schemes that lead to accuracy loss.

**Method:** Krul utilizes a dynamic compression strategy based on attention similarity, with a recomputation-loading pipeline for restoring KV caches, incorporating a preemptive selector, heterogeneous similarity estimator, and a optimized restoration scheduler.

**Key Contributions:**

	1. Dynamic KV cache compression based on attention similarity
	2. Preemptive compression strategy selector for conversational context
	3. Bubble-free restoration scheduler to optimize performance

**Result:** Empirical evaluations show Krul reduces time-to-first-token by 1.5x-2.68x and KV cache storage by 1.33x-2.35x compared to state-of-the-art methods while maintaining generation quality.

**Limitations:** 

**Conclusion:** The dynamic approach of Krul offers a nuanced solution to improve efficiency in multi-turn LLM inference and state restoration.

**Abstract:** Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.

</details>


### [33] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)

*Sebastian Walter, Hannah Bast*

**Main category:** cs.CL

**Keywords:** SPARQL, RDF, knowledge graphs, natural language processing, large language models

**Relevance Score:** 8

**TL;DR:** The paper presents a novel method for generating SPARQL queries from natural language using a large language model without the need for fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generation of SPARQL queries from natural language inputs without requiring fine-tuning on the model, thereby enhancing accessibility for users in querying RDF knowledge graphs.

**Method:** Utilizes a large language model to explore knowledge graphs by dynamically executing SPARQL queries and identifying relevant IRIs and literals, along with evaluation against various benchmarks and models.

**Key Contributions:**

	1. State-of-the-art performance on Wikidata in zero-shot settings
	2. Comparison of different searching strategies in knowledge graphs
	3. Evaluation across multiple knowledge graph types and sizes

**Result:** Achieves state-of-the-art results on Wikidata and near-best performance on Freebase in a zero-shot setup, with good results on less common knowledge graphs as well.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates the effectiveness of using large language models for SPARQL query generation without fine-tuning, with promising performance across various benchmarks.

**Abstract:** We propose a new approach for generating SPARQL queries on RDF knowledge graphs from natural language questions or keyword queries, using a large language model. Our approach does not require fine-tuning. Instead, it uses the language model to explore the knowledge graph by strategically executing SPARQL queries and searching for relevant IRIs and literals. We evaluate our approach on a variety of benchmarks (for knowledge graphs of different kinds and sizes) and language models (of different scales and types, commercial as well as open-source) and compare it with existing approaches. On Wikidata we reach state-of-the-art results on multiple benchmarks, despite the zero-shot setting. On Freebase we come close to the best few-shot methods. On other, less commonly evaluated knowledge graphs and benchmarks our approach also performs well overall. We conduct several additional studies, like comparing different ways of searching the graphs, incorporating a feedback mechanism, or making use of few-shot examples.

</details>


### [34] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)

*Reilly Raab, Mike Parker, Dan Nally, Sadie Montgomery, Anastasia Bernat, Sai Munikoti, Sameera Horawalavithana*

**Main category:** cs.CL

**Keywords:** language models, human feedback, auditable systems, healthcare, environmental review

**Relevance Score:** 9

**TL;DR:** The paper presents a framework for safely integrating language models into asynchronous code, enabling real-time improvement through expert feedback while ensuring transparency and auditability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To responsibly leverage language models in real-world applications while addressing safety, explainability, and bias concerns.

**Method:** The authors propose a framework that allows for the declaration of statically typed, LM-powered subroutines within asynchronous code, utilizing sparse human expert feedback to enhance performance during use.

**Key Contributions:**

	1. Development of a framework for statically typed LM-powered subroutines
	2. Implementation of real-time performance improvement through expert feedback
	3. Creation of CommentNEPA for public comment processing

**Result:** The framework is packaged as a library and evaluated through an application called CommentNEPA, designed to process and summarize public comments for environmental reviews, demonstrating effectiveness in comparisons to historical data.

**Limitations:** 

**Conclusion:** The proposed framework allows for safer and more effective use of language models in decision-making processes across various fields, including healthcare and legal domains.

**Abstract:** The advent of language models (LMs) has the potential to dramatically accelerate tasks that may be cast to text-processing; however, real-world adoption is hindered by concerns regarding safety, explainability, and bias. How can we responsibly leverage LMs in a transparent, auditable manner -- minimizing risk and allowing human experts to focus on informed decision-making rather than data-processing or prompt engineering? In this work, we propose a framework for declaring statically typed, LM-powered subroutines (i.e., callable, function-like procedures) for use within conventional asynchronous code -- such that sparse feedback from human experts is used to improve the performance of each subroutine online (i.e., during use). In our implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and data-dependencies) are recorded and exposed to audit on demand. We package this framework as a library to support its adoption and continued development. While this framework may be applicable across several real-world decision workflows (e.g., in healthcare and legal fields), we evaluate it in the context of public comment processing as mandated by the 1969 National Environmental Protection Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an application that compiles, organizes, and summarizes a corpus of public commentary submitted in response to a project requiring environmental review. We quantitatively evaluate the application by comparing its outputs (when operating without human feedback) to historical ``ground-truth'' data as labelled by human annotators during the preparation of official environmental impact statements.

</details>


### [35] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)

*Vivek Chari, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** Large Language Models, KV cache, compression, approximate leverage scores, context-calibrated

**Relevance Score:** 8

**TL;DR:** Compactor is a query-agnostic strategy for compressing KV caches in LLMs, achieving significant memory savings while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are trained for larger context windows, the memory requirements of the KV cache become a bottleneck, complicating deployment in real-world scenarios.

**Method:** Compactor uses approximate leverage scores to determine token importance for KV cache compression without needing query information. It includes a context-calibrated compression procedure to infer optimal compression ratios.

**Key Contributions:**

	1. Introduction of a parameter-free, query-agnostic KV cache compression strategy.
	2. Demonstration of significant memory savings while retaining performance.
	3. Development of context-calibrated compression for optimizing compression ratios.

**Result:** Compactor retains 1/2 the tokens while achieving comparable performance to existing methods and reduces KV memory by 63% on average across various tasks.

**Limitations:** 

**Conclusion:** Compactor effectively addresses memory challenges in LLM deployments, demonstrating both efficacy and generalizability.

**Abstract:** Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. Unfortunately the ability to use long contexts in generation is complicated by the large memory requirement of the KV cache, which scales linearly with the context length. This memory footprint is often the dominant resource bottleneck in real-world deployments, limiting throughput and increasing serving cost. One way to address this is by compressing the KV cache, which can be done either with knowledge of the question being asked (query-aware) or without knowledge of the query (query-agnostic). We present Compactor, a parameter-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 1/2 the tokens in both synthetic and real-world context tasks, with minimal computational overhead. We further introduce a procedure for context-calibrated compression, which allows one to infer the maximum compression ratio a given context can support. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 63%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families.

</details>


### [36] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)

*Henry J. Xie, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu*

**Main category:** cs.CL

**Keywords:** Empathy distillation, LLMs, SLMs, Human-Computer Interaction, Fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper presents a method for distilling empathy from Large Language Models (LLMs) into Smaller Language Models (SLMs), enhancing their empathetic response capabilities while reducing model size, which is vital for resource-constrained environments.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to retain empathy in Smaller Language Models after distillation from Larger Language Models is paramount for effective human interaction in resource-constrained scenarios like smartphones.

**Method:** The paper develops a comprehensive two-step fine-tuning process utilizing datasets of empathetic dialogue responses, exploring various distillation methods and proposing four unique prompt sets for empathy enhancement.

**Key Contributions:**

	1. Comprehensive approach for empathy distillation from LLMs to SLMs
	2. Two-step fine-tuning process leveraging empathetic dialogue datasets
	3. Four novel prompt sets for targeted empathy improvement.

**Result:** SLMs fine-tuned with the proposed methods achieved a 90% win rate in generating empathetic responses, significantly outperforming base SLMs and showing a 10% improvement over basic direct prompting.

**Limitations:** 

**Conclusion:** The proposed empathy distillation approach successfully enhances the empathetic capabilities of smaller models, which is crucial for applications in environments where interactions are frequent but resources are limited.

**Abstract:** The distillation of knowledge from Large Language Models (LLMs) into Smaller Language Models (SLMs), preserving the capabilities and performance of LLMs while reducing model size, has played a key role in the proliferation of LLMs. Because SLMs are considerably smaller than LLMs, they are often utilized in domains where human interaction is frequent but resources are highly constrained, e.g., smart phones. Therefore, it is crucial to ensure that empathy, a fundamental aspect of positive human interactions, already instilled into LLMs, is retained by SLMs after distillation. In this paper, we develop a comprehensive approach for effective empathy distillation from LLMs into SLMs. Our approach features a two-step fine-tuning process that fully leverages datasets of empathetic dialogue responses distilled from LLMs. We explore several distillation methods beyond basic direct prompting and propose four unique sets of prompts for targeted empathy improvement to significantly enhance the empathy distillation process. Our evaluations demonstrate that SLMs fine-tuned through the two-step fine-tuning process with distillation datasets enhanced by the targeted empathy improvement prompts significantly outperform the base SLM at generating empathetic responses with a win rate of 90%. Our targeted empathy improvement prompts substantially outperform the basic direct prompting with a 10% improvement in win rate.

</details>


### [37] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)

*Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sungmin Kang, Alperen Öziş, Hayrettin Eren Yildiz, Mitash Ashish Shah, Zhiqi Huang, Anoop Kumar, Alfy Samuel, Daben Liu, Sai Praneeth Karimireddy, Salman Avestimehr*

**Main category:** cs.CL

**Keywords:** Generative LLMs, Truthfulness Prediction, Open-source Library

**Relevance Score:** 8

**TL;DR:** Introduction of TruthTorchLM, an open-source library for truthfulness prediction of LLM outputs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for reliable truthfulness prediction in generative LLMs, especially in critical contexts, by providing a comprehensive toolkit for researchers.

**Method:** Developed an open-source Python library with over 30 methods for truthfulness prediction, compatible with HuggingFace and LiteLLM, featuring diverse approaches based on computational cost and supervision type.

**Key Contributions:**

	1. Introduction of a comprehensive library with over 30 truth prediction methods.
	2. Compatibility with popular LLM frameworks like HuggingFace and LiteLLM.
	3. Evaluation of methods against multiple datasets.

**Result:** TruthTorchLM was evaluated across three datasets, demonstrating its broad applicability compared to existing toolkits.

**Limitations:** 

**Conclusion:** TruthTorchLM offers a versatile, comprehensive solution for truthfulness prediction in LLM outputs, encouraging further research and development in this critical area.

**Abstract:** Generative Large Language Models (LLMs)inevitably produce untruthful responses. Accurately predicting the truthfulness of these outputs is critical, especially in high-stakes settings. To accelerate research in this domain and make truthfulness prediction methods more accessible, we introduce TruthTorchLM an open-source, comprehensive Python library featuring over 30 truthfulness prediction methods, which we refer to as Truth Methods. Unlike existing toolkits such as Guardrails, which focus solely on document-grounded verification, or LM-Polygraph, which is limited to uncertainty-based methods, TruthTorchLM offers a broad and extensible collection of techniques. These methods span diverse tradeoffs in computational cost, access level (e.g., black-box vs white-box), grounding document requirements, and supervision type (self-supervised or supervised). TruthTorchLM is seamlessly compatible with both HuggingFace and LiteLLM, enabling support for locally hosted and API-based models. It also provides a unified interface for generation, evaluation, calibration, and long-form truthfulness prediction, along with a flexible framework for extending the library with new methods. We conduct an evaluation of representative truth methods on three datasets, TriviaQA, GSM8K, and FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [38] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)

*Atticus Wang, Joshua Engels, Oliver Clive-Griffin*

**Main category:** cs.CL

**Keywords:** out-of-distribution generalization, fine-tuning, LoRA, large language models, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper investigates how LoRA fine-tuning in LLMs enables out-of-context reasoning (OOCR) by adding a constant steering vector that improves generalization across tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanism behind out-of-context reasoning in fine-tuned large language models and its implications for LLM deployment.

**Method:** The authors analyze the role of LoRA fine-tuning and propose that it introduces a constant steering vector that enhances model performance and generalization in related domains.

**Key Contributions:**

	1. Identification of steering vectors in LoRA fine-tuning as a key factor in OOCR.
	2. Evidence that OOCR can be induced even without conditional behavior.
	3. Insights on the implications for the deployment of LLMs in real-world tasks.

**Result:** The study finds that many instances of OOCR can be explained by this steering vector, which allows LLMs to generalize unexpectedly well, even in tasks typically reliant on conditional behavior.

**Limitations:** 

**Conclusion:** The findings suggest that understanding the learning process of fine-tuning can improve the reliability and safety of LLMs in various applications.

**Abstract:** Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs exhibit surprisingly deep out-of-distribution generalization. Rather than learning shallow heuristics, they implicitly internalize and act on the consequences of observations scattered throughout the fine-tuning data. In this work, we investigate this phenomenon mechanistically and find that many instances of OOCR in the literature have a simple explanation: the LoRA fine-tuning essentially adds a constant steering vector, steering the model towards a general concept. This improves performance on the fine-tuning task and in many other concept-related domains, causing the surprising generalization. Moreover, we can directly train steering vectors for these tasks from scratch, which also induces OOCR. We find that our results hold even for a task that seems like it must involve conditional behavior (model backdoors); it turns out that unconditionally adding a steering vector is sufficient. Overall, our work presents one explanation of what gets learned during fine-tuning for OOCR tasks, contributing to the key question of why LLMs can reason out of context, an advanced capability that is highly relevant to their safe and reliable deployment.

</details>


### [39] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)

*KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Intelligent Tutoring Systems, Item Response Theory, Educational Assessment, Proxy Students

**Relevance Score:** 9

**TL;DR:** The paper examines the effectiveness of Large Language Models (LLMs) as proxies for real students in educational assessments, finding that strong models outperform average students, while suggesting improvements for training and evaluation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how accurately LLMs can emulate real students' behavior in educational settings, particularly in Intelligent Tutoring Systems and test question piloting.

**Method:** A dataset of 489 items from the National Assessment of Educational Progress (NAEP) was analyzed using an Item Response Theory (IRT) model to compare 11 LLMs against real student populations in grades 4, 8, and 12.

**Key Contributions:**

	1. Analysis of LLMs using Item Response Theory on educational datasets
	2. Identification of performance gaps between LLMs and average students
	3. Guidelines for selecting effective LLM proxies in educational contexts

**Result:** Strong general-purpose LLMs consistently outperform average students across grades, while weaker models only incidentally align with students. Performance is significantly affected by prompts, varying model efficacy across subjects and grades.

**Limitations:** Performance may vary widely based on specific model-prompt combinations, indicating the need for further research.

**Conclusion:** The study highlights the limitations of current models in serving as proxies for students and calls for enhanced training and evaluation practices, along with specific guidelines for proxy selection.

**Abstract:** Large Language Models (LLMs) are increasingly used as proxy students in the development of Intelligent Tutoring Systems (ITSs) and in piloting test questions. However, to what extent these proxy students accurately emulate the behavior and characteristics of real students remains an open question. To investigate this, we collected a dataset of 489 items from the National Assessment of Educational Progress (NAEP), covering mathematics and reading comprehension in grades 4, 8, and 12. We then apply an Item Response Theory (IRT) model to position 11 diverse and state-of-the-art LLMs on the same ability scale as real student populations. Our findings reveal that, without guidance, strong general-purpose models consistently outperform the average student at every grade, while weaker or domain-mismatched models may align incidentally. Using grade-enforcement prompts changes models' performance, but whether they align with the average grade-level student remains highly model- and prompt-specific: no evaluated model-prompt pair fits the bill across subjects and grades, underscoring the need for new training and evaluation strategies. We conclude by providing guidelines for the selection of viable proxies based on our findings.

</details>


### [40] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)

*Ancita Maria Andrade, Tanvi Banerjee, Ramakrishna Mundugar*

**Main category:** cs.CL

**Keywords:** pain, gender differences, Natural Language Processing, HAM-CNN, health informatics

**Relevance Score:** 7

**TL;DR:** This study employs NLP to analyze gender differences in pain experiences, utilizing a Hidden Attribute Model-CNN for classification, and revealing significant linguistic and prevalence differences between genders.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the role of gender in pain experiences which was overlooked in earlier studies.

**Method:** Used the Hidden Attribute Model-Convolutional Neural Network (HAM-CNN) to classify posts into male and female corpora, achieving an F1 score of 0.86 by aggregating posts based on usernames.

**Key Contributions:**

	1. Application of NLP for analyzing gender differences in pain
	2. Introduction of HAM-CNN for post classification
	3. Identification of gender-specific linguistic patterns in pain-related discussions

**Result:** The analysis identified linguistic differences in posts, revealing that female posts are more emotionally focused. Conditions like migraine and sinusitis are more prevalent in females, with differing effects of pain medication between genders.

**Limitations:** 

**Conclusion:** The findings underline the importance of considering gender differences in pain experiences and treatment approaches.

**Abstract:** Pain is an inherent part of human existence, manifesting as both physical and emotional experiences, and can be categorized as either acute or chronic. Over the years, extensive research has been conducted to understand the causes of pain and explore potential treatments, with contributions from various scientific disciplines. However, earlier studies often overlooked the role of gender in pain experiences. In this study, we utilized Natural Language Processing (NLP) to analyze and gain deeper insights into individuals' pain experiences, with a particular focus on gender differences. We successfully classified posts into male and female corpora using the Hidden Attribute Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by aggregating posts based on usernames. Our analysis revealed linguistic differences between genders, with female posts tending to be more emotionally focused. Additionally, the study highlighted that conditions such as migraine and sinusitis are more prevalent among females and explored how pain medication affects individuals differently based on gender.

</details>


### [41] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)

*Zizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, Bing Yu*

**Main category:** cs.CL

**Keywords:** large language model, reasoning-intensive tasks, reinforcement learning, knowledge distillation, computer science

**Relevance Score:** 9

**TL;DR:** KAT is an open-source 40B large language model addressing reasoning-intensive tasks by dynamically switching between reasoning and non-reasoning modes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the overthinking problem in reasoning-intensive tasks within AI applications.

**Method:** The model utilizes a dual-regime dataset with a tagging pipeline, enhanced knowledge distillation via Multi-Token Prediction, and a cold-start initialization strategy. Additionally, it employs a reinforcement learning algorithm called Step-SRPO for guided reasoning-mode selection and improved response accuracy.

**Key Contributions:**

	1. Dynamic reasoning-mode switching based on task complexity
	2. Novel tagging pipeline for dual-regime dataset construction
	3. Introduction of reinforcement learning for structured reasoning guidance

**Result:** KAT matches or outperforms state-of-the-art models, reduces token usage by 30%, and has been successfully deployed in a coding assistant, enhancing real-world development workflows.

**Limitations:** 

**Conclusion:** KAT showcases significant efficiency and controllability in reasoning behaviors, with ongoing development of a larger 200B Mixture-of-Experts model indicating strong scalability.

**Abstract:** We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model developed to address the overthinking problem in reasoning-intensive tasks, where an automatic thinking training paradigm is proposed to dynamically switch between reasoning and non-reasoning modes based on task complexity. Specifically, first, we construct the dual-regime dataset based on a novel tagging pipeline and a multi-agent synthesis strategy, and then we apply Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling efficient and fine-grained reasoning transfer with minimal pretraining cost. Besides, we implement a cold-start initialization strategy that introduces mode-selection priors using majority-vote signals and intent-aware prompting. Finally, we propose Step-SRPO, a reinforcement learning algorithm that incorporates intermediate supervision into the GRPO framework, offering structured guidance over both reasoning-mode selection and response accuracy. Extensive experiments across multiple benchmarks demonstrate that KAT consistently matches or even outperforms current state-of-the-art models, including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of reasoning-intensive tasks while reducing token usage by up to approximately 30\%. Beyond academic evaluation, KAT has been successfully deployed in Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world development workflows with high accuracy, efficiency, and controllable reasoning behaviors. Moreover, we are actively training a 200B Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage results already demonstrate promising improvements in performance and efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [42] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)

*Yupu Liang, Yaping Zhang, Zhiyang Zhang, Zhiyuan Chen, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Document Image Machine Translation, Optical Character Recognition

**Relevance Score:** 8

**TL;DR:** This paper presents a novel fine-tuning paradigm, Synchronously Self-Reviewing (SSR), for Multimodal Large Language Models (MLLMs) to enhance their performance in Document Image Machine Translation (DIMT) without sacrificing their Optical Character Recognition (OCR) capabilities.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the challenges faced by MLLMs in DIMT tasks, particularly the problem of catastrophic forgetting when models are fine-tuned on DIMT datasets.

**Method:** The authors introduce Synchronously Self-Reviewing (SSR), where models generate OCR text before producing translation text to leverage their existing OCR capabilities while learning to translate.

**Key Contributions:**

	1. Introduction of the Synchronously Self-Reviewing (SSR) fine-tuning paradigm
	2. Demonstrated improvement in DIMT capabilities without sacrificing OCR abilities
	3. Reduction of catastrophic forgetting in MLLMs during training

**Result:** Experiments show that the SSR approach reduces catastrophic forgetting and enhances the generalization ability of MLLMs on both OCR and DIMT tasks.

**Limitations:** The paper does not explore the impact of SSR on other multimodal tasks beyond OCR and DIMT.

**Conclusion:** The SSR paradigm effectively allows MLLMs to retain their OCR proficiency while improving performance on cross-lingual translation tasks.

**Abstract:** Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model's existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept "Bilingual Cognitive Advantage". Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks.

</details>


### [43] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)

*Yinzhu Quan, Xinrui Li, Ying Chen*

**Main category:** cs.CL

**Keywords:** e-commerce, CRM, large language models, template generation, customer engagement

**Relevance Score:** 6

**TL;DR:** CRMAgent is a multi-agent system using LLMs to generate high-quality marketing message templates and guidance focused on e-commerce merchants.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Merchants in e-commerce struggle with crafting persuasive copy for customer engagement due to a lack of expertise and tools, hindering their CRM efforts.

**Method:** The system employs group-based learning from a merchant’s top-performing messages, retrieval-and-adaptation for template sourcing, and a rule-based fallback for zero-shot rewrites when needed.

**Key Contributions:**

	1. Development of CRMAgent leveraging LLMs for CRM applications
	2. Integration of group-based learning, template retrieval, and rule-based rewriting
	3. Demonstrated effectiveness through extensive experimental results

**Result:** CRMAgent outperforms original merchant templates, showing significant improvements in audience match and marketing effectiveness metrics.

**Limitations:** 

**Conclusion:** The use of CRMAgent can enhance merchant messaging strategies and improve customer engagement outcomes in e-commerce settings.

**Abstract:** In e-commerce private-domain channels such as instant messaging and e-mail, merchants engage customers directly as part of their Customer Relationship Management (CRM) programmes to drive retention and conversion. While a few top performers excel at crafting outbound messages, most merchants struggle to write persuasive copy because they lack both expertise and scalable tools. We introduce CRMAgent, a multi-agent system built on large language models (LLMs) that generates high-quality message templates and actionable writing guidance through three complementary modes. First, group-based learning enables the agent to learn from a merchant's own top-performing messages within the same audience segment and rewrite low-performing ones. Second, retrieval-and-adaptation fetches templates that share the same audience segment and exhibit high similarity in voucher type and product category, learns their successful patterns, and adapts them to the current campaign. Third, a rule-based fallback provides a lightweight zero-shot rewrite when no suitable references are available. Extensive experiments show that CRMAgent consistently outperforms merchants' original templates, delivering significant gains in both audience-match and marketing-effectiveness metrics.

</details>


### [44] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)

*Yuzheng Xu, Tosho Hirasawa, Seiya Kawano, Shota Kato, Tadashi Kozuno*

**Main category:** cs.CL

**Keywords:** patent ideation, prompt engineering, AI-based product development

**Relevance Score:** 7

**TL;DR:** MK2 is a prompt-centric pipeline that generates viable product ideas from real patents within three years, outperforming existing models in ideation quality.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Transform real patents into commercially viable product ideas using prompt engineering techniques.

**Method:** MK2 utilizes a prompt-centric approach where Gemini 2.5 drafts and edits prompts, GPT-4.1 generates ideas from these prompts, and an Elo loop with Qwen3-8B evaluates and selects the best prompts without additional training data.

**Key Contributions:**

	1. Introduction of MK2 pipeline for patent-based ideation
	2. Demonstrated effectiveness in multiple domains
	3. Competitive performance without extra training data

**Result:** MK2 achieved top rankings in the automatic leaderboard across three domains, winning 25 out of 36 tests, with the exception of the materials-chemistry track which indicated the need for more domain grounding.

**Limitations:** Limited performance in the materials-chemistry domain indicating a need for deeper grounding in that area.

**Conclusion:** Lightweight prompt engineering shows promise in delivering competitive and relevant product ideation from patents within a short timeframe.

**Abstract:** The Patent-Based Idea Generation task asks systems to turn real patents into product ideas viable within three years. We propose MK2, a prompt-centric pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all without extra training data. Across three domains, two evaluator types, and six criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the materials-chemistry track lagged, indicating the need for deeper domain grounding; yet, the results show that lightweight prompt engineering has already delivered competitive, commercially relevant ideation from patents.

</details>


### [45] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)

*Zhichao Xu, Zhiqi Huang, Shengyao Zhuang, Ashim Gupta, Vivek Srikumar*

**Main category:** cs.CL

**Keywords:** text rerankers, contrastive learning, knowledge distillation, information retrieval, machine learning

**Relevance Score:** 6

**TL;DR:** This paper compares contrastive learning and knowledge distillation for training text rerankers, finding knowledge distillation generally yields better performance when using a larger teacher model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To empirically compare the effectiveness of contrastive learning and knowledge distillation for training cross-encoder rerankers in information retrieval.

**Method:** Rerankers of different sizes and architectures were trained using contrastive learning and knowledge distillation on the same dataset, with a strong contrastive learning model as the distillation teacher.

**Key Contributions:**

	1. Empirical comparison of training strategies for text rerankers.
	2. Demonstration of knowledge distillation's advantages over contrastive learning.
	3. Guidance on training strategies based on teacher model availability.

**Result:** Knowledge distillation generally results in superior ranking performance for both in-domain and out-of-domain tasks compared to contrastive learning when a larger teacher model is available.

**Limitations:** Distillation from a teacher of the same capacity does not provide advantages, especially for out-of-domain tasks.

**Conclusion:** Knowledge distillation is recommended for training smaller rerankers when a larger teacher is accessible; otherwise, contrastive learning is a reliable alternative.

**Abstract:** Training text rerankers is crucial for information retrieval. Two primary strategies are widely used: contrastive learning (optimizing directly on ground-truth labels) and knowledge distillation (transferring knowledge from a larger reranker). While both have been studied in the literature, a clear comparison of their effectiveness for training cross-encoder rerankers under practical conditions is needed.   This paper empirically compares these strategies by training rerankers of different sizes and architectures using both methods on the same data, with a strong contrastive learning model acting as the distillation teacher. Our results show that knowledge distillation generally yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model. This finding is consistent across student model sizes and architectures. However, distilling from a teacher of the same capacity does not provide the same advantage, particularly for out-of-domain tasks. These findings offer practical guidance for choosing a training strategy based on available teacher models. Therefore, we recommend using knowledge distillation to train smaller rerankers if a larger, more powerful teacher is accessible; in its absence, contrastive learning provides a strong and more reliable alternative otherwise.

</details>


### [46] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)

*Peng Wang, Xuesi Hu, Jiageng Wu, Yuntao Zou, Qiancheng Zhang, Dagang Li*

**Main category:** cs.CL

**Keywords:** large language models, reasoning large language models, financial question answering, prompting methods, multilingual alignment

**Relevance Score:** 7

**TL;DR:** This study investigates the performance enhancement of LLMs and RLLMs in financial question-answering tasks via various prompting methods and agent frameworks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how different methods can unlock the full potential of LLMs and RLLMs in the financial domain, particularly for complex problem-solving.

**Method:** Five LLMs and three RLLMs are utilized to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks.

**Key Contributions:**

	1. Investigation of performance enhancement methods for LLMs and RLLMs in finance.
	2. Empirical comparison of various prompting and alignment strategies.
	3. Insights into the limitations of traditional methods on RLLMs.

**Result:** Current prompting methods and agent frameworks boost LLM performance via simulated Long CoT; RLLMs demonstrate inherent Long CoT capabilities that limit conventional methods' effectiveness; advanced multilingual alignment methods mainly enhance LLM multilingual performance with little impact on RLLMs.

**Limitations:** Limited focus on domains outside of financial question answering, and the complexity inherent in assessing LLMs vs RLLMs.

**Conclusion:** The findings provide valuable insights on leveraging LLMs and RLLMs for financial question answering, with implications for future research and applications.

**Abstract:** Recently, the development of large language models (LLMs) and reasoning large language models (RLLMs) have gained considerable attention from many researchers. RLLMs enhance the reasoning capabilities of LLMs through Long Chain-of-Thought (Long CoT) processes, significantly improving the performance of LLMs in addressing complex problems. However, there are few works that systematically explore what methods can fully unlock the performance of LLMs and RLLMs within the financial domain. To investigate the impact of various methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks. Our research findings indicate: (1) Current prompting methods and agent frameworks enhance the performance of LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess inherent Long CoT capabilities, which limits the effectiveness of conventional methods in further enhancing their performance; (3) Current advanced multilingual alignment methods primarily improve the multilingual performance of LLMs by extending the reasoning length, which yields minimal benefits for RLLMs. We hope that this study can serve as an important reference for LLMs and RLLMs in the field of financial question answering.

</details>


### [47] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)

*Itai Mondshine, Tzuf Paz-Argaman, Reut Tsarfaty*

**Main category:** cs.CL

**Keywords:** evaluation metrics, n-gram, neural metrics, human judgments, language typology

**Relevance Score:** 6

**TL;DR:** The paper assesses the effectiveness of n-gram-based and neural-based evaluation metrics for generative tasks across various languages, revealing that these metrics' performance varies significantly by language type.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the suitability of automatic n-gram based metrics for different languages in generative tasks and address their correlation with human judgments.

**Method:** A large-scale evaluation suite was designed to analyze n-gram-based and neural-based metrics across eight languages from four typological families, focusing on their correlation with human assessments.

**Key Contributions:**

	1. Systematic assessment of evaluation metrics across various languages and tasks.
	2. Demonstrated the impact of language type on metric effectiveness.
	3. Highlighted the importance of proper tokenization for evaluation in morphologically rich languages.

**Result:** Key findings show that n-gram-based metrics correlate less with human evaluation in fusional languages compared to isolating and agglutinative languages. Proper tokenization improves correlation in fusional languages, and neural-based metrics like COMET outperform other metrics, especially in low-resource settings.

**Limitations:** Focused primarily on specific language families and may not encompass all linguistic diversity.

**Conclusion:** The study shows significant limitations of n-gram metrics in fusional languages and calls for more attention to neural-based evaluation metrics for better alignment with human judgments.

**Abstract:** Automatic n-gram based metrics such as ROUGE are widely used for evaluating generative tasks such as summarization. While these metrics are considered indicative (even if imperfect) of human evaluation for English, their suitability for other languages remains unclear. To address this, we systematically assess evaluation metrics for generation both n-gram-based and neural based to evaluate their effectiveness across languages and tasks. Specifically, we design a large-scale evaluation suite across eight languages from four typological families: agglutinative, isolating, low-fusional, and high-fusional, spanning both low- and high-resource settings, to analyze their correlation with human judgments. Our findings highlight the sensitivity of evaluation metrics to the language type. For example, in fusional languages, n-gram-based metrics show lower correlation with human assessments compared to isolating and agglutinative languages. We also demonstrate that proper tokenization can significantly mitigate this issue for morphologically rich fusional languages, sometimes even reversing negative trends. Additionally, we show that neural-based metrics specifically trained for evaluation, such as COMET, consistently outperform other neural metrics and better correlate with human judgments in low-resource languages. Overall, our analysis highlights the limitations of n-gram metrics for fusional languages and advocates for greater investment in neural-based metrics trained for evaluation tasks.

</details>


### [48] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)

*Keisuke Ueda, Wataru Hirota, Takuto Asakura, Takahiro Omi, Kosuke Takahashi, Kosuke Arima, Tatsuya Ishigaki*

**Main category:** cs.CL

**Keywords:** multi-agent, large language models, scientific ideation, creativity, interaction design

**Relevance Score:** 9

**TL;DR:** This study analyzes multi-agent LLM dialogues for generating and critiquing research ideas, demonstrating how interaction design affects creativity outcomes.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize interactions for creative tasks using multi-agent large language models, transcending mere idea generation to include critique and iterative improvement.

**Method:** A comparative analysis across various multi-agent configurations, including role definitions, number of agents, and dialogue depth, focusing on the implications for novelty and feasibility in idea generation.

**Key Contributions:**

	1. Analysis of multi-agent LLM interaction configurations
	2. Identification of factors influencing idea novelty and feasibility
	3. Practical guidelines for effective multi-agent ideation systems

**Result:** The study finds that increasing the number of agents, deepening interactions, and expanding persona diversity enhances idea diversity, while critic diversity in an iterative process improves the feasibility of proposals.

**Limitations:** 

**Conclusion:** Effective multi-agent LLM systems can significantly enhance scientific ideation by optimizing interaction designs based on specific configuration settings.

**Abstract:** Large language models (LLMs) are increasingly used to support creative tasks such as research idea generation. While recent work has shown that structured dialogues between LLMs can improve the novelty and feasibility of generated ideas, the optimal design of such interactions remains unclear. In this study, we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific ideation. We compare different configurations of agent roles, number of agents, and dialogue depth to understand how these factors influence the novelty and feasibility of generated ideas. Our experimental setup includes settings where one agent generates ideas and another critiques them, enabling iterative improvement. Our results show that enlarging the agent cohort, deepening the interaction depth, and broadening agent persona heterogeneity each enrich the diversity of generated ideas. Moreover, specifically increasing critic-side diversity within the ideation-critique-revision loop further boosts the feasibility of the final proposals. Our findings offer practical guidelines for building effective multi-agent LLM systems for scientific ideation. Our code is available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [49] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)

*Benjamin Newman, Abhilasha Ravichander, Jaehun Jung, Rui Xin, Hamish Ivison, Yegor Kuznetsov, Pang Wei Koh, Yejin Choi*

**Main category:** cs.CL

**Keywords:** language models, hallucination, finetuning, factuality, NLP

**Relevance Score:** 9

**TL;DR:** This paper investigates how the factuality of finetuning data affects hallucination rates in language models, revealing that finetuning on internally judged model-generated data is more effective than using factual gold data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how to reduce hallucinations in language models is critical for improving their reliability and applicability, particularly in areas like health informatics.

**Method:** The study examines the correlation between the quality of finetuning data and the frequency of hallucinations during long-form text generation, evaluating various finetuning strategies.

**Key Contributions:**

	1. Revealed that finetuning on factual gold data is less effective than model-generated data judged by the model.
	2. Identified that filtering strategies based on internal judgments enhance factuality across domains.
	3. Demonstrated that improvements in factuality can transfer across multiple generation domains.

**Result:** Finetuning on model-generated data that aligns with the model's own judgments results in better factuality compared to using factual gold data or other configurations.

**Limitations:** The study focuses on long-form generation tasks, which may not generalize to all forms of language model applications.

**Conclusion:** The findings indicate that a model's internal assessments may serve as a robust guide for selecting finetuning data to mitigate hallucinations in language generation tasks.

**Abstract:** Language models are prone to hallucination - generating text that is factually incorrect. Finetuning models on high-quality factual information can potentially reduce hallucination, but concerns remain; obtaining factual gold data can be expensive and training on correct but unfamiliar data may potentially lead to even more downstream hallucination. What data should practitioners finetune on to mitigate hallucinations in language models? In this work, we study the relationship between the factuality of finetuning data and the prevalence of hallucinations in long-form generation tasks. Counterintuitively, we find that finetuning on factual gold data is not as helpful as finetuning on model-generated data that models believe to be factual. Next, we evaluate filtering strategies applied on both factual gold data and model-generated data, and find that finetuning on model-generated data that is filtered by models' own internal judgments often leads to better overall factuality compared to other configurations: training on gold data filtered by models' judgments, training on gold data alone, or training on model-generated data that is supported by gold data. These factuality improvements transfer across three domains we study, suggesting that a models' own beliefs can provide a powerful signal for factuality.

</details>


### [50] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)

*Lu Xiang, Yang Zhao, Yaping Zhang, Chengqing Zong*

**Main category:** cs.CL

**Keywords:** Large Language Models, interdisciplinary studies, methodologies, applications, research directions

**Relevance Score:** 9

**TL;DR:** This survey paper examines the application of Large Language Models (LLMs) across various disciplines, providing technical insights and exploring their interdisciplinary contributions, challenges, and research directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of LLMs in diverse research disciplines and enhance understanding of their methodologies and applicability.

**Method:** The paper reviews key technical methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration, which inform the application of LLMs in specific disciplines.

**Key Contributions:**

	1. Comprehensive overview of LLM applications in various disciplines.
	2. Identification of technical methodologies enhancing LLM effectiveness.
	3. Highlighting challenges and future research directions in interdisciplinary applications of LLMs.

**Result:** LLMs are shown to enhance processes in various fields including mathematics, physics, chemistry, biology, and the humanities, while challenges and promising research directions are identified.

**Limitations:** 

**Conclusion:** The survey serves as a comprehensive resource for researchers to understand the use of LLMs in interdisciplinary studies and navigate its complexities effectively.

**Abstract:** Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.

</details>


### [51] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)

*Zilu Dong, Xiangqing Shen, Zinong Yang, Rui Xia*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, logical reasoning, knowledge graphs, ripple effect

**Relevance Score:** 9

**TL;DR:** ChainEdit, a framework combining knowledge graph-derived logical rules with LLM reasoning, enables systematic updates and improves logical consistency in knowledge editing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the logical consistency of knowledge editing in large language models which currently faces challenges in maintaining logical coherence.

**Method:** By extracting logical patterns from structured knowledge bases and integrating them with LLMs' reasoning capabilities, ChainEdit generates and edits clusters of logically connected knowledge.

**Key Contributions:**

	1. Development of ChainEdit framework for systematic knowledge updates
	2. Improved logical generalization metrics over previous methods
	3. Addressing evaluation biases with knowledge-aware protocols

**Result:** ChainEdit shows over 30% improvement in logical generalization compared to existing methods while maintaining reliability and specificity in knowledge editing.

**Limitations:** 

**Conclusion:** ChainEdit establishes state-of-the-art performance in managing ripple effects and ensures internal logical consistency in knowledge editing processes.

**Abstract:** Current knowledge editing methods for large language models (LLMs) struggle to maintain logical consistency when propagating ripple effects to associated facts. We propose ChainEdit, a framework that synergizes knowledge graph-derived logical rules with LLM logical reasoning capabilities to enable systematic chain updates. By automatically extracting logical patterns from structured knowledge bases and aligning them with LLMs' internal logics, ChainEdit dynamically generates and edits logically connected knowledge clusters. Experiments demonstrate an improvement of more than 30% in logical generalization over baselines while preserving editing reliability and specificity. We further address evaluation biases in existing benchmarks through knowledge-aware protocols that disentangle external dependencies. This work establishes new state-of-the-art performance on ripple effect while ensuring internal logical consistency after knowledge editing.

</details>


### [52] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)

*Selina Heller, Mohamed Ibrahim, David Antony Selby, Sebastian Vollmer*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-Agent Systems, Decision-Making

**Relevance Score:** 9

**TL;DR:** This paper presents a novel LLM-based multi-agent system designed to simulate decision conferences by detecting agreement among agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs in simulating collaborative decision-making processes and enhancing the efficiency and quality of deliberations.

**Method:** The study evaluates six different LLMs on stance detection and stance polarity detection tasks within a multi-agent system to simulate decision conferences.

**Key Contributions:**

	1. Development of a novel LLM-based multi-agent system for decision conferences
	2. Evaluation of LLMs on stance detection and polarity detection
	3. Demonstration of enhanced efficiency in group debates through agreement detection

**Result:** LLMs reliably detect agreement in dynamic debates, improving the efficiency of group discussions and resulting in decision-making outcomes comparable to real-world scenarios.

**Limitations:** 

**Conclusion:** LLM-based multi-agent systems can effectively simulate group decision-making and support expert elicitation workshops in diverse fields.

**Abstract:** Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. These conferences often rely on facilitated discussions to ensure productive dialogue and collective agreement. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions. In this work, we present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance detection, which identifies the position an agent takes on a given issue, and stance polarity detection, which identifies the sentiment as positive, negative, or neutral. These models are further assessed within the multi-agent system to determine their effectiveness in complex simulations. Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making. These findings demonstrate the potential for LLM-based multi-agent systems to simulate group decision-making processes. They also highlight that such systems could be instrumental in supporting decision-making with expert elicitation workshops across various domains.

</details>


### [53] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)

*Zishan Xu, Shuyi Xie, Qingsong Lv, Shupei Xiao, Linlin Song, Sui Wenjuan, Fan Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, error attribution, Misattribution Framework, AttriData, MisAttributionLLM

**Relevance Score:** 9

**TL;DR:** This paper presents a Misattribution Framework and a dataset, AttriData, to analyze and attribute errors in Large Language Models, along with a fine-tuned model called MisAttributionLLM for generating scores and feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the performance of LLMs and diagnose failures in their answers through automated error categorization and attribution.

**Method:** Developing a Misattribution Framework with 6 primary and 15 secondary categories, creating AttriData dataset for error attribution, and fine-tuning MisAttributionLLM model on AttriData.

**Key Contributions:**

	1. Misattribution Framework with detailed categorization for error analysis.
	2. AttriData, a dataset tailored for error attribution with scores and feedback.
	3. MisAttributionLLM, a general-purpose judge model for generating scores and misattribution feedback.

**Result:** The proposed method shows effectiveness and robustness in error attribution, illustrated through extensive experiments.

**Limitations:** 

**Conclusion:** The Misattribution Framework and MisAttributionLLM provide new capabilities for performance analysis of LLMs by simultaneously generating scores, misattributions, and feedback.

**Abstract:** With the widespread application of Large Language Models (LLMs) in various tasks, the mainstream LLM platforms generate massive user-model interactions daily. In order to efficiently analyze the performance of models and diagnose failures in their answers, it is essential to develop an automated framework to systematically categorize and attribute errors. However, existing evaluation models lack error attribution capability. In this work, we establish a comprehensive Misattribution Framework with 6 primary and 15 secondary categories to facilitate in-depth analysis. Based on this framework, we present AttriData, a dataset specifically designed for error attribution, encompassing misattribution, along with the corresponding scores and feedback. We also propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first general-purpose judge model capable of simultaneously generating score, misattribution, and feedback. Extensive experiments and analyses are conducted to confirm the effectiveness and robustness of our proposed method.

</details>


### [54] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)

*Marina Luketina, Andrea Benkel, Christoph G. Schuetz*

**Main category:** cs.CL

**Keywords:** large language models, legal decision-making, VAT law, fine-tuning, retrieval-augmented generation

**Relevance Score:** 7

**TL;DR:** This paper evaluates the use of large language models (LLMs) in legal decision-making for VAT law, exploring methods like fine-tuning and retrieval-augmented generation (RAG).

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to assess the potential of LLMs in assisting tax professionals by automating workflows and aiding in legal analyses, particularly in the context of Austrian and EU VAT law.

**Method:** The research employs experimental evaluations on textbook and real-world cases, applying fine-tuning and RAG to improve LLM performance for legal reasoning.

**Key Contributions:**

	1. Evaluation of LLMs in the context of VAT law decision-making
	2. Comparison of fine-tuning and RAG methods for legal reasoning
	3. Identification of limitations in current LLM applications for legal contexts

**Result:** The experiments show that LLMs can support tax consultants by automating routine tasks and providing initial analyses, though they are not yet suitable for full automation due to the complexities of the legal domain.

**Limitations:** LLMs struggle with implicit client knowledge and context-specific documentation, limiting their applicability in sensitive legal tasks.

**Conclusion:** While LLMs exhibit potential in VAT decision-making support, limitations persist in handling specific client knowledge and documentation, indicating a need for future developments in the integration of structured information.

**Abstract:** This paper provides an experimental evaluation of the capability of large language models (LLMs) to assist in legal decision-making within the framework of Austrian and European Union value-added tax (VAT) law. In tax consulting practice, clients often describe cases in natural language, making LLMs a prime candidate for supporting automated decision-making and reducing the workload of tax professionals. Given the requirement for legally grounded and well-justified analyses, the propensity of LLMs to hallucinate presents a considerable challenge. The experiments focus on two common methods for enhancing LLM performance: fine-tuning and retrieval-augmented generation (RAG). In this study, these methods are applied on both textbook cases and real-world cases from a tax consulting firm to systematically determine the best configurations of LLM-based systems and assess the legal-reasoning capabilities of LLMs. The findings highlight the potential of using LLMs to support tax consultants by automating routine tasks and providing initial analyses, although current prototypes are not ready for full automation due to the sensitivity of the legal domain. The findings indicate that LLMs, when properly configured, can effectively support tax professionals in VAT tasks and provide legally grounded justifications for decisions. However, limitations remain regarding the handling of implicit client knowledge and context-specific documentation, underscoring the need for future integration of structured background information.

</details>


### [55] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)

*Qingliang Meng, Hao Wu, Wei Liang, Wei Xu, Qing Zhao*

**Main category:** cs.CL

**Keywords:** large language models, automatic speech recognition, iterative training

**Relevance Score:** 8

**TL;DR:** This paper introduces Iterative LoRA Training (ILT) to improve the performance of large language models combined with speech recognition, aimed at reducing overfitting during the fine-tuning process.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overfitting issue in Low-Rank Adaptation (LoRA) during the supervised fine-tuning of large language models and automatic speech recognition systems.

**Method:** The paper proposes an innovative training paradigm called Iterative LoRA Training (ILT) along with an Iterative Pseudo Labeling strategy, utilizing a three-stage training process: Focus Training, Feed Back Training, and Fix Training.

**Key Contributions:**

	1. Introduction of Iterative LoRA Training (ILT).
	2. Development of Iterative Pseudo Labeling strategy.
	3. Systematic experiments showing improved performance in speech recognition tasks.

**Result:** Experiments show that the ILT approach effectively enhances model performance, achieving 4th place in Track 1 and 1st place in Track 2 of the Interspeech 2025 MLC-SLM workshop.

**Limitations:** 

**Conclusion:** The proposed method demonstrates practical feasibility and strong application potential in speech language modeling tasks.

**Abstract:** The deep integration of large language models and automatic speech recognition systems has become a promising research direction with high practical value. To address the overfitting issue commonly observed in Low-Rank Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work proposes an innovative training paradigm Iterative LoRA Training (ILT) in combination with an Iterative Pseudo Labeling strategy, effectively enhancing the theoretical upper bound of model performance. Based on Whisper-large-v3 and Qwen2-Audio, we conduct systematic experiments using a three-stage training process: Focus Training, Feed Back Training, and Fix Training. Experimental results demonstrate the effectiveness of the proposed method. Furthermore, the MegaAIS research team applied this technique in the Interspeech 2025 Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM), achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2 (Speech Separation and Recognition Task), showcasing the practical feasibility and strong application potential of our approach.

</details>


### [56] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)

*Bruno Alexandre Rosa, Hilário Oliveira, Luiz Rodrigues, Eduardo Araujo Oliveira, Rafael Ferreira Mello*

**Main category:** cs.CL

**Keywords:** textual cohesion, machine learning, item response theory, educational essays, automatic evaluation

**Relevance Score:** 6

**TL;DR:** This paper presents a method for automatically scoring textual cohesion in educational essays using item response theory combined with machine learning.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of automatically evaluating textual cohesion in essays, which is crucial for understanding meaning within texts.

**Method:** The authors propose a cohesion score prediction approach based on item response theory, which is integrated with machine learning regression tasks. The experiments utilized a corpus of essays to extract 325 linguistic features.

**Key Contributions:**

	1. Introduction of item response theory to machine learning for scoring textual cohesion
	2. Analysis of a large corpus of essays for validation
	3. Demonstration of improved performance over traditional methods

**Result:** The proposed method outperformed conventional machine learning models and ensemble methods across various evaluation metrics, indicating its effectiveness in scoring cohesion.

**Limitations:** 

**Conclusion:** This research suggests a promising approach for enhancing the automatic evaluation of cohesion in educational essays using machine learning and item response theory.

**Abstract:** Essays are considered a valuable mechanism for evaluating learning outcomes in writing. Textual cohesion is an essential characteristic of a text, as it facilitates the establishment of meaning between its parts. Automatically scoring cohesion in essays presents a challenge in the field of educational artificial intelligence. The machine learning algorithms used to evaluate texts generally do not consider the individual characteristics of the instances that comprise the analysed corpus. In this meaning, item response theory can be adapted to the context of machine learning, characterising the ability, difficulty and discrimination of the models used. This work proposes and analyses the performance of a cohesion score prediction approach based on item response theory to adjust the scores generated by machine learning models. In this study, the corpus selected for the experiments consisted of the extended Essay-BR, which includes 6,563 essays in the style of the National High School Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235 essays written by 5th to 9th grade students from public schools. We extracted 325 linguistic features and treated the problem as a machine learning regression task. The experimental results indicate that the proposed approach outperforms conventional machine learning models and ensemble methods in several evaluation metrics. This research explores a potential approach for improving the automatic evaluation of cohesion in educational essays.

</details>


### [57] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)

*David Schlangen, Sherzod Hakimov, Jonathan Jordan, Philipp Sadler*

**Main category:** cs.CL

**Keywords:** large language models, evaluation paradigms, benchmarking, dialogue games, clembench

**Relevance Score:** 9

**TL;DR:** This paper introduces clembench, a new benchmarking tool that integrates dialogue game based evaluation for large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved evaluation paradigms for large language models that combine strength of reference-based and preference-based evaluations.

**Method:** Introduction and description of the clembench tool, enabling dialogue game based evaluation to benchmark models with user-friendly implementation.

**Key Contributions:**

	1. Introduction of a new benchmarking tool for dialogue game based evaluation of LLMs.
	2. Optimization for ease of use and extendability in benchmarking tests.
	3. Provision of comprehensive documentation and code for implementation.

**Result:** clembench provides a software platform that facilitates both the evaluation of LLMs through controlled dialogue games and the extension of benchmark tests.

**Limitations:** 

**Conclusion:** Clembench is positioned to enhance the evaluation of LLMs by making dialogue-based benchmarks more accessible and customizable.

**Abstract:** There are currently two main paradigms for evaluating large language models (LLMs), reference-based evaluation and preference-based evaluation. The first, carried over from the evaluation of machine learning models in general, relies on pre-defined task instances, for which reference task executions are available. The second, best exemplified by the LM-arena, relies on (often self-selected) users bringing their own intents to a site that routes these to several models in parallel, among whose responses the user then selects their most preferred one. The former paradigm hence excels at control over what is tested, while the latter comes with higher ecological validity, testing actual use cases interactively. Recently, a third complementary paradigm has emerged that combines some of the strengths of these approaches, offering control over multi-turn, reference-free, repeatable interactions, while stressing goal-directedness: dialogue game based evaluation. While the utility of this approach has been shown by several projects, its adoption has been held back by the lack of a mature, easily re-usable implementation. In this paper, we present clembench, which has been in continuous development since 2023 and has in its latest release been optimized for ease of general use. We describe how it can be used to benchmark one's own models (using a provided set of benchmark game instances in English), as well as how easily the benchmark itself can be extended with new, tailor-made targeted tests.

</details>


### [58] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)

*Shibo Sun, Xue Li, Donglin Di, Mingjie Wei, Lanshun Nie, Wei-Nan Zhang, Dechen Zhan, Yang Song, Lei Fan*

**Main category:** cs.CL

**Keywords:** multimodal, procedural planning, vision-language models, counterfactual reasoning, embodied AI

**Relevance Score:** 9

**TL;DR:** LLaPa is a vision-language model framework for multimodal procedural planning, addressing challenges in integrating multimodal inputs and counterfactual reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of multimodal inputs and counterfactual reasoning in procedural planning for embodied AI systems.

**Method:** LLaPa generates executable action sequences from textual descriptions and visual images, enhanced with a Task-Environment Reranker for task-sensitive feature alignment and a Counterfactual Activities Retriever for improved reasoning in counterfactual scenarios.

**Key Contributions:**

	1. Introduction of a multimodal procedural planning framework (LLaPa)
	2. Development of Task-Environment Reranker for task-sensitive feature spaces
	3. Creation of Counterfactual Activities Retriever to enhance reasoning

**Result:** LLaPa outperforms advanced models on ActPlan-1K and ALFRED benchmarks with higher-quality plans and improved LCS and correctness.

**Limitations:** 

**Conclusion:** LLaPa sets a new benchmark in multimodal procedural planning and demonstrates the importance of integrating counterfactual reasoning with multimodal inputs.

**Abstract:** While large language models (LLMs) have advanced procedural planning for embodied AI systems through strong reasoning abilities, the integration of multimodal inputs and counterfactual reasoning remains underexplored. To tackle these challenges, we introduce LLaPa, a vision-language model framework designed for multimodal procedural planning. LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary modules to improve procedural planning. The first module, the Task-Environment Reranker (TER), leverages task-oriented segmentation to create a task-sensitive feature space, aligning textual descriptions with visual environments and emphasizing critical regions for procedural execution. The second module, the Counterfactual Activities Retriever (CAR), identifies and emphasizes potential counterfactual conditions, enhancing the model's reasoning capability in counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models. The code and models are available https://github.com/sunshibo1234/LLaPa.

</details>


### [59] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)

*Mengze Hong, Chen Jason Zhang, Di Jiang*

**Main category:** cs.CL

**Keywords:** Latent Dirichlet Allocation, Large Language Models, Topic Modeling, Text Mining, Coherence Evaluation

**Relevance Score:** 7

**TL;DR:** This paper investigates enhancing Latent Dirichlet Allocation (LDA) topic models with Large Language Models (LLMs) during initialization and post-correction phases, finding mixed results in practical improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of integrating Large Language Models with Latent Dirichlet Allocation for topic modeling and to challenge the assumption of LLM superiority in text mining.

**Method:** The study employs LLM-guided topic clustering for initializing the Gibbs sampling algorithm in LDA and assesses the impact of LLM-enabled post-correction on topic coherence.

**Key Contributions:**

	1. Integration of LLMs into LDA phases
	2. Empirical analysis of LLM impacts on topic modeling
	3. Insight into the limitations of LLMs in text mining

**Result:** LLM-guided initialization improves early iterations of LDA but adversely affects overall performance, whereas LLM-enabled post-correction enhances coherence by 5.86%.

**Limitations:** The initialization from LLMs did not improve convergence and performed worse than baselines.

**Conclusion:** The findings reveal that while LLMs can be beneficial in specific phases of topic modeling, they do not universally enhance LDA performance, prompting a reevaluation of their role in text mining.

**Abstract:** Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic model used for uncovering abstract topics within document collections. In this paper, we explore the effectiveness of augmenting topic models with Large Language Models (LLMs) through integration into two key phases: Initialization and Post-Correction. Since the LDA is highly dependent on the quality of its initialization, we conduct extensive experiments on the LLM-guided topic clustering for initializing the Gibbs sampling algorithm. Interestingly, the experimental results reveal that while the proposed initialization strategy improves the early iterations of LDA, it has no effect on the convergence and yields the worst performance compared to the baselines. The LLM-enabled post-correction, on the other hand, achieved a promising improvement of 5.86% in the coherence evaluation. These results highlight the practical benefits of the LLM-in-the-loop approach and challenge the belief that LLMs are always the superior text mining alternative.

</details>


### [60] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)

*Ziyi Huang, Xia Cui*

**Main category:** cs.CL

**Keywords:** emotion detection, multilingual, TF-IDF, contextual embeddings, PCA

**Relevance Score:** 7

**TL;DR:** This paper presents a feature-centric framework for multi-label emotion detection in 28 languages, optimizing document representations and learning algorithms based on language-specific performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve text-based emotion detection using a system that effectively manages linguistic diversity and resource constraints.

**Method:** A feature-centric framework was used to dynamically adapt document representations and learning algorithms for emotion detection across multiple languages.

**Key Contributions:**

	1. Dynamic adaptation of document representations
	2. Evaluation across 28 languages
	3. Efficiency improvements using PCA

**Result:** The study finds that TF-IDF is effective for low-resource languages, while contextual embeddings and transformer-based models show varied strengths; PCA improves training efficiency.

**Limitations:** Focused primarily on feature extraction without exploring algorithm design intricacies.

**Conclusion:** The proposed framework is scalable for multilingual emotion detection and successfully addresses challenges related to linguistic diversity.

**Abstract:** This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection (Track A), which focuses on multi-label emotion detection in short texts. We propose a feature-centric framework that dynamically adapts document representations and learning algorithms to optimize language-specific performance. Our study evaluates three key components: document representation, dimensionality reduction, and model training in 28 languages, highlighting five for detailed analysis. The results show that TF-IDF remains highly effective for low-resource languages, while contextual embeddings like FastText and transformer-based document representations, such as those produced by Sentence-BERT, exhibit language-specific strengths. Principal Component Analysis (PCA) reduces training time without compromising performance, particularly benefiting FastText and neural models such as Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores the trade-off between model complexity and processing cost. Our framework provides a scalable solution for multilingual emotion detection, addressing the challenges of linguistic diversity and resource constraints.

</details>


### [61] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)

*David Pomerenke, Jonas Nothnagel, Simon Ostermann*

**Main category:** cs.CL

**Keywords:** multilingual, language models, benchmark, low-resource languages, AI proficiency

**Relevance Score:** 9

**TL;DR:** The paper introduces the AI Language Proficiency Monitor, a multilingual benchmark for assessing LLM performance across 200 languages, focusing on low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure equitable access to LLM benefits by evaluating their capabilities in various languages, especially low-resource ones.

**Method:** Developed a benchmark that includes diverse tasks such as translation and reasoning, utilizing datasets like FLORES+, MMLU, GSM8K, and others, with an open-source leaderboard.

**Key Contributions:**

	1. Introduction of a comprehensive multilingual benchmark for LLMs
	2. Focus on performance assessment in low-resource languages
	3. Creation of an open-source, auto-updating leaderboard and analysis dashboard.

**Result:** The benchmark provides a comprehensive assessment of LLM performance, highlighting strengths and gaps, and includes features like a global proficiency map and trends over time.

**Limitations:** 

**Conclusion:** The system promotes transparency and inclusivity in multilingual AI and is available on Hugging Face.

**Abstract:** To ensure equitable access to the benefits of large language models (LLMs), it is essential to evaluate their capabilities across the world's languages. We introduce the AI Language Proficiency Monitor, a comprehensive multilingual benchmark that systematically assesses LLM performance across up to 200 languages, with a particular focus on low-resource languages. Our benchmark aggregates diverse tasks including translation, question answering, math, and reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We provide an open-source, auto-updating leaderboard and dashboard that supports researchers, developers, and policymakers in identifying strengths and gaps in model performance. In addition to ranking models, the platform offers descriptive insights such as a global proficiency map and trends over time. By complementing and extending prior multilingual benchmarks, our work aims to foster transparency, inclusivity, and progress in multilingual AI. The system is available at https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [62] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)

*Benno Uthayasooriyar, Antoine Ly, Franck Vermet, Caio Corro*

**Main category:** cs.CL

**Keywords:** BERT, document understanding, self-attention, polar coordinates, layout-aware

**Relevance Score:** 6

**TL;DR:** DocPolarBERT is a layout-aware BERT model that improves document understanding by using a polar coordinate system for text block positions instead of traditional 2D positional embeddings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance document understanding by developing a model that effectively utilizes the layout of documents without the limitations of absolute positional embeddings.

**Method:** Introduces a modified self-attention mechanism using relative polar coordinates to account for text block positions, enhancing the efficiency of the model.

**Key Contributions:**

	1. Development of a layout-aware BERT model
	2. Introduction of relative polar coordinate system in self-attention
	3. Achieving state-of-the-art results with less pre-training data

**Result:** DocPolarBERT achieves state-of-the-art results despite being trained on a dataset more than six times smaller than the IIT-CDIP corpus.

**Limitations:** 

**Conclusion:** The study indicates that innovative attention mechanisms can yield significant performance improvements in document understanding even with less training data.

**Abstract:** We introduce DocPolarBERT, a layout-aware BERT model for document understanding that eliminates the need for absolute 2D positional embeddings. We extend self-attention to take into account text block positions in relative polar coordinate system rather than the Cartesian one. Despite being pre-trained on a dataset more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results demonstrate that a carefully designed attention mechanism can compensate for reduced pre-training data, offering an efficient and effective alternative for document understanding.

</details>


### [63] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)

*Marcin Pietroń, Rafał Olszowski, Jakub Gomułka, Filip Gampel, Andrzej Tomski*

**Main category:** cs.CL

**Keywords:** argument mining, large language models, classification benchmarks, Chain-of-Thoughts, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper studies the performance of large language models (LLMs) in argument mining using various datasets, highlighting strengths and weaknesses of models like ChatGPT-4o and DeepSeek-R1 in argument classification tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of research on the performance of LLMs in publicly available argument classification databases, despite advancements in argument mining facilitated by LLMs.

**Method:** The study involves testing a selection of LLMs, including versions of GPT, Llama, and DeepSeek, on datasets like Args.me and UKP, and utilizes reasoning-enhanced variants with the Chain-of-Thoughts algorithm.

**Key Contributions:**

	1. First broader analysis of LLMs in argument mining using various datasets
	2. Identification of strengths and weaknesses in argument classification performance
	3. Analysis of common errors in model outputs and prompt algorithm limitations.

**Result:** Results show that ChatGPT-4o is the best performer in argument classification benchmarks, while Deepseek-R1 excels among reasoning models. Common errors across models are also discussed.

**Limitations:** The models still make errors, and the analysis shows shortcomings in existing prompt algorithms for argument analysis.

**Conclusion:** The work provides a comprehensive analysis of argument classification datasets, reveals limitations of existing prompt algorithms, and suggests improvements for future research.

**Abstract:** Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLM's, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings.

</details>


### [64] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)

*Cristina Aggazzotti, Matthew Wiesner, Elizabeth Allyn Smith, Nicholas Andrews*

**Main category:** cs.CL

**Keywords:** speaker attribution, automatic speech recognition, transcription errors, human-computer interaction, machine learning

**Relevance Score:** 6

**TL;DR:** This paper investigates how transcription errors from automatic speech recognition (ASR) systems affect speaker attribution performance, revealing that ASR errors may not significantly hinder attribution capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenge of speaker attribution using transcripts from automatic speech recognition systems, which are often error-prone compared to human-generated transcripts.

**Method:** The authors conduct a comprehensive analysis of speaker attribution performance using various ASR-generated transcripts, measuring how different levels of transcription error affect the ability to accurately identify speakers.

**Key Contributions:**

	1. First comprehensive study on ASR impact on speaker attribution
	2. Demonstrated resilience of attribution performance to transcription errors
	3. Highlighted potential advantages of ASR transcripts for speaker identity recovery

**Result:** The research finds that speaker attribution performance remains surprisingly resilient to word-level transcription errors, with ASR-produced transcripts performing at least as well as human-transcribed data in certain contexts.

**Limitations:** The study primarily focuses on the impact of word-level errors and may not address other types of errors or contextual factors affecting attribution performance.

**Conclusion:** The findings indicate that errors in ASR transcripts can still facilitate effective speaker attribution, suggesting that reliance on error-prone transcripts may not be as detrimental as previously assumed.

**Abstract:** Speaker attribution from speech transcripts is the task of identifying a speaker from the transcript of their speech based on patterns in their language use. This task is especially useful when the audio is unavailable (e.g. deleted) or unreliable (e.g. anonymized speech). Prior work in this area has primarily focused on the feasibility of attributing speakers using transcripts produced by human annotators. However, in real-world settings, one often only has more errorful transcripts produced by automatic speech recognition (ASR) systems. In this paper, we conduct what is, to our knowledge, the first comprehensive study of the impact of automatic transcription on speaker attribution performance. In particular, we study the extent to which speaker attribution performance degrades in the face of transcription errors, as well as how properties of the ASR system impact attribution. We find that attribution is surprisingly resilient to word-level transcription errors and that the objective of recovering the true transcript is minimally correlated with attribution performance. Overall, our findings suggest that speaker attribution on more errorful transcripts produced by ASR is as good, if not better, than attribution based on human-transcribed data, possibly because ASR transcription errors can capture speaker-specific features revealing of speaker identity.

</details>


### [65] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)

*Jiyao Zhang, Chengli Zhong, Hui Xu, Qige Li, Yi Zhou*

**Main category:** cs.CL

**Keywords:** neuro-symbolic AI, machine-verifiable theorems, multilingual parallel corpora

**Relevance Score:** 6

**TL;DR:** Proposes KELPS, a neuro-symbolic framework for translating informal mathematics into machine-verifiable theorems, enhancing multilingual processing capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bottlenecks in the conversion of informal mathematics to formal theorems caused by the scarcity of quality multilingual parallel corpora.

**Method:** KELPS, an iterative framework, translates natural language into Knowledge Equations, then converts these into formal languages (Lean, Coq, Isabelle) using defined rules.

**Key Contributions:**

	1. Introduction of Knowledge Equations (KEs) for representing informal data.
	2. Development of a framework that maintains syntactic structure and semantic meaning during translation.
	3. Creation of a large parallel corpus of math problems.

**Result:** Achieved 88.9% syntactic accuracy on MiniF2F, surpassing existing models like Deepseek-V3 and Herald, with a newly created parallel corpus of over 60,000 problems.

**Limitations:** 

**Conclusion:** The KELPS framework effectively enhances the process of translating informal mathematics into formal languages, paving the way for better multilingual corpora.

**Abstract:** Modern large language models (LLMs) show promising progress in formalizing informal mathematics into machine-verifiable theorems. However, these methods still face bottlenecks due to the limited quantity and quality of multilingual parallel corpora. In this paper, we propose a novel neuro-symbolic framework KELPS (Knowledge-Equation based Logical Processing System) to address these problems. KELPS is an iterative framework for translating, synthesizing, and filtering informal data into multiple formal languages (Lean, Coq, and Isabelle). First, we translate natural language into Knowledge Equations (KEs), a novel language that we designed, theoretically grounded in assertional logic. Next, we convert them to target languages through rigorously defined rules that preserve both syntactic structure and semantic meaning. This process yielded a parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3 (81%) and Herald (81.3%) across multiple datasets. All datasets and codes are available in the supplementary materials.

</details>


### [66] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)

*Songlin Zhai, Guilin Qi, Yuan Meng*

**Main category:** cs.CL

**Keywords:** knowledge graphs, large language models, attention mechanism, real-time updates, knowledge fusion

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework for integrating knowledge graphs into large language models at test time without parameter updates, using a dual-pathway attention mechanism for real-time knowledge fusion.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance large language models with structured knowledge while avoiding risks of catastrophic forgetting and improving adaptability to real-time knowledge updates.

**Method:** The proposed framework employs a knowledge graph-guided attention (KGA) module that consists of outward and inward aggregation pathways for dynamic knowledge fusion during test-time.

**Key Contributions:**

	1. Introduction of test-time KG-augmented framework for LLMs
	2. Development of a knowledge graph-guided attention module with dual pathways
	3. Demonstration of effective real-time knowledge fusion without parameter updates

**Result:** Extensive experiments demonstrate that the KGA module achieves comparable knowledge fusion performance across five benchmarks.

**Limitations:** 

**Conclusion:** The KGA module provides a novel approach to integrate knowledge dynamically, supporting real-time updates without modifying model parameters.

**Abstract:** Knowledge graphs (KGs) play a critical role in enhancing large language models (LLMs) by introducing structured and grounded knowledge into the learning process. However, most existing KG-enhanced approaches rely on parameter-intensive fine-tuning, which risks catastrophic forgetting and degrades the pretrained model's generalization. Moreover, they exhibit limited adaptability to real-time knowledge updates due to their static integration frameworks. To address these issues, we introduce the first test-time KG-augmented framework for LLMs, built around a dedicated knowledge graph-guided attention (KGA) module that enables dynamic knowledge fusion without any parameter updates. The proposed KGA module augments the standard self-attention mechanism with two synergistic pathways: outward and inward aggregation. Specifically, the outward pathway dynamically integrates external knowledge into input representations via input-driven KG fusion. This inward aggregation complements the outward pathway by refining input representations through KG-guided filtering, suppressing task-irrelevant signals and amplifying knowledge-relevant patterns. Importantly, while the outward pathway handles knowledge fusion, the inward path selects the most relevant triples and feeds them back into the fusion process, forming a closed-loop enhancement mechanism. By synergistically combining these two pathways, the proposed method supports real-time knowledge fusion exclusively at test-time, without any parameter modification. Extensive experiments on five benchmarks verify the comparable knowledge fusion performance of KGA.

</details>


### [67] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)

*Linzheng Chai, Jian Yang, Shukai Liu, Wei Zhang, Liran Wang, Ke Jin, Tao Sun, Congnan Liu, Chenchen Zhang, Hualei Zhu, Jiaheng Liu, Xianjie Wu, Ge Zhang, Tianyu Liu, Zhoujun Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multimodal, Code Generation, UML Diagrams, Visual Workflows

**Relevance Score:** 9

**TL;DR:** Introducing MM-Coder, a Multilingual Multimodal software developer that integrates visual design inputs with textual instructions for improved code generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of text-only large language models in code generation by incorporating visual aids like diagrams and flowcharts.

**Method:** Developed MM-Coder, which utilizes a multimodal instruction-tuning dataset (MMc-Instruct) and introduces a new benchmark (MMEval) for evaluating multimodal code generation.

**Key Contributions:**

	1. Introduction of MM-Coder as a Multilingual Multimodal software developer.
	2. Development of MMc-Instruct dataset for multimodal instruction tuning.
	3. Launch of MMEval benchmark for evaluating multimodal code generation.

**Result:** MM-Coder demonstrates the potential for improved architectural alignment and code generation accuracy by synthesizing textual and visual information, overcoming challenges seen in text-only models.

**Limitations:** Significant challenges still exist in precise visual information capture, instruction following, and advanced programming knowledge for models.

**Conclusion:** The integration of visual workflows into code generation could significantly impact industrial programming practices by enabling better comprehension of complex specifications.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has significantly improved code generation, yet most models remain text-only, neglecting crucial visual aids like diagrams and flowcharts used in real-world software development. To bridge this gap, we introduce MM-Coder, a Multilingual Multimodal software developer. MM-Coder integrates visual design inputs-Unified Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with textual instructions to enhance code generation accuracy and architectural alignment. To enable this, we developed MMc-Instruct, a diverse multimodal instruction-tuning dataset including visual-workflow-based code generation, allowing MM-Coder to synthesize textual and graphical information like human developers, distinct from prior work on narrow tasks. Furthermore, we introduce MMEval, a new benchmark for evaluating multimodal code generation, addressing existing text-only limitations. Our evaluations using MMEval highlight significant remaining challenges for models in precise visual information capture, instruction following, and advanced programming knowledge. Our work aims to revolutionize industrial programming by enabling LLMs to interpret and implement complex specifications conveyed through both text and visual designs.

</details>


### [68] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)

*Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano*

**Main category:** cs.CL

**Keywords:** cache steering, language models, implicit steering, chain-of-thought reasoning, controlled generation

**Relevance Score:** 8

**TL;DR:** Cache steering offers a one-shot method for enhancing reasoning in language models by modifying the key-value cache, improving model behavior without fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of language models through a lightweight intervention method.

**Method:** Cache steering is implemented as a one-shot intervention directly applied to the key-value cache of language models, utilizing GPT-4o-generated reasoning traces to create steering vectors.

**Key Contributions:**

	1. Introduction of cache steering for language models
	2. Demonstrated improvements in reasoning capabilities
	3. Substantial advantages over continuous activation steering methods

**Result:** Experimental evaluations show that cache steering enhances reasoning structure qualitatively and improves quantitative task performance across diverse benchmarks.

**Limitations:** 

**Conclusion:** Cache steering is more efficient and stable compared to prior activation steering techniques, making it a practical solution for controlled generation.

**Abstract:** We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.

</details>


### [69] [Answer Generation for Questions With Multiple Information Sources in E-Commerce](https://arxiv.org/abs/2111.14003)

*Anand A. Rajasekar, Nikesh Garera*

**Main category:** cs.CL

**Keywords:** E-commerce, automatic question answering, natural language processing, BERT, T5

**Relevance Score:** 6

**TL;DR:** Proposes a novel pipeline (MSQAP) for automatic question answering in E-commerce by using multiple information sources and addressing challenges of relevancy and ambiguity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There's a high demand for efficient automatic answer generation systems in E-commerce due to the large volume of user queries about products, necessitating effective use of available information sources.

**Method:** A novel pipeline (MSQAP) is proposed, which performs relevancy and ambiguity prediction separately before generating a response, effectively using reviews, similar questions, and specifications.

**Key Contributions:**

	1. Introduction of the MSQAP pipeline for E-commerce QA systems
	2. Demonstration of efficacy through substantial metric improvements
	3. First approach to combine specifications, similar questions, and reviews for answer generation

**Result:** The relevancy prediction model (BERT-QA) shows a 12.36% improvement in F1 score over the BERT-base baseline. The generation model (T5-QA) significantly surpasses baselines in content preservation metrics, with a 35.02% improvement in ROUGE and 198.75% in BLEU. The overall pipeline shows a 30.7% improvement in accuracy over T5-QA.

**Limitations:** 

**Conclusion:** The proposed method, combining diverse sources of information, represents a significant advancement in automatically generating accurate natural language answers in the E-commerce domain.

**Abstract:** Automatic question answering is an important yet challenging task in E-commerce given the millions of questions posted by users about the product that they are interested in purchasing. Hence, there is a great demand for automatic answer generation systems that provide quick responses using related information about the product. There are three sources of knowledge available for answering a user posted query, they are reviews, duplicate or similar questions, and specifications. Effectively utilizing these information sources will greatly aid us in answering complex questions. However, there are two main challenges present in exploiting these sources: (i) The presence of irrelevant information and (ii) the presence of ambiguity of sentiment present in reviews and similar questions. Through this work we propose a novel pipeline (MSQAP) that utilizes the rich information present in the aforementioned sources by separately performing relevancy and ambiguity prediction before generating a response.   Experimental results show that our relevancy prediction model (BERT-QA) outperforms all other variants and has an improvement of 12.36% in F1 score compared to the BERT-base baseline. Our generation model (T5-QA) outperforms the baselines in all content preservation metrics such as BLEU, ROUGE and has an average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the highest performing baseline (HSSC-q). Human evaluation of our pipeline shows us that our method has an overall improvement in accuracy of 30.7% over the generation model (T5-QA), resulting in our full pipeline-based approach (MSQAP) providing more accurate answers. To the best of our knowledge, this is the first work in the e-commerce domain that automatically generates natural language answers combining the information present in diverse sources such as specifications, similar questions, and reviews data.

</details>


### [70] [Comparing Spoken Languages using Paninian System of Sounds and Finite State Machines](https://arxiv.org/abs/2301.12463)

*Shreekanth M Prabhu, Abhisek Midya*

**Main category:** cs.CL

**Keywords:** Linguistic Development, Phonetics, Morphological Finite Automata

**Relevance Score:** 0

**TL;DR:** This paper challenges the traditional family tree model of language development by proposing an Ecosystem Model centered around Sanskrit, utilizing phonetic mapping and Morphological Finite Automata for linguistic analysis.

**Read time:** 63 min

<details>
  <summary>Details</summary>

**Motivation:** To reexamine the classification of languages and propose an alternative model for linguistic development that better accounts for the complexities of language evolution.

**Method:** The paper constructs a phonetic map using the Paninian system of sounds and represents words across languages as state transitions on this map, creating Morphological Finite Automata for groups of words.

**Key Contributions:**

	1. Introduction of the Ecosystem Model for Linguistic Development centered on Sanskrit
	2. Utilization of phonetic mapping and Morphological Finite Automata in language analysis
	3. Challenging established linguistic theories regarding language families

**Result:** The proposed Ecosystem Model provides a nuanced understanding of language relationships, challenging the conventional family tree perspective and emphasizing continuous language evolution and interaction.

**Limitations:** 

**Conclusion:** This work suggests a paradigm shift in linguistic study that departs from traditional models, paving the way for more dynamic and holistic approaches to understanding language development.

**Abstract:** The study of spoken languages comprises phonology, morphology, and grammar. The languages can be classified as root languages, inflectional languages, and stem languages. In addition, languages continually change over time and space by picking isoglosses, as speakers move from region to/through region. All these factors lead to the formation of vocabulary, which has commonality/similarity across languages as well as distinct and subtle differences among them. Comparison of vocabularies across languages and detailed analysis has led to the hypothesis of language families. In particular, in the view of Western linguists, Vedic Sanskrit is a daughter language, part of the Indo-Iranian branch of the Indo-European Language family, and Dravidian Languages belong to an entirely different family. These and such conclusions are reexamined in this paper. Based on our study and analysis, we propose an Ecosystem Model for Linguistic Development with Sanskrit at the core, in place of the widely accepted family tree model. To that end, we leverage the Paninian system of sounds to construct a phonetic map. Then we represent words across languages as state transitions on the phonetic map and construct corresponding Morphological Finite Automata (MFA) that accept groups of words. Regardless of whether the contribution of this paper is significant or minor, it is an important step in challenging policy-driven research that has plagued this field.

</details>


### [71] [Large Language Models in Mental Health Care: a Scoping Review](https://arxiv.org/abs/2401.02984)

*Yining Hua, Fenglin Liu, Kailai Yang, Zehan Li, Hongbin Na, Yi-han Sheu, Peilin Zhou, Lauren V. Moran, Sophia Ananiadou, David A. Clifton, Andrew Beam, John Torous*

**Main category:** cs.CL

**Keywords:** Large Language Models, mental health care, systematic review, diagnostics, therapy

**Relevance Score:** 9

**TL;DR:** This review analyzes the effectiveness of Large Language Models (LLMs) in mental health care, identifying challenges and future potential applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the utilization of LLMs in mental health care and explore their effectiveness and challenges.

**Method:** A systematic review was conducted on studies employing LLMs for mental health care published between October 2019 and December 2023, utilizing multiple databases.

**Key Contributions:**

	1. Comprehensive overview of LLM applications in mental health care.
	2. Identification of key challenges in the implementation of LLMs in clinical settings.
	3. Recommendations for future research and interdisciplinary collaborations.

**Result:** 34 relevant studies were identified that highlighted the applications of LLMs in diagnostics, therapy, and enhancing patient engagement, along with challenges regarding data and ethical considerations.

**Limitations:** Noted limitations include gaps in clinical applicability, ethical considerations, and the need for robust datasets and evaluation methods.

**Conclusion:** LLMs have significant potential in mental health care enhancement, necessitating development of robust datasets and ethical guidelines to mitigate limitations.

**Abstract:** Objectieve:This review aims to deliver a comprehensive analysis of Large Language Models (LLMs) utilization in mental health care, evaluating their effectiveness, identifying challenges, and exploring their potential for future application. Materials and Methods: A systematic search was performed across multiple databases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv in November 2023. The review includes all types of original research, regardless of peer-review status, published or disseminated between October 1, 2019, and December 2, 2023. Studies were included without language restrictions if they employed LLMs developed after T5 and directly investigated research questions within mental health care settings. Results: Out of an initial 313 articles, 34 were selected based on their relevance to LLMs applications in mental health care and the rigor of their reported outcomes. The review identified various LLMs applications in mental health care, including diagnostics, therapy, and enhancing patient engagement. Key challenges highlighted were related to data availability and reliability, the nuanced handling of mental states, and effective evaluation methods. While LLMs showed promise in improving accuracy and accessibility, significant gaps in clinical applicability and ethical considerations were noted. Conclusion: LLMs hold substantial promise for enhancing mental health care. For their full potential to be realized, emphasis must be placed on developing robust datasets, development and evaluation frameworks, ethical guidelines, and interdisciplinary collaborations to address current limitations.

</details>


### [72] [Swap distance minimization beyond entropy minimization in word order variation](https://arxiv.org/abs/2404.14192)

*Víctor Franco-Sánchez, Arnau Martí-Llobet, Ramon Ferrer-i-Cancho*

**Main category:** cs.CL

**Keywords:** entropy minimization, swap distance minimization, linguistic structure, word order frequency, Polya urn process

**Relevance Score:** 4

**TL;DR:** The paper investigates how linguistic structures' word order frequencies are influenced by entropy and swap distance minimization principles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the constraints on word order in linguistic structures and how they are shaped by principles of entropy and swap distance minimization.

**Method:** The study employs a die rolling experiment and a Polya urn process to examine the frequency of word orders in linguistic structures with different quantities of elements (n=3 and n=4).

**Key Contributions:**

	1. Introduced the average swap distance score for linguistic research.
	2. Provided empirical evidence for both entropy minimization and swap distance minimization in linguistic structures.
	3. Demonstrated the effects of word order frequency shuffling on minimizing swap distances.

**Result:** The analysis reveals strong evidence supporting both entropy minimization and swap distance minimization, especially for n=4, indicating these principles shape word order frequencies in linguistic structures.

**Limitations:** Evidence for swap distance minimization is weaker for n=3 compared to n=4.

**Conclusion:** The findings suggest that swap distance minimization effects exist independently of pressures to reduce word order entropy, demonstrating complex influences on linguistic structures.

**Abstract:** Consider a linguistic structure formed by $n$ elements, for instance, subject, direct object and verb ($n=3$) or subject, direct object, indirect object and verb ($n=4$). We investigate whether the frequency of the $n!$ possible orders is constrained by two principles. First, entropy minimization, a principle that has been suggested to shape natural communication systems at distinct levels of organization. Second, swap distance minimization, namely a preference for word orders that require fewer swaps of adjacent elements to be produced from a source order. We present average swap distance, a novel score for research on swap distance minimization. We find strong evidence of pressure for entropy minimization and swap distance minimization with respect to a die rolling experiment in distinct linguistic structures with $n=3$ or $n=4$. Evidence with respect to a Polya urn process is strong for $n=4$ but weaker for $n=3$. We still find evidence consistent with the action of swap distance minimization when word order frequencies are shuffled, indicating that swap distance minimization effects are beyond pressure to reduce word order entropy.

</details>


### [73] [SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths](https://arxiv.org/abs/2405.19715)

*Kaixuan Huang, Xudong Guo, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** Speculative Decoding, Large Language Models, Markov Decision Process, Adaptation, Acceptance Prediction

**Relevance Score:** 8

**TL;DR:** This paper presents SpecDec++, an enhanced speculative decoding method for large language models that adaptively determines candidate length to improve inference speed and accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the inference latency of large language models by optimally determining the candidate length for speculative decoding, which traditionally relies on simple heuristics.

**Method:** Formulated the candidate length as a Markov Decision Process and proposed a threshold policy for deciding when to stop speculation. Developed an enhanced model called SpecDec++ that integrates an acceptance prediction mechanism to predict token acceptance probabilities.

**Key Contributions:**

	1. Formulated candidate length as a Markov Decision Process.
	2. Developed an adaptive method to determine candidate length on the fly.
	3. Demonstrated performance improvements across multiple datasets.

**Result:** SpecDec++ achieves speedups of 2.04x on the Alpaca dataset and 2.26x on the GSM8K dataset while improving accuracy compared to baseline methods.

**Limitations:** 

**Conclusion:** The introduction of SpecDec++ demonstrates a significant improvement in decoding speed and reliability for large language models, with practical implementations available.

**Abstract:** Speculative decoding reduces the inference latency of a target large language model via utilizing a smaller and faster draft model. Its performance depends on a hyperparameter K -- the candidate length, i.e., the number of candidate tokens for the target model to verify in each round. However, previous methods often use simple heuristics to choose K, which may result in sub-optimal performance. We study the choice of the candidate length K and formulate it as a Markov Decision Process. We theoretically show that the optimal policy of this Markov decision process takes the form of a threshold policy, i.e., the current speculation should stop and be verified when the probability of getting a rejection exceeds a threshold value. Motivated by this theory, we propose SpecDec++, an enhanced version of speculative decoding that adaptively determines the candidate length on the fly. We augment the draft model with a trained acceptance prediction head to predict the conditional acceptance probability of the candidate tokens. SpecDec++ will stop the current speculation when the predicted probability that at least one token gets rejected exceeds a threshold. We implement SpecDec++ and apply it to the llama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup on the Alpaca dataset (7.2% improvement over the baseline speculative decoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x speedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively. The code of this paper is available at https://github.com/Kaffaljidhmah2/SpecDec_pp.

</details>


### [74] [HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew](https://arxiv.org/abs/2406.03897)

*Tzuf Paz-Argaman, Itai Mondshine, Asaf Achi Mordechai, Reut Tsarfaty*

**Main category:** cs.CL

**Keywords:** large language models, Hebrew, abstractive summarization, HeSum, machine learning

**Relevance Score:** 4

**TL;DR:** HeSum introduces a benchmark for evaluating abstractive summarization with LLMs in Modern Hebrew, addressing resource gaps in lower-resourced languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the performance gap of large language models in generative tasks for lower-resourced languages, particularly Hebrew.

**Method:** The paper introduces HeSum, a benchmarking dataset of 10,000 article-summary pairs from Hebrew news sites, analyzing linguistic features and summarization challenges.

**Key Contributions:**

	1. Introduction of HeSum benchmark for Hebrew summarization
	2. Linguistic analysis highlighting summary abstractness
	3. Establishment of distinct challenges for LLMs in lower-resourced languages

**Result:** HeSum is shown to present unique difficulties for state-of-the-art LLMs, revealing the challenges in handling the morphological richness of Hebrew.

**Limitations:** 

**Conclusion:** HeSum provides a valuable resource for testing generative models in Hebrew, contributing to understanding challenges in low-resource language contexts.

**Abstract:** While large language models (LLMs) excel in various natural language tasks in English, their performance in lower-resourced languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction. In this paper, we address this resource and evaluation gap by introducing HeSum, a novel benchmark specifically designed for abstractive text summarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties for contemporary state-of-the-art LLMs, establishing it as a valuable testbed for generative language technology in Hebrew, and MRLs generative challenges in general.

</details>


### [75] [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)

*Usneek Singh, José Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen*

**Main category:** cs.CL

**Keywords:** large language models, synthetic data, validation, fine-tuning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of validating synthetic natural language (NL) utterances generated for fine-tuning large language models (LLMs) used in spreadsheet formula writing. It shows that validation improves model performance despite pruning challenging examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of LLMs in writing spreadsheet formulas by generating synthetic training data, and to assess the efficacy of validating these synthetic examples.

**Method:** The paper uses empirical evaluation to compare model performance with and without validation of synthetic NL annotations across four models (two open and two closed weight).

**Key Contributions:**

	1. Empirical assessment of synthetic NL validation on model performance
	2. Demonstration of the balance between pruning challenging examples and increasing problem complexity
	3. Novel approaches to fine-tuning LLMs with validated synthetic data

**Result:** Validation of synthetic training examples leads to improved performance on the models; despite pruning more challenging examples, it enhances the complexity of problems the models can solve post fine-tuning.

**Limitations:** 

**Conclusion:** Validating synthetic data is crucial for leveraging LLMs effectively, offering significant improvements in performance and problem-solving capabilities.

**Abstract:** Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.

</details>


### [76] [EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees](https://arxiv.org/abs/2503.08893)

*Zhiyuan Zeng, Yizhong Wang, Hannaneh Hajishirzi, Pang Wei Koh*

**Main category:** cs.CL

**Keywords:** language models, weakness profiling, capability tree

**Relevance Score:** 9

**TL;DR:** This paper presents EvalTree, a method for generating weakness profiles of language models that identifies specific weaknesses based on their performance across benchmark instances, guiding subsequent data collection and training improvements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve model evaluation by identifying weaknesses in language models and providing actionable insights for refining these models.

**Method:** The authors introduce EvalTree, which constructs a capability tree with nodes representing different capabilities of a language model and links to benchmark instances testing these capabilities, allowing for a focused analysis of weaknesses.

**Key Contributions:**

	1. Introduction of a weakness profiling method called EvalTree.
	2. Creation of a capability tree to link weaknesses to benchmark instances.
	3. Demonstration of improved performance in LM training data collection by addressing weaknesses identified by EvalTree.

**Result:** EvalTree demonstrates superior performance in identifying weaknesses over baseline methods on the MATH and WildChat benchmarks, leading to more effective training data collection.

**Limitations:** 

**Conclusion:** Weakness profiling through EvalTree not only enhances the identification of weaknesses but also informs better strategies for training data collection, ultimately improving the performance of language models.

**Abstract:** An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for language model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also introduce a weakness profiling method EvalTree. EvalTree constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we provide an interface that allows practitioners to interactively explore the capability trees built by EvalTree.

</details>


### [77] [REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives](https://arxiv.org/abs/2503.11924)

*Kun Su, Krishna Sayana, Hubert Pham, James Pine, Yuri Vasilevski, Raghavendra Vasudeva, Marialena Kyriakidi, Liam Hebert, Ambarish Jash, Anushya Subbiah, Sukhdeep Sodhi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conversational Recommendation, Recommendation Systems

**Relevance Score:** 7

**TL;DR:** The paper introduces the REGEN dataset for benchmarking the conversational capabilities of recommender LLMs and presents the LUMEN modeling framework for generating recommendations and narratives based on user history.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing datasets for conversational recommendation that mainly focus on sequential item prediction.

**Method:** The authors extend the Amazon Product Reviews dataset by adding user critiques and rich narratives. They introduce the LUMEN framework for training models on generating critiques, recommendations, and narratives.

**Key Contributions:**

	1. Introduction of the REGEN dataset for conversational recommendation
	2. Development of the LUMEN framework for model training
	3. Demonstration that critiques improve recommendation quality and narrative generation

**Result:** The dataset's quality was evaluated, and traditional and LLM-based recommender models were trained, showing that critiques enhance recommendation quality and LLMs can generate effective contextual narratives.

**Limitations:** 

**Conclusion:** Incorporating critiques into recommendations significantly improves the learning of language understanding and integrates this with recommendation signals, achieving performance comparable to state-of-the-art systems.

**Abstract:** This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative Narratives), designed to benchmark the conversational capabilities of recommender Large Language Models (LLMs), addressing the limitations of existing datasets that primarily focus on sequential item prediction. REGEN extends the Amazon Product Reviews dataset by inpainting two key natural language features: (1) user critiques, representing user "steering" queries that lead to the selection of a subsequent item, and (2) narratives, rich textual outputs associated with each recommended item taking into account prior context. The narratives include product endorsements, purchase explanations, and summaries of user preferences.   Further, we establish an end-to-end modeling benchmark for the task of conversational recommendation, where models are trained to generate both recommendations and corresponding narratives conditioned on user history (items and critiques). For this joint task, we introduce a modeling framework LUMEN (LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives) which uses an LLM as a backbone for critiquing, retrieval and generation. We also evaluate the dataset's quality using standard auto-rating techniques and benchmark it by training both traditional and LLM-based recommender models. Our results demonstrate that incorporating critiques enhances recommendation quality by enabling the recommender to learn language understanding and integrate it with recommendation signals. Furthermore, LLMs trained on our dataset effectively generate both recommendations and contextual narratives, achieving performance comparable to state-of-the-art recommenders and language models.

</details>


### [78] [Multi-Token Attention](https://arxiv.org/abs/2504.00927)

*Olga Golovneva, Tianlu Wang, Jason Weston, Sainbayar Sukhbaatar*

**Main category:** cs.CL

**Keywords:** Multi-Token Attention, LLM, attention mechanism, language modeling, information retrieval

**Relevance Score:** 7

**TL;DR:** Multi-Token Attention (MTA) improves LLM attention mechanisms by using multiple query and key vectors, enhancing context relevance determination.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of single token attention in LLMs, which restricts the use of information when identifying relevant context.

**Method:** The authors propose a new attention method called Multi-Token Attention, which uses convolution operations over multiple query and key vectors to condition attention weights simultaneously.

**Key Contributions:**

	1. Introduction of Multi-Token Attention method
	2. Improved performance on language modeling tasks
	3. Enhanced ability to process long-context information retrieval.

**Result:** MTA shows improved performance over standard Transformer models, especially in tasks involving long contexts and information retrieval.

**Limitations:** 

**Conclusion:** The new attention mechanism offers richer information processing capabilities, outperforming traditional models in various benchmarks.

**Abstract:** Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.

</details>
