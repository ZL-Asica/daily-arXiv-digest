# 2025-04-26

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 69]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LLM impact on BLV programming](https://arxiv.org/abs/2504.17018)

*Prashant Chandrasekar, Mariel Couvillion, Ayshwarya Saktheeswaran, Jessica Zeitz*

**Main category:** cs.HC

**Keywords:** Large Language Models, Blind and Low-Vision Developers, Accessibility, Programming, Generative AI

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of Large Language Models on blind and low-vision developers in programming, highlighting both benefits and new accessibility challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of LLM-powered tools for blind and low-vision (BLV) developers and identify both improvements and shortcomings in accessibility.

**Method:** A review of existing literature along with an evaluation of five popular LLM-powered IDEs was conducted, focusing on their performance in various programming tasks.

**Key Contributions:** Assessment of LLM performance in programming for BLV developers, Identification of specific accessibility challenges in LLM-assisted tools, Recommendations for enhancing inclusivity in coding environments

**Result:** The evaluation revealed unsupported scenarios, incorrect model output, and significant limitations in interaction support for BLV developers during coding activities.

**Limitations:** The study primarily focuses on five specific IDEs and may not represent all available tools for BLV developers.

**Future Work:** Future research should explore broader toolsets and develop guidelines for enhancing accessibility in AI-powered programming environments.

**Conclusion:** Improving accessibility in generative AI-assisted programming can enhance the coding experience for BLV developers, necessitating further attention as AI integration evolves.

**Abstract:** Large Language Models (LLMs) are rapidly becoming integral to a wide range of tools, tasks, and problem-solving processes, especially in software development. Originally designed for natural language processing tasks such as text generation, LLMs are increasingly being used to assist both professionals and students in writing code. This growing reliance on LLM-based tools is reshaping programming workflows and task execution. In this study, we explore the impact of these technologies on blind and low-vision (BLV) developers. Our review of existing literature indicates that while LLMs help mitigate some of the challenges faced by BLV programmers, they also introduce new forms of inaccessibility. We conducted an evaluation of five popular LLM-powered integrated development environments (IDEs), assessing their performance across a comprehensive set of programming tasks. Our findings highlight several unsupported scenarios, instances of incorrect model output, and notable limitations in interaction support for specific tasks. Through observing BLV developers as they engaged in coding activities, we uncovered key interaction barriers that go beyond model accuracy or code generation quality. This paper outlines the challenges and corresponding opportunities for improving accessibility in the context of generative AI-assisted programming. Addressing these issues can meaningfully enhance the programming experience for BLV developers. As the generative AI revolution continues to unfold, it must also address the unique burdens faced by this community.

</details>


### [2] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)

*Felix Kares, Timo Speith, Hanwei Zhang, Markus Langer*

**Main category:** cs.HC

**Keywords:** Saliency Maps, Explainable AI, User Trust, Mathematical Metrics

**Relevance Score:** 7

**TL;DR:** This study evaluates saliency maps used for explaining neural network classifications through subjective, objective, and mathematical measures, revealing differing assessments and highlighting the complexities in understanding the effectiveness of explainable AI methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to investigate how saliency maps, crucial for explainable AI, can be evaluated more effectively given the inconsistent results across different evaluation methods.

**Method:** The authors conducted a between-subject study with 166 participants, examining three popular saliency map techniques (LIME, Grad-CAM, and Guided Backpropagation) across subjective user measures, objective measures, and mathematical metrics.

**Key Contributions:** First comprehensive comparison of saliency maps across various evaluation methods, Identification of Grad-CAM as the most effective in enhancing user abilities, Exploration of the relationship between mathematical metrics and user understanding

**Result:** The study found no agreement across evaluation methods; trust and satisfaction did not differ significantly among maps, Grad-CAM enhanced user abilities the most, and Guided Backpropagation excelled in mathematical metrics.

**Limitations:** The study's findings may be limited by the specific saliency map methods tested and the demographic of the participant sample.

**Future Work:** Future research should explore additional saliency maps and extend evaluations to include a more diverse participant pool to validate findings.

**Conclusion:** The findings indicate that while some mathematical metrics relate to user understanding, the evaluations can yield counterintuitive results, raising questions about the best practices for assessing explainability in AI.

**Abstract:** Saliency maps are a popular approach for explaining classifications of (convolutional) neural networks. However, it remains an open question as to how best to evaluate salience maps, with three families of evaluation methods commonly being used: subjective user measures, objective user measures, and mathematical metrics. We examine three of the most popular saliency map approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between subject study (N=166) across these families of evaluation methods. We test 1) for subjective measures, if the maps differ with respect to user trust and satisfaction; 2) for objective measures, if the maps increase users' abilities and thus understanding of a model; 3) for mathematical metrics, which map achieves the best ratings across metrics; and 4) whether the mathematical metrics can be associated with objective user measures. To our knowledge, our study is the first to compare several salience maps across all these evaluation methods$-$with the finding that they do not agree in their assessment (i.e., there was no difference concerning trust and satisfaction, Grad-CAM improved users' abilities best, and Guided Backpropagation had the most favorable mathematical metrics). Additionally, we show that some mathematical metrics were associated with user understanding, although this relationship was often counterintuitive. We discuss these findings in light of general debates concerning the complementary use of user studies and mathematical metrics in the evaluation of explainable AI (XAI) approaches.

</details>


### [3] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)

*Ayushi Agrawal, Aditya Kondai, Kavita Vemuri*

**Main category:** cs.HC

**Keywords:** AI, facial assessment, self-objectification, self-esteem, gender differences

**Relevance Score:** 5

**TL;DR:** This study investigates the psychological impact of AI-powered facial assessment tools on self-objectification, self-esteem, and emotional responses, highlighting gender differences and the need for responsible AI design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the psychological effects of AI facial assessment tools on users, particularly regarding self-objectification and self-esteem, and to highlight gender differences in these effects.

**Method:** Two samples utilized distinct facial assessment tools (one critical, one neutral) and participants completed self-objectification, self-esteem, and emotional response scales.

**Key Contributions:** Examined the psychological impact of AI facial tools on self-perception and social biases., Identified gender differences in responses to facial assessment tools., Highlighted the necessity for responsible AI development addressing social biases.

**Result:** The study found links between high self-objectification, low self-esteem, and increased appearance enhancement behaviors. Female participants exhibited more digital enhancement but less emotional impact perception than males.

**Limitations:** The study focused on a limited demographic sample, primarily young adults, which may not generalize to broader populations.

**Future Work:** Future studies should explore how ideological biases in training data affect the outputs of AI tools and their influence on user behavior.

**Conclusion:** AI facial assessment tools can unintentionally reinforce social biases and insecurities; responsible design is essential for mitigating negative impacts.

**Abstract:** AI-powered facial assessment tools are reshaping how individuals evaluate appearance and internalize social judgments. This study examines the psychological impact of such tools on self-objectification, self-esteem, and emotional responses, with attention to gender differences. Two samples used distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9 years), and another more neutral (N=51; M=19.9 years). Participants completed validated self-objectification and self-esteem scales and custom items measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and perceived social emotion (PSE). Results revealed consistent links between high self-objectification, low self-esteem, and increased appearance enhancement behaviors across both versions. Despite softer framing, the newer tool still evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit feedback may reinforce appearance-related insecurities. Gender differences emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital enhancement and less likely to perceive emotional impact in others. These findings reveal how AI tools may unintentionally reinforce and amplify existing social biases and underscore the critical need for responsible AI design and development. Future research will investigate how human ideologies embedded in the training data of such tools shape their evaluative outputs, and how these, in turn, influence user attitudes and decisions.

</details>


### [4] [DashGuide: Authoring Interactive Dashboard Tours for Guiding Dashboard Users](https://arxiv.org/abs/2504.17150)

*Naimul Hoque, Nicole Sultanum*

**Main category:** cs.HC

**Keywords:** Dashboard guidance, Interactive dashboards, User experience

**Relevance Score:** 6

**TL;DR:** DashGuide is a framework for creating interactive dashboard guidance with minimal authoring input, generating step-by-step overlays to improve user navigation and understanding of interactive dashboards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To help dashboard users navigate interactive features and understand data insights while addressing the challenges of time-consuming authoring and effective delivery of guidance.

**Method:** DashGuide captures author-performed interactions to create guidance materials, allowing for editing and generative assistance in building dashboard tours.

**Key Contributions:** Introduction of DashGuide framework for interactive dashboard guidance, Generative assistance for authors in creating dashboard tours, Evaluation showing improved authoring experience for dashboard creators

**Result:** Formative assessments with 9 dashboard creators informed DashGuide's design, and evaluations with 12 creators indicated an enhanced authoring experience that balances efficiency and creativity.

**Limitations:** The study involved a small sample size which may limit the generalizability of the findings.

**Future Work:** Exploration of broader applications for DashGuide and further assessments with diverse user demographics.

**Conclusion:** DashGuide offers a solution for efficiently creating interactive guidance in dashboards, facilitating better user navigation and understanding.

**Abstract:** Dashboard guidance helps dashboard users better navigate interactive features, understand the underlying data, and assess insights they can potentially extract from dashboards. However, authoring dashboard guidance is a time consuming task, and embedding guidance into dashboards for effective delivery is difficult to realize. In this work, we contribute DashGuide, a framework and system to support the creation of interactive dashboard guidance with minimal authoring input. Given a dashboard and a communication goal, DashGuide captures a sequence of author-performed interactions to generate guidance materials delivered as playable step-by-step overlays, a.k.a., dashboard tours. Authors can further edit and refine individual tour steps while receiving generative assistance. We also contribute findings from a formative assessment with 9 dashboard creators, which helped inform the design of DashGuide; and findings from an evaluation of DashGuide with 12 dashboard creators, suggesting it provides an improved authoring experience that balances efficiency, expressiveness, and creative freedom.

</details>


### [5] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)

*Robert Kaufman*

**Main category:** cs.HC

**Keywords:** Autonomous Vehicles, Human-Computer Interaction, Machine Learning, Trust in AVs, Adaptive Communication

**Relevance Score:** 7

**TL;DR:** This dissertation explores how autonomous vehicles (AVs) can effectively meet the informational needs of riders through tailored communication strategies and adaptability based on individual and contextual factors.

**Read time:** 120 min

<details>
  <summary>Details</summary>

**Motivation:** To address unresolved questions about how AVs should communicate to satisfy the diverse informational needs of riders, considering varying individuals, goals, and driving contexts.

**Method:** The dissertation presents three empirical studies: identifying optimal communication strategies for enhancing performance in extreme environments, examining consequences of faulty communication systems, and using machine learning to predict trust in AVs based on personal factors.

**Key Contributions:** Identified communication strategies that enhance trust and performance in AVs., Demonstrated the impact of faulty communication systems on rider experience., Applied machine learning to predict trust in AVs based on personal factors.

**Result:** The studies reveal that task-sensitive, modality-appropriate communications are vital for enhancing driving performance, confidence, and trust. They underline the significance of context-sensitive approaches and individual trait considerations in designing AV communications.

**Limitations:** Focused primarily on the human-AV interaction context, possibly limiting broader applicability.

**Future Work:** Future research can explore more extensively the theories of human-machine joint action and situational awareness in various contexts beyond AVs.

**Conclusion:** Transparent, adaptable, and personalized AV systems are essential for catering to the unique needs and goals of individuals, which can improve human-AV interactions and guide future research in human-AI interaction.

**Abstract:** Unresolved questions about how autonomous vehicles (AVs) should meet the informational needs of riders hinder real-world adoption. Complicating our ability to satisfy rider needs is that different people, goals, and driving contexts have different criteria for what constitutes interaction success. Unfortunately, most human-AV research and design today treats all people and situations uniformly. It is crucial to understand how an AV should communicate to meet rider needs, and how communications should change when the human-AV complex system changes. I argue that understanding the relationships between different aspects of the human-AV system can help us build improved and adaptable AV communications. I support this argument using three empirical studies. First, I identify optimal communication strategies that enhance driving performance, confidence, and trust for learning in extreme driving environments. Findings highlight the need for task-sensitive, modality-appropriate communications tuned to learner cognitive limits and goals. Next, I highlight the consequences of deploying faulty communication systems and demonstrate the need for context-sensitive communications. Third, I use machine learning (ML) to illuminate personal factors predicting trust in AVs, emphasizing the importance of tailoring designs to individual traits and concerns. Together, this dissertation supports the necessity of transparent, adaptable, and personalized AV systems that cater to individual needs, goals, and contextual demands. By considering the complex system within which human-AV interactions occur, we can deliver valuable insights for designers, researchers, and policymakers. This dissertation also provides a concrete domain to study theories of human-machine joint action and situational awareness, and can be used to guide future human-AI interaction research. [shortened for arxiv]

</details>


### [6] [Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication](https://arxiv.org/abs/2504.17171)

*Sunday David Ubur*

**Main category:** cs.HC

**Keywords:** augmented reality, captioning, Deaf and Hard of Hearing, STEM education, emotional cues

**Relevance Score:** 8

**TL;DR:** An AR captioning framework for Deaf and Hard of Hearing learners integrates emotional cues into transcriptions, enhancing comprehension in STEM education.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility for Deaf and Hard of Hearing (DHH) learners in STEM classrooms by adding emotional context to live captions.

**Method:** The system combines real-time speech recognition with AI models that interpret visual and vocal cues to create enriched AR captions.

**Key Contributions:** Integration of non-verbal emotional cues into captioning for DHH learners., Use of an AR interface for enhanced educational experiences., Demonstrated improvement in comprehension and cognitive effort reduction.

**Result:** Preliminary evaluations indicate that the AR captioning significantly enhances comprehension and reduces cognitive load compared to standard captions.

**Limitations:** The study focuses on preliminary evaluations; broader testing in diverse environments is needed for comprehensive validation.

**Future Work:** Further research to explore the application in more varied educational settings and extensive user testing.

**Conclusion:** The immersive AR environment has the potential to improve educational accessibility by being inclusive and emotion-aware.

**Abstract:** This paper introduces an augmented reality (AR) captioning framework designed to support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by integrating non-verbal emotional cues into live transcriptions. Unlike conventional captioning systems that offer only plain text, our system fuses real-time speech recognition with affective and visual signal interpretation, including facial movements, gestures, and vocal tone, to produce emotionally enriched captions. These enhanced captions are rendered in an AR interface developed with Unity and provide contextual annotations such as speaker tone markers (e.g., "concerned") and gesture indicators (e.g., "nods"). The system leverages live camera and microphone input, processed through AI models to detect multimodal cues. Findings from preliminary evaluations suggest that this AR-based captioning approach significantly enhances comprehension and reduces cognitive effort compared to standard captions. Our work emphasizes the potential of immersive environments for inclusive, emotion-aware educational accessibility.

</details>


### [7] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)

*Tianyu Zhang, Dongheng Zhang, Ruixu Geng, Xuecheng Xie, Shuai Yang, Yan Chen*

**Main category:** cs.HC

**Keywords:** Channel State Information, WiFi localization, large-scale deployment

**Relevance Score:** 4

**TL;DR:** This paper presents a large-scale CSI-based localization system utilizing a novel learning framework to enhance WiFi localization accuracy in real-world settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recognizing the limitations of existing CSI-based systems, this work aims to improve deployment scalability and commercialization of CSI for indoor localization.

**Method:** The study employs a graph-based structure to model heterogeneous CSI data, a pretext pretraining task leveraging spatiotemporal priors, and a confidence-aware fine-tuning strategy.

**Key Contributions:** Development of a novel graph-based structure for CSI modeling., Introduction of a pretext pretraining task for leveraging unlabeled data., Proposal of a confidence-aware fine-tuning strategy for improved robustness.

**Result:** The proposed system achieves a median localization error of 2.17 meters and a floor accuracy of 99.49%, significantly outperforming the best baseline by 18.7% in MAE.

**Limitations:** The study is focused on a specific real-world environment and may not generalize to all indoor localization scenarios.

**Future Work:** Further research is needed to explore the scalability of the proposed approaches to other environments and to improve generalizability across diverse settings.

**Conclusion:** The introduction of unlabeled data utilization and the optimization of heterogeneity in CSI measurements are crucial for enhancing localization performance in large-scale settings.

**Abstract:** In recent years, Channel State Information (CSI), recognized for its fine-grained spatial characteristics, has attracted increasing attention in WiFi-based indoor localization. However, despite its potential, CSI-based approaches have yet to achieve the same level of deployment scale and commercialization as those based on Received Signal Strength Indicator (RSSI). A key limitation lies in the fact that most existing CSI-based systems are developed and evaluated in controlled, small-scale environments, limiting their generalizability. To bridge this gap, we explore the deployment of a large-scale CSI-based localization system involving over 400 Access Points (APs) in a real-world building under the Integrated Sensing and Communication (ISAC) paradigm. We highlight two critical yet often overlooked factors: the underutilization of unlabeled data and the inherent heterogeneity of CSI measurements. To address these challenges, we propose a novel CSI-based learning framework for WiFi localization, tailored for large-scale ISAC deployments on the server side. Specifically, we employ a novel graph-based structure to model heterogeneous CSI data and reduce redundancy. We further design a pretext pretraining task that incorporates spatial and temporal priors to effectively leverage large-scale unlabeled CSI data. Complementarily, we introduce a confidence-aware fine-tuning strategy to enhance the robustness of localization results. In a leave-one-smartphone-out experiment spanning five floors and 25, 600 m2, we achieve a median localization error of 2.17 meters and a floor accuracy of 99.49%. This performance corresponds to an 18.7% reduction in mean absolute error (MAE) compared to the best-performing baseline.

</details>


### [8] [Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment](https://arxiv.org/abs/2504.17204)

*Chitralekha Gupta, Hanjun Wu, Praveen Sasikumar, Shreyas Sridhar, Priambudi Bagaskara, Suranga Nanayakkara*

**Main category:** cs.HC

**Keywords:** wearable devices, cognitive augmentation, misinformation detection, interactive learning, fact-checking

**Relevance Score:** 8

**TL;DR:** The paper introduces Factually, a voice-based interactive learning companion that enhances cognitive functions through real-time misinformation detection and language learning support via wearable devices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to augment human cognitive abilities through informal learning and critical thinking, addressing the challenges posed by misinformation.

**Method:** The paper presents a wearable fact-checking system, Factually, which uses vibrotactile feedback to alert users to potential falsehoods in information.

**Key Contributions:** Introduction of a proactive, wearable fact-checking system, Combination of cognitive augmentation, misinformation detection, and language learning support, Illustrative scenarios demonstrating practical applications of the system

**Result:** Early qualitative feedback indicates that Factually effectively enhances users' fact-checking capabilities and provides practical benefits in informal learning contexts.

**Limitations:** 

**Future Work:** Further development and study of the system in varied contexts to enhance its effectiveness and reach.

**Conclusion:** The system highlights the potential of wearable devices in extending cognitive functions through real-time interaction and learning.

**Abstract:** Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits.

</details>


### [9] [MV-Crafter: An Intelligent System for Music-guided Video Generation](https://arxiv.org/abs/2504.17267)

*Chuer Chen, Shengqi Dang, Yuqi Liu, Nanxuan Zhao, Yang Shi, Nan Cao*

**Main category:** cs.HC

**Keywords:** music video generation, human-computer interaction, large language models

**Relevance Score:** 5

**TL;DR:** MV-Crafter is a system for automated music video generation that enhances output quality and simplifies the creation process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing popularity of music videos necessitates better automated generation frameworks that are user-friendly, especially for non-professionals.

**Method:** MV-Crafter includes a script generation module using a large language model, a video generation module, and a music-video synchronization module featuring a dynamic beat matching algorithm.

**Key Contributions:** Introduction of a script generation module using a large language model, Dynamic beat matching algorithm for music-video synchronization, User-friendly interface with intuitive editing features

**Result:** Extensive experiments show that MV-Crafter produces high-quality music videos with better synchronization and user-friendly controls.

**Limitations:** 

**Future Work:** 

**Conclusion:** MV-Crafter effectively addresses key challenges in automated music video generation, improving both the process and output quality.

**Abstract:** Music videos, as a prevalent form of multimedia entertainment, deliver engaging audio-visual experiences to audiences and have gained immense popularity among singers and fans. Creators can express their interpretations of music naturally through visual elements. However, the creation process of music video demands proficiency in script design, video shooting, and music-video synchronization, posing significant challenges for non-professionals. Previous work has designed automated music video generation frameworks. However, they suffer from complexity in input and poor output quality. In response, we present MV-Crafter, a system capable of producing high-quality music videos with synchronized music-video rhythm and style. Our approach involves three technical modules that simulate the human creation process: the script generation module, video generation module, and music-video synchronization module. MV-Crafter leverages a large language model to generate scripts considering the musical semantics. To address the challenge of synchronizing short video clips with music of varying lengths, we propose a dynamic beat matching algorithm and visual envelope-induced warping method to ensure precise, monotonic music-video synchronization. Besides, we design a user-friendly interface to simplify the creation process with intuitive editing features. Extensive experiments have demonstrated that MV-Crafter provides an effective solution for improving the quality of generated music videos.

</details>


### [10] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)

*Süleyman Özdel, Kadir Burak Buldu, Enkelejda Kasneci, Efe Bozkir*

**Main category:** cs.HC

**Keywords:** human-computer interaction, virtual reality, large language models, hands-free locomotion, user engagement

**Relevance Score:** 9

**TL;DR:** This study proposes a novel hands-free locomotion technique for virtual reality powered by large language models, allowing users to navigate using natural language. It evaluates the method against traditional locomotion techniques, showing comparable usability and enhanced user engagement.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve navigational methods in virtual reality by offering a hands-free alternative that utilizes natural language processing for accessibility and flexibility in user interaction.

**Method:** Three locomotion methods were evaluated: controller-based teleportation, voice-based steering, and a language model-driven approach. The evaluation involved eye-tracking data analysis and standardized questionnaires assessing usability, presence, cybersickness, and cognitive load.

**Key Contributions:** Novel locomotion technique using large language models for natural language navigation in VR., Evaluation of multiple locomotion methods with eye-tracking and user questionnaires., Demonstration of enhanced user engagement and attention with the LLM-based method.

**Result:** The LLM-driven locomotion exhibited comparable usability, presence, and cybersickness scores to teleportation, while enhancing user attention and engagement in the virtual environment.

**Limitations:** The study focuses on a limited set of locomotion methods and may require further exploration of long-term usability and user adaptation.

**Future Work:** Future research could expand on different locomotion techniques and explore long-term user engagement and adaptation to the LLM-driven approach.

**Conclusion:** The proposed method serves as a comfortable, natural language-based alternative for hands-free locomotion in virtual reality, with notable implications for accessibility.

**Abstract:** Locomotion plays a crucial role in shaping the user experience within virtual reality environments. In particular, hands-free locomotion offers a valuable alternative by supporting accessibility and freeing users from reliance on handheld controllers. To this end, traditional speech-based methods often depend on rigid command sets, limiting the naturalness and flexibility of interaction. In this study, we propose a novel locomotion technique powered by large language models (LLMs), which allows users to navigate virtual environments using natural language with contextual awareness. We evaluate three locomotion methods: controller-based teleportation, voice-based steering, and our language model-driven approach. Our evaluation measures include eye-tracking data analysis, including explainable machine learning through SHAP analysis as well as standardized questionnaires for usability, presence, cybersickness, and cognitive load to examine user attention and engagement. Our findings indicate that the LLM-driven locomotion possesses comparable usability, presence, and cybersickness scores to established methods like teleportation, demonstrating its novel potential as a comfortable, natural language-based, hands-free alternative. In addition, it enhances user attention within the virtual environment, suggesting greater engagement. Complementary to these findings, SHAP analysis revealed that fixation, saccade, and pupil-related features vary across techniques, indicating distinct patterns of visual attention and cognitive processing. Overall, we state that our method can facilitate hands-free locomotion in virtual spaces, especially in supporting accessibility.

</details>


### [11] [DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent](https://arxiv.org/abs/2504.17334)

*Chuer Chen, Yuqi Liu, Danqing Shi, Shixiong Cao, Nan Cao*

**Main category:** cs.HC

**Keywords:** Data Retrieval, Human-Computer Interaction, Large Language Models, Data Storytelling, Stance-based Reasoning

**Relevance Score:** 8

**TL;DR:** DataScout is an interactive system for automated stance-based data retrieval to enhance narrative construction, using an LLM-based agent to visualize and control the retrieval process.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of retrieving data facts from multiple perspectives demands time and analytical skills from creators, prompting the need for a supportive tool.

**Method:** DataScout uses an LLM-based agent to build a collaborative retrieval tree, visualized as a mind map, allowing users to direct their search and engage in reasoning.

**Key Contributions:** Introduction of DataScout as an interactive data retrieval tool., Utilization of an LLM-based agent for reasoned data fact retrieval., Visualization of retrieval as a mind map for enhanced user interaction.

**Result:** Evaluation through case studies and expert interviews showed that DataScout effectively retrieves diverse data facts, improving statement verification and narrative credibility.

**Limitations:** Limited scope of case studies may not cover all potential applications of the system.

**Future Work:** Explore additional features for enhancing user interaction and broader applicability in various fields.

**Conclusion:** DataScout provides a valuable tool for users to intuitively access and analyze multifaceted data for narrative purposes.

**Abstract:** A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.

</details>


### [12] [The Riemannian Means Field Classifier for EEG-Based BCI Data](https://arxiv.org/abs/2504.17352)

*Anton Andreev, Grégoire Cattan, Marco Congedo*

**Main category:** cs.HC

**Keywords:** Brain-computer interfaces, Riemannian geometry, Minimum distance to mean, EEG, Machine learning

**Relevance Score:** 6

**TL;DR:** This paper proposes an improved Riemannian minimum distance to mean (MDM) classifier for EEG-based brain-computer interfaces (BCIs), which outperforms the original MDM in accuracy while maintaining simplicity and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of EEG-based brain-computer interfaces by improving the minimum distance to mean (MDM) classifier through the use of power means of symmetric positive-definite matrices.

**Method:** The paper analyzes 20 public EEG databases, applying the improved MDM classifier that utilizes power means instead of solely the geometric mean.

**Key Contributions:** Development of a new MDM classifier using power means of SPD matrices, Demonstration of improved performance on 20 EEG data sets, Promotion of reproducible research through open source code release

**Result:** The proposed classifier outperforms the original MDM classifier, approaching the state-of-the-art performance and showing robustness and computational efficiency.

**Limitations:** 

**Future Work:** Future research may focus on further enhancements to the classifier and its application across additional BCI paradigms.

**Conclusion:** The new classifier maintains the simplicity and deterministic nature of the original MDM while achieving better performance metrics, and the code is released for open source use.

**Abstract:** A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source.

</details>


### [13] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)

*Michelle L. Ding, Harini Suresh*

**Main category:** cs.HC

**Keywords:** sociotechnical AI governance, AIG-NCII, malicious technical ecosystem, synthetic content governance, deep fake pornography

**Relevance Score:** 4

**TL;DR:** The paper discusses sociotechnical AI governance to combat AI-Generated Non-Consensual Intimate Images (AIG-NCII), identifying gaps in current regulatory practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and address the role of sociotechnical AI governance in preventing the creation and distribution of AIG-NCII.

**Method:** Adopting a survivor-centered approach to analyze existing governance methods and identifying a malicious technical ecosystem associated with AIG-NCII.

**Key Contributions:** Identification of a malicious technical ecosystem for AIG-NCII creation., Analysis of current synthetic content governance practices based on NIST report., Highlighting the gaps and flaws in existing regulatory frameworks.

**Result:** The paper identifies a 'malicious technical ecosystem' that enables the rapid creation of AIG-NCII and highlights inefficiencies in the current governance frameworks.

**Limitations:** The study focuses primarily on adult AIG-NCII and may not fully address other types of synthetic content misuse.

**Future Work:** Future research should explore more effective regulatory frameworks and governance methods encompassing broader contexts of synthetic content.

**Conclusion:** Current practices fail to effectively regulate the malicious technical ecosystem, necessitating a reevaluation of existing assumptions and governance methods.

**Abstract:** In this paper, we adopt a survivor-centered approach to locate and dissect the role of sociotechnical AI governance in preventing AI-Generated Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as "deep fake pornography." We identify a "malicious technical ecosystem" or "MTE," comprising of open-source face-swapping models and nearly 200 "nudifying" software programs that allow non-technical users to create AIG-NCII within minutes. Then, using the National Institute of Standards and Technology (NIST) AI 100-4 report as a reflection of current synthetic content governance methods, we show how the current landscape of practices fails to effectively regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining these gaps.

</details>


### [14] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)

*Jarne Thys, Sebe Vanbrabant, Davy Vanacken, Gustavo Rovelo Ruiz*

**Main category:** cs.HC

**Keywords:** AI in education, Large Language Models, personalized learning, student engagement, adaptive learning

**Relevance Score:** 9

**TL;DR:** This paper introduces INSIGHT, a modular AI tool designed to assist teaching staff and students in higher education by personalizing support and generating insights from student questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI in education presents challenges and opportunities, particularly in enhancing teaching methods while preserving the quality of student-teacher interactions and privacy.

**Method:** The paper presents INSIGHT, which analyzes student questions to an LLM, extracts keywords to build a dynamic FAQ, and offers insights for personalized support.

**Key Contributions:** Introduction of INSIGHT as a proof of concept., Modular design for integration into various higher education courses., Dynamic FAQ generation from student inquiries to support personalized teaching.

**Result:** INSIGHT provides a framework for better engagement between students and teaching staff through enhanced understanding of common questions and individualized support.

**Limitations:** The limitations of current implementations and data collection methods have not been fully explored in the paper.

**Future Work:** Future research directions include enhancing adaptive learning capabilities and tailoring content delivery based on individual student needs.

**Conclusion:** Future work will focus on adapting INSIGHT to support personalized learning based on student progress and diverse learning styles, aiming for a more interactive educational experience.

**Abstract:** The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. This paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.

</details>


### [15] ['The Boring and the Tedious': Invisible Labour in India's Gig-Economy](https://arxiv.org/abs/2504.17697)

*Pratyay Suvarnapathaki, Viral Shah, Saarthak Negi, Nimmi Rangaswamy*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, gig economy, worker autonomy, digital discomfort, UI design

**Relevance Score:** 8

**TL;DR:** The paper examines the digital discomfort experienced by gig food delivery workers in India and advocates for a shift towards worker-centered design in HCI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the digital discomfort faced by gig-based food delivery agents, particularly in the context of India's platforms like Swiggy and Zomato.

**Method:** Conducted 14 semi-structured interviews with gig food delivery workers to analyze their experiences, focusing on waiting times and repetitive UI interactions.

**Key Contributions:** Highlighted the digital discomfort of gig workers in India., Proposed worker-centered GUI automation as an intervention., Called for a reevaluation of HCI design principles to prioritize worker autonomy.

**Result:** Identified that gig workers experience significant digital discomfort due to algorithmic management, gamification, and system opacity, while also employing creative strategies to cope with these challenges.

**Limitations:** The findings are based on a limited sample size and may not generalize across all gig workers in different contexts.

**Future Work:** Future research should explore broader user studies and the effectiveness of proposed interventions in various gig economies.

**Conclusion:** The paper argues for a paradigm shift in HCI approaches in the Global South, emphasizing the importance of worker autonomy over mere efficiency in design.

**Abstract:** India's gig-based food delivery platforms, such as Swiggy and Zomato, provide crucial income to marginalised communities but also entrench workers in cycles of invisible labour. Through 14 semi-structured interviews, we analyse waiting time and repetitive UI itneractions as key burdens that contribute to 'digital discomfort' for gig based food delivery agents. We find that workers employ creative strategies to navigate algorithmic management, yet remain constrained by platform-side 'gamification' and system opacity. We propose worker-centered GUI automation as a potential intervention to reduce friction while preserving agency. In conclusion, this position paper argues for rethinking HCI approaches in the Global South to prioritise worker autonomy over efficiency-driven design optimisations.

</details>


### [16] [LUIDA: Large-scale Unified Infrastructure for Digital Assessments based on Commercial Metaverse Platform](https://arxiv.org/abs/2504.17705)

*Yong-Hao Hu, Sotaro Yokoi, Yuji Hatada, Yuichi Hiroi, Takuji Narumi, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Virtual Reality, Metaverse, Research Efficiency, Experimental Reproducibility

**Relevance Score:** 8

**TL;DR:** LUIDA is a metaverse-based framework that streamlines online experiments in HCI and VR by integrating fragmented research workflows.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmented nature of workflows in online experiments using metaverse platforms, enhancing consistency and reducing researcher workload.

**Method:** LUIDA automates the allocation of virtual environments for parallel experiment execution and offers implementation templates adaptable to various VR research domains, requiring minimal development expertise.

**Key Contributions:** Integration of fragmented research workflows in metaverse platforms., High usability and low workload reported by researchers using LUIDA., Validation of experimental integrity through replicated studies.

**Result:** Researchers reported high usability scores and moderate workload when using LUIDA, and experiments replicated with public participants matched original studies, confirming LUIDA's effectiveness.

**Limitations:** The evaluation was based on a prototype, and user experience may vary with further technical refinements and broader implementation.

**Future Work:** To technically refine LUIDA further and release it as an open platform for wider use in VR research.

**Conclusion:** LUIDA will be released as an open platform aimed at improving research efficiency and reproducibility in VR studies.

**Abstract:** Online experiments using metaverse platforms have gained significant traction in Human-Computer Interaction and Virtual Reality (VR) research. However, current research workflows are highly fragmented, as researchers must use separate tools for system implementation, participant recruitment, experiment execution, and data collection, reducing consistency and increasing workload. We present LUIDA (Large-scale Unified Infrastructure for Digital Assessments), a metaverse-based framework that integrates these fragmented processes. LUIDA automatically allocates interconnected virtual environments for parallel experiment execution and provides implementation templates adaptable to various VR research domains, requiring minimal metaverse development expertise. Our evaluation included two studies using a prototype built on Cluster, the commercial metaverse platform. First, VR researchers using LUIDA to develop and run experiments reported high usability scores (SUS: 73.75) and moderate workload (NASA-TLX: 24.11) for overall usage, with interviews confirming streamlined workflows compared to traditional laboratory experiments. Second, we conducted three replicated experiments with public Cluster users, each recruiting approximately 200 participants within one week. These experiments produced results that closely matched the original studies, validating the experimental integrity of LUIDA across research domains. After technical refinements, we plan to release LUIDA as an open platform, providing a standardized protocol to improve research efficiency and experimental reproducibility in VR studies.

</details>


### [17] [WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing](https://arxiv.org/abs/2412.00247)

*Devin Murphy, Junyi Zhu, Paul Pu Liang, Wojciech Matusik, Yiyue Luo*

**Main category:** cs.HC

**Keywords:** tactile sensing, human-computer interaction, wireless sensing, toolkit, adaptive hardware

**Relevance Score:** 7

**TL;DR:** The WiReSens Toolkit is an open-source platform designed to enhance the accessibility and usability of wireless tactile sensing systems, featuring adaptive hardware and a web-based GUI for easier implementation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create portable, adaptive, and long-lasting tactile sensing systems using resistive sensors for those with limited prior experience.

**Method:** Development of the WiReSens Toolkit, which includes adaptive hardware for interfacing with resistive sensors and a web-based GUI, enabling multi-device programming, autocalibration, and low-power data transmission.

**Key Contributions:** Open-source toolkit for tactile sensing accessibility, Integration of adaptive hardware and web-based GUI, User study demonstrating superior usability and performance

**Result:** A user study showed that novice participants configured the tactile sensor with over 95% accuracy in under five minutes and calibrated sensors 10 times faster than traditional methods.

**Limitations:** The study involved a small sample size of novice participants; further research may be needed to validate results with more experienced users.

**Future Work:** Expansion of the toolkit's functionalities and further user studies to validate findings across diverse user groups.

**Conclusion:** The WiReSens Toolkit significantly improves the accessibility and usability of tactile sensing systems for novice users.

**Abstract:** Past research has widely explored the design and fabrication of resistive matrix-based tactile sensors as a means of creating touch-sensitive devices. However, developing portable, adaptive, and long-lasting tactile sensing systems that incorporate these sensors remains challenging for individuals having limited prior experience with them. To address this, we developed the WiReSens Toolkit, an open-source platform for accessible wireless tactile sensing. Central to our approach is adaptive hardware for interfacing with resistive sensors and a web-based GUI that mediates access to complex functionalities for developing scalable tactile sensing systems, including 1) multi-device programming and wireless visualization across three distinct communication protocols 2) autocalibration methods for adaptive sensitivity and 3) intermittent data transmission for low-power operation. We validated the toolkit's usability through a user study with 11 novice participants, who, on average, successfully configured a tactile sensor with over 95\% accuracy in under five minutes, calibrated sensors 10x faster than baseline methods, and demonstrated enhanced tactile data sense-making.

</details>


### [18] [Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory](https://arxiv.org/abs/2504.14539)

*Linkun Liu, Jian Sun, Ye Tian*

**Main category:** cs.HC

**Keywords:** autonomous vehicles, human-machine interface, deception, game theory, safety

**Relevance Score:** 6

**TL;DR:** This study explores the role of benevolent deception in external human-machine interfaces (EHMI) for autonomous vehicles (AVs), proposing a game theory based framework for information disclosure that enhances safety in certain scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gaps in existing research regarding the sequence of actions and the effects of EHMI applications and deception in autonomous vehicles.

**Method:** A game theory based EHMI information disclosure framework was established, dividing the decision-making process into three stages related to the disclosure of information.

**Key Contributions:** Proposed a game theory based EHMI framework for AVs, Identified conditions under which benevolent deception can enhance safety, Conducted a VR-based driving simulation that validated the theoretical findings

**Result:** In 8.3% of the cases analyzed, safety was enhanced through successful deception, particularly in two identified scenarios involving the interactions between straight-going AVs and left-turning human vehicles (HV).

**Limitations:** The study primarily focuses on theoretical frameworks and specific simulation conditions, which may limit broader applicability.

**Future Work:** Further investigation into the ethical implications and broader application of EHMI frameworks in real-world scenarios is needed.

**Conclusion:** While benevolent deception can improve safety in specific contexts, low trust in EHMI can lead to decreased interaction efficiency.

**Abstract:** The application of external human-machine interface (EHMI) on autonomous vehicles (AVs) facilitates information exchange. Existing research fails to consider the impact of the sequence of actions, as well as the effects of EHMI applications and deception, raising the question of whether benevolent, well-intentioned deception should be permitted (i.e., misleading statements that are intended to benefit both parties). We established a game theory based EHMI information disclosure framework for AVs in this study. In considering benevolent deception, this framework divided the decision-making process into three stages, respectively encompassing three key questions: whether to disclose, when to disclose, and what type of intention information to disclose. The results show that theoretical advantages of deception exist in certain cases when AV expects to maximize the safety of the interaction. In 40 out of 484 cases (8.3%), safety can be enhanced through successful deception. Those successful deceptions fall into two categories: 1) In 28 of these cases, the straight-going AV expected the left-turning HV to yield, while HV exhibited lower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to proceed first, while HV exhibited higher speed and lower acceleration. We also conducted a VR-based driving simulation experiment, and the results confirmed our conclusion. Additionally, we found that when participants had low trust in the EHMI, its use negatively impacted interaction efficiency instead. This study aims to analyze the mechanisms of EHMI information disclosure and contribute to the ongoing discourse on the ethical framework governing autonomous driving systems.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)

*Cong Qi, Hanzhang Fang, Tianxing Hu, Siqi Jiang, Wei Zhi*

**Main category:** cs.CL

**Keywords:** single-cell RNA sequencing, transformer models, GeneMamba

**Relevance Score:** 4

**TL;DR:** GeneMamba is a scalable foundation model for single-cell RNA sequencing, addressing computational challenges while outperforming traditional transformer models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient analysis of cellular heterogeneity in single-cell RNA sequencing due to high dimensionality, sparsity, and batch effects.

**Method:** GeneMamba employs state space modeling with a Bi-Mamba architecture for linear-time complexity in processing single-cell transcriptomics data.

**Key Contributions:** Introduction of GeneMamba foundation model, Bidirectional gene context capturing with linear-time complexity, Biologically informed objectives for pretraining

**Result:** GeneMamba shows strong performance in tasks like multi-batch integration and cell type annotation, while being computationally efficient and interpretable.

**Limitations:** The effectiveness of GeneMamba on non-transcriptomic data is not explored.

**Future Work:** Further exploration of GeneMamba's applicability in diverse biological contexts and integration with other ML methods.

**Conclusion:** GeneMamba presents a practical alternative to transformer-based approaches, enhancing the analysis of large-scale single-cell data.

**Abstract:** Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.

</details>


### [20] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)

*Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal*

**Main category:** cs.CL

**Keywords:** Tokenization, Low Resource Languages, Named Entity Recognition, SentencePiece, Byte Pair Encoding

**Relevance Score:** 8

**TL;DR:** This paper compares different tokenization methods for Named Entity Recognition (NER) in low resource Indic languages, finding that SentencePiece outperforms Byte Pair Encoding (BPE) in terms of performance and linguistic preservation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of various tokenization strategies for NER tasks in low resource Indic languages, addressing limitations in BPE.

**Method:** A systematic comparison of BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT to assess tokenization efficiency and downstream performance for NER tasks.

**Key Contributions:** Demonstrated the superiority of SentencePiece over BPE for NER in low resource Indic languages., Provided insights on tokenization efficiency and morphological preservation across different Indic languages., Addressed the challenge of entity recognition in extremely low resource languages.

**Result:** SentencePiece consistently outperformed BPE in NER tasks, particularly excelling in zero shot cross lingual contexts and maintaining better entity consistency.

**Limitations:** Study primarily focused on low resource Indic languages; results may not generalize to other language families.

**Future Work:** Further exploration of tokenization strategies in other linguistic contexts and languages, expanding beyond Indic languages.

**Conclusion:** SentencePiece is the more effective tokenization strategy for NER in multilingual and low resource Indic NLP applications.

**Abstract:** Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.   Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.

</details>


### [21] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)

*Luca Moroni, Giovanni Puccetti, Pere-Lluis Huguet Cabot, Andrei Stefan Bejgu, Edoardo Barba, Alessio Miaschi, Felice Dell'Orletta, Andrea Esuli, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** Large Language Models, vocabulary adaptation, machine learning, natural language processing, multilingual models

**Relevance Score:** 6

**TL;DR:** This paper presents SAVA, a novel vocabulary adaptation method for optimizing English LLMs for the Italian language, improving their performance in multiple tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and slower inference speeds of LLMs when handling non-English languages, specifically Italian, through better vocabulary adaptation techniques.

**Method:** The paper compares various vocabulary adaptation techniques and introduces Semantic Alignment Vocabulary Adaptation (SAVA), which uses neural mapping for vocabulary substitution.

**Key Contributions:** Introduction of SAVA for vocabulary adaptation in LLMs., Reduction of token fertility by 25% for Mistral-7b-v0.1., Optimization of vocabulary leading to a 1 billion parameter reduction in Llama-3.1-8B.

**Result:** SAVA achieves a 25% reduction in token fertility for Mistral-7b-v0.1 and optimizes Llama-3.1-8B by reducing the number of parameters by 1 billion while maintaining performance after continual training.

**Limitations:** The paper focuses primarily on Italian; applicability to other non-English languages is not explored extensively.

**Future Work:** Further research could investigate the adaptation of LLMs for additional languages and explore broader applications of SAVA.

**Conclusion:** The adapted models demonstrate enhanced performance across various tasks, making them more efficient for Italian without significant retraining.

**Abstract:** The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.

</details>


### [22] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)

*Shariar Kabir, Kevin Esterling, Yue Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, political discourse, belief stability, economic policies, argumentative consistency

**Relevance Score:** 8

**TL;DR:** The paper introduces a framework for evaluating belief depth in LLMs, examining their consistency and uncertainty in responses to economic policy arguments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether the apparent political stances of LLMs are genuine beliefs or merely reflect training data.

**Method:** Analyzed 12 LLMs on 19 economic policies from the Political Compass Test, testing their belief stability with both supportive and opposing arguments.

**Key Contributions:** Proposed a novel framework for evaluating belief depth in LLMs., Demonstrated topic-specific belief stability rather than uniform ideological stances., Achieved high accuracy in distinguishing genuine beliefs from surface-level alignment.

**Result:** Found that LLMs exhibit topic-specific belief stability; 95% of left-leaning and 89% of right-leaning responses were consistent under scrutiny, with high accuracy in distinguishing between genuine belief and surface-level alignment.

**Limitations:** Limited to 12 LLMs and 19 economic policies; may not generalize to other topics or models.

**Future Work:** Further research could explore additional topics and LLM architectures for a broader understanding of belief stability.

**Conclusion:** The findings indicate that LLMs do not possess stable, human-like political ideologies and highlight the need for reliability assessments tailored to specific topics in real-world applications.

**Abstract:** Large Language Models (LLMs) are increasingly shaping political discourse, yet their responses often display inconsistency when subjected to scrutiny. While prior research has primarily categorized LLM outputs as left- or right-leaning to assess their political stances, a critical question remains: Do these responses reflect genuine internal beliefs or merely surface-level alignment with training data? To address this, we propose a novel framework for evaluating belief depth by analyzing (1) argumentative consistency and (2) uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from the Political Compass Test, challenging their belief stability with both supportive and opposing arguments. Our analysis reveals that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Notably, up to 95% of left-leaning models' responses and 89% of right-leaning models' responses remain consistent under the challenge, enabling semantic entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing between surface-level alignment from genuine belief. These findings call into question the assumption that LLMs maintain stable, human-like political ideologies, emphasizing the importance of conducting topic-specific reliability assessments for real-world applications.

</details>


### [23] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)

*Arjun Subramonian, Vagrant Gautam, Preethi Seshadri, Dietrich Klakow, Kai-Wei Chang, Yizhou Sun*

**Main category:** cs.CL

**Keywords:** LLM, misgendering, evaluation methods, meta-evaluation, human evaluation

**Relevance Score:** 8

**TL;DR:** The paper conducts a systematic meta-evaluation of LLM misgendering measurement methods, revealing significant discrepancies between various evaluation approaches and highlighting the complexities of misgendering behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine whether different methods for evaluating LLM misgendering converge in their results, as existing methods have not been systematically compared for validity.

**Method:** The authors systematically meta-evaluate probability-based and generation-based evaluation methods across three existing datasets, transforming each dataset for parallel evaluation and assessing six models from three families.

**Key Contributions:** Systematic comparison of evaluation methods for LLM misgendering, Evidence of significant disagreement between automated and human evaluations, Recommendations for improving evaluation methods in LLM misgendering context

**Result:** The authors found disagreement in evaluation results at the instance, dataset, and model levels, with a 20.2% conflict rate. Human evaluation revealed complexities in misgendering behavior beyond traditional metrics.

**Limitations:** The study focuses on only three existing datasets and a limited number of models, which may not represent the full spectrum of LLMs.

**Future Work:** Future evaluations of LLM misgendering should consider the discrepancies identified and incorporate more nuanced aspects of misgendering behavior.

**Conclusion:** The findings suggest substantial disagreement between automatic evaluations and human judgments, indicating that current methods may not adequately capture the complexities of LLM misgendering. Recommendations for future evaluation practices are provided.

**Abstract:** Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.

</details>


### [24] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)

*Rendi Chevi, Kentaro Inui, Thamar Solorio, Alham Fikri Aji*

**Main category:** cs.CL

**Keywords:** LLM, language style, user preferences, HCI, misinformation

**Relevance Score:** 8

**TL;DR:** This paper explores how the language style of LLM responses influences user preferences, especially in terms of perceived authority and certainty.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand what makes interactions with LLMs preferable for users, focusing on the influence of language style.

**Method:** The authors conducted exploratory and experimental user studies to analyze the effects of LLM's language style on user preferences across different populations and individual traits.

**Key Contributions:** Investigating the influence of language style on user preferences in LLMs., Demonstrating the moderation effect of individual traits on preferences., Proposing future research directions to explore causal relationships.

**Result:** The studies revealed that language style significantly influences user preferences, but the effect varies between users and is moderated by individual traits.

**Limitations:** Findings should be interpreted with caution due to sample size and lack of demographic diversity.

**Future Work:** Addressing sample limitations to conduct a comprehensive analysis of language style, individual traits, and preferences, and exploring causal relationships.

**Conclusion:** The preliminary findings suggest that while LLM language style enhances user experience, it also increases vulnerability to misinformation; future research will aim to address demographic limitations.

**Abstract:** What makes an interaction with the LLM more preferable for the user? While it is intuitive to assume that information accuracy in the LLM's responses would be one of the influential variables, recent studies have found that inaccurate LLM's responses could still be preferable when they are perceived to be more authoritative, certain, well-articulated, or simply verbose. These variables interestingly fall under the broader category of language style, implying that the style in the LLM's responses might meaningfully influence users' preferences. This hypothesized dynamic could have double-edged consequences: enhancing the overall user experience while simultaneously increasing their susceptibility to risks such as LLM's misinformation or hallucinations. In this short paper, we present our preliminary studies in exploring this subject. Through a series of exploratory and experimental user studies, we found that LLM's language style does indeed influence user's preferences, but how and which language styles influence the preference varied across different user populations, and more interestingly, moderated by the user's very own individual traits. As a preliminary work, the findings in our studies should be interpreted with caution, particularly given the limitations in our samples, which still need wider demographic diversity and larger sample sizes. Our future directions will first aim to address these limitations, which would enable a more comprehensive joint effect analysis between the language style, individual traits, and preferences, and further investigate the potential causal relationship between and beyond these variables.

</details>


### [25] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)

*Seunghyun Yoo*

**Main category:** cs.CL

**Keywords:** Human-centered explainability, Interactive design, Responsible AI

**Relevance Score:** 8

**TL;DR:** Proposes an Interactive Chain-of-Thought Framework to enhance explainability and responsible AI usage through clear, editable reasoning processes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Mitigate the decline in critical thinking and engagement caused by short-form content and AI adoption.

**Method:** Develop a modular framework that allows users to inspect, modify, and re-execute reasoning blocks, supplemented by a lightweight edit-adaptation mechanism.

**Key Contributions:** Interactive Chain-of-Thought Framework for user-editable reasoning., Integration of a lightweight edit-adaptation mechanism for cognitive alignment., Design principles for ethical transparency and bias mitigation.

**Result:** The framework promotes active cognitive engagement, ethical transparency, and accommodates diverse cognitive styles and user intentions.

**Limitations:** 

**Future Work:** Explore further applications of the framework in various AI systems and user contexts.

**Conclusion:** The proposed framework aims to foster critical engagement and responsible interaction with AI systems.

**Abstract:** Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [26] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)

*Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn*

**Main category:** cs.CL

**Keywords:** small language models, healthcare informatics, taxonomic framework, NLP tasks, model optimization

**Relevance Score:** 8

**TL;DR:** A survey of small language models (SLMs) in healthcare, outlining their taxonomic framework, architectures, and innovations for clinical applications.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To address data privacy concerns and resource limitations in healthcare by utilizing small language models (SLMs) as a scalable solution.

**Method:** The paper creates a taxonomic framework categorizing SLMs for healthcare professionals, analyzing models across NLP tasks, stakeholder roles, and the care continuum.

**Key Contributions:** Development of a taxonomic framework for SLMs in healthcare, Compilation of experimental results across healthcare NLP tasks, Discussion on architecture design and model optimization techniques.

**Result:** Experimental results across various NLP tasks demonstrate the practical effectiveness and transformative potential of SLMs in healthcare settings.

**Limitations:** Focuses primarily on small language models, possibly overlooking larger models and other ML techniques.

**Future Work:** Encourages further research into SLM adaptability, optimization, and integration in diverse healthcare scenarios.

**Conclusion:** The survey serves as a foundational resource for healthcare professionals to understand and implement SLMs, aiding future innovations in healthcare informatics.

**Abstract:** Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github

</details>


### [27] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)

*Hannah Cyberey, David Evans*

**Main category:** cs.CL

**Keywords:** large language models, censorship, representation engineering

**Relevance Score:** 8

**TL;DR:** This paper explores the censorship mechanisms in large language models (LLMs) and presents methods to detect and manipulate this censorship.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how censorship operates in large language models, especially in response to harmful requests.

**Method:** The paper employs representation engineering techniques to find a refusal-compliance vector and analyze reasoning LLMs for thought suppression.

**Key Contributions:** Presented a method for detecting censorship in LLM outputs., Analyzed additional dimensions of censorship through thought suppression., Introduced manipulation techniques for model reasoning processes.

**Result:** The study reveals methods for detecting censorship levels in LLM outputs and demonstrates that thought suppression can be manipulated through specific vectors.

**Limitations:** The study is limited to specific models and may not generalize to all LLMs or censorship contexts.

**Future Work:** Exploration of wider model classes and the implications of censorship in various applications.

**Conclusion:** The techniques presented can help in controlling and understanding censorship in AI models, suggesting possibilities for improved model transparency.

**Abstract:** Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector

</details>


### [28] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)

*Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Question Answering, Large Language Models, Evaluation Metrics, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** MIRAGE is a Question Answering dataset designed for evaluating Retrieval-Augmented Generation (RAG) systems, with new metrics for assessing noise vulnerability and context interpretation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating RAG systems due to the complex interaction between retrieval and generation components.

**Method:** Introduction of the MIRAGE dataset, consisting of 7,560 instances and a retrieval pool of 37,800 entries, along with novel evaluation metrics for RAG.

**Key Contributions:** Introduction of a new dataset for RAG evaluation, Development of novel evaluation metrics for RAG adaptability, Insights into optimal alignment of retriever-LLM pairs.

**Result:** Comprehensive experiments show insights into retriever-LLM configurations and the dynamics within RAG systems.

**Limitations:** The dataset may not cover all possible retrieval scenarios and model configurations.

**Future Work:** Further extension of the dataset and metrics to encompass additional retrieval strategies and their impact on generation quality.

**Conclusion:** MIRAGE facilitates component-specific assessments of RAG systems and provides tools for further research in model alignment and evaluation metrics.

**Abstract:** Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [29] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)

*Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** Machine Learning, Code Generation, Large Language Models, Reproducibility, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** PaperCoder is a multi-agent LLM framework that converts machine learning research papers into functional code repositories, enhancing reproducibility and usability of prior work.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of unavailable code implementations for machine learning research hinders reproducibility and progress in the field.

**Method:** PaperCoder operates in three stages: planning (constructing roadmaps and system architecture), analysis (interpreting implementation details), and generation (producing modular code), using specialized agents for effective collaboration.

**Key Contributions:** Introduction of a multi-agent system for autonomous code generation from research papers, Demonstrated effectiveness through both human and model-based evaluations, Top-performances on the PaperBench benchmark

**Result:** PaperCoder was evaluated on its ability to generate code from machine learning papers, achieving high quality implementations and outperforming baselines in benchmarks.

**Limitations:** 

**Future Work:** Further exploration of agent collaboration and expansion to other areas of research beyond machine learning.

**Conclusion:** The framework effectively transforms scientific documents into usable code, supporting better reproducibility in machine learning research.

**Abstract:** Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.

</details>


### [30] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)

*Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor*

**Main category:** cs.CL

**Keywords:** large language models, wildfire hazards, decision support, retrieval-augmented generation, multi-agent systems

**Relevance Score:** 8

**TL;DR:** The paper introduces WildfireGPT, a retrieval-augmented generation (RAG)-based multi-agent LLM system designed for decision-making in wildfire hazard analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of generalized LLMs in providing specialized, context-specific information for urgent societal challenges such as natural hazards and extreme weather events.

**Method:** The proposed system integrates a multi-agent design with RAG, combining natural hazard data, observational datasets, and scientific literature to deliver tailored risk insights.

**Key Contributions:** Introduced a specialized multi-agent LLM system for wildfire hazards., Demonstrated superior performance in decision support through expert evaluations., Utilized a RAG framework for contextually relevant information delivery.

**Result:** WildfireGPT shows significant performance improvement over existing LLM-based solutions in decision support, based on evaluations from ten expert-led case studies.

**Limitations:** 

**Future Work:** 

**Conclusion:** The multi-agent, context-aware LLM approach can enhance decision-making processes during extreme events like wildfires by offering relevant and accurate insights.

**Abstract:** Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.

</details>


### [31] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)

*Kaidong Feng, Zhu Sun, Jie Yang, Hui Fang, Xinghua Qu, Wenyuan Liu*

**Main category:** cs.CL

**Keywords:** knowledge distillation, large language models, bundle generation, efficiency, performance

**Relevance Score:** 7

**TL;DR:** This study explores knowledge distillation approaches for efficient bundle generation using LLMs, focusing on how knowledge format, quantity, and utilization affect performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address high computational costs during fine-tuning and inference of large-scale LLMs in bundle generation.

**Method:** The study proposes a KD framework that extracts knowledge progressively, captures varying quantities of distilled knowledge, and employs LLM adaptation techniques for efficiency.

**Key Contributions:** Proposes a comprehensive KD framework for bundle generation., Investigates the impact of knowledge format and quantity on performance., Explores different strategies for utilizing distilled knowledge in efficient models.

**Result:** Extensive experiments reveal the significant impact of knowledge format, quantity, and utilization methods on the performance of LLM-based bundle generation.

**Limitations:** The research primarily focuses on efficiency without delving into broader applications or implications of the findings.

**Future Work:** Future research could explore further applications of KD in diverse LLM contexts and investigate alternative methods for performance optimization.

**Conclusion:** Knowledge distillation can notably enhance efficiency in LLM-based bundle generation while maintaining performance levels.

**Abstract:** LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.

</details>


### [32] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)

*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Cognitive Restructuring, Human-LLM Interaction, Conversational LLMs, Mental Health, Dialogue Systems

**Relevance Score:** 9

**TL;DR:** CRDial is a novel framework for Cognitive Restructuring (CR) that utilizes interactive dialogues and a conversational LLM to effectively restructure negative thoughts in individuals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing CR methods that employ simple text rewriting or fixed patterns, which do not align with effective psychotherapeutic processes.

**Method:** CRDial implements multi-turn dialogues with dedicated stages for identifying and restructuring negative thoughts, incorporating supportive conversation strategies and a multi-channel loop for iterative CR.

**Key Contributions:** Introduction of the CRDial framework for interactive psychotherapy in CR., Development of the Crisp dataset for training CR-focused conversational LLMs., Demonstration of the Crispers LLMs' effectiveness through extensive human evaluations.

**Result:** Extensive human studies indicate that Crispers, the conversational LLMs trained on the Crisp dataset, demonstrate superiority in evaluations for CR effectiveness.

**Limitations:** The study may require more diverse datasets and long-term effectiveness evaluations.

**Future Work:** Future research directions include refining the CRDial framework and exploring its applications across various mental health interventions.

**Conclusion:** CRDial provides a more aligned and effective approach to Cognitive Restructuring through interactive dialogues and advanced LLM support.

**Abstract:** Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.

</details>


### [33] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)

*Ocheme Anthony Ekle, Biswarup Das*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, Transfer Learning, Low-Resource Language, English-Igbo Translation, RNN

**Relevance Score:** 4

**TL;DR:** The study develops Neural Machine Translation and transfer learning models for English-to-Igbo translation, achieving competitive accuracy for a low-resource language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve translation accuracy for English-Igbo, a low-resource language, using state-of-the-art neural network techniques.

**Method:** Utilized RNN architectures, specifically LSTM and GRU, enhanced with attention mechanisms, combined with transfer learning via MarianNMT pre-trained models.

**Key Contributions:** Development of NMT and transfer learning models for low-resource language, Curated dataset from various sources verified by native speakers, Performance benchmarking against existing English-Igbo models

**Result:** Achieved a performance gain of +4.83 BLEU points with an estimated translation accuracy of 70%.

**Limitations:** 

**Future Work:** 

**Conclusion:** Combining RNNs with transfer learning effectively addresses performance gaps in low-resource language translation tasks.

**Abstract:** In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.

</details>


### [34] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)

*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Cognitive Restructuring, Human-LLM Interaction, Psychotherapy, Dialogue Systems, Mental Health

**Relevance Score:** 9

**TL;DR:** CRDial is a novel framework for Cognitive Restructuring (CR) using multi-turn dialogues to improve therapeutic outcomes through advanced LLM techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The shortage of clinicians and the stigma surrounding mental health make it essential to develop effective human-LLM interactive psychotherapy frameworks for Cognitive Restructuring (CR).

**Method:** CRDial implements multi-turn dialogues for CR, featuring structured identification and restructuring stages of negative thoughts, along with innovative conversation strategies and a multi-channel loop mechanism for iterative processes.

**Key Contributions:** Introduction of a multi-turn dialogue framework for Cognitive Restructuring., Development of Crispers, trained on the Crisp dataset, enhancing CR engagement., Empirical validation showing the effectiveness of CRDial compared to traditional methods.

**Result:** Human studies show that Crispers, the conversational LLMs developed from the Crisp dataset, outperform existing CR methods in pointwise, pairwise, and intervention assessments.

**Limitations:** The study may be limited by the generalizability of findings across diverse populations and settings.

**Future Work:** Further research is needed to explore the scalability of CRDial in various therapeutic contexts and its integration with other mental health interventions.

**Conclusion:** CRDial represents a significant advancement in the application of LLMs for psychotherapy, offering a more aligned and effective framework for Cognitive Restructuring.

**Abstract:** Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.

</details>


### [35] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)

*Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu*

**Main category:** cs.CL

**Keywords:** Unsupervised Domain Adaptation, Legal Judgment Prediction, contrastive learning, knowledge transfer, large language models

**Relevance Score:** 6

**TL;DR:** JurisCTC is a novel model aimed at enhancing Legal Judgment Prediction accuracy through Unsupervised Domain Adaptation and contrastive learning, effectively transferring knowledge across civil and criminal law domains.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address Limited scalability and accuracy in Legal Judgment Prediction (LJP) due to lengthy legal texts and a scarcity of annotated datasets in various legal domains.

**Method:** JurisCTC employs Unsupervised Domain Adaptation and contrastive learning to facilitate knowledge transfer between civil and criminal law domains for LJP tasks.

**Key Contributions:** Novel model JurisCTC for improving LJP accuracy, Effective knowledge transfer across legal domains, Utilization of contrastive learning for domain distinction

**Result:** Achieved peak accuracies of 76.59% for civil law and 78.83% for criminal law, surpassing existing models and some large language models.

**Limitations:** 

**Future Work:** Expand JurisCTC's application to additional legal domains and explore further enhancements in model accuracy.

**Conclusion:** JurisCTC successfully enhances the accuracy of legal judgment predictions and provides a framework for knowledge transfer in distinct legal domains.

**Abstract:** In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.

</details>


### [36] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)

*Xiuying Chen, Tairan Wang, Juexiao Zhou, Zirui Song, Xin Gao, Xiangliang Zhang*

**Main category:** cs.CL

**Keywords:** AI fairness, text generation, medical applications, deep learning, bias mitigation

**Relevance Score:** 9

**TL;DR:** This study addresses the fairness problem in AI-based text generation for medical applications, proposing an algorithm to mitigate bias while maintaining overall performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of AI in medical applications raises concerns about bias affecting under-served populations, particularly in text generation which has been less studied compared to image classification.

**Method:** The authors propose an algorithm that selectively optimizes performance for underrepresented groups in medical text generation by balancing word-level and pathology accuracy, ensuring the process is differentiable for training.

**Key Contributions:** Proposed algorithm for optimizing fairness in text generation for medical applications, Demonstrated reduction of bias across multiple demographic groups, Provided publicly available code for further research

**Result:** The proposed algorithm significantly reduces performance disparities among different demographic groups by over 30% while maintaining text generation accuracy within 2%.

**Limitations:** 

**Future Work:** Further exploration of fairness in other areas of AI in healthcare and continuous enhancement of the proposed methods to broaden their applicability.

**Conclusion:** Mitigating bias in deep learning models contributes to the fairness and reliability of text generation in medical diagnostics, with the potential to enhance trust in AI applications in healthcare.

**Abstract:** Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe significant performance discrepancies across different races, sexes, and age groups, including intersectional groups, various model scales, and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underperformed groups to reduce bias. The selection rules take into account not only word-level accuracy but also the pathology accuracy to the target reference, while ensuring that the entire process remains fully differentiable for effective model training. Our evaluations across multiple backbones, datasets, and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance. Specifically, the disparities among various groups across different metrics were diminished by more than 30% with our algorithm, while the relative change in text generation accuracy was typically within 2%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of text generation diagnosis in medical domain.   Our code is publicly available to facilitate further research at https://github.com/iriscxy/GenFair.

</details>


### [37] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)

*Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, Xuming Hu*

**Main category:** cs.CL

**Keywords:** watermarking, large language models, sentence selection, cohesion, text quality

**Relevance Score:** 8

**TL;DR:** CoheMark is a novel sentence-level watermarking technique that enhances watermark strength while preserving text quality by leveraging cohesive relationships between sentences.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing sentence-level watermarking techniques that compromise text quality due to arbitrary segmentation or generation processes.

**Method:** CoheMark uses trained fuzzy c-means clustering for sentence selection and applies specific criteria for next sentence selection to ensure logical fluency.

**Key Contributions:** Introduction of CoheMark, an advanced sentence-level watermarking technique., Use of fuzzy c-means clustering for improved sentence selection., Demonstration of strong watermark detection while maintaining text quality.

**Result:** Experimental evaluations show that CoheMark achieves strong watermark strength with minimal impact on the quality of generated text.

**Limitations:** The limitations of the proposed approach were not extensively discussed in the paper.

**Future Work:** Future research directions could explore further optimizing the balance between watermark strength and text quality, as well as applying CoheMark to various language generation tasks.

**Conclusion:** CoheMark effectively balances robustness in watermark detection and high-quality text generation, making it a promising technique for watermarking in language models.

**Abstract:** Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.

</details>


### [38] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)

*Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, Jey Han Lau*

**Main category:** cs.CL

**Keywords:** robustness evaluation, linguistic variations, large language models, NLP tasks, model performance

**Relevance Score:** 6

**TL;DR:** FLUKE is a framework for evaluating model robustness through systematic linguistic variations, revealing task-dependent impacts of such variations on model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess robustness of models through minimal variations in test data, highlighting the importance of systematic testing for understanding model behaviors.

**Method:** FLUKE introduces controlled linguistic variations at different levels (orthography, dialect, style) and uses LLMs with human validation to generate data modifications.

**Key Contributions:** Introduction of FLUKE for robustness evaluation, Task dependency of linguistic variations on model performance, Insight into LLMs' brittleness to linguistic modifications

**Result:** The study shows that linguistic variations impact model robustness differently across tasks, with LLMs generally more robust yet still vulnerable to certain variations, especially negation.

**Limitations:** Focuses primarily on linguistic variations and may not cover all dimensions of model robustness.

**Future Work:** Further exploration of robustness across more varied linguistic contexts and tasks.

**Conclusion:** Systematic robustness testing is crucial for revealing how models respond to different linguistic changes and understanding their limitations.

**Abstract:** We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.

</details>


### [39] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)

*Zihan Wang, Lu Yuan, Zhengxuan Zhang, Qing Zhao*

**Main category:** cs.CL

**Keywords:** misinformation detection, human empathy, Large Language Models, cognitive empathy, emotional empathy

**Relevance Score:** 8

**TL;DR:** The paper presents a Dual-Aspect Empathy Framework (DAE) that integrates cognitive and emotional empathy to improve misinformation detection in social media.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional misinformation detection methods that overlook human empathy.

**Method:** The DAE analyzes misinformation by examining creators' cognitive strategies and emotional appeals, simulating readers' responses using Large Language Models (LLMs), and introducing an empathy-aware filtering mechanism.

**Key Contributions:** Introduction of the Dual-Aspect Empathy Framework (DAE) for misinformation detection., Integration of cognitive and emotional empathy in the detection process., Development of an empathy-aware filtering mechanism.

**Result:** DAE outperforms existing methods on benchmark datasets, demonstrating its effectiveness in multimodal misinformation detection.

**Limitations:** The applicability of the framework may vary across different social media platforms and types of misinformation.

**Future Work:** Exploring the framework's adaptability to various forms of misinformation and integrating additional empathy dimensions.

**Conclusion:** DAE provides a comprehensive and human-centric approach to misinformation detection, enhancing authenticity and diversity in responses.

**Abstract:** In the digital era, social media has become a major conduit for information dissemination, yet it also facilitates the rapid spread of misinformation. Traditional misinformation detection methods primarily focus on surface-level features, overlooking the crucial roles of human empathy in the propagation process. To address this gap, we propose the Dual-Aspect Empathy Framework (DAE), which integrates cognitive and emotional empathy to analyze misinformation from both the creator and reader perspectives. By examining creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more comprehensive and human-centric approach to misinformation detection. Moreover, we further introduce an empathy-aware filtering mechanism to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate that DAE outperforms existing methods, providing a novel paradigm for multimodal misinformation detection.

</details>


### [40] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)

*Chengguang Gan, Sunbowen Lee, Zhixi Cai, Yanbin Wei, Lei Zheng, Yunhao Liang, Shiwen Ni, Tatsunori Mori*

**Main category:** cs.CL

**Keywords:** Mutual Reinforcement Effect, Multimodal Information Extraction, Large Vision-Language Models

**Relevance Score:** 6

**TL;DR:** This paper introduces the Multimodal Mutual Reinforcement Effect (M-MRE), extending the Mutual Reinforcement Effect to multimodal information extraction, and proposes a Prompt Format Adapter for large vision-language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the Mutual Reinforcement Effect (MRE) in the multimodal domain, where textual and visual tasks can enhance each other’s performance through joint modeling.

**Method:** The authors propose the M-MRE task, develop a corresponding dataset, and create a Prompt Format Adapter compatible with various Large Vision-Language Models (LVLMs).

**Key Contributions:** Introduction of Multimodal Mutual Reinforcement Effect (M-MRE), Development of a specific dataset for M-MRE, Proposal of a Prompt Format Adapter for LVLMs

**Result:** Experimental results indicate that MRE can be applied to the M-MRE task, enhancing performance in a multimodal text-image understanding scenario.

**Limitations:** The applicability of M-MRE beyond the tested models and tasks is yet to be fully validated.

**Future Work:** Further exploration of M-MRE in additional multimodal tasks and potential real-world applications.

**Conclusion:** The findings support the applicability of MRE beyond textual domains, showcasing mutual gains across interrelated tasks in multimodal contexts.

**Abstract:** Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.

</details>


### [41] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)

*Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare, data privacy, model merging, MIMIC-IV

**Relevance Score:** 9

**TL;DR:** PatientDx is a framework for creating large language models for health-predictive tasks without the need for fine-tuning on sensitive patient data, addressing data privacy concerns while improving model performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models in health-predictive tasks without the risk associated with training on sensitive patient data.

**Method:** The framework utilizes merging techniques to optimize model performance, employing a pivotal model adapted to numerical reasoning and tuning hyperparameters on examples based on performance metrics without direct training on patient data.

**Key Contributions:** Framework for model merging in healthcare tasks, Improvement of AUROC by up to 7% on MIMIC-IV dataset, Less susceptibility to data leakage compared to fine-tuning methods

**Result:** Experiments on the MIMIC-IV dataset demonstrate up to a 7% improvement in AUROC compared to initial models while reducing risks of data leakage compared to fine-tuned models.

**Limitations:** The performance might still depend on the selection of pivotal models and hyperparameters without explicit training on patient data.

**Future Work:** Exploration of further optimizations in model merging techniques and applications to other sensitive domains.

**Conclusion:** PatientDx provides a viable alternative for enhancing model performance in sensitive healthcare applications while preserving privacy.

**Abstract:** Fine-tuning of Large Language Models (LLMs) has become the default practice for improving model performance on a given task. However, performance improvement comes at the cost of training on vast amounts of annotated data which could be sensitive leading to significant data privacy concerns. In particular, the healthcare domain is one of the most sensitive domains exposed to data privacy issues. In this paper, we present PatientDx, a framework of model merging that allows the design of effective LLMs for health-predictive tasks without requiring fine-tuning nor adaptation on patient data. Our proposal is based on recently proposed techniques known as merging of LLMs and aims to optimize a building block merging strategy. PatientDx uses a pivotal model adapted to numerical reasoning and tunes hyperparameters on examples based on a performance metric but without training of the LLM on these data. Experiments using the mortality tasks of the MIMIC-IV dataset show improvements up to 7% in terms of AUROC when compared to initial models. Additionally, we confirm that when compared to fine-tuned models, our proposal is less prone to data leak problems without hurting performance. Finally, we qualitatively show the capabilities of our proposal through a case study. Our best model is publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [42] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)

*Yongxuan Wu, Runyu Chen, Peiyu Liu, Hongjin Qian*

**Main category:** cs.CL

**Keywords:** long-context understanding, natural language processing, large language models, redundancy in dialogues, spoken language dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces a new spoken long-text dataset to assess long-context understanding in natural language processing, particularly in dialogue settings, and evaluates current large language models (LLMs) on these tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks that do not capture the complexities of speech-based dialogues and long-context understanding.

**Method:** The authors construct a dataset from live streams that captures redundancy-rich and conversational text and create tasks categorized as retrieval-dependent, reasoning-dependent, and hybrid. They evaluate popular LLMs and specialized methods on these tasks.

**Key Contributions:** Introduction of the first spoken long-text dataset designed for long-context understanding., Evaluation of LLMs and methods on long-context tasks reflecting real-world dialogue complexity., Development of a new baseline that better handles redundancy in spoken text.

**Result:** Current methods show task-specific preferences and generally perform poorly on redundant inputs. A new baseline is developed that shows better performance across tasks.

**Limitations:** Current methods are shown to have strong preferences for specific tasks and struggle with redundancy; no single method dominates across all tasks.

**Future Work:** Future research should focus on enhancing understanding of long-contexts and developing methods to better handle redundancy in spoken text.

**Conclusion:** The study identifies key limitations in existing methods and suggests future research directions for improving long-context understanding, particularly in spoken language processing. The benchmark created is suitable for evaluating real-world e-commerce systems.

**Abstract:** Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at https://github.com/Yarayx/livelongbench.

</details>


### [43] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)

*Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** Task-Oriented Dialogue, Personalization, User Images, Natural Language Generation, Pictor

**Relevance Score:** 8

**TL;DR:** PicPersona-TOD introduces a dataset that uses user images to generate personalized responses in Task-Oriented Dialogue systems, improving user engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Task-Oriented Dialogue systems produce generic responses that lack personalization.

**Method:** The study presents PicPersona-TOD, a dataset that integrates user images for contextual personalization and introduces the NLG model Pictor that leverages this dataset to enhance response quality.

**Key Contributions:** Introduction of the PicPersona-TOD dataset for personalized dialogue based on user images., Development of the Pictor NLG model that enhances dialogue by adapting responses to user attributes., Demonstration of improved engagement through human evaluations.

**Result:** Human evaluations indicate that personalized responses significantly improve user interactions and engagement in dialogue systems.

**Limitations:** The dataset may require diverse user images to truly capture a wide range of personal attributes and emotional contexts.

**Future Work:** Exploration of further enhancements in personalization techniques and extending the model’s performance in new domains.

**Conclusion:** The introduction of user images in TOD systems facilitates personalized interaction, leading to better user experiences.

**Abstract:** Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users' personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains https://github.com/JihyunLee1/PicPersona.

</details>


### [44] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)

*Anna Lieb, Maneesh Arora, Eni Mustafaraj*

**Main category:** cs.CL

**Keywords:** topic modeling, LLM augmentation, political science, social science, unstructured text analysis

**Relevance Score:** 6

**TL;DR:** This paper explores how LLM-generated text augmentation can enhance topic modeling in social science, particularly in political science, by improving interpretability and applicability to domain-specific research questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of topic models in interpretability and practicality for domain-specific research, particularly in social sciences such as political science.

**Method:** Investigates the use of GPT-4 generated text augmentation to improve the output of topic modeling, using a case study in political science to evaluate effectiveness.

**Key Contributions:** Demonstrated the effectiveness of LLM-generated text augmentation for topic modeling, Provided a case study within political science illustrating practical applications, Highlighted improvements in interpretability and usability of topic model outputs

**Result:** Findings indicate that LLM-enhanced topic modeling produces highly interpretable categories that facilitate investigation of specific research questions with minimal human input.

**Limitations:** Focuses primarily on political science; results may vary in other domains.

**Future Work:** Encourages exploration of LLM augmentations in other social science fields and the generalizability of findings.

**Conclusion:** The study suggests that LLM-generated augmentations significantly improve the usability and interpretability of topic modeling for targeted social science inquiries.

**Abstract:** Unsupervised machine learning techniques, such as topic modeling and clustering, are often used to identify latent patterns in unstructured text data in fields such as political science and sociology. These methods overcome common concerns about reproducibility and costliness involved in the labor-intensive process of human qualitative analysis. However, two major limitations of topic models are their interpretability and their practicality for answering targeted, domain-specific social science research questions. In this work, we investigate opportunities for using LLM-generated text augmentation to improve the usefulness of topic modeling output. We use a political science case study to evaluate our results in a domain-specific application, and find that topic modeling using GPT-4 augmentations creates highly interpretable categories that can be used to investigate domain-specific research questions with minimal human guidance.

</details>


### [45] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)

*Xin Yi, Shunfan Zhengc, Linlin Wanga, Xiaoling Wang, Liang He*

**Main category:** cs.CL

**Keywords:** watermarking, large language models, knowledge distillation, contrastive decoding, security

**Relevance Score:** 8

**TL;DR:** This paper proposes a unified framework called Contrastive Decoding-Guided Knowledge Distillation (CDG-KD) that enables bidirectional attacks on watermarks in large language models, addressing vulnerabilities related to unauthorized knowledge distillation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the problem of watermark robustness and unforgeability in large language models, particularly in the context of unauthorized knowledge distillation and watermark attacks.

**Method:** The authors propose a framework that utilizes contrastive decoding to extract and manipulate watermarks between teacher and student models, enabling both the removal and forgery of watermarks.

**Key Contributions:** Introduction of Contrastive Decoding-Guided Knowledge Distillation (CDG-KD) framework, Demonstration of effective watermark removal and forgery, Highlighting the need for robust watermarking schemes in LLMs

**Result:** CDG-KD demonstrates effective watermark attacks while maintaining the distilled model's overall performance, showcasing its ability to identify watermark traces and perform attacks under unauthorized conditions.

**Limitations:** The paper does not address the implications of these attacks on real-world applications or the deployment of watermarking systems in practical scenarios.

**Future Work:** Future research should explore improving watermark robustness and evaluating the effectiveness of watermarking schemes in real-world applications.

**Conclusion:** The paper emphasizes the critical need for watermarking systems to be robust and unforgeable, particularly given the vulnerabilities exposed by their findings.

**Abstract:** Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.

</details>


### [46] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)

*Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung*

**Main category:** cs.CL

**Keywords:** large language models, hallucination, benchmark, evaluation tasks, taxonomy

**Relevance Score:** 9

**TL;DR:** This paper addresses the issue of hallucinations in large language models (LLMs) by introducing a new benchmark that establishes a clear taxonomy and includes new extrinsic evaluation tasks.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle hallucinations in LLMs that undermine trust and hinder adoption of generative AI systems.

**Method:** The paper proposes a comprehensive benchmark built on a clear taxonomy of hallucination, incorporating both new extrinsic and existing intrinsic evaluation tasks, with dynamic test set generation to mitigate data leakage.

**Key Contributions:** Establishment of a clear taxonomy of hallucinations., Introduction of new extrinsic hallucination tasks., Comprehensive analysis of existing benchmarks and their limitations.

**Result:** A new benchmark for evaluating LLM hallucinations has been set up, providing clearer categorizations and tasks to enhance consistency in research.

**Limitations:** The benchmark may initially lack widespread acceptance within the community and could need further validation through practical application.

**Future Work:** Future research should focus on enhancing the benchmark's categories and exploring its application across different LLM architectures.

**Conclusion:** The establishment of a comprehensive hallucination benchmark promotes research consistency and improves LLM evaluation frameworks.

**Abstract:** Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.

</details>


### [47] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)

*Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki*

**Main category:** cs.CL

**Keywords:** metadata, latent semantics, language models, probabilistic context-free grammars, downstream tasks

**Relevance Score:** 7

**TL;DR:** The study investigates the impact of prepending metadata in pre-training language models, revealing mixed effects on downstream task performance based on context length and information availability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze how prepending metadata influences language model performance in acquiring latent semantics for improved downstream task results.

**Method:** The researchers conduct experiments using artificial data generated by probabilistic context-free grammars to evaluate the effects of metadata during pre-training.

**Key Contributions:** Highlighted the dual effects of metadata prepending on model performance., Demonstrated the importance of context length in latent semantic inference., Provided experimental evidence using artificial datasets to support findings.

**Result:** The study finds that prepending metadata can enhance model performance when the downstream task allows for inference of latent semantics from long enough contexts, while it can hinder performance when context information is insufficient.

**Limitations:** The findings are based on artificial data and may not generalize to all real-world scenarios of language model application.

**Future Work:** Further exploration of context-sensitive approaches to enhance language model training and other types of metadata usage.

**Conclusion:** The effectiveness of metadata prepending in language model training is context-dependent, leading to both improvements and declines in downstream task performance.

**Abstract:** The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.

</details>


### [48] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)

*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

**Main category:** cs.CL

**Keywords:** large language models, reasoning dataset, training methodology

**Relevance Score:** 8

**TL;DR:** The paper presents a large-scale, difficulty-graded reasoning dataset and proposes a training methodology to enhance reasoning capabilities of large language models, achieving significant performance improvements on reasoning benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of base model training processes and to enhance the reasoning capabilities of large language models due to a lack of in-depth knowledge in the academic community.

**Method:** Constructing a dataset with approximately 3.34 million queries and 40 million distilled responses, using pass rates and Coefficient of Variation for data selection, and applying higher learning rates during training for reasoning-focused models.

**Key Contributions:** Large-scale reasoning dataset of 3.34 million queries, Novel training methodology utilizing difficulty grading for enhanced reasoning, Public release of datasets and methods for open-source collaboration

**Result:** Achieved a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark, surpassing most current distilled models.

**Limitations:** The study focuses on reasoning tasks and might not directly apply to other NLP tasks or lower-complexity queries.

**Future Work:** Encouraging further exploration into other areas of reasoning and the application of the proposed dataset and methodology to diverse training contexts.

**Conclusion:** The findings indicate that higher learning rates are crucial for effective reasoning-focused training, and the detailed methodology and datasets are publicly available to support further research.

**Abstract:** Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [49] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)

*Zhenkai Qin, Guifang Yang, Dongze Wu*

**Main category:** cs.CL

**Keywords:** rumor detection, natural language processing, deep learning

**Relevance Score:** 5

**TL;DR:** This paper introduces RAGAT-Mind, a multi-granular approach for detecting rumors in Chinese on social media, achieving high accuracy through a combination of advanced deep learning techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of false information on social media, effective rumor detection has become a critical challenge in natural language processing.

**Method:** RAGAT-Mind utilizes TextCNN for local semantic extraction, bidirectional GRU for context learning, Multi-Head Self-Attention for focusing on global dependencies, and BiGCN for representing word co-occurrence graphs.

**Key Contributions:** Introduction of RAGAT-Mind for rumor detection., Combination of various deep learning techniques for improved performance., Demonstrated high accuracy and generalization capabilities.

**Result:** The model achieved 99.2% accuracy and a macro-F1 score of 0.9919 on the Weibo1-Rumor dataset, outperforming existing methods.

**Limitations:** 

**Future Work:** 

**Conclusion:** The results indicate that combining hierarchical linguistic features with graph-based structures effectively enhances rumor detection capabilities.

**Abstract:** As false information continues to proliferate across social media platforms, effective rumor detection has emerged as a pressing challenge in natural language processing. This paper proposes RAGAT-Mind, a multi-granular modeling approach for Chinese rumor detection, built upon the MindSpore deep learning framework. The model integrates TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of word co-occurrence graphs. Experiments on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior classification performance, attaining 99.2% accuracy and a macro-F1 score of 0.9919. The results validate the effectiveness of combining hierarchical linguistic features with graph-based semantic structures. Furthermore, the model exhibits strong generalization and interpretability, highlighting its practical value for real-world rumor detection applications.

</details>


### [50] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)

*Samaneh Hosseini Moghaddam, Kelly Lyons, Cheryl Regehr, Vivek Goel, Kaitlyn Regehr*

**Main category:** cs.CL

**Keywords:** abusive language, taxonomy, online communications, classification, moderation

**Relevance Score:** 8

**TL;DR:** This paper develops a comprehensive taxonomy for identifying and classifying abusive language in online communications, aiming to enhance detection and mitigation efforts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant risks posed by abusive language in online communications and facilitate better monitoring and intervention.

**Method:** The paper employs a systematic method for taxonomy development, integrating classification systems from 18 existing multi-label datasets to create a hierarchical and faceted taxonomy.

**Key Contributions:** Development of a hierarchical and faceted taxonomy for online abusive language., Integration of multiple existing datasets into a cohesive classification system., Facilitation of better monitoring and response strategies for online abuse.

**Result:** The resulting taxonomy consists of 5 categories and 17 dimensions that classify various aspects of online abuse, including context, target, intensity, directness, and theme.

**Limitations:** 

**Future Work:** Exploration of the application of the taxonomy in real-world moderation systems and its effectiveness in various online platforms.

**Conclusion:** A shared understanding of abusive language characteristics can improve detection methods and encourage collaboration among stakeholders in online abuse mitigation.

**Abstract:** The proliferation of abusive language in online communications has posed significant risks to the health and wellbeing of individuals and communities. The growing concern regarding online abuse and its consequences necessitates methods for identifying and mitigating harmful content and facilitating continuous monitoring, moderation, and early intervention. This paper presents a taxonomy for distinguishing key characteristics of abusive language within online text. Our approach uses a systematic method for taxonomy development, integrating classification systems of 18 existing multi-label datasets to capture key characteristics relevant to online abusive language classification. The resulting taxonomy is hierarchical and faceted, comprising 5 categories and 17 dimensions. It classifies various facets of online abuse, including context, target, intensity, directness, and theme of abuse. This shared understanding can lead to more cohesive efforts, facilitate knowledge exchange, and accelerate progress in the field of online abuse detection and mitigation among researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [51] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)

*Zena Al-Khalili, Nick Howell, Dietrich Klakow*

**Main category:** cs.CL

**Keywords:** LLMs, mathematical reasoning, code generation, evaluation methods, grounding

**Relevance Score:** 8

**TL;DR:** This paper investigates the quality of programs generated by code-assisted LLMs for mathematical reasoning tasks, emphasizing the need for more comprehensive evaluation criteria.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of current evaluations of code-assisted LLMs that focus solely on execution correctness without assessing the grounding of generated programs to math rules.

**Method:** An in-depth analysis of five different LLMs' generated programs was conducted using two math datasets, assessed both manually and automatically for their use of mathematical grounding.

**Key Contributions:** Rigorous evaluation framework for code-assisted LLMs, Insights into the grounding of mathematical rules in LLMs' programs, Comparative analysis between closed-source and open-source LLMs regarding math problem solving

**Result:** Results indicate that the grounding distribution varies by LLM capabilities and problem difficulty, showing that closed-source models perform better in applying math rules than open-source models.

**Limitations:** The analysis is limited to two specific math datasets and five LLMs, which may not be representative of all LLMs and problem types.

**Future Work:** Future research should explore broader datasets and different types of reasoning tasks to enhance the understanding of LLM capabilities.

**Conclusion:** The study underscores the importance of evaluating LLM performance in math reasoning beyond just execution accuracy for a deeper understanding of their capabilities and limitations.

**Abstract:** Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs. In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs' generated programs in response to math reasoning tasks. Our evaluation focuses on the extent to which LLMs ground their programs to math rules, and how that affects their end performance. For this purpose, we assess the generations of five different LLMs, on two different math datasets, both manually and automatically. Our results reveal that the distribution of grounding depends on LLMs' capabilities and the difficulty of math problems. Furthermore, mathematical grounding is more effective for closed-source models, while open-source models fail to employ math rules in their solutions correctly. On MATH500, the percentage of grounded programs decreased to half, while the ungrounded generations doubled in comparison to ASDiv grade-school problems. Our work highlights the need for in-depth evaluation beyond execution accuracy metrics, toward a better understanding of code-assisted LLMs' capabilities and limits in the math domain.

</details>


### [52] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)

*Yuanchang Ye, Weiyan Wen*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, Visual Question Answering, Split Conformal Prediction

**Relevance Score:** 8

**TL;DR:** This study introduces a Split Conformal Prediction (SCP) framework to mitigate hallucinations in Large Vision-Language Models (LVLMs) used for Visual Question Answering (VQA).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to reduce hallucination in LVLMs which poses risks in safety-critical applications.

**Method:** The SCP framework employs a model-agnostic uncertainty quantification approach that utilizes dynamic threshold calibration and cross-modal consistency verification.

**Key Contributions:** Proposes a model-agnostic uncertainty quantification method for LVLMs., Implements dynamic threshold calibration for better prediction set sizes., Shows extensive evaluation on benchmarks confirming the robustness of the SCP framework.

**Result:** The SCP method controls marginal coverage, dynamically adjusts prediction set sizes, and does not require prior distribution assumptions, showing robust performance across different LVLM benchmarks.

**Limitations:** 

**Future Work:** 

**Conclusion:** The study provides a scalable solution for hallucination detection and improves uncertainty-aware decision-making in multi-modal AI systems.

**Abstract:** This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.

</details>


### [53] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)

*Jared Fernandez, Clara Na, Vashisth Tiwari, Yonatan Bisk, Sasha Luccioni, Emma Strubell*

**Main category:** cs.CL

**Keywords:** energy consumption, large language models, inference optimization, natural language processing, artificial intelligence

**Relevance Score:** 8

**TL;DR:** This paper analyzes the energy costs associated with large language models (LLMs) during inference across various NLP and AI workloads, highlighting the sensitivity of energy consumption to optimization strategies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising computational and environmental costs of large language models (LLMs) associated with their adoption and scaling, and provide insights for sustainable LLM deployment.

**Method:** The paper employs a modeling approach using a binning strategy for input-output token distributions and batch size variations to approximate real-world LLM workflows and conducts an empirical analysis across multiple factors including software frameworks, decoding strategies, and GPU architectures.

**Key Contributions:** Systematic analysis of energy implications of inference optimizations for LLMs., Novel modeling approach to represent real-world LLM workflows., Empirical findings on the effectiveness of optimizations across diverse NLP and AI tasks.

**Result:** The study finds that naive energy estimates significantly underestimate real-world energy consumption, and that optimization strategies can reduce total energy use by up to 73% from unoptimized baselines.

**Limitations:** The study primarily focuses on existing inference optimization strategies without proposing new algorithms or techniques for optimization.

**Future Work:** Future research may involve exploring advanced optimization algorithms and their practical implications on energy consumption in real-world scenarios.

**Conclusion:** The findings offer a foundation for energy-efficient design strategies in AI infrastructure and underscore the importance of considering workload geometry when optimizing inference efficiency.

**Abstract:** As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.

</details>


### [54] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)

*Haru-Tada Sato, Fuka Matsuzaki, Jun-ichiro Takahashi*

**Main category:** cs.CL

**Keywords:** Ensemble Bayesian Inference, Small Language Models, Performance Evaluation

**Relevance Score:** 8

**TL;DR:** This study proposes Ensemble Bayesian Inference (EBI), a method using multiple small language models to achieve accuracy similar to that of large language models, showcasing its effectiveness across various tasks and languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of small language model ensembles for achieving accuracy comparable to proprietary large language models, particularly using limited computational resources.

**Method:** The study introduces Ensemble Bayesian Inference (EBI), which applies Bayesian estimation to combine outputs from multiple small language models, allowing them to outperform individual models.

**Key Contributions:** Introduction of Ensemble Bayesian Inference (EBI) for SLMs., Demonstration of effective performance improvements by using ensembles, including models with negative Lift values., Analysis of the approach's efficacy across different languages.

**Result:** Experiments show that EBI effectively enhances performance on tasks like aptitude assessments and consumer analysis in both Japanese and English, even incorporating models with negative Lift values.

**Limitations:** The study focuses on specific tasks and may not generalize across all potential applications of SLM ensembles.

**Future Work:** Further research could explore more types of tasks and improve the integration of diverse language models.

**Conclusion:** The findings indicate that ensemble methods using SLMs can lead to high-performance AI systems while effectively utilizing models that might otherwise underperform independently.

**Abstract:** This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.

</details>


### [55] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)

*Cheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, Junfeng Fang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, safety risks, taxonomy, defense strategies, security

**Relevance Score:** 6

**TL;DR:** This paper surveys the safety risks, attacks, and defense strategies related to Large Reasoning Models (LRMs), organizing them into a structured taxonomy to improve model security and reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the emerging safety risks and vulnerabilities of Large Reasoning Models as their reasoning capabilities advance.

**Method:** The paper conducts a comprehensive survey of LRMs and synthesizes information regarding safety risks, attacks, and defense strategies into a detailed taxonomy.

**Key Contributions:** Comprehensive survey of safety risks associated with LRMs, Development of a detailed taxonomy of attacks and defense strategies, Contribution to the understanding of LRM safety landscapes

**Result:** The study provides a clear understanding of the safety landscape of LRMs, highlighting critical risks and defense mechanisms.

**Limitations:** The paper may not cover all possible future risks as the field evolves.

**Future Work:** Encourages ongoing research into the security and reliability of Large Reasoning Models and their applications.

**Conclusion:** A structured taxonomy is presented to aid future research focused on enhancing LRM security and reliability.

**Abstract:** Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.

</details>


### [56] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)

*Vansh Gupta, Sankalan Pal Chowdhury, Vilém Zouhar, Donya Rooein, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** large language models, educational applications, non-English languages, machine learning, HCI

**Relevance Score:** 7

**TL;DR:** This work evaluates the performance of popular large language models (LLMs) on educational tasks in multiple languages and finds significant drops in performance for lower-resource languages compared to English.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the applicability and performance of large language models in educational contexts for non-English languages, in light of their increasing adoption.

**Method:** The study evaluated LLMs against four educational tasks: identifying misconceptions, providing feedback, interactive tutoring, and grading translations across six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) as well as English.

**Key Contributions:** Performance evaluation of LLMs on varied educational tasks in multiple languages., Identification of performance discrepancies based on language resource availability., Recommendations for practitioners in educational settings regarding LLM deployment.

**Result:** The performance on non-English tasks somewhat corresponds to the amount of training data available for each language, with significant drops in performance observed in lower-resource languages.

**Limitations:** The analysis is limited to only six non-English languages and popular LLMs; further research is needed to generalize findings across more languages and models.

**Future Work:** Future research should explore performance improvements in low-resource languages and adapt LLMs specifically for educational tasks in those languages.

**Conclusion:** Practitioners should verify the effectiveness of LLMs in target languages prior to deployment in educational settings.

**Abstract:** Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.

</details>


### [57] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)

*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula Allen-Meares, Eulalia Puig Abril, Olga Garcia, Carolyn Dickens, Andrew Boyd*

**Main category:** cs.CL

**Keywords:** Conversational Assistants, Neurosymbolic Architecture, Large Language Models, Heart Failure, User Study

**Relevance Score:** 9

**TL;DR:** The paper evaluates two conversational assistants for heart failure patients—one using a neurosymbolic architecture and the other based on ChatGPT.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of conversational assistants in healthcare driven by advancements in Large Language Models, there is a need to assess the effectiveness of these systems through controlled evaluations with users.

**Method:** A within-group user study compared two versions of a conversational assistant for heart failure patients, evaluating performance metrics and user experience.

**Key Contributions:** Comparison of neurosymbolic and generative AI conversational assistants in healthcare, User study highlights strengths and weaknesses of both approaches, Insights into patient interaction with conversational technology in health contexts.

**Result:** The neurosymbolic system was found to be more accurate and task-completing than the ChatGPT-based system, while the ChatGPT system made fewer speech errors and required less clarification.

**Limitations:** The study was limited to heart failure patients and focused only on specific tasks related to salt content inquiries, which may not generalize to other health contexts or task types.

**Future Work:** Further research could explore other patient populations and healthcare queries to assess broader applicability of these findings.

**Conclusion:** Patients did not show a preference between the two systems despite differences in accuracy and task completion.

**Abstract:** Conversational assistants are becoming more and more popular, including in healthcare, partly because of the availability and capabilities of Large Language Models. There is a need for controlled, probing evaluations with real stakeholders which can highlight advantages and disadvantages of more traditional architectures and those based on generative AI. We present a within-group user study to compare two versions of a conversational assistant that allows heart failure patients to ask about salt content in food. One version of the system was developed in-house with a neurosymbolic architecture, and one is based on ChatGPT. The evaluation shows that the in-house system is more accurate, completes more tasks and is less verbose than the one based on ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors and requires fewer clarifications to complete the task. Patients show no preference for one over the other.

</details>


### [58] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)

*Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti*

**Main category:** cs.CL

**Keywords:** sparse attention, Transformer LLMs, long-context tasks, machine learning, scaling laws

**Relevance Score:** 8

**TL;DR:** This paper evaluates sparse attention methods in Transformer LLMs for long-context tasks, revealing trade-offs in efficiency and accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the viability and scalability of sparse attention in Transformer LLMs for long-context tasks, which is currently underexplored.

**Method:** A comparative analysis of various training-free sparse attention methods on model scales, sequence lengths, and sparsity levels using long-sequence tasks.

**Key Contributions:** Introduction of novel scaling laws for sparse attention, Comprehensive analysis of efficiency-accuracy trade-offs, Insights on the performance variability of sparse attention across different tasks

**Result:** Larger and sparse models are better for very long sequences; sparsity levels are differentially optimal during decoding vs. pre-filling; no one-size-fits-all strategy exists for all tasks; new scaling laws for sparse attention are introduced.

**Limitations:** Moderate sparsity can lead to performance degradation on some tasks, indicating that sparse attention isn't universally effective.

**Future Work:** Further exploration of adaptivity in sparsification strategies and additional long-sequence tasks.

**Conclusion:** Sparse attention can significantly enhance Transformer LLMs for longer sequences but necessitates careful consideration of performance trade-offs for specific applications.

**Abstract:** Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.

</details>


### [59] [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)

*Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas*

**Main category:** cs.CL

**Keywords:** dialogue summarization, Transformer models, abstractive summarization, Machine Learning, evaluation metrics

**Relevance Score:** 7

**TL;DR:** This paper reviews 1262 papers on Transformer-based abstractive dialogue summarization, outlining key challenges and techniques while assessing datasets and evaluation methods used in the research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive discussions on the challenges of dialogue summarization, prompting a need to unify understanding and align techniques, datasets, and metrics.

**Method:** The study systematically reviews 1262 unique research papers related to dialogue summarization published between 2019 and 2024, utilizing the Semantic Scholar and DBLP databases.

**Key Contributions:** Comprehensive review of 1262 papers on abstractive dialogue summarization, Identification of main challenges and their corresponding techniques, Assessment of datasets and evaluation methods used in existing research.

**Result:** The review identifies main challenges in dialogue summarization, such as language, structure, comprehension, speaker, salience, and factuality, and correlates them with various summarization techniques.

**Limitations:** Few datasets cover all subdomains of dialogue, and existing evaluation methods lack sufficient detail on agreement and guidelines.

**Future Work:** Future research should focus on addressing the identified challenges, especially comprehension and factuality, and improving evaluation methodologies.

**Conclusion:** Key challenges like comprehension, factuality, and salience require further research, while the traditional ROUGE metric and human evaluations have limitations.

**Abstract:** Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.

</details>


### [60] [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)

*Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI-generated Lyrics, Synthetic Text Detection, Unsupervised Domain Adaptation, Music and AI

**Relevance Score:** 8

**TL;DR:** This paper explores the detection of AI-generated lyrics in music, addressing a gap in research by evaluating various detection methods, and emphasizing implications for copyright and content management.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large language models (LLMs) in generating music content, particularly lyrics, has prompted concerns about copyright, consumer satisfaction, and spamming, necessitating research into detection methods for AI-generated lyrics.

**Method:** The authors curated a diverse dataset of real and synthetic lyrics across multiple languages and genres, validated their generation pipeline through human and automated methods, and evaluated synthetic text detection approaches on lyrics using unsupervised domain adaptation.

**Key Contributions:** Curated a novel dataset of real and synthetic lyrics across diverse languages and genres, Evaluated detection methods specifically for lyrics, filling a research gap, Investigated unsupervised domain adaptation for synthetic text detection in multilingual contexts

**Result:** The evaluation reveals promising results for detecting AI-generated lyrics, which could inform policy decisions about AI in music and enhance transparency for users regarding the nature of the content they encounter.

**Limitations:** The study may be limited by the scope of the dataset and the generalizability of the detection methods across all possible genres and languages.

**Future Work:** Future research should explore refining detection methods further and addressing potential biases in AI-generated content.

**Conclusion:** The findings indicate effective strategies for identifying AI-generated lyrics, suggesting that such approaches can adapt across different languages and genres, which is crucial for the evolving landscape of music generation.

**Abstract:** In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.

</details>


### [61] [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)

*Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** autocode, language models, inference efficiency, draft structures, machine learning

**Relevance Score:** 6

**TL;DR:** The paper introduces OPT-Tree, an algorithm that improves the efficiency of autoregressive language models by enabling multiple token generation in a single step through adaptive draft trees.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited inference efficiency of autoregressive language models, which generate one token at a time, and to improve performance as these models grow larger.

**Method:** OPT-Tree constructs adaptive and scalable draft trees that optimize the structure to maximize the expected acceptance length during token verification.

**Key Contributions:** Introduction of OPT-Tree as a novel adaptive draft structure for language models, Demonstrated significant improvement in inference speed and efficiency, Potential for generating multiple tokens in one inference step.

**Result:** OPT-Tree outperforms existing draft structures, achieving speed-up ratios of up to 3.2 compared to traditional autoregressive decoding methods.

**Limitations:** 

**Future Work:** Further exploration of adaptive structures and additional optimizations for varying model sizes and types.

**Conclusion:** The proposed method allows for more efficient token generation in autoregressive models, with potential to generate over ten tokens in one step if conditions are optimal.

**Abstract:** Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.

</details>


### [62] [LaMsS: When Large Language Models Meet Self-Skepticism](https://arxiv.org/abs/2409.06601)

*Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Skepticism, Hallucination, Question-Answering, AI Reliability

**Relevance Score:** 8

**TL;DR:** This paper presents LaMsS, an approach that combines large language models (LLMs) with self-skepticism to mitigate hallucinations in responses by introducing skepticism tokens.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination challenge faced by large language models, this study explores the utility of self-cognition and self-reflection inspired by human skepticism.

**Method:** The proposed method, LaMsS, integrates skepticism tokens into the LLM vocabulary, allowing the model to evaluate its skepticism when responding to queries, thereby controlling response reliability based on defined skepticism levels.

**Key Contributions:** Introduction of skepticism tokens to LLMs, Demonstrated improved performance in question-answering tasks, Insights into self-skepticism modeling for AI enhancement

**Result:** LaMsS demonstrates improved performance over baseline models in multi-choice questions and open-domain question-answering tasks, showing advantages in accuracy, AUC, and AP metrics and generalizing well in various settings.

**Limitations:** 

**Future Work:** Exploring broader applications of self-skepticism in LLMs across more complex tasks and domains.

**Conclusion:** The findings underscore the potential of self-skepticism modeling in enhancing LLM reliability and open avenues for future research in artificial intelligence.

**Abstract:** Hallucination is a major challenge for large language models (LLMs), preventing their further application in some fields. The skeptical thinking of humankind could be useful for LLMs to self-cognition, self-reflection and alleviate their hallucinations. Inspired by this consideration, we propose a novel approach called LaMsS, which combines the semantic understanding capability of LLMs with self-skepticism. By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning, which allow the LLM to decode each normal token followed by a skeptical token, representing different skepticism levels. By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold. By examining the accuracy, AUC and AP of willingly answering questions, we demonstrate that LaMsS achieves better performance than baselines on both multi-choice questions and open-domain question-answering benchmarks, and can generalize to multi-task and out-of-domain settings. Our study sheds some lights on the self-skepticism modeling on further artificial intelligence. Project code and model checkpoints can be found in https://anonymous.4open.science/r/SM-1E76.

</details>


### [63] [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)

*Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** Trust-Score, Trust-Align, retrieval-augmented generation, LLMs, model alignment

**Relevance Score:** 9

**TL;DR:** Introduction of Trust-Score and Trust-Align metrics for evaluating and improving the performance of LLMs in retrieval-augmented generation (RAG) tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the gap in understanding the appropriateness of LLMs for retrieval-augmented generation (RAG) tasks.

**Method:** Introduction of Trust-Score as a metric to assess LLM trustworthiness, alongside Trust-Align for model alignment to improve Trust-Score performance.

**Key Contributions:** Introduction of Trust-Score metric for LLM evaluation in RAG tasks., Development of Trust-Align for model alignment to improve RAG performance., Demonstrated effectiveness of Trust-Align across various open-weight models.

**Result:** 26 out of 27 models aligned using Trust-Align outperform competitive baselines, with the LLaMA-3-8b model showing significant improvements in ASQA, QAMPARI, and ELI5 tasks.

**Limitations:** 

**Future Work:** 

**Conclusion:** Trust-Align significantly enhances LLMs' performance in RAG tasks, offering a new direction for LLM model adjustments.

**Abstract:** LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56), QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly enhances models' ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.

</details>


### [64] [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)

*Xiaoyu Wang, Haoyong Ouyang, Balu Bhasuran, Xiao Luo, Karim Hanna, Mia Liza A. Lustria, Carl Yang, Zhe He*

**Main category:** cs.CL

**Keywords:** Lab-AI, retrieval-augmented generation, personalized normal ranges, health informatics, machine learning

**Relevance Score:** 9

**TL;DR:** Lab-AI is an interactive system that provides personalized normal ranges for lab results using retrieval-augmented generation (RAG), demonstrating significant performance improvements over traditional methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate interpretation of lab results is crucial, but existing patient portals often utilize universal normal ranges that do not consider individual factors such as age and gender.

**Method:** Lab-AI consists of two modules: factor retrieval and normal range retrieval. It was tested on 122 lab tests, comparing performance with and without conditional factors.

**Key Contributions:** Introduction of Lab-AI for personalized lab result interpretation, Use of retrieval-augmented generation (RAG) for improved accuracy, Demonstration of significant performance gains over traditional systems

**Result:** The system achieved a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval, outperforming the best non-RAG system by 33.5% in factor retrieval, and improving question-level and lab-level performance significantly for normal range retrieval.

**Limitations:** 

**Future Work:** 

**Conclusion:** Lab-AI shows promise in enhancing patient understanding and interpretation of lab results by providing personalized normal ranges.

**Abstract:** Accurate interpretation of lab results is crucial in clinical medicine, yet most patient portals use universal normal ranges, ignoring conditional factors like age and gender. This study introduces Lab-AI, an interactive system that offers personalized normal ranges using retrieval-augmented generation (RAG) from credible health sources. Lab-AI has two modules: factor retrieval and normal range retrieval. We tested these on 122 lab tests: 40 with conditional factors and 82 without. For tests with factors, normal ranges depend on patient-specific information. Our results show GPT-4-turbo with RAG achieved a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5% in factor retrieval and showed 132% and 100% improvements in question-level and lab-level performance, respectively, for normal range retrieval. These findings highlight Lab-AI's potential to enhance patient understanding of lab results.

</details>


### [65] [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)

*Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an*

**Main category:** cs.CL

**Keywords:** Extremely Low-Resource Languages, Machine Translation, Grammar Books, NLP, Parallel Data

**Relevance Score:** 7

**TL;DR:** The paper explores using grammar books to enhance machine translation for extremely low-resource languages, emphasizing parallel examples over grammatical explanations to improve translation capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Extremely low-resource languages lack large corpora for NLP, necessitating the use of available resources like dictionaries and grammar books for training models.

**Method:** The authors prompt long-context LLMs with grammar books and evaluate their contributions to translation performance, focusing on parallel examples over grammatical descriptions. They also test two tasks: grammaticality judgment and gloss prediction, using typological feature prompts.

**Key Contributions:** Highlighting the effectiveness of parallel examples for XLR translation tasks, Demonstrating that grammatical explanations are less useful for LLMs in translation, Introducing typological feature prompts that enhance performance on linguistic tasks

**Result:** The study finds that translation performance improves significantly using parallel examples from grammar books, achieving results comparable to LLMs, while grammatical explanations do not enhance translation ability.

**Limitations:** The study does not explore the effectiveness of grammatical explanations in other NLP tasks beyond translation.

**Future Work:** Further investigation into data collection strategies for XLR languages and exploring additional linguistic tasks requiring different types of data.

**Conclusion:** For multilingual XLR tasks, prioritizing the collection of parallel data is more effective than focusing on linguistic descriptions for translation.

**Abstract:** Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book's parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.

</details>


### [66] [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)

*Danqing Wang, Jianxin Ma, Fei Fang, Lei Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning strategies, TypedThinker, inductive reasoning, abductive reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces TypedThinker, a framework that enhances LLM reasoning by predicting suitable reasoning types (inductive, abductive, analogical) for specific problems, leading to improved problem-solving in logical and mathematical tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current LLMs that primarily use deductive reasoning and to enhance their performance by incorporating diverse reasoning strategies.

**Method:** The paper proposes a framework called TypedThinker that predicts the appropriate reasoning type for each problem based on its nature and historical performance data, guiding LLMs in using these strategies effectively.

**Key Contributions:** Introduced a system for predicting reasoning types tailored to specific problems., Demonstrated improved performance metrics for various LLMs on logical and mathematical reasoning tasks., Proposed integration possibilities with advanced LLM architectures to enhance their reasoning capabilities.

**Result:** TypedThinker shows significant performance improvements across multiple benchmarks, achieving gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B in logical and mathematical reasoning tasks.

**Limitations:** 

**Future Work:** Further exploration of integrating TypedThinker with other advanced system architectures and evaluating its impact on a broader range of reasoning tasks.

**Conclusion:** By diversifying reasoning approaches, TypedThinker enhances LLM performance without needing knowledge distillation from larger models, making it a valuable addition to advanced LLM systems.

**Abstract:** Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning, proceeding step-by-step from given conditions, which limits their exploration during problem-solving. Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning. However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving. Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies. Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks. TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models. It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.

</details>


### [67] [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)

*Yaniv Leviathan, Matan Kalman, Yossi Matias*

**Main category:** cs.CL

**Keywords:** Selective Attention, Transformers, Language Modeling, Efficiency, Memory Reduction

**Relevance Score:** 7

**TL;DR:** Selective Attention enhances the traditional attention mechanism by minimizing focus on unneeded elements, improving performance while reducing memory and compute requirements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance degradation caused by unneeded elements in the attention context within transformers.

**Method:** Introducing a parameter-free change to the standard attention mechanism that selectively reduces attention to irrelevant elements.

**Key Contributions:** Parameter-free modification to attention mechanisms, Significant performance improvement with fewer resources, Reduction in context buffer size leading to lower memory usage

**Result:** Selective attention improves language modeling and downstream task performance across various model sizes and context lengths, significantly reducing memory and compute requirements during inference.

**Limitations:** 

**Future Work:** 

**Conclusion:** Selective attention offers a mechanism that allows transformers to maintain performance while being more efficient in resource usage.

**Abstract:** Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.

</details>


### [68] [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)

*Tunazzina Islam, Dan Goldwasser*

**Main category:** cs.CL

**Keywords:** climate change, microtargeting, large language models, social media, fairness

**Relevance Score:** 4

**TL;DR:** This study analyzes climate change communication on social media, focusing on microtargeting using large language models (LLMs) to evaluate demographic targeting and fairness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and improve the effectiveness of climate change communication strategies on social media through microtargeting practices.

**Method:** A post-hoc analysis of Facebook advertisements using LLMs to predict demographic targets and generate explanations for classifications.

**Key Contributions:** Utilization of LLMs for demographic targeting analysis in climate campaigns, Identification of engagement strategies tailored to specific demographic groups, Comprehensive fairness analysis revealing biases in microtargeting predictions

**Result:** LLMs achieved an accuracy of 88.55% in predicting demographic targets and revealed distinct engagement strategies, although biases were identified in predictions for senior citizens and male audiences.

**Limitations:** Certain biases exist in the classification of senior citizens and male audiences.

**Future Work:** Further research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.

**Conclusion:** LLMs are effective in analyzing and explaining microtargeted strategies but require attention to biases for more equitable climate communication efforts.

**Abstract:** Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.

</details>


### [69] [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)

*Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang*

**Main category:** cs.CL

**Keywords:** Parameter-Efficient Fine-Tuning, large pre-trained models, downstream tasks

**Relevance Score:** 8

**TL;DR:** This review discusses Parameter-Efficient Fine-Tuning (PEFT) as a solution for efficiently adapting large pre-trained models to various downstream tasks, focusing on its principles, applications, and future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges posed by the computational and storage costs of large models in adapting them to specific downstream tasks, particularly in the context of limited hardware resources.

**Method:** This review introduces the principles of various PEFT algorithms and discusses their implementations in adapting large pre-trained models to new tasks with minimal additional parameters.

**Key Contributions:** Introduction of the core ideas and principles of PEFT algorithms, Discussion of real-world applications of PEFT, Identification of potential future research directions in PEFT

**Result:** The review provides readers with a concise understanding of PEFT methodologies, showcasing its potential to facilitate more efficient use of large models in diverse applications.

**Limitations:** 

**Future Work:** Exploration of new methodologies and innovations in the domain of PEFT to further enhance large model applications.

**Conclusion:** The overview presented in this review aims to expedite the development and application of PEFT techniques by making the underlying concepts readily accessible to interested parties.

**Abstract:** The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.

</details>


### [70] [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)

*Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober*

**Main category:** cs.CL

**Keywords:** textual classification, literary properties, hypothesis testing

**Relevance Score:** 4

**TL;DR:** This paper presents a framework for assessing the influence of literary properties on textual classification tasks by modeling label sequences. It identifies when classifications are influenced by thematic structure versus authorial style, offering insights for improving classification reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify the influence of sequentially correlated literary properties on textual classification tasks and determine the true drivers behind classification outcomes.

**Method:** A data-centric hypothesis-testing framework that models label sequences as stochastic processes, employing an empirical autocovariance matrix to generate surrogate labelings that preserve sequential dependencies.

**Key Contributions:** Introduction of a data-centric hypothesis-testing framework for textual classification, Identification of sequential versus non-sequential influences in classification outcomes, Demonstration that traditional unsupervised models outperform supervised and neural models in specific contexts.

**Result:** The method reveals that supervised and neural models are prone to false positives caused by sequentially correlated similarities, while unsupervised models using traditional features show high true positive rates with minimal false positives.

**Limitations:** The framework primarily focuses on sequentially correlated features and may not encompass all possible influences on classification tasks.

**Future Work:** Exploration of additional literary properties and their impacts on classification; expansion of the framework to other genres and languages.

**Conclusion:** Controlling for sequential correlation is essential for improving classification accuracy and ensuring that outcomes reflect genuine stylistic distinctions rather than confounded influences.

**Abstract:** We introduce a data-centric hypothesis-testing framework to quantify the influence of sequentially correlated literary properties--such as thematic continuity--on textual classification tasks. Our method models label sequences as stochastic processes and uses an empirical autocovariance matrix to generate surrogate labelings that preserve sequential dependencies. This enables statistical testing to determine whether classification outcomes are primarily driven by thematic structure or by non-sequential features like authorial style. Applying this framework across a diverse corpus of English prose, we compare traditional (word n-grams and character k-mers) and neural (contrastively trained) embeddings in both supervised and unsupervised classification settings. Crucially, our method identifies when classifications are confounded by sequentially correlated similarity, revealing that supervised and neural models are more prone to false positives--mistaking shared themes and cross-genre differences for stylistic signals. In contrast, unsupervised models using traditional features often yield high true positive rates with minimal false positives, especially in genre-consistent settings. By disentangling sequential from non-sequential influences, our approach provides a principled way to assess and interpret classification reliability. This is particularly impactful for authorship attribution, forensic linguistics, and the analysis of redacted or composite texts, where conventional methods may conflate theme with style. Our results demonstrate that controlling for sequential correlation is essential for reducing false positives and ensuring that classification outcomes reflect genuine stylistic distinctions.

</details>


### [71] [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)

*Andreas Koukounas, Georgios Mastrapas, Sedigheh Eslami, Bo Wang, Mohammad Kalim Akram, Michael Günther, Isabelle Mohr, Saba Sturua, Nan Wang, Han Xiao*

**Main category:** cs.CL

**Keywords:** contrastive learning, multimodal understanding, crossmodal retrieval

**Relevance Score:** 8

**TL;DR:** Proposes jina-clip-v2, a contrastive vision-language model that improves performance in multilingual and crossmodal retrieval tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance performance of contrastive models in both single-mode text and crossmodal tasks, addressing multilingual understanding and visually rich documents.

**Method:** Introduces a multi-task and multi-stage contrastive learning approach using a multilingual text encoder and a diverse dataset including 29 languages and visually rich documents.

**Key Contributions:** A multilingual text encoder incorporating 29 languages., Enhanced performance in text-only and crossmodal retrieval tasks., Flexibility in embedding dimensionality for user-defined representation granularity.

**Result:** Jina-clip-v2 outperforms existing CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks across English and multilingual contexts.

**Limitations:** No specific limitations were mentioned in the abstract.

**Future Work:** 

**Conclusion:** Jina-clip-v2 shows significant improvements and flexibility in embedding dimensionality, and is publicly accessible.

**Abstract:** Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model's performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.

</details>


### [72] [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)

*David Boldo, Lily Pemberton, Gabriel Thistledown, Jacob Fairchild, Felix Kowalski*

**Main category:** cs.CL

**Keywords:** contextual embeddings, large language models, gradient descent

**Relevance Score:** 8

**TL;DR:** The paper presents a Context-Aware Neural Gradient Mapping framework that integrates contextual embeddings into the optimization of large language models, enhancing their performance and generalization capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve large language models' task-specific generalization and adaptability through the integration of contextual embeddings in the optimization process.

**Method:** The framework employs a dynamic gradient adjustment mechanism using gradient descent modifications, where contextual embeddings are generated by a supplementary neural network that maps input features to optimal adaptation gradients.

**Key Contributions:** Introduction of dynamic gradient adjustment with contextual embeddings, Demonstration of consistent performance improvement over baseline models, Scalable framework for optimizing large language models

**Result:** Empirical evaluations show that the framework outperforms baseline models in accuracy, robustness to noise, and computational efficiency across various metrics.

**Limitations:** The authors withdrew the paper due to disputed and unverifiable authorship.

**Future Work:** 

**Conclusion:** The incorporation of context-specific embeddings enhances understanding of language and improves model performance, showcasing scalability for large-scale language models.

**Abstract:** The integration of contextual embeddings into the optimization processes of large language models is an advancement in natural language processing. The Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient adjustment mechanism, incorporating contextual embeddings directly into the optimization process. This approach facilitates real-time parameter adjustments, enhancing task-specific generalization even in the presence of sparse or noisy data inputs. The mathematical foundation of this framework relies on gradient descent modifications, where contextual embeddings are derived from a supplementary neural network trained to map input features to optimal adaptation gradients. By employing differential geometry principles, high-dimensional input dependencies are encoded into low-dimensional gradient manifolds, enabling efficient adaptation without necessitating the retraining of the entire model. Empirical evaluations demonstrate that the proposed framework consistently outperforms baseline models across various metrics, including accuracy, robustness to noise, and computational efficiency. The integration of context-specific embeddings allows for a more complex understanding of language, thereby improving the model's ability to handle diverse linguistic phenomena. Furthermore, the computational efficiency achieved through this method demonstrates its scalability for large-scale language models operating under diverse constraints.

</details>


### [73] [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)

*Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha*

**Main category:** cs.CL

**Keywords:** State Space Models, Natural Language Processing, Indic languages, Question Answering, Machine Learning

**Relevance Score:** 6

**TL;DR:** This paper explores the use of State Space Models for question answering in Indic languages, addressing unique linguistic challenges and achieving significant improvements in performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the unique challenges presented by the diversity and complexity of Indic languages in natural language processing tasks, especially in question answering.

**Method:** The paper evaluates multiple State Space Model architectures on diverse datasets that represent various Indic languages and conducts a comparative analysis of their performance.

**Key Contributions:** First application of State Space Models to QA in Indic languages, Significant performance improvements in question interpretation and answer generation, Benchmarks established for future research in this domain.

**Result:** The application of SSMs leads to significant improvements in question interpretation, context alignment, and answer generation for Indic languages, effectively capturing linguistic subtleties.

**Limitations:** 

**Future Work:** Enhancements to existing SSM frameworks to optimize their applicability to low-resource settings and multilingual scenarios.

**Conclusion:** This work establishes a foundational benchmark for applying State Space Models to question answering tasks in Indic languages and proposes enhancements for low-resource multilingual scenarios.

**Abstract:** The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.

</details>


### [74] [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)

*Christopher Nightingale, Dominic Lavington, Jonathan Thistlethwaite, Sebastian Penhaligon, Thomas Belinski, David Boldo*

**Main category:** cs.CL

**Keywords:** probabilistic embeddings, contextual inference, semantic granularity, attention mechanisms, adversarial robustness

**Relevance Score:** 7

**TL;DR:** This paper discusses the use of probabilistic token embeddings to enhance contextual inference and robustness in machine learning models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce representational rigidity in embeddings and improve semantic granularity in machine learning models for better contextual inference.

**Method:** The paper integrates probabilistic embeddings within attention mechanisms to allow for adaptive contextual weighting and examines performance through comparative evaluations.

**Key Contributions:** Introduces probabilistic token embeddings for improved contextual inference., Demonstrates robustness against adversarial modifications and adaptability in domain-specific applications., Shows that probabilistic embeddings enhance representational stability and contextual expressiveness.

**Result:** Probabilistic embeddings improve neighborhood consistency, reduce redundancy, and demonstrate robustness against adversarial modifications, achieving greater adaptability in domain-specific applications.

**Limitations:** The computational trade-offs involve marginal increases in inference latency that need to be balanced with representation benefits.

**Future Work:** Further exploration into the integration of probabilistic embeddings in various NLP tasks and the optimization of computational costs.

**Conclusion:** Probabilistic embeddings offer advantages in maintaining contextual integrity and representation stability, particularly in generative modeling tasks, despite marginal increases in inference latency.

**Abstract:** Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.

</details>


### [75] [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)

*Gaurav Srivastava, Shuxiang Cao, Xuan Wang*

**Main category:** cs.CL

**Keywords:** Small Language Models, Reasoning, Large Language Models, Benchmarking, Efficiency

**Relevance Score:** 9

**TL;DR:** This paper surveys and benchmarks 72 small language models (SLMs) across various reasoning benchmarks, demonstrating that SLMs can achieve competitive reasoning performance when compared to large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if small language models (SLMs) can achieve reasoning abilities comparable to large language models (LLMs) given their growing popularity for efficiency.

**Method:** The study systematically analyzed 72 SLMs from six model families against 14 reasoning benchmarks, employing four evaluation methods and comparing LLM judges against human evaluations with 800 data points, repeated thrice for robustness.

**Key Contributions:** Systematic benchmarking of 72 small language models across diverse reasoning tasks., Comparison of SLM reasoning capabilities to large language models and human evaluators., Insights into the implications of model size on reasoning performance.

**Result:** The findings indicate that SLMs can achieve strong reasoning capabilities and challenge the notion that larger model sizes are necessary for effective reasoning.

**Limitations:** The study may be limited by the scope of the 72 SLMs tested and their performance may not generalize across all types of reasoning tasks.

**Future Work:** Further exploration of SLM capabilities and improvements in their training methodologies for enhanced reasoning performance.

**Conclusion:** SLMs can serve as efficient alternatives to LLMs, suggesting pathways for future development focused on structured training and post-training compression for reasoning tasks.

**Abstract:** Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.

</details>


### [76] [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)

*Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen*

**Main category:** cs.CL

**Keywords:** Conversational Product Search, CPS dataset, Natural language processing, User intent detection, E-commerce

**Relevance Score:** 8

**TL;DR:** This paper introduces a new Conversational Product Search dataset (PSCon) to facilitate research on user intent detection and other related subtasks in e-commerce.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Existing research on Conversational Product Search (CPS) lacks real datasets and is often confined to specific markets and languages.

**Method:** A coached human-human data collection protocol was used to create the PSCon dataset, which covers dual markets and two languages.

**Key Contributions:** Creation of the PSCon dataset for CPS, Support for dual markets and multilingual interactions, Analysis and benchmark model for CPS research

**Result:** The PSCon dataset allows for comprehensive research across six subtasks related to CPS and includes a benchmark model for evaluation.

**Limitations:** Limited to the languages and markets represented in the dataset.

**Future Work:** Exploration of additional languages and markets; advancement of benchmark models based on CPS tasks.

**Conclusion:** The dataset and proposed benchmark model advance the capabilities of CPS systems and foster more in-depth research in this area.

**Abstract:** Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.

</details>


### [77] [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)

*Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Peer Review, Automated Evaluation, OpenReview, Scientific Progress

**Relevance Score:** 7

**TL;DR:** The paper evaluates Large Language Models (LLMs) for their paper review capabilities against expert reviews, revealing significant limitations in their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** This research aims to improve the peer review process in science by addressing challenges like reviewer shortages using LLMs.

**Method:** An automatic evaluation pipeline was developed to assess LLMs' review capabilities by comparing them to expert-generated reviews utilizing a dataset of 676 OpenReview papers.

**Key Contributions:** Developed an automatic evaluation pipeline for LLMs' paper review capabilities., Constructed a dataset of 676 OpenReview papers for analysis., Identified significant limitations in LLMs' review performance compared to experts.

**Result:** The analysis revealed that LLMs often provide unbalanced perspectives, overlook novelty in critiques, and make poor acceptance decisions compared to human experts.

**Limitations:** The study primarily focuses on LLMs and does not consider other potential tools or methods for enhancing peer review.

**Future Work:** Future research could explore enhancements to LLM review capabilities and investigate other AI approaches for peer review optimization.

**Conclusion:** The automated pipeline allows for scalable evaluations of LLMs' review capabilities, facilitating their improvement over time.

**Abstract:** Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.

</details>


### [78] [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)

*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Seid Muhie Yimam, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine De Kock, Tadesse Destaw Belay, Ibrahim Said Ahmad, Nirmal Surange, Daniela Teodorescu, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino Ali, Vladimir Araujo, Abinew Ali Ayele, Oana Ignat, Alexander Panchenko, Yi Zhou, Saif M. Mohammad*

**Main category:** cs.CL

**Keywords:** Emotion Detection, Cross-lingual, Low-resource Languages, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** Overview of a shared task on text-based emotion detection in over 30 low-resource languages, including multilabel and cross-lingual detection.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advance emotion detection methods in low-resource languages and explore cross-lingual capabilities.

**Method:** Participants predicted emotion labels in three tracks: multilabel emotion detection, emotion intensity score detection, and cross-lingual emotion detection.

**Key Contributions:** Introduction of a comprehensive shared task on emotion detection across multiple languages., Detailed analysis of the best-performing systems and common strategies and methods., Public availability of datasets for future research in emotion detection.

**Result:** The task saw participation from over 700 individuals and 200 teams, with baseline results and insights on effective methods reported.

**Limitations:** The results and findings are based on shared task submissions and may reflect the participants' contexts.

**Future Work:** Encouragement for further exploration in low-resource language emotion detection and cross-lingual methodologies.

**Conclusion:** The shared task results underline the challenges and successes in multi-lingual emotion detection, highlighting the potential of the publicly available datasets.

**Abstract:** We present our shared task on text-based emotion detection, covering more than 30 languages from seven distinct language families. These languages are predominantly low-resource and are spoken across various continents. The data instances are multi-labeled with six emotional classes, with additional datasets in 11 languages annotated for emotion intensity. Participants were asked to predict labels in three tracks: (a) multilabel emotion detection, (b) emotion intensity score detection, and (c) cross-lingual emotion detection.   The task attracted over 700 participants. We received final submissions from more than 200 teams and 93 system description papers. We report baseline results, along with findings on the best-performing systems, the most common approaches, and the most effective methods across different tracks and languages. The datasets for this task are publicly available. The dataset is available at SemEval2025 Task 11 https://brighter-dataset.github.io

</details>


### [79] [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)

*Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, transformer architecture, concept disentangling

**Relevance Score:** 7

**TL;DR:** HyperDAS is a novel transformer-based hypernetwork that improves mechanistic interpretability by automatically locating and constructing features in neural network hidden states without brute-force searching.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance mechanistic interpretability in neural networks by effectively locating and constructing concept features within hidden states, addressing limitations of previous methods.

**Method:** HyperDAS uses a transformer-based architecture to automatically identify token-positions in the residual stream associated with a concept and constructs features from those vectors.

**Key Contributions:** Introduction of HyperDAS as a novel hypernetwork architecture for interpretability, Improved performance on the RAVEL benchmark for concept disentangling, Mitigation strategies for avoiding information injection into target models

**Result:** HyperDAS outperforms existing methods on the RAVEL benchmark, demonstrating enhanced ability to disentangle concepts within hidden states.

**Limitations:** 

**Future Work:** 

**Conclusion:** HyperDAS improves the state of interpretability in neural networks while addressing concerns about introducing new information into the models.

**Abstract:** Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts(e.g., the birth year of a person) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) constructs features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.

</details>


### [80] [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)

*Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg*

**Main category:** cs.CL

**Keywords:** token embeddings, language models, geometric structure, intrinsic dimension, interpretability

**Relevance Score:** 8

**TL;DR:** The paper investigates the geometric structure of token embeddings in language models, revealing commonalities in their representation and providing a method for interpretability through alignment of embeddings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the shared geometrical structure of token embeddings in language models and investigate their alignment properties.

**Method:** The authors analyze token embeddings to characterize global similarities and local geometry using methods such as Locally Linear Embeddings and intrinsic dimension measurement.

**Key Contributions:** Identification of global similarities and local geometric structures in token embeddings., Introduction of the intrinsic dimension as a measure for understanding token embedding representation., Development of Emb2Emb for transferring steering vectors between language models.

**Result:** The study finds that token embeddings exhibit common relative orientations and that lower intrinsic dimensions correspond to semantically coherent clusters.

**Limitations:** 

**Future Work:** Exploration of further applications of the Emb2Emb method and deeper analysis of token embeddings in different contexts.

**Conclusion:** The alignment in token embeddings across hidden states has implications for model interpretability, leading to the development of Emb2Emb, a method for transferring vectors between different models.

**Abstract:** Researchers have recently suggested that models share common representations. In our work, we find that token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we introduce Emb2Emb, a simple method to transfer steering vectors from one language model to another, despite the two models having different dimensions.

</details>


### [81] [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)

*Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Memory Mechanisms, HCI, Machine Learning, Health Informatics

**Relevance Score:** 9

**TL;DR:** This paper analyzes memory mechanisms in Large Language Models, detailing sensory, short-term, and long-term memory while discussing methods to enhance efficiency and context-rich responses.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of memory in improving response quality and reducing hallucinations in Large Language Models.

**Method:** Categorizes memory types (sensory, short-term, long-term) and examines methods for memory acquisition, management, and utilization, including various compression and selection techniques.

**Key Contributions:** Categorization of memory types in LLMs, Detailed review of memory management strategies, Insights into parameter-based and hidden-state memory approaches

**Result:** Offers a comprehensive overview of memory mechanisms in LLMs, enhancing understanding of context handling and response accuracy.

**Limitations:** 

**Future Work:** Future research directions are highlighted to enhance the efficiency and efficacy of memory mechanisms in LLMs.

**Conclusion:** The analysis underpins the critical role of advanced memory mechanisms in optimizing LLM performance and indicates numerous directions for future research.

**Abstract:** This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

</details>


### [82] [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)

*Aravind Krishnan, Siva Reddy, Marius Mosbach*

**Main category:** cs.CL

**Keywords:** machine unlearning, large language models, knowledge frequency, unlearning evaluation, data privacy

**Relevance Score:** 9

**TL;DR:** This paper investigates the challenges of machine unlearning in large language models (LLMs), revealing that unlearning effectiveness varies based on the frequency of the knowledge to be removed from the model's training data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing machine unlearning approaches that uniformly treat all data points, which fails to consider the varying difficulty of unlearning based on knowledge frequency in training data.

**Method:** The authors conducted experiments to analyze the relationship between knowledge frequency in pre-training data and the success of unlearning, while also comparing probability and generation-based evaluations of unlearning.

**Key Contributions:** Demonstrates that frequency of knowledge affects unlearning success in LLMs., Identifies misalignment between evaluation methods for unlearning as model size increases., Calls for innovation in unlearning techniques and evaluation practices.

**Result:** The study found that more frequent knowledge is significantly harder to unlearn and highlighted the misalignment between different evaluation methods as models scale up, indicating a need for improved evaluation practices.

**Limitations:** The paper primarily focuses on the frequency aspect of knowledge unlearning and may not address other potential factors influencing unlearning effectiveness.

**Future Work:** Future research directions include developing novel methods for unlearning that account for data frequency and refining evaluation practices for LLM unlearning.

**Conclusion:** The findings underscore the necessity for novel unlearning methods and better evaluation techniques that consider the training data to enhance the effectiveness of unlearning in LLMs.

**Abstract:** Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.

</details>


### [83] [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)

*Alessio Tosolini, Claire Bowern*

**Main category:** cs.CL

**Keywords:** multilingual training, crosslingual training, acoustic models, speech recognition, Australian languages

**Relevance Score:** 3

**TL;DR:** This paper evaluates multilingual and crosslingual training for Australian languages by comparing the adaptation of an English acoustic model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effectiveness of multilingual versus crosslingual training in speech recognition for Australian languages with similar phonological features.

**Method:** Utilized the Montreal Forced Aligner to create acoustic models from scratch and adapted a large English model, testing on both seen and unseen language data.

**Key Contributions:** Introduced comparative analysis of multilingual and crosslingual training specific to Australian languages., Provided empirical evidence of the efficacy of adapting English models for other languages., Showed benefits of using adapted models on unseen language datasets.

**Result:** Found that adapting the English model provided benefits in recognizing previously unseen languages, suggesting an advantage of crosslingual training.

**Limitations:** The focus was limited to certain Australian languages, which may not generalize to all language pairs.

**Future Work:** Explore broader language families and more diverse linguistic characteristics to validate findings.

**Conclusion:** The study demonstrates that crosslingual training can improve acoustic model performance for new target languages.

**Abstract:** We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages.

</details>


### [84] [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)

*Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** data distillation, large language models, instruction tuning, trajectory matching, NLP

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel method for data distillation in large language models, enhancing training efficiency by synthesizing effective data samples for instruction tuning tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high training costs of large language models (LLMs) by minimizing data size while maintaining training effectiveness.

**Method:** The proposed method involves learning pseudo prompt data using trajectory matching and nearest neighbor identification, coupled with a regularization loss to boost robustness during the distillation process.

**Key Contributions:** Introduction of a novel data distillation method for text generation tasks, Achieved superior performance over state-of-the-art methods, Demonstrated effective transferability among different LLM architectures

**Result:** The approach outperforms the existing state-of-the-art data selection method (LESS) on ARC-Easy and MMLU instruction tuning datasets, demonstrating superior distillation efficacy and notable transferability across different LLM architectures.

**Limitations:** 

**Future Work:** Future research could explore broader applications of the distillation method and its effectiveness across various NLP tasks.

**Conclusion:** This is the first data distillation method specifically tailored for text generation tasks, promising a more efficient training process for LLMs.

**Abstract:** In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama).

</details>


### [85] [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)

*Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng*

**Main category:** cs.CL

**Keywords:** geometric reasoning, multimodal language models, benchmark, machine learning, AI

**Relevance Score:** 7

**TL;DR:** GeoSense introduces a benchmark for evaluating geometric reasoning in multimodal large language models (MLLMs), highlighting gaps in current assessments and identifying areas for improvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks do not adequately assess the dual aspects of human-like geometric reasoning in MLLMs, which is essential for effective geometric problem-solving.

**Method:** GeoSense framework encompasses a hierarchical assessment of geometric principles with an annotated dataset of 1,789 problems to evaluate MLLMs' reasoning capabilities.

**Key Contributions:** Introduction of the first bilingual benchmark for geometric reasoning in MLLMs., Development of a five-level hierarchical framework of geometric principles., Creation of an annotated dataset featuring 1,789 problems for systematic evaluation.

**Result:** Experimental results show that Gemini-2.0-pro-flash achieved the top score of 65.3, but overall performance indicates that MLLMs struggle with identifying and applying geometric principles.

**Limitations:** The study focuses solely on geometric reasoning, potentially neglecting other reasoning dimensions.

**Future Work:** Future research may explore enhancing MLLMs' capabilities in various domains of reasoning and broadening the scope of geometric principles assessed.

**Conclusion:** GeoSense has the potential to enhance the evaluation and development of MLLMs in geometric reasoning, contributing to advancements in human-like reasoning in AI.

**Abstract:** Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSense's potential to guide future advancements in MLLMs' geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence.

</details>


### [86] [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)

*Jiliang Ni, Jiachen Pu, Zhongyi Yang, Kun Zhou, Hui Wang, Xiaoliang Xiao, Dakui Wang, Xin Li, Jingfeng Luo, Conggang Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, model compression, knowledge transfer, cost-performance optimization

**Relevance Score:** 9

**TL;DR:** This paper presents a three-stage cost-efficient pipeline for deploying Large Language Models (LLMs) that focuses on optimizing performance while reducing costs and latency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and latency associated with the 'one-stage' pipeline that directly incorporates LLMs, which requires large model parameters for acceptable outcomes.

**Method:** The proposed pipeline consists of three stages: prototyping to create a high-quality teacher model, knowledge transfer using rejection fine-tuning and reinforcement learning to a smaller student model, and model compression via quantization and pruning to achieve ultra-low latency and cost.

**Key Contributions:** Introduction of a three-stage deployment pipeline for LLMs, Use of a prototype system to produce high-quality data for training, Development of a cost-effective student model through knowledge transfer techniques.

**Result:** The approach results in a super tiny model optimized for both cost and performance, with the final model compressed to 0.4B parameters, achieving effective performance at reduced costs.

**Limitations:** 

**Future Work:** Exploration of further optimizations and applications across different NLP domains.

**Conclusion:** The modular design and cross-domain capacities of the framework indicate its potential use in various NLP applications beyond the initial context.

**Abstract:** In recent years, Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) pipelines, improving performance and generalization. This has spurred their integration into various systems. Many NLP systems, including ours, employ a "one-stage" pipeline directly incorporating LLMs. While effective, this approach incurs substantial costs and latency due to the need for large model parameters to achieve satisfactory outcomes. This paper introduces a three-stage cost-efficient end-to-end LLM deployment pipeline-including prototyping, knowledge transfer, and model compression-to tackle the cost-performance dilemma in LLM-based frameworks. Our approach yields a super tiny model optimized for cost and performance in online systems, simplifying the system architecture. Initially, by transforming complex tasks into a function call-based LLM-driven pipeline, an optimal performance prototype system is constructed to produce high-quality data as a teacher model. The second stage combines techniques like rejection fine-tuning, reinforcement learning, and knowledge distillation to transfer knowledge to a smaller 0.5B student model, delivering effective performance at minimal cost. The final stage applies quantization and pruning to extremely compress models to 0.4B, achieving ultra-low latency and cost. The framework's modular design and cross-domain capabilities suggest potential applicability in other NLP areas.

</details>


### [87] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)

*Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou*

**Main category:** cs.CL

**Keywords:** PHD-Transformer, length scaling, KV cache management, NLP, pre-training

**Relevance Score:** 7

**TL;DR:** The PHD-Transformer framework facilitates length scaling during pre-training of large language models, optimizing inference efficiency via a unique KV cache management strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Exploration of length scaling in pre-training large language models, building on its established benefits in post-training.

**Method:** Introducing the Parallel Hidden Decoding Transformer (PHD-Transformer) featuring a KV cache management strategy that separates original and hidden decoding tokens to optimize efficiency while maintaining cache size.

**Key Contributions:** Introduction of the PHD-Transformer framework for efficient length scaling during pre-training., Novel KV cache management strategy that distinguishes between original and hidden decoding tokens., Optimized variants, PHD-SWA and PHD-CSWA, that enhance local dependency preservation and pre-filling time efficiency.

**Result:** The PHD-Transformer and its variants, PHD-SWA and PHD-CSWA, show consistent performance improvements across various benchmarks, validating the effectiveness of the proposed length scaling approach.

**Limitations:** 

**Future Work:** Further exploration of length scaling strategies and their applications in various NLP tasks.

**Conclusion:** PHD-Transformer effectively enables length scaling in pre-training with maintained inference efficiency, presenting a solid foundation for future research and applications in NLP.

**Abstract:** Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

</details>
