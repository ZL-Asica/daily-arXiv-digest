# 2025-04-26

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 70]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LLM impact on BLV programming](https://arxiv.org/abs/2504.17018)

*Prashant Chandrasekar, Mariel Couvillion, Ayshwarya Saktheeswaran, Jessica Zeitz*

**Main category:** cs.HC

**Keywords:** Large Language Models, Blind and Low-Vision Developers, Software Development, Accessibility, Generative AI

**Relevance Score:** 9

**TL;DR:** This study examines the impact of Large Language Models (LLMs) on blind and low-vision (BLV) developers, identifying both benefits and new accessibility challenges.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing integration of LLMs into software development raises questions about their efficacy and accessibility for blind and low-vision developers.

**Method:** A review of existing literature was conducted, followed by an evaluation of five popular LLM-powered IDEs through a comprehensive set of programming tasks.

**Key Contributions:** Identified new accessibility challenges introduced by LLMs for BLV developers., Evaluated the performance of LLM-powered IDEs in the context of BLV programming tasks., Highlighted key interaction barriers faced by BLV developers beyond just model accuracy.

**Result:** The evaluation revealed unsupported scenarios, incorrect model outputs, and significant interaction support limitations for BLV developers during coding tasks.

**Limitations:** The study may not capture all existing LLM applications or user experiences, and the sample of IDEs evaluated is limited.

**Future Work:** Further research is needed to develop tailored solutions that address the specific usability challenges BLV developers face when using LLMs.

**Conclusion:** Improving accessibility in generative AI-assisted programming can enhance the development experience for BLV programmers, necessitating focus on their unique challenges.

**Abstract:** Large Language Models (LLMs) are rapidly becoming integral to a wide range of tools, tasks, and problem-solving processes, especially in software development. Originally designed for natural language processing tasks such as text generation, LLMs are increasingly being used to assist both professionals and students in writing code. This growing reliance on LLM-based tools is reshaping programming workflows and task execution. In this study, we explore the impact of these technologies on blind and low-vision (BLV) developers. Our review of existing literature indicates that while LLMs help mitigate some of the challenges faced by BLV programmers, they also introduce new forms of inaccessibility. We conducted an evaluation of five popular LLM-powered integrated development environments (IDEs), assessing their performance across a comprehensive set of programming tasks. Our findings highlight several unsupported scenarios, instances of incorrect model output, and notable limitations in interaction support for specific tasks. Through observing BLV developers as they engaged in coding activities, we uncovered key interaction barriers that go beyond model accuracy or code generation quality. This paper outlines the challenges and corresponding opportunities for improving accessibility in the context of generative AI-assisted programming. Addressing these issues can meaningfully enhance the programming experience for BLV developers. As the generative AI revolution continues to unfold, it must also address the unique burdens faced by this community.

</details>


### [2] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)

*Felix Kares, Timo Speith, Hanwei Zhang, Markus Langer*

**Main category:** cs.HC

**Keywords:** saliency maps, explainable AI, user study, evaluation methods, machine learning

**Relevance Score:** 8

**TL;DR:** This study evaluates saliency maps (LIME, Grad-CAM, Guided Backpropagation) using subjective, objective, and mathematical metrics to assess their effectiveness in explaining neural network classifications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the open question of how to effectively evaluate saliency maps in explainable AI.

**Method:** A between-subject study involving 166 participants to assess trust, satisfaction, user abilities, and mathematical metrics across three popular saliency map approaches.

**Key Contributions:** First comprehensive comparison of saliency maps across multiple evaluation methods., Demonstrates the disconnect between user experience metrics and mathematical evaluations., Provides insights into the complex relationship between mathematical metrics and user understanding.

**Result:** Results showed no significant differences in trust and satisfaction, Grad-CAM improved user abilities the most, and Guided Backpropagation had the highest mathematical metrics. Some mathematical metrics correlated with user understanding, albeit in counterintuitive ways.

**Limitations:** The study's findings may not generalize across all contexts of explainable AI due to specific participant demographics and the focus on only three saliency map methods.

**Future Work:** Further exploration of saliency map evaluations in diverse user groups and contexts, and examination of additional saliency techniques.

**Conclusion:** The findings highlight the lack of agreement among evaluation methods for saliency maps and suggest the need for complementary approaches in the evaluation of explainable AI.

**Abstract:** Saliency maps are a popular approach for explaining classifications of (convolutional) neural networks. However, it remains an open question as to how best to evaluate salience maps, with three families of evaluation methods commonly being used: subjective user measures, objective user measures, and mathematical metrics. We examine three of the most popular saliency map approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between subject study (N=166) across these families of evaluation methods. We test 1) for subjective measures, if the maps differ with respect to user trust and satisfaction; 2) for objective measures, if the maps increase users' abilities and thus understanding of a model; 3) for mathematical metrics, which map achieves the best ratings across metrics; and 4) whether the mathematical metrics can be associated with objective user measures. To our knowledge, our study is the first to compare several salience maps across all these evaluation methods$-$with the finding that they do not agree in their assessment (i.e., there was no difference concerning trust and satisfaction, Grad-CAM improved users' abilities best, and Guided Backpropagation had the most favorable mathematical metrics). Additionally, we show that some mathematical metrics were associated with user understanding, although this relationship was often counterintuitive. We discuss these findings in light of general debates concerning the complementary use of user studies and mathematical metrics in the evaluation of explainable AI (XAI) approaches.

</details>


### [3] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)

*Ayushi Agrawal, Aditya Kondai, Kavita Vemuri*

**Main category:** cs.HC

**Keywords:** AI, facial assessment, self-objectification, self-esteem, gender differences

**Relevance Score:** 9

**TL;DR:** This study investigates the psychological effects of AI-powered facial assessment tools on self-objectification and self-esteem, highlighting gender differences and the potential reinforcement of social biases.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the psychological impacts of facial assessment tools and their influence on self-perception, particularly regarding self-objectification and self-esteem, with a focus on gender differences.

**Method:** Participants used two versions of a facial analysis tool (one critical and one neutral), completing self-objectification and self-esteem scales along with measures of emotions and appearance enhancement behaviors.

**Key Contributions:** Identification of psychological impacts of AI facial assessment tools, Highlighting gender differences in response to these tools, Call for responsible AI design to mitigate social bias reinforcement

**Result:** High self-objectification correlated with low self-esteem and increased appearance enhancement behaviors in both tool versions. The neutral tool still elicited negative emotional responses, suggesting implicit biases persist.

**Limitations:** The study does not address long-term effects of using AI tools or the generalizability of findings across diverse populations.

**Future Work:** Further research on how ideologies in training data influence AI outputs and user perceptions is needed.

**Conclusion:** AI facial assessment tools can unintentionally reinforce social biases and insecurities, emphasizing the need for ethical considerations in AI design.

**Abstract:** AI-powered facial assessment tools are reshaping how individuals evaluate appearance and internalize social judgments. This study examines the psychological impact of such tools on self-objectification, self-esteem, and emotional responses, with attention to gender differences. Two samples used distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9 years), and another more neutral (N=51; M=19.9 years). Participants completed validated self-objectification and self-esteem scales and custom items measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and perceived social emotion (PSE). Results revealed consistent links between high self-objectification, low self-esteem, and increased appearance enhancement behaviors across both versions. Despite softer framing, the newer tool still evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit feedback may reinforce appearance-related insecurities. Gender differences emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital enhancement and less likely to perceive emotional impact in others. These findings reveal how AI tools may unintentionally reinforce and amplify existing social biases and underscore the critical need for responsible AI design and development. Future research will investigate how human ideologies embedded in the training data of such tools shape their evaluative outputs, and how these, in turn, influence user attitudes and decisions.

</details>


### [4] [DashGuide: Authoring Interactive Dashboard Tours for Guiding Dashboard Users](https://arxiv.org/abs/2504.17150)

*Naimul Hoque, Nicole Sultanum*

**Main category:** cs.HC

**Keywords:** Dashboard guidance, User interaction, Authoring system

**Relevance Score:** 8

**TL;DR:** DashGuide is a framework that simplifies creating interactive dashboard guidance with minimal authoring input, allowing users to generate and refine dashboard tours.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Creating effective dashboard guidance is time-consuming and embedding it effectively poses challenges. This study aims to streamline the process to enhance user experience in navigating and understanding dashboards.

**Method:** DashGuide captures user interactions on a dashboard and uses them to create step-by-step guidance materials, known as dashboard tours. Authors can edit these tours while receiving generative assistance.

**Key Contributions:** Introduction of DashGuide framework for interactive guidance creation., Generative assistance to authors during the editing of guidance steps., Formative assessment and evaluation demonstrating improved authoring experience.

**Result:** An evaluation with 12 dashboard creators showed that DashGuide significantly improves the authoring experience, promoting efficiency and creative freedom in guidance creation.

**Limitations:** The study involved a small sample size of 21 dashboard creators, which may limit the generalizability of the findings.

**Future Work:** Further exploration of DashGuide's application in different contexts and with larger user groups to refine its effectiveness and usability.

**Conclusion:** DashGuide effectively balances the need for quick, expressive, and flexible authoring of dashboard guidance, addressing key challenges identified by dashboard creators.

**Abstract:** Dashboard guidance helps dashboard users better navigate interactive features, understand the underlying data, and assess insights they can potentially extract from dashboards. However, authoring dashboard guidance is a time consuming task, and embedding guidance into dashboards for effective delivery is difficult to realize. In this work, we contribute DashGuide, a framework and system to support the creation of interactive dashboard guidance with minimal authoring input. Given a dashboard and a communication goal, DashGuide captures a sequence of author-performed interactions to generate guidance materials delivered as playable step-by-step overlays, a.k.a., dashboard tours. Authors can further edit and refine individual tour steps while receiving generative assistance. We also contribute findings from a formative assessment with 9 dashboard creators, which helped inform the design of DashGuide; and findings from an evaluation of DashGuide with 12 dashboard creators, suggesting it provides an improved authoring experience that balances efficiency, expressiveness, and creative freedom.

</details>


### [5] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)

*Robert Kaufman*

**Main category:** cs.HC

**Keywords:** Autonomous vehicles, Human-AV interaction, Communication strategies, Machine learning, Trust

**Relevance Score:** 8

**TL;DR:** This dissertation investigates how autonomous vehicles (AVs) can effectively meet the varying informational needs of riders through adaptive communication strategies.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The adoption of autonomous vehicles is hindered by unresolved questions about how they should communicate with riders to meet diverse informational needs, taking into account different user goals and driving contexts.

**Method:** Three empirical studies were conducted: 1) identifying optimal communication strategies in extreme driving environments, 2) highlighting the consequences of faulty communication systems, and 3) using machine learning to predict trust in AVs based on personal factors.

**Key Contributions:** Identification of optimal communication strategies for different driving contexts., Demonstration of the critical role of context-sensitive communication to avoid negative outcomes., Application of machine learning to predict user trust in AVs based on individual traits.

**Result:** The findings indicate that communication must be task-sensitive, context-sensitive, and personalized to enhance driving performance, confidence, and trust, while also addressing learners' cognitive limits and goals.

**Limitations:** The studies may not cover all potential rider demographics and contexts, limiting the generalizability of the findings.

**Future Work:** Future research should explore broader demographics and additional contexts to further refine AV communication designs.

**Conclusion:** The dissertation emphasizes the need for transparent, adaptable, and personalized communication systems in AVs, suggesting that understanding the complex human-AV interaction landscape can lead to better design and policy decisions.

**Abstract:** Unresolved questions about how autonomous vehicles (AVs) should meet the informational needs of riders hinder real-world adoption. Complicating our ability to satisfy rider needs is that different people, goals, and driving contexts have different criteria for what constitutes interaction success. Unfortunately, most human-AV research and design today treats all people and situations uniformly. It is crucial to understand how an AV should communicate to meet rider needs, and how communications should change when the human-AV complex system changes. I argue that understanding the relationships between different aspects of the human-AV system can help us build improved and adaptable AV communications. I support this argument using three empirical studies. First, I identify optimal communication strategies that enhance driving performance, confidence, and trust for learning in extreme driving environments. Findings highlight the need for task-sensitive, modality-appropriate communications tuned to learner cognitive limits and goals. Next, I highlight the consequences of deploying faulty communication systems and demonstrate the need for context-sensitive communications. Third, I use machine learning (ML) to illuminate personal factors predicting trust in AVs, emphasizing the importance of tailoring designs to individual traits and concerns. Together, this dissertation supports the necessity of transparent, adaptable, and personalized AV systems that cater to individual needs, goals, and contextual demands. By considering the complex system within which human-AV interactions occur, we can deliver valuable insights for designers, researchers, and policymakers. This dissertation also provides a concrete domain to study theories of human-machine joint action and situational awareness, and can be used to guide future human-AI interaction research. [shortened for arxiv]

</details>


### [6] [Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication](https://arxiv.org/abs/2504.17171)

*Sunday David Ubur*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Captioning, Emotion Awareness, Deaf and Hard of Hearing, STEM Education

**Relevance Score:** 8

**TL;DR:** This paper presents an AR captioning framework that integrates non-verbal emotional cues into live transcriptions for DHH learners in STEM classrooms.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve educational accessibility for Deaf and Hard of Hearing learners by enhancing captioning systems to include emotional context.

**Method:** The study developed an augmented reality system using Unity that combines real-time speech recognition with analysis of non-verbal cues like facial expressions and gestures, rendering enriched captions with contextual markers.

**Key Contributions:** Development of an AR captioning framework specifically for DHH learners, Integration of emotional cues into live captions, Demonstration of improved comprehension and reduced cognitive effort in educational contexts

**Result:** Preliminary evaluations indicate that the AR captioning approach significantly enhances comprehension and lowers cognitive load compared to traditional captioning systems.

**Limitations:** Limited sample size in preliminary evaluations and specific to STEM contexts.

**Future Work:** Further research is needed to refine the technology and evaluate its effectiveness across different educational settings and content areas.

**Conclusion:** The findings highlight the effectiveness of immersive, emotion-aware captioning in supporting DHH learners in educational settings.

**Abstract:** This paper introduces an augmented reality (AR) captioning framework designed to support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by integrating non-verbal emotional cues into live transcriptions. Unlike conventional captioning systems that offer only plain text, our system fuses real-time speech recognition with affective and visual signal interpretation, including facial movements, gestures, and vocal tone, to produce emotionally enriched captions. These enhanced captions are rendered in an AR interface developed with Unity and provide contextual annotations such as speaker tone markers (e.g., "concerned") and gesture indicators (e.g., "nods"). The system leverages live camera and microphone input, processed through AI models to detect multimodal cues. Findings from preliminary evaluations suggest that this AR-based captioning approach significantly enhances comprehension and reduces cognitive effort compared to standard captions. Our work emphasizes the potential of immersive environments for inclusive, emotion-aware educational accessibility.

</details>


### [7] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)

*Tianyu Zhang, Dongheng Zhang, Ruixu Geng, Xuecheng Xie, Shuai Yang, Yan Chen*

**Main category:** cs.HC

**Keywords:** Channel State Information, WiFi localization, large-scale deployment

**Relevance Score:** 8

**TL;DR:** This paper presents a novel large-scale CSI-based localization system that enhances accuracy by leveraging unlabeled data and addressing data heterogeneity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the deployment of CSI-based localization systems, which have not achieved widespread commercialization compared to RSSI-based systems due to limited generalizability from small-scale environments.

**Method:** The authors propose a graph-based learning framework for WiFi localization that utilizes a pretext pretraining task to leverage large-scale unlabeled CSI data and a confidence-aware fine-tuning strategy to improve robustness.

**Key Contributions:** Development of a novel graph-based structure for heterogeneous CSI data, Introduction of a pretext pretraining task utilizing spatial and temporal priors, Implementation of a confidence-aware fine-tuning strategy for enhanced robustness

**Result:** The system achieved a median localization error of 2.17 meters and a floor accuracy of 99.49%, leading to an 18.7% reduction in mean absolute error compared to the best-performing baseline.

**Limitations:** 

**Future Work:** Exploration of further enhancements in the framework by expanding to diverse environments and integrating additional sensor modalities.

**Conclusion:** The proposed CSI-based localization framework demonstrates significant improvements in localization accuracy, highlighting the effectiveness of using unlabeled data and addressing CSI data heterogeneity.

**Abstract:** In recent years, Channel State Information (CSI), recognized for its fine-grained spatial characteristics, has attracted increasing attention in WiFi-based indoor localization. However, despite its potential, CSI-based approaches have yet to achieve the same level of deployment scale and commercialization as those based on Received Signal Strength Indicator (RSSI). A key limitation lies in the fact that most existing CSI-based systems are developed and evaluated in controlled, small-scale environments, limiting their generalizability. To bridge this gap, we explore the deployment of a large-scale CSI-based localization system involving over 400 Access Points (APs) in a real-world building under the Integrated Sensing and Communication (ISAC) paradigm. We highlight two critical yet often overlooked factors: the underutilization of unlabeled data and the inherent heterogeneity of CSI measurements. To address these challenges, we propose a novel CSI-based learning framework for WiFi localization, tailored for large-scale ISAC deployments on the server side. Specifically, we employ a novel graph-based structure to model heterogeneous CSI data and reduce redundancy. We further design a pretext pretraining task that incorporates spatial and temporal priors to effectively leverage large-scale unlabeled CSI data. Complementarily, we introduce a confidence-aware fine-tuning strategy to enhance the robustness of localization results. In a leave-one-smartphone-out experiment spanning five floors and 25, 600 m2, we achieve a median localization error of 2.17 meters and a floor accuracy of 99.49%. This performance corresponds to an 18.7% reduction in mean absolute error (MAE) compared to the best-performing baseline.

</details>


### [8] [Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment](https://arxiv.org/abs/2504.17204)

*Chitralekha Gupta, Hanjun Wu, Praveen Sasikumar, Shreyas Sridhar, Priambudi Bagaskara, Suranga Nanayakkara*

**Main category:** cs.HC

**Keywords:** Wearable Technology, Misinformation Detection, Interactive Learning Companion

**Relevance Score:** 8

**TL;DR:** The paper presents a voice-based interactive learning companion, Factually, aimed at enhancing cognitive functions and misinformation detection through wearable technology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To augment cognitive abilities and enhance critical thinking through informal learning via a wearable device.

**Method:** Introduction of Factually, a wearable fact-checking system that provides vibrotactile feedback to users when detecting potential misinformation.

**Key Contributions:** Development of Factually, a proactive wearable fact-checking system., Integration of contextual interactive quizzes to foster critical thinking and mindfulness., Proactive detection of misinformation through real-time feedback.

**Result:** Illustrative scenarios demonstrate Factually's ability to help users critically assess information in real time, with early feedback indicating enhanced fact-checking capabilities.

**Limitations:** Initial qualitative feedback suggests potential, but lacks quantitative data on effectiveness and user engagement.

**Future Work:** Further research into the effectiveness of Factually across diverse user populations and extended functionalities for cognitive enhancement.

**Conclusion:** Factually offers practical and experiential benefits that could significantly improve cognitive functions related to misinformation detection.

**Abstract:** Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits.

</details>


### [9] [MV-Crafter: An Intelligent System for Music-guided Video Generation](https://arxiv.org/abs/2504.17267)

*Chuer Chen, Shengqi Dang, Yuqi Liu, Nanxuan Zhao, Yang Shi, Nan Cao*

**Main category:** cs.HC

**Keywords:** music video generation, AI, synchronization, large language model, user-friendly interface

**Relevance Score:** 8

**TL;DR:** MV-Crafter is a system that automates the creation of high-quality music videos with synchronized audio and visuals, using AI-driven modules for script generation, video generation, and synchronization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in non-professionals creating quality music videos, despite their popularity as a multimedia format requiring expertise in script design and video synchronization.

**Method:** MV-Crafter utilizes three modules: a script generation module that employs a large language model, a video generation module, and a music-video synchronization module that incorporates a dynamic beat matching algorithm and visual envelope-induced warping.

**Key Contributions:** Introduction of a dynamic beat matching algorithm for synchronization, Development of a user-friendly interface with intuitive editing features, Leveraging a large language model for script generation based on musical semantics

**Result:** Extensive experiments show that MV-Crafter significantly improves the quality of generated music videos compared to previous automated frameworks.

**Limitations:** 

**Future Work:** Further enhancement of the system's capabilities and exploring additional use cases in multimedia generation.

**Conclusion:** MV-Crafter effectively simplifies the music video creation process while enhancing output quality, making it accessible for non-professionals.

**Abstract:** Music videos, as a prevalent form of multimedia entertainment, deliver engaging audio-visual experiences to audiences and have gained immense popularity among singers and fans. Creators can express their interpretations of music naturally through visual elements. However, the creation process of music video demands proficiency in script design, video shooting, and music-video synchronization, posing significant challenges for non-professionals. Previous work has designed automated music video generation frameworks. However, they suffer from complexity in input and poor output quality. In response, we present MV-Crafter, a system capable of producing high-quality music videos with synchronized music-video rhythm and style. Our approach involves three technical modules that simulate the human creation process: the script generation module, video generation module, and music-video synchronization module. MV-Crafter leverages a large language model to generate scripts considering the musical semantics. To address the challenge of synchronizing short video clips with music of varying lengths, we propose a dynamic beat matching algorithm and visual envelope-induced warping method to ensure precise, monotonic music-video synchronization. Besides, we design a user-friendly interface to simplify the creation process with intuitive editing features. Extensive experiments have demonstrated that MV-Crafter provides an effective solution for improving the quality of generated music videos.

</details>


### [10] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)

*Süleyman Özdel, Kadir Burak Buldu, Enkelejda Kasneci, Efe Bozkir*

**Main category:** cs.HC

**Keywords:** locomotion, virtual reality, large language models, user experience, accessibility

**Relevance Score:** 9

**TL;DR:** This study introduces a novel hands-free locomotion technique in virtual reality that leverages large language models, allowing natural language navigation and evaluating its effectiveness against traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user experience in virtual reality by offering a hands-free, natural language-based locomotion alternative that promotes accessibility and reduces reliance on handheld controllers.

**Method:** A comparative evaluation of three locomotion methods: controller-based teleportation, voice-based steering, and a language model-driven approach, assessed using eye-tracking data and standardized questionnaires measuring usability, presence, cybersickness, and cognitive load.

**Key Contributions:** Introduction of a hands-free locomotion technique using large language models in virtual reality., Evaluation of user experience compared to traditional locomotion methods., Insights into visual attention and cognitive processing through SHAP analysis of eye-tracking data.

**Result:** The LLM-driven locomotion technique exhibited comparable usability and presence scores to traditional methods while improving user attention, indicating its effectiveness as a hands-free navigation method.

**Limitations:** The study may be limited by the context of the virtual environments tested and the diversity of user experiences across different demographics.

**Future Work:** Further exploration of the scalability of the LLM-driven approach in more diverse VR applications and settings.

**Conclusion:** This method supports hands-free locomotion in virtual environments and is promising for enhancing accessibility.

**Abstract:** Locomotion plays a crucial role in shaping the user experience within virtual reality environments. In particular, hands-free locomotion offers a valuable alternative by supporting accessibility and freeing users from reliance on handheld controllers. To this end, traditional speech-based methods often depend on rigid command sets, limiting the naturalness and flexibility of interaction. In this study, we propose a novel locomotion technique powered by large language models (LLMs), which allows users to navigate virtual environments using natural language with contextual awareness. We evaluate three locomotion methods: controller-based teleportation, voice-based steering, and our language model-driven approach. Our evaluation measures include eye-tracking data analysis, including explainable machine learning through SHAP analysis as well as standardized questionnaires for usability, presence, cybersickness, and cognitive load to examine user attention and engagement. Our findings indicate that the LLM-driven locomotion possesses comparable usability, presence, and cybersickness scores to established methods like teleportation, demonstrating its novel potential as a comfortable, natural language-based, hands-free alternative. In addition, it enhances user attention within the virtual environment, suggesting greater engagement. Complementary to these findings, SHAP analysis revealed that fixation, saccade, and pupil-related features vary across techniques, indicating distinct patterns of visual attention and cognitive processing. Overall, we state that our method can facilitate hands-free locomotion in virtual spaces, especially in supporting accessibility.

</details>


### [11] [DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent](https://arxiv.org/abs/2504.17334)

*Chuer Chen, Yuqi Liu, Danqing Shi, Shixiong Cao, Nan Cao*

**Main category:** cs.HC

**Keywords:** DataStory, DataScout, LLM-based agent, Data Retrieval, Interactive System

**Relevance Score:** 8

**TL;DR:** DataScout is an interactive system that facilitates stance-based data facts retrieval to support the creation of comprehensive narratives.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and effectiveness of data fact retrieval in storytelling, addressing the challenges of time-consuming data search and the analytical skills required by creators.

**Method:** DataScout utilizes an LLM-based agent to construct a retrieval tree that supports collaborative control between users and the agent, visualized as a mind map to enhance intuitive navigation and reasoning.

**Key Contributions:** Introduction of an interactive system for stance-based data retrieval, Utilization of an LLM-based agent for constructing retrieval trees, Visualization of data retrieval as a mind map for enhanced user engagement

**Result:** The evaluation through case studies and expert interviews shows that DataScout effectively retrieves multifaceted data facts from various stances, aiding users in verifying their statements and boosting story credibility.

**Limitations:** The study's evaluation may be limited to specific case studies and expert feedback, which may not be generalizable to all user demographics or contexts.

**Future Work:** Exploration of broader applications and improvements to the DataScout system for diverse storytelling scenarios and enhancements in user interactivity.

**Conclusion:** DataScout enhances user experience in data storytelling by streamlining the retrieval process and improving narrative credibility through collaborative and visual means.

**Abstract:** A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.

</details>


### [12] [The Riemannian Means Field Classifier for EEG-Based BCI Data](https://arxiv.org/abs/2504.17352)

*Anton Andreev, Grégoire Cattan, Marco Congedo*

**Main category:** cs.HC

**Keywords:** Riemannian classifier, EEG-based BCI, power means, reproducibility, open source

**Relevance Score:** 8

**TL;DR:** This paper improves the Riemannian MDM classifier for EEG-based BCIs by using power means of SPD matrices, achieving superior performance while maintaining simplicity and determinism.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Riemannian minimum distance to mean (MDM) classifiers in EEG-based brain-computer interfaces through the use of power means of symmetric positive-definite (SPD) matrices.

**Method:** The proposed method involves using multiple power means of SPD matrices instead of just the geometric mean for classifier training.

**Key Contributions:** Introduction of power means for SPD matrices in MDM classification, Demonstration of performance improvement over traditional MDM, Open-source code release for reproducibility

**Result:** The improved classifier outperforms the traditional MDM classifier based on analysis across 20 public databases, achieving state-of-the-art performance.

**Limitations:** 

**Future Work:** Further exploration of additional matrix means and their potential applications in other domains.

**Conclusion:** The proposed power means method enhances classifier performance while preserving the desirable properties of MDM, such as simplicity and determinism, and contributes to reproducible research through open source code release.

**Abstract:** A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source.

</details>


### [13] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)

*Michelle L. Ding, Harini Suresh*

**Main category:** cs.HC

**Keywords:** AI governance, deep fake pornography, AIG-NCII, sociotechnical systems, malicious technical ecosystem

**Relevance Score:** 8

**TL;DR:** The paper examines the failure of sociotechnical AI governance in regulating the creation of AI-Generated Non-Consensual Intimate Images (AIG-NCII), highlighting the role of a malicious technical ecosystem.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the alarming rise of deep fake pornography and the inadequacies in current regulatory practices surrounding AIG-NCII.

**Method:** The study adopts a survivor-centered approach, identifies a malicious technical ecosystem related to AIG-NCII creation, and critiques existing governance methods using the NIST AI 100-4 report as a benchmark.

**Key Contributions:** Identification of a malicious technical ecosystem for creating AIG-NCII., Analysis of the inadequacies in existing AI governance frameworks., Recommendations for improving regulatory practices to protect individuals from AIG-NCII.

**Result:** The paper reveals significant gaps in the current governance framework for synthetic content, particularly in regulating tools that facilitate the creation of AIG-NCII.

**Limitations:** The paper primarily focuses on adult AIG-NCII and may not address related issues in child protection or other demographics.

**Future Work:** Further research is needed to develop effective governance methods that address the identified gaps and protect vulnerable populations.

**Conclusion:** Current regulatory practices are insufficient, and flawed assumptions underpin these methods, necessitating a re-evaluation of the governance landscape surrounding AIG-NCII.

**Abstract:** In this paper, we adopt a survivor-centered approach to locate and dissect the role of sociotechnical AI governance in preventing AI-Generated Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as "deep fake pornography." We identify a "malicious technical ecosystem" or "MTE," comprising of open-source face-swapping models and nearly 200 "nudifying" software programs that allow non-technical users to create AIG-NCII within minutes. Then, using the National Institute of Standards and Technology (NIST) AI 100-4 report as a reflection of current synthetic content governance methods, we show how the current landscape of practices fails to effectively regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining these gaps.

</details>


### [14] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)

*Jarne Thys, Sebe Vanbrabant, Davy Vanacken, Gustavo Rovelo Ruiz*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Large Language Models, Education Technology

**Relevance Score:** 8

**TL;DR:** The paper introduces INSIGHT, a modular AI tool aimed at assisting educators and students in higher education by dynamically addressing student inquiries and enhancing personalized teaching.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of AI technologies, particularly Large Language Models, into educational settings to improve teaching and learning experiences while addressing potential issues such as privacy and student-teacher interaction degradation.

**Method:** INSIGHT uses a modular design to analyze student questions to a Large Language Model (LLM) by extracting keywords for building a dynamic FAQ. This helps teaching staff gain insights for personalized student support.

**Key Contributions:** Introduction of a modular AI tool for educational support, Dynamic FAQ generation based on student queries, Insights-driven approach to enhance personalized teaching

**Result:** INSIGHT demonstrates the ability to provide personalized insights for teachers based on student queries, improving the instructional support provided to students in real-time.

**Limitations:** The paper is a proof of concept and may require further validation in diverse educational contexts.

**Future Work:** Future iterations of INSIGHT could leverage collected data for adaptive learning tailored to individual student progress and learning styles.

**Conclusion:** The implementation of INSIGHT shows promise in enhancing the educational experience by better supporting educators with the aid of AI while suggesting avenues for future adaptive learning applications.

**Abstract:** The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. This paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.

</details>


### [15] ['The Boring and the Tedious': Invisible Labour in India's Gig-Economy](https://arxiv.org/abs/2504.17697)

*Pratyay Suvarnapathaki, Viral Shah, Saarthak Negi, Nimmi Rangaswamy*

**Main category:** cs.HC

**Keywords:** gig economy, HCI, worker autonomy, digital discomfort, delivery platforms

**Relevance Score:** 8

**TL;DR:** This paper examines the challenges faced by gig food delivery workers in India, focusing on digital discomfort caused by waiting times and repetitive UI interactions, and proposes worker-centered GUI automation as a solution.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of invisible labour and digital discomfort among gig workers on food delivery platforms in India.

**Method:** Conducted 14 semi-structured interviews with gig food delivery agents to analyze their experiences and challenges.

**Key Contributions:** Identification of digital discomfort factors in gig food delivery apps, Proposition of worker-centered GUI automation as a solution, A critical perspective on HCI design in the Global South prioritizing worker agency

**Result:** Identified waiting time, repetitive UI interactions, and algorithmic management as key burdens, leading to the proposal of worker-centered GUI automation to enhance autonomy.

**Limitations:** Focuses on gig food delivery agents in India, which may limit generalizability to other contexts and gig economies.

**Future Work:** Encourages further research on HCI practices that support worker autonomy in diverse labor contexts.

**Conclusion:** The paper advocates for rethinking HCI designs in the Global South to prioritize the autonomy of workers over mere efficiency.

**Abstract:** India's gig-based food delivery platforms, such as Swiggy and Zomato, provide crucial income to marginalised communities but also entrench workers in cycles of invisible labour. Through 14 semi-structured interviews, we analyse waiting time and repetitive UI itneractions as key burdens that contribute to 'digital discomfort' for gig based food delivery agents. We find that workers employ creative strategies to navigate algorithmic management, yet remain constrained by platform-side 'gamification' and system opacity. We propose worker-centered GUI automation as a potential intervention to reduce friction while preserving agency. In conclusion, this position paper argues for rethinking HCI approaches in the Global South to prioritise worker autonomy over efficiency-driven design optimisations.

</details>


### [16] [LUIDA: Large-scale Unified Infrastructure for Digital Assessments based on Commercial Metaverse Platform](https://arxiv.org/abs/2504.17705)

*Yong-Hao Hu, Sotaro Yokoi, Yuji Hatada, Yuichi Hiroi, Takuji Narumi, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** metaverse, virtual reality, HCI, research efficiency, experimental reproducibility

**Relevance Score:** 9

**TL;DR:** LUIDA is a metaverse-based framework that streamlines the workflow of VR research by integrating tools for implementation, participant recruitment, execution, and data collection.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current workflows in VR research are fragmented and inefficient, requiring separate tools for implementation and data collection, which increases workload and reduces consistency.

**Method:** LUIDA integrates multiple processes into a single framework, automatically managing interconnected virtual environments for parallel experiment execution and providing adaptable implementation templates.

**Key Contributions:** Integration of fragmented research processes in VR studies, High usability and reduced workload reported by researchers, Validation of experimental integrity through replicated studies

**Result:** Users reported high usability scores (SUS: 73.75) and moderate workload (NASA-TLX: 24.11). Three replicated experiments with Cluster users recruited around 600 participants in total, validating the framework's integrity by producing consistent results with prior studies.

**Limitations:** The framework requires minimal metaverse development expertise, which may still pose a barrier for some researchers.

**Future Work:** After technical refinements, the team plans to release LUIDA as an open platform with standardized protocols.

**Conclusion:** LUIDA streamlines workflows in VR research and will be released as an open platform to enhance research efficiency and reproducibility.

**Abstract:** Online experiments using metaverse platforms have gained significant traction in Human-Computer Interaction and Virtual Reality (VR) research. However, current research workflows are highly fragmented, as researchers must use separate tools for system implementation, participant recruitment, experiment execution, and data collection, reducing consistency and increasing workload. We present LUIDA (Large-scale Unified Infrastructure for Digital Assessments), a metaverse-based framework that integrates these fragmented processes. LUIDA automatically allocates interconnected virtual environments for parallel experiment execution and provides implementation templates adaptable to various VR research domains, requiring minimal metaverse development expertise. Our evaluation included two studies using a prototype built on Cluster, the commercial metaverse platform. First, VR researchers using LUIDA to develop and run experiments reported high usability scores (SUS: 73.75) and moderate workload (NASA-TLX: 24.11) for overall usage, with interviews confirming streamlined workflows compared to traditional laboratory experiments. Second, we conducted three replicated experiments with public Cluster users, each recruiting approximately 200 participants within one week. These experiments produced results that closely matched the original studies, validating the experimental integrity of LUIDA across research domains. After technical refinements, we plan to release LUIDA as an open platform, providing a standardized protocol to improve research efficiency and experimental reproducibility in VR studies.

</details>


### [17] [WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing](https://arxiv.org/abs/2412.00247)

*Devin Murphy, Junyi Zhu, Paul Pu Liang, Wojciech Matusik, Yiyue Luo*

**Main category:** cs.HC

**Keywords:** tactile sensing, open-source, user study, adaptive hardware, low-power operation

**Relevance Score:** 8

**TL;DR:** The WiReSens Toolkit is an open-source platform designed to simplify the development of portable tactile sensing systems using resistive sensors, aimed at novice users.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create accessible, adaptive, and long-lasting tactile sensing systems for individuals with limited prior experience in using resistive sensors.

**Method:** Development of the WiReSens Toolkit, which includes adaptive hardware for interfacing with resistive sensors and a web-based GUI for enabling functionalities like multi-device programming, autocalibration methods, and low-power data transmission.

**Key Contributions:** Open-source platform for accessible tactile sensing development, Integration of adaptive hardware with a web-based GUI, Significant improvement in calibration speed and user experience

**Result:** User study with 11 novice participants showed that they configured tactile sensors with over 95% accuracy in under five minutes and calibrated sensors 10 times faster than baseline methods.

**Limitations:** 

**Future Work:** Exploration of further enhancements to expand functionalities and user expertise in tactile sensing systems.

**Conclusion:** The WiReSens Toolkit effectively enhances usability and performance in developing tactile sensing systems for novice users.

**Abstract:** Past research has widely explored the design and fabrication of resistive matrix-based tactile sensors as a means of creating touch-sensitive devices. However, developing portable, adaptive, and long-lasting tactile sensing systems that incorporate these sensors remains challenging for individuals having limited prior experience with them. To address this, we developed the WiReSens Toolkit, an open-source platform for accessible wireless tactile sensing. Central to our approach is adaptive hardware for interfacing with resistive sensors and a web-based GUI that mediates access to complex functionalities for developing scalable tactile sensing systems, including 1) multi-device programming and wireless visualization across three distinct communication protocols 2) autocalibration methods for adaptive sensitivity and 3) intermittent data transmission for low-power operation. We validated the toolkit's usability through a user study with 11 novice participants, who, on average, successfully configured a tactile sensor with over 95\% accuracy in under five minutes, calibrated sensors 10x faster than baseline methods, and demonstrated enhanced tactile data sense-making.

</details>


### [18] [Should Benevolent Deception be Allowed in EHMI? A Mechanism Explanation Based on Game Theory](https://arxiv.org/abs/2504.14539)

*Linkun Liu, Jian Sun, Ye Tian*

**Main category:** cs.HC

**Keywords:** autonomous vehicles, human-machine interface, benevolent deception, game theory, safety

**Relevance Score:** 8

**TL;DR:** The study investigates the role of benevolent deception in EHMI for autonomous vehicles, proposing a game theory-based decision framework and identifying conditions under which deception can enhance safety.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing research on human-machine interfaces for autonomous vehicles overlooks the sequence of actions and the implications of deception, questioning whether benevolent deception should be accepted in information disclosure.

**Method:** A game theory-based framework was developed, structuring the decision-making process into three stages: whether to disclose, when to disclose, and what type of intention information to disclose. A VR-based driving simulation was conducted to validate the findings.

**Key Contributions:** Proposed a game theory-based EHMI information disclosure framework for AVs., Identified specific conditions under which deception can enhance safety in interactions between AVs and HVs., Provided empirical support through VR-based simulation experiments.

**Result:** In 8.3% of scenarios (40 out of 484), safety improved due to successful deception. The cases included instances where the AV's expectations about a human vehicle (HV) yielded better interaction outcomes, despite trust levels in the EHMI playing a significant role in efficiency.

**Limitations:** The study focuses on specific interaction scenarios, and findings may not generalize across all autonomous vehicle contexts or user perceptions.

**Future Work:** Future research should explore broader applications of the framework and investigate how to build trust in EHMI to improve interaction efficiency.

**Conclusion:** Benevolent deception can enhance safety in specific conditions, but low trust in EHMI reduces interaction efficiency, suggesting that ethical frameworks for autonomous driving must consider these dynamics.

**Abstract:** The application of external human-machine interface (EHMI) on autonomous vehicles (AVs) facilitates information exchange. Existing research fails to consider the impact of the sequence of actions, as well as the effects of EHMI applications and deception, raising the question of whether benevolent, well-intentioned deception should be permitted (i.e., misleading statements that are intended to benefit both parties). We established a game theory based EHMI information disclosure framework for AVs in this study. In considering benevolent deception, this framework divided the decision-making process into three stages, respectively encompassing three key questions: whether to disclose, when to disclose, and what type of intention information to disclose. The results show that theoretical advantages of deception exist in certain cases when AV expects to maximize the safety of the interaction. In 40 out of 484 cases (8.3%), safety can be enhanced through successful deception. Those successful deceptions fall into two categories: 1) In 28 of these cases, the straight-going AV expected the left-turning HV to yield, while HV exhibited lower speed and higher acceleration; 2) In 12 of these cases, AV expected HV to proceed first, while HV exhibited higher speed and lower acceleration. We also conducted a VR-based driving simulation experiment, and the results confirmed our conclusion. Additionally, we found that when participants had low trust in the EHMI, its use negatively impacted interaction efficiency instead. This study aims to analyze the mechanisms of EHMI information disclosure and contribute to the ongoing discourse on the ethical framework governing autonomous driving systems.

</details>


### [19] [Subthreshold Jitter in VR Can Induce Visual Discomfort](https://arxiv.org/abs/2504.16295)

*Samuel J. Levulis, Kevin W. Rio, Pablo Ramon Soria, James Wilmott, Charlie S. Burlingham, Phillip Guan*

**Main category:** cs.HC

**Keywords:** visual-vestibular conflicts, virtual reality, jitter, motion sickness, comfort measurement

**Relevance Score:** 8

**TL;DR:** Subthreshold visual-vestibular conflicts can induce discomfort during VR use, highlighting the importance of frequent surveys to measure visual discomfort in naturalistic settings.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Existing studies on visual-vestibular conflicts primarily focus on extreme conditions that do not translate well to regular VR usage, necessitating an exploration of less intense, subthreshold conflicts that can impact comfort over prolonged use.

**Method:** A psychophysical study was conducted to identify perceptual thresholds for sinusoidal noise in render pose, followed by a repeated-measures study using the Meta Quest 3 VR HMD that introduced subthreshold jitter while participants played a game.

**Key Contributions:** Identification of perceptual thresholds for visual-vestibular conflicts in VR settings., Demonstration that subthreshold jitter can impact user comfort during gaming sessions., Advocacy for lightweight, frequent surveys to effectively measure visual discomfort in ecologically-valid contexts.

**Result:** Participants reported visual discomfort when exposed to subthreshold levels of jitter, however, traditional pre- and post-test comparisons of simulator sickness showed no significant comfort differences; significant changes were observed in real-time using more frequent surveys.

**Limitations:** The study was limited to a specific VR game and may not generalize to all VR experiences or applications.

**Future Work:** Further research is needed to explore the impact of subthreshold visual-vestibular conflicts across a wider range of VR applications and to optimize survey methodologies for discomfort measurement.

**Conclusion:** Incorporating more frequent and time-resolved measurement tools may lead to better detection of visual discomfort in VR, emphasizing the need for adaptive survey methods for future assessments.

**Abstract:** Visual-vestibular conflicts (VVCs) are a primary contributor to visually induced motion sickness (VIMS) in head-mounted displays (HMDs). However, virtual reality (VR) comfort studies often rely on exposing seated or standing users to experiences with high intensity visual motion (such as roller coasters). These drastic VVCs tend to induce pronounced VIMS symptoms that can be reliably detected across individuals using common survey measures. The conclusions from studies using these extreme motion-based conflicts may not accurately generalize to naturalistic use cases in VR where efforts are made to minimize, rather than maximize, VIMS symptoms. In this work, we show that a subthreshold visual-vestibular conflict can induce measurable discomfort during naturalistic, long duration use. We first present a psychophysical study, conducted outside of an HMD, to rigorously identify the perceptual thresholds for sinusoidal noise in render pose (i.e., jitter) resulting in erroneous 3D motion of rendered content. We next introduce subthreshold levels of jitter to a Meta Quest 3 VR HMD and demonstrate that this can induce visual discomfort in participants playing the commercially-available game Cubism across a three-session, repeated-measures study. Importantly, we did not identify statistically significant comfort differences between control and jitter conditions with traditional pre- and post-test comparison of Simulator Sickness Questionnaire (SSQ) scores. Significant differences were only identified using the Motion Illness Symptoms Classification (MISC) survey administered every 10 minutes across each 90 minute session. This highlights the benefits of incorporating time-resolved data points and suggests that lightweight, more frequent surveys may be important tools for measuring visual discomfort in more ecologically-valid scenarios.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)

*Cong Qi, Hanzhang Fang, Tianxing Hu, Siqi Jiang, Wei Zhi*

**Main category:** cs.CL

**Keywords:** single-cell RNA sequencing, transformer models, GeneMamba, state space modeling, biologically informed objectives

**Relevance Score:** 8

**TL;DR:** GeneMamba is a scalable foundation model for single-cell RNA sequencing that efficiently captures gene context with linear-time complexity, outperforming traditional transformer models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Single-cell RNA sequencing faces challenges due to high dimensionality, sparsity, and batch effects, necessitating improved computational methods.

**Method:** GeneMamba employs state space modeling and the Bi-Mamba architecture to achieve linear-time complexity while capturing bidirectional gene context. It was pretrained on nearly 30 million cells with biologically informed objectives.

**Key Contributions:** Introduction of GeneMamba, a new model for scRNA-seq analysis., Achieves linear-time complexity for gene context capture., Utilizes biologically informed training objectives.

**Result:** GeneMamba exhibits strong performance in multi-batch integration, cell type annotation, and gene-gene correlation, alongside enhanced interpretability and robustness compared to transformer baselines.

**Limitations:** Potential limitations were not explicitly mentioned in the abstract.

**Future Work:** Further exploration of GeneMamba's application in other biological contexts and optimizing its performance in various single-cell scenarios.

**Conclusion:** GeneMamba is a powerful alternative to existing transformer-based methods for single-cell transcriptomics, offering significant computational improvements and practical applicability.

**Abstract:** Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.

</details>


### [21] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)

*Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal*

**Main category:** cs.CL

**Keywords:** Tokenization, Named Entity Recognition, Low Resource Languages, SentencePiece, Byte Pair Encoding

**Relevance Score:** 8

**TL;DR:** This paper compares tokenization methods for Named Entity Recognition (NER) in low resource Indic languages, highlighting SentencePiece as superior to Byte Pair Encoding (BPE).

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of different tokenization strategies for Named Entity Recognition in low resource Indic languages, particularly given the morphological complexity of these languages and the limitations of current methods like BPE.

**Method:** The study systematically compares BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks across various low and extremely low resource Indic languages, assessing linguistic properties and downstream performance.

**Key Contributions:** Systematic comparison of tokenization strategies for NER in low resource Indic languages., Demonstration of SentencePiece's effectiveness in preserving linguistic structure and improving NER performance., Insights into the limitations of BPE in handling morphological complexity and maintaining entity consistency.

**Result:** Experiments show SentencePiece outperforms BPE in NER tasks for low resource Indic languages, particularly in zero shot cross lingual contexts, preserving entity consistency and offering better generalization.

**Limitations:** The study is limited to specific Indic languages and may not generalize across all languages or NLP tasks.

**Future Work:** Exploration of tokenization methods in additional low resource languages and further optimization of SentencePiece for enhanced performance in diverse linguistic contexts.

**Conclusion:** SentencePiece is the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications, especially for morphologically rich languages.

**Abstract:** Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.   Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.

</details>


### [22] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)

*Luca Moroni, Giovanni Puccetti, Pere-Lluis Huguet Cabot, Andrei Stefan Bejgu, Edoardo Barba, Alessio Miaschi, Felice Dell'Orletta, Andrea Esuli, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vocabulary Adaptation, Semantic Alignment, Italian Language, Neural Mapping

**Relevance Score:** 8

**TL;DR:** The paper introduces SAVA, a method for optimizing English LLMs for Italian by using vocabulary adaptation techniques, achieving improved performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in existing LLMs for non-English languages, specifically Italian, caused by language contamination and suboptimal multilingual pretraining.

**Method:** The paper compares different vocabulary adaptation techniques and presents Semantic Alignment Vocabulary Adaptation (SAVA), which employs neural mapping for vocabulary substitution in LLMs.

**Key Contributions:** Introduction of Semantic Alignment Vocabulary Adaptation (SAVA) for optimizing LLMs for non-English languages., Empirical comparison of vocabulary adaptation techniques for Italian LLMs., Demonstration of performance improvement and reduced token fertility in two prominent LLMs.

**Result:** SAVA demonstrated enhanced performance in grounded alignment strategies, reducing token fertility by 25% for Mistral-7b-v0.1 and optimizing the Llama-3.1-8B model, while decreasing its parameters by 1 billion.

**Limitations:** 

**Future Work:** Exploration of SAVA's applications in additional languages and other LLM architectures to broaden its impact.

**Conclusion:** The adapted models can recover their performance with limited continual training on the target language, showcasing their effectiveness in multi-choice and generative tasks.

**Abstract:** The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.

</details>


### [23] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)

*Shariar Kabir, Kevin Esterling, Yue Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Political Discourse, Belief Depth, Argumentative Consistency, Uncertainty Quantification

**Relevance Score:** 9

**TL;DR:** This paper proposes a framework to evaluate the belief depth of Large Language Models (LLMs) through argumentative consistency and uncertainty quantification, finding that LLMs exhibit topic-specific belief stability rather than uniform ideological stances.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing influence of LLMs on political discourse, it is essential to understand whether their responses reflect genuine beliefs or just align with training data.

**Method:** The paper analyzes 12 LLMs on 19 economic policies from the Political Compass Test by challenging their belief stability with both supportive and opposing arguments and quantifying uncertainty.

**Key Contributions:** Proposed a novel framework for evaluating belief depth in LLMs, Analyzed belief stability across multiple economic policy topics, Demonstrated high accuracy in distinguishing between genuine belief and surface-level alignment using semantic entropy

**Result:** The analysis shows that LLM responses are consistently stable, with 95% of left-leaning and 89% of right-leaning models maintaining consistency, and demonstrates high accuracy in using semantic entropy to distinguish genuine beliefs from surface-level alignment.

**Limitations:** The study is limited to 12 LLMs and 19 specific economic policies, which may not represent the broader spectrum of LLM capabilities and beliefs.

**Future Work:** Future research should explore belief stability across other topics and include a broader range of LLMs to enhance the framework's applicability.

**Conclusion:** The findings challenge the notion that LLMs have stable political ideologies and highlight the need for topic-specific reliability assessments in their applications.

**Abstract:** Large Language Models (LLMs) are increasingly shaping political discourse, yet their responses often display inconsistency when subjected to scrutiny. While prior research has primarily categorized LLM outputs as left- or right-leaning to assess their political stances, a critical question remains: Do these responses reflect genuine internal beliefs or merely surface-level alignment with training data? To address this, we propose a novel framework for evaluating belief depth by analyzing (1) argumentative consistency and (2) uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from the Political Compass Test, challenging their belief stability with both supportive and opposing arguments. Our analysis reveals that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Notably, up to 95% of left-leaning models' responses and 89% of right-leaning models' responses remain consistent under the challenge, enabling semantic entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing between surface-level alignment from genuine belief. These findings call into question the assumption that LLMs maintain stable, human-like political ideologies, emphasizing the importance of conducting topic-specific reliability assessments for real-world applications.

</details>


### [24] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)

*Arjun Subramonian, Vagrant Gautam, Preethi Seshadri, Dietrich Klakow, Kai-Wei Chang, Yizhou Sun*

**Main category:** cs.CL

**Keywords:** LLM misgendering, evaluation methods, convergent validity

**Relevance Score:** 8

**TL;DR:** This paper conducts a systematic meta-evaluation of methods for measuring LLM misgendering, finding significant discrepancies between evaluation methods and recommending improved future evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether different evaluation methods for LLM misgendering align in their results, as existing methods have not been examined for convergent validity.

**Method:** A systematic meta-evaluation across three existing datasets for LLM misgendering, transforming datasets for parallel evaluation of probability- and generation-based methods, and using a suite of 6 models from 3 families with human validation of LLM generations.

**Key Contributions:** Systematic meta-evaluation of LLM misgendering assessment methods, Demonstration of significant discrepancies between evaluation methods, Recommendations for future LLM misgendering evaluations

**Result:** Discrepancies were found between evaluation methods at the instance (20.2% disagreement), dataset, and model levels, with human evaluations highlighting the complexity of misgendering behavior beyond just pronouns.

**Limitations:** The study is a work in progress and may not cover all dimensions of misgendering evaluation or represent all models.

**Future Work:** Development of improved methodologies for evaluating LLM misgendering that better aligns with human evaluations and captures the complexity of the issue.

**Conclusion:** The findings suggest that existing automatic evaluations do not capture the full scope of misgendering, indicating essential disagreement with human evaluations and calling for refined evaluation methods in LLM research.

**Abstract:** Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.

</details>


### [25] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)

*Rendi Chevi, Kentaro Inui, Thamar Solorio, Alham Fikri Aji*

**Main category:** cs.CL

**Keywords:** Language Style, User Preference, LLM Interaction

**Relevance Score:** 8

**TL;DR:** LLM response styles influence user preferences, with implications for user experience and misinformation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how language style impacts user preferences in interactions with LLMs, considering the dual effects on user experience and susceptibility to misinformation.

**Method:** Conducted exploratory and experimental user studies to assess the influence of LLM language styles on user preferences across different populations.

**Key Contributions:** Identified the importance of language style in user preferences for LLM responses., Highlighted the variability of influence based on individual user traits and demographics., Pointed out the potential risks of misinformation due to stylistic factors.

**Result:** Found that LLM language style significantly influences user preferences, with variations based on user traits and demographics.

**Limitations:** Limited sample diversity and size; findings should be interpreted cautiously.

**Future Work:** Aim to address the limitations in sample sizes and demographics for more comprehensive analysis and causal relationships.

**Conclusion:** The findings suggest the need for more diverse samples and cautious interpretation of results, with further research planned to analyze the interplay between language style and user traits.

**Abstract:** What makes an interaction with the LLM more preferable for the user? While it is intuitive to assume that information accuracy in the LLM's responses would be one of the influential variables, recent studies have found that inaccurate LLM's responses could still be preferable when they are perceived to be more authoritative, certain, well-articulated, or simply verbose. These variables interestingly fall under the broader category of language style, implying that the style in the LLM's responses might meaningfully influence users' preferences. This hypothesized dynamic could have double-edged consequences: enhancing the overall user experience while simultaneously increasing their susceptibility to risks such as LLM's misinformation or hallucinations. In this short paper, we present our preliminary studies in exploring this subject. Through a series of exploratory and experimental user studies, we found that LLM's language style does indeed influence user's preferences, but how and which language styles influence the preference varied across different user populations, and more interestingly, moderated by the user's very own individual traits. As a preliminary work, the findings in our studies should be interpreted with caution, particularly given the limitations in our samples, which still need wider demographic diversity and larger sample sizes. Our future directions will first aim to address these limitations, which would enable a more comprehensive joint effect analysis between the language style, individual traits, and preferences, and further investigate the potential causal relationship between and beyond these variables.

</details>


### [26] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)

*Seunghyun Yoo*

**Main category:** cs.CL

**Keywords:** Interactive Chain-of-Thought, explainability, responsible AI

**Relevance Score:** 9

**TL;DR:** The paper presents an Interactive Chain-of-Thought (CoT) Framework that enhances explainability and responsible AI usage by allowing users to inspect, modify, and re-execute the model's reasoning process, promoting critical thinking and ethical transparency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The proliferation of short-form content and the rapid adoption of AI are diminishing opportunities for deep thinking and critical engagement with AI-generated outputs.

**Method:** The proposed framework decomposes reasoning into transparent, modular blocks that users can actively engage with and edit, and integrates a lightweight edit-adaptation mechanism for user alignment.

**Key Contributions:** Introduction of an Interactive Chain-of-Thought Framework for AI explainability., User-editable and inspectable model reasoning process., Integration of ethical transparency measures and bias checkpoint functionality.

**Result:** The framework encourages active cognitive engagement, ensures ethical transparency through metadata disclosure and bias checkpoint functionality, and adapts to diverse cognitive styles.

**Limitations:** 

**Future Work:** Explore further applications of the CoT Framework in various AI systems and its impact on user engagement and understanding.

**Conclusion:** This work outlines design principles and architecture to foster critical engagement and responsible interaction with AI systems to tackle complex social issues.

**Abstract:** Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [27] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)

*Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn*

**Main category:** cs.CL

**Keywords:** Small Language Models, Healthcare Informatics, Natural Language Processing, Model Optimization, Data Privacy

**Relevance Score:** 9

**TL;DR:** The paper surveys small language models (SLMs) for healthcare applications, providing a taxonomic framework and highlighting their role in resource-constrained environments.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To address growing concerns about data privacy and the need for scalable healthcare informatics solutions in resource-constrained settings.

**Method:** The authors developed a taxonomic framework to categorize SLMs and analyzed contributions across NLP tasks, stakeholder roles, and the continuum of care.

**Key Contributions:** Established a taxonomic framework for SLMs in healthcare., Compilations of experimental results across NLP tasks in healthcare., Resources and guidance for healthcare professionals on model optimization.

**Result:** The survey compiles experimental results of SLMs applied to several NLP tasks in healthcare, showcasing their potential for transformation in the field.

**Limitations:** 

**Future Work:** Encouragement for future research and development in model optimization and application in varied healthcare scenarios.

**Conclusion:** SLMs present a clinically viable solution for healthcare informatics, promoting accessibility and optimization while encouraging future research in model development.

**Abstract:** Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github

</details>


### [28] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)

*Hannah Cyberey, David Evans*

**Main category:** cs.CL

**Keywords:** large language models, censorship, representation engineering, thought suppression, safety-tuning

**Relevance Score:** 8

**TL;DR:** The paper investigates censorship in large language models (LLMs) and introduces methods for detecting and controlling censorship levels in model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how censorship mechanisms in LLMs function and to analyze their impact on information access.

**Method:** The authors use representation engineering techniques to study safety-tuned models and develop a refusal-compliance vector for controlling censorship. They also analyze reasoning LLMs for an additional dimension of censorship related to 'thought suppression'.

**Key Contributions:** Development of a refusal-compliance vector to control censorship, Identification of 'thought suppression' as a form of censorship in LLMs, Methods to negate censorship effects using vector manipulation

**Result:** The paper presents a method for identifying vectors that control the level of censorship and shows how to negate censorship effects by applying negative multiples of these vectors.

**Limitations:** 

**Future Work:** Further exploration of censorship mechanisms in other models and the implications of reduced censorship on model behavior.

**Conclusion:** The findings enhance our understanding of censorship in LLMs and provide tools to mitigate its effects on model outputs.

**Abstract:** Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector

</details>


### [29] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)

*Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Evaluation Metrics, Question Answering, Dataset

**Relevance Score:** 9

**TL;DR:** MIRAGE is a dataset for the evaluation of Retrieval-Augmented Generation (RAG) systems, addressing challenges in assessing the interplay between retrieval and generation components.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The evaluation of RAG systems is complicated due to the interdependency of retrieval and generation components, leading to a lack of appropriate benchmarks.

**Method:** MIRAGE consists of 7,560 question-answer instances aligned with a retrieval pool of 37,800 entries, enabling component-specific evaluations of RAG systems. Novel evaluation metrics assess RAG adaptability through various dimensions.

**Key Contributions:** Introduction of the MIRAGE dataset for RAG evaluation, Development of new evaluation metrics focusing on RAG adaptability, Insights into optimal model pair alignments in RAG configurations

**Result:** Experimental results reveal insights into the best alignments of model pairs and the dynamics within RAG systems, highlighting how different configurations perform under specific conditions.

**Limitations:** The dataset may not cover all possible contexts or retrieval scenarios, limiting its applicability in some niche areas of RAG evaluation.

**Future Work:** Further exploration of additional retrieval strategies and their effects on RAG system performance is needed, alongside expanding the dataset to cover more diverse contexts.

**Conclusion:** The MIRAGE dataset and its accompanying evaluation metrics provide a comprehensive tool for researchers to better evaluate and understand RAG systems.

**Abstract:** Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [30] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)

*Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** machine learning, code generation, large language models, reproducibility, PaperCoder

**Relevance Score:** 10

**TL;DR:** PaperCoder is a multi-agent LLM framework that converts machine learning papers into functional code repositories, addressing the lack of available code implementations for research reproducibility.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The unavailability of code implementations for machine learning research makes it difficult for researchers to reproduce results and build on prior work.

**Method:** PaperCoder operates in three stages: planning, analysis, and generation, utilizing a set of specialized agents that collaborate effectively to produce modular and dependency-aware code.

**Key Contributions:** Introduction of a multi-agent LLM framework for code generation from research papers., Three-stage process for transforming paper details into functional code., Evaluation showing superior performance on the PaperBench benchmark.

**Result:** PaperCoder generates high-quality code implementations from machine learning papers and outperforms strong baselines on the PaperBench benchmark.

**Limitations:** The current implementation may not cover all types of machine learning literature or complex models.

**Future Work:** Exploring enhancements for broader types of machine learning papers and improving agent collaboration efficiency.

**Conclusion:** PaperCoder effectively transforms machine learning research into accessible code, facilitating reproducibility and further developments in the field.

**Abstract:** Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.

</details>


### [31] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)

*Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor*

**Main category:** cs.CL

**Keywords:** Large language models, Retrieval-augmented generation, Natural hazards, WildfireGPT, Decision support

**Relevance Score:** 8

**TL;DR:** This paper presents WildfireGPT, a RAG-based multi-agent system designed to enhance decision-making concerning wildfire hazards by providing context-specific insights to diverse stakeholders.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the limitations of generalized large language models (LLMs) in providing context-specific information for decision-making in the face of extreme natural hazard events, like wildfires.

**Method:** The authors propose a retrieval-augmented generation (RAG) framework that integrates multi-agent design and diverse datasets, including natural hazard projections, observational data, and scientific literature, to deliver tailored insights about wildfires.

**Key Contributions:** Development of WildfireGPT, a specialized multi-agent LLM system for natural hazard decision support., Integration of RAG with diverse data sources to enhance contextuality and accuracy in insights., Evaluation showing superior performance over existing LLM-based solutions in real-world case studies.

**Result:** WildfireGPT significantly outperformed existing LLM-based decision support solutions in ten expert-led case studies, demonstrating its effectiveness in delivering accurate and contextually relevant information.

**Limitations:** The research is a proof of concept and may require further validation and adaptation to encompass a broader range of natural hazards beyond wildfires.

**Future Work:** Future research will focus on extending the system's capabilities to other types of natural hazards and integrating more advanced AI techniques for improved performance.

**Conclusion:** The results suggest that using a specialized RAG-based system like WildfireGPT can substantially improve decision-making processes related to natural hazards by providing users with relevant insights.

**Abstract:** Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.

</details>


### [32] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)

*Kaidong Feng, Zhu Sun, Jie Yang, Hui Fang, Xinghua Qu, Wenyuan Liu*

**Main category:** cs.CL

**Keywords:** knowledge distillation, bundle generation, large language models, efficiency, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates knowledge distillation (KD) approaches for efficient bundle generation using LLMs, focusing on performance impacts from knowledge formats, quantities, and utilization methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the efficiency challenges of deploying large-scale LLMs for bundle generation due to their high computational costs during fine-tuning and inference.

**Method:** A comprehensive KD framework is proposed that extracts knowledge progressively, captures various quantities through different strategies, and utilizes complementary LLM adaptation techniques.

**Key Contributions:** Systematic investigation of KD for LLM-based bundle generation, Development of a comprehensive KD framework, Insights into the impact of knowledge format, quantity, and utilization on performance

**Result:** Extensive experiments reveal insights into the effects of knowledge format, quantity, and utilization on the performance of LLM-based bundle generation, highlighting KD’s potential for efficiency and effectiveness.

**Limitations:** The study may not address all potential variations in knowledge distillation methods and their contexts in real-world applications.

**Future Work:** Future research could explore more diverse knowledge sources and advanced adaptation techniques to further enhance efficiency in LLM applications.

**Conclusion:** The findings indicate that with effective knowledge distillation, smaller models can achieve enhanced performance for bundle generation while reducing computational demands.

**Abstract:** LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.

</details>


### [33] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)

*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Cognitive Restructuring, LLM, Psychotherapy, Dialogue Framework, Mental Health

**Relevance Score:** 9

**TL;DR:** CRDial proposes a novel framework using multi-turn dialogues to enhance Cognitive Restructuring (CR) in psychotherapy through LLMs, addressing clinician shortages and stigma in mental health.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The shortage of clinicians and the stigma associated with seeking mental health care necessitate innovative solutions like human-LLM interactive psychotherapy to facilitate Cognitive Restructuring (CR).

**Method:** CRDial introduces a framework that incorporates multi-turn dialogues specifically structured for identifying and restructuring negative thoughts, alongside sentence-level supportive strategies and a multi-channel loop for iterative CR.

**Key Contributions:** Introduction of a novel multi-turn dialogue framework for CR, Development of Crisp dataset for training CR-focused LLMs, Evaluation of Crispers showing superiority in CR effectiveness.

**Result:** Human studies demonstrate that Crispers, based on the Crisp dataset and trained on conversational LLMs, outperform existing methods in CR effectiveness as measured through pointwise, pairwise, and intervention evaluations.

**Limitations:** The study's focus on LLMs may not address all facets of human interaction in psychotherapy, and further research is needed to explore long-term efficacy and adaptability in diverse settings.

**Future Work:** Exploration of CRDial's adaptability to various mental health conditions and populations, along with integrating more nuanced human interactions.

**Conclusion:** CRDial presents a promising solution for enhancing cognitive restructuring in therapy through interactive dialogues, showing significant improvements over traditional CR approaches.

**Abstract:** Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.

</details>


### [34] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)

*Ocheme Anthony Ekle, Biswarup Das*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, Transfer Learning, English-Igbo, RNN, Low-Resource Languages

**Relevance Score:** 8

**TL;DR:** This study investigates Neural Machine Translation for English-to-Igbo using RNN architectures and transfer learning, achieving notable improvements in translation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of low-resource language translation for Igbo, a language spoken by over 40 million people in Nigeria and West Africa.

**Method:** Develops NMT and Transformer-based models using RNNs (LSTM and GRU) with attention mechanisms, trained on a curated dataset that includes diverse sources verified by native experts. Utilizes transfer learning with pre-trained MarianNMT models in the SimpleTransformers framework.

**Key Contributions:** Development of efficient NMT models for a low-resource language, Demonstration of transfer learning benefits in translation accuracy, Curated dataset featuring diverse sources and expert validation

**Result:** Achieved +4.83 BLEU points improvement in translation accuracy, reaching an estimated 70% accuracy which closely matches existing benchmarks.

**Limitations:** The dataset is limited to specific sources, which might affect the generalizability of the results to other domains of Igbo language.

**Future Work:** Explore further enhancements in model architectures and training data diversification, as well as application to other low-resource languages.

**Conclusion:** The combination of RNNs and transfer learning effectively narrows the performance gap in English-Igbo translation tasks, showcasing the potential of these techniques for low-resource languages.

**Abstract:** In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.

</details>


### [35] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)

*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Cognitive Restructuring, dialogue systems, mental health, LLMs, interactive psychotherapy

**Relevance Score:** 9

**TL;DR:** The paper introduces CRDial, a framework for Cognitive Restructuring (CR) using multi-turn dialogues and specialized conversation strategies to improve interactive psychotherapy with LLMs, outperforming existing simple methods.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the clinician shortage and stigma in mental health, this research aims to enhance Cognitive Restructuring (CR) through human-LLM interactive psychotherapy methods.

**Method:** The proposed framework, CRDial, engages users in multi-turn dialogues with structured stages for identifying and restructuring negative thoughts. It uses supportive conversation strategies and a multi-channel loop for iterative CR.

**Key Contributions:** Development of CRDial framework for multi-turn CR, Creation of Crisp, a bilingual dialogue dataset for training, Training Crispers LLMs with enhanced performance for psychotherapy

**Result:** Human studies demonstrated that Crispers, conversational LLMs trained from the Crisp dataset, outperformed existing methods in evaluations of effectiveness.

**Limitations:** The study may require further validation across diverse populations and real-world scenarios outside controlled environments.

**Future Work:** Future research may explore further enhancements to the framework and real-world application impacts of CRDial in different therapeutic settings.

**Conclusion:** CRDial provides a more effective approach to Cognitive Restructuring by leveraging LLMs in an iterative, engaging dialogue format, indicating potential for future mental health applications.

**Abstract:** Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.

</details>


### [36] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)

*Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu*

**Main category:** cs.CL

**Keywords:** Unsupervised Domain Adaptation, Legal Judgment Prediction, contrastive learning, knowledge transfer, legal domains

**Relevance Score:** 8

**TL;DR:** JurisCTC is an Unsupervised Domain Adaptation model designed to enhance Legal Judgment Prediction by facilitating knowledge transfer between civil and criminal law domains using contrastive learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of legal text complexity and the lack of large-scale annotated datasets in the application of UDA for legal domains. It aims to improve model generalization in Legal Judgment Prediction (LJP) tasks.

**Method:** JurisCTC employs a novel approach for effective knowledge transfer across legal domains by utilizing contrastive learning techniques to distinguish between samples from different domains, specifically targeting civil and criminal law.

**Key Contributions:** Introduction of JurisCTC for Unsupervised Domain Adaptation in legal settings, Utilization of contrastive learning for effective knowledge transfer, Achievement of high accuracy in Legal Judgment Prediction tasks across legal domains.

**Result:** JurisCTC achieves peak accuracies of 76.59% and 78.83%, outperforming existing models and specific large language models (LLMs) on LJP tasks.

**Limitations:** Limited exploration of other legal domains beyond civil and criminal law; reliance on the availability of further annotated datasets for improvements.

**Future Work:** Further research could involve the application of JurisCTC to additional legal domains and the investigation of its performance across more diverse legal texts.

**Conclusion:** The model demonstrates significant advancements in the accurate prediction of legal judgments across different domains, highlighting the potential of UDA in the legal field.

**Abstract:** In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.

</details>


### [37] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)

*Xiuying Chen, Tairan Wang, Juexiao Zhou, Zirui Song, Xin Gao, Xiangliang Zhang*

**Main category:** cs.CL

**Keywords:** fairness, text generation, medical applications, deep learning, bias reduction

**Relevance Score:** 9

**TL;DR:** This study addresses fairness issues in AI-based text generation within the medical field, proposing an algorithm to reduce bias across demographics while maintaining performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and mitigate the fairness problem in text generation for medical applications, as existing AI models may reflect human biases, especially in underserved populations.

**Method:** An algorithm is proposed that selectively optimizes underperformed demographic groups by considering word-level and pathology accuracy, ensuring the process is fully differentiable for effective model training.

**Key Contributions:** Development of a bias-reducing algorithm for text generation in the medical field., Demonstration of significant reduction in performance discrepancies among demographic groups., Maintaining model performance while improving fairness metrics.

**Result:** Our evaluations showed that the disparities in performance among various demographic groups were reduced by over 30%, with a typical relative change in text generation accuracy of within 2%.

**Limitations:** Further testing across more diverse datasets and additional demographic variables is needed to validate the proposed approach.

**Future Work:** Future research may focus on extending the methodology to other domains and exploring additional fairness metrics and optimization techniques.

**Conclusion:** The proposed algorithm enhances fairness in medical text generation tasks without compromising overall performance, potentially addressing concerns about AI bias in medical diagnostics.

**Abstract:** Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe significant performance discrepancies across different races, sexes, and age groups, including intersectional groups, various model scales, and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underperformed groups to reduce bias. The selection rules take into account not only word-level accuracy but also the pathology accuracy to the target reference, while ensuring that the entire process remains fully differentiable for effective model training. Our evaluations across multiple backbones, datasets, and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance. Specifically, the disparities among various groups across different metrics were diminished by more than 30% with our algorithm, while the relative change in text generation accuracy was typically within 2%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of text generation diagnosis in medical domain.   Our code is publicly available to facilitate further research at https://github.com/iriscxy/GenFair.

</details>


### [38] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)

*Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, Xuming Hu*

**Main category:** cs.CL

**Keywords:** watermarking, language models, text quality, fuzzy c-means clustering, semantic integrity

**Relevance Score:** 8

**TL;DR:** CoheMark is a novel sentence-level watermarking technique that improves watermark detection and maintains text quality by leveraging cohesive relationships between sentences.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing sentence-level watermarking methods that compromise text quality due to arbitrary segmentation and generation processes.

**Method:** CoheMark utilizes trained fuzzy c-means clustering to select sentences and applies specific next sentence selection criteria for watermark embedding.

**Key Contributions:** Introduction of CoheMark for advanced sentence-level watermarking, Utilization of cohesive relationships between sentences for watermarking, Demonstration of strong watermark strength with minimal text quality impact

**Result:** Experimental results show that CoheMark provides strong watermark detection capabilities with minimal impact on the quality of generated text.

**Limitations:** 

**Future Work:** Further research could explore additional enhancements to watermarking techniques and applications in various content generation domains.

**Conclusion:** CoheMark represents an effective solution to enhance watermarking techniques without degrading the semantic integrity of the generated content.

**Abstract:** Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.

</details>


### [39] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)

*Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, Jey Han Lau*

**Main category:** cs.CL

**Keywords:** robustness evaluation, linguistic variations, large language models, task-agnostic, NLP tasks

**Relevance Score:** 8

**TL;DR:** FLUKE is a framework for evaluating model robustness through systematic linguistic variations, showing that robustness is task-dependent and highlighting vulnerabilities in both fine-tuned models and LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To assess model robustness systematically across diverse linguistic variations and understand how these variations impact performance on different NLP tasks.

**Method:** FLUKE introduces controlled variations at linguistic levels using large language models with human validation to generate and evaluate test data.

**Key Contributions:** Introduction of FLUKE as a task-agnostic robustness evaluation framework., Demonstration of the task-dependent nature of linguistic variation impacts on model performance., Identification of significant vulnerabilities in both LLMs and fine-tuned models, especially relating to negation.

**Result:** The study reveals that the impact of linguistic variations is task-dependent, LLMs are generally more robust than fine-tuned models yet still have vulnerabilities, particularly to negation modifications across tasks.

**Limitations:** The framework may not exhaustively cover all linguistic variations or tasks and may require extensive human validation for different contexts.

**Future Work:** Further exploration of linguistic variations not covered in this study and development of automated methods for robustness evaluation without significant human input.

**Conclusion:** Systematic robustness testing is crucial for better understanding model behaviors and improving their resilience across various linguistic inputs.

**Abstract:** We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.

</details>


### [40] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)

*Zihan Wang, Lu Yuan, Zhengxuan Zhang, Qing Zhao*

**Main category:** cs.CL

**Keywords:** misinformation detection, Dual-Aspect Empathy Framework, human empathy, Large Language Models, empathy-aware filtering

**Relevance Score:** 9

**TL;DR:** The paper introduces the Dual-Aspect Empathy Framework (DAE) for misinformation detection, integrating human empathy aspects to improve analysis and response authenticity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of traditional misinformation detection methods that overlook the role of human empathy in information propagation.

**Method:** The Dual-Aspect Empathy Framework (DAE) analyzes misinformation by examining both creators' cognitive strategies and emotional appeals, and simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs).

**Key Contributions:** Proposes the Dual-Aspect Empathy Framework (DAE) for misinformation detection., Integrates cognitive and emotional empathy in analyzing misinformation., Introduces an empathy-aware filtering mechanism to enhance response authenticity.

**Result:** Experimental results show that DAE outperforms existing misinformation detection methods on benchmark datasets, demonstrating its effectiveness and introducing a novel paradigm for multimodal misinformation detection.

**Limitations:** The paper may not address the complexities of all types of misinformation or every cultural context in which misinformation spreads.

**Future Work:** Further exploration of the DAE framework in diverse contexts and its applicability across various platforms and languages.

**Conclusion:** DAE provides a comprehensive and human-centric approach to misinformation detection, enhancing response authenticity and diversity through an empathy-aware filtering mechanism.

**Abstract:** In the digital era, social media has become a major conduit for information dissemination, yet it also facilitates the rapid spread of misinformation. Traditional misinformation detection methods primarily focus on surface-level features, overlooking the crucial roles of human empathy in the propagation process. To address this gap, we propose the Dual-Aspect Empathy Framework (DAE), which integrates cognitive and emotional empathy to analyze misinformation from both the creator and reader perspectives. By examining creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more comprehensive and human-centric approach to misinformation detection. Moreover, we further introduce an empathy-aware filtering mechanism to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate that DAE outperforms existing methods, providing a novel paradigm for multimodal misinformation detection.

</details>


### [41] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)

*Chengguang Gan, Sunbowen Lee, Zhixi Cai, Yanbin Wei, Lei Zheng, Yunhao Liang, Shiwen Ni, Tatsunori Mori*

**Main category:** cs.CL

**Keywords:** Multimodal, Mutual Reinforcement Effect, Information Extraction, Vision-Language Models, Prompt Format Adapter

**Relevance Score:** 8

**TL;DR:** This paper introduces the Multimodal Mutual Reinforcement Effect (M-MRE) in information extraction, demonstrating the effectiveness of mutual reinforcement across multimodal tasks for the first time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the applicability of the Mutual Reinforcement Effect (MRE) in the multimodal domain, extending its benefits beyond textual tasks to visual and multimodal tasks.

**Method:** The authors developed a new task called M-MRE and created a dataset to support it, along with a Prompt Format Adapter (PFA) for compatibility with Large Vision-Language Models (LVLMs).

**Key Contributions:** Introduction of Multimodal Mutual Reinforcement Effect (M-MRE) as a new task., Creation of a dataset to facilitate M-MRE research., Development of a Prompt Format Adapter (PFA) for LVLM compatibility.

**Result:** Experimental results show that the M-MRE task exhibits mutual reinforcement effects, indicating performance improvements across interrelated multimodal tasks.

**Limitations:** The study is limited to the validation of M-MRE in a specific set of multimodal tasks and does not cover all possible applications in the visual domain.

**Future Work:** Future research may explore broader applications of M-MRE and further improvements in the functionality of PFA across various multimodal models.

**Conclusion:** The findings confirm the generalizability of MRE principles from textual domains to multimodal understanding scenarios, supporting enhanced performance through joint modeling.

**Abstract:** Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.

</details>


### [42] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)

*Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine*

**Main category:** cs.CL

**Keywords:** Large Language Models, Health Informatics, Data Privacy, Model Merging, Machine Learning

**Relevance Score:** 9

**TL;DR:** PatientDx is a framework for merging Large Language Models (LLMs) designed for healthcare that improves performance without fine-tuning on sensitive patient data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve LLM performance in healthcare while addressing data privacy concerns associated with training on sensitive annotated data.

**Method:** PatientDx utilizes a model merging strategy that optimizes a pivotal model adapted to numerical reasoning, tuning hyperparameters based on performance metrics without requiring fine-tuning on actual patient data.

**Key Contributions:** Introduction of a model merging framework for healthcare applications, Avoidance of fine-tuning on patient data while improving performance, Demonstrated reduced data leakage risks compared to traditional fine-tuning methods.

**Result:** Improvements of up to 7% in AUROC on mortality tasks compared to initial models, and reduced risk of data leakage compared to fine-tuned models.

**Limitations:** The framework's effectiveness may vary with different health-predictive tasks and datasets, and further validation is needed across various domains.

**Future Work:** Exploration of additional health-predictive tasks and enhancement of the model merging strategy to include diverse data types.

**Conclusion:** PatientDx offers a promising approach to developing health-predictive LLMs that maintain performance while safeguarding patient data privacy.

**Abstract:** Fine-tuning of Large Language Models (LLMs) has become the default practice for improving model performance on a given task. However, performance improvement comes at the cost of training on vast amounts of annotated data which could be sensitive leading to significant data privacy concerns. In particular, the healthcare domain is one of the most sensitive domains exposed to data privacy issues. In this paper, we present PatientDx, a framework of model merging that allows the design of effective LLMs for health-predictive tasks without requiring fine-tuning nor adaptation on patient data. Our proposal is based on recently proposed techniques known as merging of LLMs and aims to optimize a building block merging strategy. PatientDx uses a pivotal model adapted to numerical reasoning and tunes hyperparameters on examples based on a performance metric but without training of the LLM on these data. Experiments using the mortality tasks of the MIMIC-IV dataset show improvements up to 7% in terms of AUROC when compared to initial models. Additionally, we confirm that when compared to fine-tuned models, our proposal is less prone to data leak problems without hurting performance. Finally, we qualitatively show the capabilities of our proposal through a case study. Our best model is publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [43] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)

*Yongxuan Wu, Runyu Chen, Peiyu Liu, Hongjin Qian*

**Main category:** cs.CL

**Keywords:** long-context understanding, spoken language, redundancy, natural language processing, benchmark

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenges of long-context understanding in natural language processing by constructing a novel spoken long-text dataset and proposing a new baseline method for handling redundancy in dialogue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve long-context understanding in natural language processing, especially for real-world dialogues that are often redundant and conversational, as existing benchmarks do not accurately reflect these complexities.

**Method:** The authors created a spoken long-text dataset derived from live streams, characterized by redundancy and conversational elements, and designed tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid, to evaluate LLM performance.

**Key Contributions:** Construction of the first spoken long-text dataset from live streams, Development of tasks that capture redundancy in conversations, Proposal of a new baseline for better long-context understanding

**Result:** Evaluation of popular LLMs and specialized methods revealed they performed poorly on highly redundant inputs, with no single method consistently outperforming others. A new baseline was proposed that better manages redundancy and shows strong performance across tasks.

**Limitations:** Current methods show strong task-specific preferences and struggle with redundancy, indicating limitations in generalizable performance.

**Future Work:** Future research should focus on enhancing long-context understanding capabilities and exploring better approaches for redundancy in various dialogue systems.

**Conclusion:** This work fills a gap in evaluating long-context spoken language understanding and provides a solid foundation for future applications in e-commerce systems and other real-world scenarios.

**Abstract:** Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at https://github.com/Yarayx/livelongbench.

</details>


### [44] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)

*Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** Task-Oriented Dialogue, Personalization, NLG, User Images, Machine Learning

**Relevance Score:** 8

**TL;DR:** Introducing PicPersona-TOD, a dataset integrating user images for personalized responses in Task-Oriented Dialogue systems, along with the NLG model Pictor.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Task-Oriented Dialogue systems often provide generic responses and lack personalization based on user attributes, negatively impacting user experience.

**Method:** The PicPersona-TOD dataset uses user images to enrich dialogues, applying first impressions, dialogue policy-guided prompting, and external knowledge to minimize hallucinations.

**Key Contributions:** Introduction of the PicPersona-TOD dataset for personalized dialogue, Development of the Pictor NLG model that personalizes and performs well in unseen domains, Incorporation of visual context to enhance dialogue interactions

**Result:** Evaluations show that personalized responses significantly improve user engagement and experience in dialogue interactions.

**Limitations:** The dataset may require diverse image inputs and further testing across various domains to confirm robustness.

**Future Work:** Exploration of additional user factors and adaptation of other dialogue systems to leverage this personalization methodology.

**Conclusion:** The integration of user images into dialogue systems leads to personalized and engaging interactions, suggesting a move toward more individualized user experiences.

**Abstract:** Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users' personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains https://github.com/JihyunLee1/PicPersona.

</details>


### [45] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)

*Anna Lieb, Maneesh Arora, Eni Mustafaraj*

**Main category:** cs.CL

**Keywords:** LLM-generated text augmentation, topic modeling, social science research

**Relevance Score:** 9

**TL;DR:** This paper explores the use of LLM-generated text augmentation to enhance topic modeling outputs for social science research, demonstrating improved interpretability and practicality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Topic modeling and clustering are frequently employed in social sciences, but they face challenges like interpretability and specific applicability to research questions. This study aims to address these limitations using LLM-generated text augmentation.

**Method:** The authors conduct a case study within political science, applying GPT-4 for text augmentation in topic modeling to generate more interpretable and relevant categories for social science research.

**Key Contributions:** Introduces LLM-generated text augmentation to improve topic modeling interpretability., Demonstrates practical applications in political science with domain-specific relevance., Shows reduction in human labor required for qualitative analysis through automated methods.

**Result:** The application of GPT-4 augmentations resulted in highly interpretable topic categories that facilitate targeted investigations into social science research questions with limited human intervention required.

**Limitations:** The study may be limited by the specific domain of political science and the reliance on LLM-generated content, which could introduce biases.

**Future Work:** Further investigation is suggested in other domains and with different LLM architectures to validate findings and explore broader applicability.

**Conclusion:** LLM-generated text augmentation significantly enhances the interpretability and practicality of topic modeling outputs, making it a valuable tool for researchers in social sciences.

**Abstract:** Unsupervised machine learning techniques, such as topic modeling and clustering, are often used to identify latent patterns in unstructured text data in fields such as political science and sociology. These methods overcome common concerns about reproducibility and costliness involved in the labor-intensive process of human qualitative analysis. However, two major limitations of topic models are their interpretability and their practicality for answering targeted, domain-specific social science research questions. In this work, we investigate opportunities for using LLM-generated text augmentation to improve the usefulness of topic modeling output. We use a political science case study to evaluate our results in a domain-specific application, and find that topic modeling using GPT-4 augmentations creates highly interpretable categories that can be used to investigate domain-specific research questions with minimal human guidance.

</details>


### [46] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)

*Xin Yi, Shunfan Zhengc, Linlin Wanga, Xiaoling Wang, Liang He*

**Main category:** cs.CL

**Keywords:** Watermarking, Knowledge Distillation, Large Language Models, Contrastive Decoding, Model Security

**Relevance Score:** 9

**TL;DR:** This paper presents a framework called Contrastive Decoding-Guided Knowledge Distillation (CDG-KD) for conducting bidirectional watermark attacks under unauthorized knowledge distillation in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to combat misinformation and protect intellectual property in large language models through robust watermarking techniques.

**Method:** The proposed CDG-KD framework employs contrastive decoding to extract compromised watermark texts from outputs of student models and weakly watermarked references, coupled with bidirectional distillation for watermark removal and forgery.

**Key Contributions:** Introduces the concept of watermark radioactivity and its implications for knowledge distillation., Presents CDG-KD as a unified framework for simultaneous watermark removal and spoofing attacks., Demonstrates that existing watermarking schemes are vulnerable and need improvement.

**Result:** CDG-KD enables effective watermark attacks while maintaining the general performance of the distilled models, revealing vulnerabilities in current watermarking methods.

**Limitations:** The study primarily focuses on watermarking techniques and their vulnerabilities, but may not address other aspects of model security.

**Future Work:** Further research is needed to develop watermarking schemes that are inherently robust and unforgeable against various attack vectors.

**Conclusion:** There is a critical need to enhance watermarking schemes to ensure their robustness and prevent forgery under unauthorized knowledge distillation.

**Abstract:** Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.

</details>


### [47] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)

*Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung*

**Main category:** cs.CL

**Keywords:** large language models, hallucination benchmark, taxonomy of hallucination, extrinsic evaluation, trust in AI

**Relevance Score:** 9

**TL;DR:** This paper addresses the issue of hallucinations in large language models (LLMs) by proposing a comprehensive benchmark featuring a clear taxonomy and new extrinsic evaluation tasks.

**Read time:** 42 min

<details>
  <summary>Details</summary>

**Motivation:** The proliferation of hallucinations in LLM outputs affects user trust and system adoption, making it crucial to develop methods to reliably benchmark and address this issue.

**Method:** The paper introduces a clear taxonomy that differentiates between extrinsic and intrinsic hallucinations, along with a new benchmark that allows for dynamic test set generation to ensure robustness against data leakage.

**Key Contributions:** A clear taxonomy distinguishing extrinsic from intrinsic hallucinations., Introduction of new extrinsic hallucination tasks with dynamic data generation., Comprehensive analysis of existing benchmarks highlighting their limitations.

**Result:** The proposed benchmark improves upon existing evaluations by providing consistent definitions and tools for measuring hallucinations, thus advancing research in LLM reliability.

**Limitations:** The study does not encompass all possible types of hallucinations and may require further validation in diverse contexts.

**Future Work:** Future research could explore the integration of this benchmark into practical LLM applications and extend the taxonomy to cover more nuanced hallucination types.

**Conclusion:** By establishing a unified framework for hallucination evaluation, the study aims to bolster the trustworthiness of LLMs and their acceptance in practical applications.

**Abstract:** Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.

</details>


### [48] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)

*Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki*

**Main category:** cs.CL

**Keywords:** language models, metadata, latent semantics, model performance, context awareness

**Relevance Score:** 8

**TL;DR:** Prepending metadata in language model pre-training can enhance performance for certain tasks but has mixed effects depending on context length.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effects of prepending metadata on language model performance across different tasks and understand the inconsistent results seen in prior research.

**Method:** The authors examined the impact of metadata in pre-training using artificial data generated by probabilistic context-free grammars, analyzing the performance of models on downstream tasks.

**Key Contributions:** Identification of mixed effects of metadata prepending on language model performance., Insights into context dependency for latent semantics inference., Empirical analysis using artificial data to support findings.

**Result:** The study found that prepending metadata can lead to both positive and negative outcomes, depending on whether the downstream task's prompt allows for inferring latent semantics based on context length.

**Limitations:** The study primarily relies on artificial data, which may not fully capture real-world complexities present in diverse language tasks.

**Future Work:** Further exploration into optimal metadata usage across various models and tasks to enhance language model performance.

**Conclusion:** The effectiveness of using metadata in pre-training is context-dependent; it enhances performance with longer prompts but may hinder it with shorter, less informative ones.

**Abstract:** The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.

</details>


### [49] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)

*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

**Main category:** cs.CL

**Keywords:** large language models, reasoning dataset, training methodology

**Relevance Score:** 9

**TL;DR:** This paper constructs a large-scale reasoning dataset to improve the training of large language models (LLMs), resulting in enhanced reasoning capabilities and state-of-the-art performance on a benchmark.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of base model training processes and data quality for large language models, which have shown remarkable performance but lack transparency.

**Method:** A large-scale, difficulty-graded reasoning dataset is created with 3.34 million queries and 40 million distilled responses. The dataset is refined based on pass rates and Coefficient of Variation to enhance training.

**Key Contributions:** Construction of a large-scale difficulty-graded reasoning dataset, Identification of the importance of higher learning rates for reasoning-focused training, Public release of datasets and methodologies for the research community

**Result:** The enhanced reasoning capabilities allowed the model to achieve a pass rate of 79.2% on the AIME2024 benchmark, surpassing most distilled models and nearing state-of-the-art performance.

**Limitations:** 

**Future Work:** Exploration of additional training methodologies and further improvements in reasoning capabilities using the released datasets.

**Conclusion:** The training method significantly improves reasoning capabilities and the dataset and methods are publicly available to support open-source LLM development.

**Abstract:** Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [50] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)

*Zhenkai Qin, Guifang Yang, Dongze Wu*

**Main category:** cs.CL

**Keywords:** rumor detection, natural language processing, deep learning, Chinese language, graph convolutional networks

**Relevance Score:** 8

**TL;DR:** RAGAT-Mind is a proposed model for effective Chinese rumor detection, achieving high accuracy through a multi-granular approach integrating various deep learning techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective rumor detection in the face of increasing false information on social media platforms.

**Method:** RAGAT-Mind combines TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of co-occurrence graphs.

**Key Contributions:** Introduction of RAGAT-Mind, a novel multi-granular modeling approach for rumor detection, Integration of multiple deep learning techniques for improved performance, Demonstrated high accuracy and macro-F1 score on a significant dataset

**Result:** RAGAT-Mind achieved 99.2% accuracy and a macro-F1 score of 0.9919 on the Weibo1-Rumor dataset, demonstrating superior classification performance.

**Limitations:** 

**Future Work:** Further explorations in adapting the model for other languages and broader applications in real-world scenarios.

**Conclusion:** The combination of hierarchical linguistic features and graph-based semantic structures enhances both generalization and interpretability for rumor detection.

**Abstract:** As false information continues to proliferate across social media platforms, effective rumor detection has emerged as a pressing challenge in natural language processing. This paper proposes RAGAT-Mind, a multi-granular modeling approach for Chinese rumor detection, built upon the MindSpore deep learning framework. The model integrates TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of word co-occurrence graphs. Experiments on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior classification performance, attaining 99.2% accuracy and a macro-F1 score of 0.9919. The results validate the effectiveness of combining hierarchical linguistic features with graph-based semantic structures. Furthermore, the model exhibits strong generalization and interpretability, highlighting its practical value for real-world rumor detection applications.

</details>


### [51] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)

*Samaneh Hosseini Moghaddam, Kelly Lyons, Cheryl Regehr, Vivek Goel, Kaitlyn Regehr*

**Main category:** cs.CL

**Keywords:** abusive language, online communication, taxonomy, classification, online abuse detection

**Relevance Score:** 8

**TL;DR:** This paper presents a hierarchical and faceted taxonomy of abusive language in online communications, aimed at enhancing classification and mitigation efforts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of abusive language in online communications poses risks to individuals and communities, highlighting the need for effective identification and management strategies.

**Method:** The study employs a systematic method for taxonomy development, integrating classification systems from 18 multi-label datasets to define characteristics of abusive language.

**Key Contributions:** Development of a comprehensive taxonomy for abusive language, Integration of 18 multi-label datasets for better classification, Facilitation of knowledge exchange among stakeholders in the field.

**Result:** The resulting taxonomy consists of 5 categories and 17 dimensions that detail aspects of online abuse, including context, target, intensity, and theme.

**Limitations:** 

**Future Work:** Further research could explore the application of this taxonomy in real-world moderation systems and its effectiveness in various contexts.

**Conclusion:** The proposed taxonomy fosters a shared understanding of abusive language, promoting collaborative efforts in detecting and mitigating online abuse.

**Abstract:** The proliferation of abusive language in online communications has posed significant risks to the health and wellbeing of individuals and communities. The growing concern regarding online abuse and its consequences necessitates methods for identifying and mitigating harmful content and facilitating continuous monitoring, moderation, and early intervention. This paper presents a taxonomy for distinguishing key characteristics of abusive language within online text. Our approach uses a systematic method for taxonomy development, integrating classification systems of 18 existing multi-label datasets to capture key characteristics relevant to online abusive language classification. The resulting taxonomy is hierarchical and faceted, comprising 5 categories and 17 dimensions. It classifies various facets of online abuse, including context, target, intensity, directness, and theme of abuse. This shared understanding can lead to more cohesive efforts, facilitate knowledge exchange, and accelerate progress in the field of online abuse detection and mitigation among researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [52] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)

*Zena Al-Khalili, Nick Howell, Dietrich Klakow*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Generation, Mathematical Reasoning, Evaluation Metrics, Grounding

**Relevance Score:** 9

**TL;DR:** This paper analyzes the effectiveness of code-assisted LLMs in solving math reasoning tasks through a detailed evaluation of their generated programs, highlighting the importance of grounding in mathematical rules.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation methods for code-assisted LLMs, which currently focus solely on execution correctness, by analyzing how well these models adhere to mathematical rules in their programs.

**Method:** The study evaluates the generated outputs of five different LLMs on two math datasets, employing both manual and automatic assessments to measure grounding in mathematical rules.

**Key Contributions:** Introduced a comprehensive evaluation framework for code-assisted LLMs in math tasks., Provided insights on the interaction between model types (open-source vs. closed-source) and their grounding abilities in mathematical reasoning., Highlighted the disparity in grounding effectiveness depending on problem complexity.

**Result:** The findings reveal that the success of grounding in generated programs varies between different LLMs and is influenced by problem difficulty; closed-source models demonstrate better grounding than open-source models, with a significant drop in grounded programs on a more challenging dataset.

**Limitations:** The study primarily focuses on a limited number of LLMs and two datasets, which may not represent the full landscape of code-assisted LLM capabilities.

**Future Work:** Future research should explore more diverse datasets and additional LLMs, as well as investigate methodologies to enhance grounding in open-source models.

**Conclusion:** A deeper evaluation approach is necessary to understand the capabilities and limitations of code-assisted LLMs in math problem-solving, moving beyond traditional evaluation metrics.

**Abstract:** Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs. In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs' generated programs in response to math reasoning tasks. Our evaluation focuses on the extent to which LLMs ground their programs to math rules, and how that affects their end performance. For this purpose, we assess the generations of five different LLMs, on two different math datasets, both manually and automatically. Our results reveal that the distribution of grounding depends on LLMs' capabilities and the difficulty of math problems. Furthermore, mathematical grounding is more effective for closed-source models, while open-source models fail to employ math rules in their solutions correctly. On MATH500, the percentage of grounded programs decreased to half, while the ungrounded generations doubled in comparison to ASDiv grade-school problems. Our work highlights the need for in-depth evaluation beyond execution accuracy metrics, toward a better understanding of code-assisted LLMs' capabilities and limits in the math domain.

</details>


### [53] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)

*Yuanchang Ye, Weiyan Wen*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, Visual Question Answering, Split Conformal Prediction, uncertainty quantification, hallucination mitigation

**Relevance Score:** 8

**TL;DR:** This paper introduces a Split Conformal Prediction (SCP) framework to mitigate hallucination in Large Vision-Language Models (LVLMs) for Visual Question Answering, ensuring robust uncertainty quantification and prediction reliability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the significant issue of hallucinations in LVLMs, particularly when applied in safety-critical scenarios like healthcare and autonomous systems.

**Method:** The proposed methodology utilizes a model-agnostic uncertainty quantification technique that combines dynamic threshold calibration and cross-modal consistency verification, with data partitioning into calibration and test sets to compute nonconformity scores for prediction sets.

**Key Contributions:** Introduction of a model-agnostic uncertainty quantification method in LVLMs for VQA tasks., Dynamic adjustment of prediction sets based on confidence levels, reducing hallucination occurrences., Rigorous control of empirical error rates below specified risk levels.

**Result:** The SCP framework demonstrates control of marginal coverage below user-defined risk levels, dynamic adjustment of prediction set sizes, and it eliminates prior distribution assumptions.

**Limitations:** The study may require further evaluation on diverse datasets and in various application domains to validate robustness.

**Future Work:** Future research could explore extending the SCP framework to other multi-modal AI applications and investigate its scalability in different real-world scenarios.

**Conclusion:** The findings indicate that the SCP framework effectively enforces theoretical guarantees and exhibits stability across varying calibration-to-test ratios, making it suitable for real-world applications.

**Abstract:** This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.

</details>


### [54] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)

*Jared Fernandez, Clara Na, Vashisth Tiwari, Yonatan Bisk, Sasha Luccioni, Emma Strubell*

**Main category:** cs.CL

**Keywords:** energy efficiency, large language models, inference optimization, natural language processing, sustainable AI

**Relevance Score:** 8

**TL;DR:** This paper analyzes the energy costs of inference optimizations in large language models (LLMs), highlighting their sensitivity to various factors and demonstrating that proper optimizations can significantly reduce energy consumption.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising computational and environmental costs associated with scaling large language models (LLMs) and to improve the benchmarking of inference efficiency across diverse workloads.

**Method:** The authors employ a modeling approach that approximates real-world LLM workflows using a binning strategy for input-output token distributions and variations in batch sizes, analyzing a wide range of software frameworks, decoding strategies, GPU architectures, and serving settings.

**Key Contributions:** Introduced a modeling approach for real-world LLM workflows, Identified the sensitivity of energy consumption to workload geometry and hardware configurations, Demonstrated significant energy savings from inference optimizations

**Result:** The study demonstrates that naive estimates of energy consumption based on FLOPs and GPU utilization tend to significantly underestimate actual energy use. Proper optimizations can reduce energy consumption by up to 73% compared to unoptimized workflows.

**Limitations:** The analysis is based on specific configurations and may not generalize across all possible NLP and AI workloads or hardware setups.

**Future Work:** Further investigation into additional optimization techniques and their applicability across various LLM deployment scenarios.

**Conclusion:** Insights from this research can help in developing sustainable LLM deployment and inform strategies for energy-efficient AI infrastructure design.

**Abstract:** As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.

</details>


### [55] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)

*Haru-Tada Sato, Fuka Matsuzaki, Jun-ichiro Takahashi*

**Main category:** cs.CL

**Keywords:** Ensemble Bayesian Inference, small language models, performance evaluation

**Relevance Score:** 8

**TL;DR:** This study introduces Ensemble Bayesian Inference (EBI), a method combining multiple small language models to achieve performance comparable to large language models.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To explore ways to achieve high performance using small language models by leveraging ensemble techniques, particularly in resource-constrained environments.

**Method:** The authors propose Ensemble Bayesian Inference (EBI), which uses Bayesian estimation to integrate outputs from multiple small language models, even those with negative Lift values, to enhance overall performance.

**Key Contributions:** Introduction of Ensemble Bayesian Inference (EBI), Demonstration of improved performance through negative Lift integration, Cross-linguistic effectiveness in model performance evaluation

**Result:** Experiments demonstrate that EBI significantly improves accuracy in various tasks, including aptitude assessments and consumer profile analysis in both Japanese and English.

**Limitations:** The study primarily focuses on the use of small language models and may not generalize to other model types or larger-scale systems.

**Future Work:** Further research could explore the scalability of EBI and its application to a wider variety of tasks and languages.

**Conclusion:** The findings indicate that small language model ensembles can effectively utilize models with lower individual performance to create high-efficiency AI systems.

**Abstract:** This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.

</details>


### [56] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)

*Cheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, Junfeng Fang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, safety risks, defense strategies, vulnerabilities, taxonomy

**Relevance Score:** 8

**TL;DR:** This paper surveys the safety risks, attacks, and defense strategies associated with Large Reasoning Models (LRMs), providing a structured taxonomy for better understanding and future research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the advancement of LRMs in tasks like mathematics and coding, concerns about their safety and vulnerabilities have intensified, necessitating a systematic exploration of these issues.

**Method:** The paper organizes safety risks, potential attacks, and defense strategies related to LRMs into a comprehensive taxonomy.

**Key Contributions:** Comprehensive taxonomy of safety risks and defense strategies for LRMs, Identification of newly emerged threats and vulnerabilities, Structured understanding of the safety landscape of LRMs

**Result:** The study identifies and categorizes various safety risks and attacks against LRMs, while also summarizing existing defense strategies to mitigate these risks.

**Limitations:** 

**Future Work:** Encourages further research into targeted defense mechanisms and continuous evaluation of LRM safety as their capabilities evolve.

**Conclusion:** A detailed taxonomy of LRM safety issues is presented to aid researchers and developers in enhancing the security and reliability of these models.

**Abstract:** Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.

</details>


### [57] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)

*Vansh Gupta, Sankalan Pal Chowdhury, Vilém Zouhar, Donya Rooein, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Educational Tasks, Non-English Languages, Performance Evaluation, Language Representation

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of large language models (LLMs) in educational tasks across multiple non-English languages and highlights the importance of verifying their suitability for specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the application of LLMs in educational contexts beyond English and assess their effectiveness in non-English languages.

**Method:** Evaluated the performance of popular LLMs on four educational tasks across six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) plus English.

**Key Contributions:** Evaluation of LLMs in six non-English languages for educational tasks, Quantitative analysis of model performance related to training data representation, Recommendations for practitioners regarding LLM deployment in non-English contexts

**Result:** Performance varies by language, correlating with the amount of training data available; lower-resource languages exhibit poorer performance, with significant drops compared to English.

**Limitations:** Lower-resource languages show poorer performance; the study does not address the underlying causes of this disparity.

**Future Work:** Further research is needed to improve LLM performance for lower-resource languages and to explore the factors affecting their effectiveness in educational tasks.

**Conclusion:** Before deploying LLMs in educational settings for non-English languages, practitioners should validate their performance for specific tasks in those languages.

**Abstract:** Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.

</details>


### [58] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)

*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula Allen-Meares, Eulalia Puig Abril, Olga Garcia, Carolyn Dickens, Andrew Boyd*

**Main category:** cs.CL

**Keywords:** Conversational Assistants, Healthcare, Large Language Models, Neurosymbolic Architecture, User Study

**Relevance Score:** 8

**TL;DR:** A user study compares a neurosymbolic conversational assistant and one based on ChatGPT for heart failure nutritional inquiries, revealing distinct strengths and weaknesses in their performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of conversational assistants in healthcare, particularly comparing traditional architectures with those based on generative AI like ChatGPT.

**Method:** A within-group user study involving heart failure patients comparing two versions of a conversational assistant: one using a neurosymbolic architecture and the other based on ChatGPT.

**Key Contributions:** Comparison of traditional vs. generative AI in healthcare conversational agents, Insight into patient interactions with AI systems, Identification of strengths and weaknesses of different AI architectures

**Result:** The in-house neurosymbolic system was more accurate and less verbose, while the ChatGPT system made fewer speech errors and required fewer clarifications; patients showed no preference.

**Limitations:** Study limited to heart failure patients and specific queries, may not generalize to all healthcare contexts or other conversational tasks.

**Future Work:** Further research is needed to explore other domains and enhance the architectures based on user preferences and specific healthcare needs.

**Conclusion:** Both systems have their advantages, suggesting that the choice of architecture may depend on specific use cases in healthcare delivery.

**Abstract:** Conversational assistants are becoming more and more popular, including in healthcare, partly because of the availability and capabilities of Large Language Models. There is a need for controlled, probing evaluations with real stakeholders which can highlight advantages and disadvantages of more traditional architectures and those based on generative AI. We present a within-group user study to compare two versions of a conversational assistant that allows heart failure patients to ask about salt content in food. One version of the system was developed in-house with a neurosymbolic architecture, and one is based on ChatGPT. The evaluation shows that the in-house system is more accurate, completes more tasks and is less verbose than the one based on ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors and requires fewer clarifications to complete the task. Patients show no preference for one over the other.

</details>


### [59] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)

*Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti*

**Main category:** cs.CL

**Keywords:** Sparse Attention, Transformer LLMs, Long-sequence Tasks, Efficiency-Accuracy Trade-offs, Scaling Laws

**Relevance Score:** 8

**TL;DR:** Sparse attention enhances Transformer LLMs' long-context capabilities but requires careful consideration of efficiency and accuracy trade-offs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the viability, efficiency-accuracy trade-offs, and systematic scaling of sparse attention methods in Transformer LLMs for long-sequence tasks.

**Method:** A comparison of training-free sparse attention methods at various model scales, sequence lengths, and sparsity levels across multiple long-sequence tasks.

**Key Contributions:** Careful comparison of sparse attention methods, Introduction of novel scaling laws for sparse attention, Insights into efficiency-accuracy trade-offs in long-context tasks

**Result:** 1) Larger, highly sparse models are preferable for very long sequences. 2) Higher sparsity is attainable during decoding than prefilling, correlating with model size. 3) No single best strategy for all tasks; moderate sparsity often degrades performance on some tasks. 4) Introduced novel scaling laws tailored for sparse attention.

**Limitations:** Sparse attention is not a universal solution; moderate sparsity can lead to performance degradation in certain tasks.

**Future Work:** Further exploration of sparse attention strategies across varied tasks and evaluation of models beyond the current experimental range.

**Conclusion:** Sparse attention enhances Transformer LLMs for long sequences but necessitates careful trade-off evaluations for performance-critical applications.

**Abstract:** Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.

</details>


### [60] [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)

*Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas*

**Main category:** cs.CL

**Keywords:** abstractive dialogue summarization, Transformer-based models, evaluation metrics

**Relevance Score:** 8

**TL;DR:** This paper reviews 1262 research papers on Transformer-based abstractive dialogue summarization, identifying key challenges and corresponding techniques, and assessing evaluation metrics and datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in comprehensive understanding and systematic review of the challenges in abstractive dialogue summarization and to align techniques and metrics with these challenges.

**Method:** A systematic review of 1262 unique research papers from Semantic Scholar and DBLP databases, focusing on the challenges of dialogue summarization and relevant techniques.

**Key Contributions:** Systematic review of 1262 papers on dialogue summarization, Identification of main challenges and aligned techniques, Assessment of datasets and evaluation metrics in the field

**Result:** The study highlights advancements in language challenges but notes that comprehension, factuality, and salience remain significant hurdles, with only a few datasets encompassing all subdomains.

**Limitations:** Limited attention to inner-annotator agreement and specific annotation guidelines in human evaluations.

**Future Work:** Further exploration of existing challenges and potential shifts due to large language models.

**Conclusion:** The established challenge taxonomy remains relevant despite the exploration of large language models, indicating ongoing opportunities for research in dialogue summarization.

**Abstract:** Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.

</details>


### [61] [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)

*Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure*

**Main category:** cs.CL

**Keywords:** large language models, music lyrics, synthetic text detection, unsupervised domain adaptation, AI-generated content

**Relevance Score:** 8

**TL;DR:** This paper investigates the generation and detection of AI-generated music lyrics, addressing copyright concerns and content quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the implications of using large language models (LLMs) in generating music lyrics and the associated challenges such as copyright issues and content spamming.

**Method:** Curated a diverse dataset of real and synthetic lyrics; assessed existing synthetic text detection methods; applied unsupervised domain adaptation for feature adaptation.

**Key Contributions:** Curated a novel dataset of lyrics from various languages and genres., Evaluated synthetic text detection methodologies specifically for lyrics., Proposed adaptations for performance improvements in multilingual and few-shot settings.

**Result:** Promising detection results for synthetic lyrics, indicating potential for informing policy and enhancing user transparency in AI-generated music.

**Limitations:** Focus is primarily on lyrics, leaving other music content types unexamined; constraints of data availability and quality may affect generalizability.

**Future Work:** Future research should explore other musical content types and further refine detection capabilities across diverse genres and languages.

**Conclusion:** The study indicates that adapting detection methods can effectively address challenges in recognizing AI-generated lyrics across different languages and genres.

**Abstract:** In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.

</details>


### [62] [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)

*Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** autonomous language models, inference efficiency, draft and verify, adaptive algorithms, token generation

**Relevance Score:** 9

**TL;DR:** The paper introduces OPT-Tree, an algorithm that improves inference efficiency in autoregressive language models by employing adaptive draft trees for token generation, resulting in significant speed-up compared to existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiency of autoregressive language models' one-step-one-word generation, especially as model sizes increase, highlighting the need for faster inference methods.

**Method:** The proposed OPT-Tree algorithm constructs adaptive draft trees that maximize the expected acceptance length during token generation, allowing multiple tokens to be generated in one step.

**Key Contributions:** Introduction of OPT-Tree for adaptive draft tree construction., Demonstrated speed-up of up to 3.2 times over traditional methods., Ability to generate multiple tokens in a single decoding step based on a robust draft model.

**Result:** Experimental results indicate that OPT-Tree outperforms existing draft structures, achieving a speed-up ratio of up to 3.2 times faster than traditional autoregressive decoding, with capabilities to generate over ten tokens per step if resources allow.

**Limitations:** The efficiency gain depends on the draft model's power and the available node budget for generating tokens.

**Future Work:** Future research could explore the application of OPT-Tree in different language tasks and further optimizations for even larger models.

**Conclusion:** OPT-Tree significantly enhances the efficiency of autoregressive models by optimizing the token generation process, offering a scalable solution for faster inference.

**Abstract:** Autoregressive language models demonstrate excellent performance in various scenarios. However, the inference efficiency is limited by its one-step-one-word generation mode, which has become a pressing problem recently as the models become increasingly larger. Speculative decoding employs a "draft and then verify" mechanism to allow multiple tokens to be generated in one step, realizing lossless acceleration. Existing methods mainly adopt fixed heuristic draft structures, which fail to adapt to different situations to maximize the acceptance length during verification. To alleviate this dilemma, we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft trees. It searches the optimal tree structure that maximizes the mathematical expectation of the acceptance length in each decoding step. Experimental results reveal that OPT-Tree outperforms the existing draft structures and achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding. If the draft model is powerful enough and the node budget is sufficient, it can generate more than ten tokens in a single step. Our code is available at https://github.com/Jikai0Wang/OPT-Tree.

</details>


### [63] [LaMsS: When Large Language Models Meet Self-Skepticism](https://arxiv.org/abs/2409.06601)

*Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji*

**Main category:** cs.CL

**Keywords:** large language models, self-skepticism, hallucination, question answering, AI performance

**Relevance Score:** 9

**TL;DR:** LaMsS introduces a new approach leveraging self-skepticism in LLMs to reduce hallucinations and improve accuracy in question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucination in LLMs which limits their applicability, using human-like skepticism to enhance model self-cognition and reflection.

**Method:** The approach incorporates skepticism tokens into the LLM's vocabulary, allowing it to gauge response skepticism and limit answers to a defined skepticism threshold during response generation.

**Key Contributions:** Proposed a novel LLM architecture integrating skepticism tokens., Demonstrated enhanced performance in answering questions compared to existing models., Provided insights into self-skepticism as a modeling technique for AI.

**Result:** LaMsS outperformed baseline models in accuracy, AUC, and AP metrics on multi-choice and open-domain question-answering tasks, demonstrating the ability to generalize across tasks.

**Limitations:** The applicability of self-skepticism may vary across different domains and tasks; further exploration is needed to fully understand its limitations.

**Future Work:** Expanding the understanding and implementation of self-skepticism in broader AI contexts and exploring its effects in various applications.

**Conclusion:** The implementation of self-skepticism in LLMs can significantly improve their performance and reliability, suggesting a new avenue in AI research.

**Abstract:** Hallucination is a major challenge for large language models (LLMs), preventing their further application in some fields. The skeptical thinking of humankind could be useful for LLMs to self-cognition, self-reflection and alleviate their hallucinations. Inspired by this consideration, we propose a novel approach called LaMsS, which combines the semantic understanding capability of LLMs with self-skepticism. By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning, which allow the LLM to decode each normal token followed by a skeptical token, representing different skepticism levels. By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold. By examining the accuracy, AUC and AP of willingly answering questions, we demonstrate that LaMsS achieves better performance than baselines on both multi-choice questions and open-domain question-answering benchmarks, and can generalize to multi-task and out-of-domain settings. Our study sheds some lights on the self-skepticism modeling on further artificial intelligence. Project code and model checkpoints can be found in https://anonymous.4open.science/r/SM-1E76.

</details>


### [64] [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)

*Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** Trust-Score, Trust-Align, LLMs, RAG, performance enhancement

**Relevance Score:** 8

**TL;DR:** This paper introduces Trust-Score and Trust-Align, metrics and methods for improving the performance of LLMs in retrieval-augmented generation (RAG) tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a gap in understanding the appropriateness of LLMs for the RAG task, despite the focus on end-to-end RAG system evaluations.

**Method:** Trust-Score is introduced as a holistic metric to evaluate the trustworthiness of LLMs in RAG frameworks, and Trust-Align is proposed to improve their alignment and performance.

**Key Contributions:** Introduction of Trust-Score as a metric for LLM trustworthiness in RAG tasks., Development of Trust-Align method that aligns LLMs for better performance., Empirical results showing significant performance enhancements across multiple models and tasks.

**Result:** Trust-Align leads to substantial performance improvements in 26 out of 27 models on ASQA, QAMPARI, and ELI5 tasks, showing specific gains in LLaMA-3-8b models over competitive baselines.

**Limitations:** 

**Future Work:** Future research could explore further refinements to the Trust-Align method and its applicability across additional tasks and models.

**Conclusion:** The study indicates that Trust-Align significantly enhances LLMs' performance, including their ability to refuse irrelevant inputs and provide quality citations.

**Abstract:** LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56), QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly enhances models' ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align.

</details>


### [65] [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)

*Xiaoyu Wang, Haoyong Ouyang, Balu Bhasuran, Xiao Luo, Karim Hanna, Mia Liza A. Lustria, Carl Yang, Zhe He*

**Main category:** cs.CL

**Keywords:** Lab-AI, personalized normal ranges, retrieval-augmented generation, patient portals, clinical medicine

**Relevance Score:** 10

**TL;DR:** Lab-AI is an interactive system that provides personalized normal ranges for lab tests using retrieval-augmented generation (RAG) considering patient-specific factors like age and gender.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate interpretation of lab results is crucial in clinical medicine, yet current patient portals often overlook individual factors, leading to potentially misleading normal ranges.

**Method:** Lab-AI consists of two modules: factor retrieval and normal range retrieval, utilizing GPT-4-turbo with RAG to personalize results based on patient-specific information.

**Key Contributions:** Introduction of Lab-AI for personalized lab result interpretation, Utilization of retrieval-augmented generation (RAG) for health insights, Significant performance improvements over existing non-RAG systems.

**Result:** The system achieved an F1 score of 0.948 for factor retrieval and 0.995 accuracy for normal range retrieval, significantly outperforming non-RAG systems.

**Limitations:** 

**Future Work:** Further exploration of additional conditional factors and integration of Lab-AI into clinical workflows.

**Conclusion:** Lab-AI demonstrates the potential to significantly improve patient understanding of lab results through personalized interpretations that consider individual differences.

**Abstract:** Accurate interpretation of lab results is crucial in clinical medicine, yet most patient portals use universal normal ranges, ignoring conditional factors like age and gender. This study introduces Lab-AI, an interactive system that offers personalized normal ranges using retrieval-augmented generation (RAG) from credible health sources. Lab-AI has two modules: factor retrieval and normal range retrieval. We tested these on 122 lab tests: 40 with conditional factors and 82 without. For tests with factors, normal ranges depend on patient-specific information. Our results show GPT-4-turbo with RAG achieved a 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5% in factor retrieval and showed 132% and 100% improvements in question-level and lab-level performance, respectively, for normal range retrieval. These findings highlight Lab-AI's potential to enhance patient understanding of lab results.

</details>


### [66] [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)

*Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an*

**Main category:** cs.CL

**Keywords:** extremely low-resource languages, machine translation, grammar books, linguistic tasks, parallel examples

**Relevance Score:** 8

**TL;DR:** This paper demonstrates that using a grammar book with parallel examples improves translation for extremely low-resource languages, highlighting the need for task-appropriate data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, necessitating the use of available resources like dictionaries and grammar books.

**Method:** The study investigates the impact of grammar books on XLR language translation, specifically analyzing English-Kalamang and comparing improvements from parallel examples versus grammatical explanations. It also fine-tunes an encoder-decoder translation model based on the findings.

**Key Contributions:** Demonstrates the effective use of grammar books for XLR language translation, Establishes that parallel examples are more beneficial than grammatical explanations, Suggests refined data collection strategies for multilingual XLR tasks

**Result:** The improvements in translation ability were primarily due to parallel examples rather than grammatical content, with fine-tuning leading to performance on par with long-context LLMs.

**Limitations:** The study does not explore the potential of grammatical explanations beyond translation tasks, limiting its findings to specific scenarios.

**Future Work:** Future research should investigate methods to better leverage grammatical knowledge in NLP tasks and explore more XLR languages.

**Conclusion:** The research advises that for multilingual XLR translation tasks, data collection should prioritize parallel examples over linguistic descriptions, as grammatical explanations did not enhance the models' effectiveness.

**Abstract:** Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book's parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.

</details>


### [67] [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)

*Danqing Wang, Jianxin Ma, Fei Fang, Lei Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Strategies, TypedThinker, Problem Solving, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** TypedThinker enhances LLM reasoning by incorporating diverse reasoning types, leading to significant performance improvements in problem-solving tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs primarily rely on deductive reasoning, which limits their ability to solve certain problems that require diverse reasoning strategies such as inductive, abductive, or analogical reasoning.

**Method:** The paper proposes TypedThinker, which predicts suitable reasoning types for specific problems based on their previous effectiveness and provides relevant demonstrations to assist LLMs in applying these strategies.

**Key Contributions:** Introduction of TypedThinker for diverse reasoning strategies, Demonstrated improvements in reasoning performance across multiple benchmarks, Proposed integration into advanced LLM systems for enhanced problem-solving capabilities

**Result:** TypedThinker resulted in performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B across logical and mathematical reasoning tasks.

**Limitations:** 

**Future Work:** Further investigations into the adaptability of TypedThinker for a wider range of reasoning tasks and different LLM architectures.

**Conclusion:** The integration of TypedThinker into LLMs enhances their reasoning capabilities without the need for knowledge distillation from larger models.

**Abstract:** Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning, proceeding step-by-step from given conditions, which limits their exploration during problem-solving. Our analysis reveals that certain problems are exclusively solvable through specific reasoning strategies like inductive, abductive, or analogical reasoning. However, incorporating diverse reasoning approaches presents two key challenges: identifying the appropriate reasoning type for each problem and exploiting this approach during problem-solving. Therefore, we propose the TypedThinker that predicts suitable reasoning types based on the problem and their previous effectiveness and provides relevant demonstrations to guide LLMs in applying these strategies. Experimental results show significant improvements across multiple benchmarks, with performance gains of 3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and mathematical reasoning tasks. TypedThinker enhances LLM reasoning without requiring knowledge distillation from larger models. It can be integrated into more advanced systems like GPT-4o or specialized models like MetaMath to diversify their reasoning approaches and improve their problem-solving capabilities.

</details>


### [68] [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)

*Yaniv Leviathan, Matan Kalman, Yossi Matias*

**Main category:** cs.CL

**Keywords:** Selective Attention, Transformer Models, Language Modeling

**Relevance Score:** 8

**TL;DR:** Selective Attention is a parameter-free modification to standard attention mechanisms that enhances performance by reducing focus on irrelevant elements, thus improving language modeling and reducing memory and compute requirements during inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the degradation of performance caused by unneeded elements in the attention's context and to improve efficiency in transformer models.

**Method:** Selective Attention is introduced as a simple, parameter-free change to the existing attention mechanism that systematically reduces attention to unnecessary elements.

**Key Contributions:** Introduction of Selective Attention which enhances transformer models, Demonstrated significant improvements in memory efficiency, Achieved competitive performance with reduced computation requirements.

**Result:** Selective Attention consistently improves performance in language modeling and downstream tasks across various model sizes and context lengths, achieving results comparable to standard transformers with significantly more heads and parameters.

**Limitations:** 

**Future Work:** Further exploration of how selective attention can be broadly applied across different architectures and tasks.

**Conclusion:** The implementation of Selective Attention results in meaningful reductions in memory and compute requirements, allowing for smaller context sizes without loss of performance.

**Abstract:** Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.

</details>


### [69] [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)

*Tunazzina Islam, Dan Goldwasser*

**Main category:** cs.CL

**Keywords:** climate change, microtargeting, large language models, fairness analysis, social media

**Relevance Score:** 8

**TL;DR:** This study analyzes microtargeting strategies in climate change communication on social media using large language models (LLMs) to assess their effectiveness and fairness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand how microtargeting in climate campaigns on social media can effectively reach specific demographic groups and the implications it has for fairness and transparency.

**Method:** The study employs LLMs to analyze Facebook advertisements, focusing on two main aspects: predicting demographic targets and assessing fairness in model predictions.

**Key Contributions:** Demonstration of LLMs in analyzing microtargeting strategies in climate campaigns., Identification of biases in demographic targeting by LLMs., Framework for enhancing transparency and accountability in social media climate communication.

**Result:** The LLMs achieved an accuracy of 88.55% in predicting demographic targets. The analysis revealed that young adults are targeted through activism-related messages, while women are engaged via caregiving themes. However, biases were identified, particularly in predicting senior citizens and male audiences.

**Limitations:** The study acknowledges certain biases in model predictions regarding specific demographic groups, particularly senior citizens and males.

**Future Work:** The authors suggest future research should focus on enhancing the fairness and inclusivity of LLM predictions in social media campaigns.

**Conclusion:** The paper demonstrates the potential of LLMs to analyze microtargeted messaging, while also highlighting the need for addressing biases in AI predictions.

**Abstract:** Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.

</details>


### [70] [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)

*Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang*

**Main category:** cs.CL

**Keywords:** Parameter-Efficient Fine-Tuning, large models, computational resources

**Relevance Score:** 8

**TL;DR:** This paper reviews Parameter-Efficient Fine-Tuning (PEFT) methods to adapt large pre-trained models to specific tasks, addressing the challenges of computational resources and storage costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The unprecedented scale of large models leads to high computational and storage costs, making fine-tuning on limited hardware challenging.

**Method:** This review introduces the fundamentals of PEFT, covering various algorithms, their core principles, applications, and future research directions.

**Key Contributions:** Introduction of PEFT concepts and principles, Review of various PEFT algorithms, Discussion of future research directions in PEFT

**Result:** PEFT offers a practical approach to efficiently fine-tune large models while minimizing additional parameter introduction and resource usage.

**Limitations:** 

**Future Work:** Further exploration of PEFT applications and potential enhancements in methodologies.

**Conclusion:** The review believes that by understanding PEFT, interested parties can accelerate its development and innovation in adapting large models.

**Abstract:** The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.

</details>


### [71] [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)

*Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober*

**Main category:** cs.CL

**Keywords:** text classification, sequential correlation, thematic continuity, hypothesis-testing framework, false positives

**Relevance Score:** 8

**TL;DR:** This paper presents a data-centric hypothesis-testing framework to evaluate the impact of sequentially correlated literary properties on textual classification, revealing that traditional supervised and neural models are prone to false positives due to thematic similarities, while unsupervised models with traditional features show better reliability.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a principled approach for assessing the influence of thematic continuity on textual classification and reduce false positives in classification tasks stemming from sequential correlations.

**Method:** The framework models label sequences as stochastic processes, using an empirical autocovariance matrix to create surrogate labelings that maintain sequential dependencies, allowing for statistical testing to compare the impact of thematic structure versus non-sequential features.

**Key Contributions:** Introduces a hypothesis-testing framework for analyzing sequential dependencies in text classification., Demonstrates that traditional supervised models are more susceptible to false positives due to thematic similarities., Establishes that unsupervised models with traditional features can achieve high true positive rates with minimal false positives.

**Result:** The analysis indicates that supervised and neural models often mistake thematic similarities for stylistic signals, leading to higher false positives, while unsupervised models with traditional features achieve better classification reliability in genre-consistent settings.

**Limitations:** The framework may need further validation across additional literary genres or languages to ascertain its generalizability.

**Future Work:** Future research could explore the application of this framework in more diverse linguistic contexts and integrate additional features to enhance classification robustness.

**Conclusion:** Controlling for sequential correlation is crucial for improving the accuracy of classification tasks and ensuring outcomes reflect authentic stylistic distinctions rather than confounded thematic correlations.

**Abstract:** We introduce a data-centric hypothesis-testing framework to quantify the influence of sequentially correlated literary properties--such as thematic continuity--on textual classification tasks. Our method models label sequences as stochastic processes and uses an empirical autocovariance matrix to generate surrogate labelings that preserve sequential dependencies. This enables statistical testing to determine whether classification outcomes are primarily driven by thematic structure or by non-sequential features like authorial style. Applying this framework across a diverse corpus of English prose, we compare traditional (word n-grams and character k-mers) and neural (contrastively trained) embeddings in both supervised and unsupervised classification settings. Crucially, our method identifies when classifications are confounded by sequentially correlated similarity, revealing that supervised and neural models are more prone to false positives--mistaking shared themes and cross-genre differences for stylistic signals. In contrast, unsupervised models using traditional features often yield high true positive rates with minimal false positives, especially in genre-consistent settings. By disentangling sequential from non-sequential influences, our approach provides a principled way to assess and interpret classification reliability. This is particularly impactful for authorship attribution, forensic linguistics, and the analysis of redacted or composite texts, where conventional methods may conflate theme with style. Our results demonstrate that controlling for sequential correlation is essential for reducing false positives and ensuring that classification outcomes reflect genuine stylistic distinctions.

</details>


### [72] [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)

*Andreas Koukounas, Georgios Mastrapas, Sedigheh Eslami, Bo Wang, Mohammad Kalim Akram, Michael Günther, Isabelle Mohr, Saba Sturua, Nan Wang, Han Xiao*

**Main category:** cs.CL

**Keywords:** CLIP, multilingual, text-only retrieval, crossmodal, vision-language model

**Relevance Score:** 8

**TL;DR:** jina-clip-v2 is a multilingual contrastive vision-language model designed for both text-only and crossmodal tasks, outperforming existing models in several benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing CLIP models that primarily optimize for crossmodal tasks and suffer from poor performance in single-mode text tasks, as well as their lack of multilingual understanding and inadequate handling of visually rich documents.

**Method:** The model is trained using a multi-task and multi-stage contrastive learning paradigm on text pairs, triplets, and image-text pairs, incorporating a multilingual text encoder and a diverse training dataset with texts in 29 non-English languages.

**Key Contributions:** Introduction of a multilingual text encoder, Training on a diverse dataset from 29 non-English languages, Significant performance improvements in text-only and crossmodal retrieval tasks

**Result:** jina-clip-v2 shows notable improvements in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks, outperforming state-of-the-art CLIP-based models in both English and multilingual contexts.

**Limitations:** The paper does not address potential biases in the multilingual data or the generalizability of the model across all languages and document types.

**Future Work:** Further research could explore improvements in reducing biases, extending the training data to more languages, and enhancing the model's ability to process various types of documents.

**Conclusion:** The enhancements provided by jina-clip-v2 make it a strong candidate for applications requiring robust multilingual capabilities and flexible representation dimensionality.

**Abstract:** Contrastive Language-Image Pretraining (CLIP) has been widely used for crossmodal information retrieval and multimodal understanding tasks. However, CLIP models are mainly optimized for crossmodal vision-language tasks and underperform in single-mode text tasks. Moreover, these models are often trained on English datasets and therefore lack multilingual understanding. Additionally, from a visual understanding perspective, previous CLIP-based models exhibit insufficient understanding of visually rich documents. In this work, we propose jina-clip-v2, a contrastive vision-language model trained on text pairs, triplets and image-text pairs via a multi-task and multi-stage contrastive learning paradigm in order to support both text-only and crossmodal tasks. We employ a multilingual text encoder and expand the training dataset to include multilingual texts from 29 non-English languages, including Hindi, Chinese, German, French, and others, as well as images of visually rich documents. We evaluate the model's performance and show that jina-clip-v2 achieves notable improvements over state-of-the-art CLIP-based models in zero-shot text-only retrieval, semantic textual similarity, and crossmodal retrieval tasks in both English and multilingual settings. jina-clip-v2 also provides for flexibility in embedding dimensionality, enabling users to select the granularity of the representations. jina-clip-v2 is publicly available at https://huggingface.co/jinaai/jina-clip-v2.

</details>


### [73] [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)

*David Boldo, Lily Pemberton, Gabriel Thistledown, Jacob Fairchild, Felix Kowalski*

**Main category:** cs.CL

**Keywords:** contextual embeddings, gradient optimization, large language models, natural language processing, computational efficiency

**Relevance Score:** 8

**TL;DR:** The paper presents a Context-Aware Neural Gradient Mapping framework integrating contextual embeddings into large language model optimization, enhancing task-specific generalization with dynamic gradient adjustments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve optimization processes in large language models by incorporating contextual embeddings, particularly in challenging conditions such as sparse or noisy data.

**Method:** The framework introduces a dynamic gradient adjustment mechanism, utilizing contextual embeddings derived from a supplementary neural network to tailor training gradients, while employing differential geometry to manage high-dimensional gradient dependencies.

**Key Contributions:** Introduction of dynamic gradient adjustments in language models, Utilization of differential geometry for low-dimensional gradient encoding, Demonstrated improvements in model performance under noise and sparsity

**Result:** Empirical evaluations reveal that the framework outperforms baseline models, achieving higher accuracy, robustness to noise, and computational efficiency across various metrics.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship, limiting access to complete findings.

**Future Work:** Further exploration of contextual embedding integration techniques and their applications across diverse NLP tasks is suggested.

**Conclusion:** The integration of context-specific embeddings enhances language understanding and offers scalable computational efficiency for large-scale language models.

**Abstract:** The integration of contextual embeddings into the optimization processes of large language models is an advancement in natural language processing. The Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient adjustment mechanism, incorporating contextual embeddings directly into the optimization process. This approach facilitates real-time parameter adjustments, enhancing task-specific generalization even in the presence of sparse or noisy data inputs. The mathematical foundation of this framework relies on gradient descent modifications, where contextual embeddings are derived from a supplementary neural network trained to map input features to optimal adaptation gradients. By employing differential geometry principles, high-dimensional input dependencies are encoded into low-dimensional gradient manifolds, enabling efficient adaptation without necessitating the retraining of the entire model. Empirical evaluations demonstrate that the proposed framework consistently outperforms baseline models across various metrics, including accuracy, robustness to noise, and computational efficiency. The integration of context-specific embeddings allows for a more complex understanding of language, thereby improving the model's ability to handle diverse linguistic phenomena. Furthermore, the computational efficiency achieved through this method demonstrates its scalability for large-scale language models operating under diverse constraints.

</details>


### [74] [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)

*Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha*

**Main category:** cs.CL

**Keywords:** State Space Models, Indic languages, Question answering, Natural language processing, Multilingual scenarios

**Relevance Score:** 8

**TL;DR:** This paper explores using State Space Models (SSMs) for question answering in Indic languages, demonstrating improved performance in handling their complexities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The diversity and complexity of Indic languages create challenges for natural language processing tasks, especially in question answering.

**Method:** The authors applied multiple SSM architectures to various datasets representing different Indic languages and conducted a comparative performance analysis.

**Key Contributions:** First application of SSMs to QA in Indic languages, Establishment of foundational benchmarks for future research, Proposed optimizations for low-resource settings

**Result:** The models effectively captured linguistic subtleties, resulting in significant improvements in question interpretation, context alignment, and answer generation.

**Limitations:** 

**Future Work:** Enhancements to SSM frameworks to better serve low-resource settings and multilingual scenarios in Indic languages.

**Conclusion:** This study is the first to apply SSMs to QA tasks in Indic languages and sets a benchmark for future research, proposing enhancements for low-resource and multilingual scenarios.

**Abstract:** The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.

</details>


### [75] [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)

*Christopher Nightingale, Dominic Lavington, Jonathan Thistlethwaite, Sebastian Penhaligon, Thomas Belinski, David Boldo*

**Main category:** cs.CL

**Keywords:** Probabilistic Embeddings, Attention Mechanisms, Contextual Inference

**Relevance Score:** 8

**TL;DR:** The paper introduces probabilistic embeddings for token representations, enhancing contextual inference and robustness while reducing redundancy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the flexibility and coherence of token embeddings in natural language processing by leveraging probability distributions over learned manifolds.

**Method:** The authors propose using probabilistic subspaces within attention mechanisms to achieve adaptive contextual weighting and improve the structural coherence of token relationships across different contexts.

**Key Contributions:** Introduction of probabilistic embeddings for token representations, Improved neighborhood consistency and reduced redundancy in embeddings, Enhanced robustness against adversarial attacks

**Result:** Experimental results show improved neighborhood consistency, reduced redundancy, increased robustness against adversarial modifications, and greater adaptability in domain-specific applications.

**Limitations:** Computational trade-offs include marginal increases in inference latency, but are within operationally feasible limits.

**Future Work:** Further exploration of applications in diverse linguistic domains and optimization of computational efficiency for real-time implementations.

**Conclusion:** Probabilistic representations enhance semantic granularity and maintain contextual integrity, making them advantageous for generative modeling tasks requiring coherence over extended sequences.

**Abstract:** Representing token embeddings as probability distributions over learned manifolds allows for more flexible contextual inference, reducing representational rigidity while enhancing semantic granularity. Comparative evaluations demonstrate that probabilistic embeddings improve neighborhood consistency and decrease redundancy, ensuring that token relationships remain more structurally coherent across fine-tuning iterations. The integration of probabilistic subspaces within attention mechanisms facilitates more adaptive contextual weighting, enabling models to capture latent dependencies that would otherwise be obscured in conventional embeddings. Experimental results highlight increased robustness against adversarial modifications, with probabilistic embeddings preserving contextual integrity even under perturbation-based evaluation scenarios. Performance assessments indicate that probabilistic representations achieve greater adaptability in domain-specific applications, mitigating the need for extensive retraining when shifting across linguistic domains. Computational trade-offs remain within operationally feasible limits, with marginal increases in inference latency balanced against the benefits of enhanced representation stability and contextual expressiveness. The capacity to encode structured uncertainty provides advantages in generative modeling tasks, particularly where maintaining coherence across extended sequences requires a representation framework capable of handling ambiguous or context-dependent linguistic constructs.

</details>


### [76] [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)

*Gaurav Srivastava, Shuxiang Cao, Xuan Wang*

**Main category:** cs.CL

**Keywords:** small language models, reasoning performance, model evaluation, benchmarking, adversarial robustness

**Relevance Score:** 9

**TL;DR:** This paper systematically surveys the reasoning abilities of small language models (SLMs) across various benchmarks, challenging the notion that only large language models (LLMs) can perform well in reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether small language models (SLMs) can achieve competitive reasoning performance similar to large language models (LLMs) despite their smaller size.

**Method:** The authors analyzed 72 SLMs from six model families across 14 reasoning benchmarks, employing four evaluation methods and comparing LLM judges to human evaluations on 800 data points. The experiments were repeated three times to ensure robustness.

**Key Contributions:** Systematic evaluation of 72 SLMs across various reasoning benchmarks., Demonstrated that smaller models can achieve competitive reasoning capabilities compared to larger ones., Provided insights into effective prompting strategies and the robustness of SLMs under adversarial conditions.

**Result:** The findings reveal that SLMs can achieve strong reasoning capabilities, undermining the belief that scalability is the only path to effective reasoning; structured training and post-training compression can also produce efficient models.

**Limitations:** The paper primarily focuses on 72 SLMs, which may not include all relevant models or architectures that could display different performance characteristics.

**Future Work:** Further exploration into structured training methods and post-training compression techniques to enhance the reasoning abilities of SLMs is encouraged.

**Conclusion:** SLMs can be developed with robust reasoning abilities and present a viable alternative to LLMs for reasoning-intensive tasks, thus broadening the potential applications of language models.

**Abstract:** Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.

</details>


### [77] [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)

*Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen*

**Main category:** cs.CL

**Keywords:** Conversational Product Search, PSCon dataset, human-like language, cross-market, multi-lingual

**Relevance Score:** 8

**TL;DR:** This paper introduces PSCon, a new conversational product search dataset that supports human-like language interactions across multiple markets and languages, enabling comprehensive research on various subtasks related to conversational product search.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of a real CPS dataset with human-like language limits the development and evaluation of conversational product search systems, especially for cross-market and multi-lingual applications.

**Method:** We propose a CPS data collection protocol and create the PSCon dataset using a coached human-human data collection approach, which caters to dual markets and two languages.

**Key Contributions:** Introduction of the PSCon dataset for conversational product search, Support for cross-market and multi-lingual datasets, Comprehensive task formulation enabling in-depth research on six subtasks.

**Result:** The PSCon dataset facilitates research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation, along with a benchmark model for evaluation.

**Limitations:** The dataset's applicability may still be limited to the specific markets and languages it covers, and further expansion is needed for broader analyses.

**Future Work:** Future research should focus on expanding the dataset to more languages and markets, as well as enhancing the model's capabilities in a real-world setting.

**Conclusion:** The dataset and the proposed benchmark model provide a foundation for future research in conversational product search systems, addressing current gaps in existing data and methodologies.

**Abstract:** Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.

</details>


### [78] [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)

*Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, peer review, automated evaluation, OpenReview, evaluation pipeline

**Relevance Score:** 8

**TL;DR:** Automated evaluation of LLMs in peer review reveals significant shortcomings compared to expert reviewers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in peer review, such as reviewer shortages and increasing workloads, by assessing the capability of Large Language Models (LLMs) in generating paper reviews.

**Method:** Developed an automatic evaluation pipeline to compare LLM-generated reviews with expert reviews using a dataset of 676 OpenReview papers, focusing on strength and weakness identifications.

**Key Contributions:** Development of an automatic evaluation pipeline for LLMs, Empirical analysis of LLM review capabilities using expert comparisons, Identification of key weaknesses in LLM-generated reviews

**Result:** LLMs showed a lack of balanced perspectives, significantly overlooked novelty assessments, and produced poor acceptance decisions compared to expert reviews.

**Limitations:** The study only analyzes LLM reviews against expert reviews from a specific dataset, which may not represent all scientific fields.

**Future Work:** Exploration of methods to enhance LLM review quality and addressing their identified weaknesses for better integration in the peer review process.

**Conclusion:** The automated evaluation pipeline facilitates a scalable way to assess and improve LLMs' review capabilities over time.

**Abstract:** Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.

</details>


### [79] [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)

*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Seid Muhie Yimam, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine De Kock, Tadesse Destaw Belay, Ibrahim Said Ahmad, Nirmal Surange, Daniela Teodorescu, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino Ali, Vladimir Araujo, Abinew Ali Ayele, Oana Ignat, Alexander Panchenko, Yi Zhou, Saif M. Mohammad*

**Main category:** cs.CL

**Keywords:** emotion detection, multilingual, low-resource languages, machine learning, shared task

**Relevance Score:** 8

**TL;DR:** A shared task on text-based emotion detection covering 30+ languages, with over 700 participants and findings on emotion classification and intensity detection.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the understanding of emotion detection in low-resource languages across various linguistic backgrounds.

**Method:** Participants engaged in three tracks: multilabel emotion detection, emotion intensity score detection, and cross-lingual emotion detection, with data collected from more than 30 languages.

**Key Contributions:** Introduction of a shared task for low-resource language emotion detection, Comprehensive dataset spanning multiple languages and emotional classes, Engagement of 700+ participants fostering collaboration in the field.

**Result:** Final submissions were received from over 200 teams, with baseline results presented that highlight the best-performing systems and common approaches.

**Limitations:** The challenge mainly involves low-resource languages, which may affect generalizability of findings.

**Future Work:** Future research can explore more sophisticated models and methods for better accuracy in emotion detection across diverse languages.

**Conclusion:** The datasets are publicly available for further research in emotion detection tasks.

**Abstract:** We present our shared task on text-based emotion detection, covering more than 30 languages from seven distinct language families. These languages are predominantly low-resource and are spoken across various continents. The data instances are multi-labeled with six emotional classes, with additional datasets in 11 languages annotated for emotion intensity. Participants were asked to predict labels in three tracks: (a) multilabel emotion detection, (b) emotion intensity score detection, and (c) cross-lingual emotion detection.   The task attracted over 700 participants. We received final submissions from more than 200 teams and 93 system description papers. We report baseline results, along with findings on the best-performing systems, the most common approaches, and the most effective methods across different tracks and languages. The datasets for this task are publicly available. The dataset is available at SemEval2025 Task 11 https://brighter-dataset.github.io

</details>


### [80] [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)

*Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, transformer-based hypernetwork, concept features, hidden states, RAVEL benchmark

**Relevance Score:** 8

**TL;DR:** HyperDAS introduces a transformer-based hypernetwork that enhances mechanistic interpretability by automatically locating token positions and constructing concept features, achieving state-of-the-art performance in interpreting neural networks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mechanistic interpretability of neural networks by efficiently identifying and utilizing concept features without brute force searching in hidden state representations.

**Method:** HyperDAS employs a transformer-based hypernetwork to automatically identify relevant token positions in hidden states and construct meaningful features of residual stream vectors related to specific concepts.

**Key Contributions:** Introduction of HyperDAS, a novel transformer-based hypernetwork architecture., Automatic identification of token positions in the residual stream for concept realization., Demonstration of state-of-the-art performance on the RAVEL benchmark.

**Result:** HyperDAS demonstrates state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states, outperforming traditional methods.

**Limitations:** The method assumes that the hypernetwork's design choices effectively mitigate the risks of introducing new information, which requires further validation.

**Future Work:** Exploration of HyperDAS's applicability to other model architectures and larger datasets, as well as further investigations into interpretability concerns.

**Conclusion:** The design of HyperDAS alleviates potential concerns regarding injecting new information into the model, enhancing interpretability without sacrificing fidelity.

**Abstract:** Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts(e.g., the birth year of a person) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) constructs features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it.

</details>


### [81] [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)

*Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg*

**Main category:** cs.CL

**Keywords:** token embeddings, geometric structure, intrinsic dimension, language models, interpretability

**Relevance Score:** 8

**TL;DR:** This paper investigates the geometric structure of token embeddings in language models, uncovering shared global and local properties, and introduces a method for transferring representations between models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the commonalities in geometric structures of token embeddings across different language models and improve interpretability in applications.

**Method:** The study employs Locally Linear Embeddings to analyze local geometry and introduces a measure of intrinsic dimension for token embeddings, revealing that they lie on a lower-dimensional manifold.

**Key Contributions:** Identification of global similarities in token embeddings across language models., Characterization of local geometry using intrinsic dimensions revealing lower-dimensional structures., Development of the Emb2Emb method for transferring representations between models.

**Result:** Findings indicate that token embeddings exhibit similar relative orientations, and those with lower intrinsic dimensions form semantically coherent clusters while higher dimensions do not; alignment persists through hidden states.

**Limitations:** The study focuses on token embeddings and may not generalize to all components of language models or other architectures.

**Future Work:** Further investigation into the implications of shared geometric structures on model performance and the development of more sophisticated transfer methods for different model types.

**Conclusion:** The research suggests that the shared geometric properties of token embeddings can enhance representation transfer capabilities in language models, leading to improved interpretability methods like Emb2Emb.

**Abstract:** Researchers have recently suggested that models share common representations. In our work, we find that token embeddings of language models exhibit common geometric structure. First, we find ``global'' similarities: token embeddings often share similar relative orientations. Next, we characterize local geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a simple measure for the intrinsic dimension of each token embedding. Our intrinsic dimension demonstrates that token embeddings lie on a lower dimensional manifold. We qualitatively show that tokens with lower intrinsic dimensions often have semantically coherent clusters, while those with higher intrinsic dimensions do not. Both characterizations allow us to find similarities in the local geometry of token embeddings. Perhaps most surprisingly, we find that alignment in token embeddings persists through the hidden states of language models, allowing us to develop an application for interpretability. Namely, we introduce Emb2Emb, a simple method to transfer steering vectors from one language model to another, despite the two models having different dimensions.

</details>


### [82] [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)

*Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, memory mechanisms, efficiency, context-rich responses, hallucinations

**Relevance Score:** 9

**TL;DR:** The paper analyzes memory mechanisms in Large Language Models, classifying them into sensory, short-term, and long-term memory to enhance context-awareness and reduce inaccuracies.

**Read time:** 37 min

<details>
  <summary>Details</summary>

**Motivation:** To improve context-rich responses, lessen hallucinations, and enhance the efficiency of Large Language Models through effective memory utilization.

**Method:** The paper categorizes memory types and discusses acquisition, management, and utilization methods, including KV cache-based and parameter-based strategies for memory handling.

**Key Contributions:** Categorization of memory into sensory, short-term, and long-term types., In-depth analysis of text-based and KV cache-based memory mechanisms., Examination of parameter-based and hidden-state-based memory methods for model efficiency.

**Result:** A comprehensive understanding of LLM memory mechanisms is provided, highlighting their roles in improving model efficiency and text processing capabilities.

**Limitations:** 

**Future Work:** Further exploration of advanced memory management strategies and their impact on model performance in practical applications.

**Conclusion:** Effective memory mechanisms are crucial for enhancing the performance of Large Language Models, and there is a significant scope for further research in this field.

**Abstract:** This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

</details>


### [83] [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)

*Aravind Krishnan, Siva Reddy, Marius Mosbach*

**Main category:** cs.CL

**Keywords:** machine unlearning, large language models, privacy, evaluation methods, knowledge frequency

**Relevance Score:** 9

**TL;DR:** This paper explores the challenges of machine unlearning in large language models, discovering that the frequency of knowledge in training data impacts unlearning effectiveness and revealing issues with current evaluation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to unlearn knowledge from specific data points in large language models (LLMs) for privacy reasons, particularly concerning named entities.

**Method:** The authors investigate the impact of the frequency of knowledge in pre-training data on the unlearning process and compare probability versus generation-based evaluation methods.

**Key Contributions:** Identification of frequency dependence in LLM unlearning, Demonstration of misalignment in evaluation methods for unlearning, Recommendations for novel unlearning methods considering training data

**Result:** The study finds that more frequent knowledge is harder to unlearn and identifies a misalignment between different unlearning evaluation methods, particularly in larger models.

**Limitations:** The study primarily focuses on unlearning related to named entities and may not generalize to all types of knowledge.

**Future Work:** Future research should explore broader types of knowledge unlearning and develop universally applicable evaluation frameworks.

**Conclusion:** There is a critical need for improved evaluation practices and new methods for unlearning that consider training data characteristics.

**Abstract:** Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.

</details>


### [84] [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)

*Alessio Tosolini, Claire Bowern*

**Main category:** cs.CL

**Keywords:** multilingual training, crosslingual training, acoustic models, Australian languages, phonological inventories

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of multilingual vs. crosslingual training for Australian languages with shared phonological characteristics, highlighting the advantages of adapting an existing English model for new languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the training outcomes of multilingual and crosslingual methods in acoustic modeling for lesser-studied Australian languages, focusing on phonological similarities.

**Method:** The study employs the Montreal Forced Aligner to create acoustic models from scratch and adapts a large English model, assessing performance using various datasets including seen and unseen languages and data.

**Key Contributions:** Demonstrated effectiveness of crosslingual training for related languages., Highlighted advantages of adapting English acoustic models for Australian languages., Provided insights on unseen language processing in multilingual settings.

**Result:** The findings reveal that adapting the English baseline model yields better results for previously unseen languages compared to training from scratch or using unrelated models.

**Limitations:** 

**Future Work:** Further research could explore the scalability of this approach to a broader set of languages and dialects, including more extensive data collections.

**Conclusion:** Adapting existing English models significantly benefits the recognition of new languages, indicating a potential strategy for expanding language models in acoustic training.

**Abstract:** We compare the outcomes of multilingual and crosslingual training for related and unrelated Australian languages with similar phonological inventories. We use the Montreal Forced Aligner to train acoustic models from scratch and adapt a large English model, evaluating results against seen data, unseen data (seen language), and unseen data and language. Results indicate benefits of adapting the English baseline model for previously unseen languages.

</details>


### [85] [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)

*Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** data distillation, large language models, text generation, training efficiency, instruction tuning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel data distillation method for large language models (LLMs) that synthesizes pseudo prompt data to enhance training efficiency and transferability across architectures, demonstrating superior performance over existing data selection methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing size of large language models leads to higher training costs, necessitating a method to minimize data size while maintaining training effectiveness.

**Method:** The proposed method involves learning pseudo prompt data through trajectory matching and identifying nearest neighbor IDs, supplemented by a regularization loss to enhance data robustness during the distillation process.

**Key Contributions:** Introduction of a novel data distillation method for LLM training in NLP., Demonstration of improved performance over the existing state-of-the-art data selection method LESS., Establishment of transferability of distilled data across different LLM architectures.

**Result:** Evaluations on ARC-Easy and MMLU instruction tuning datasets show that this distillation method outperforms the state-of-the-art data selection method, LESS, and exhibits strong transferability across different LLM architectures such as OPT and Llama.

**Limitations:** The exploration is limited to specific datasets (ARC-Easy and MMLU) and may require further validation across more diverse text generation tasks.

**Future Work:** Future research could investigate the application of this method on a broader range of NLP tasks and explore additional data distillation techniques.

**Conclusion:** This work is the first to apply data distillation to text generation tasks, achieving effective instruction tuning with reduced data requirements.

**Abstract:** In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama).

</details>


### [86] [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)

*Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng*

**Main category:** cs.CL

**Keywords:** geometric reasoning, multimodal large language models, GeoSense, benchmark, AI reasoning

**Relevance Score:** 9

**TL;DR:** GeoSense is a novel bilingual benchmark for evaluating the geometric reasoning abilities of multimodal large language models (MLLMs) with a focus on human-like reasoning mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical gap in assessing the human-like geometric reasoning capacities of MLLMs, as existing benchmarks do not evaluate both visual comprehension and symbolic reasoning together.

**Method:** GeoSense features a hierarchical framework of five levels of geometric principles, along with an annotated dataset of 1,789 problems and innovative evaluation strategies to test MLLMs on geometric reasoning tasks.

**Key Contributions:** Introduction of GeoSense, a bilingual benchmark for geometric reasoning in MLLMs., Development of a hierarchical framework for geometric principles., An annotated dataset of 1,789 geometric problems.

**Result:** Extensive experiments show that Gemini-2.0-pro-flash performs the best on the GeoSense benchmark with a score of 65.3, revealing that the understanding and application of geometric principles is a major limitation in current MLLMs' reasoning ability.

**Limitations:** The identification and application of geometric principles are still bottlenecks for leading MLLMs.

**Future Work:** Future advancements should focus on enhancing MLLMs' capabilities in understanding and applying geometric principles effectively.

**Conclusion:** GeoSense has the potential to improve future research on MLLMs' geometric reasoning capabilities, facilitating the development of more human-like reasoning in AI.

**Abstract:** Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSense's potential to guide future advancements in MLLMs' geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence.

</details>


### [87] [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)

*Jiliang Ni, Jiachen Pu, Zhongyi Yang, Kun Zhou, Hui Wang, Xiaoliang Xiao, Dakui Wang, Xin Li, Jingfeng Luo, Conggang Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Model Compression, Knowledge Transfer, Cost Efficiency

**Relevance Score:** 8

**TL;DR:** This paper presents a three-stage cost-efficient deployment pipeline for Large Language Models (LLMs) that aims to optimize performance and reduce latency and costs through model compression techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the high costs and latency associated with deploying large language models in NLP systems, aiming to improve performance while maintaining efficiency.

**Method:** The proposed methodology consists of three stages: prototyping an optimal performance system, transferring knowledge to a smaller student model through techniques like rejection fine-tuning and knowledge distillation, and compressing the model using quantization and pruning techniques.

**Key Contributions:** Introduction of a three-stage pipeline for LLM deployment, Effective knowledge transfer techniques to reduce model size and costs, Demonstration of model compression through quantization and pruning

**Result:** The framework leads to a super tiny model that achieves effective performance with reduced latency and costs, demonstrating modular design and potential for cross-domain applicability.

**Limitations:** The approach may require additional validation in varied NLP applications to confirm effectiveness across different contexts.

**Future Work:** Future research could explore enhancing the robustness of the compressed models and evaluating their performance in diverse NLP tasks.

**Conclusion:** The approach effectively tackles the cost-performance dilemma in LLM deployment in NLP systems, providing a simplified architecture for future applications.

**Abstract:** In recent years, Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) pipelines, improving performance and generalization. This has spurred their integration into various systems. Many NLP systems, including ours, employ a "one-stage" pipeline directly incorporating LLMs. While effective, this approach incurs substantial costs and latency due to the need for large model parameters to achieve satisfactory outcomes. This paper introduces a three-stage cost-efficient end-to-end LLM deployment pipeline-including prototyping, knowledge transfer, and model compression-to tackle the cost-performance dilemma in LLM-based frameworks. Our approach yields a super tiny model optimized for cost and performance in online systems, simplifying the system architecture. Initially, by transforming complex tasks into a function call-based LLM-driven pipeline, an optimal performance prototype system is constructed to produce high-quality data as a teacher model. The second stage combines techniques like rejection fine-tuning, reinforcement learning, and knowledge distillation to transfer knowledge to a smaller 0.5B student model, delivering effective performance at minimal cost. The final stage applies quantization and pruning to extremely compress models to 0.4B, achieving ultra-low latency and cost. The framework's modular design and cross-domain capabilities suggest potential applicability in other NLP areas.

</details>


### [88] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)

*Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou*

**Main category:** cs.CL

**Keywords:** PHD-Transformer, length scaling, KV cache management, sliding window attention, language models

**Relevance Score:** 8

**TL;DR:** The PHD-Transformer framework enhances length scaling during pre-training of language models while ensuring inference efficiency through innovative KV cache management.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of length scaling during pre-training of language models, which has been underutilized compared to post-training.

**Method:** The PHD-Transformer introduces a KV cache management strategy that differentiates between original tokens and hidden decoding tokens, retaining only the KV cache of original tokens for long-range dependencies, and introducing two variants to optimize performance: PHD-SWA and PHD-CSWA.

**Key Contributions:** Introduction of the PHD-Transformer for efficient length scaling during pre-training, Innovative KV cache management strategy, Two performance-optimizing variants: PHD-SWA and PHD-CSWA

**Result:** Extensive experiments show consistent improvements in performance across multiple benchmarks with the new framework and its variants.

**Limitations:** 

**Future Work:** Further exploration of length scaling techniques and their applications in various language model tasks.

**Conclusion:** The proposed PHD-Transformer effectively enables length scaling during pre-training without compromising inference efficiency, offering a promising direction for future research in language model optimization.

**Abstract:** Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

</details>


### [89] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)

*Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang*

**Main category:** cs.CL

**Keywords:** multimodal language analysis, multimodal large language models, benchmark, human conversational semantics, cognitive-level semantics

**Relevance Score:** 8

**TL;DR:** This paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) on cognitive-level semantics, highlighting current limitations in performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in research regarding the capability of MLLMs to comprehend cognitive-level semantics in human conversations.

**Method:** The study introduces the MMLA benchmark, comprising over 61K multimodal utterances across six dimensions, and evaluates eight branches of LLMs and MLLMs using zero-shot inference, supervised fine-tuning, and instruction tuning.

**Key Contributions:** Introduction of the MMLA benchmark for multimodal language analysis., Exhaustive evaluation of eight LLMs and MLLMs on cognitive-level semantics., Identification of current limitations in multimodal model performance.

**Result:** The evaluation shows that even fine-tuned models reach only 60%-70% accuracy, revealing significant limitations in current MLLMs' understanding of complex language.

**Limitations:** The models evaluated achieve only moderate accuracy (60%-70%), indicating room for improvement.

**Future Work:** Future research should delve into enhancing MLLM capabilities to better comprehend complex human language and expand on the MMLA framework.

**Conclusion:** MMLA provides a foundational resource for advancing research in multimodal language analysis and understanding the capabilities of large language models in this domain.

**Abstract:** Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.

</details>
