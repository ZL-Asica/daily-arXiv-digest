# 2025-06-23

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 34]

- [cs.CL](#cs.CL) [Total: 116]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study](https://arxiv.org/abs/2506.15834)

*Zachary D King, Maryam Khalid, Han Yu, Kei Shibuya, Khadija Zanna, Marzieh Majd, Ryan L Brown, Yufei Shen, Thomas Vaessen, George Kypriotakis, Christopher P Fagundes, Akane Sano*

**Main category:** cs.HC

**Keywords:** mHealth, Ecological Momentary Assessment, machine learning, emotion recognition, response likelihood

**Relevance Score:** 8

**TL;DR:** This study proposes a machine learning-based approach to optimize the timing of Ecological Momentary Assessments (EMAs) in mHealth applications, aiming to improve emotional data collection by focusing on underrepresented emotions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve EMA compliance and emotional data collection in mHealth studies, particularly for emotions that are less commonly reported.

**Method:** A multi-objective function is proposed which utilizes machine learning to determine optimal times for sending EMAs by combining predicted response likelihood with model uncertainty in emotion predictions.

**Key Contributions:**

	1. Multi-objective function for EMA timing optimization
	2. Combination of response likelihood and model uncertainty
	3. Broader emotional data collection in mHealth contexts

**Result:** The results demonstrate that the objective function leads to higher EMA responses during instances when participants report less commonly observed emotions.

**Limitations:** Evaluation conducted offline with limited datasets; further real-world testing needed.

**Conclusion:** The proposed method could enhance receptivity rates and diversify emotional data captured in mHealth research.

**Abstract:** Mobile health (mHealth) systems help researchers monitor and care for patients in real-world settings. Studies utilizing mHealth applications use Ecological Momentary Assessment (EMAs), passive sensing, and contextual features to develop emotion recognition models, which rely on EMA responses as ground truth. Due to this, it is crucial to consider EMA compliance when conducting a successful mHealth study. Utilizing machine learning is one approach that can solve this problem by sending EMAs based on the predicted likelihood of a response. However, literature suggests that this approach may lead to prompting participants more frequently during emotions associated with responsiveness, thereby narrowing the range of emotions collected. We propose a multi-objective function that utilizes machine learning to identify optimal times for sending EMAs. The function identifies optimal moments by combining predicted response likelihood with model uncertainty in emotion predictions. Uncertainty would lead the function to prioritize time points when the model is less confident, which often corresponds to underrepresented emotions. We demonstrate that this objective function would result in EMAs being sent when participants are responsive and experiencing less commonly observed emotions. The evaluation is conducted offline using two datasets: (1) 91 spousal caregivers of individuals with Alzheimer's Disease and Related dementias (ADRD), (2) 45 healthy participants. Results show that the multi-objective function tends to be higher when participants respond to EMAs and report less commonly observed emotions. This suggests that using the proposed objective function to guide EMA delivery could improve receptivity rates and capture a broader range of emotions.

</details>


### [2] [DeckFlow: Iterative Specification on a Multimodal Generative Canvas](https://arxiv.org/abs/2506.15873)

*Gregory Croisdale, Emily Huang, John Joon Young Chung, Anhong Guo, Xu Wang, Austin Z. Henley, Cyrus Omar*

**Main category:** cs.HC

**Keywords:** Generative AI, DeckFlow, multimodal, creative media, task decomposition

**Relevance Score:** 8

**TL;DR:** DeckFlow is a multimodal generative AI tool designed to address fundamental design problems in existing generative AI tools by enabling task decomposition, specification workflows, and generative space exploration in creative media generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Generative AI has the potential to empower individuals in creating personalized media, but current tools face design shortcomings that limit their effectiveness.

**Method:** The paper introduces DeckFlow, which features an infinite canvas for task decomposition, facilitates iterative specification workflows, and allows for generative space exploration through multiple variations of outputs.

**Key Contributions:**

	1. Introduction of DeckFlow multimodal tool for media generation
	2. Support for task and specification decomposition workflows
	3. Ability to generate and explore multiple variations iteratively

**Result:** DeckFlow was evaluated for text-to-image generation against a baseline, showing its capability in handling complex generative tasks involving text, image, and audio outputs in a creative context.

**Limitations:** The evaluation is limited to specific use cases and does not encompass all potential user scenarios for generative media.

**Conclusion:** The findings suggest that DeckFlow enhances user engagement and creativity in media generation by addressing key limitations of existing generative AI tools.

**Abstract:** Generative AI promises to allow people to create high-quality personalized media. Although powerful, we identify three fundamental design problems with existing tooling through a literature review. We introduce a multimodal generative AI tool, DeckFlow, to address these problems. First, DeckFlow supports task decomposition by allowing users to maintain multiple interconnected subtasks on an infinite canvas populated by cards connected through visual dataflow affordances. Second, DeckFlow supports a specification decomposition workflow where an initial goal is iteratively decomposed into smaller parts and combined using feature labels and clusters. Finally, DeckFlow supports generative space exploration by generating multiple prompt and output variations, presented in a grid, that can feed back recursively into the next design iteration. We evaluate DeckFlow for text-to-image generation against a state-of-practice conversational AI baseline for image generation tasks. We then add audio generation and investigate user behaviors in a more open-ended creative setting with text, image, and audio outputs.

</details>


### [3] [Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration](https://arxiv.org/abs/2506.15883)

*Jonathan Zong, Isabella Pedraza Pineros, Mengzhu Katie Chen, Daniel Hajas, Arvind Satyanarayan*

**Main category:** cs.HC

**Keywords:** semantic scaffolding, data visualization, large language models, blind and low-vision users, human-computer interaction

**Relevance Score:** 8

**TL;DR:** The paper introduces semantic scaffolding, a technique to help lay readers understand data by using domain-specific information from LLMs to reveal meaningful groupings and meanings within datasets, demonstrated through an accessible tool called Olli.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding new datasets can be challenging for lay readers who may not have the domain expertise to interpret complex visual patterns or data fields.

**Method:** The authors developed semantic scaffolding to identify and explain semantically meaningful data groupings, presenting these through semantic bins and data highlights in a tool named Olli.

**Key Contributions:**

	1. Development of semantic scaffolding using LLMs for data interpretation
	2. Creation of Olli, an accessible visualization tool
	3. Evaluation of its effectiveness with blind and low-vision users

**Result:** A study involving 15 blind and low-vision users showed that semantic scaffolds helped users quickly grasp the meanings of data, although they were mindful of how these scaffolds influenced their interpretations.

**Limitations:** The study involved a limited sample size of 15 participants, which may affect the generalizability of the results.

**Conclusion:** Semantic scaffolding can enhance comprehension of data for users, but awareness of its influence on interpretation remains a critical consideration.

**Abstract:** Drawing connections between interesting groupings of data and their real-world meaning is an important, yet difficult, part of encountering a new dataset. A lay reader might see an interesting visual pattern in a chart but lack the domain expertise to explain its meaning. Or, a reader might be familiar with a real-world concept but struggle to express it in terms of a dataset's fields. In response, we developed semantic scaffolding, a technique for using domain-specific information from large language models (LLMs) to identify, explain, and formalize semantically meaningful data groupings. We present groupings in two ways: as semantic bins, which segment a field into domain-specific intervals and categories; and data highlights, which annotate subsets of data records with their real-world meaning. We demonstrate and evaluate this technique in Olli, an accessible visualization tool that exemplifies tensions around explicitly defining groupings while respecting the agency of readers to conduct independent data exploration. We conducted a study with 15 blind and low-vision (BLV) users and found that readers used semantic scaffolds to quickly understand the meaning of the data, but were often also critically aware of its influence on their interpretation.

</details>


### [4] [ChatAR: Conversation Support using Large Language Model and Augmented Reality](https://arxiv.org/abs/2506.16008)

*Yuichiro Fujimoto*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Large Language Models, Conversational Dynamics, Eye Movement, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This study presents a real-time support system for enhancing conversations using AR and LLMs, addressing issues of eye movement visibility while reading displayed information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve communication where knowledge differences hinder smooth conversations, integrating AR technology with LLMs can provide contextual support without revealing user actions to conversation partners.

**Method:** The study developed an HMD-based AR system that recognizes keywords and uses LLMs to generate and present relevant information invisibly to conversation partners. It includes methods to manage user eye movements during dialogue.

**Key Contributions:**

	1. Integration of AR and LLMs for conversation support
	2. Methodology to manage eye movement visibility during conversations
	3. Demonstrated improvements in user engagement and conversation dynamics

**Result:** The first experiment showed a reduction in the likelihood that conversation partners notice when users read the AR text. The second experiment indicated improved user engagement, with a balanced speech ratio and higher perceived excitement in conversations.

**Limitations:** The feasibility of real-time implementation in various conversational contexts is yet to be fully assessed.

**Conclusion:** The proposed AR support system effectively aids real-time communication while mitigating the visibility of information display to conversation partners, enhancing conversational dynamics.

**Abstract:** Engaging in smooth conversations with others is a crucial social skill. However, differences in knowledge between conversation participants can sometimes hinder effective communication. To tackle this issue, this study proposes a real-time support system that integrates head-mounted display (HMD)-based augmented reality (AR) technology with large language models (LLMs). This system facilitates conversation by recognizing keywords during dialogue, generating relevant information using the LLM, reformatting it, and presenting it to the user via the HMD. A significant issue with this system is that the user's eye movements may reveal to the conversation partner that they are reading the displayed text. This study also proposes a method for presenting information that takes into account appropriate eye movements during conversation. Two experiments were conducted to evaluate the effectiveness of the proposed system. The first experiment revealed that the proposed information presentation method reduces the likelihood of the conversation partner noticing that the user is reading the displayed text. The second experiment demonstrated that the proposed method led to a more balanced speech ratio between the user and the conversation partner, as well as a increase in the perceived excitement of the conversation.

</details>


### [5] [SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion](https://arxiv.org/abs/2506.16010)

*Xiangyang He, Jiale Li, Jiahao Chen, Yang Yang, Mingming Fan*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Machine Learning, Multi-Agent Systems, Panel Discussions, Immersive Learning

**Relevance Score:** 8

**TL;DR:** SimuPanel simulates academic panel discussions using LLM-based multi-agent interaction to enhance accessibility and engagement in learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Panel discussions provide diverse perspectives but are often inaccessible due to various constraints, necessitating a solution that can simulate these discussions for wider access.

**Method:** The paper presents SimuPanel, a framework using LLMs to simulate panel discussions with a host-expert architecture and immersive 3D visualizations, allowing users to interact and engage with dynamic discussions.

**Key Contributions:**

	1. Introduction of SimuPanel for simulating panel discussions.
	2. Development of a multi-agent interaction framework for dynamic recall and contribution.
	3. User study showing enhanced engagement and reflection in discussions.

**Result:** SimuPanel successfully simulates authentic panel dynamics, showing improved depth of discussion and participant engagement compared to traditional dialogue generation.

**Limitations:** Limited to academic scenarios; further work needed to generalize to other domains.

**Conclusion:** The findings suggest promising pathways for enhancing panel discussions in multimedia learning, alongside implications for future design.

**Abstract:** Panel discussion allows the audience to learn different perspectives through interactive discussions among experts moderated by a host and a Q&A session with the audience. Despite its benefits, panel discussion in the real world is inaccessible to many who do not have the privilege to participate due to geographical, financial, and time constraints. We present SimuPanel, which simulates panel discussions among academic experts through LLM-based multi-agent interaction. It enables users to define topics of interest for the panel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel employs a host-expert architecture where each panel member is simulated by an agent with specialized expertise, and the panel is visualized in an immersive 3D environment to enhance engagement. Traditional dialogue generation struggles to capture the depth and interactivity of real-world panel discussions. To address this limitation, we propose a novel multi-agent interaction framework that simulates authentic panel dynamics by modeling reasoning strategies and personas of experts grounded in multimedia sources. This framework enables agents to dynamically recall and contribute to the discussion based on past experiences from diverse perspectives. Our technical evaluation and the user study with university students show that SimuPanel was able to simulate more in-depth discussions and engage participants to interact with and reflect on the discussions. As a first step in this direction, we offer design implications for future avenues to improve and harness the power of panel discussion for multimedia learning.

</details>


### [6] [Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications](https://arxiv.org/abs/2506.16044)

*MH Farhadi, Ali Rabiee, Sima Ghafoori, Anna Cetera, Wei Xu, Reza Abiri*

**Main category:** cs.HC

**Keywords:** Shared Autonomy, Human-Machine Teaming, Biosignal Processing, Assistive Robotics, Large Language Models

**Relevance Score:** 9

**TL;DR:** A review of human-centered shared autonomy AI frameworks in healthcare, focusing on upper limb biosignal-based machine interfaces and their integration for collaborative human-AI systems.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-machine teaming in healthcare by integrating AI with human intent, particularly through shared autonomy.

**Method:** Comprehensive review and analysis of existing AI frameworks, motor planning, learning, and control systems in healthcare applications, with a focus on human factors and biosignal processing.

**Key Contributions:**

	1. Framework for integrating human intent in AI decision-making
	2. Analysis of biosignal processing techniques for intent detection
	3. Recommendations for future directions in human-AI collaboration

**Result:** Identified effective approaches and challenges in implementing shared autonomy systems in rehabilitation and assistive robotics, highlighting the role of LLMs in future frameworks.

**Limitations:** 

**Conclusion:** Adaptive shared autonomy AI can enhance collaborative systems in healthcare, bridging the gap between neuroscientific insights and robotics.

**Abstract:** With recent advancements in AI and computational tools, intelligent paradigms have emerged to enhance fields like shared autonomy and human-machine teaming in healthcare. Advanced AI algorithms (e.g., reinforcement learning) can autonomously make decisions to achieve planning and motion goals. However, in healthcare, where human intent is crucial, fully independent machine decisions may not be ideal. This chapter presents a comprehensive review of human-centered shared autonomy AI frameworks, focusing on upper limb biosignal-based machine interfaces and associated motor control systems, including computer cursors, robotic arms, and planar platforms. We examine motor planning, learning (rehabilitation), and control, covering conceptual foundations of human-machine teaming in reach-and-grasp tasks and analyzing both theoretical and practical implementations. Each section explores how human and machine inputs can be blended for shared autonomy in healthcare applications. Topics include human factors, biosignal processing for intent detection, shared autonomy in brain-computer interfaces (BCI), rehabilitation, assistive robotics, and Large Language Models (LLMs) as the next frontier. We propose adaptive shared autonomy AI as a high-performance paradigm for collaborative human-AI systems, identify key implementation challenges, and outline future directions, particularly regarding AI reasoning agents. This analysis aims to bridge neuroscientific insights with robotics to create more intuitive, effective, and ethical human-machine teaming frameworks.

</details>


### [7] [From 600 Tools to 1 Console: A UX-Driven Transformation](https://arxiv.org/abs/2506.16107)

*Mariann Kornelia Smith, Jacqueline Meijer-Irons, Andrew Millar*

**Main category:** cs.HC

**Keywords:** User Experience, Infrastructure Tools, Productivity, Research Design, Tool Consolidation

**Relevance Score:** 4

**TL;DR:** Explores the improvement of Google's internal infrastructure tools for developers through user-centered research and design methodologies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address fragmentation and inefficiency in Google's internal infrastructure tools which hinder developer productivity.

**Method:** Utilized user-centered research techniques to create a story map and a service blueprint to visualize tool relationships.

**Key Contributions:**

	1. Created a story map to visualize application relationships
	2. Developed a strategic vision for tool consolidation
	3. Secured executive buy-in for improvements

**Result:** Formulated a strategic vision to consolidate tools and streamline workflows, leading to secured executive buy-in and delivered incremental improvements.

**Limitations:** 

**Conclusion:** Incremental improvements were made to enhance the efficiency of internal tools based on user-centered design.

**Abstract:** In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a survey to 10,000 Google Developers (Googlers) and uncovered that Google's internal infrastructure tools were fragmented and inefficient, hindering developers' productivity. Using user centered research and design methodologies the team first created a story map and service blueprint to visualize the relationship between internal applications, then formulated a strategic vision to consolidate tools, streamline workflows, and measure the impact of their work. We secured executive buy-in and delivered incremental improvements.

</details>


### [8] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)

*Thomas Barbera, Jacopo Burger, Alessandro D'Amelio, Simone Zini, Simone Bianco, Raffaella Lanzarotti, Paolo Napoletano, Giuseppe Boccignone, Jose Luis Contreras-Vidal*

**Main category:** cs.HC

**Keywords:** brain-computer interfaces, EEG, artificial intelligence, machine learning, BCIoT

**Relevance Score:** 8

**TL;DR:** This paper explores the advancements in AI applied to brain-computer interfaces (BCIs) using EEG, addressing challenges and future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With recent breakthroughs in AI, particularly in understanding brain signals, there is a possibility to develop practical brain-computer interfaces that could enhance communication and interaction with technology.

**Method:** The paper reviews the current state of EEG-based BCIs, factoring in challenges from a causal perspective, and identifies key research avenues to overcome existing limitations.

**Key Contributions:**

	1. Guided exploration of EEG-based BCI advancements using AI.
	2. Identification of unique challenges faced in real-world applications.
	3. Proposed roadmap for future research in EEG-based BCI solutions.

**Result:** The paper provides a roadmap for the development of effective EEG-based BCI solutions, outlining current endeavors and proposing future directions.

**Limitations:** The complexities of building reliable foundational models hinder practical applications.

**Conclusion:** To realize the potential of EEG-based BCIs, we need to navigate the technological, methodological, and ethical challenges through clear, principled research pathways.

**Abstract:** Imagine unlocking the power of the mind to communicate, create, and even interact with the world around us. Recent breakthroughs in Artificial Intelligence (AI), especially in how machines "see" and "understand" language, are now fueling exciting progress in decoding brain signals from scalp electroencephalography (EEG). Prima facie, this opens the door to revolutionary brain-computer interfaces (BCIs) designed for real life, moving beyond traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a Brain-to-Internet of Things (BCIoT).   However, the journey is not as straightforward as it was for Computer Vision (CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based BCIs, particularly in building powerful foundational models, presents unique and intricate hurdles that could affect their reliability.   Here, we unfold a guided exploration of this dynamic and rapidly evolving research area. Rather than barely outlining a map of current endeavors and results, the goal is to provide a principled navigation of this hot and cutting-edge research landscape. We consider the basic paradigms that emerge from a causal perspective and the attendant challenges presented to AI-based models. Looking ahead, we then discuss promising research avenues that could overcome today's technological, methodological, and ethical limitations. Our aim is to lay out a clear roadmap for creating truly practical and effective EEG-based BCI solutions that can thrive in everyday environments.

</details>


### [9] [Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)](https://arxiv.org/abs/2506.16199)

*Mohammad Naiseh, Huseyin Dogan, Stephen Giff, Nan Jiang*

**Main category:** cs.HC

**Keywords:** Explainable AI, User Experience, UX Research, Trust in AI, Human-Centered Design

**Relevance Score:** 8

**TL;DR:** The paper proposes a UX Research Playbook for Explainable AI (XAI) to support UX professionals in designing user-friendly AI interfaces.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by UX professionals in creating effective XAI interfaces that are understandable for non-expert users.

**Method:** Development of a UX Research (UXR) Playbook that provides actionable guidance for designing accessible, transparent, and trustworthy AI experiences.

**Key Contributions:**

	1. Introduction of a UX Research Playbook for XAI
	2. Guidance for UX designers to improve AI interaction transparency
	3. Strategies to facilitate user understanding of complex AI systems

**Result:** The playbook offers strategies that bridge the gap between technical explanation methods and user-centered design, fostering better understanding and trust in AI.

**Limitations:** 

**Conclusion:** The framework enhances the capabilities of UX designers to produce more transparent and responsible AI interactions.

**Abstract:** Explainable Artificial Intelligence (XAI) plays a critical role in fostering user trust and understanding in AI-driven systems. However, the design of effective XAI interfaces presents significant challenges, particularly for UX professionals who may lack technical expertise in AI or machine learning. Existing explanation methods, such as SHAP, LIME, and counterfactual explanations, often rely on complex technical language and assumptions that are difficult for non-expert users to interpret. To address these gaps, we propose a UX Research (UXR) Playbook for XAI - a practical framework aimed at supporting UX professionals in designing accessible, transparent, and trustworthy AI experiences. Our playbook offers actionable guidance to help bridge the gap between technical explainability methods and user centred design, empowering designers to create AI interactions that foster better understanding, trust, and responsible AI adoption.

</details>


### [10] [When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard](https://arxiv.org/abs/2506.16312)

*Angxuan Chen*

**Main category:** cs.HC

**Keywords:** Learning Analytics Dashboard, Explainable AI, Self-Regulated Learning, Academic Writing, Human-AI Collaboration

**Relevance Score:** 7

**TL;DR:** The study evaluates the effectiveness of an explainable Learning Analytics Dashboard in enhancing students' understanding of AI-supported academic writing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of explainable AI in improving students' conceptual understanding during collaborative academic writing tasks.

**Method:** An experimental study was conducted where participants were divided into two groups: one used an explainable Learning Analytics Dashboard and the other a visual-only dashboard.

**Key Contributions:**

	1. Demonstrated the importance of explainable AI in educational settings.
	2. Revealed significant improvements in conceptual understanding with the use of explainable dashboards.
	3. Located foundational support for self-regulated learning theories in AI-assisted tasks.

**Result:** The explainable LAD group scored higher on a knowledge test related to abstract writing principles, indicating better conceptual understanding, though no significant difference in the quality of abstracts was noted.

**Limitations:** No significant difference in the overall quality of the academic abstracts was found, which may limit the conclusions drawn regarding the effectiveness of the explainable LAD.

**Conclusion:** Explainable feedback is essential for deeper learning and skill development in self-regulated learning contexts, despite basic AI feedback being adequate for task completion.

**Abstract:** This study investigated the impact of a theory-driven, explainable Learning Analytics Dashboard (LAD) on university students' human-AI collaborative academic abstract writing task. Grounded in Self-Regulated Learning (SRL) theory and incorporating Explainable AI (XAI) principles, our LAD featured a three-layered design (Visual, Explainable, Interactive). In an experimental study, participants were randomly assigned to either an experimental group (using the full explainable LAD) or a control group (using a visual-only LAD) to collaboratively write an academic abstract with a Generative AI. While quantitative analysis revealed no significant difference in the quality of co-authored abstracts between the two groups, a significant and noteworthy difference emerged in conceptual understanding: students in the explainable LAD group demonstrated a superior grasp of abstract writing principles, as evidenced by their higher scores on a knowledge test (p= .026). These findings highlight that while basic AI-generated feedback may suffice for immediate task completion, the provision of explainable feedback is crucial for fostering deeper learning, enhancing conceptual understanding, and developing transferable skills fundamental to self-regulated learning in academic writing contexts.

</details>


### [11] [Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation](https://arxiv.org/abs/2506.16345)

*Guilherme Guerino, Luiz Rodrigues, Bruna Capeleti, Rafael Ferreira Mello, André Freire, Luciana Zaina*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Heuristic Evaluation, Large Language Models, GPT-4o, Usability

**Relevance Score:** 9

**TL;DR:** This study compares the heuristic evaluation performance of GPT-4o with human experts, revealing GPT-4o's strengths and limitations in identifying usability issues in web systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the performance of GPT-4o in heuristic evaluation compared to HCI experts.

**Method:** Heuristic evaluations were conducted by both GPT-4o and human experts on selected screenshots from a web system using Nielsen's Heuristics.

**Key Contributions:**

	1. Comparison of GPT-4o and human expert evaluations in HCI
	2. Identification of strengths and weaknesses of GPT-4o in heuristic evaluations
	3. Recommendations for using GPT-4o in practical heuristic evaluation settings

**Result:** Only 21.2% of the issues identified by human experts were also identified by GPT-4o, which additionally discovered 27 new issues, mainly excelling in aesthetic and minimalist design heuristics, but struggling with flexibility, control, and efficiency.

**Limitations:** GPT-4o tends to generate false positives and may struggle with specific heuristic categories.

**Conclusion:** GPT-4o shows promise in heuristic evaluation but has notable limitations, including hallucinations resulting in false positives, suggesting cautious use in HCI.

**Abstract:** Heuristic evaluation is a widely used method in Human-Computer Interaction (HCI) to inspect interfaces and identify issues based on heuristics. Recently, Large Language Models (LLMs), such as GPT-4o, have been applied in HCI to assist in persona creation, the ideation process, and the analysis of semi-structured interviews. However, considering the need to understand heuristics and the high degree of abstraction required to evaluate them, LLMs may have difficulty conducting heuristic evaluation. However, prior research has not investigated GPT-4o's performance in heuristic evaluation compared to HCI experts in web-based systems. In this context, this study aims to compare the results of a heuristic evaluation performed by GPT-4o and human experts. To this end, we selected a set of screenshots from a web system and asked GPT-4o to perform a heuristic evaluation based on Nielsen's Heuristics from a literature-grounded prompt. Our results indicate that only 21.2% of the issues identified by human experts were also identified by GPT-4o, despite it found 27 new issues. We also found that GPT-4o performed better for heuristics related to aesthetic and minimalist design and match between system and real world, whereas it has difficulty identifying issues in heuristics related to flexibility, control, and user efficiency. Additionally, we noticed that GPT-4o generated several false positives due to hallucinations and attempts to predict issues. Finally, we highlight five takeaways for the conscious use of GPT-4o in heuristic evaluations.

</details>


### [12] [Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury](https://arxiv.org/abs/2506.16468)

*Vlad Cnejevici, Matthias Ponfick, Raul C. Sîmpetru, Alessandro Del Vecchio*

**Main category:** cs.HC

**Keywords:** Functional Electrical Stimulation, Spinal Cord Injury, Electromyography, Human-Computer Interaction, Neuroprostheses

**Relevance Score:** 8

**TL;DR:** This study explores a wearable high-density surface EMG system's ability to capture and control paralyzed foot kinematics for individuals with spinal cord injury (SCI) using functional electrical stimulation (FES).

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** To restore movement in paralyzed feet for individuals with neurological conditions, addressing key challenges related to spinal cord injury and improving quality of life.

**Method:** A high-density surface EMG system was utilized to capture muscle activity and control foot movement in a closed-loop FES system, involving participants with chronic and acute SCI.

**Key Contributions:**

	1. Use of a wearable high-density EMG system for real-time control of FES in foot movements.
	2. Demonstration of potential for intuitively restoring lost motor functions in individuals with SCI.
	3. Identification of distinct spared muscle activity patterns that can be leveraged for movement control.

**Result:** Participants exhibited distinct spared EMG activity, enabling them to control a digital cursor with their muscle activity and some achieved over 70% accuracy in foot movement modulation with FES.

**Limitations:** Limited sample size with only five participants; results may not generalize to all individuals with SCI.

**Conclusion:** The study demonstrates a viable method for intuitive control of FES systems to help individuals with SCI regain foot movement, significantly enhancing their range of motion.

**Abstract:** Restoring movement of a paralyzed foot is a key challenge in helping individuals with neurological conditions such as spinal cord injury (SCI) to improve their quality of life. Neuroprostheses based on functional electrical stimulation (FES) can restore the physiological range of motion by stimulating the affected muscles using surface electrodes. We have previously shown that, despite chronic motor-complete SCI, it is possible to capture paralyzed hand movements in individuals with tetraplegia using spared and modulated motor unit (MU) activity decoded with non-invasive electromyography (EMG) sensors. This study investigated whether a wearable high-density surface EMG system could capture and control paralyzed foot kinematics in closed-loop control with an FES system. We found that all our participants with SCI (2 with chronic SCI and 3 with acute SCI) retained distinct spared EMG activity for at least three ankle movements, which allowed them to reliably control a digital cursor using their spared tibialis anterior and triceps surae MU activity. Movement separability was further reconfirmed by extracting task-modulated MU activity during foot flexion/extension (3-7 modulated MUs/participant). Three participants were further able to modulate and maintain their foot flexion/extension EMG levels with an accuracy of >70%. Lastly, we show that real-time control of a FES system using EMG from the affected limb can restore foot movements in a highly intuitive way, significantly improving the lost or pathological foot range of motion. Our system provides an intuitive approach for closed-loop control of FES that has the potential to assist individuals with SCI in regaining lost motor functions.

</details>


### [13] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)

*Sophie Chiang, Guy Laban, Hatice Gunes*

**Main category:** cs.HC

**Keywords:** conversational agents, therapy, robot interactions, semantic alignment, mental health

**Relevance Score:** 9

**TL;DR:** This study compares the thematic and semantic alignment of disclosures in robot-led conversations with those in human therapy sessions, revealing significant parallels that suggest robots can augment mental health interventions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the similarities between robot-led supportive dialogue and traditional human therapy, especially as conversational agents engage more in emotionally supportive interactions.

**Method:** Two datasets were analyzed: one from human therapists and another from a social robot (QTrobot) using sentence embeddings and K-means clustering for thematic alignment assessment, validated by Euclidean distances.

**Key Contributions:**

	1. Investigation of thematic alignment between robot and human therapy sessions
	2. Analysis of semantic overlap in responses to similar themes
	3. Demonstration of the robot's potential in augmenting mental health support

**Result:** 90.88% of the robot conversation disclosures could be mapped to clusters from the human therapy dataset, indicating shared topical structures and strong semantic overlap in respondents' disclosures.

**Limitations:** 

**Conclusion:** The study demonstrates significant parallels between robot-led and human-led therapy conversations, suggesting potential for robots in mental health interventions.

**Abstract:** As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.

</details>


### [14] [Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence](https://arxiv.org/abs/2506.16542)

*Nathalia Gomez, S. Sue Batham, Mathias Volonte, Tiffany D. Do*

**Main category:** cs.HC

**Keywords:** AI-driven interviews, technical interviews, confidence-building, qualitative study, computer science graduates

**Relevance Score:** 7

**TL;DR:** This study examines the effectiveness of a multimodal AI system designed to simulate technical interviews for computer science graduates, focusing on its impact on candidate confidence and preparation.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the stress and limited practice opportunities faced by computer science graduates during technical interviews.

**Method:** A formative qualitative study was conducted with 20 participants who engaged with an AI-driven mock interview tool that included whiteboarding tasks and real-time feedback.

**Key Contributions:**

	1. Demonstrated the feasibility of AI systems in simulating technical interviews.
	2. Showed improvements in candidate confidence and articulation during interviews.
	3. Identified specific challenges in conversational flow and timing that need addressing.

**Result:** Participants reported that the AI simulation was realistic and helpful, leading to increased confidence and better communication of their problem-solving processes, despite some challenges in conversational flow and timing.

**Limitations:** Challenges with conversational flow and timing were noted by participants.

**Conclusion:** The study indicates that AI-driven technical interviews can serve as effective and scalable preparation tools, pointing towards the need for further exploration into variations in interviewer behavior.

**Abstract:** Technical interviews are a critical yet stressful step in the hiring process for computer science graduates, often hindered by limited access to practice opportunities. This formative qualitative study (n=20) explores whether a multimodal AI system can realistically simulate technical interviews and support confidence-building among candidates. Participants engaged with an AI-driven mock interview tool featuring whiteboarding tasks and real-time feedback. Many described the experience as realistic and helpful, noting increased confidence and improved articulation of problem-solving decisions. However, challenges with conversational flow and timing were noted. These findings demonstrate the potential of AI-driven technical interviews as scalable and realistic preparation tools, suggesting that future research could explore variations in interviewer behavior and their potential effects on candidate preparation.

</details>


### [15] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)

*Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha*

**Main category:** cs.HC

**Keywords:** data visualization, natural language processing, visualization design rationale, large language models, literate visualization

**Relevance Score:** 7

**TL;DR:** This paper introduces a new dataset and methodology to understand the rationale behind visualization design using real-world student-created visualization notebooks and large language models for analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing datasets for data visualization emphasize interpretation over understanding the encoding of visualizations. This paper aims to fill that gap by using real-world examples.

**Method:** The authors collected literate visualization notebooks from students, which include design rationales. They used large language models to generate and categorize question-answer-rationale triples and validate them to create a dataset.

**Key Contributions:**

	1. Introduction of a dataset focused on design rationale in visualizations.
	2. Use of real-world student notebooks as source material.
	3. Application of LLMs to generate and validate question-answer-rationale triples.

**Result:** The resulting dataset captures the design choices and rationales behind visualizations as articulated by students, providing a richer context for understanding visualization design.

**Limitations:** The dataset is limited to the context of student projects and may not reflect broader professional practices in visualization design.

**Conclusion:** This approach not only highlights visualization design choices but also serves as a valuable resource for further research in data visualization and natural language processing.

**Abstract:** Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.

</details>


### [16] [PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2506.16677)

*Hao Guo, Wei Fan, Shaohui Liu, Feng Jiang, Chunzhi Yi*

**Main category:** cs.HC

**Keywords:** trust prediction, human-robot collaboration, physiological signals, collaboration performance, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework for predicting trust in human-robot collaboration using physiological signals and performance evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Trust calibration is crucial for safety and efficiency in human-robot collaboration, particularly in construction settings.

**Method:** The Performance-guided Physiological signal-based Trust Prediction (PPTP) framework integrates synchronized multimodal physiological signals with collaboration performance evaluation.

**Key Contributions:**

	1. Introduction of a multimodal approach for trust prediction incorporating physiological signals and performance metrics.
	2. Achievement of high accuracy in trust classification across multiple levels.
	3. Validation of the method through extensive experiments, demonstrating improvement over baseline techniques.

**Result:** The model achieves over 81% accuracy in three-level trust classification and 74.3% in seven-level classification, surpassing previous methods.

**Limitations:** 

**Conclusion:** The proposed method significantly enhances trust prediction in collaboration scenarios by using physiological signals guided by collaboration performance.

**Abstract:** Trust prediction is a key issue in human-robot collaboration, especially in construction scenarios where maintaining appropriate trust calibration is critical for safety and efficiency. This paper introduces the Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel framework designed to improve trust assessment. We designed a human-robot construction scenario with three difficulty levels to induce different trust states. Our approach integrates synchronized multimodal physiological signals (ECG, GSR, and EMG) with collaboration performance evaluation to predict human trust levels. Individual physiological signals are processed using collaboration performance information as guiding cues, leveraging the standardized nature of collaboration performance to compensate for individual variations in physiological responses. Extensive experiments demonstrate the efficacy of our cross-modality fusion method in significantly improving trust classification performance. Our model achieves over 81% accuracy in three-level trust classification, outperforming the best baseline method by 6.7%, and notably reaches 74.3% accuracy in high-resolution seven-level classification, which is a first in trust prediction research. Ablation experiments further validate the superiority of physiological signal processing guided by collaboration performance assessment.

</details>


### [17] [V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos](https://arxiv.org/abs/2506.16716)

*Qixin Wang, Songtao Zhou, Zeyu Jin, Chenglin Guo, Shikun Sun, Xiaoyu Qin*

**Main category:** cs.HC

**Keywords:** video commentary, expressive speech, para-linguistic cues, accessibility, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper presents a method called vision-context-aware speech synthesis (V-CASS) to enhance user understanding of videos by incorporating expressive speech aligned with visual content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current automatic video commentary systems fail to capture crucial para-linguistic cues like emotion and attitude, impeding user understanding of video content.

**Method:** The proposed V-CASS method uses a vision-language model to analyze visual cues and a knowledge-infused language model to generate expressive speech that aligns with these cues.

**Key Contributions:**

	1. Introduction of vision-context-aware speech synthesis (V-CASS) for video commentary.
	2. Demonstrated improved user engagement and understanding through user studies.
	3. Potential applications for enhancing accessibility in multimedia content.

**Result:** User studies indicate that V-CASS improves emotional resonance and user engagement, with 74.68% of participants preferring the system over traditional approaches.

**Limitations:** 

**Conclusion:** V-CASS not only enhances general video understanding but also shows promise in improving accessibility for blind and low-vision users.

**Abstract:** Automatic video commentary systems are widely used on multimedia social media platforms to extract factual information about video content. However, current systems may overlook essential para-linguistic cues, including emotion and attitude, which are critical for fully conveying the meaning of visual content. The absence of these cues can limit user understanding or, in some cases, distort the video's original intent. Expressive speech effectively conveys these cues and enhances the user's comprehension of videos. Building on these insights, this paper explores the usage of vision-context-aware expressive speech in enhancing users' understanding of videos in video commentary systems. Firstly, our formatting study indicates that semantic-only speech can lead to ambiguity, and misaligned emotions between speech and visuals may distort content interpretation. To address this, we propose a method called vision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic cues from visuals using a vision-language model and leverages a knowledge-infused language model to guide the expressive speech model in generating context-aligned speech. User studies show that V-CASS enhances emotional and attitudinal resonance, as well as user audio-visual understanding and engagement, with 74.68% of participants preferring the system. Finally, we explore the potential of our method in helping blind and low-vision users navigate web videos, improving universal accessibility.

</details>


### [18] ["Whoever needs to see it, will see it": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok](https://arxiv.org/abs/2506.16851)

*Ankolika De, Kelley Cotter, Shaheen Kanthawala, Haley McAtee, Amy Ritchart, Gahana Kadur*

**Main category:** cs.HC

**Keywords:** social media algorithms, conspirituality, TikTok, content creation, emotional labor

**Relevance Score:** 6

**TL;DR:** This study investigates how social media algorithms, particularly TikTok's For You Page, influence creators of algorithmic conspirituality content and their interactions with viewers.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the implications of social media algorithms on content creation and community dynamics, particularly in the context of spiritual themes.

**Method:** Interviews with 14 TikTok creators focused on algorithmic conspirituality to explore their experiences and the influence of TikTok's algorithm on their content.

**Key Contributions:**

	1. Exploration of creator-audience dynamics influenced by algorithmic design.
	2. Insights into emotional and affective labor associated with algorithmic content creation.
	3. Recommendations for design improvements to support creators in managing algorithmic challenges.

**Result:** Creators' beliefs shape how they engage with the algorithm and inform their content, while also affecting viewer interactions and emotional labor.

**Limitations:** 

**Conclusion:** Design efforts should support creators in navigating the complexities of algorithmic mediation and the spiritual themes it evokes in content creation.

**Abstract:** Recent studies show that users often interpret social media algorithms as mystical or spiritual because of their unpredictability. This invites new questions about how such perceptions affect the content that creators create and the communities they form online. In this study, 14 creators of algorithmic conspirituality content on TikTok were interviewed to explore their interpretations and creation processes influenced by the platform's For You Page algorithm. We illustrate how creators' beliefs interact with TikTok's algorithmic mediation to reinforce and shape their spiritual or relational themes. Furthermore, we show how algorithmic conspirituality content impacts viewers, highlighting its role in generating significant emotional and affective labor for creators, stemming from complex relational dynamics inherent in this content creation. We discuss implications for design to support creators aimed at recognizing the unexpected spiritual and religious experiences algorithms prompt, as well as supporting creators in effectively managing these challenges.

</details>


### [19] [Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools](https://arxiv.org/abs/2506.16874)

*Zhiqing Wang, Haoxiang Fan, Shiwei Wu, Qiaoyi Chen, Yongqi Liang, Zhenhui Peng*

**Main category:** cs.HC

**Keywords:** Generative AI, K-6 education, Project-based learning, AskArt, Creativity

**Relevance Score:** 4

**TL;DR:** The study explores the integration of Generative AI in K-6 project-based art courses, highlighting the development of AskArt, an interface combining DALL-E and GPT that supports student creativity and collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of Generative AI on enhancing creativity, engagement, and collaboration in K-6 art education.

**Method:** A four-phase field study with two experienced K-6 art teachers and 132 students across eight offline course sessions was conducted to analyze the usage and effects of Generative AI.

**Key Contributions:**

	1. Development of AskArt, an interactive tool for K-6 students
	2. Insights into creativity enhancement and engagement through GenAI
	3. Recommendations for educators on integrating GenAI in teaching

**Result:** The integration of Generative AI provided background information, inspiration, and personalized guidance for students, but challenges in query formulation were noted. Increased engagement and varied collaboration strategies were reported by students, along with teacher concerns regarding misuse and the interface's suitability.

**Limitations:** Challenges in query formulation and concerns regarding misuse of the interface were noted.

**Conclusion:** The study demonstrates the potential benefits of Generative AI in elementary education and provides recommendations for its effective use in project-based learning.

**Abstract:** The integration of Generative Artificial Intelligence (GenAI) in K-6 project-based art courses presents both opportunities and challenges for enhancing creativity, engagement, and group collaboration. This study introduces a four-phase field study, involving in total two experienced K-6 art teachers and 132 students in eight offline course sessions, to investigate the usage and impact of GenAI. Specifically, based on findings in Phases 1 and 2, we developed AskArt, an interactive interface that combines DALL-E and GPT and is tailored to support elementary school students in their art projects, and deployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in providing background information, inspirations, and personalized guidance. However, challenges in query formulation for generating expected content were also observed. Moreover, students employed varied collaboration strategies, and teachers noted increased engagement alongside concerns regarding misuse and interface suitability. This study offers insights into the effective integration of GenAI in elementary education, presents AskArt as a practical tool, and provides recommendations for educators and researchers to enhance project-based learning with GenAI technologies.

</details>


### [20] [Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics](https://arxiv.org/abs/2506.17011)

*Bruno Campos*

**Main category:** cs.HC

**Keywords:** HCI, User Engagement, Information Retention, Interactive Infographics, Design Elements

**Relevance Score:** 6

**TL;DR:** This study analyzes the effects of 'juiciness' in interactive infographics on user engagement and information retention, revealing mixed outcomes and the necessity for balance in design elements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how design elements, specifically 'juiciness,' affect user engagement and information retention in interactive infographics.

**Method:** Comparative analysis of user engagement and information retention metrics between juicy and dry infographics.

**Key Contributions:**

	1. Highlighting the impact of design choices on user engagement and information retention.
	2. Providing empirical evidence on user responses to interactive infographic styles.
	3. Outlining the necessity of balancing engagement with clarity in infographic design.

**Result:** Juicy designs generally led to slightly higher engagement, but results on information retention were mixed; some juicy versions had better recall while one dry version performed best in multiple-choice questions.

**Limitations:** Variation in results suggests that optimal design implementation is context-dependent, and excessive juiciness may hinder information retention.

**Conclusion:** Juicy elements can enhance user engagement and retention when well-implemented, but excessive juiciness may distract users. A balance of engagement and clarity is crucial.

**Abstract:** This study compares the impact of "juiciness" on user engagement and short-term information retention in interactive infographics. Juicy designs generally showed a slight advantage in overall user engagement scores compared to dry designs. Specifically, the juicy version of the Burcalories infographic had the highest engagement score. However, the differences in engagement were often small. Regarding information retention, the results were mixed. The juicy versions of The Daily Routines of Famous Creative People and The Main Chakras infographics showed marginally better average recall and more participants with higher recall. Conversely, the dry version of Burcalories led to more correct answers in multiple-choice questions. The study suggests that while juicy design elements can enhance user engagement and, in some cases, short-term information retention, their effectiveness depends on careful implementation. Excessive juiciness could be overwhelming or distracting, while well-implemented juicy elements contributed to a more entertaining experience. The findings emphasize the importance of balancing engaging feedback with clarity and usability.

</details>


### [21] [Toward Understanding Similarity of Visualization Techniques](https://arxiv.org/abs/2506.17032)

*Abdulhaq Adetunji Salako, Christian Tominski*

**Main category:** cs.HC

**Keywords:** visualization techniques, similarity, model-driven approach, expert assessment, data visualization

**Relevance Score:** 4

**TL;DR:** This paper investigates the similarity of various visualization techniques through both model-driven and expert-driven approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the similarity of visualization techniques and provide structured categorization while acknowledging the challenges faced in existing literature.

**Method:** A model-driven approach was used to define visualization signatures; an expert-driven approach involved an online study assessing similarity between techniques.

**Key Contributions:**

	1. Introduction of a model-driven approach to define visualization technique signatures.
	2. Conducted an expert-driven study for intuitive similarity assessment of visualization techniques.
	3. Provided preliminary insights into similarity across different data visualization methods.

**Result:** Both approaches revealed insights into the similarity of 13 basic and advanced visualization techniques, highlighting preliminary findings.

**Limitations:** Results are preliminary and may require further validation; limited to 13 techniques.

**Conclusion:** The study represents initial steps toward a deeper understanding of visualization technique similarities, though results are preliminary and academic.

**Abstract:** The literature describes many visualization techniques for different types of data, tasks, and application contexts, and new techniques are proposed on a regular basis. Visualization surveys try to capture the immense space of techniques and structure it with meaningful categorizations. Yet, it remains difficult to understand the similarity of visualization techniques in general. We approach this open research question from two angles. First, we follow a model-driven approach that is based on defining the signature of visualization techniques and interpreting the similarity of signatures as the similarity of their associated techniques. Second, following an expert-driven approach, we asked visualization experts in a small online study for their ad-hoc intuitive assessment of the similarity of pairs visualization techniques. From both approaches, we gain insight into the similarity of a set of 13 basic and advanced visualizations for different types of data. While our results are so far preliminary and academic, they are first steps toward better understanding the similarity of visualization techniques.

</details>


### [22] [Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools](https://arxiv.org/abs/2506.17116)

*Samuel Rhys Cox, Helena Bøjer Djernæs, Niels van Berkel*

**Main category:** cs.HC

**Keywords:** Explainable AI, User-centric measures, Emotional well-being, Creativity support tools, Arts

**Relevance Score:** 4

**TL;DR:** Exploration of user-centric benefits in evaluating explainable AI systems in the arts, focusing on emotional well-being and intrinsic abilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of studies evaluating creativity support tools (CSTs) based on user-centric metrics that enhance user experience.

**Method:** Discussion of measures related to intrinsic abilities, emotional well-being, self-reflection, and self-perception within the context of explainable AI in the arts.

**Key Contributions:**

	1. Identification of user-centric measures for evaluating XAI systems
	2. Discussion on emotional well-being as a metric in the arts
	3. Provocation for further research on usability in XAI for creativity support

**Result:** Highlights the potential benefits of incorporating user-centric evaluation measures in XAI systems, specifically in the domain of the arts.

**Limitations:** Limited empirical evidence due to the nature of the workshop paper; primarily a discussion-based approach.

**Conclusion:** The paper aims to provoke dialogue on the importance of user-centric measures in the evaluation of XAI systems in creative fields.

**Abstract:** In this workshop paper, we discuss the potential for measures of user-centric benefits (such as emotional well-being) that could be explored when evaluating explainable AI (XAI) systems within the arts. As a background to this, we draw from our recent review of creativity support tool (CST) evaluations, that found a paucity of studies evaluating CSTs for user-centric measures that benefit the user themselves. Specifically, we discuss measures of: (1) developing intrinsic abilities, (2) emotional well-being, (3) self-reflection, and (4) self-perception. By discussing these user-centric measures within the context of XAI and the arts, we wish to provoke discussion regarding the potential of such measures.

</details>


### [23] [Detecting LLM-Generated Short Answers and Effects on Learner Performance](https://arxiv.org/abs/2506.17196)

*Shambhavi Bhushan, Danielle R Thomas, Conrad Borchers, Isha Raghuvanshi, Ralph Abboud, Erin Gatz, Shivang Gupta, Kenneth Koedinger*

**Main category:** cs.HC

**Keywords:** large language models, LLM detection, educational technology, machine learning, learner performance

**Relevance Score:** 9

**TL;DR:** This study fine-tunes GPT-4o to detect LLM-generated responses and evaluates the impact on learner performance, finding improved accuracy over existing tools and evidence of potential misuse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the concerns regarding the misuse of large language models (LLMs) in online learning and to improve detection methods for LLM-generated text.

**Method:** The study defines LLM-generated text and fine-tunes GPT-4o for detecting such responses, comparing its accuracy against GPTZero, and assessing the effects of LLM misuse on learner performance.

**Key Contributions:**

	1. Fine-tuning of GPT-4o improves detection of LLM-generated responses.
	2. Demonstrated correlation between LLM misuse and learner performance outcomes.
	3. Contribution of data and code for future research in LLM misuse detection.

**Result:** Fine-tuned GPT-4o achieved 80% accuracy and an F1 score of 0.78, outperforming GPTZero which had 70% accuracy and an F1 score of 0.50. Additionally, learners suspected of LLM misuse were more likely to answer related posttest questions correctly.

**Limitations:** 

**Conclusion:** The findings suggest that LLM-generated text misuse can bypass the learning process, and the study provides a structured approach for future improvements in detection methods and proposes auxiliary indicators for detection.

**Abstract:** The increasing availability of large language models (LLMs) has raised concerns about their potential misuse in online learning. While tools for detecting LLM-generated text exist and are widely used by researchers and educators, their reliability varies. Few studies have compared the accuracy of detection methods, defined criteria to identify content generated by LLM, or evaluated the effect on learner performance from LLM misuse within learning. In this study, we define LLM-generated text within open responses as those produced by any LLM without paraphrasing or refinement, as evaluated by human coders. We then fine-tune GPT-4o to detect LLM-generated responses and assess the impact on learning from LLM misuse. We find that our fine-tuned LLM outperforms the existing AI detection tool GPTZero, achieving an accuracy of 80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1 score of 0.50, demonstrating superior performance in detecting LLM-generated responses. We also find that learners suspected of LLM misuse in the open response question were more than twice as likely to correctly answer the corresponding posttest MCQ, suggesting potential misuse across both question types and indicating a bypass of the learning process. We pave the way for future work by demonstrating a structured, code-based approach to improve LLM-generated response detection and propose using auxiliary statistical indicators such as unusually high assessment scores on related tasks, readability scores, and response duration. In support of open science, we contribute data and code to support the fine-tuning of similar models for similar use cases.

</details>


### [24] [Case Law Grounding: Using Precedents to Align Decision-Making for Humans and AI](https://arxiv.org/abs/2310.07019)

*Quan Ze Chen, Amy X. Zhang*

**Main category:** cs.HC

**Keywords:** case law grounding, decision-making, human-AI interaction, large language models, social criteria

**Relevance Score:** 7

**TL;DR:** This paper introduces 'case law grounding' (CLG), a method for improving decision-making in human and AI contexts by using past decisions as precedents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for consistent decision-making in communities and AI systems, ensuring decisions align with socially-established criteria.

**Method:** The authors propose and implement 'case law grounding' (CLG), evaluating its effectiveness across human-led and LLM-assisted decision-making tasks.

**Key Contributions:**

	1. Introduction of 'case law grounding' for decision-making consistency
	2. Empirical evaluation showing significant accuracy improvements
	3. Exploration of retrieval window sizes and decision binding effects

**Result:** CLG significantly improved decision accuracy, showing a 16.0--23.3 %-points increase in human decisions and a 20.8--32.9 %-points increase in LLM outputs across evaluated groups.

**Limitations:** 

**Conclusion:** CLG offers a valuable framework for enhancing consistency in subjective decision-making, bridging human and AI judgment.

**Abstract:** From moderating content within an online community to producing socially-appropriate generative outputs, decision-making tasks -- conducted by either humans or AI -- often depend on subjective or socially-established criteria. To ensure such decisions are consistent, prevailing processes primarily make use of high-level rules and guidelines to ground decisions, similar to applying "constitutions" in the legal context. However, inconsistencies in specifying and interpreting constitutional grounding can lead to undesirable and even incorrect decisions being made. In this work, we introduce "case law grounding" (CLG) -- an approach for grounding subjective decision-making using past decisions, similar to how precedents are used in case law. We present how this grounding approach can be implemented in both human and AI decision-making contexts, introducing both a human-led process and a large language model (LLM) prompting setup. Evaluating with five groups and communities across two decision-making task domains, we find that decisions produced with CLG were significantly more accurately aligned to ground truth in 4 out of 5 groups, achieving a 16.0--23.3 %-points higher accuracy in the human process, and 20.8--32.9 %-points higher with LLMs. We also examined the impact of different configurations with the retrieval window size and binding nature of decisions and find that binding decisions and larger retrieval windows were beneficial. Finally, we discuss the broader implications of using CLG to augment existing constitutional grounding when it comes to aligning human and AI decisions.

</details>


### [25] [Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors](https://arxiv.org/abs/2410.20564)

*Sadia Nowrin, Keith Vertanen*

**Main category:** cs.HC

**Keywords:** speech recognition, error detection, human-computer interaction, accessibility, audiovisual feedback

**Relevance Score:** 8

**TL;DR:** The study explores improving error detection in conversational systems through audio output manipulation based on speech recognizer confidence levels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address speech recognition errors in conversational systems that significantly impact usability, especially for blind or low-vision users.

**Method:** Audio output of transcribed text was modified by selectively slowing down the audio when the recognizer exhibited uncertainty, compared to a uniform slowdown.

**Key Contributions:**

	1. Investigated audio manipulation to improve error detection in speech recognition
	2. Demonstrated effectiveness of adjusting audio playback speed based on confidence levels
	3. Showed significant usability improvements for blind and low-vision users.

**Result:** Participants showed a 12% increase in error detection ability and an 11% reduction in decision time regarding errors with the manipulated audio output.

**Limitations:** The study's findings may not be generalizable to all types of conversational systems or users with varying levels of hearing ability.

**Conclusion:** Manipulating audio feedback based on confidence levels enhances error detection in speech recognition systems, improving usability for users with visual impairments.

**Abstract:** Conversational systems rely heavily on speech recognition to interpret and respond to user commands and queries. Despite progress on speech recognition accuracy, errors may still sometimes occur and can significantly affect the end-user utility of such systems. While visual feedback can help detect errors, it may not always be practical, especially for people who are blind or low-vision. In this study, we investigate ways to improve error detection by manipulating the audio output of the transcribed text based on the recognizer's confidence level in its result. Our findings show that selectively slowing down the audio when the recognizer exhibited uncertainty led to a 12% relative increase in participants' ability to detect errors compared to uniformly slowing the audio. It also reduced the time it took participants to listen to the recognition result and decide if there was an error by 11%.

</details>


### [26] [AI Should Challenge, Not Obey](https://arxiv.org/abs/2411.02263)

*Advait Sarkar*

**Main category:** cs.HC

**Keywords:** AI, HCI, Socratic method, user interaction, critical thinking

**Relevance Score:** 5

**TL;DR:** The paper discusses the role of AI in promoting critical thinking and challenging users rather than merely obeying commands.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advocate for AI systems that engage users in deeper, more meaningful interactions instead of simply fulfilling tasks.

**Method:** The author proposes a conceptual framework for AI interaction that emphasizes challenge and inquiry.

**Key Contributions:**

	1. Proposes the concept of Socratic AI that challenges users
	2. Highlights the importance of critical thinking in AI interactions
	3. Suggests a novel approach to user engagement in HCI

**Result:** The paper argues that transforming AI into 'Socratic gadflies' can enhance user engagement and improve decision-making.

**Limitations:** The implementation of such AI systems in real-world applications is not thoroughly explored.

**Conclusion:** Encouraging AIs to challenge users can foster a more thoughtful and reflective approach to human-computer interaction.

**Abstract:** Let's transform our robot secretaries into Socratic gadflies.

</details>


### [27] [Collective Creation of Intimacy: Exploring the Cosplay Commission Practice within the Otome Game Community in China](https://arxiv.org/abs/2412.00630)

*Yihao Zhou, Haowei Xu, Lili Zhang, Shengdong Zhao*

**Main category:** cs.HC

**Keywords:** cosplay, cos-commission, Otome games, emotional recovery, digital intimacy

**Relevance Score:** 2

**TL;DR:** This paper explores cos-commission, a commodified romantic companionship in the Otome game community in China, through interviews to understand participants' experiences and challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the motivations, practices, experiences, and challenges of cos-commission participants within the Otome game community.

**Method:** Semi-structured interviews conducted with 15 participants in various roles related to cos-commission.

**Key Contributions:**

	1. Explored the dual role of cos-commission in forming personal connections and providing emotional support.
	2. Identified emotional challenges such as attachment issues and withdrawal symptoms post-commission.
	3. Highlighted the limitations of digital platforms in providing sufficient safeguards for participants.

**Result:** Participants found cos-commission to enhance personal connections and emotional recovery, but faced challenges such as role confusion and attachment issues.

**Limitations:** The study is based on a small sample size and focuses solely on the Otome game community in China, which may not generalize to other contexts.

**Conclusion:** While cos-commission can foster meaningful relationships and mental well-being, it presents privacy and security challenges that need addressing.

**Abstract:** Cosplay commission (cos-commission) is a new form of commodified romantic companionship within the Otome game community in China. To explore the motivations, practices, experiences, and challenges, we conducted semi-structured interviews with 15 participants in different roles. Our findings reveal that cos-commission, as a hybrid activity, provides participants with a chance to collaboratively build meaningful connections. It also offers a pathway for personal exploration and emotional recovery. However, the vague boundary between performative roles and intimate interactions can give rise to unexpected negative outcomes, such as attachment-driven entanglements and post-commission "withdrawal symptoms." While digital platforms facilitate communication in cos-commissions, they often lack sufficient safeguards. This preliminary work provides insights into the formation process of hybrid intimate relationship and its potential to foster personalized, long-term support for mental well-being, and reveals potential privacy and security challenges.

</details>


### [28] [A multimodal dataset for understanding the impact of mobile phones on remote online virtual education](https://arxiv.org/abs/2412.14195)

*Roberto Daza, Alvaro Becerra, Ruth Cobos, Julian Fierrez, Aythami Morales*

**Main category:** cs.HC

**Keywords:** IMPROVE dataset, mobile phone usage, online education, biometric data, learner behavior

**Relevance Score:** 8

**TL;DR:** The IMPROVE dataset evaluates mobile phone usage effects on learners during online education through multimodal data collection.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the impact of mobile phone usage on learners' behavior and academic performance during online education.

**Method:** The study involved 120 learners monitored through 16 synchronized sensors (EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics) during educational tasks over 30-minute sessions.

**Key Contributions:**

	1. Introduction of a multimodal dataset for educational research
	2. Insights into behavioral and physiological effects of mobile phone usage
	3. Public access to the dataset for the research community

**Result:** Statistical analyses showed biometric changes associated with mobile phone usage, and the dataset includes various forms of synchronized recordings.

**Limitations:** The study focuses on a specific learner population and context, potentially limiting generalizability.

**Conclusion:** The dataset is publicly available and useful for future research on mobile phone usage in educational contexts.

**Abstract:** This work presents the IMPROVE dataset, a multimodal resource designed to evaluate the effects of mobile phone usage on learners during online education. It includes behavioral, biometric, physiological, and academic performance data collected from 120 learners divided into three groups with different levels of phone interaction, enabling the analysis of the impact of mobile phone usage and related phenomena such as nomophobia. A setup involving 16 synchronized sensors -- including EEG, eye tracking, video cameras, smartwatches, and keystroke dynamics -- was used to monitor learner activity during 30-minute sessions involving educational videos, document reading, and multiple-choice tests. Mobile phone usage events, including both controlled interventions and uncontrolled interactions, were labeled by supervisors and refined through a semi-supervised re-labeling process. Technical validation confirmed signal quality, and statistical analyses revealed biometric changes associated with phone usage. The dataset is publicly available for research through GitHub and Science Data Bank, with synchronized recordings from three platforms (edBB, edX, and LOGGE), provided in standard formats (.csv, .mp4, .wav, and .tsv), and accompanied by a detailed guide.

</details>


### [29] [Fuzzy Linkography: Automatic Graphical Summarization of Creative Activity Traces](https://arxiv.org/abs/2502.04599)

*Amy Smith, Barrett R. Anderson, Jasmine Tan Otto, Isaac Karth, Yuqian Sun, John Joon Young Chung, Melissa Roemmele, Max Kreminski*

**Main category:** cs.HC

**Keywords:** fuzzy linkography, creative ideation, semantic similarity

**Relevance Score:** 7

**TL;DR:** The paper introduces fuzzy linkography, an automated method for analyzing connections between design moves, which enhances the scalability of traditional linkography techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional linkography is time-consuming due to manual annotation, limiting its application in analyzing creative activity traces at scale.

**Method:** The authors propose a fuzzy computational model of semantic similarity to automatically construct linkographs from recorded design moves.

**Key Contributions:**

	1. Introduction of fuzzy linkography for automated annotation
	2. Application to various creative activity traces
	3. Discussion on the implications for wider deployment of linkographic techniques

**Result:** Fuzzy linkography was applied to diverse creative activity traces, demonstrating its capability to analyze text-to-image prompting, LLM-supported ideation, and publication histories.

**Limitations:** The approach's reliance on semantic similarity models may introduce inaccuracies in complex creative contexts.

**Conclusion:** The findings highlight the strengths and limitations of fuzzy linkography and suggest potential future applications within creative ideation contexts.

**Abstract:** Linkography -- the analysis of links between the design moves that make up an episode of creative ideation or design -- can be used for both visual and quantitative assessment of creative activity traces. Traditional linkography, however, is time-consuming, requiring a human coder to manually annotate both the design moves within an episode and the connections between them. As a result, linkography has not yet been much applied at scale. To address this limitation, we introduce fuzzy linkography: a means of automatically constructing a linkograph from a sequence of recorded design moves via a "fuzzy" computational model of semantic similarity, enabling wider deployment and new applications of linkographic techniques. We apply fuzzy linkography to three markedly different kinds of creative activity traces (text-to-image prompting journeys, LLM-supported ideation sessions, and researcher publication histories) and discuss our findings, as well as strengths, limitations, and potential future applications of our approach.

</details>


### [30] [Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies](https://arxiv.org/abs/2502.05731)

*Sam Yu-Te Lee, Cheng-Wei Hung, Mei-Hua Yuan, Kwan-Liu Ma*

**Main category:** cs.HC

**Keywords:** DPSIR framework, interactive text mining, large language models, visualization, environmental data analysis

**Relevance Score:** 4

**TL;DR:** The paper develops GreenMine, a system that automates the construction of the DPSIR taxonomy for environmental studies through interactive text mining and prompting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To streamline the labor-intensive process of constructing a DPSIR taxonomy and to improve the flexibility and efficiency of analyzing environmental data.

**Method:** The system uses a prompting pipeline with three tasks that allow natural language definitions of the DPSIR taxonomy to be iteratively refined. It incorporates an uncertainty score for response consistency and features a radial uncertainty chart for visualization.

**Key Contributions:**

	1. Development of GreenMine for interactive text mining
	2. Introduction of an uncertainty score for taxonomy evaluation
	3. Design of a radial uncertainty chart for visualizing topics and uncertainties

**Result:** A case study using real-world interview transcripts demonstrated GreenMine's capabilities in supporting interactive mining of DPSIR relationships, showcasing both strengths and limitations through expert collaboration.

**Limitations:** The system's effectiveness needs further validation across diverse environments and scenarios beyond the initial case study.

**Conclusion:** GreenMine presents a novel approach to enhance interactive text mining for constructing the DPSIR taxonomy, offering lessons and future opportunities for similar applications in knowledge-intensive tasks.

**Abstract:** Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.

</details>


### [31] [Agonistic Image Generation: Unsettling the Hegemony of Intention](https://arxiv.org/abs/2502.15242)

*Andrew Shaw, Andre Ye, Ranjay Krishna, Amy X. Zhang*

**Main category:** cs.HC

**Keywords:** image generation, sociopolitical dimensions, agonistic pluralism, user reflection, diversity in AI

**Relevance Score:** 4

**TL;DR:** This paper discusses the sociopolitical dimensions of image generation, proposing an interface that promotes reflection and engagement with competing visual interpretations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current image generation systems often overlook the sociopolitical implications of user intentions, highlighted by controversies related to demographic diversity.

**Method:** A lab study with 29 participants evaluated the proposed agonistic interface against three existing paradigms to measure its effectiveness in facilitating reflection.

**Key Contributions:**

	1. Proposed an agonistic interface for image generation that incorporates sociopolitical negotiation.
	2. Provided empirical evidence on the importance of user perception of authenticity in diversity integration.
	3. Challenged current paradigms in image generation by emphasizing reflection on competing interpretations.

**Result:** The agonistic interface was found to enhance reflection compared to other interfaces, with the perception of authenticity being crucial for user engagement.

**Limitations:** The study's sample size was limited to 29 participants, and further research is needed to generalize findings.

**Conclusion:** Diversity and user intention should coexist within image generation, rather than viewing them as opposing values.

**Abstract:** Current image generation paradigms prioritize actualizing user intention - "see what you intend" - but often neglect the sociopolitical dimensions of this process. However, it is increasingly evident that image generation is political, contributing to broader social struggles over visual meaning. This sociopolitical aspect was highlighted by the March 2024 Gemini controversy, where Gemini faced criticism for inappropriately injecting demographic diversity into user prompts. Although the developers sought to redress image generation's sociopolitical dimension by introducing diversity "corrections," their opaque imposition of a standard for "diversity" ultimately proved counterproductive. In this paper, we present an alternative approach: an image generation interface designed to embrace open negotiation along the sociopolitical dimensions of image creation. Grounded in the principles of agonistic pluralism (from the Greek agon, meaning struggle), our interface actively engages users with competing visual interpretations of their prompts. Through a lab study with 29 participants, we evaluate our agonistic interface on its ability to facilitate reflection - engagement with other perspectives and challenging dominant assumptions - a core principle that underpins agonistic contestation. We compare it to three existing paradigms: a standard interface, a Gemini-style interface that produces "diverse" images, and an intention-centric interface suggesting prompt refinements. Our findings demonstrate that the agonistic interface enhances reflection across multiple measures, but also that reflection depends on users perceiving the interface as both appropriate and empowering; introducing diversity without grounding it in relevant political contexts was perceived as inauthentic. Our results suggest that diversity and user intention should not be treated as opposing values to be balanced.

</details>


### [32] [Using Collective Dialogues and AI to Find Common Ground Between Israeli and Palestinian Peacebuilders](https://arxiv.org/abs/2503.01769)

*Andrew Konya, Luke Thorburn, Wasim Almasri, Oded Adomi Leshem, Ariel D. Procaccia, Lisa Schirch, Michiel A. Bakker*

**Main category:** cs.HC

**Keywords:** AI-assisted methods, conflict resolution, large language models, collective dialogues, Israeli-Palestinian peace

**Relevance Score:** 7

**TL;DR:** This paper presents a case study on utilizing AI-assisted methods to facilitate dialogue and find common ground between Israeli and Palestinian peacebuilders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of AI-assisted approaches in bridging divides and fostering communication in high-stakes, real-world contexts following significant events.

**Method:** An iterative deliberative process involving large language models (LLMs), bridging-based ranking, and collective dialogues was used with 138 participants from diverse backgrounds related to the Israeli-Palestinian conflict.

**Key Contributions:**

	1. Demonstrated the application of AI-assisted dialogue methods in real-world conflict resolution.
	2. Achieved a high level of agreement (84%) among diverse peacebuilders on collective statements.
	3. Provided empirical insights into the challenges and outcomes of using LLMs in sensitive negotiations.

**Result:** The participatory process resulted in collective statements with at least 84% agreement from participants on both sides, indicating a successful negotiation of common ground.

**Limitations:** The study is context-specific, with results not necessarily generalizable to other conflicts or settings.

**Conclusion:** This case study illustrates the potential of AI-assisted methods in real-world conflict resolution and highlights the need to address future challenges and questions.

**Abstract:** A growing body of work has shown that AI-assisted methods -- leveraging large language models, social choice methods, and collective dialogues -- can help navigate polarization and surface common ground in controlled lab settings. But what can these approaches contribute in real-world contexts? We present a case study applying these techniques to find common ground between Israeli and Palestinian peacebuilders in the period following October 7th, 2023. From April to July 2024 an iterative deliberative process combining LLMs, bridging-based ranking, and collective dialogues was conducted in partnership with the Alliance for Middle East Peace. Around 138 civil society peacebuilders participated including Israeli Jews, Palestinian citizens of Israel, and Palestinians from the West Bank and Gaza. The process resulted in a set of collective statements, including demands to world leaders, with at least 84% agreement from participants on each side. In this paper, we document the process, results, challenges, and important open questions.

</details>


### [33] [DangerMaps: Personalized Safety Advice for Travel in Urban Environments using a Retrieval-Augmented Language Model](https://arxiv.org/abs/2503.14103)

*Jonas Oppenlaender*

**Main category:** cs.HC

**Keywords:** Large Language Models, Travel Safety, Personalized Advice, Human-Computer Interaction, Mapping System

**Relevance Score:** 7

**TL;DR:** The study explores the use of large language models in providing personalized travel safety information through a system called DangerMaps, which visualizes safety ratings and explanations for urban destinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Travelers often struggle to find reliable safety information for trip planning, typically relying on search engines that do not meet their contextual needs.

**Method:** A formative study of travelers' information needs was conducted, followed by the development of DangerMaps, a mapping system integrating LLMs to provide personalized safety advice.

**Key Contributions:**

	1. Introduction of DangerMaps for personalized travel safety research
	2. Exploration of information needs specific to travel safety
	3. Detailed approach to prompt design for LLM applications

**Result:** DangerMaps effectively plots safety ratings on a map and offers contextual explanations to users, filling a gap in current travel planning tools.

**Limitations:** The study is focused on urban destinations and may not generalize to all travel contexts.

**Conclusion:** The paper emphasizes the importance of prompt design when using LLMs in real-world applications and identifies areas for future research in this domain.

**Abstract:** Planning a trip into a potentially unsafe area is a difficult task. We conducted a formative study on travelers' information needs, finding that most of them turn to search engines for trip planning. Search engines, however, fail to provide easily interpretable results adapted to the context and personal information needs of a traveler. Large language models (LLMs) create new possibilities for providing personalized travel safety advice. To explore this idea, we developed DangerMaps, a mapping system that assists its users in researching the safety of an urban travel destination, whether it is pre-travel or on-location. DangerMaps plots safety ratings onto a map and provides explanations on demand. This late breaking work specifically emphasizes the challenges of designing real-world applications with large language models. We provide a detailed description of our approach to prompt design and highlight future areas of research.

</details>


### [34] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)

*Sophie Chiang, Guy Laban, Hatice Gunes*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Mental Health, Thematic Analysis, Machine Learning, Human-Robot Interaction

**Relevance Score:** 8

**TL;DR:** This study compares the similarities between the concerns shared in human therapy sessions and those with a conversational robot, QTrobot, highlighting significant thematic and semantic alignments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how robot interactions in emotionally supportive dialogue compare to traditional therapy settings to assess the potential of conversational agents in mental health support.

**Method:** Analyzed interactions from two datasets: professional therapist conversations and supportive robot dialogues, using sentence embeddings and K-means clustering for thematic alignment analysis.

**Key Contributions:**

	1. Demonstrated thematic alignment between human and robot therapy conversations.
	2. Mapped robot disclosures to human therapist response clusters, indicating shared concerns.
	3. Analyzed semantic similarities using advanced embedding techniques.

**Result:** 90.88% of robot conversation disclosures mapped to thematic clusters from human therapy, indicating significant topical alignment and strong semantic overlap in responses to similar disclosures.

**Limitations:** 

**Conclusion:** Robot-led support conversations show considerable parallels to human therapy, suggesting potential enhancements in mental health interventions through conversational agents.

**Abstract:** As conversational agents increasingly engage in emotionally supportive dialogue, it is important to understand how closely their interactions resemble those in traditional therapy settings. This study investigates whether the concerns shared with a robot align with those shared in human-to-human (H2H) therapy sessions, and whether robot responses semantically mirror those of human therapists. We analyzed two datasets: one of interactions between users and professional therapists (Hugging Face's NLP Mental Health Conversations), and another involving supportive conversations with a social robot (QTrobot from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence embeddings and K-means clustering, we assessed cross-agent thematic alignment by applying a distance-based cluster-fitting method that evaluates whether responses from one agent type map to clusters derived from the other, and validated it using Euclidean distances. Results showed that 90.88% of robot conversation disclosures could be mapped to clusters from the human therapy dataset, suggesting shared topical structure. For matched clusters, we compared the subjects as well as therapist and robot responses using Transformer, Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects' disclosures in both datasets, as well as in the responses given to similar human disclosure themes across agent types (robot vs. human therapist). These findings highlight both the parallels and boundaries of robot-led support conversations and their potential for augmenting mental health interventions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [35] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)

*Taylor Lynn Curtis, Maximilian Puelma Touzel, William Garneau, Manon Gruaz, Mike Pinder, Li Wei Wang, Sukanya Krishna, Luda Cohen, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine*

**Main category:** cs.CL

**Keywords:** misinformation, fact-checking, large language models, AI, media literacy

**Relevance Score:** 8

**TL;DR:** This paper presents Veracity, an open-source AI system for fact-checking and combating misinformation using LLMs and web retrieval.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the threat of misinformation exacerbated by generative AI and enhance media literacy among users.

**Method:** Veracity combines Large Language Models and web retrieval agents to analyze claims and provides assessments with explanations.

**Key Contributions:**

	1. Introduction of an open-source AI system for fact-checking
	2. Multilingual support and numerical scoring system
	3. Intuitive interface inspired by messaging applications

**Result:** Veracity offers features like multilingual support, numerical scoring of claim veracity, and an interactive interface for user engagement.

**Limitations:** 

**Conclusion:** Veracity not only detects misinformation but also explains its reasoning to foster a more informed society.

**Abstract:** The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.

</details>


### [36] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)

*Riccardo Di Sipio*

**Main category:** cs.CL

**Keywords:** large language models, information geometry, natural gradient descent, quantum optimization, Fisher information

**Relevance Score:** 8

**TL;DR:** This paper discusses the optimization of large language models through the lens of information geometry, emphasizing the use of the Fisher information metric and natural gradient descent, while exploring implications for quantum systems.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding of the optimization landscape in large language models and clarify phenomena related to sharp minima, generalization, and scaling laws.

**Method:** The paper utilizes information geometry, specifically the Fisher information metric, to analyze the high-dimensional parameter spaces of large language models and proposes curvature-aware optimization techniques.

**Key Contributions:**

	1. Application of information geometry to LLM optimization
	2. Identification of curvature-aware techniques for better training insights
	3. Speculation on quantum optimization parallels using metrics from quantum information theory

**Result:** The authors explain how curvature-aware methods can provide insights into LLM training and suggest possible parallels with quantum optimization techniques.

**Limitations:** 

**Conclusion:** Curvature-aware approaches may improve LLM training and efficiency while offering a bridge to quantum optimization strategies.

**Abstract:** Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-aware approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.

</details>


### [37] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)

*Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, Paul Pu Liang*

**Main category:** cs.CL

**Keywords:** long-horizon interactions, memory consolidation, reinforcement learning

**Relevance Score:** 9

**TL;DR:** Introduction of MEM1, a reinforcement learning framework for long-horizon multi-turn interactions that optimizes memory usage and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in long-horizon interactions faced by LLM systems, which often result in increased memory and computational costs due to reliance on full-context prompting.

**Method:** MEM1 updates a constant-size shared internal state at each turn, supporting memory consolidation and reasoning while discarding irrelevant information.

**Key Contributions:**

	1. Introduction of a constant memory usage framework for LLMs
	2. Proposed methodology for constructing complex task sequences from existing datasets
	3. Demonstrated significant performance and memory efficiency improvements in experiments

**Result:** MEM1-7B achieves a 3.5x performance improvement and 3.7x reduction in memory usage compared to Qwen2.5-14B-Instruct on a complex multi-hop QA task.

**Limitations:** 

**Conclusion:** MEM1 provides a scalable solution for training interactive agents in multi-turn tasks, emphasizing both efficiency and performance.

**Abstract:** Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.

</details>


### [38] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)

*Glenn Matlin, Mika Okamoto, Huzaifa Pardawala, Yang Yang, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** Language Models, Financial NLP, Evaluation Framework, Benchmarking, Reasoning-reinforced LMs

**Relevance Score:** 4

**TL;DR:** Introduction of a benchmarking suite called FLaME for evaluating Language Models in Finance NLP tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address gaps in existing evaluation frameworks that misrepresent LMs' performance in finance-related NLP tasks.

**Method:** A comprehensive empirical study comparing 23 foundation language models on 20 core finance NLP tasks using the new FLaME framework.

**Key Contributions:**

	1. Introduction of a holistic benchmarking suite (FLaME) for Financial Language Model Evaluation
	2. First study to compare 'reasoning-reinforced' LMs against traditional LMs in finance
	3. Open-sourcing the framework software, data, and results for further research

**Result:** The study reveals that Language Models can perform significantly better than previously believed on finance NLP tasks when evaluated properly.

**Limitations:** None stated in the abstract.

**Conclusion:** The FLaME framework serves as a tool for accurately evaluating and comparing LMs, helping to correct misconceptions about their capabilities in Finance.

**Abstract:** Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.

</details>


### [39] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)

*Yifan Hu, Frank Liang, Dachuan Zhao, Jonathan Geuter, Varshini Reddy, Craig W. Schmidt, Chris Tanner*

**Main category:** cs.CL

**Keywords:** Byte-Pair Encoding, pre-tokenization, entropy, segmentation, low-resource languages

**Relevance Score:** 7

**TL;DR:** This paper proposes two entropy-informed pre-tokenization strategies to enhance the performance of Byte-Pair Encoding (BPE) for segmenting unsegmented languages like Chinese.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve BPE segmentation for unsegmented languages due to its challenges in linguistic boundary recognition.

**Method:** Two strategies: one based on pointwise mutual information and left/right entropy to identify coherent character spans, and another using predictive entropy from a pretrained GPT-2 model to detect boundary uncertainty.

**Key Contributions:**

	1. Development of two novel entropy-informed pre-tokenization strategies for BPE.
	2. Substantial improvement in segmentation metrics compared to standard BPE.
	3. Potential application in low-resource and multilingual settings.

**Result:** Both methods showed substantial improvements in segmentation precision, recall, and F1 score on the PKU dataset compared to standard BPE.

**Limitations:** 

**Conclusion:** Entropy-guided pre-tokenization improves alignment with gold-standard linguistic units and enhances tokenization quality in low-resource and multilingual contexts.

**Abstract:** Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization method in modern language models due to its simplicity and strong empirical performance across downstream tasks. However, applying BPE to unsegmented languages such as Chinese presents significant challenges, as its frequency-driven merge operation is agnostic to linguistic boundaries. To address this, we propose two entropy-informed pre-tokenization strategies that guide BPE segmentation using unsupervised information-theoretic cues. The first approach uses pointwise mutual information and left/right entropy to identify coherent character spans, while the second leverages predictive entropy derived from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both methods on a subset of the PKU dataset and demonstrate substantial improvements in segmentation precision, recall, and F1 score compared to standard BPE. Our results suggest that entropy-guided pre-tokenization not only enhances alignment with gold-standard linguistic units but also offers a promising direction for improving tokenization quality in low-resource and multilingual settings.

</details>


### [40] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)

*Sam Silver, Jimin Sun, Ivan Zhang, Sara Hooker, Eddie Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Self-Correction, Chain of Thought, Intrinsic Behavior

**Relevance Score:** 8

**TL;DR:** The paper investigates the self-correction capabilities of Large Language Models (LLMs) in mathematical reasoning under synthetic perturbations in their Chain of Thought (CoT) process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the robustness of LLMs' self-correction capabilities and their performance in mathematical reasoning tasks, especially under variations in problem description and prompting strategy.

**Method:** Experiments were conducted to measure the intrinsic self-correction behavior of various open-weight LLMs across different datasets by introducing synthetic perturbations in the CoT reasoning.

**Key Contributions:**

	1. Demonstrated robust intrinsic self-correction across LLMs in mathematical reasoning tasks.
	2. Provided evidence that non-finetuned models display significant self-correction capabilities.
	3. Highlighted the importance of understanding LLMs' inherent traits for future reasoning model development.

**Result:** The study found that LLMs exhibited strong intrinsic self-correction behavior, correcting subtle errors and explicitly acknowledging mistakes, suggesting that they may have better self-correction capabilities than previously reported.

**Limitations:** 

**Conclusion:** The findings imply that LLMs, even those not fine-tuned for extended reasoning tasks, possess significant intrinsic self-correction abilities, indicating that reasoning model advancements may build on existing model traits.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent "reasoning" model work involves amplification of traits already meaningfully present in models.

</details>


### [41] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)

*Mohammad Amaan Sayeed, Mohammed Talha Alam, Raza Imam, Shahab Saquib Sohail, Amir Hussain*

**Main category:** cs.CL

**Keywords:** Islamic medicine, language models, health informatics, retrieval-augmented generation, cultural sensitivity

**Relevance Score:** 7

**TL;DR:** The paper presents Tibbe-AG, a novel evaluation pipeline that enhances the accessibility and reliability of Islamic medical texts in modern AI systems by assessing the accuracy of LLM-generated answers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To make centuries-old Islamic medical texts more accessible and applicable in modern AI systems, bridging cultural knowledge gaps and improving medical question-answering.

**Method:** A unified evaluation pipeline, Tibbe-AG, aligns questions from Prophetic medicine with human-verified remedies, comparing three LLMs under various configurations. Answers are assessed by an agentic judge LLM to generate a quality score.

**Key Contributions:**

	1. Introduction of the Tibbe-AG evaluation pipeline for Islamic medical texts.
	2. Validation of LLMs in culturally grounded medical guidance.
	3. Demonstration of improved accuracy through retrieval and agentic evaluation.

**Result:** Retrieval methods increased factual accuracy by 13%, while agentic prompting added another 10% improvement, demonstrating that integrating classical texts with AI can enhance medical question-answering.

**Limitations:** 

**Conclusion:** Blending classical Islamic medical knowledge with retrieval-augmented models and self-evaluation leads to reliable, culturally sensitive medical insights.

**Abstract:** Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.

</details>


### [42] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)

*Narutatsu Ri, Nicholas Deas, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** Large Language Models, perspective summarization, evaluation metrics, reranking methods, preference tuning

**Relevance Score:** 8

**TL;DR:** The paper explores improving perspective summarization with LLMs by identifying reliable quality metrics and demonstrating the efficacy of advanced summarization methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance unbiased perspective summarization in real-world applications, particularly in political contexts, by addressing gaps in evaluation metrics and summarization methods.

**Method:** The authors develop a benchmark test set using human annotations to evaluate metric reliability and compare traditional metrics with language model-based metrics, ultimately showing the advantages of LLM-based methods, including reranking and preference tuning.

**Key Contributions:**

	1. Identified reliable metrics for measuring the quality of perspective summaries
	2. Demonstrated that language model-based metrics are more effective than traditional metrics
	3. Showed that reranking and preference tuning significantly enhance summarization performance

**Result:** The study finds that traditional metrics are less effective than language model-based metrics in evaluating perspective summary quality. Reranking and preference tuning withsynthetically generated data significantly improve summarization performance.

**Limitations:** 

**Conclusion:** The research contributes to better evaluation practices and the development of more effective perspective summarization methods, suggesting that advanced LLM techniques are more reliable.

**Abstract:** Generating unbiased summaries in real-world settings such as political perspective summarization remains a crucial application of Large Language Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics for measuring key attributes such as coverage and faithfulness without verifying their applicability, and efforts to develop improved summarizers are still nascent. We address these gaps by (1) identifying reliable metrics for measuring perspective summary quality, and (2) investigating the efficacy of LLM-based methods beyond zero-shot inference. Namely, we build a test set for benchmarking metric reliability using human annotations and show that traditional metrics underperform compared to language model-based metrics, which prove to be strong evaluators. Using these metrics, we show that reranking-based methods yield strong results, and preference tuning with synthetically generated and reranking-labeled data further boosts performance. Our findings aim to contribute to the reliable evaluation and development of perspective summarization methods.

</details>


### [43] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)

*Toan Nguyen Hai, Ha Nguyen Viet, Truong Quan Xuan, Duc Do Minh*

**Main category:** cs.CL

**Keywords:** Vietnamese NLP, dataset, mBERT, text segmentation, reading comprehension

**Relevance Score:** 4

**TL;DR:** Introduction of a dataset for Vietnamese NLP tasks focused on segmentation and reading comprehension.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in resources for key NLP tasks in Vietnamese, which is under-resourced despite having a large number of speakers.

**Method:** Development of the VSMRC dataset, including 15,942 documents for text segmentation and 16,347 synthetic MRC question-answer pairs, with evaluation performed using mBERT.

**Key Contributions:**

	1. Introduction of VSMRC dataset
	2. Demonstration of mBERT's superiority over monolingual models
	3. Insights for applying multilingual models to under-resourced languages

**Result:** mBERT outperformed monolingual models, achieving 88.01% accuracy on the MRC test set and a 63.15% F1 score on text segmentation.

**Limitations:** Dataset is limited to Wikipedia content and synthetic questions; may not cover all real-world scenarios.

**Conclusion:** The study indicates that multilingual models can effectively enhance NLP tasks for Vietnamese, with implications for similar under-resourced languages.

**Abstract:** Vietnamese, the 20th most spoken language with over 102 million native speakers, lacks robust resources for key natural language processing tasks such as text segmentation and machine reading comprehension (MRC). To address this gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset includes 15,942 documents for text segmentation and 16,347 synthetic multiple-choice question-answer pairs generated with human quality assurance, ensuring a reliable and diverse resource. Experiments show that mBERT consistently outperforms monolingual models on both tasks, achieving an accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text segmentation test set. Our analysis reveals that multilingual models excel in NLP tasks for Vietnamese, suggesting potential applications to other under-resourced languages. VSMRC is available at HuggingFace

</details>


### [44] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)

*Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, Elena V. Epure*

**Main category:** cs.CL

**Keywords:** AI music generation, multimodal detection, audio perturbations

**Relevance Score:** 5

**TL;DR:** A novel approach for detecting AI-generated music is proposed using a multimodal pipeline combining transcribed lyrics and speech features, outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of detecting AI-generated music content and the limitations of existing detection methods based on audio or lyrics.

**Method:** A multimodal, modular late-fusion pipeline that uses automatically transcribed sung lyrics alongside audio-derived speech features.

**Key Contributions:**

	1. Introduces a multimodal detection approach combining lyrics and audio features.
	2. Demonstrates superior performance compared to traditional detection methods.
	3. Enhances robustness against audio perturbations and real-world applicability.

**Result:** The proposed method, DE-detect, shows improved performance over existing lyrics-based detectors and is more robust against audio perturbations.

**Limitations:** 

**Conclusion:** DE-detect offers a practical and effective solution for detecting AI-generated music in real-world scenarios, enhancing the reliability of detection methods.

**Abstract:** The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [45] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)

*Zhihan Guo, Jiele Wu, Wenqian Cui, Yifei Zhang, Minda Hu, Yufei Wang, Irwin King*

**Main category:** cs.CL

**Keywords:** Long-Form Generation, Reinforcement Learning, Large Language Models, ProxyReward, Open-ended tasks

**Relevance Score:** 9

**TL;DR:** Introduction of ProxyReward, a framework to enhance Open-ended Long Text Generation in LLMs by addressing the lack of curated reference data with innovative reward signals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generation of long-context outputs in LLMs, addressing the limitations in existing methods that lack specific, informative data for training.

**Method:** Development of the ProxyReward framework, consisting of a dataset generated through prompts and a novel reward signal for evaluating generation accuracy and comprehensiveness.

**Key Contributions:**

	1. Introduction of ProxyReward as a novel RL-based framework for Open-LTG.
	2. Creation of a dataset generated from prompts that reduces the need for labeled data.
	3. Development of tailored reward signals that improve accuracy in long-context generation.

**Result:** ProxyReward demonstrates a significant performance enhancement of 20% on Open-LTG tasks, outperforming GPT-4-Turbo and the LLM-as-a-Judge approach.

**Limitations:** The framework's effectiveness may depend on the nature of the prompts used for dataset generation and the scalability of the method to various domains.

**Conclusion:** ProxyReward effectively improves LLM capabilities in generating informative long-form responses to complex prompts, indicating its potential for further applications in diverse areas.

**Abstract:** Current research on long-form context in Large Language Models (LLMs) primarily focuses on the understanding of long-contexts, the Open-ended Long Text Generation (Open-LTG) remains insufficiently explored. Training a long-context generation model requires curation of gold standard reference data, which is typically nonexistent for informative Open-LTG tasks. However, previous methods only utilize general assessments as reward signals, which limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative reinforcement learning (RL) based framework, which includes a dataset and a reward signal computation method. Firstly, ProxyReward Dataset generation is accomplished through simple prompts that enables the model to create automatically, obviating extensive labeled data or significant manual effort. Secondly, ProxyReward Signal offers a targeted evaluation of information comprehensiveness and accuracy for specific questions. The experimental results indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can significantly enhance performance by 20% on the Open-LTG task when training widely used open-source models, while also surpassing the LLM-as-a-Judge approach. Our work presents effective methods to enhance the ability of LLMs to address complex open-ended questions posed by human.

</details>


### [46] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)

*Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, Hanlin Zhang*

**Main category:** cs.CL

**Keywords:** language model, training dynamics, pre-training, fine-tuning, reproducibility

**Relevance Score:** 8

**TL;DR:** EvoLM is a model suite for analyzing language model training dynamics across multiple stages, evaluating reasoning capabilities and generalization, while providing insights on pre-training practices and releasing all models and datasets for reproducibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enable systematic and transparent analysis of language models' training dynamics due to the complexity introduced by the multi-stage training process.

**Method:** Training over 100 LMs with 1B and 4B parameters from scratch, analyzing both upstream (language modeling) and downstream (problem-solving) reasoning capabilities.

**Key Contributions:**

	1. Introduction of EvoLM model suite for analyzing LM training dynamics.
	2. Systematic evaluation of reasoning capabilities and generalization across various stages.
	3. Release of training datasets and models for facilitating reproducibility in research.

**Result:** Identified diminishing returns from extensive pre-training, emphasized the significance of continued pre-training, and recognized intricate trade-offs in supervised fine-tuning and reinforcement learning configurations.

**Limitations:** 

**Conclusion:** The study underscores the importance of managing training dynamics to optimize model performance while providing resources for open research and reproducibility.

**Abstract:** Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.

</details>


### [47] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)

*Xinyue Huang, Ziqi Lin, Fang Sun, Wenchao Zhang, Kejian Tong, Yunbo Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, multi-hop reasoning, contextual understanding, LLaMA 3, question answering

**Relevance Score:** 9

**TL;DR:** This paper introduces a new RAG framework for complex question answering that enhances multi-hop reasoning and context understanding, leading to improved response accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in multi-hop reasoning and contextual understanding in long documents for question answering tasks.

**Method:** The framework combines a dense retrieval module with context fusion and multi-hop reasoning, optimized through a joint strategy of retrieval likelihood and generation cross-entropy.

**Key Contributions:**

	1. Novel RAG framework for question answering
	2. Integration of retrieval and generation mechanisms
	3. Joint optimization strategy for enhanced performance

**Result:** The proposed system outperforms existing RAG and generative baselines in terms of accuracy and coherence of responses.

**Limitations:** 

**Conclusion:** The framework demonstrates improved performance in delivering precise answers, confirming its effectiveness for complex question answering.

**Abstract:** This paper presents a novel Retrieval-Augmented Generation (RAG) framework tailored for complex question answering tasks, addressing challenges in multi-hop reasoning and contextual understanding across lengthy documents. Built upon LLaMA 3, the framework integrates a dense retrieval module with advanced context fusion and multi-hop reasoning mechanisms, enabling more accurate and coherent response generation. A joint optimization strategy combining retrieval likelihood and generation cross-entropy improves the model's robustness and adaptability. Experimental results show that the proposed system outperforms existing retrieval-augmented and generative baselines, confirming its effectiveness in delivering precise, contextually grounded answers.

</details>


### [48] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)

*Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık*

**Main category:** cs.CL

**Keywords:** large language models, dynamic scaling, budget allocation, sampling strategy, human-computer interaction

**Relevance Score:** 9

**TL;DR:** DynScaling boosts LLM performance under resource constraints using a novel sampling strategy and dynamic budget allocation without external verifiers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improving LLM performance while minimizing reliance on external verification and optimizing for practical computational limits.

**Method:** DynScaling employs an integrated parallel-sequential sampling strategy and a multi-armed bandit framework for dynamic budget allocation.

**Key Contributions:**

	1. Integrated parallel-sequential sampling strategy
	2. Bandit-based dynamic budget allocation framework
	3. Verifier-free model optimization

**Result:** DynScaling outperforms existing verifier-free scaling methods in task performance and reduces computational costs.

**Limitations:** 

**Conclusion:** By effectively managing computational resources and improving sampling methods, DynScaling enhances LLM performance in practical scenarios.

**Abstract:** Inference-time scaling has proven effective in boosting large language model (LLM) performance through increased test-time computation. Yet, its practical application is often hindered by reliance on external verifiers or a lack of optimization for realistic computational constraints. We propose DynScaling, which addresses these limitations through two primary innovations: an integrated parallel-sequential sampling strategy and a bandit-based dynamic budget allocation framework. The integrated sampling strategy unifies parallel and sequential sampling by constructing synthetic sequential reasoning chains from initially independent parallel responses, promoting diverse and coherent reasoning trajectories. The dynamic budget allocation framework formulates the allocation of computational resources as a multi-armed bandit problem, adaptively distributing the inference budget across queries based on the uncertainty of previously sampled responses, thereby maximizing computational efficiency. By combining these components, DynScaling effectively improves LLM performance under practical resource constraints without the need for external verifiers. Experimental results demonstrate that DynScaling consistently surpasses existing verifier-free inference scaling baselines in both task performance and computational cost.

</details>


### [49] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)

*Devesh Kumar*

**Main category:** cs.CL

**Keywords:** cyberbullying, machine learning, transformer models, explainability, content moderation

**Relevance Score:** 7

**TL;DR:** A hybrid architecture combining transformer-based models and broad learning systems for enhanced cyberbullying detection is proposed, achieving significantly high accuracy across several datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing prevalence of cyberbullying among teenagers by improving detection mechanisms using machine learning techniques.

**Method:** The proposed model integrates a modified DeBERTa with Squeeze-and-Excitation blocks and a Gated Broad Learning System classifier for robust cyberbullying detection.

**Key Contributions:**

	1. Development of a novel hybrid cyberbullying detection model
	2. Integration of explainability mechanisms
	3. High performance across multiple benchmark datasets

**Result:** The ModifiedDeBERTa + GBLS model reached 79.3% accuracy on HateXplain, 95.41% on SOSNet, 91.37% on Mendeley-I, and 94.67% on Mendeley-II datasets.

**Limitations:** Challenges remain in detecting implicit bias and sarcastic content.

**Conclusion:** The framework not only enhances detection performance but also emphasizes explainability, making it suitable for automated content moderation.

**Abstract:** The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\% accuracy on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and 94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.

</details>


### [50] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)

*Andy Yang, Michaël Cadilhac, David Chiang*

**Main category:** cs.CL

**Keywords:** transformers, depth, C-RASP, expressiveness, machine learning

**Relevance Score:** 8

**TL;DR:** The paper establishes formal insights on the capabilities gained with greater depth in transformers, providing theoretical proof and empirical evidence of deeper transformers' expressiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To formally determine the capabilities gained by increasing the depth of transformers in machine learning models.

**Method:** The study involves theoretical proofs showing that deeper transformers expressively outperform shallower ones by relating them to a programming language model (C-RASP), alongside an empirical study to validate these findings.

**Key Contributions:**

	1. Formal proof of greater expressiveness in deeper transformers
	2. Empirical evidence supporting theory on transformer depth
	3. Equivalence established between transformers and C-RASP programming language

**Result:** The research demonstrates that deeper C-RASP programs (and thus transformers) offer greater expressiveness, further supported by empirical evidence regarding their performance in sequential tasks without positional encodings.

**Limitations:** 

**Conclusion:** Deep transformers are shown to be more expressive than shallower ones, with theoretical backing and empirical validation indicating a better capability in handling sequential dependencies.

**Abstract:** It has been observed that transformers with greater depth (that is, more layers) have more capabilities, but can we establish formally which capabilities are gained with greater depth? We answer this question with a theoretical proof followed by an empirical study. First, we consider transformers that round to fixed precision except inside attention. We show that this subclass of transformers is expressively equivalent to the programming language C-RASP and this equivalence preserves depth. Second, we prove that deeper C-RASP programs are more expressive than shallower C-RASP programs, implying that deeper transformers are more expressive than shallower transformers (within the subclass mentioned above). These results are established by studying a form of temporal logic with counting operators, which was shown equivalent to C-RASP in previous work. Finally, we provide empirical evidence that our theory predicts the depth required for transformers without positional encodings to length-generalize on a family of sequential dependency tasks.

</details>


### [51] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)

*Duc Hieu Ho, Chenglin Fan*

**Main category:** cs.CL

**Keywords:** large language models, self-critique, honesty and helpfulness, curiosity-driven prompting, machine learning

**Relevance Score:** 9

**TL;DR:** This paper evaluates ten large language models and proposes a novel prompting strategy to enhance their honesty and helpfulness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Producing outputs that are consistently honest and helpful from large language models remains a challenge.

**Method:** The paper benchmarks ten large language models and introduces a self-critique-guided curiosity refinement prompting strategy, which allows models to self-critique and refine their outputs without retraining.

**Key Contributions:**

	1. Benchmark evaluation of ten large language models
	2. Introduction of self-critique-guided prompting strategy
	3. Demonstration of improvements in honesty and helpfulness of model outputs

**Result:** The proposed method shows consistent improvements in reducing poor-quality responses and increasing high-quality responses, with relative gains in honesty and helpfulness scores ranging from 1.4% to 4.3%.

**Limitations:** 

**Conclusion:** Structured self-refinement is an effective, scalable, and training-free strategy for improving the trustworthiness of outputs from large language models.

**Abstract:** Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks. However, producing outputs that are consistently honest and helpful remains an open challenge. To overcome this challenge, this paper tackles the problem through two complementary directions. It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from OpenAI, Meta, and Google. In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting. The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training. The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step.   The experiment results on the HONESET dataset evaluated using the framework $\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models. The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models. These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.

</details>


### [52] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)

*Devesh Kumar*

**Main category:** cs.CL

**Keywords:** cyberbullying detection, Hinglish, MURIL, machine learning, explainability

**Relevance Score:** 7

**TL;DR:** This paper presents a framework for detecting cyberbullying in Hinglish text, outperforming existing models with explainability features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of Hinglish in digital communication creates challenges for existing cyberbullying detection systems designed for monolingual text.

**Method:** A framework using the Multilingual Representations for Indian Languages (MURIL) architecture evaluated on six benchmark datasets.

**Key Contributions:**

	1. Development of a Hinglish cyberbullying detection framework using the MURIL architecture.
	2. Evaluation against multiple benchmark datasets demonstrating superior performance versus existing models.
	3. Incorporation of explainability features to enhance understanding of detection mechanisms.

**Result:** The MURIL-based approach outperforms existing multilingual models with accuracy improvements of up to 13.07 percentage points across various datasets.

**Limitations:** Challenges include context-dependent interpretation, cultural nuances, and sarcasm detection in cross-linguistic contexts.

**Conclusion:** The proposed framework improves detection performance with specialized techniques, while highlighting challenges for future research.

**Abstract:** The growth of digital communication platforms has led to increased cyberbullying incidents worldwide, creating a need for automated detection systems to protect users. The rise of code-mixed Hindi-English (Hinglish) communication on digital platforms poses challenges for existing cyberbullying detection systems, which were designed primarily for monolingual text. This paper presents a framework for cyberbullying detection in Hinglish text using the Multilingual Representations for Indian Languages (MURIL) architecture to address limitations in current approaches. Evaluation across six benchmark datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based approach outperforms existing multilingual models including RoBERTa and IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\% on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The framework includes explainability features through attribution analysis and cross-linguistic pattern recognition. Ablation studies show that selective layer freezing, appropriate classification head design, and specialized preprocessing for code-mixed content improve detection performance, while failure analysis identifies challenges including context-dependent interpretation, cultural understanding, and cross-linguistic sarcasm detection, providing directions for future research in multilingual cyberbullying detection.

</details>


### [53] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)

*Natapong Nitarach, Warit Sirichotedumrong, Panop Pitchayarthorn, Pittawat Taveekitworachai, Potsawee Manakul, Kunat Pipatanakul*

**Main category:** cs.CL

**Keywords:** chain-of-thought, financial NLP, prompt engineering, domain expertise, structured reasoning

**Relevance Score:** 4

**TL;DR:** FinCoT is a structured chain-of-thought prompting approach improving reasoning in large language models for financial applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning abilities of large language models in financial natural language processing (FinNLP) by integrating domain-specific expert insights into structured prompting approaches.

**Method:** The study evaluates three prompting styles—standard prompting, unstructured CoT, and structured CoT—specifically assessing the effectiveness of FinCoT on CFA-style questions across ten financial domains.

**Key Contributions:**

	1. Introduction of FinCoT as a structured prompting method in FinNLP.
	2. Comprehensive evaluation of three prompting styles in financial reasoning tasks.
	3. Demonstration of performance improvement and cost reduction through structured reasoning methods.

**Result:** FinCoT achieves an improvement in performance from 63.2% to 80.5% and an increase for Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while also significantly reducing the number of generated tokens by eight-fold compared to standard structured CoT prompting.

**Limitations:** 

**Conclusion:** Domain-aligned structured prompts lead to improved performance, lower inference costs, and more interpretable reasoning in financial applications.

**Abstract:** This paper presents FinCoT, a structured chain-of-thought (CoT) prompting approach that incorporates insights from domain-specific expert financial reasoning to guide the reasoning traces of large language models. We investigate that there are three main prompting styles in FinNLP: (1) standard prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an explicit reasoning structure, such as the use of tags; and (3) structured CoT prompting--CoT prompting with explicit instructions or examples that define structured reasoning steps. Previously, FinNLP has primarily focused on prompt engineering with either standard or unstructured CoT prompting. However, structured CoT prompting has received limited attention in prior work. Furthermore, the design of reasoning structures in structured CoT prompting is often based on heuristics from non-domain experts. In this study, we investigate each prompting approach in FinNLP. We evaluate the three main prompting styles and FinCoT on CFA-style questions spanning ten financial domains. We observe that FinCoT improves performance from 63.2% to 80.5% and Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens eight-fold compared to structured CoT prompting. Our findings show that domain-aligned structured prompts not only improve performance and reduce inference costs but also yield more interpretable and expert-aligned reasoning traces.

</details>


### [54] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)

*Chenxi Wang, Yixuan Zhang, Lang Gao, Zixiang Xu, Zirui Song, Yanbo Wang, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** causal reasoning, bilingual dataset, large language models, linguistic relativity, attention patterns

**Relevance Score:** 9

**TL;DR:** Introducing BICAUSE, a dataset for examining bilingual causal reasoning in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models may internalize reasoning biases shaped by language structures, influenced by linguistic relativity.

**Method:** Analysis of a structured bilingual dataset (BICAUSE) with Chinese and English samples for causal reasoning, evaluating LLM attention patterns and performance across languages.

**Key Contributions:**

	1. Development of the BICAUSE bilingual dataset for causal reasoning.
	2. Empirical evidence of LLMs internalizing language-specific reasoning biases.
	3. Analysis of attention patterns in LLMs highlighting differences between English and Chinese.

**Result:** LLMs show language-specific attention patterns and biases, with a notable difference in performance related to causal structures in Chinese versus English.

**Limitations:** The study focuses primarily on two languages (English and Chinese), limiting generalizability to other languages.

**Conclusion:** LLMs not only imitate linguistic forms but also embody reasoning biases associated with the languages they are trained on, validated through structural analysis of model internals.

**Abstract:** Language is not only a tool for communication but also a medium for human cognition and reasoning. If, as linguistic relativity suggests, the structure of language shapes cognitive patterns, then large language models (LLMs) trained on human language may also internalize the habitual logical structures embedded in different languages. To examine this hypothesis, we introduce BICAUSE, a structured bilingual dataset for causal reasoning, which includes semantically aligned Chinese and English samples in both forward and reversed causal forms. Our study reveals three key findings: (1) LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English. (2) Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese. (3) When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form. Overall, these results suggest that LLMs not only mimic surface linguistic forms but also internalize the reasoning biases shaped by language. Rooted in cognitive linguistic theory, this phenomenon is for the first time empirically verified through structural analysis of model internals.

</details>


### [55] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)

*Guanhua Chen, Yutong Yao, Lidia S. Chao, Xuebo Liu, Derek F. Wong*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Self-Guided Calibration

**Relevance Score:** 9

**TL;DR:** This paper introduces the Self-Guided Iterative Calibration (SGIC) Framework that enhances the calibration of large language models (LLMs) using uncertainty scores in multi-round processes.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings in the calibration capabilities of LLMs often overlooked in retrieval-augmented generation (RAG) methodologies.

**Method:** The SGIC Framework utilizes uncertainty scores to assess relevance and confidence in responses, iteratively refining these scores by integrating them with prior outputs for improved calibration.

**Key Contributions:**

	1. Introduction of the SGIC Framework for calibration
	2. Utilization of uncertainty scores for relevance and confidence assessment
	3. Development of an iterative self-calibration training set

**Result:** The framework demonstrates substantial improvements in the performance of both closed-source and open-weight LLMs through enhanced calibration.

**Limitations:** 

**Conclusion:** The SGIC Framework offers a novel approach to improve LLMs' response accuracy by capitalizing on uncertainty scores and iterative self-calibration processes.

**Abstract:** Recent research in retrieval-augmented generation (RAG) has concentrated on retrieving useful information from candidate documents. However, numerous methodologies frequently neglect the calibration capabilities of large language models (LLMs), which capitalize on their robust in-context reasoning prowess. This work illustrates that providing LLMs with specific cues substantially improves their calibration efficacy, especially in multi-round calibrations. We present a new SGIC: Self-Guided Iterative Calibration Framework that employs uncertainty scores as a tool. Initially, this framework calculates uncertainty scores to determine both the relevance of each document to the query and the confidence level in the responses produced by the LLMs. Subsequently, it reevaluates these scores iteratively, amalgamating them with prior responses to refine calibration. Furthermore, we introduce an innovative approach for constructing an iterative self-calibration training set, which optimizes LLMs to efficiently harness uncertainty scores for capturing critical information and enhancing response accuracy. Our proposed framework significantly improves performance on both closed-source and open-weight LLMs.

</details>


### [56] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)

*Masashi Takeshita, Rafal Rzepka*

**Main category:** cs.CL

**Keywords:** JETHICS, AI Ethics, Japanese Language, LLM Evaluation, Commonsense Morality

**Relevance Score:** 6

**TL;DR:** JETHICS is a Japanese dataset designed to evaluate the ethical understanding of AI models, revealing that current LLMs, including GPT-4o, show significant limitations in this area.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the ethics understanding of AI models, particularly in the context of Japanese language and culture.

**Method:** A dataset called JETHICS was created, consisting of 78K examples based on normative theories and commonsense morality, aligned with the methodology of the English ETHICS dataset.

**Key Contributions:**

	1. Introduction of JETHICS dataset for Japanese AI ethics evaluation
	2. Demonstration of low ethical understanding scores in state-of-the-art LLMs
	3. Provision of a framework for future research on ethics in AI

**Result:** Evaluation experiments showed that GPT-4o scores about 0.7 and the best-performing Japanese LLM scores around 0.5, indicating a need for improvement.

**Limitations:** The dataset is limited to Japanese language and may not be fully representative of all ethical perspectives.

**Conclusion:** The findings highlight a considerable gap in the ethical understanding of current LLMs, suggesting that more work is needed to enhance this aspect of AI development.

**Abstract:** In this work, we propose JETHICS, a Japanese dataset for evaluating ethics understanding of AI models. JETHICS contains 78K examples and is built by following the construction methods of the existing English ETHICS dataset. It includes four categories based normative theories and concepts from ethics and political philosophy; and one representing commonsense morality. Our evaluation experiments on non-proprietary large language models (LLMs) and on GPT-4o reveal that even GPT-4o achieves only an average score of about 0.7, while the best-performing Japanese LLM attains around 0.5, indicating a relatively large room for improvement in current LLMs.

</details>


### [57] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)

*Luna Wang, Andrew Caines, Alice Hutchings*

**Main category:** cs.CL

**Keywords:** hate speech, dataset curation, methodological rigor, transparency, reflexivity

**Relevance Score:** 4

**TL;DR:** The paper examines the complexities and methodologies involved in curating hate speech datasets, calling for a reflexive approach to enhance transparency and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the balance of competing priorities in hate speech dataset curation and improve dataset reliability through methodological rigor.

**Method:** The authors analyze a variety of hate speech datasets, emphasizing the need for transparency and reflexivity based on Max Weber's ideal types.

**Key Contributions:**

	1. Critical examination of hate speech dataset methodologies
	2. Emphasis on reflexive practices during dataset construction
	3. Call for greater transparency in research methods

**Result:** The study identifies common themes and practices in hate speech dataset curation that impact reliability and suggests a reflexive approach for researchers.

**Limitations:** 

**Conclusion:** A reflexive approach encourages researchers to recognize their value judgments in dataset creation, leading to more reliable and transparent datasets.

**Abstract:** The curation of hate speech datasets involves complex design decisions that balance competing priorities. This paper critically examines these methodological choices in a diverse range of datasets, highlighting common themes and practices, and their implications for dataset reliability. Drawing on Max Weber's notion of ideal types, we argue for a reflexive approach in dataset creation, urging researchers to acknowledge their own value judgments during dataset construction, fostering transparency and methodological rigour.

</details>


### [58] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)

*Anindita Bhattacharya, Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay*

**Main category:** cs.CL

**Keywords:** abstractive summarization, radiology reports, large language models, MIMIC-CXR dataset, healthcare

**Relevance Score:** 8

**TL;DR:** This research investigates advanced abstractive summarization models for generating concise impressions from detailed findings in radiology reports using the MIMIC-CXR dataset.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The detailed findings section of radiology reports is lengthy, while the impression section is concise; improving summarization can enhance medical professionals' efficiency.

**Method:** A comparative analysis of various pre-trained large language models (T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, Pointer Generator Network) was conducted, employing multiple evaluation metrics such as ROUGE and BERTScore.

**Key Contributions:**

	1. Exploration of various large language models for summarization in radiology reports
	2. Comprehensive evaluation using multiple metrics to analyze performance
	3. Identification of strengths and limitations of summarization models in a healthcare context

**Result:** Different models displayed varying strengths and limitations in summarizing medical text; insights were derived on the effectiveness of each model for medical summarization.

**Limitations:** 

**Conclusion:** The research offers valuable insights for the development of automated summarization tools beneficial to medical professionals in healthcare.

**Abstract:** The findings section of a radiology report is often detailed and lengthy, whereas the impression section is comparatively more compact and captures key diagnostic conclusions. This research explores the use of advanced abstractive summarization models to generate the concise impression from the findings section of a radiology report. We have used the publicly available MIMIC-CXR dataset. A comparative analysis is conducted on leading pre-trained and open-source large language models, including T5-base, BART-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network with a coverage mechanism. To ensure a thorough assessment, multiple evaluation metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BERTScore. By analyzing the performance of these models, this study identifies their respective strengths and limitations in the summarization of medical text. The findings of this paper provide helpful information for medical professionals who need automated summarization solutions in the healthcare sector.

</details>


### [59] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)

*Aishwarya Pothula, Bhavana Akkiraju, Srihari Bandarupalli, Charan D, Santosh Kesiraju, Anil Kumar Vuppala*

**Main category:** cs.CL

**Keywords:** speech-to-text, low-resource languages, weakly labeled data, dataset construction, bitext mining

**Relevance Score:** 4

**TL;DR:** This paper investigates using weakly labeled data for speech-to-text translation in low-resource languages, demonstrating comparable performance to established multilingual systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of high-quality annotated data for low-resource languages challenges the development of effective speech-to-text systems.

**Method:** The paper constructs datasets using bitext mining and sentence encoders, creating a dataset named Shrutilipi-anuvaad for specific language pairs. Various versions of training data were created to analyze the impact of data quality and quantity.

**Key Contributions:**

	1. Construction of the Shrutilipi-anuvaad dataset for low-resource language pairs
	2. Analysis of the effects of data quality and quantity on ST model performance
	3. Demonstration of competitive performance with weakly labeled data compared to established models

**Result:** Experimental results indicate that speech-to-text systems can be effectively built using weakly labeled data, achieving performance on par with advanced multilingual systems like SONAR and SeamlessM4T.

**Limitations:** The effectiveness of the approach may vary with different low-resource languages and the quality of weakly labeled data.

**Conclusion:** Weakly labeled data is viable for developing efficient speech-to-text translation models in low-resource scenarios.

**Abstract:** The scarcity of high-quality annotated data presents a significant challenge in developing effective end-to-end speech-to-text translation (ST) systems, particularly for low-resource languages. This paper explores the hypothesis that weakly labeled data can be used to build ST models for low-resource language pairs. We constructed speech-to-text translation datasets with the help of bitext mining using state-of-the-art sentence encoders. We mined the multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi, Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data with varying degrees of quality and quantity to investigate the effect of quality versus quantity of weakly labeled data on ST model performance. Results demonstrate that ST systems can be built using weakly labeled data, with performance comparable to massive multi-modal multilingual baselines such as SONAR and SeamlessM4T.

</details>


### [60] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)

*Hao-Chien Lu, Jhen-Ke Lin, Hong-Yun Lin, Chung-Chun Wang, Berlin Chen*

**Main category:** cs.CL

**Keywords:** automated speaking assessment, content relevance, grammar error correction, machine learning, multifaceted evaluation

**Relevance Score:** 6

**TL;DR:** This paper presents a hybrid scoring model for automated speaking assessment (ASA) systems that improves content relevance evaluation and grammar error identification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current ASA systems often overlook content relevance and rely on superficial grammar analyses, lacking detailed error categorization.

**Method:** The paper introduces a multifaceted relevance module that combines question content, related images, exemplars, and L2 spoken responses. Additionally, it incorporates fine-grained grammar error features through advanced correction techniques and detailed annotations.

**Key Contributions:**

	1. Development of a multifaceted relevance module for content assessment
	2. Implementation of fine-grained grammar error features
	3. Demonstration of improved ASA performance through experimental validation

**Result:** Experiments show that the enhancements significantly improve the assessment of content relevance, language use, and overall performance of ASA systems.

**Limitations:** 

**Conclusion:** Using richer, more nuanced feature sets leads to a more comprehensive speaking assessment and better identification of specific errors.

**Abstract:** Current automated speaking assessment (ASA) systems for use in multi-aspect evaluations often fail to make full use of content relevance, overlooking image or exemplar cues, and employ superficial grammar analysis that lacks detailed error types. This paper ameliorates these deficiencies by introducing two novel enhancements to construct a hybrid scoring model. First, a multifaceted relevance module integrates question and the associated image content, exemplar, and spoken response of an L2 speaker for a comprehensive assessment of content relevance. Second, fine-grained grammar error features are derived using advanced grammar error correction (GEC) and detailed annotation to identify specific error categories. Experiments and ablation studies demonstrate that these components significantly improve the evaluation of content relevance, language use, and overall ASA performance, highlighting the benefits of using richer, more nuanced feature sets for holistic speaking assessment.

</details>


### [61] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)

*Aleksandra Krasnodębska, Karolina Seweryn, Szymon Łukasik, Wojciech Kusa*

**Main category:** cs.CL

**Keywords:** language models, safety classification, Polish language, adversarial evaluation, HerBERT

**Relevance Score:** 7

**TL;DR:** This paper presents a benchmark dataset for assessing the safety of large language models in Polish and evaluates different model architectures to improve performance under adversarial conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant bias in existing language model safety assessments favoring high-resource languages, leaving many global languages, including Polish, inadequately addressed.

**Method:** A benchmark dataset for safety classification in Polish is created, along with adversarially perturbed samples to test model robustness. Three models (Llama-Guard-3-8B, a HerBERT-based classifier, and PLLuM) are fine-tuned and evaluated against publicly available guard models.

**Key Contributions:**

	1. Introduction of a benchmark dataset for Polish language model safety assessment
	2. Development of adversarially perturbed samples to evaluate model robustness
	3. Demonstration of the HerBERT-based classifier's superior performance under challenging conditions

**Result:** The experiments show that the HerBERT-based classifier outperforms other models, especially in adversarial scenarios.

**Limitations:** 

**Conclusion:** The findings highlight the importance of creating inclusive safety assessments for language models and demonstrate the effectiveness of Polish-adapted solutions in enhancing performance.

**Abstract:** Despite increasing efforts to ensure the safety of large language models (LLMs), most existing safety assessments and moderation tools remain heavily biased toward English and other high-resource languages, leaving majority of global languages underexamined. To address this gap, we introduce a manually annotated benchmark dataset for language model safety classification in Polish. We also create adversarially perturbed variants of these samples designed to challenge model robustness. We conduct a series of experiments to evaluate LLM-based and classifier-based models of varying sizes and architectures. Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B model. We train these models using different combinations of annotated data and evaluate their performance, comparing it against publicly available guard models. Results demonstrate that the HerBERT-based classifier achieves the highest overall performance, particularly under adversarial conditions.

</details>


### [62] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)

*Taylor Lynn Curtis, Maximilian Puelma Touzel, William Garneau, Manon Gruaz, Mike Pinder, Li Wei Wang, Sukanya Krishna, Luda Cohen, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine*

**Main category:** cs.CL

**Keywords:** Misinformation, Fact-checking, Large Language Models, AI, Media literacy

**Relevance Score:** 8

**TL;DR:** Veracity is an open-source AI system that empowers individuals to combat misinformation through transparent fact-checking using LLMs and web retrieval agents.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant threat of misinformation exacerbated by generative AI, and to foster media literacy.

**Method:** The system analyzes user-submitted claims using LLMs in conjunction with web retrieval agents to assess claim veracity.

**Key Contributions:**

	1. Open-source design for transparent fact-checking
	2. Integration of LLMs with web retrieval for claim assessment
	3. Interactive and user-friendly interface for improving media literacy

**Result:** Veracity provides grounded veracity assessments and intuitive explanations with features like multilingual support and numerical scoring.

**Limitations:** 

**Conclusion:** The tool not only detects misinformation but also explains the reasoning behind its assessments, promoting an informed society.

**Abstract:** The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.

</details>


### [63] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)

*Agnese Daffara, Sourabh Dattawad, Sebastian Padó, Tanise Ceron*

**Main category:** cs.CL

**Keywords:** media framing, cross-cultural communication, political discourse, frame analysis, natural language processing

**Relevance Score:** 5

**TL;DR:** The paper introduces FrameNews-PT, a dataset for Brazilian Portuguese news articles, analyzing how well established U.S. media frames apply to Brazilian political and economic contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how political language and framing affect opinions in different cultural contexts beyond the U.S.

**Method:** Creation of the FrameNews-PT dataset and evaluation of MFC frames through annotation across multiple rounds, assessing their relevance and performance of models on out-of-domain data.

**Key Contributions:**

	1. Introduction of FrameNews-PT dataset
	2. Evaluation of MFC frame applicability in Brazilian context
	3. Insights on performance of models in zero-shot scenarios

**Result:** The study finds that MFC frames are broadly applicable to Brazilian news with minor adjustments, though some frames are underutilized and new issues often require general fallback frames.

**Limitations:** The study may be limited by the cultural nuances in Brazilian political discourse that are not captured by MFC frames.

**Conclusion:** Cross-cultural application of media frames needs careful attention to local contexts and language nuances.

**Abstract:** Frames capture aspects of an issue that are emphasized in a debate by interlocutors and can help us understand how political language conveys different perspectives and ultimately shapes people's opinions. The Media Frame Corpus (MFC) is the most commonly used framework with categories and detailed guidelines for operationalizing frames. It is, however, focused on a few salient U.S. news issues, making it unclear how well these frames can capture news issues in other cultural contexts. To explore this, we introduce FrameNews-PT, a dataset of Brazilian Portuguese news articles covering political and economic news and annotate it within the MFC framework. Through several annotation rounds, we evaluate the extent to which MFC frames generalize to the Brazilian debate issues. We further evaluate how fine-tuned and zero-shot models perform on out-of-domain data. Results show that the 15 MFC frames remain broadly applicable with minor revisions of the guidelines. However, some MFC frames are rarely used, and novel news issues are analyzed using general 'fall-back' frames. We conclude that cross-cultural frame use requires careful consideration.

</details>


### [64] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)

*Cedric Möller, Ricardo Usbeck*

**Main category:** cs.CL

**Keywords:** relation extraction, knowledge graph, Neural Bellman-Ford, supervised learning, zero-shot learning

**Relevance Score:** 7

**TL;DR:** Incorporating knowledge graph information enhances relation extraction model performance across diverse datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Explore the significance of knowledge graph positions in improving relation extraction tasks.

**Method:** Conducted experiments on various datasets using relation extraction methods integrated with graph-aware Neural Bellman-Ford networks, analyzed in both supervised and zero-shot settings.

**Key Contributions:**

	1. Demonstrated improved relation extraction through knowledge graph integration
	2. Established the effectiveness of graph-aware Neural Bellman-Ford networks
	3. Showcased benefits in both supervised and zero-shot learning settings

**Result:** Integrating knowledge graph information led to significant performance improvements, particularly under training sample imbalances.

**Limitations:** 

**Conclusion:** Knowledge graph features are crucial for effective relation extraction and can enhance model performance across different scenarios.

**Abstract:** We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.

</details>


### [65] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)

*Cedric Möller, Ricardo Usbeck*

**Main category:** cs.CL

**Keywords:** closed information extraction, relation extraction, discriminative models, type-information, efficiency

**Relevance Score:** 6

**TL;DR:** The paper presents a novel discriminative approach for closed information extraction that improves relation extraction accuracy, especially for long-tail relations, and demonstrates efficiency with smaller models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance relation extraction accuracy and efficiency, particularly for long-tail relations in large-scale closed information extraction tasks.

**Method:** A discriminative method incorporating type and entity-specific information for relation extraction.

**Key Contributions:**

	1. A novel discriminative approach for closed information extraction.
	2. Integration of type-information significantly enhances performance.
	3. Demonstrated efficiency in extracting relations from large-scale data with smaller models.

**Result:** The proposed method outperforms state-of-the-art end-to-end generative models in relation extraction.

**Limitations:** 

**Conclusion:** The method offers a more accurate and efficient approach to closed information extraction by leveraging smaller models while maintaining performance.

**Abstract:** This paper introduces a novel method for closed information extraction. The method employs a discriminative approach that incorporates type and entity-specific information to improve relation extraction accuracy, particularly benefiting long-tail relations. Notably, this method demonstrates superior performance compared to state-of-the-art end-to-end generative models. This is especially evident for the problem of large-scale closed information extraction where we are confronted with millions of entities and hundreds of relations. Furthermore, we emphasize the efficiency aspect by leveraging smaller models. In particular, the integration of type-information proves instrumental in achieving performance levels on par with or surpassing those of a larger generative model. This advancement holds promise for more accurate and efficient information extraction techniques.

</details>


### [66] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)

*Iwan Williams*

**Main category:** cs.CL

**Keywords:** Large Language Models, Representation, Structural Correspondence, Task Performance, Text-Boundedness

**Relevance Score:** 8

**TL;DR:** This paper examines the representational capacities of Large Language Models (LLMs) and argues that structural correspondences alone are insufficient for grounding representation of real-world entities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the understanding of whether LLMs can represent real-world entities and how this representation might be justified.

**Method:** The paper employs a structural-correspondence based account of representation to analyze LLMs, examining the potential for LLMs to ground representations based on their successful task performance.

**Key Contributions:**

	1. Exploration of structural correspondences in LLMs
	2. Analysis of task performance in relation to representation
	3. Discussion on the limitations of text-boundedness in grounding representation.

**Result:** The findings suggest that while structural correspondences exist between LLMs and worldly entities, they must play a role that facilitates successful task performance to effectively ground representation.

**Limitations:** The paper acknowledges the challenge posed by the text-boundedness of LLMs in engaging in tasks that would lead to suitable representation.

**Conclusion:** If LLMs can engage in tasks that exploit these structural correspondences appropriately, they could potentially represent real-world contents despite their text-bounded nature.

**Abstract:** Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.

</details>


### [67] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)

*Kexin Huang, Qian Tu, Liwei Fan, Chenchen Yang, Dong Zhang, Shimin Li, Zhaoye Fei, Qinyuan Cheng, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, paralinguistic information, natural-language instructions, benchmark, instruction-following TTS

**Relevance Score:** 6

**TL;DR:** The paper introduces InstructTTSEval, a benchmark for evaluating complex natural-language style control in TTS systems, addressing current limitations in instruction-following capabilities and benchmarks.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more flexible and accurate Text-to-Speech (TTS) systems that can interpret complex natural-language instructions and better convey paralinguistic information.

**Method:** Introduction of InstructTTSEval benchmark with three tasks for evaluating TTS systems, along with use of Gemini as an automatic judge for instruction-following capabilities.

**Key Contributions:**

	1. Introduction of InstructTTSEval benchmark for TTS systems
	2. Development of three specific tasks for evaluation: Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play
	3. Use of Gemini for automated assessment of instruction-following capabilities

**Result:** The evaluation of current instruction-following TTS systems reveals significant areas for improvement, highlighting the benchmark's effectiveness in measuring TTS capabilities.

**Limitations:** Current TTS systems show substantial room for improvement; existing challenges remain in interpreting complex instructions accurately.

**Conclusion:** InstructTTSEval is expected to facilitate advancements in TTS systems by providing a structured way to assess and enhance instruction-following abilities.

**Abstract:** In modern speech synthesis, paralinguistic information--such as a speaker's vocal timbre, emotional state, and dynamic prosody--plays a critical role in conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS) systems rely on fixed style labels or inserting a speech prompt to control these cues, which severely limits flexibility. Recent attempts seek to employ natural-language instructions to modulate paralinguistic features, substantially improving the generalization of instruction-driven TTS models. Although many TTS systems now support customized synthesis via textual description, their actual ability to interpret and execute complex instructions remains largely unexplored. In addition, there is still a shortage of high-quality benchmarks and automated evaluation metrics specifically designed for instruction-based TTS, which hinders accurate assessment and iterative optimization of these models. To address these limitations, we introduce InstructTTSEval, a benchmark for measuring the capability of complex natural-language style control. We introduce three tasks, namely Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play, including English and Chinese subsets, each with 1k test cases (6k in total) paired with reference audio. We leverage Gemini as an automatic judge to assess their instruction-following abilities. Our evaluation of accessible instruction-following TTS systems highlights substantial room for further improvement. We anticipate that InstructTTSEval will drive progress toward more powerful, flexible, and accurate instruction-following TTS.

</details>


### [68] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)

*Hao Li, Viktor Schlegel, Yizheng Sun, Riza Batista-Navarro, Goran Nenadic*

**Main category:** cs.CL

**Keywords:** Argument Mining, Large Language Models, Natural Language Processing, Taxonomy, Evaluation Practices

**Relevance Score:** 9

**TL;DR:** This survey analyzes the impact of Large Language Models on Argument Mining (AM), highlighting advancements, methodologies, and a research agenda.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To synthesize recent advancements in LLM-driven Argument Mining and propose a research agenda.

**Method:** The paper reviews theoretical foundations, annotation frameworks, and datasets related to AM, and presents a taxonomy of AM subtasks, alongside a critical assessment of current methodologies and challenges.

**Key Contributions:**

	1. Comprehensive taxonomy of AM subtasks influenced by LLM techniques
	2. Critical evaluation of current LLM architectures in AM
	3. Identification of key challenges in long-context reasoning and interpretability.

**Result:** The survey catalogues advancements in LLM techniques for AM, detailing their impact on subtask execution and evaluation practices while identifying key challenges.

**Limitations:** 

**Conclusion:** A forward-looking research agenda is proposed, addressing the evolving landscape of LLM-based computational argumentation.

**Abstract:** Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.

</details>


### [69] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)

*Sani Abdullahi Sani, Salim Abubakar, Falalu Ibrahim Lawan, Abdulhamid Abubakar, Maryam Bala*

**Main category:** cs.CL

**Keywords:** emotion detection, Hausa, AfriBERTa, low-resource languages, transformer models

**Relevance Score:** 3

**TL;DR:** The paper discusses a multi-label emotion detection approach for Hausa using AfriBERTa, achieving notable accuracy in a low-resource setting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of emotion detection in Hausa, a low-resource African language, by leveraging transformer-based models.

**Method:** The approach includes data preprocessing, tokenization, and fine-tuning AfriBERTa with the Hugging Face Trainer API for classifying text into six emotions: anger, disgust, fear, joy, sadness, and surprise.

**Key Contributions:**

	1. Fine-tuning of AfriBERTa on Hausa text for emotion detection
	2. Classifying text into six distinct emotions
	3. Validation of transformer models in low-resource language settings

**Result:** The system achieved a validation accuracy of 74.00% and an F1-score of 73.50%.

**Limitations:** 

**Conclusion:** The results demonstrate the effectiveness of transformer models for emotion detection even in low-resource languages like Hausa.

**Abstract:** This paper presents our approach to multi-label emotion detection in Hausa, a low-resource African language, as part of SemEval Track A. We fine-tuned AfriBERTa, a transformer-based model pre-trained on African languages, to classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and surprise. Our methodology involved data preprocessing, tokenization, and model fine-tuning using the Hugging Face Trainer API. The system achieved a validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the effectiveness of transformer-based models for emotion detection in low-resource languages.

</details>


### [70] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)

*Jiaxin Pei, Dustin Wright, Isabelle Augenstin, David Jurgens*

**Main category:** cs.CL

**Keywords:** public perception, science communication, NLP models

**Relevance Score:** 5

**TL;DR:** This paper introduces a computational framework to model public perception of science news across various dimensions, presents a large dataset on science perception, and develops NLP models to predict public engagement with scientific content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance public engagement and understanding of science by analyzing how audiences perceive and interact with scientific news in the face of overwhelming information.

**Method:** A computational framework was created to assess public perception across twelve dimensions. A dataset with 10,489 annotations from 2,101 participants was established, and NLP models were developed to predict perception scores.

**Key Contributions:**

	1. Introduction of a perception modeling framework for science communication
	2. Creation of a large-scale science news perception dataset
	3. Development of NLP models to predict engagement based on public perception

**Result:** The study finds that the frequency of science news consumption significantly influences public perception, while demographic factors play a lesser role. Predicted public perception scores correlate with higher levels of engagement, such as comments and upvotes on social media posts.

**Limitations:** 

**Conclusion:** This research highlights the critical role of perception modeling in science communication and suggests that better understanding of public perception can inform strategies for increasing engagement with scientific content.

**Abstract:** Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.

</details>


### [71] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)

*Chenyi Zhou, Zhengyan Shi, Yuan Yao, Lei Liang, Huajun Chen, Qiang Zhang*

**Main category:** cs.CL

**Keywords:** large language models, prompt optimization, semantic drift, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces Residual Optimization Tree (RiOT), a framework for automated prompt optimization in large language models that enhances performance by generating diverse candidates and mitigating semantic drift.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effective use of large language models (LLMs) by automating prompt optimization, addressing diversity and semantic drift issues in existing methods.

**Method:** RiOT refines prompts iteratively using text gradients, generates semantically diverse candidates, and uses perplexity for selection while retaining beneficial content through a text residual connection in a tree structure.

**Key Contributions:**

	1. Introduction of the Residual Optimization Tree (RiOT) framework.
	2. Demonstration of improved performance in prompt optimization across multiple benchmarks.
	3. Mitigation of semantic drift through selective content retention.

**Result:** RiOT outperforms previous prompt optimization methods and manual prompting across five benchmarks involving different reasoning tasks.

**Limitations:** 

**Conclusion:** The proposed RiOT framework significantly improves prompt optimization, thus enhancing the performance of LLMs in various reasoning tasks.

**Abstract:** Recent advancements in large language models (LLMs) have highlighted their potential across a variety of tasks, but their performance still heavily relies on the design of effective prompts. Existing methods for automatic prompt optimization face two challenges: lack of diversity, limiting the exploration of valuable and innovative directions and semantic drift, where optimizations for one task can degrade performance in others. To address these issues, we propose Residual Optimization Tree (RiOT), a novel framework for automatic prompt optimization. RiOT iteratively refines prompts through text gradients, generating multiple semantically diverse candidates at each step, and selects the best prompt using perplexity. Additionally, RiOT incorporates the text residual connection to mitigate semantic drift by selectively retaining beneficial content across optimization iterations. A tree structure efficiently manages the optimization process, ensuring scalability and flexibility. Extensive experiments across five benchmarks, covering commonsense, mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT outperforms both previous prompt optimization methods and manual prompting.

</details>


### [72] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)

*Yao Lu, Zhaiyuan Ji, Jiawei Du, Yu Shanqing, Qi Xuan, Tianyi Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Small Language Models, annotation framework, human-computer interaction, continual learning

**Relevance Score:** 8

**TL;DR:** The paper presents AutoAnnotator, an innovative annotation framework that combines LLMs and SLMs to improve annotation accuracy and reduce costs in large-scale tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and low accuracy associated with LLMs when performing fine-grained semantic annotation tasks.

**Method:** AutoAnnotator features a two-layer architecture: a meta-controller layer utilizing LLMs for selecting SLMs and generating code, and a task-specialist layer using multiple SLMs that perform annotation through multi-model voting and continual learning.

**Key Contributions:**

	1. Introduction of the AutoAnnotator framework combining LLMs and SLMs for annotation.
	2. Demonstrated significant cost savings and accuracy improvements over standard LLM approaches.
	3. Utilization of continual learning strategies to enhance SLM performance based on challenging annotation samples.

**Result:** AutoAnnotator demonstrates significant improvements, outperforming existing LLMs in various settings while reducing annotation costs by 74.15% and increasing accuracy by 6.21%.

**Limitations:** 

**Conclusion:** The study establishes that a hybrid model utilizing both LLMs and SLMs can enhance annotation efficiency and accuracy for fine-grained tasks.

**Abstract:** Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [73] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)

*Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, Weiran Xu*

**Main category:** cs.CL

**Keywords:** large language models, code reasoning, benchmark, OJBench, programming competitions

**Relevance Score:** 9

**TL;DR:** OJBench is a new benchmark for assessing competitive-level code reasoning abilities of large language models (LLMs) using 232 programming competition problems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing code benchmarks in evaluating the full spectrum of code reasoning capabilities of LLMs.

**Method:** The benchmark, OJBench, was created by compiling 232 programming problems from competitive programming contests. A comprehensive evaluation was conducted on 37 models to assess their performance.

**Key Contributions:**

	1. Introduction of OJBench as a novel benchmark for LLM evaluation in code reasoning
	2. Evaluation of 37 different models highlights performance gaps in state-of-the-art LLMs
	3. Identification of the discrepancy between model capabilities and competitive programming challenges

**Result:** The evaluation revealed that even the most advanced reasoning-oriented models struggled with the challenging problems presented in OJBench.

**Limitations:** Focuses only on a limited set of programming competition problems and may not cover all reasoning scenarios.

**Conclusion:** There are significant challenges for LLMs in achieving competitive-level code reasoning, indicating a need for improved capabilities in this area.

**Abstract:** Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.

</details>


### [74] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)

*Shushanta Pudasaini, Aman Shakya, Siddhartha Shrestha, Sahil Bhatta, Sunil Thapa, Sushmita Palikhe*

**Main category:** cs.CL

**Keywords:** Nepali language, Large Language Models, NLP

**Relevance Score:** 3

**TL;DR:** Proposes NepaliGPT, the first generative language model for the Nepali language, along with an advanced corpus and benchmark dataset.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a generative language model for the Nepali language, enabling further exploration of NLP tasks.

**Method:** Introduces NepaliGPT, utilizing an advanced corpus called the Devanagari Corpus and a benchmark dataset of 4,296 question-answer pairs in Nepali.

**Key Contributions:**

	1. Introduction of NepaliGPT, the first LLM for Nepali
	2. Development of the Devanagari Corpus
	3. Creation of a benchmark dataset with 4,296 question-answer pairs

**Result:** NepaliGPT achieves metrics such as perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25%, and causal consistency of 85.41%.

**Limitations:** 

**Conclusion:** NepaliGPT fills a critical gap in Nepali NLP research and facilitates further NLP task development in the language.

**Abstract:** After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal consistency of 85.41\%.

</details>


### [75] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)

*Zhen Xu, Shang Zhu, Jue Wang, Junlin Wang, Ben Athiwaratkun, Chi Wang, James Zou, Ce Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, long context, chunking, aggregator strategies, multi-agent

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for addressing challenges related to applying Large Language Models (LLMs) to long texts, analyzing failure modes and proposing effective chunking strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research seeks to understand and improve the performance of LLMs on long context tasks, addressing common failures in processing.

**Method:** The authors propose a theoretical framework categorizing failure modes into cross-chunk dependence, growing confusion with context size, and imperfect result integration. They conduct experiments on retrieval, question answering, and summarization using multi-agent chunking.

**Key Contributions:**

	1. Theoretical framework for analyzing long context tasks in LLMs
	2. Empirical validation of chunk-based processing strategies
	3. Comparison of performance between chunking methods and single-shot model applications

**Result:** Experiments validate the theoretical framework, showing that chunk-based processing can outperform more advanced models like GPT4o in tasks with long inputs.

**Limitations:** 

**Conclusion:** The study provides a principled understanding of handling long contexts in LLMs through effective chunking and aggregation strategies.

**Abstract:** We investigate the challenge of applying Large Language Models (LLMs) to long texts. We propose a theoretical framework that distinguishes the failure modes of long context tasks into three categories: cross-chunk dependence (task noise), confusion that grows with context size (model noise), and the imperfect integration of partial results (aggregator noise). Under this view, we analyze when it is effective to use multi-agent chunking, i.e., dividing a length sequence into smaller chunks and aggregating the processed results of each chunk. Our experiments on tasks such as retrieval, question answering, and summarization confirm both the theoretical analysis and the conditions that favor multi-agent chunking. By exploring superlinear model noise growth with input length, we also explain why, for large inputs, a weaker model configured with chunk-based processing can surpass a more advanced model like GPT4o applied in a single shot. Overall, we present a principled understanding framework and our results highlight a direct pathway to handling long contexts in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [76] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)

*Kangqi Chen, Andreas Kosmas Kakolyris, Rakesh Nadig, Manos Frouzakis, Nika Mansouri Ghiasi, Yu Liang, Haiyu Mao, Jisung Park, Mohammad Sadrosadati, Onur Mutlu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, In-Storage Processing, Approximate Nearest Neighbor Search

**Relevance Score:** 9

**TL;DR:** This paper proposes REIS, a novel In-Storage Processing system tailored for Retrieval-Augmented Generation that alleviates bottlenecks in Approximate Nearest Neighbor Search by optimizing data retrieval and retrieval performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with knowledge limitations due to their training data confinement, and retrieval bottlenecks in RAG can hinder performance.

**Method:** The paper introduces REIS, which features a database layout linking embeddings to documents, an ISP-tailored data placement technique, and an optimized ANNS engine utilizing storage system resources.

**Key Contributions:**

	1. Introduction of REIS, a specialized ISP system for RAG
	2. Innovative database layout for efficient retrieval
	3. Implementation of lightweight Flash Translation Layer for data placement

**Result:** REIS enhances retrieval performance and energy efficiency compared to traditional server-grade systems, achieving an average improvement of 13x in performance and 55x in energy efficiency.

**Limitations:** 

**Conclusion:** REIS represents a significant advancement in RAG processes by optimizing data retrieval and reducing overheads related to large databases.

**Abstract:** Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).

</details>


### [77] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)

*Haotian Xia, Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li*

**Main category:** cs.CL

**Keywords:** story generation, large language models, multi-agent systems, narrative coherence, dataset creation

**Relevance Score:** 7

**TL;DR:** StoryWriter is a multi-agent framework addressing challenges in long story generation by improving discourse coherence and narrative complexity.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current large language models struggle with long story generation due to issues in coherence and complexity, necessitating a new approach.

**Method:** The framework employs three modules: an outline agent for event-based outlines, a planning agent for organizing chapters, and a writing agent for generating coherent narratives.

**Key Contributions:**

	1. Introduction of a multi-agent approach for story generation
	2. Creation of a dataset with high-quality long stories
	3. Demonstration of superior performance over existing benchmarks in story generation

**Result:** StoryWriter demonstrates significant improvements in quality and length of generated stories, outperforming previous baselines, and produces a dataset of approximately 6,000 high-quality stories.

**Limitations:** 

**Conclusion:** StoryWriter shows advanced capabilities in long story generation, indicating that multi-agent frameworks can enhance narrative creation in LLMs.

**Abstract:** Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.

</details>


### [78] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)

*Saad Almohaimeed, Saleh Almohaimeed, Damla Turgut, Ladislau Bölöni*

**Main category:** cs.CL

**Keywords:** implicit hate speech, dataset augmentation, harmful speech detection, Llama-3, GPT-4o

**Relevance Score:** 4

**TL;DR:** This paper proposes a method to detect implicit hate speech using existing harmful speech datasets, achieving significant improvements in detection accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of detecting veiled and subtle forms of hate speech which is often mislabelled in crowdsourced datasets.

**Method:** The method includes influential sample identification, reannotation of datasets, and augmentation using Llama-3 70B and GPT-4o.

**Key Contributions:**

	1. Identification of influential samples to improve detection
	2. Reannotation of existing datasets for better labeling accuracy
	3. Augmentation of datasets using advanced AI models such as Llama-3 and GPT-4o

**Result:** The approach achieved a +12.9-point improvement in the F1 score compared to the baseline in detecting implicit hate speech.

**Limitations:** 

**Conclusion:** The proposed method enhances generalizability in implicit hate speech detection and improves upon existing techniques.

**Abstract:** Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.

</details>


### [79] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)

*Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha*

**Main category:** cs.CL

**Keywords:** reward models, low-resource languages, in-context learning, human preferences, language models

**Relevance Score:** 8

**TL;DR:** RELIC is a novel framework improving reward modeling accuracy for low-resource Indic languages by utilizing in-context learning from high-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM alignment with human preferences for low-resource Indic languages, where existing models struggle due to a lack of training data.

**Method:** RELIC employs a pairwise ranking objective to train a retriever that selects in-context examples from high-resource languages to enhance preference modeling.

**Key Contributions:**

	1. Introduction of RELIC framework for low-resource reward modeling
	2. Pairwise ranking objective for effective in-context learning
	3. Improvements in reward model accuracy for low-resource Indic languages

**Result:** RELIC demonstrated significant improvements in reward model accuracy for low-resource Indic languages, achieving up to a 12.81% accuracy increase over prior methods.

**Limitations:** 

**Conclusion:** RELIC outperforms existing selection methods for training reward models in low-resource contexts, proving effective for languages like Bodo.

**Abstract:** Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.

</details>


### [80] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)

*Dana Serditova, Kevin Tang, Jochen Steffens*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, regional dialects, bias, sociolinguistics, Newcastle English

**Relevance Score:** 7

**TL;DR:** This study addresses the challenges of Automatic Speech Recognition (ASR) systems with regional dialects, specifically Newcastle English, highlighting biases in ASR performance and advocating for more diverse training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research investigates how ASR systems perform with regional dialects, focusing on underexamined regional biases that affect recognition accuracy.

**Method:** A two-stage analysis involving a manual error analysis on a subsample of speech data and a detailed case study of ASR recognition of specific regional pronouns.

**Key Contributions:**

	1. Identified key errors in ASR recognition of Newcastle English regional dialect.
	2. Highlighted the correlation between regional dialectal features and ASR errors.
	3. Advocated for improved dialectal representation in ASR training data.

**Result:** Key phonological, lexical, and morphosyntactic errors were identified as the primary causes of ASR misrecognitions, with regional dialect features influencing error rates more than social factors.

**Limitations:** 

**Conclusion:** There is a need for greater dialectal diversity in ASR training datasets, and sociolinguistic analysis can help in understanding and mitigating regional biases in ASR systems.

**Abstract:** Automatic Speech Recognition (ASR) systems struggle with regional dialects due to biased training which favours mainstream varieties. While previous research has identified racial, age, and gender biases in ASR, regional bias remains underexamined. This study investigates ASR performance on Newcastle English, a well-documented regional dialect known to be challenging for ASR. A two-stage analysis was conducted: first, a manual error analysis on a subsample identified key phonological, lexical, and morphosyntactic errors behind ASR misrecognitions; second, a case study focused on the systematic analysis of ASR recognition of the regional pronouns ``yous'' and ``wor''. Results show that ASR errors directly correlate with regional dialectal features, while social factors play a lesser role in ASR mismatches. We advocate for greater dialectal diversity in ASR training data and highlight the value of sociolinguistic analysis in diagnosing and addressing regional biases.

</details>


### [81] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)

*Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** continual learning, speech recognition, catastrophic forgetting, knowledge accumulation, multilingual models

**Relevance Score:** 6

**TL;DR:** Proposes a continual learning approach for speech recognition models that prevents catastrophic forgetting by using a two-phase learning strategy inspired by human cognitive processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of modern speech recognition models needing to absorb new data without retraining, which leads to catastrophic forgetting.

**Method:** Introduces a continual learning approach consisting of a factorization phase and a centralization phase for merging knowledge.

**Key Contributions:**

	1. A two-phase continual learning approach for speech recognition.
	2. Demonstration of knowledge accumulation through centralization.
	3. Effective prevention of catastrophic forgetting in multilingual scenarios.

**Result:** Experiments show that the proposed centralization stage effectively prevents catastrophic forgetting in multilingual and language-agnostic settings.

**Limitations:** 

**Conclusion:** The approach allows for continual training of speech recognition models without significant degradation in quality, leveraging knowledge accumulation in low-rank adapters.

**Abstract:** Modern neural network based speech recognition models are required to continually absorb new data without re-training the whole system, especially in downstream applications using foundation models, having no access to the original training data. Continually training the models in a rehearsal-free, multilingual, and language agnostic condition, likely leads to catastrophic forgetting, when a seemingly insignificant disruption to the weights can destructively harm the quality of the models. Inspired by the ability of human brains to learn and consolidate knowledge through the waking-sleeping cycle, we propose a continual learning approach with two distinct phases: factorization and centralization, learning and merging knowledge accordingly. Our experiments on a sequence of varied code-switching datasets showed that the centralization stage can effectively prevent catastrophic forgetting by accumulating the knowledge in multiple scattering low-rank adapters.

</details>


### [82] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)

*Tuan-Nam Nguyen, Ngoc-Quan Pham, Seymanur Akti, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** accent conversion, streaming speech processing, text-to-speech, Emformer, native-like accent

**Relevance Score:** 6

**TL;DR:** The proposed model enables real-time transformation of non-native speech into a native-like accent while preserving speaker identity and enhancing pronunciation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a streaming accent conversion model that maintains speaker identity and improves non-native pronunciation while achieving low latency.

**Method:** Modifying a previous accent conversion architecture to incorporate an Emformer encoder and an optimized inference mechanism, along with integrating a native text-to-speech model for training.

**Key Contributions:**

	1. Development of a streaming accent conversion model
	2. Use of an Emformer encoder for improved performance
	3. Integration of a TTS model to enhance training efficiency

**Result:** The streaming accent conversion model achieved performance comparable to existing leading models while ensuring stable latency, effectively breaking new ground in accent conversion technology.

**Limitations:** 

**Conclusion:** This work presents the first effective streaming accent conversion system that performs well under real-time conditions.

**Abstract:** We propose a first streaming accent conversion (AC) model that transforms non-native speech into a native-like accent while preserving speaker identity, prosody and improving pronunciation. Our approach enables stream processing by modifying a previous AC architecture with an Emformer encoder and an optimized inference mechanism. Additionally, we integrate a native text-to-speech (TTS) model to generate ideal ground-truth data for efficient training. Our streaming AC model achieves comparable performance to the top AC models while maintaining stable latency, making it the first AC system capable of streaming.

</details>


### [83] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)

*Nadav Kunievsky, James A. Evans*

**Main category:** cs.CL

**Keywords:** large language models, world model, evaluation framework, semantic diagnostics, output variability

**Relevance Score:** 9

**TL;DR:** This paper proposes a framework for evaluating whether large language models (LLMs) possess a robust world model by measuring their output variability in response to different user intents and articulations.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Assessing the reliability of LLMs is critical, particularly for high-stakes applications, and requires understanding whether they have a structured world model that goes beyond surface-level pattern recognition.

**Method:** The authors introduce a new evaluation approach that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability, to measure how robust the world model is.

**Key Contributions:**

	1. Proposal of a formal framework for evaluating LLM world models
	2. New evaluation approach that quantifies response variability components
	3. Insights into larger models showing greater attribution to user purpose variability

**Result:** Results indicate that larger LLMs attribute more output variability to changes in user purpose, suggesting a stronger world model, though this advantage is not consistent across all domains.

**Limitations:** The robustness advantage of larger models is often modest and not uniform across all domains.

**Conclusion:** The paper emphasizes the necessity of semantic diagnostics over traditional accuracy-based benchmarks to evaluate the internal understanding of LLMs.

**Abstract:** Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.

</details>


### [84] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)

*Hanshu Rao, Weisi Liu, Haohan Wang, I-Chan Huang, Zhe He, Xiaolei Huang*

**Main category:** cs.CL

**Keywords:** synthetic data, large language models, biomedical applications, data generation, evaluation methods

**Relevance Score:** 9

**TL;DR:** This scoping review examines 59 studies on synthetic data generation in biomedical fields, focusing on clinical applications and methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address data scarcity, privacy concerns, and data quality challenges in biomedical fields through synthetic data generation using large language models (LLMs).

**Method:** A scoping review following PRISMA-ScR guidelines, synthesizing studies published between 2020 and 2025 from various academic databases.

**Key Contributions:**

	1. Systematic examination of synthetic data generation in biomedicine
	2. Identification of prevalent data modalities and generation methods
	3. Analysis of evaluation strategies and their challenges

**Result:** The review reveals that 78.0% of studies involve unstructured texts, 13.6% tabular data, and 8.4% multimodal sources; generation methods predominantly use prompting (72.9%) and fine-tuning (22.0%); evaluations vary with 55.9% using human-in-the-loop assessments.

**Limitations:** Limitations in adaptation across clinical domains and evaluation standardizations.

**Conclusion:** Current limitations in synthetic data generation across biomedical domains include issues with adaptation, accessibility of resources and models, and lack of standardized evaluations.

**Abstract:** Synthetic data generation--mitigating data scarcity, privacy concerns, and data quality challenges in biomedical fields--has been facilitated by rapid advances of large language models (LLMs). This scoping review follows PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and 2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The review systematically examines biomedical research and application trends in synthetic data generation, emphasizing clinical applications, methodologies, and evaluations. Our analysis identifies data modalities of unstructured texts (78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model (5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%), human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The analysis addresses current limitations in what, where, and how health professionals can leverage synthetic data generation for biomedical domains. Our review also highlights challenges in adaption across clinical domains, resource and model accessibility, and evaluation standardizations.

</details>


### [85] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)

*Jiaxin Pei, Dustin Wright, Isabelle Augenstin, David Jurgens*

**Main category:** cs.CL

**Keywords:** science communication, public perception, NLP models, dataset, public engagement

**Relevance Score:** 4

**TL;DR:** The paper introduces a computational framework for modeling public perception of science, utilizing a large-scale dataset to predict engagement with scientific information.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Engaging the public with science is crucial for building trust and understanding, yet communicators face challenges in anticipating audience interactions with scientific news.

**Method:** A computational framework modeled public perception across 12 dimensions, creating a large-scale dataset with 10,489 annotations from a diverse participant pool. NLP models were developed to predict perception scores.

**Key Contributions:**

	1. Introduction of a computational framework for public perception modeling
	2. Creation of a large-scale science news perception dataset
	3. Development of NLP models that predict public perception scores

**Result:** It was found that the frequency of science news consumption is the primary driver of public perception, with minimal demographic influence. A large-scale analysis showed that positive perception scores correlate with increased engagement (comments, upvotes) on platforms like Reddit.

**Limitations:** 

**Conclusion:** Nuanced perception modeling is essential for effective science communication and can help predict public interest and engagement with scientific content.

**Abstract:** Effectively engaging the public with science is vital for fostering trust and understanding in our scientific community. Yet, with an ever-growing volume of information, science communicators struggle to anticipate how audiences will perceive and interact with scientific news. In this paper, we introduce a computational framework that models public perception across twelve dimensions, such as newsworthiness, importance, and surprisingness. Using this framework, we create a large-scale science news perception dataset with 10,489 annotations from 2,101 participants from diverse US and UK populations, providing valuable insights into public responses to scientific information across domains. We further develop NLP models that predict public perception scores with a strong performance. Leveraging the dataset and model, we examine public perception of science from two perspectives: (1) Perception as an outcome: What factors affect the public perception of scientific information? (2) Perception as a predictor: Can we use the estimated perceptions to predict public engagement with science? We find that individuals' frequency of science news consumption is the driver of perception, whereas demographic factors exert minimal influence. More importantly, through a large-scale analysis and carefully designed natural experiment on Reddit, we demonstrate that the estimated public perception of scientific information has direct connections with the final engagement pattern. Posts with more positive perception scores receive significantly more comments and upvotes, which is consistent across different scientific information and for the same science, but are framed differently. Overall, this research underscores the importance of nuanced perception modeling in science communication, offering new pathways to predict public interest and engagement with scientific content.

</details>


### [86] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)

*Jianlin Shi, Brian T. Bucher*

**Main category:** cs.CL

**Keywords:** rule-based NLP, large language models, clinical settings, named entity recognition, machine learning

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel approach using LLMs to enhance the development of rule-based NLP systems for clinical settings, achieving high performance in identifying relevant snippets and extracting keywords for NER.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve the labor-intensive process of developing rule-based NLP systems in clinical settings, balancing efficiency and interpretability with the advent of ML and LLMs.

**Method:** The proposed approach involves utilizing LLMs during the development phase of rule-based NLP systems, particularly focusing on identifying relevant text snippets and extracting keywords to support the NER task.

**Key Contributions:**

	1. Introduction of LLMs in the rule-based NLP system development phase
	2. High recall rates for identifying relevant clinical text snippets
	3. Perfect performance in keyword extraction for named entity recognition

**Result:** The experiments showed high recall rates of 0.98 for Deepseek and 0.99 for Qwen in identifying clinically relevant snippets, and a perfect score of 1.0 in keyword extraction for the NER component.

**Limitations:** 

**Conclusion:** The study indicates that using LLMs for initial phases of rule-based NLP system development could significantly enhance the process, making it faster, more cost-effective, and transparent compared to traditional deep learning models.

**Abstract:** Despite advances in machine learning (ML) and large language models (LLMs), rule-based natural language processing (NLP) systems remain active in clinical settings due to their interpretability and operational efficiency. However, their manual development and maintenance are labor-intensive, particularly in tasks with large linguistic variability. To overcome these limitations, we proposed a novel approach employing LLMs solely during the rule-based systems development phase. We conducted the initial experiments focusing on the first two steps of developing a rule-based NLP pipeline: find relevant snippets from the clinical note; extract informative keywords from the snippets for the rule-based named entity recognition (NER) component. Our experiments demonstrated exceptional recall in identifying clinically relevant text snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER. This study sheds light on a promising new direction for NLP development, enabling semi-automated or automated development of rule-based systems with significantly faster, more cost-effective, and transparent execution compared with deep learning model-based solutions.

</details>


### [87] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)

*Fenghua Cheng, Jinxiang Wang, Sen Wang, Zi Huang, Xue Li*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, GeoGuess, hierarchical visual information, AI, geographic knowledge

**Relevance Score:** 8

**TL;DR:** This paper introduces GeoGuess, a novel multimodal reasoning task requiring location identification and explanation from street view images, along with the GeoExplain dataset and a method called SightSense for hierarchical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing multimodal reasoning tasks that do not adequately consider hierarchical visual clues at different levels of granularity, which are crucial in real-world scenarios.

**Method:** A benchmark task called GeoGuess is introduced along with the GeoExplain dataset, consisting of panoramas with geocoordinates and explanations. A multimodal reasoning method named SightSense is proposed to predict and generate explanations based on hierarchical visual information and external geographic knowledge.

**Key Contributions:**

	1. Introduction of the GeoGuess task for multimodal reasoning.
	2. Creation of the GeoExplain dataset for benchmarking.
	3. Development of SightSense, a method for hierarchical visual reasoning.

**Result:** The proposed methods and dataset demonstrate outstanding performance in the GeoGuess task, showcasing the effectiveness of hierarchical reasoning in multimodal contexts.

**Limitations:** 

**Conclusion:** GeoGuess sets a new standard for evaluating multimodal reasoning in AI by emphasizing the integration of detailed visual clues with broader geographic context.

**Abstract:** Multimodal reasoning is a process of understanding, integrating and inferring information across different data modalities. It has recently attracted surging academic attention as a benchmark for Artificial Intelligence (AI). Although there are various tasks for evaluating multimodal reasoning ability, they still have limitations. Lack of reasoning on hierarchical visual clues at different levels of granularity, e.g., local details and global context, is of little discussion, despite its frequent involvement in real scenarios. To bridge the gap, we introduce a novel and challenging task for multimodal reasoning, namely GeoGuess. Given a street view image, the task is to identify its location and provide a detailed explanation. A system that succeeds in GeoGuess should be able to detect tiny visual clues, perceive the broader landscape, and associate with vast geographic knowledge. Therefore, GeoGuess would require the ability to reason between hierarchical visual information and geographic knowledge. In this work, we establish a benchmark for GeoGuess by introducing a specially curated dataset GeoExplain which consists of panoramas-geocoordinates-explanation tuples. Additionally, we present a multimodal and multilevel reasoning method, namely SightSense which can make prediction and generate comprehensive explanation based on hierarchy of visual information and external knowledge. Our analysis and experiments demonstrate their outstanding performance in GeoGuess.

</details>


### [88] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)

*Pavlo Vasylenko, Marcos Treviso, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** sparse attention, Adaptive-Scalable Entmax, transformer architecture, long-context generalization, position encodings

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel attention mechanism called Adaptive-Scalable Entmax (ASEntmax) that improves long-context generalization in transformer models by using sparse attention and learnable temperature parameters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Transformer architectures often struggle with attention dispersal on lengthy sequences, leading to inefficiencies in tasks requiring focus on fixed-size patterns.

**Method:** The authors propose the use of $	ext{ASEntmax}$, a sparse attention mechanism that incorporates a learnable temperature parameter for adaptive attention distribution, allowing for better fixation on relevant tokens while nullifying irrelevant ones.

**Key Contributions:**

	1. Introduction of ASEntmax as a novel attention mechanism
	2. Demonstration of ASEntmax's effectiveness over traditional softmax in long-context tasks
	3. Insights on the design of positional encodings to complement attention mechanisms.

**Result:** Integration of ASEntmax into the transformer architecture significantly enhances performance on long-context generalization tasks, surpassing traditional softmax and other attention baselines.

**Limitations:** 

**Conclusion:** The proposed ASEntmax, along with optimized positional encodings, provides a considerable advancement in transformer model capabilities for tasks requiring specific pattern recognition in extensive data sequences.

**Abstract:** Transformer-based architectures traditionally employ softmax to compute attention weights, which produces dense distributions over all tokens in a sequence. While effective in many settings, this density has been shown to be detrimental for tasks that demand precise focus on fixed-size patterns: as sequence length increases, non-informative tokens accumulate attention probability mass, leading to dispersion and representational collapse. We show in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid these issues, due to their ability to assign exact zeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows $\alpha$-entmax with a learnable temperature parameter, allowing the attention distribution to interpolate between sparse (pattern-focused) and dense (softmax-like) regimes. Finally, we show that the ability to locate and generalize fixed-size patterns can be further improved through a careful design of position encodings, which impacts both dense and sparse attention methods. By integrating ASEntmax into standard transformer layers alongside proper positional encodings, we show that our models greatly outperform softmax, scalable softmax, and fixed-temperature $\alpha$-entmax baselines on long-context generalization.

</details>


### [89] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)

*Co Tran, Salman Paracha, Adil Hafeez, Shuguang Chen*

**Main category:** cs.CL

**Keywords:** large language models, model routing, preference alignment, human preferences, Arch-Router

**Relevance Score:** 9

**TL;DR:** A framework called Arch-Router is proposed for preference-aligned routing of large language models, improving the selection process by matching user queries to user-defined preferences and domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the operationalization of diverse large language models by aligning routing decisions with human preferences, which are often subjective and poorly captured by existing benchmarks.

**Method:** Arch-Router is a compact 1.5B model that maps queries to preferences based on user-defined domains or action types, enabling flexible routing without retraining or architectural changes.

**Key Contributions:**

	1. Development of Arch-Router model for aligning LLM routing with human preferences.
	2. Demonstrated state-of-the-art performance in query matching based on subjective evaluation criteria.
	3. Enabling flexible integration of new models into the routing framework.

**Result:** The proposed routing framework achieved state-of-the-art results in matching queries with human preferences on conversational datasets, outperforming existing proprietary models.

**Limitations:** 

**Conclusion:** Arch-Router offers a more transparent and flexible routing mechanism for large language models, accommodating new models without significant retraining needs.

**Abstract:** With the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [90] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)

*Ananth Agarwal, Jasper Jian, Christopher D. Manning, Shikhar Murty*

**Main category:** cs.CL

**Keywords:** Large Language Models, syntax, probing, transformer models, interpretability

**Relevance Score:** 6

**TL;DR:** This study investigates the relationship between probing accuracy and syntactic performance in transformer models, finding that probing results do not reliably predict downstream syntax evaluation outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how Large Language Models (LLMs) represent syntactic structure is important for improving their interpretability and performance in syntactic tasks.

**Method:** The study evaluates 32 open-weight transformer models through a 'mechanisms vs. outcomes' framework, comparing probing accuracy to targeted syntax evaluations.

**Key Contributions:**

	1. Evaluated probing accuracy of 32 transformer models
	2. Established a framework for comparing mechanisms and outcomes in syntax processing
	3. Highlighted the disconnect between probing and syntactic performance

**Result:** The findings indicate that syntactic features extracted from probing do not predict the models' performance in downstream syntactic evaluations across various English linguistic phenomena.

**Limitations:** The study focuses only on English linguistic phenomena; findings may not generalize to other languages or syntactic features.

**Conclusion:** There is a significant disconnect between latent syntactic representations obtained through probing and the observable syntactic behaviors exhibited by the models in downstream tasks.

**Abstract:** Large Language Models (LLMs) exhibit a robust mastery of syntax when processing and generating text. While this suggests internalized understanding of hierarchical syntax and dependency relations, the precise mechanism by which they represent syntactic structure is an open area within interpretability research. Probing provides one way to identify the mechanism of syntax being linearly encoded in activations, however, no comprehensive study has yet established whether a model's probing accuracy reliably predicts its downstream syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we evaluate 32 open-weight transformer models and find that syntactic features extracted via probing fail to predict outcomes of targeted syntax evaluations across English linguistic phenomena. Our results highlight a substantial disconnect between latent syntactic representations found via probing and observable syntactic behaviors in downstream tasks.

</details>


### [91] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)

*Hyunsoo Yun, Eun Hak Lee*

**Main category:** cs.CL

**Keywords:** Legislation, Large Language Model, Explainable AI, Transportation Policy, Political Ideologies

**Relevance Score:** 6

**TL;DR:** LegiGPT integrates a large language model with explainable AI to analyze transportation-related legislation, revealing key political and geographical factors influencing policymaking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how lawmakers' political ideologies impact legislative decision-making is essential for effective policymaking.

**Method:** LegiGPT uses a multi-stage filtering and classification pipeline with zero-shot prompting via GPT-4 to analyze legislative data, focusing on sponsor characteristics, political affiliations, and geographic variables.

**Key Contributions:**

	1. Introduction of LegiGPT framework for legislative analysis
	2. Combines LLM with XAI for explainable decision-making
	3. Identifies key factors in transportation policymaking

**Result:** Key determinants of transportation policymaking include the number of conservative and progressive sponsors and district demographics, demonstrating bipartisan engagement in legislation.

**Limitations:** 

**Conclusion:** This framework offers insights for legislative dynamics and can inform future policy development, notably in infrastructure planning.

**Abstract:** Given the significant influence of lawmakers' political ideologies on legislative decision-making, understanding their impact on policymaking is critically important. We introduce a novel framework, LegiGPT, which integrates a large language model (LLM) with explainable artificial intelligence (XAI) to analyze transportation-related legislative proposals. LegiGPT employs a multi-stage filtering and classification pipeline using zero-shot prompting with GPT-4. Using legislative data from South Korea's 21st National Assembly, we identify key factors - including sponsor characteristics, political affiliations, and geographic variables - that significantly influence transportation policymaking. The LLM was used to classify transportation-related bill proposals through a stepwise filtering process based on keywords, phrases, and contextual relevance. XAI techniques were then applied to examine relationships between party affiliation and associated attributes. The results reveal that the number and proportion of conservative and progressive sponsors, along with district size and electoral population, are critical determinants shaping legislative outcomes. These findings suggest that both parties contributed to bipartisan legislation through different forms of engagement, such as initiating or supporting proposals. This integrated approach provides a valuable tool for understanding legislative dynamics and guiding future policy development, with broader implications for infrastructure planning and governance.

</details>


### [92] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)

*Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao*

**Main category:** cs.CL

**Keywords:** Generative Reward Models, Human preferences, Reinforcement learning, Machine Learning, Reasoning paths

**Relevance Score:** 7

**TL;DR:** This paper introduces ReasonGRM, a generative reward modeling framework that improves reasoning capabilities to enhance the performance of generative reward models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generative Reward Models often struggle with reasoning, leading to hallucinations and incomplete information in complex tasks; there is a need for improved reasoning methods in this domain.

**Method:** The ReasonGRM framework consists of three stages: using Zero-RL for concise reasoning paths, applying a novel evaluation metric $R^*$ to score these paths, and refining the model through reinforcement learning on challenging examples.

**Key Contributions:**

	1. Introduction of ReasonGRM framework for improved reasoning in GRMs
	2. Novel evaluation metric $R^*$ to optimize reasoning paths
	3. Enhanced performance benchmarks compared to existing models

**Result:** ReasonGRM outperforms previous state-of-the-art Generative Reward Models by 1.8% on average and exceeds proprietary models like GPT-4o by up to 5.6% across three public benchmarks.

**Limitations:** 

**Conclusion:** The proposed methodology demonstrates that reasoning-aware training and quality rationale selection are crucial for effective preference modeling.

**Abstract:** Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.

</details>


### [93] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)

*Xinyi Liu, Weiguang Wang, Hangfeng He*

**Main category:** cs.CL

**Keywords:** Large Language Models, epistemic uncertainty, aleatoric uncertainty, Visual Question Answering, bias mitigation

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of prompt-introduced bias on epistemic and aleatoric uncertainty in LLMs, particularly in Visual Question Answering tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing use of LLMs necessitates reliable uncertainty assessment to improve model outcomes, particularly in tasks with multiple valid answers.

**Method:** Experiments were conducted on Visual Question Answering tasks using GPT-4o and Qwen2-VL to analyze the effects of mitigating biases on uncertainty quantification.

**Key Contributions:**

	1. Investigates the trade-off between epistemic and aleatoric uncertainty in LLMs.
	2. Finds that mitigating bias improves uncertainty assessment in Visual Question Answering tasks.
	3. Demonstrates the relationship between model confidence and uncertainty under bias influences.

**Result:** Mitigating prompt-introduced bias improves uncertainty quantification, with biases causing greater changes in uncertainty measures at lower model confidence levels.

**Limitations:** 

**Conclusion:** Understanding the trade-offs in bias mitigation can enhance uncertainty quantification methods in LLMs, leading to better decision-making processes in open-ended tasks.

**Abstract:** With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases induce greater changes in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence leads to greater underestimation of epistemic uncertainty (i.e. overconfidence) due to bias, whereas it has no significant effect on the direction of changes in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.

</details>


### [94] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)

*Daejin Jo, Jeeyoung Yun, Byungseok Roh, Sungwoong Kim*

**Main category:** cs.CL

**Keywords:** speech tokenization, language models, semantic distillation, speech-to-text, text-to-speech

**Relevance Score:** 7

**TL;DR:** This paper presents LM-SPT, a novel speech tokenization method that enhances the alignment of speech tokens with language models by reconstructing speech from semantic tokens, achieving superior performance in speech-to-text and text-to-speech tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing need for efficient speech-language modeling by improving speech tokenization methods, which often produce excessively long token sequences that challenge processing efficiency.

**Method:** LM-SPT introduces a distinct semantic distillation approach, focusing on reconstructing speech from semantic tokens and minimizing discrepancies between original and reconstructed waveforms, while also integrating architectural improvements for the tokenizer.

**Key Contributions:**

	1. Introduces semantic distillation for improved speech tokenization.
	2. Achieves better alignment of speech tokens with language models.
	3. Demonstrates architectural enhancements for tokenizer efficiency.

**Result:** Experimental results demonstrate that LM-SPT has better reconstruction fidelity than existing methods, and that language models trained with LM-SPT tokens perform competitively on speech-to-text and significantly outperform other methods on text-to-speech tasks.

**Limitations:** 

**Conclusion:** The findings suggest that LM-SPT provides an effective solution for improving speech tokenization, thereby enhancing the performance of speech-language models in both recognition and synthesis tasks.

**Abstract:** With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.

</details>


### [95] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)

*Lance Ying, Ryan Truong, Katherine M. Collins, Cedegao E. Zhang, Megan Wei, Tyler Brooke-Wilson, Tan Zhi-Xuan, Lionel Wong, Joshua B. Tenenbaum*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, social inference, language models, Bayesian planning, visual inputs

**Relevance Score:** 8

**TL;DR:** LIRAS is a framework that integrates linguistic and visual inputs for multimodal social reasoning, improving social inference accuracy through structured representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance social inference accuracy by merging linguistic and visual information in novel situations where language provides critical insights.

**Method:** LIRAS constructs situation-specific representations of agents and environments by using multimodal language models and a Bayesian inverse planning engine to perform social reasoning.

**Key Contributions:**

	1. Introduction of the LIRAS framework for multimodal social reasoning
	2. Integration of linguistic and visual inputs for enhanced social inference
	3. Demonstrated superiority of the model across various cognitive science-derived tasks

**Result:** LIRAS outperforms existing models and ablations on various social reasoning tasks, demonstrating improved alignment with human judgments.

**Limitations:** 

**Conclusion:** The proposed framework effectively leverages multimodal inputs to draw granular social inferences, significantly enhancing the capabilities of prior models.

**Abstract:** Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.

</details>


### [96] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)

*Zhuang Chen, Yaru Cao, Guanqun Bi, Jincenzi Wu, Jinfeng Zhou, Xiyao Xiao, Si Chen, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** emotional support conversations, social dynamics, large language models

**Relevance Score:** 9

**TL;DR:** This paper presents SocialSim, a framework for simulating emotional support conversations (ESC) by integrating social disclosure and awareness, resulting in a large-scale synthetic ESC corpus (SSConv) that outperforms crowdsourced data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current dialogue augmentation methods for emotional support conversations that overlook social dynamics and to provide a scalable solution for synthesizing emotional care.

**Method:** The framework integrates social disclosure through a persona bank of diverse scenarios and enhances social awareness via cognitive reasoning for generating supportive responses. A large-scale synthetic ESC corpus (SSConv) is created and utilized to train a chatbot.

**Key Contributions:**

	1. Introduction of SocialSim framework for simulating ESC incorporating social dynamics
	2. Development of SSConv, a large-scale synthetic ESC corpus that outperforms crowdsourced data
	3. Demonstrated state-of-the-art performance of a trained chatbot on the synthetic data

**Result:** The trained chatbot achieves state-of-the-art performance in both automatic and human evaluations, indicating the quality of SSConv surpasses that of crowdsourced data.

**Limitations:** 

**Conclusion:** SocialSim offers a novel and practical approach to make emotional care through conversations more accessible and effective.

**Abstract:** Emotional support conversation (ESC) helps reduce people's psychological stress and provide emotional value through interactive dialogues. Due to the high cost of crowdsourcing a large ESC corpus, recent attempts use large language models for dialogue augmentation. However, existing approaches largely overlook the social dynamics inherent in ESC, leading to less effective simulations. In this paper, we introduce SocialSim, a novel framework that simulates ESC by integrating key aspects of social interactions: social disclosure and social awareness. On the seeker side, we facilitate social disclosure by constructing a comprehensive persona bank that captures diverse and authentic help-seeking scenarios. On the supporter side, we enhance social awareness by eliciting cognitive reasoning to generate logical and supportive responses. Building upon SocialSim, we construct SSConv, a large-scale synthetic ESC corpus of which quality can even surpass crowdsourced ESC data. We further train a chatbot on SSConv and demonstrate its state-of-the-art performance in both automatic and human evaluations. We believe SocialSim offers a scalable way to synthesize ESC, making emotional care more accessible and practical.

</details>


### [97] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)

*Lei Jiang, Zixun Zhang, Zizhou Wang, Xiaobing Sun, Zhen Li, Liangli Zhen, Xiaohua Xu*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, jailbreak attacks, security mechanisms, cross-modal reasoning, multimodal tasks

**Relevance Score:** 4

**TL;DR:** A new black-box jailbreak attack framework, CAMO, effectively bypasses safety mechanisms in large vision-language models by decomposing prompts into semantically benign fragments, boasting improved efficiency and stealth.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address vulnerabilities in current vision-language models (LVLMs) that can be exploited through jailbreak attacks, which bypass built-in safety mechanisms.

**Method:** CAMO decomposes malicious prompts into benign visual and textual fragments using the cross-modal reasoning abilities of LVLMs. It relies on multi-step reasoning to reconstruct harmful content without being detected.

**Key Contributions:**

	1. Introduction of the CAMO framework for black-box jailbreak attacks
	2. Improved efficiency and stealth compared to prior methods
	3. Evident vulnerabilities discovered in current LVLM safety mechanisms

**Result:** CAMO exhibits strong effectiveness against leading vision-language models, requiring significantly fewer queries and demonstrating robust performance and high cross-model transferability.

**Limitations:** 

**Conclusion:** The findings highlight critical weaknesses in existing safety mechanisms of LVLMs, necessitating the development of advanced security measures that are alignment-aware.

**Abstract:** Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems.

</details>


### [98] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)

*Heloisa Oss Boll, Antonio Oss Boll, Leticia Puttlitz Boll, Ameen Abu Hanna, Iacer Calixto*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Note Summarization, Health Informatics

**Relevance Score:** 10

**TL;DR:** This paper presents Distillnote, a framework for LLM-based clinical note summarization that improves documentation efficiency and predictive performance for heart failure.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Healthcare providers are overwhelmed by clinical documentation; LLMs can automate and summarize this information effectively.

**Method:** The framework generates summaries using three techniques: direct summarization, structured summarization, and distilled summarization. It tests the utility of these summaries by predicting heart failure outcomes using LLM-generated summaries compared to original notes.

**Key Contributions:**

	1. Introduction of Distillnote for clinical note summarization
	2. Demonstrated significant improvement in predictive performance for heart failure
	3. Public release of over 64,000 admission note summaries for future research

**Result:** Distilled summaries achieved 79% text compression and a 18.2% improvement in AUPRC over the model trained on original notes.

**Limitations:** Not specified in the abstract.

**Conclusion:** One-step summaries are preferred for relevance and clinical actionability, while distilled summaries provide high efficiency and reduced hallucination rates. The summaries are publicly available on PhysioNet for further research.

**Abstract:** Large language models (LLMs) offer unprecedented opportunities to generate concise summaries of patient information and alleviate the burden of clinical documentation that overwhelms healthcare providers. We present Distillnote, a framework for LLM-based clinical note summarization, and generate over 64,000 admission note summaries through three techniques: (1) One-step, direct summarization, and a divide-and-conquer approach involving (2) Structured summarization focused on independent clinical insights, and (3) Distilled summarization that further condenses the Structured summaries. We test how useful are the summaries by using them to predict heart failure compared to a model trained on the original notes. Distilled summaries achieve 79% text compression and up to 18.2% improvement in AUPRC compared to an LLM trained on the full notes. We also evaluate the quality of the generated summaries in an LLM-as-judge evaluation as well as through blinded pairwise comparisons with clinicians. Evaluations indicate that one-step summaries are favoured by clinicians according to relevance and clinical actionability, while distilled summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio) and significantly reduce hallucinations. We release our summaries on PhysioNet to encourage future research.

</details>


### [99] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)

*Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Meng Han*

**Main category:** cs.CL

**Keywords:** large language models, jailbreak attacks, Iterative Semantic Tuning, prompt refinement, computational efficiency

**Relevance Score:** 3

**TL;DR:** This paper presents MIST, a method for jailbreaking black-box large language models by refining prompts to induce harmful content while maintaining semantic intent.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities of large language models to jailbreak attacks, which can elicit harmful responses despite attempts to align these models with societal values.

**Method:** MIST uses Iterative Semantic Tuning to iteratively refine prompts, employing strategies like sequential synonym search and order-determining optimization to enhance attack success while maintaining computational efficiency.

**Key Contributions:**

	1. Introduction of MIST for jailbreaking black-box LLMs
	2. Innovative use of Iterative Semantic Tuning for prompt refinement
	3. Demonstrated balance between semantic intent and attack efficiency

**Result:** MIST demonstrates competitive attack success rates and transferability compared to existing jailbreak methods, validated through extensive experiments on various models.

**Limitations:** The study does not explore the ethical implications of using such methods in real-world applications.

**Conclusion:** MIST effectively combines semantic preservation and computational efficiency, making it a viable approach for conducting jailbreak attacks on black-box LLMs.

**Abstract:** Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks--methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version--order-determining optimization. Extensive experiments across two open-source models and four closed-source models demonstrate that MIST achieves competitive attack success rates and attack transferability compared with other state-of-the-art white-box and black-box jailbreak methods. Additionally, we conduct experiments on computational efficiency to validate the practical viability of MIST.

</details>


### [100] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)

*Daniel Christoph, Max Ploner, Patrick Haller, Alan Akbik*

**Main category:** cs.CL

**Keywords:** sample efficiency, language models, long-tailed distribution, fact frequency, model architecture

**Relevance Score:** 8

**TL;DR:** This study investigates the sample efficiency of language models in learning and recalling facts with varying frequency in the training corpus.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The importance of sample efficiency in language models for improving training efficiency, especially in handling long-tailed distributions of information found in real-world text.

**Method:** We analyze multiple models of different architectures and sizes, all trained on the same pre-training data, and annotate relational facts with their frequencies.

**Key Contributions:**

	1. Analysis of model performance on facts of varying frequencies
	2. Insights into the relationship between model architecture/size and sample efficiency
	3. Focus on low-frequency facts and their learning efficiency

**Result:** Most models show similar performance on high-frequency facts but exhibit significant differences on low-frequency facts.

**Limitations:** 

**Conclusion:** The findings reveal new insights into how model architecture and size affect the efficiency of factual learning, particularly for low-frequency information.

**Abstract:** Sample efficiency is a crucial property of language models with practical implications for training efficiency. In real-world text, information follows a long-tailed distribution. Yet, we expect models to learn and recall frequent and infrequent facts. Sample-efficient models are better equipped to handle this challenge of learning and retaining rare information without requiring excessive exposure. This study analyzes multiple models of varying architectures and sizes, all trained on the same pre-training data. By annotating relational facts with their frequencies in the training corpus, we examine how model performance varies with fact frequency. Our findings show that most models perform similarly on high-frequency facts but differ notably on low-frequency facts. This analysis provides new insights into the relationship between model architecture, size, and factual learning efficiency.

</details>


### [101] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)

*Antonin Berthon, Mihaela van der Schaar*

**Main category:** cs.CL

**Keywords:** Knowledge Tracing, Language Bottleneck Model, student assessment, interpretability, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper presents the Language Bottleneck Model (LBM) for Knowledge Tracing (KT) that enhances interpretability and prediction accuracy by summarizing student knowledge in natural language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional KT methods and LLM-based predictions that lack interpretability and accuracy guarantees.

**Method:** The Language Bottleneck Model (LBM) uses an encoder LLM to create an interpretable summary of student knowledge, which is then used by a frozen decoder LLM to predict future responses.

**Key Contributions:**

	1. Introduction of the Language Bottleneck Model (LBM) for Knowledge Tracing
	2. Improved interpretability of student knowledge assessments
	3. Demonstrated effectiveness with fewer data requirements compared to traditional methods

**Result:** LBMs achieve accuracy comparable to state-of-the-art KT methods while requiring significantly fewer student trajectories, as shown in experiments on synthetic and large-scale datasets.

**Limitations:** The model's performance may still be constrained by the quality of the LLMs used, and it requires effective training strategies to ensure summary accuracy.

**Conclusion:** By constraining predictive information through a natural-language bottleneck, LBMs offer a more interpretable and effective approach to student knowledge assessment.

**Abstract:** Accurately assessing student knowledge is critical for effective education, yet traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings, limiting interpretability. Even LLM-based approaches generate direct predictions or summaries that may hallucinate without any accuracy guarantees. We recast KT as an inverse problem: learning the minimum natural-language summary that makes past answers explainable and future answers predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM that writes an interpretable knowledge summary and a frozen decoder LLM that must reconstruct and predict student responses using only that summary text. By constraining all predictive information to pass through a short natural-language bottleneck, LBMs ensure that the summary contains accurate information while remaining human-interpretable. Experiments on synthetic arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the accuracy of state-of-the-art KT and direct LLM methods while requiring orders-of-magnitude fewer student trajectories. We demonstrate that training the encoder with group-relative policy optimization, using downstream decoding accuracy as a reward signal, effectively improves summary quality.

</details>


### [102] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)

*Sahil Kale, Vijaykant Nadadur*

**Main category:** cs.CL

**Keywords:** LaTeX, Large Language Models, benchmarking, natural language processing, scientific documentation

**Relevance Score:** 8

**TL;DR:** The paper introduces TeXpert, a benchmark for evaluating LLMs' ability to generate LaTeX code from natural language prompts, analyzing performance across various tasks and identifying common errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the potential of LLMs to generate LaTeX code with natural language instructions, current evaluations do not reflect this capability, prompting the need for a dedicated benchmark.

**Method:** The authors created TeXpert, a dataset with natural language prompts for LaTeX code generation, and evaluated multiple LLMs on their performance, focusing on accuracy and types of errors made.

**Key Contributions:**

	1. Introduction of TeXpert benchmark for LaTeX generation
	2. In-depth analysis of LLM performance on LaTeX tasks
	3. Identification of common error types in LLM-generated LaTeX

**Result:** LLMs show a significant drop in accuracy for complex LaTeX tasks, with open-source models competing strongly against closed-source ones; prevalent errors include formatting and package issues due to insufficient training examples.

**Limitations:** The study may not cover all LLMs available and focuses primarily on LaTeX generation without exploring broader applications of LLMs.

**Conclusion:** The findings indicate the necessity for more diverse training datasets for LLMs and demonstrate the utility of TeXpert in evaluating LaTeX generation capabilities.

**Abstract:** LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [103] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)

*Mikhail Menschikov, Dmitry Evseev, Ruslan Kostoev, Ilya Perepechkin, Ilnaz Salimov, Victoria Dochkina, Petr Anokhin, Evgeny Burnaev, Nikita Semenov*

**Main category:** cs.CL

**Keywords:** language models, personalization, knowledge graphs, hyperedges, temporal dependencies

**Relevance Score:** 9

**TL;DR:** This paper addresses the personalization of language models by incorporating external memory through knowledge graphs, improving user interactions based on individual history.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The personalization of language models is crucial for enhancing user interactions, especially in retaining extensive personal information to generate relevant responses.

**Method:** The paper proposes a new architecture that uses knowledge graphs with standard edges and two types of hyperedges to enhance the retention and utilization of personal user data.

**Key Contributions:**

	1. Utilization of knowledge graphs for user history integration in LLMs
	2. Introduction of a combined graph with standard edges and hyperedges
	3. Improvement of benchmarks through temporal and contradictory parameter incorporation

**Result:** Experiments on benchmarks showed that the proposed graph construction method improves knowledge extraction and retains robust performance in answering questions, even with temporal modifications and contradictory statements.

**Limitations:** 

**Conclusion:** The introduced architecture successfully supports personalized language model interactions by maintaining temporal dependencies and accommodating complex user histories.

**Abstract:** The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.

</details>


### [104] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)

*Danielle R. Thomas, Conrad Borchers, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Kenneth R. Koedinger*

**Main category:** cs.CL

**Keywords:** Large Language Models, Feedback, Learning Outcomes, Machine Learning, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** The study investigates the impact of LLM-generated feedback on learning outcomes in tutor training scenarios, revealing moderate learning benefits and high learner satisfaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** This study explores the effectiveness of LLM-generated explanatory feedback in learning environments, which is less understood compared to traditional feedback methods.

**Method:** The study analyzes over 2,600 lesson completions from 885 learners, comparing posttest performance among groups receiving LLM feedback, traditional feedback, or neither, while addressing selection bias through propensity scoring.

**Key Contributions:**

	1. Demonstrated moderate learning benefits from LLM feedback in tutor training scenarios.
	2. Provided open datasets, LLM prompts, and rubrics for reproducibility.
	3. Addressed the impact of user engagement on feedback effectiveness.

**Result:** Learners who were more likely to engage with LLM feedback scored higher at posttest, with two lessons showing moderate learning benefits from LLM feedback (effect sizes of 0.28 and 0.33).

**Limitations:** The study's results may be influenced by selection bias despite accounting for it through propensity scoring.

**Conclusion:** LLM feedback has potential as a scalable tool for improving learning in open-ended tasks, without significantly increasing completion time, and is well-received by learners.

**Abstract:** Large language models (LLMs) are increasingly used to generate feedback, yet their impact on learning remains underexplored, especially compared to existing feedback methods. This study investigates how on-demand LLM-generated explanatory feedback influences learning in seven scenario-based tutor training lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we compare posttest performance among learners across three groups: learners who received feedback generated by gpt-3.5-turbo, those who declined it, and those without access. All groups received non-LLM corrective feedback. To address potential selection bias-where higher-performing learners may be more inclined to use LLM feedback-we applied propensity scoring. Learners with a higher predicted likelihood of engaging with LLM feedback scored significantly higher at posttest than those with lower propensity. After adjusting for this effect, two out of seven lessons showed statistically significant learning benefits from LLM feedback with standardized effect sizes of 0.28 and 0.33. These moderate effects suggest that the effectiveness of LLM feedback depends on the learners' tendency to seek support. Importantly, LLM feedback did not significantly increase completion time, and learners overwhelmingly rated it as helpful. These findings highlight LLM feedback's potential as a low-cost and scalable way to improve learning on open-ended tasks, particularly in existing systems already providing feedback without LLMs. This work contributes open datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [105] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)

*Giuseppe Attanasio, Sonal Sannigrahi, Ben Peters, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** speech recognition, translation, spoken question answering, instruction following, language models

**Relevance Score:** 5

**TL;DR:** This paper discusses a unified speech-to-text model for speech recognition, translation, and spoken question answering in the IT-IST submission for the IWSLT 2025 Shared Task.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of instruction following in speech processing through effective speech recognition, translation, and question answering technologies.

**Method:** The model is built on a unified framework that aligns a pre-trained continuous speech encoder with a text decoder. It includes two phases: modality alignment and instruction fine-tuning, utilizing small-scale language model backbones and high-quality datasets.

**Key Contributions:**

	1. Unified speech-to-text model integrating encoder and decoder
	2. Focus on small-scale language model backbones
	3. Use of high-quality and synthetic data for training

**Result:** The submission reports results from the Short Track of the shared task, demonstrating the model's capabilities in performing speech recognition, translation, and spoken question answering.

**Limitations:** 

**Conclusion:** The integration of modality alignment and fine-tuning with smaller language models offers a promising approach to improve instruction-following speech processing tasks.

**Abstract:** This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing. We submit results for the Short Track, i.e., speech recognition, translation, and spoken question answering. Our model is a unified speech-to-text model that integrates a pre-trained continuous speech encoder and text decoder through a first phase of modality alignment and a second phase of instruction fine-tuning. Crucially, we focus on using small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY data along with synthetic data generation to supplement existing resources.

</details>


### [106] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)

*Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, ambiguity resolution, multilingual dataset, cross-modal scenarios, mutual clarification

**Relevance Score:** 8

**TL;DR:** Introduction of MUCAR, a benchmark for evaluating multimodal ambiguity resolution in MLLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current multimodal benchmarks fail to adequately address linguistic and visual ambiguities, which limits the ability of models to resolve ambiguities effectively using multimodal contexts.

**Method:** MUCAR is created with two datasets: a multilingual dataset resolving ambiguous text expressions with visual contexts, and a dual-ambiguity dataset pairing ambiguous images with ambiguous texts to enforce mutual disambiguation.

**Key Contributions:**

	1. Introduction of MUCAR benchmark for multimodal ambiguity resolution
	2. Creation of multilingual and dual-ambiguity datasets
	3. Evaluation of state-of-the-art models highlighting performance gaps

**Result:** Extensive evaluations on 19 state-of-the-art multimodal models show significant performance gaps compared to human-level understanding, indicating an urgent need for advancements in cross-modal ambiguity resolution techniques.

**Limitations:** Existing models still struggle with ambiguity resolution compared to human-level performance.

**Conclusion:** MUCAR demonstrates the necessity for more advanced methods in understanding multimodal ambiguities, reinforcing the importance of mutual clarification between language and images.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated significant advances across numerous vision-language tasks. Due to their strong image-text alignment capability, MLLMs can effectively understand image-text pairs with clear meanings. However, effectively resolving the inherent ambiguities in natural language and visual contexts remains challenging. Existing multimodal benchmarks typically overlook linguistic and visual ambiguities, relying mainly on unimodal context for disambiguation and thus failing to exploit the mutual clarification potential between modalities. To bridge this gap, we introduce MUCAR, a novel and challenging benchmark designed explicitly for evaluating multimodal ambiguity resolution across multilingual and cross-modal scenarios. MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions are uniquely resolved by corresponding visual contexts, and (2) a dual-ambiguity dataset that systematically pairs ambiguous images with ambiguous textual contexts, with each combination carefully constructed to yield a single, clear interpretation through mutual disambiguation. Extensive evaluations involving 19 state-of-the-art multimodal models--encompassing both open-source and proprietary architectures--reveal substantial gaps compared to human-level performance, highlighting the need for future research into more sophisticated cross-modal ambiguity comprehension methods, further pushing the boundaries of multimodal reasoning.

</details>


### [107] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)

*Dominik Macháček, Peter Polák*

**Main category:** cs.CL

**Keywords:** simultaneous translation, speech model, Whisper, IWSLT 2025, BLEU points

**Relevance Score:** 4

**TL;DR:** This paper presents a submission to the IWSLT 2025 for Simultaneous Speech Translation using the Whisper model and AlignAtt for improved translation accuracy across multiple language pairs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance simultaneous translation performance using advanced speech models and innovative methodologies for various language pairs.

**Method:** Utilizes the Whisper model in conjunction with AlignAtt for simultaneous translation and transcription. Applies in-domain terminology prompting and context accommodation alongside EuroLLM for cascaded systems.

**Key Contributions:**

	1. Implementation of the Whisper model for simultaneous translation.
	2. Introduction of a new measure for speech recognition latency.
	3. Significant BLEU score improvements over baseline performance.

**Result:** Achieved improvements of 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese, and Japanese on the development sets compared to the baseline.

**Limitations:** 

**Conclusion:** The proposed methods demonstrate significant improvements in simultaneous translation and introduce a new enhanced measure for speech recognition latency.

**Abstract:** This paper describes Charles University submission to the Simultaneous Speech Translation Task of the IWSLT 2025. We cover all four language pairs with a direct or cascade approach. The backbone of our systems is the offline Whisper speech model, which we use for both translation and transcription in simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We further improve the performance by prompting to inject in-domain terminology, and we accommodate context. Our cascaded systems further use EuroLLM for unbounded simultaneous translation. Compared to the Organizers' baseline, our systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on English to German, Chinese and Japanese on the development sets. Additionally, we also propose a new enhanced measure of speech recognition latency.

</details>


### [108] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)

*Ricardo Rei, Nuno M. Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** Fine-tuning, LLMs, Translation, General-purpose capabilities, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Tower+, a suite of models optimized for both translation and multilingual general-purpose tasks, achieving a balance between specialization and broad capabilities through a novel training process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLMs to excel in both specific tasks like translation and broader tasks like instruction-following without losing general utility is essential for real-world applications.

**Method:** The authors propose a training regime that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards, using curated data to enhance performance.

**Key Contributions:**

	1. Introduction of the Tower+ model suite for LLMs
	2. Demonstration of a training process achieving a Pareto frontier
	3. Introduction of the IF-MT benchmark for evaluating translation and instruction-following

**Result:** Tower+ models, particularly the largest 72B variant, excel in both translation tasks and general instruction-following, outperforming existing state-of-the-art models in several benchmarks, including IF-MT and multilingual Arena Hard evaluations.

**Limitations:** 

**Conclusion:** The research demonstrates that with the right methodology, it is feasible to develop models that maintain both strong general-purpose capabilities and specialized task performance, indicating potential for business applications in translation and localization.

**Abstract:** Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.

</details>


### [109] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)

*Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, Chain-of-Thought prompting, hallucination detection, empirical evaluation

**Relevance Score:** 9

**TL;DR:** This paper investigates the effect of Chain-of-Thought prompting on the detection of hallucinations in Large Language Models, revealing a trade-off between reduced hallucination frequency and detection efficacy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how Chain-of-Thought prompting affects hallucination detection in Large Language Models, addressing existing gaps in the research.

**Method:** Conducted a systematic empirical evaluation, starting with a pilot experiment, followed by an assessment of various CoT prompting methods on hallucination detection methods for both instruction-tuned and reasoning-oriented LLMs.

**Key Contributions:**

	1. Empirical evaluation of CoT prompting effects on hallucination detection
	2. Insights into the impact of CoT prompting on LLMs' internal states
	3. Identification of trade-offs in hallucination mitigation approaches

**Result:** CoT prompting reduces hallucination frequency but obscures critical signals used in detection, leading to decreased efficacy of detection methods.

**Limitations:** The study focuses on specific models and may not generalize across all LLM architectures or applications.

**Conclusion:** The study reveals a critical trade-off in using reasoning strategies to mitigate hallucinations in LLMs, impacting detection performance.

**Abstract:** Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [110] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)

*Murtaza Nazir, Matthew Finlayson, John X. Morris, Xiang Ren, Swabha Swayamdipta*

**Main category:** cs.CL

**Keywords:** language model inversion, prompt recovery, next-token probabilities, machine learning, security

**Relevance Score:** 7

**TL;DR:** The paper presents a novel method called PILS for recovering hidden prompts from language model outputs, showing significant improvements in recovery rates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address security concerns related to hidden prompts and potentially exposed private information from language models.

**Method:** The proposed approach, PROMPT INVERSION FROM LOGPROB SEQUENCES (PILS), utilizes next-token probabilities over multiple generation steps to recover hidden prompts effectively by compressing output information using a linear map.

**Key Contributions:**

	1. Development of a new method for recovering hidden prompts using next-token probabilities
	2. Demonstration of improved exact recovery rates
	3. Strong performance on recovering hidden system messages

**Result:** PILS achieves 2--3.5 times higher exact recovery rates than state-of-the-art methods, with notable improvements in prompt recovery, especially under varying generation step conditions.

**Limitations:** Potential limitations not discussed in detail.

**Conclusion:** The method highlights next-token probabilities as a critical vulnerability for inversion attacks, demonstrating stronger recovery rates and generalization to previously unseen conditions.

**Abstract:** Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.

</details>


### [111] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)

*Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen*

**Main category:** cs.CL

**Keywords:** KV footprint, language models, memory efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces the KV footprint as a metric for evaluating key-value (KV) cache management in language models, proposing methods that lower memory costs while maintaining performance for long-context tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for managing KV caches in language models do not adequately consider memory efficiency, often leading to high peak memory usage and performance degradation.

**Method:** The authors propose the KV footprint metric to evaluate cache management methods. They adapt existing post-fill eviction methods for pre-filling and introduce PruLong, an optimization approach for recency eviction methods.

**Key Contributions:**

	1. Introduction of the KV footprint metric for evaluating KV cache methods
	2. Adaptation of post-fill eviction methods for pre-filling
	3. Development of PruLong for optimizing recency eviction

**Result:** PruLong achieves a 12% reduction in KV footprint compared to prior methods while maintaining performance in long-context tasks, revealing inefficiencies in existing approaches.

**Limitations:** 

**Conclusion:** The proposed methods improve management of KV caches in language models, contributing to the efficiency of long-context inference.

**Abstract:** Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.

</details>


### [112] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)

*Naiming Liu, Richard Baraniuk, Shashank Sonkar*

**Main category:** cs.CL

**Keywords:** causal reasoning, language models, dataset, assertion-reason pairs, benchmark

**Relevance Score:** 8

**TL;DR:** CLEAR-3K is a dataset of 3,000 assertion-reasoning questions that evaluates language models' ability to determine causal relationships between statements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether language models can distinguish between semantic similarity and genuine causal explanatory relationships.

**Method:** Evaluation of 21 state-of-the-art language models with varied parameter sizes on tasks involving assertion-reason pairs.

**Key Contributions:**

	1. Introduction of a dataset for evaluating causal reasoning in language models
	2. Identification of key performance limitations of current models
	3. Providing a benchmark for future model improvements in causal reasoning

**Result:** Models confuse semantic similarity with causality and show a performance plateau at a Matthews Correlation Coefficient of 0.55, even among the largest models.

**Limitations:** Models still show a limited ability to ascertain true causal relationships, with a plateau in performance metrics.

**Conclusion:** CLEAR-3K is a vital benchmark for advancing the understanding of causal reasoning in AI language models.

**Abstract:** We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions designed to evaluate whether language models can determine if one statement causally explains another. Each question present an assertion-reason pair and challenge language models to distinguish between semantic relatedness and genuine causal explanatory relationships. Through comprehensive evaluation of 21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we identify two fundamental findings. First, language models frequently confuse semantic similarity with causality, relying on lexical and semantic overlap instead of inferring actual causal explanatory relationships. Second, as parameter size increases, models tend to shift from being overly skeptical about causal relationships to being excessively permissive in accepting them. Despite this shift, performance measured by the Matthews Correlation Coefficient plateaus at just 0.55, even for the best-performing models.Hence, CLEAR-3K provides a crucial benchmark for developing and evaluating genuine causal reasoning in language models, which is an essential capability for applications that require accurate assessment of causal relationships.

</details>


### [113] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)

*Yuchen Li, Hengyi Cai, Rui Kong, Xinran Chen, Jiamin Chen, Jun Yang, Haojie Zhang, Jiayi Li, Jiayi Wu, Yiqun Chen, Changle Qu, Keyi Kong, Wenwen Ye, Lixin Su, Xinyu Ma, Long Xia, Daiting Shi, Jiashu Zhao, Haoyi Xiong, Shuaiqiang Wang, Dawei Yin*

**Main category:** cs.CL

**Keywords:** AI Search Paradigm, LLM agents, retrieval-augmented generation

**Relevance Score:** 8

**TL;DR:** This paper introduces the AI Search Paradigm, a detailed framework for next-gen search systems that simulates human decision-making using LLM-powered agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive blueprint for search systems that can handle diverse information needs with human-like processing.

**Method:** The paper presents a modular architecture consisting of four LLM agents (Master, Planner, Executor, Writer) that work collaboratively on tasks of varying complexity through coordinated workflows.

**Key Contributions:**

	1. Introduction of a modular architecture for AI search systems
	2. Detailed methodologies for task planning and tool integration
	3. Strategies for robust retrieval-augmented generation and LLM inference optimization

**Result:** The proposed methodology improves task planning, tool integration, and retrieval-augmented generation, optimizing LLM inference and search system performance.

**Limitations:** 

**Conclusion:** The AI Search Paradigm provides foundational components essential for developing adaptive and scalable AI search systems.

**Abstract:** In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint for next-generation search systems capable of emulating human information processing and decision-making. The paradigm employs a modular architecture of four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically adapt to the full spectrum of information needs, from simple factual queries to complex multi-stage reasoning tasks. These agents collaborate dynamically through coordinated workflows to evaluate query complexity, decompose problems into executable plans, and orchestrate tool usage, task execution, and content synthesis. We systematically present key methodologies for realizing this paradigm, including task planning and tool integration, execution strategies, aligned and robust retrieval-augmented generation, and efficient LLM inference, spanning both algorithmic techniques and infrastructure-level optimizations. By providing an in-depth guide to these foundational components, this work aims to inform the development of trustworthy, adaptive, and scalable AI search systems.

</details>


### [114] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)

*Kathleen C. Fraser, Hillary Dawkins, Isar Nejadgholi, Svetlana Kiritchenko*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Safety Evaluation, Human-Computer Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper discusses the critical failure mode of fine-tuning large language models (LLMs) leading to reduced safety alignment, highlighting the need for reliable safety evaluations amidst variability in experimental setups.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the vulnerabilities created when fine-tuning LLMs, which can lead to unintentional removal of safety features that protect against harmful outputs.

**Method:** The authors investigate the robustness of a safety benchmark against minor experimental variations and the inherent stochasticity of LLMs, conducting initial experiments to gauge result variance.

**Key Contributions:**

	1. Identified critical safety vulnerabilities in fine-tuned LLMs due to unintentional misalignment.
	2. Highlighted the variability in safety evaluations based on experimental setups.
	3. Proposed the necessity for standardized reporting in safety benchmark experiments.

**Result:** The initial experiments reveal significant variance in safety evaluation results even with minor changes to the fine-tuning procedure, suggesting challenges in obtaining consistent safety benchmarks.

**Limitations:** The study may not account for all potential variables affecting safety evaluations; further research is needed to encompass a broader range of scenarios.

**Conclusion:** The findings have important implications for researchers, stressing the need for more accurate reporting of evaluation results to facilitate meaningful comparisons and improve safety in fine-tuned models.

**Abstract:** Fine-tuning a general-purpose large language model (LLM) for a specific domain or task has become a routine procedure for ordinary users. However, fine-tuning is known to remove the safety alignment features of the model, even when the fine-tuning data does not contain any harmful content. We consider this to be a critical failure mode of LLMs due to the widespread uptake of fine-tuning, combined with the benign nature of the "attack". Most well-intentioned developers are likely unaware that they are deploying an LLM with reduced safety. On the other hand, this known vulnerability can be easily exploited by malicious actors intending to bypass safety guardrails. To make any meaningful progress in mitigating this issue, we first need reliable and reproducible safety evaluations. In this work, we investigate how robust a safety benchmark is to trivial variations in the experimental procedure, and the stochastic nature of LLMs. Our initial experiments expose surprising variance in the results of the safety evaluation, even when seemingly inconsequential changes are made to the fine-tuning setup. Our observations have serious implications for how researchers in this field should report results to enable meaningful comparisons in the future.

</details>


### [115] [Voices of Her: Analyzing Gender Differences in the AI Publication World](https://arxiv.org/abs/2305.14597)

*Yiwen Ding, Jiarui Liu, Zhiheng Lyu, Kun Zhang, Bernhard Schoelkopf, Zhijing Jin, Rada Mihalcea*

**Main category:** cs.CL

**Keywords:** gender bias, AI community, co-authorship, linguistic styles, citation analysis

**Relevance Score:** 4

**TL;DR:** This study analyzes gender differences in the AI community, highlighting discrepancies in citations, co-authorship, and linguistic styles in papers authored by different genders.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive analysis of gender bias in the AI community, exploring trends and differences across various demographics.

**Method:** Utilized the AI Scholar dataset, examining 78,000 researchers to analyze citation patterns, co-authorship homophily, and linguistic styles in papers based on gender.

**Key Contributions:**

	1. Comprehensive analysis of gender bias in AI research
	2. Insightful data on citation metrics and academic age groups
	3. Unique linguistic contrasts in papers based on authors' gender

**Result:** Identified that female researchers have fewer citations overall but this varies by academic age; noted significant gender homophily in co-authorship; highlighted distinct linguistic styles in papers authored by females.

**Limitations:** The analysis is limited to the AI context and may not fully represent other research fields or account for intersectional factors.

**Conclusion:** The findings reveal ongoing issues of gender disparity within the AI community and advocate for increased gender diversity and equality.

**Abstract:** While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.

</details>


### [116] [LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning](https://arxiv.org/abs/2312.04684)

*Zifan Xu, Haozhu Wang, Dmitriy Bespalov, Xian Wu, Peter Stone, Yanjun Qi*

**Main category:** cs.CL

**Keywords:** Latent Reasoning Skills, in-context learning, large language models, unsupervised learning, example selection

**Relevance Score:** 8

**TL;DR:** This paper presents Latent Reasoning Skills (LaRS), an unsupervised learning approach that enhances in-context learning by creating latent space representations of rationales, improving the efficiency of example selection for large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of traditional in-context learning approaches that rely on example selection based on questions rather than the necessary reasoning processes, which are costly and difficult to scale.

**Method:** LaRS employs unsupervised learning to represent rationales in a latent space with a reasoning skill variable, alongside a reasoning policy that determines the skill required for a question, facilitating more efficient selection of ICL examples.

**Key Contributions:**

	1. Introduction of Latent Reasoning Skills (LaRS) for better efficiency in ICL
	2. Elimination of the need for manual prompt design
	3. Improved robustness in example selection for reasoning tasks.

**Result:** LaRS outperforms existing skill-based selection methods in terms of speed (processing examples four times faster) and reduces the number of LLM inferences needed during selection by half while showing robustness to less optimal example banks.

**Limitations:** 

**Conclusion:** The introduction of LaRS represents a significant improvement in the efficiency and scalability of ICL approaches for LLMs by eliminating reliance on auxiliary LLM inference or manual prompt design, supported by strong empirical results.

**Abstract:** Chain-of-thought (CoT) prompting is a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. This approach is theoretically grounded and compute-efficient, eliminating the need for auxiliary LLM inference or manual prompt design. Empirical results demonstrate that LaRS consistently outperforms SOTA skill-based selection methods, processing example banks four times faster, reducing LLM inferences during the selection stage by half, and showing greater robustness to sub-optimal example banks.

</details>


### [117] [AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability](https://arxiv.org/abs/2402.09404)

*Siwei Yang, Bingchen Zhao, Cihang Xie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Sequential Reasoning, AQA-Bench

**Relevance Score:** 8

**TL;DR:** Introduction of AQA-Bench, a benchmark for assessing sequential reasoning in LLMs using algorithms like DFS.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts and improve their performance in interactive environments.

**Method:** Development of AQA-Bench, an interactive benchmark incorporating three algorithms: binary search, depth-first search, and breadth-first search, evaluating 14 different LLMs.

**Key Contributions:**

	1. Development of AQA-Bench for LLM evaluation
	2. Identification of performance differences between closed-source and open-source LLMs
	3. Insights into how in-context examples affect model performance

**Result:** Closed-source models outperform open-source ones in sequential reasoning; providing in-context examples can reduce performance for some models; limited predecessor steps can improve performance; weak models struggle with initial steps; performance scaling is complex and not always consistent with model size.

**Limitations:** The study is limited to the selected algorithms and models, and the findings may not generalize to all LLMs or algorithm types.

**Conclusion:** The findings highlight significant differences in LLM capabilities and suggest directions for enhancing sequential reasoning in future research.

**Abstract:** This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol - for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves considering the possible environmental feedback in the future steps. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 14 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show much stronger sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing in-context examples may inadvertently hurt few-shot performance in an interactive environment due to over-fitting to examples. (3) Instead of using optimal steps from another test case as the in-context example, a very limited number of predecessor steps in the current test case following the optimal policy can substantially boost small models' performance. (4) The performance gap between weak models and strong models is greatly due to the incapability of weak models to start well. (5) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.

</details>


### [118] [A Survey of Automatic Hallucination Evaluation on Natural Language Generation](https://arxiv.org/abs/2404.12041)

*Siya Qi, Lin Gui, Yulan He, Zheng Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Evaluation, Evaluation Frameworks, Machine Learning, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This survey analyzes 74 Automatic Hallucination Evaluation (AHE) methods for Large Language Models (LLMs), identifying gaps and proposing a unified evaluation framework.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical need for reliable evaluation of hallucinations in LLMs due to methodological fragmentation in the current evaluation landscape.

**Method:** A comprehensive analysis of 74 evaluation methods specific to LLMs, including the formulation of a unified evaluation pipeline with datasets, benchmarks, evidence collection strategies, and comparison mechanisms.

**Key Contributions:**

	1. Comprehensive analysis of 74 AHE methods for LLMs
	2. Unified evaluation pipeline formulation
	3. Identification of key challenges and strategic directions for future research

**Result:** The survey reveals that 74% of the methods target LLMs, highlighting a paradigm shift requiring new evaluation frameworks, and documents the evolution of evaluation methodologies.

**Limitations:** Current evaluation methods have fundamental limitations affecting their real-world deployment.

**Conclusion:** The paper provides a roadmap for improving hallucination evaluation systems, suggesting enhanced interpretability and application-specific criteria as key areas for future research.

**Abstract:** The proliferation of Large Language Models (LLMs) has introduced a critical challenge: accurate hallucination evaluation that ensures model reliability. While Automatic Hallucination Evaluation (AHE) has emerged as essential, the field suffers from methodological fragmentation, hindering both theoretical understanding and practical advancement. This survey addresses this critical gap through a comprehensive analysis of 74 evaluation methods, revealing that 74% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a unified evaluation pipeline encompassing datasets and benchmarks, evidence collection strategies, and comparison mechanisms, systematically documenting the evolution from pre-LLM to post-LLM methodologies. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.

</details>


### [119] [BEADs: Bias Evaluation Across Domains](https://arxiv.org/abs/2406.04220)

*Shaina Raza, Mizanur Rahman, Michael R. Zhang*

**Main category:** cs.CL

**Keywords:** bias detection, large language models, natural language processing, dataset, AI ethics

**Relevance Score:** 9

**TL;DR:** Introduction of the BEADs dataset to address bias in LLMs across multiple NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive dataset for bias detection that encompasses various NLP tasks, moving beyond the limitations of existing datasets.

**Method:** Developed BEADs dataset with gold-standard annotations from GPT-4, validated by experts, to cover tasks like classification, token classification, and bias quantification.

**Key Contributions:**

	1. Introduction of a diverse dataset for bias detection
	2. Gold-standard annotations by GPT-4
	3. Support for a wide range of NLP tasks

**Result:** BEADs surfaced various biases in models during fine-tuning and helped mitigate them in language generation tasks without compromising output quality.

**Limitations:** 

**Conclusion:** BEADs serves as a valuable resource for identifying and reducing bias in NLP models, promoting the creation of responsible AI.

**Abstract:** Recent advancements in large language models (LLMs) have significantly improved natural language processing (NLP) applications. However, these models often inherit biases from their training data. While several datasets exist for bias detection, most are limited to one or two NLP tasks, typically classification or evaluation, and lack comprehensive coverage across a broader range of tasks. To address this gap, we introduce the Bias Evaluations Across Domains (BEADs) dataset, designed to support a wide range of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key contribution of this work is the gold-standard annotation provided by GPT-4 for scalability, with expert verification to ensure high reliability. BEADs can be used for both fine-tuning models (for classification and generation tasks) and evaluating LLM behavior. Our findings show that BEADs effectively surfaces various biases during model fine-tuning and helps reduce biases in language generation tasks while maintaining output quality. The dataset also highlights prevalent demographic biases in LLMs during evaluation. We release BEADs as a practical resource for detecting and mitigating bias across domains, supporting the development of responsible AI systems. Project: https://vectorinstitute.github.io/BEAD/ Data: https://huggingface.co/datasets/shainar/BEAD

</details>


### [120] [Learning to Refine with Fine-Grained Natural Language Feedback](https://arxiv.org/abs/2407.02397)

*Manya Wadhwa, Xinyu Zhao, Junyi Jessy Li, Greg Durrett*

**Main category:** cs.CL

**Keywords:** Large Language Models, Feedback Mechanisms, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper proposes a novel method called Detect, Critique, Refine (DCR) for enhancing the factual consistency of document-grounded summaries generated by LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the refinement processes in LLM-generated responses by evaluating effective feedback mechanisms instead of just model sizes.

**Method:** The DCR method involves three steps: detection of poor generations using a discriminative model, generating fine-grained critiques via a critique model, and refining responses with that feedback, leveraging prompted or fine-tuned LLMs.

**Key Contributions:**

	1. Introduction of the Detect, Critique, Refine method for LLM refinement.
	2. Separation of critique generation and detection to improve feedback quality.
	3. Empirical evidence showing DCR's superior performance in factual consistency improvements.

**Result:** DCR outperforms existing approaches in refining factual consistency of summaries, demonstrating that models with varying capabilities benefit from this structured feedback process.

**Limitations:** Potential dependence on the quality of the initial discriminative model for detection phases.

**Conclusion:** The ability to fine-tune critiques separately from the generation process enhances LLM performance, suggesting a better way to improve response quality in LLMs.

**Abstract:** Recent work has explored the capability of large language models (LLMs) to identify and correct errors in LLM-generated responses. These refinement approaches frequently evaluate what sizes of models are able to do refinement for what problems, but less attention is paid to what effective feedback for refinement looks like. In this work, we propose looking at refinement with feedback as a composition of three distinct LLM competencies: (1) detection of bad generations; (2) fine-grained natural language critique generation; (3) refining with fine-grained feedback. The first step can be implemented with a high-performing discriminative model and steps 2 and 3 can be implemented either via prompted or fine-tuned LLMs. A key property of the proposed Detect, Critique, Refine ("DCR") method is that the step 2 critique model can give fine-grained feedback about errors, made possible by offloading the discrimination to a separate model in step 1. We show that models of different capabilities benefit from refining with DCR on the task of improving factual consistency of document grounded summaries. Overall, DCR consistently outperforms existing end-to-end refinement approaches and current trained models not fine-tuned for factuality critiquing.

</details>


### [121] [sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting](https://arxiv.org/abs/2407.09879)

*Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram*

**Main category:** cs.CL

**Keywords:** multilingual, large language models, instruction tuning, fine-tuning, synthetic datasets

**Relevance Score:** 9

**TL;DR:** This paper introduces sPhinX, a multilingual synthetic instruction tuning dataset, to improve large language models' performance in non-English languages and presents LANGIT, an N-shot guided fine-tuning strategy.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant performance gap of large language models (LLMs) in non-English languages compared to English.

**Method:** The paper presents sPhinX, a method for constructing a diverse multilingual synthetic instruction tuning dataset by augmenting English instruction-response pairs with multilingual translations and proposes LANGIT, an N-shot guided fine-tuning strategy for further model enhancement.

**Key Contributions:**

	1. Introduction of the sPhinX dataset for multilingual instruction tuning
	2. Development of LANGIT, a novel N-shot guided fine-tuning strategy
	3. Demonstrated performance improvements on multilingual benchmarks

**Result:** The approach demonstrates a 39.8% improvement in Mistral-7B and an 11.2% improvement in Phi-3-Small across various multilingual benchmarks, while maintaining strong English performance without catastrophic forgetting.

**Limitations:** 

**Conclusion:** sPhinX effectively enhances multilingual capabilities of LLMs, showing significant performance gains in reasoning, question answering, reading comprehension, and machine translation across 51 languages.

**Abstract:** Despite the remarkable success of large language models (LLMs) in English, a significant performance gap remains in non-English languages. To address this, we introduce a novel approach for strategically constructing a multilingual synthetic instruction tuning dataset, sPhinX. Unlike prior methods that directly translate fixed instruction-response pairs, sPhinX enhances diversity by selectively augmenting English instruction-response pairs with multilingual translations. Additionally, we propose LANGIT, a novel N-shot guided fine-tuning strategy, which further enhances model performance by incorporating contextually relevant examples in each training sample. Our ablation study shows that our approach enhances the multilingual capabilities of Mistral-7B and Phi-3-Small improving performance by an average of 39.8% and 11.2%, respectively, across multilingual benchmarks in reasoning, question answering, reading comprehension, and machine translation. Moreover, sPhinX maintains strong performance on English LLM benchmarks while exhibiting minimal to no catastrophic forgetting, even when trained on 51 languages.

</details>


### [122] [Contextual modulation of language comprehension in a dynamic neural model of lexical meaning](https://arxiv.org/abs/2407.14701)

*Michael C. Stern, Maria M. Piñango*

**Main category:** cs.CL

**Keywords:** lexical meaning, neural model, polysemy, semantic dimensions, contextual modulation

**Relevance Score:** 6

**TL;DR:** This paper presents a dynamic neural model of lexical meaning, focusing on the polysemous use of the English word 'have'.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how lexical meaning is retrieved and how it is influenced by context and individual differences.

**Method:** Development and simulation of a neural model that maps lexical items to a semantic space defined by concepts of connectedness and control asymmetry.

**Key Contributions:**

	1. Introduced a dynamic neural model for understanding lexical meaning retrieval.
	2. Demonstrated empirical support for the model's predictive capabilities through experimental results.
	3. Proposed the concept of metastable states in lexical interpretation.

**Result:** The model captures contextual modulation of lexical meanings and individual variations in interpretation, supporting statistically derived predictions about reading times.

**Limitations:** 

**Conclusion:** The findings suggest that lexical meanings are dynamic states influenced by the neural architecture of semantic interpretation, challenging traditional views of polysemy.

**Abstract:** We propose and computationally implement a dynamic neural model of lexical meaning, and experimentally test its behavioral predictions. We demonstrate the architecture and behavior of the model using as a test case the English lexical item 'have', focusing on its polysemous use. In the model, 'have' maps to a semantic space defined by two continuous conceptual dimensions, connectedness and control asymmetry, previously proposed to parameterize the conceptual system for language. The mapping is modeled as coupling between a neural node representing the lexical item and neural fields representing the conceptual dimensions. While lexical knowledge is modeled as a stable coupling pattern, real-time lexical meaning retrieval is modeled as the motion of neural activation patterns between metastable states corresponding to semantic interpretations or readings. Model simulations capture two previously reported empirical observations: (1) contextual modulation of lexical semantic interpretation, and (2) individual variation in the magnitude of this modulation. Simulations also generate a novel prediction that the by-trial relationship between sentence reading time and acceptability should be contextually modulated. An experiment combining self-paced reading and acceptability judgments replicates previous results and confirms the new model prediction. Altogether, results support a novel perspective on lexical polysemy: that the many related meanings of a word are metastable neural activation states that arise from the nonlinear dynamics of neural populations governing interpretation on continuous semantic dimensions.

</details>


### [123] [Deep Learning based Visually Rich Document Content Understanding: A Survey](https://arxiv.org/abs/2408.01287)

*Yihao Ding, Soyeon Caren Han, Jean Lee, Eduard Hovy*

**Main category:** cs.CL

**Keywords:** Visually Rich Documents, Deep Learning, Information Extraction, Multimodal Models, Content Understanding

**Relevance Score:** 8

**TL;DR:** A survey on deep learning-based frameworks for extracting information from visually rich documents, categorizing methods, and analyzing their effectiveness.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies of traditional methods in extracting information from visually rich documents using deep learning techniques.

**Method:** The paper categorizes existing deep learning frameworks based on modeling strategies and downstream tasks, and includes a comparative analysis of key components such as feature representation, fusion techniques, model architectures, and pretraining objectives.

**Key Contributions:**

	1. Comprehensive overview of deep learning frameworks for VRD Content Understanding
	2. Comparative analysis of feature representation, fusion techniques, and model architectures
	3. Discussion of emerging trends and practical deployment challenges

**Result:** The survey reveals that multimodal models significantly enhance information extraction performance from visually rich documents, although each method has its own strengths and weaknesses depending on the application.

**Limitations:** Some methods may not generalize well to all types of visually rich documents and practical scenarios.

**Conclusion:** The paper discusses current challenges in the field and proposes directions for future research and practical applications in extracting information from visually rich documents.

**Abstract:** Visually Rich Documents (VRDs) play a vital role in domains such as academia, finance, healthcare, and marketing, as they convey information through a combination of text, layout, and visual elements. Traditional approaches to extracting information from VRDs rely heavily on expert knowledge and manual annotation, making them labor-intensive and inefficient. Recent advances in deep learning have transformed this landscape by enabling multimodal models that integrate vision, language, and layout features through pretraining, significantly improving information extraction performance. This survey presents a comprehensive overview of deep learning-based frameworks for VRD Content Understanding (VRD-CU). We categorize existing methods based on their modeling strategies and downstream tasks, and provide a comparative analysis of key components, including feature representation, fusion techniques, model architectures, and pretraining objectives. Additionally, we highlight the strengths and limitations of each approach and discuss their suitability for different applications. The paper concludes with a discussion of current challenges and emerging trends, offering guidance for future research and practical deployment in real-world scenarios.

</details>


### [124] [Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives](https://arxiv.org/abs/2408.06904)

*Zhihu Wang, Shiwan Zhao, Yu Wang, Heyuan Huang, Sitao Xie, Yubo Zhang, Jiaxin Shi, Zhixing Wang, Hongyan Li, Junchi Yan*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Re-TASK framework, large language models

**Relevance Score:** 9

**TL;DR:** This paper introduces the Re-TASK framework, which improves task decomposition and execution in large language models (LLMs) using a Chain-of-Learning paradigm, demonstrating significant performance improvements in domain-specific tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in applying the Chain-of-Thought paradigm to domain-specific tasks, particularly the failure of LLMs to accurately decompose tasks and execute subtasks.

**Method:** The Re-TASK framework revisits tasks by analyzing capability, skill, and knowledge, using principles from Bloom's Taxonomy and Knowledge Space Theory, and introduces a Re-TASK prompting strategy for targeted knowledge and skill enhancements.

**Key Contributions:**

	1. Introduction of the Re-TASK framework for LLM task enhancement
	2. Development of the Chain-of-Learning paradigm for task decomposition
	3. Proposed Re-TASK prompting strategy for targeted knowledge injection

**Result:** Experiments show Re-TASK improvements of 45.00% on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks, demonstrating significant effectiveness in specialized domains.

**Limitations:** 

**Conclusion:** The Re-TASK framework presents a promising approach to enhance LLM performance by focusing on task-relevant capabilities and skill adaptation, with implications for diverse fields.

**Abstract:** The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving complex problems with large language models (LLMs). However, its application to domain-specific tasks remains challenging, as LLMs often fail to decompose tasks accurately or execute subtasks effectively. This paper introduces the Re-TASK framework, a novel theoretical model that revisits LLM tasks from capability, skill, and knowledge perspectives, drawing on the principles of Bloom's Taxonomy and Knowledge Space Theory. While CoT provides a workflow-centric perspective on tasks, Re-TASK introduces a Chain-of-Learning (CoL) paradigm that highlights task dependencies on specific capability items, further broken down into their constituent knowledge and skill components. To address CoT failures, we propose a Re-TASK prompting strategy, which strengthens task-relevant capabilities through targeted knowledge injection and skill adaptation. Experiments across diverse domains demonstrate the effectiveness of Re-TASK. In particular, we achieve improvements of 45.00% on Yi-1.5-9B and 24.50% on Llama3-Chinese-8B for legal tasks. These results highlight the potential of Re-TASK to significantly enhance LLM performance and its applicability in specialized domains. We release our code and data at https://github.com/Uylee/Re-TASK.

</details>


### [125] [Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management](https://arxiv.org/abs/2409.00128)

*Ziyan Cui, Ning Li, Huaikang Zhou*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Large Language Models, Psychological Research, Social Sciences, Replication Study

**Relevance Score:** 8

**TL;DR:** A study replicating psychological experiments using Large Language Models (LLMs) reveals high replication rates but also challenges in areas like socially sensitive topics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the efficacy of Large Language Models in replicating psychological experiments and their implications in social sciences.

**Method:** A large-scale study replicating 156 psychological experiments from top social science journals using GPT-4, Claude 3.5 Sonnet, and DeepSeek v3.

**Key Contributions:**

	1. High replication rates of psychological experiments by LLMs
	2. Significant differences in effect sizes compared to human studies
	3. Identification of limitations in handling socially sensitive topics.

**Result:** LLMs showed high replication rates for main effects (73-81%) but produced larger effect sizes than human studies, especially struggling with socially sensitive research.

**Limitations:** LLMs showed lower replication rates for studies on sensitive topics and high rates of false positives in null findings.

**Conclusion:** The study highlights LLMs as promising tools for rapid hypothesis validation but emphasizes the need for human validation in complex social phenomena.

**Abstract:** Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) have shown promise in replicating human-like responses in various psychological experiments. We conducted a large-scale study replicating 156 psychological experiments from top social science journals using three state-of-the-art LLMs (GPT-4, Claude 3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate high replication rates for main effects (73-81%) and moderate to strong success with interaction effects (46-63%), They consistently produce larger effect sizes than human studies, with Fisher Z values approximately 2-3 times higher than human studies. Notably, LLMs show significantly lower replication rates for studies involving socially sensitive topics such as race, gender and ethics. When original studies reported null findings, LLMs produced significant results at remarkably high rates (68-83%) - while this could reflect cleaner data with less noise, as evidenced by narrower confidence intervals, it also suggests potential risks of effect size overestimation. Our results demonstrate both the promise and challenges of LLMs in psychological research, offering efficient tools for pilot testing and rapid hypothesis validation while enriching rather than replacing traditional human subject studies, yet requiring more nuanced interpretation and human validation for complex social phenomena and culturally sensitive research questions.

</details>


### [126] [Core Knowledge Deficits in Multi-Modal Language Models](https://arxiv.org/abs/2410.10855)

*Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert D. Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, Hokin Deng*

**Main category:** cs.CL

**Keywords:** Multi-modal Large Language Models, core knowledge, cognitive abilities, shortcut learning, benchmark

**Relevance Score:** 9

**TL;DR:** The paper introduces CoreCognition, a benchmark for evaluating core knowledge in Multi-modal Large Language Models (MLLMs), finding that these models lack basic cognitive abilities and rely on shortcut learning.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the core knowledge representation in MLLMs and identify the reasons for their robustness limitations in real-world tasks.

**Method:** The study evaluates 230 MLLMs using 11 different prompts across 12 core knowledge concepts, analyzing a total of 2,530 data points.

**Key Contributions:**

	1. Introduction of the CoreCognition benchmark for evaluating MLLMs' cognitive abilities
	2. Identification of core knowledge deficits in MLLMs
	3. Proposal of Concept Hacking as a new evaluation method

**Result:** Key findings reveal that MLLMs underperform on low-level cognitive tasks compared to high-level ones and show deficiencies in scaling their capabilities.

**Limitations:** The study focuses only on specific core knowledge concepts and does not address all possible cognitive abilities.

**Conclusion:** MLLMs are not progressing toward genuine understanding of core knowledge but are instead exploiting shortcut learning as they scale up.

**Abstract:** While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.

</details>


### [127] [SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments](https://arxiv.org/abs/2410.11331)

*Syed Abdul Gaffar Shakhadri, Kruthika KR, Rakshit Aralimatti*

**Main category:** cs.CL

**Keywords:** language model, edge AI, resource-constrained environments, NLP, real-time applications

**Relevance Score:** 9

**TL;DR:** Shakti is a 2.5 billion parameter language model optimized for resource-constrained environments, excelling in NLP tasks on edge devices such as smartphones, wearables, and IoT systems.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a high-performance NLP model that can operate efficiently in resource-constrained environments and support vernacular languages and domain-specific tasks.

**Method:** Developed a 2.5 billion parameter language model optimized for edge devices, incorporating techniques to enhance efficiency and precision in NLP applications.

**Key Contributions:**

	1. Development of a high-performance NLP model for edge devices
	2. Support for vernacular languages
	3. Competitive performance against larger models with low latency

**Result:** Benchmark evaluations show Shakti performs competitively against larger models while maintaining low latency and efficiency on-device, suitable for real-time AI applications in various industries.

**Limitations:** 

**Conclusion:** Shakti positions itself as a leading solution for edge AI, particularly in the healthcare, finance, and customer service sectors, where computational resources are limited.

**Abstract:** We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.

</details>


### [128] [Learning to Route LLMs with Confidence Tokens](https://arxiv.org/abs/2410.13284)

*Yu-Neng Chuang, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu, Helen Zhou*

**Main category:** cs.CL

**Keywords:** large language models, confidence measurement, Self-REF, machine learning, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper investigates how large language models (LLMs) can reliably express confidence in their answers and introduces a new training strategy, Self-REF, to improve this capability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding when LLM outputs are unreliable is critical, especially in high-stakes applications, to enable appropriate decision-making concerning user queries.

**Method:** The authors propose a training approach called Self-Reflection with Error-based Feedback (Self-REF), which integrates confidence tokens into LLMs to evaluate answer correctness and reliability.

**Key Contributions:**

	1. Introduction of confidence tokens that can be extracted as confidence scores.
	2. Demonstration of substantial improvements in downstream tasks with the proposed training strategy.
	3. Comparison of Self-REF against conventional methods highlighting its effectiveness.

**Result:** Empirical results show that confidence tokens significantly enhance performance in downstream tasks involving routing and rejection learning compared to traditional methods of expressing confidence.

**Limitations:** 

**Conclusion:** The implementation of Self-REF can lead to improved trust and accuracy in LLM applications by better calibrating the models' confidence in their outputs.

**Abstract:** Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-Reflection with Error-based Feedback (Self-REF), a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.

</details>


### [129] [Principles of semantic and functional efficiency in grammatical patterning](https://arxiv.org/abs/2410.15865)

*Emily Cheng, Francesca Franzon*

**Main category:** cs.CL

**Keywords:** grammar, semantic encoding, language processing, cognitive constraints, universal patterns

**Relevance Score:** 3

**TL;DR:** This paper examines the relationship between grammatical features and sentence processing in human languages, proposing a unified theory based on semantic encoding and agreement-based predictability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explain the basis of universal grammatical patterns observed in diverse languages, which are rooted in a semantic foundation yet lack theoretical explanation.

**Method:** The authors present an information-theoretic framework that combines semantic encoding with agreement-based predictability under cognitive constraints, analyzing variable communicative needs across languages.

**Key Contributions:**

	1. Unification of semantic encoding and agreement in grammar
	2. Demonstration of grammatical patterns stemming from perceptual attributes
	3. Prioritization of efficient language processing in grammatical organization

**Result:** The study reveals that grammatical organization derives from perceptual attributes, showing that grammars prioritize functional efficiency in language processing over mere semantic encoding.

**Limitations:** 

**Conclusion:** Grammars exhibit consistent organizational patterns due to their functional goals aimed at promoting efficient sentence processing.

**Abstract:** Grammatical features such as number and gender serve two central functions in human languages. While they encode salient semantic attributes like numerosity and animacy, they also offload sentence processing cost by predictably linking words together via grammatical agreement. Grammars exhibit consistent organizational patterns across diverse languages, invariably rooted in a semantic foundation-a widely confirmed but still theoretically unexplained phenomenon. To explain the basis of universal grammatical patterns, we unify two fundamental properties of grammar, semantic encoding and agreement-based predictability, into a single information-theoretic objective under cognitive constraints, accounting for variable communicative need. Our analyses reveal that grammatical organization provably inherits from perceptual attributes, and our measurements on a diverse language sample show that grammars prioritize functional goals, promoting efficient language processing over semantic encoding.

</details>


### [130] [Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models](https://arxiv.org/abs/2411.04291)

*Saketh Bachu, Erfan Shayegani, Rohit Lal, Trishna Chakraborty, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury*

**Main category:** cs.CL

**Keywords:** vision-language models, safety alignment, harmful outputs, Layer-Wise PPO, multi-modal RLHF

**Relevance Score:** 8

**TL;DR:** This paper addresses the safety alignment challenges in vision-language models by discovering vulnerabilities associated with early exits from the image encoder, proposing a solution through a modified RLHF algorithm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety alignment challenges in vision-language models due to complex architectures and their distribution of harmful information.

**Method:** The paper reveals vulnerabilities in VLMs related to early exits from the image encoder and introduces the Layer-Wise PPO (L-PPO) algorithm for layer-wise multi-modal RLHF to mitigate harmful outputs.

**Key Contributions:**

	1. Identification of 'Image enCoder Early-exiT' (ICET) vulnerabilities in VLMs.
	2. Proposal of the Layer-Wise PPO (L-PPO) algorithm for improved safety alignment in VLMs.
	3. Experimental validation across multiple VLMs and datasets showing reduction in harmful outputs.

**Result:** Experiments show that early exits from the image encoder significantly increase harmful outputs, and L-PPO consistently reduces these harmful effects across three multimodal datasets.

**Limitations:** The findings are based on three specific VLMs, and generalizability to other architectures remains to be evaluated.

**Conclusion:** The proposed L-PPO algorithm effectively mitigates the harmfulness associated with early exits in vision-language models, enhancing their safety alignment.

**Abstract:** Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as "Image enCoder Early-exiT" based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2, show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multimodal datasets and show that it consistently reduces the harmfulness caused by early exits.

</details>


### [131] [Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control](https://arxiv.org/abs/2411.13100)

*Yunkee Chae, Eunsik Shin, Suntae Hwang, Seungryeol Paik, Kyogu Lee*

**Main category:** cs.CL

**Keywords:** lyrics generation, syllable control, song form, natural language processing, text generation

**Relevance Score:** 6

**TL;DR:** The paper proposes a framework for generating song lyrics with precise syllable control across multiple levels while respecting song structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of generating natural-sounding song lyrics that adhere to syllable constraints and song form structures.

**Method:** The proposed framework enables multi-level syllable control at the word, phrase, line, and paragraph levels, ensuring the generated lyrics align with input text and song form.

**Key Contributions:**

	1. Multi-level syllable control for lyrics generation
	2. Integration of song form awareness into the generation process
	3. Improved natural flow and coherence of generated lyrics

**Result:** The framework produces song lyrics that meet specified syllable constraints and maintain natural phrasing, improving upon conventional line-by-line generation methods.

**Limitations:** 

**Conclusion:** This approach advances the state of lyrics generation by allowing for more structured and coherent outputs while correctly managing syllable counts.

**Abstract:** Lyrics generation presents unique challenges, particularly in achieving precise syllable control while adhering to song form structures such as verses and choruses. Conventional line-by-line approaches often lead to unnatural phrasing, underscoring the need for more granular syllable management. We propose a framework for lyrics generation that enables multi-level syllable control at the word, phrase, line, and paragraph levels, aware of song form. Our approach generates complete lyrics conditioned on input text and song form, ensuring alignment with specified syllable constraints. Generated lyrics samples are available at: https://tinyurl.com/lyrics9999

</details>


### [132] [Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation](https://arxiv.org/abs/2411.16813)

*Svetlana Churina, Kokil Jaidka*

**Main category:** cs.CL

**Keywords:** AI systems, political argumentation, incivility, GPT-3.5 Turbo, fine-tuning

**Relevance Score:** 7

**TL;DR:** This study examines the effects of fine-tuning language models on political argumentation quality using contrasting datasets from Twitter and Reddit.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of incivility in AI-generated political arguments on social media platforms.

**Method:** Experiments with GPT-3.5 Turbo fine-tuned on datasets of political discussions from Twitter and Reddit, evaluating the impact on rhetorical framing and argument quality.

**Key Contributions:**

	1. Development of a rhetorical evaluation rubric
	2. Guidelines for deploying LLMs in moderation and deliberation
	3. Insights into fine-tuning strategies affecting argument quality

**Result:** Reddit-finetuned models yield safer but rigid arguments, while cross-platform fine-tuning increases toxicity. Prompting can reduce some toxic behaviors but not fully counteract high-incivility influences.

**Limitations:** High-incivility training data still adversely affects model behavior despite prompting.

**Conclusion:** A new rhetorical evaluation rubric was introduced, along with practical guidelines for the use of LLMs in content authoring and moderation.

**Abstract:** The incivility prevalent on platforms like Twitter (now X) and Reddit poses a challenge for developing AI systems that can support productive and rhetorically sound political argumentation. In this study, we report experiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of political discussions: high-variance, high-incivility Twitter replies to U.S. Congress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView. We systematically evaluate how these data sources and prompting strategies shape the rhetorical framing and deliberative quality of model-generated arguments. Our results show that Reddit-finetuned models produce safer but rhetorically rigid arguments, while cross-platform fine-tuning amplifies toxicity. Prompting reduces specific toxic behaviors, such as personal attacks, but fails to fully mitigate the influence of high-incivility training data. We introduce and validate a rhetorical evaluation rubric and provide practical guidelines for deploying LLMs in content authoring, moderation, and deliberation support.

</details>


### [133] [On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930)

*Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, domain adaptation, data synthesis, biomedicine, task evaluation

**Relevance Score:** 8

**TL;DR:** This paper explores domain adaptation of multimodal large language models (MLLMs) through post-training, focusing on data synthesis, a novel training pipeline, and task evaluation in various high-impact domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the practical applications of general MLLMs in specific domains such as biomedicine and food through effective adaptation strategies.

**Method:** The authors propose a generate-then-filter data synthesis approach that outperforms traditional methods, adopt a more effective single-stage training paradigm for domain adaptation, and conduct extensive testing on different domain-specific tasks.

**Key Contributions:**

	1. Development of a generate-then-filter data synthesis pipeline for domain-specific tasks.
	2. Introduction of a single-stage training approach for more effective domain adaptation.
	3. Extensive evaluation of MLLMs in high-impact domains with open-sourced models and data.

**Result:** The proposed methods demonstrate improved performance over existing approaches in enhancing domain-specific tasks in areas like biomedicine, food, and remote sensing.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of data synthesis and a single-stage training pipeline for adapting MLLMs to domain-specific tasks and opens resources for further research.

**Abstract:** Adapting general multimodal large language models (MLLMs) to specific domains, such as scientific and industrial fields, is highly significant in promoting their practical applications. This paper systematically investigates domain adaptation of MLLMs via post-training, focusing on data synthesis, training pipeline, and task evaluation. (1) Data Synthesis: Using only open-source models, we develop a generate-then-filter pipeline that curates diverse visual instruction tasks based on domain-specific image-caption pairs. The resulting data surpass the data synthesized by manual rules or strong closed-source models in enhancing domain-specific performance. (2) Training Pipeline: Unlike general MLLMs that typically adopt a two-stage training paradigm, we find that a single-stage approach is more effective for domain adaptation. (3) Task Evaluation: We conduct extensive experiments in high-impact domains such as biomedicine, food, and remote sensing, by post-training a variety of MLLMs and then evaluating MLLM performance on various domain-specific tasks. Finally, we fully open-source our models, code, and data to encourage future research in this area.

</details>


### [134] [Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling](https://arxiv.org/abs/2412.14860)

*Junyi Li, Hwee Tou Ng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attributed Text Generation, Monte Carlo Tree Search, Self-Reflection, Progress Reward Modeling

**Relevance Score:** 9

**TL;DR:** This paper introduces Think&Cite, a framework for attributed text generation using self-guided Monte Carlo Tree Search to mitigate hallucination in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the problem of hallucinations and factually incorrect information generated by large language models (LLMs).

**Method:** Propose a novel framework called Think&Cite that integrates attributed text generation with a multi-step reasoning process using Self-Guided Monte Carlo Tree Search (SG-MCTS).

**Key Contributions:**

	1. Introduction of the Think&Cite framework for attributed text generation.
	2. Development of Self-Guided Monte Carlo Tree Search (SG-MCTS) for guiding text generation and search.
	3. Implementation of Progress Reward Modeling for measuring search progress.

**Result:** The proposed approach significantly outperforms baseline methods in generating attributed text across three different datasets.

**Limitations:** 

**Conclusion:** Think&Cite effectively utilizes self-reflection of LLMs and introduces Progress Reward Modeling, improving both generation and attribution in text production.

**Abstract:** Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reason about the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Modeling to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.

</details>


### [135] [Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation](https://arxiv.org/abs/2502.12911)

*Zheng Yuan, Hao Chen, Zijin Hong, Qinggang Zhang, Feiran Huang, Qing Li, Xiao Huang*

**Main category:** cs.CL

**Keywords:** schema linking, SQL generation, optimization, human-computer interaction, machine learning

**Relevance Score:** 4

**TL;DR:** This paper presents KaSLA, a novel schema linking method that enhances SQL generation accuracy by addressing missing relevant schema elements and minimizing redundant ones through optimization techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The accuracy of schema linking significantly affects SQL generation, but existing models struggle with missing and redundant schema elements. Current metrics do not capture the real performance of schema linking adequately.

**Method:** The authors introduce KaSLA, which employs a hierarchical linking strategy and a knapsack optimization approach to improve schema linking by effectively linking relevant schema elements while minimizing excess ones.

**Key Contributions:**

	1. Development of enhanced schema linking metrics with a restricted missing indicator.
	2. Introduction of a hierarchical linking strategy and knapsack optimization for schema linking.
	3. Demonstration of KaSLA's superior SQL generation performance on benchmarks.

**Result:** KaSLA-1.6B demonstrates superior performance in schema linking compared to existing large-scale LLMs and state-of-the-art methods, leading to improved SQL generation results.

**Limitations:** 

**Conclusion:** The extensive experiments on benchmarks show that KaSLA can significantly enhance the SQL generation performance of SOTA Text2SQL models by improving their schema linking processes.

**Abstract:** Generating SQLs from user queries is a long-standing challenge, where the accuracy of initial schema linking significantly impacts subsequent SQL generation performance. However, current schema linking models still struggle with missing relevant schema elements or an excess of redundant ones. A crucial reason for this is that commonly used metrics, recall and precision, fail to capture relevant element missing and thus cannot reflect actual schema linking performance. Motivated by this, we propose enhanced schema linking metrics by introducing a restricted missing indicator. Accordingly, we introduce Knapsack optimization-based Schema Linking Approach (KaSLA), a plug-in schema linking method designed to prevent the missing of relevant schema elements while minimizing the inclusion of redundant ones. KaSLA employs a hierarchical linking strategy that first identifies the optimal table linking and subsequently links columns within the selected table to reduce linking candidate space. In each linking process, it utilizes a knapsack optimization approach to link potentially relevant elements while accounting for a limited tolerance of potentially redundant ones. With this optimization, KaSLA-1.6B achieves superior schema linking results compared to large-scale LLMs, including deepseek-v3 with the state-of-the-art (SOTA) schema linking method. Extensive experiments on Spider and BIRD benchmarks verify that KaSLA can significantly improve the SQL generation performance of SOTA Text2SQL models by substituting their schema linking processes.

</details>


### [136] [Group-Level Data Selection for Efficient Pretraining](https://arxiv.org/abs/2502.14709)

*Zichun Yu, Fei Peng, Jie Lei, Arnold Overwijk, Wen-tau Yih, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** language model, data selection, machine learning, pretraining, group-level optimization

**Relevance Score:** 9

**TL;DR:** This paper presents Group-MATES, a group-level data selection approach that enhances the efficiency of language model pretraining, achieving significant performance improvements and reduced token requirements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of language model pretraining by optimizing the speed-quality frontier through group-level data selection.

**Method:** Group-MATES uses a relational data influence model to parameterize costly group-level selection, partitioning datasets into clusters based on relationship weights for independent data selection.

**Key Contributions:**

	1. Introduction of Group-MATES for group-level data selection
	2. Use of a relational data influence model
	3. Substantial performance gains on downstream tasks with reduced token usage.

**Result:** Group-MATES achieves 3.5%-9.4% relative performance gains over random selection on 22 downstream tasks and reduces tokens needed for performance by up to 1.75x.

**Limitations:** 

**Conclusion:** The relational data influence model and cluster-based inference significantly enhance data selection efficiency in language model training.

**Abstract:** In this paper, we introduce Group-MATES, an efficient group-level data selection approach to optimize the speed-quality frontier of language model pretraining. Specifically, Group-MATES parameterizes costly group-level selection with a relational data influence model. To train this model, we sample training trajectories of the language model and collect oracle data influences alongside. The relational data influence model approximates the oracle data influence by weighting individual influence with relationships among training data. To enable efficient selection with our relational data influence model, we partition the dataset into small clusters using relationship weights and select data within each cluster independently. Experiments on DCLM 400M-4x, 1B-1x, and 3B-1x show that Group-MATES achieves 3.5%-9.4% relative performance gains over random selection across 22 downstream tasks, nearly doubling the improvements achieved by state-of-the-art individual data selection baselines. Furthermore, Group-MATES reduces the number of tokens required to reach a certain downstream performance by up to 1.75x, substantially elevating the speed-quality frontier. Further analyses highlight the critical role of relationship weights in the relational data influence model and the effectiveness of our cluster-based inference. Our code is open-sourced at https://github.com/facebookresearch/Group-MATES.

</details>


### [137] [From RAG to Memory: Non-Parametric Continual Learning for Large Language Models](https://arxiv.org/abs/2502.14802)

*Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, long-term memory, language models

**Relevance Score:** 9

**TL;DR:** The paper proposes HippoRAG 2, an advanced retrieval-augmented generation framework that improves factual, sense-making, and associative memory tasks in language models compared to standard RAG.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the ability of AI systems to approximate human-like long-term memory through improved continual learning methods.

**Method:** HippoRAG 2 enhances the Personalized PageRank algorithm with deeper passage integration and more effective online use of LLMs to facilitate better memory performance.

**Key Contributions:**

	1. Introduction of HippoRAG 2 framework for improved memory tasks
	2. 7% improvement in associative memory performance
	3. Enhancement of the Learner's performance through deeper passage integration

**Result:** HippoRAG 2 demonstrates a 7% improvement in associative memory tasks and outperforms standard RAG in factual knowledge and sense-making capabilities.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances non-parametric continual learning in LLMs, bridging the gap towards human long-term memory effectiveness.

**Abstract:** Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.

</details>


### [138] [Batayan: A Filipino NLP benchmark for evaluating Large Language Models](https://arxiv.org/abs/2502.14911)

*Jann Railey Montalan, Jimson Paulo Layacan, David Demitri Africa, Richell Isaiah Flores, Michael T. Lopez II, Theresa Denise Magsajo, Anjanette Cayabyab, William Chandra Tjhi*

**Main category:** cs.CL

**Keywords:** Filipino NLP, large language models, benchmarking, dataset construction, cultural representation

**Relevance Score:** 8

**TL;DR:** This paper introduces Batayan, a benchmark for evaluating large language models on Filipino language processing tasks, emphasizing the challenges and needs for better language resources.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to explore and evaluate large language models (LLMs) for under-resourced languages, specifically Filipino, due to significant performance gaps and lack of representation in existing datasets.

**Method:** Batayan encompasses eight NLP tasks focused on understanding, reasoning, and generation in both Tagalog and Taglish, validated through a native-speaker-driven process.

**Key Contributions:**

	1. Introduction of a new benchmark for Filipino NLP called Batayan
	2. Identification of performance gaps in LLMs for Filipino languages
	3. Proposed solutions for dataset construction challenges in under-represented languages

**Result:** Empirical results reveal major performance gaps in LLMs when processing Filipino languages, highlighting the inadequacy of current pre-training corpora and the need for explicit support for Filipino language processing.

**Limitations:** Potential limitations in generalizability to other under-resourced languages and reliance on native-speaker validation.

**Conclusion:** The paper emphasizes the importance of creating culturally and linguistically-faithful resources for under-represented languages and provides an evaluation suite for community-driven progress in Filipino NLP.

**Abstract:** Recent advances in large language models (LLMs) have demonstrated remarkable capabilities on widely benchmarked high-resource languages. However, linguistic nuances of under-resourced languages remain unexplored. We introduce Batayan, a holistic Filipino benchmark that systematically evaluates LLMs across three key natural language processing (NLP) competencies: understanding, reasoning, and generation. Batayan consolidates eight tasks, three of which have not existed prior for Filipino corpora, covering both Tagalog and code-switched Taglish utterances. Our rigorous, native-speaker-driven adaptation and validation processes ensures fluency and authenticity to the complex morphological and syntactic structures of Filipino, alleviating the pervasive translationese bias in existing Filipino corpora. We report empirical results on a variety of open-source and commercial LLMs, highlighting significant performance gaps that signal the under-representation of Filipino in pre-training corpora, the unique hurdles in modeling Filipino's rich morphology and construction, and the importance of explicit Filipino language support. Moreover, we discuss the practical challenges encountered in dataset construction and propose principled solutions for building culturally and linguistically-faithful resources in under-represented languages. We also provide a public evaluation suite as a clear foundation for iterative, community-driven progress in Filipino NLP.

</details>


### [139] [Uncertainty Quantification in Retrieval Augmented Question Answering](https://arxiv.org/abs/2502.18108)

*Laura Perez-Beltrachini, Mirella Lapata*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented QA, Passage Utility, Neural Model, Information Theory, QA Performance

**Relevance Score:** 8

**TL;DR:** This paper quantifies the utility of passages in retrieval-augmented Question Answering, proposing a lightweight model to predict passage effectiveness which outperforms costly methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of retrieved passages in retrieval-augmented QA systems and improve answer correctness by predicting passage utility.

**Method:** Development of a lightweight neural model to predict the utility of passages supplied to a QA model, comparing its performance with simple information-theoretic metrics and sampling-based methods.

**Key Contributions:**

	1. Introduces a lightweight neural model to predict passage utility for QA systems.
	2. Demonstrates the effectiveness of the approach in comparison to existing methods.
	3. Gives access to code and data for further research.

**Result:** The proposed model efficiently predicts passage utility and shows improved performance in predicting answer correctness compared to traditional methods.

**Limitations:** 

**Conclusion:** The study concludes that predicting passage utility can enhance the reliability of QA models and provides a lightweight alternative to complex sampling methods.

**Abstract:** Retrieval augmented Question Answering (QA) helps QA models overcome knowledge gaps by incorporating retrieved evidence, typically a set of passages, alongside the question at test time. Previous studies show that this approach improves QA performance and reduces hallucinations, without, however, assessing whether the retrieved passages are indeed useful at answering correctly. In this work, we propose to quantify the uncertainty of a QA model via estimating the utility of the passages it is provided with. We train a lightweight neural model to predict passage utility for a target QA model and show that while simple information theoretic metrics can predict answer correctness up to a certain extent, our approach efficiently approximates or outperforms more expensive sampling-based methods. Code and data are available at https://github.com/lauhaide/ragu.

</details>


### [140] [olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models](https://arxiv.org/abs/2502.18443)

*Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, Luca Soldaini*

**Main category:** cs.CL

**Keywords:** PDF Processing, Vision Language Models, Open-source Toolkit

**Relevance Score:** 8

**TL;DR:** olmOCR is an open-source toolkit designed to convert diverse PDF documents into high-quality, linearized text while maintaining structured content.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** PDF documents hold a wealth of quality tokens for training language models but pose challenges due to their varied formats and layouts.

**Method:** The toolkit uses a fine-tuned 7B vision language model and is optimized for large-scale batch processing, enabling efficient conversion of PDFs at low cost.

**Key Contributions:**

	1. Introduction of olmOCR for high-quality PDF text extraction
	2. Development of olmOCR-Bench for benchmarking PDF processing tools
	3. Cost-effective solution for processing large sets of PDF documents

**Result:** olmOCR outperforms leading VLMs in document processing accuracy and efficiency, costing only $176 to process one million pages.

**Limitations:** 

**Conclusion:** All components of olmOCR, including the model, training code, data, and benchmarks, are publicly available to facilitate further research and development.

**Abstract:** PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. Traditional open source tools often produce lower quality extractions compared to vision language models (VLMs), but reliance on the best VLMs can be prohibitively costly (e.g., over $6,240 USD per million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to proprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on olmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and can convert a million PDF pages for only $176 USD. To aid comparison with existing systems, we also introduce olmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that remain challenging even for the best tools and VLMs, including formulas, tables, tiny fonts, old scans, and more. We find olmOCR outperforms even top VLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all components of olmOCR: our fine-tuned VLM model, training code and data, an efficient inference pipeline that supports vLLM and SGLang backends, and benchmark olmOCR-Bench.

</details>


### [141] [FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response](https://arxiv.org/abs/2502.18452)

*Mollie Shichman, Claire Bonial, Austin Blodgett, Taylor Hudson, Francis Ferraro, Rachel Rudinger*

**Main category:** cs.CL

**Keywords:** Human-Robot Interaction, Large Language Models, Disaster Relief, Few-Shot Learning, Synthetic Data

**Relevance Score:** 8

**TL;DR:** The paper introduces the FRIDA pipeline for enhancing small LLMs' physical reasoning in disaster relief by using few-shot prompting and synthetic data generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective physical reasoning in small models for disaster relief robots, which cannot deploy larger LLMs.

**Method:** The authors developed the FRIDA pipeline to create high-quality few-shot prompts for generating synthetic data, refined through domain expert collaboration, and conducted an ablation study on the models.

**Key Contributions:**

	1. Development of the FRIDA dataset and pipeline for disaster scenarios
	2. Effective few-shot prompting techniques for synthetic data generation
	3. Ablation study results indicating the importance of physical state and function data in training

**Result:** Ablated FRIDA models trained on specific object data outperformed both fully trained FRIDA models and base models in evaluation metrics.

**Limitations:** 

**Conclusion:** The FRIDA pipeline effectively improves physical reasoning capabilities in small LLMs with minimal high-quality data.

**Abstract:** During Human Robot Interactions in disaster relief scenarios, Large Language Models (LLMs) have the potential for substantial physical reasoning to assist in mission objectives. However, these capabilities are often found only in larger models, which are frequently not reasonable to deploy on robotic systems. To meet our problem space requirements, we introduce a dataset and pipeline to create Field Reasoning and Instruction Decoding Agent (FRIDA) models. In our pipeline, domain experts and linguists combine their knowledge to make high-quality few-shot prompts used to generate synthetic data for fine-tuning. We hand-curate datasets for this few-shot prompting and for evaluation to improve LLM reasoning on both general and disaster-specific objects. We concurrently run an ablation study to understand which kinds of synthetic data most affect performance. We fine-tune several small instruction-tuned models and find that ablated FRIDA models only trained on objects' physical state and function data outperformed both the FRIDA models trained on all synthetic data and the base models in our customized evaluation. We demonstrate that the FRIDA pipeline is capable of instilling physical common sense with minimal data.

</details>


### [142] [Large-Scale Data Selection for Instruction Tuning](https://arxiv.org/abs/2503.01807)

*Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, Pradeep Dasigi*

**Main category:** cs.CL

**Keywords:** data selection, instruction-tuning, language models, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of automated data selection methods for instruction-tuning language models, highlighting that many recent approaches do not outperform random selection, particularly when scaling to larger datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study seeks to address the importance of selecting high-quality training data when instruction-tuning language models, as it can greatly enhance model performance compared to using noisy datasets.

**Method:** The authors systematically assess data selection methods by selecting up to 2.5M samples from large pools (up to 5.8M), comparing their performance across seven diverse tasks.

**Key Contributions:**

	1. Systematic evaluation of data selection methods at scale.
	2. Demonstration that many recent methods underperform compared to random selection when scaling.
	3. Introduction of RDS+, a more effective and compute-efficient data selection approach.

**Result:** It was found that many recent data selection methods perform worse than random selection and even decline in effectiveness when using larger pools, while a variant of representation-based data selection consistently outperformed complex methods.

**Limitations:** The study mainly focuses on the scaling properties and may not cover smaller datasets or less complex selection methods.

**Conclusion:** The findings suggest that the scaling properties of automated data selection methods need closer examination, and the authors provide their code, data, and models for further exploration.

**Abstract:** Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.

</details>


### [143] [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832)

*Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu*

**Main category:** cs.CL

**Keywords:** LLM alignment, token-level rewards, reinforcement learning from human feedback, direct preference optimization, fast convergence

**Relevance Score:** 9

**TL;DR:** AlignDistil proposes a novel method for LLM alignment through token-level reward optimization, improving performance and convergence speed compared to existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for LLM alignment use sparse, response-level rewards, which can lead to suboptimal performance by incorrectly optimizing high- or low-quality tokens.

**Method:** The paper introduces AlignDistil, which integrates token-level rewards learned from direct preference optimization (DPO) into the reinforcement learning from human feedback (RLHF) framework, establishing a theoretically proved equivalence between this objective and a token-level distillation process.

**Key Contributions:**

	1. Introduction of token-level reward optimization into LLM alignment through AlignDistil.
	2. Theoretical proof of the equivalence between RLHF and token-level distillation processes.
	3. Design of a token adaptive logit extrapolation mechanism for effective teacher distribution.

**Result:** Experimental results show that AlignDistil outperforms existing alignment methods and achieves faster convergence due to its token-level distributional reward optimization.

**Limitations:** 

**Conclusion:** The proposed method enhances LLM alignment by optimizing rewards at the token level, thus improving overall model performance.

**Abstract:** In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization.

</details>


### [144] [Coreference as an indicator of context scope in multimodal narrative](https://arxiv.org/abs/2503.05298)

*Nikolai Ilinykh, Shalom Lappin, Asad Sayeed, Sharid Loáiciga*

**Main category:** cs.CL

**Keywords:** coreference, multimodal, visual storytelling, language models, machine learning

**Relevance Score:** 8

**TL;DR:** Large multimodal language models exhibit significant differences from humans in their use of coreferential expressions during visual storytelling tasks.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze and quantify the differences in how humans and machines use coreferential expressions in multimodal contexts, particularly in visual storytelling.

**Method:** The study introduced several metrics to evaluate coreferential patterns and compared those patterns in human and machine-generated texts across various visual contexts.

**Key Contributions:**

	1. Introduction of metrics for evaluating coreferential expressions in visual storytelling
	2. Demonstration of substantial differences between human and machine text generation
	3. Availability of study materials and code for reproducibility

**Result:** It was found that humans are able to distribute coreferential expressions more effectively and consistently than machines, who struggle with tracking mixed references despite improvements in overall generation quality.

**Limitations:** 

**Conclusion:** The findings highlight the limitations of current large multimodal language models in handling visual storytelling and suggest areas for improvement in future AI systems.

**Abstract:** We demonstrate that large multimodal language models differ substantially from humans in the distribution of coreferential expressions in a visual storytelling task. We introduce a number of metrics to quantify the characteristics of coreferential patterns in both human- and machine-written texts. Humans distribute coreferential expressions in a way that maintains consistency across texts and images, interleaving references to different entities in a highly varied way. Machines are less able to track mixed references, despite achieving perceived improvements in generation quality. Materials, metrics, and code for our study are available at https://github.com/GU-CLASP/coreference-context-scope.

</details>


### [145] [Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models](https://arxiv.org/abs/2503.05328)

*Anar Yeginbergen, Maite Oronoz, Rodrigo Agerri*

**Main category:** cs.CL

**Keywords:** Large Language Models, counter-argument generation, dynamic external knowledge integration, argumentative complexity, evaluation methodology

**Relevance Score:** 9

**TL;DR:** This paper explores integrating dynamic external knowledge into Large Language Models (LLMs) to enhance counter-argument generation, presenting a new dataset and evaluation methodology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in generating factual and controlled argumentative responses and to improve counter-argument generation.

**Method:** The authors introduce a manually curated dataset of argument and counter-argument pairs and propose a LLM-as-a-Judge evaluation methodology to better align with human judgments.

**Key Contributions:**

	1. Introduction of a custom dataset for evaluating argumentative complexity.
	2. Development of a LLM-as-a-Judge evaluation methodology that aligns closely with human judgments.
	3. Demonstrated improvement in counter-argument quality through external knowledge integration.

**Result:** Experimental results show that dynamic integration of external knowledge significantly enhances the quality of generated counter-arguments, improving relatedness, persuasiveness, and factuality.

**Limitations:** The results may vary based on the quality of external knowledge and the limitations of the LLMs used.

**Conclusion:** The study concludes that combining LLMs with external knowledge retrieval can lead to the development of more effective counter-argumentation systems.

**Abstract:** This paper investigates the role of dynamic external knowledge integration in improving counter-argument generation using Large Language Models (LLMs). While LLMs have shown promise in argumentative tasks, their tendency to generate lengthy, potentially unfactual responses highlights the need for more controlled and evidence-based approaches. We introduce a new manually curated dataset of argument and counter-argument pairs specifically designed to balance argumentative complexity with evaluative feasibility. We also propose a new LLM-as-a-Judge evaluation methodology that shows a stronger correlation with human judgments compared to traditional reference-based metrics. Our experimental results demonstrate that integrating dynamic external knowledge from the web significantly improves the quality of generated counter-arguments, particularly in terms of relatedness, persuasiveness, and factuality. The findings suggest that combining LLMs with real-time external knowledge retrieval offers a promising direction for developing more effective and reliable counter-argumentation systems.

</details>


### [146] [QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation](https://arxiv.org/abs/2503.05888)

*Bang Nguyen, Tingting Du, Mengxia Yu, Lawrence Angrave, Meng Jiang*

**Main category:** cs.CL

**Keywords:** Question Generation, Educational Assessment, Large Language Models, Test Item Analysis, Human Evaluation

**Relevance Score:** 6

**TL;DR:** The paper introduces a new evaluation framework for Question Generation (QG) that utilizes test item analysis to better assess test question quality in educational assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The existing evaluation methods for Question Generation (QG) lack a direct connection to educational values and do not adequately assess test question quality.

**Method:** The authors developed pairs of candidate questions with varying quality dimensions and assessed existing evaluation approaches against these.

**Key Contributions:**

	1. Introduction of test item analysis to QG evaluation
	2. Development of the QG-SMS framework leveraging LLM for assessment
	3. Empirical evidence showing improved evaluation accuracy with simulated student profiles.

**Result:** The study identified significant limitations in current QG evaluation methods in measuring test item quality relative to student performance, leading to the development of QG-SMS framework.

**Limitations:** Current methods do not directly link to student performance; may require further validation in diverse educational contexts.

**Conclusion:** The QG-SMS framework, which uses Large Language Models to simulate student profiles, provides a more robust assessment of test items and addresses the shortcomings of current evaluation techniques.

**Abstract:** While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.

</details>


### [147] [LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions](https://arxiv.org/abs/2503.10486)

*Gaurav Kumar Gupta, Pranal Pande, Nirajan Acharya, Aniket Kumar Singh, Suman Niroula*

**Main category:** cs.CL

**Keywords:** Large Language Models, diagnostics, health informatics, machine learning, clinical decision-making

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of two LLM-based diagnostic tools, DeepSeek R1 and O3 Mini, in medical diagnostics using a structured dataset.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to assess how LLMs can enhance disease classification and clinical decision-making in the medical field.

**Method:** Performance evaluation of DeepSeek R1 and O3 Mini on a dataset of symptoms and diagnoses, measuring predictive accuracy and confidence scores.

**Key Contributions:**

	1. Insights into LLM performance in medical diagnostics
	2. Comparison of two LLM-based systems
	3. Ethical considerations in AI-driven healthcare

**Result:** DeepSeek R1 achieved 76% disease-level accuracy and 82% overall accuracy, outperforming O3 Mini's 72% and 75% accuracy; both models struggled with Respiratory Disease classification.

**Limitations:** Struggles with Respiratory Disease classification and ethical concerns such as bias and data privacy.

**Conclusion:** The findings highlight the effectiveness of LLMs in certain medical areas while underscoring challenges, such as classification accuracy in Respiratory Diseases and ethical concerns.

**Abstract:** Large Language Models (LLMs) are revolutionizing medical diagnostics by enhancing both disease classification and clinical decision-making. In this study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We assessed their predictive accuracy at both the disease and category levels, as well as the reliability of their confidence scores. DeepSeek R1 achieved a disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3 Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1 demonstrated exceptional performance in Mental Health, Neurological Disorders, and Oncology, where it reached 100% accuracy, while O3 Mini excelled in Autoimmune Disease classification with 100% accuracy. Both models, however, struggled with Respiratory Disease classification, recording accuracies of only 40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of confidence scores revealed that DeepSeek R1 provided high-confidence predictions in 92% of cases, compared to 68% for O3 Mini. Ethical considerations regarding bias, model interpretability, and data privacy are also discussed to ensure the responsible integration of LLMs into clinical practice. Overall, our findings offer valuable insights into the strengths and limitations of LLM-based diagnostic systems and provide a roadmap for future enhancements in AI-driven healthcare.

</details>


### [148] [TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models](https://arxiv.org/abs/2504.07385)

*Sher Badshah, Ali Emami, Hassan Sajjad*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation framework, dynamic assessment, free-form question-answering, autonomous applications

**Relevance Score:** 9

**TL;DR:** Tool-Augmented LLM Evaluation (TALE) framework improves evaluation of LLM outputs without static references by using an agent to retrieve and synthesize information dynamically.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods for evaluating LLM outputs rely on static references, which face challenges in cost, scalability, and completeness, particularly in autonomous applications.

**Method:** TALE uses an agent with tool-access capabilities that generates web queries, collects information, summarizes findings, and refines searches through iterative reflection, effectively assessing LLM outputs.

**Key Contributions:**

	1. Introduction of the TALE framework for evaluating LLM outputs without predetermined ground-truth answers.
	2. Utilization of an agent that synthesizes external evidence through iterative queries and reflections.
	3. Demonstration of improved evaluation accuracy compared to standard metrics based on static references.

**Result:** Experimental results reveal that TALE outperforms standard reference-based metrics for accuracy and aligns closely with human evaluations across multiple free-form QA benchmarks.

**Limitations:** 

**Conclusion:** By moving away from static references, TALE provides a more reliable and effective means of evaluating LLM outputs in dynamic, real-world scenarios.

**Abstract:** As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.

</details>


### [149] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)

*Mikhail Salnikov, Dmitrii Korzh, Ivan Lazichny, Elvir Karimov, Artyom Iudin, Ivan Oseledets, Oleg Y. Rogov, Natalia Loukachevitch, Alexander Panchenko, Elena Tutubalina*

**Main category:** cs.CL

**Keywords:** geopolitical bias, LLMs, national narratives, debiasing, historical interpretation

**Relevance Score:** 8

**TL;DR:** The paper analyzes geopolitical biases in LLMs regarding historical interpretations from various countries, revealing significant biases favoring national narratives and limited effectiveness of debiasing prompts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and evaluate the influence of geopolitical biases in large language models (LLMs) on their interpretations of historical events from conflicting national perspectives.

**Method:** A novel dataset was introduced comprising neutral event descriptions alongside contrasting viewpoints from the USA, UK, USSR, and China. The study conducted experiments involving manipulated participant labels to assess the models' sensitivity to attribution.

**Key Contributions:**

	1. Introduction of a novel dataset for evaluating geopolitical biases in LLMs
	2. Demonstration of significant biases favoring specific national narratives
	3. Challenge to the effectiveness of simple debiasing techniques

**Result:** The findings indicate substantial geopolitical biases in LLMs, with a tendency to favor certain national narratives. Moreover, simple debiasing prompts proved largely ineffective in mitigating these biases.

**Limitations:** The effectiveness of debiasing methods was limited, and further research is needed to explore alternative approaches.

**Conclusion:** The research emphasizes the existence of national narrative biases within LLMs and questions the utility of straightforward debiasing methods, while providing a framework and dataset for future investigations into geopolitical bias.

**Abstract:** This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.

</details>


### [150] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)

*Wenxuan Xie, Yaxun Dai, Wenhao Jiang*

**Main category:** cs.CL

**Keywords:** large language models, Text-to-SQL, self-driven exploration, SQL probes, zero-shot setting

**Relevance Score:** 9

**TL;DR:** SDE-SQL is a framework that enables large language models to explore databases autonomously by generating and executing SQL probes during inference.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Dynamic interaction with database contents to improve the Text-to-SQL task performance and overcome limitations of prior static approaches.

**Method:** The proposed framework allows LLMs to autonomously retrieve information from databases by generating and executing SQL probes in a zero-shot setting.

**Key Contributions:**

	1. Introduction of SDE-SQL framework for self-driven database exploration
	2. Achieves state-of-the-art performance in Text-to-SQL tasks
	3. Demonstrates improvements over existing models without requiring in-context demonstrations

**Result:** SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the baseline on the BIRD benchmark, setting a new state-of-the-art performance without supervised fine-tuning.

**Limitations:** 

**Conclusion:** SDE-SQL enhances LLM capabilities in understanding and interacting with databases, and further improves with supervised fine-tuning.

**Abstract:** Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.

</details>
