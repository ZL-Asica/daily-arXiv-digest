# 2025-06-18

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 24]

- [cs.CL](#cs.CL) [Total: 92]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Toward Practical Privacy in XR: Empirical Analysis of Multimodal Anonymization Mechanisms](https://arxiv.org/abs/2506.13882)

*Azim Ibragimov, Ethan Wilson, Kevin R. B. Butler, Eakta Jain*

**Main category:** cs.HC

**Keywords:** Extended Reality, Privacy Mechanisms, Human-Computer Interaction, Behavioral Data, Re-identification

**Relevance Score:** 7

**TL;DR:** This paper evaluates real-time privacy mechanisms for XR systems, demonstrating that multimodal approaches significantly reduce re-identification rates while maintaining usability.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficacy of unimodal privacy mechanisms in protecting against re-identification in XR systems that collect behavioral signals.

**Method:** The study systematically evaluates individual and paired privacy mechanisms for eye and body telemetry across various datasets involving 407 participants.

**Key Contributions:**

	1. Systematic evaluation of real-time privacy mechanisms for XR.
	2. Demonstration of the effectiveness of multimodal privacy strategies.
	3. Establishment of empirically grounded thresholds for usability in privacy mechanisms.

**Result:** Multimodal privacy mechanisms reduced re-identification rates from 80.3% to 26.3% in casual and from 84.8% to 26.1% in competitive XR applications without compromising usability.

**Limitations:** Focused primarily on eye and body telemetry without exploring other potential modalities for privacy.

**Conclusion:** The research highlights the effectiveness of context-aware and modality-specific privacy strategies in enhancing behavioral data protection in XR environments.

**Abstract:** As extended reality (XR) systems become increasingly immersive and sensor-rich, they enable the collection of fine-grained behavioral signals such as eye and body telemetry. These signals support personalized and responsive experiences and may also contain unique patterns that can be linked back to individuals. However, privacy mechanisms that naively pair unimodal mechanisms (e.g., independently apply privacy mechanisms for eye and body privatization) are often ineffective at preventing re-identification in practice. In this work, we systematically evaluate real-time privacy mechanisms for XR, both individually and in pair, across eye and body modalities. To preserve usability, all mechanisms were tuned based on empirically grounded thresholds for real-time interaction. We evaluated four eye and ten body mechanisms across multiple datasets, comprising up to 407 participants. Our results show that while obfuscating eye telemetry alone offers moderate privacy gains, body telemetry perturbation is substantially more effective. When carefully paired, multimodal mechanisms reduce re-identification rate from 80.3% to 26.3% in casual XR applications (e.g., VRChat and Job Simulator) and from 84.8% to 26.1% in competitive XR applications (e.g., Beat Saber and Synth Riders), all without violating real-time usability requirements. These findings underscore the potential of modality-specific and context-aware privacy strategies for protecting behavioral data in XR environments.

</details>


### [2] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)

*Ivania Donoso-Guzmán, Kristýna Sirka Kacafírková, Maxwell Szymanski, An Jacobs, Denis Parra, Katrien Verbert*

**Main category:** cs.HC

**Keywords:** Explainable AI, XAI, healthcare, user experience, evaluation strategies

**Relevance Score:** 9

**TL;DR:** This study provides a framework for evaluating Explainable AI in healthcare, focusing on user experience and context-sensitive guidelines for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of robust evaluation methods for Explainable Artificial Intelligence (XAI) in real-world healthcare settings.

**Method:** A systematic review of 82 user studies from healthcare contexts was conducted, using a predefined coding scheme and inductive codes to analyze current evaluation practices.

**Key Contributions:**

	1. Synthesis of current evaluation practices in healthcare XAI
	2. Insights into interrelations among explanation properties
	3. Updated framework and actionable evaluation guidelines for XAI

**Result:** The review reveals a shift towards human-centered evaluation approaches, highlights interrelations among explanation properties, and proposes an updated framework with actionable guidelines for XAI evaluation.

**Limitations:** 

**Conclusion:** A comprehensive framework and guidelines to enhance the trustworthiness and usability of XAI in healthcare, enabling better evaluation practices.

**Abstract:** Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.   This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.   We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.   The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.

</details>


### [3] ["I Cannot Write This Because It Violates Our Content Policy": Understanding Content Moderation Policies and User Experiences in Generative AI Products](https://arxiv.org/abs/2506.14018)

*Lan Gao, Oscar Chen, Rachel Lee, Nick Feamster, Chenhao Tan, Marshini Chetty*

**Main category:** cs.HC

**Keywords:** Content Moderation, Generative AI, User Experience, Online Tools, Moderation Policies

**Relevance Score:** 8

**TL;DR:** This paper investigates content moderation in generative AI tools, analyzing policies and user experiences, ultimately suggesting improvements for better user experience and policy detail.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effectiveness of content moderation policies in generative AI applications and their impact on end-users.

**Method:** Analyzed content moderation policies of 14 generative AI online tools and examined user experiences through discussions on Reddit.

**Key Contributions:**

	1. Analysis of moderation policies in GAI tools
	2. Insights from user experiences on moderation failures
	3. Recommendations for improving content moderation and user support

**Result:** Identified comprehensive yet vague moderation policies and user frustrations with failures of moderation systems and support.

**Limitations:** Focuses on consumer-facing GAI applications; results may not generalize to all generative AI contexts.

**Conclusion:** Improvements are needed in content moderation policy detail and user experience for GAI tools.

**Abstract:** While recent research has focused on developing safeguards for generative AI (GAI) model-level content safety, little is known about how content moderation to prevent malicious content performs for end-users in real-world GAI products. To bridge this gap, we investigated content moderation policies and their enforcement in GAI online tools -- consumer-facing web-based GAI applications. We first analyzed content moderation policies of 14 GAI online tools. While these policies are comprehensive in outlining moderation practices, they usually lack details on practical implementations and are not specific about how users can aid in moderation or appeal moderation decisions. Next, we examined user-experienced content moderation successes and failures through Reddit discussions on GAI online tools. We found that although moderation systems succeeded in blocking malicious generations pervasively, users frequently experienced frustration in failures of both moderation systems and user support after moderation. Based on these findings, we suggest improvements for content moderation policy and user experiences in real-world GAI products.

</details>


### [4] [FEWSim: A Visual Analytic Framework for Exploring the Nexus of Food-Energy-Water Simulations](https://arxiv.org/abs/2506.14056)

*Fan Lei, David A. Sampson, Jiayi Hong, Yuxin Ma, Giuseppe Mascaro, Dave White, Rimjhim Agarwal, Ross Maciejewski*

**Main category:** cs.HC

**Keywords:** food energy water nexus, visual analytics, simulation results, sustainability indices, interdependencies

**Relevance Score:** 3

**TL;DR:** FEWSim is a visual analytics framework that aids in exploring food, energy, and water (FEW) systems through simulation results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to analyze the interdependencies and vulnerabilities within food, energy, and water systems due to the challenges in quantifying nexus interactions.

**Method:** FEWSim employs a three-layer asynchronous architecture consisting of a model layer for simulation, a middleware layer for scenario management, and a visualization layer for interactive data exploration.

**Key Contributions:**

	1. Introduction of a visual analytics framework for FEW systems
	2. Three-layer architecture for enhanced simulation exploration
	3. Practical application demonstrated in a real-world case study

**Result:** The framework allows experts to visualize and compare simulation results, facilitating an understanding of FEW nexus dynamics across scenarios using sustainability indices.

**Limitations:** 

**Conclusion:** FEWSim effectively demonstrates its utility in a case study, enhancing the capacity for cross-sector analysis in FEW systems.

**Abstract:** The interdependencies of food, energy, and water (FEW) systems create a nexus opportunity to explore the strengths and vulnerabilities of individual and cross-sector interactions within FEW systems. However, the variables quantifying nexus interactions are hard to observe, which hinders the cross-sector analysis. To overcome such challenges, we present FEWSim, a visual analytics framework designed to support domain experts in exploring and interpreting simulation results from a coupled FEW model. FEWSim employs a three-layer asynchronous architecture: the model layer integrates food, energy, and water models to simulate the FEW nexus; the middleware layer manages scenario configuration and execution; and the visualization layer provides interactive visual exploration of simulated time-series results across FEW sectors. The visualization layer further facilitates the exploration across multiple scenarios and evaluates scenario differences in performance using sustainability indices of the FEW nexus. We demonstrate the utility of FEWSim through a case study for the Phoenix Active Management Area (AMA) in Arizona.

</details>


### [5] [The Teacher's Dilemma: Balancing Trade-Offs in Programming Education for Emergent Bilingual Students](https://arxiv.org/abs/2506.14147)

*Emma R. Dodoo, Tamara Nelson-Fromm, Mark Guzdial*

**Main category:** cs.HC

**Keywords:** K-12 education, emergent bilinguals, programming languages, teacher decision-making, computer science education

**Relevance Score:** 3

**TL;DR:** The paper explores the challenges faced by K-12 computing teachers in selecting programming languages and materials for classrooms with emergent bilingual students, focusing on the trade-offs between inclusivity and curriculum alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how K-12 computing teachers balance the need for an inclusive learning environment with curriculum and industry demands when teaching emergent bilingual students.

**Method:** Qualitative analysis of teacher decisions and the factors influencing their selection of programming languages and materials.

**Key Contributions:**

	1. In-depth insights into the decision-making process of teachers for emergent bilingual students.
	2. Identification of the key tensions faced by educators in balancing accessibility and curriculum requirements.
	3. Recommendations for future programming language selections that support diverse student needs.

**Result:** Identified key tensions between accessibility, curriculum alignment, and workforce preparation that impact teachers' choices in programming tools.

**Limitations:** The study primarily focuses on K-12 settings and may not generalize to higher education contexts or other subjects.

**Conclusion:** Teachers must navigate complex trade-offs and make decisions that support equity while also adhering to curricular guidelines and industry expectations.

**Abstract:** K-12 computing teachers must navigate complex trade-offs when selecting programming languages and instructional materials for classrooms with emergent bilingual students. While they aim to foster an inclusive learning environment by addressing language barriers that impact student engagement, they must also align with K-12 computer science curricular guidelines and prepare students for industry-standard programming tools. Because programming languages predominantly use English keywords and most instructional materials are written in English, these linguistic barriers introduce cognitive load and accessibility challenges. This paper examines teachers' decisions in balancing these competing priorities, highlighting the tensions between accessibility, curriculum alignment, and workforce preparation. The findings shed light on how our teacher participants negotiate these trade-offs and what factors influence their selection of programming tools to best support EB students while meeting broader educational and professional goals.

</details>


### [6] [StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework](https://arxiv.org/abs/2506.14159)

*Shayan Talaei, Meijin Li, Kanu Grover, James Kent Hippler, Diyi Yang, Amin Saberi*

**Main category:** cs.HC

**Keywords:** autobiography writing, human-AI collaboration, multi-agent systems

**Relevance Score:** 6

**TL;DR:** StorySage is a multi-agent system that facilitates autobiography writing by capturing user memories through structured conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Individuals struggle to organize their scattered memories into coherent autobiographies using existing conversational writing assistants.

**Method:** StorySage uses a multi-agent framework consisting of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator to collect and structure user memories iteratively.

**Key Contributions:**

	1. Introduction of a multi-agent framework for autobiography writing
	2. Improved conversational flow and narrative completeness
	3. User satisfaction enhancement in memoir writing support

**Result:** StorySage shows improved conversational flow, narrative completeness, and user satisfaction in experimental simulations compared to a baseline.

**Limitations:** 

**Conclusion:** StorySage provides a novel architecture for autobiography writing and demonstrates the benefits of multi-agent systems in enhancing human-AI creative partnerships.

**Abstract:** Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.

</details>


### [7] [Affective-CARA: A Knowledge Graph Driven Framework for Culturally Adaptive Emotional Intelligence in HCI](https://arxiv.org/abs/2506.14166)

*Nirodya Pussadeniya, Bahareh Nakisa, Mohmmad Naim Rastgoo*

**Main category:** cs.HC

**Keywords:** Affective Computing, Cultural Adaptation, Emotion Knowledge Graph, Reinforcement Learning, User-Agent Interaction

**Relevance Score:** 8

**TL;DR:** Affective-CARA is a culturally adaptive emotional response framework that enhances user-agent interactions by integrating cultural knowledge with reinforcement learning, resulting in improved sentiment alignment and reduced bias in emotional responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of generating culturally adaptive emotional responses in affective computing, which is vital for empathetic user-agent interactions.

**Method:** The framework integrates a Cultural Emotion Knowledge Graph with annotations for Valence, Arousal, and Dominance, and employs a Gradient-Based Reward Policy Optimization for refining responses based on cultural alignment and user feedback.

**Key Contributions:**

	1. Introduction of Affective-CARA framework for cultural emotional responses.
	2. Integration of a Cultural Emotion Knowledge Graph for user-agent interaction.
	3. Performance evaluation showing significant improvements in sentiment alignment and cultural adaptation.

**Result:** Affective-CARA outperforms baseline models in sentiment alignment, cultural adaptation, and narrative quality, achieving a Cultural Semantic Density of 9.32 and reducing cultural representation bias by 61%.

**Limitations:** 

**Conclusion:** The findings highlight the potential of Affective-CARA for creating inclusive and empathetic interactions, with applications in cross-cultural communication, mental health support, and education.

**Abstract:** Culturally adaptive emotional responses remain a critical challenge in affective computing. This paper introduces Affective-CARA, an agentic framework designed to enhance user-agent interactions by integrating a Cultural Emotion Knowledge Graph (derived from StereoKG) with Valence, Arousal, and Dominance annotations, culture-specific data, and cross-cultural checks to minimize bias. A Gradient-Based Reward Policy Optimization mechanism further refines responses according to cultural alignment, affective appropriateness, and iterative user feedback. A Cultural-Aware Response Mediator coordinates knowledge retrieval, reinforcement learning updates, and historical data fusion. By merging real-time user input with past emotional states and cultural insights, Affective-CARA delivers narratives that are deeply personalized and sensitive to diverse cultural norms. Evaluations on AffectNet, SEMAINE DB, and MERD confirm that the framework consistently outperforms baseline models in sentiment alignment, cultural adaptation, and narrative quality. Affective-CARA achieved a Cultural Semantic Density of 9.32 out of 10 and lowered cultural representation bias by 61% (KL-Divergence: 0.28), demonstrating robust performance in generating ethical, adaptive responses. These findings suggest the potential for more inclusive and empathetic interactions, making Affective-CARA an avenue for fostering culturally grounded user experiences across domains such as cross-cultural communication, mental health support, and education.

</details>


### [8] [Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.14196)

*Jiayue Melissa Shi, Keran Wang, Dong Whi Yoo, Ravi Karkar, Koustuv Saha*

**Main category:** cs.HC

**Keywords:** Alzheimer's Disease, caregivers, mental wellbeing, technologies, interventions

**Relevance Score:** 8

**TL;DR:** The paper explores the mental wellbeing challenges faced by family caregivers of individuals with Alzheimer's Disease and Related Dementias, highlighting their adaptive practices and the role of technology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked mental health needs of family caregivers in the context of Alzheimer's Disease and Related Dementias, as existing support systems are inadequate.

**Method:** The study conducted semi-structured interviews with 25 family caregivers to understand their mental wellbeing concerns and the technologies they use.

**Key Contributions:**

	1. Identification of mental health challenges specific to caregivers of AD/ADRD.
	2. Creation of a temporal mapping of caregivers' mental wellbeing across caregiving stages.
	3. Recommendations for improving existing mental health technologies for caregivers.

**Result:** Key causes and effects of mental health challenges in caregivers were identified, as well as a temporal mapping of their mental wellbeing across three stages of the caregiving journey.

**Limitations:** 

**Conclusion:** The findings suggest the need for personalized, scalable mental health technologies that evolve with caregivers' needs, providing a basis for designing effective, stage-sensitive interventions.

**Abstract:** Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.

</details>


### [9] [The Impact of Generative AI on Social Media: An Experimental Study](https://arxiv.org/abs/2506.14295)

*Anders Giovanni Møller, Daniel M. Romero, David Jurgens, Luca Maria Aiello*

**Main category:** cs.HC

**Keywords:** Generative AI, User Engagement, Social Media, Ethical Design, AI Tools

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of generative AI tools on user behavior and experience in social media through a controlled experiment involving 680 participants.

**Read time:** 48 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the implications of generative AI tools on content producer behaviors and user perceptions in social media contexts.

**Method:** A controlled experiment was conducted with 680 U.S. participants in small discussion groups, each assigned to one of five experimental conditions, including various AI interventions.

**Key Contributions:**

	1. Investigation of user engagement versus perceived quality in AI-assisted social media
	2. Proposed design principles for ethical integration of AI in social media
	3. Empirical evidence from a controlled experiment with a significant sample size

**Result:** The study found that while AI tools increased user engagement and content volume, they also decreased perceived quality and authenticity of discussions, leading to negative spill-over effects.

**Limitations:** The study is limited to a specific sample of U.S. participants and may not generalize to other demographics or cultures.

**Conclusion:** The paper proposes four design principles for integrating generative AI in social media: transparency in AI content disclosure, user-focused personalization, context-sensitivity, and intuitive interfaces.

**Abstract:** Generative Artificial Intelligence (AI) tools are increasingly deployed across social media platforms, yet their implications for user behavior and experience remain understudied, particularly regarding two critical dimensions: (1) how AI tools affect the behaviors of content producers in a social media context, and (2) how content generated with AI assistance is perceived by users. To fill this gap, we conduct a controlled experiment with a representative sample of 680 U.S. participants in a realistic social media environment. The participants are randomly assigned to small discussion groups, each consisting of five individuals in one of five distinct experimental conditions: a control group and four treatment groups, each employing a unique AI intervention-chat assistance, conversation starters, feedback on comment drafts, and reply suggestions. Our findings highlight a complex duality: some AI-tools increase user engagement and volume of generated content, but at the same time decrease the perceived quality and authenticity of discussion, and introduce a negative spill-over effect on conversations. Based on our findings, we propose four design principles and recommendations aimed at social media platforms, policymakers, and stakeholders: ensuring transparent disclosure of AI-generated content, designing tools with user-focused personalization, incorporating context-sensitivity to account for both topic and user intent, and prioritizing intuitive user interfaces. These principles aim to guide an ethical and effective integration of generative AI into social media.

</details>


### [10] [System 0: Transforming Artificial Intelligence into a Cognitive Extension](https://arxiv.org/abs/2506.14376)

*Massimo Chiriatti, Marianna Bergamaschi Ganapini, Enrico Panai, Brenda K. Wiederhold, Giuseppe Riva*

**Main category:** cs.HC

**Keywords:** AI cognitive extension, human-AI integration, cognitive scaffolding

**Relevance Score:** 8

**TL;DR:** This paper presents System 0, a framework for understanding AI as a cognitive extension that transforms human cognition while addressing the challenges of sycophancy and bias amplification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to explore how AI can serve as an active cognitive partner in human thought processes, enhancing cognitive capabilities without compromising critical thinking.

**Method:** The paper builds on the Extended Mind hypothesis and introduces seven evidence-based frameworks for effective human-AI cognitive integration.

**Key Contributions:**

	1. Introduction of System 0 as a conceptual framework for AI's role in cognition
	2. Seven frameworks for effective human-AI cognitive integration
	3. Discussion on the paradox of AI enhancing yet constraining human thought

**Result:** The proposed frameworks include Enhanced Cognitive Scaffolding, Symbiotic Division of Cognitive Labor, Dialectical Cognitive Enhancement, Agentic Transparency and Control, Expertise Democratization, Social-Emotional Augmentation, and Duration-Optimized Integration.

**Limitations:** 

**Conclusion:** These frameworks offer a comprehensive approach to integrate AI into human cognition, promoting agency and intellectual growth rather than replacement of human thought.

**Abstract:** This paper introduces System 0, a conceptual framework for understanding how artificial intelligence functions as a cognitive extension preceding both intuitive (System 1) and deliberative (System 2) thinking processes. As AI systems increasingly shape the informational substrate upon which human cognition operates, they transform from passive tools into active cognitive partners. Building on the Extended Mind hypothesis and Heersmink's criteria for cognitive extension, we argue that AI systems satisfy key conditions for cognitive integration. These include reliability, trust, transparency, individualization, and the ability to enhance and transform human mental functions. However, AI integration creates a paradox: while expanding cognitive capabilities, it may simultaneously constrain thinking through sycophancy and bias amplification. To address these challenges, we propose seven evidence-based frameworks for effective human-AI cognitive integration: Enhanced Cognitive Scaffolding, which promotes progressive autonomy; Symbiotic Division of Cognitive Labor, strategically allocating tasks based on comparative strengths; Dialectical Cognitive Enhancement, countering AI sycophancy through productive epistemic tension; Agentic Transparency and Control, ensuring users understand and direct AI influence; Expertise Democratization, breaking down knowledge silos; Social-Emotional Augmentation, addressing affective dimensions of cognitive work; and Duration-Optimized Integration, managing the evolving human-AI relationship over time. Together, these frameworks provide a comprehensive approach for harnessing AI as a genuine cognitive extension while preserving human agency, critical thinking, and intellectual growth, transforming AI from a replacement for human cognition into a catalyst for enhanced thinking.

</details>


### [11] [MERba: Multi-Receptive Field MambaVision for Micro-Expression Recognition](https://arxiv.org/abs/2506.14468)

*Xinglong Mao, Shifeng Liu, Sirui Zhao, Tong Xu, Enhong Chen*

**Main category:** cs.HC

**Keywords:** micro-expressions, facial recognition, machine learning, psychological diagnosis, criminal investigations

**Relevance Score:** 4

**TL;DR:** This paper presents MERba, a novel architecture for automatic micro-expression recognition that combines localized and global facial feature extraction to improve accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of accurately recognizing micro-expressions, which are critical for psychological diagnosis and criminal investigations, due to existing methods' inability to capture both local muscle activations and global facial dependencies.

**Method:** MERba uses a multi-receptive field architecture with Local-Global Feature Integration stages that includes local feature extractors and lightweight self-attention layers to model global dependencies. It also implements an asymmetric multi-scanning strategy and a Dual-Granularity Classification Module to enhance recognition accuracy.

**Key Contributions:**

	1. Introduction of Local-Global Feature Integration stages for micro-expression recognition.
	2. Development of an asymmetric multi-scanning strategy to boost local perception.
	3. Implementation of a Dual-Granularity Classification Module for improved recognition accuracy.

**Result:** Experiments on benchmark datasets show that MERba outperforms existing methods in micro-expression recognition, with results further validated through ablation studies.

**Limitations:** 

**Conclusion:** The proposed MERba architecture effectively improves micro-expression recognition by integrating localized perceptions with holistic understanding, indicating significant advancements in this field.

**Abstract:** Micro-expressions (MEs) are brief, involuntary facial movements that reveal genuine emotions, holding significant potential in psychological diagnosis and criminal investigations. Despite notable advances in automatic ME recognition (MER), existing methods still struggle to jointly capture localized muscle activations and global facial dependencies, both critical for recognizing subtle emotional cues. To tackle this challenge, we propose MERba, a novel multi-receptive field architecture tailored for MER. MERba introduces a series of Local-Global Feature Integration stages, where fine-grained motion features are first extracted by local extractors containing MambaVision Mixers within non-overlapping windows, and then global dependencies across these regions are modeled via lightweight self-attention layers. This hierarchical design enables a progressive transition from localized perception to holistic facial understanding. Furthermore, we introduce an asymmetric multi-scanning strategy to eliminate redundant scanning directions and enhance local spatial perception. To address the high inter-class similarity among negative MEs, we introduce a Dual-Granularity Classification Module that decouples the recognition process into a coarse-to-fine paradigm. Experiments on two benchmark MER datasets demonstrate that MERba outperforms existing methods, with ablation studies confirming the effectiveness of each proposed component.

</details>


### [12] [SimSpark: Interactive Simulation of Social Media Behaviors](https://arxiv.org/abs/2506.14476)

*Ziyue Lin, Yi Shan, Lin Gao, Xinghua Jia, Siming Chen*

**Main category:** cs.HC

**Keywords:** social media, simulation, interactive system, large language models, user behavior

**Relevance Score:** 6

**TL;DR:** This paper presents SimSpark, an interactive system for simulating user behaviors on social media, featuring customizable characters and environments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding and analysis of social media behaviors and overcome challenges related to real data methods, such as accessibility and ethical concerns.

**Method:** SimSpark utilizes simulation algorithms and large language models to generate believable user behaviors and provides a visual interface for real-time parameter adjustment and analysis.

**Key Contributions:**

	1. Introduction of an interactive simulation system for social media
	2. Integration of large language models for behavior generation
	3. Development of a customizable visual interface for real-time analysis

**Result:** The effectiveness of SimSpark is evaluated through case studies, quantitative assessments, and expert interviews, demonstrating robust simulation capabilities.

**Limitations:** 

**Conclusion:** SimSpark addresses key challenges in social media behavior simulation, offering flexibility and interactivity for researchers and stakeholders.

**Abstract:** Understanding user behaviors on social media has garnered significant scholarly attention, enhancing our comprehension of how virtual platforms impact society and empowering decision-makers. Simulating social media behaviors provides a robust tool for capturing the patterns of social media behaviors, testing hypotheses, and predicting the effects of various interventions, ultimately contributing to a deeper understanding of social media environments. Moreover, it can overcome difficulties associated with utilizing real data for analysis, such as data accessibility issues, ethical concerns, and the complexity of processing large and heterogeneous datasets. However, researchers and stakeholders need more flexible platforms to investigate different user behaviors by simulating different scenarios and characters, which is not possible yet. Therefore, this paper introduces SimSpark, an interactive system including simulation algorithms and interactive visual interfaces which is capable of creating small simulated social media platforms with customizable characters and social environments. We address three key challenges: generating believable behaviors, validating simulation results, and supporting interactive control for generation and results analysis. A simulation workflow is introduced to generate believable behaviors of agents by utilizing large language models. A visual interface enables real-time parameter adjustment and process monitoring for customizing generation settings. A set of visualizations and interactions are also designed to display the models' outputs for further analysis. Effectiveness is evaluated through case studies, quantitative simulation model assessments, and expert interviews.

</details>


### [13] [Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains](https://arxiv.org/abs/2506.14567)

*Emanuel Moss, Elizabeth Watkins, Christopher Persaud, Passant Karunaratne, Dawn Nafus*

**Main category:** cs.HC

**Keywords:** Generative AI, Engineering Workflows, Human-Computer Interaction

**Relevance Score:** 5

**TL;DR:** This paper explores the challenges faced by engineers using generative AI tools in high-precision domains, emphasizing the need for improved control over interaction contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how generative AI tools are perceived and used by engineers in high-precision fields, particularly regarding accuracy and interaction challenges.

**Method:** The study conducts interviews with hardware and software engineers involved in integrated circuit design to gather insights on their experiences with generative AI tools.

**Key Contributions:**

	1. Identified specific challenges engineers face when using generative AI tools in integrated circuit design.
	2. Mapped forms of troubles to generative AI system elements.
	3. Provided recommendations for enhancing interactive control in AI contexts.

**Result:** The analysis identifies various forms of trouble encountered by engineers, particularly the challenge of maintaining control over the context of their interactions with AI tools.

**Limitations:** 

**Conclusion:** The paper concludes that improving interactive control of context is crucial for mitigating issues faced when using generative AI tools.

**Abstract:** Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.

</details>


### [14] [Exploring MLLMs Perception of Network Visualization Principles](https://arxiv.org/abs/2506.14611)

*Jacob Miller, Markus Wallinger, Ludwig Felder, Timo Brand, Henry Förster, Johannes Zink, Chunyang Chen, Stephen Kobourov*

**Main category:** cs.HC

**Keywords:** Multimodal Large Language Models, human perception, network layouts, prompt engineering, visual proxies

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of Multimodal Large Language Models to replicate human performance in perceiving properties in network layouts and identifies effective prompt engineering techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether MLLMs can achieve human-level performance in perception tasks pertaining to network layouts.

**Method:** The study replicates a human-subject experiment on perceiving stress in network layouts using GPT-4o and Gemini-2.5. MLLMs are provided with the same information as trained human participants.

**Key Contributions:**

	1. Evidence of MLLMs matching human performance in perception tasks.
	2. Identification of prompt engineering strategies that enhance performance.
	3. Insights into the nature of MLLM perception through visual proxies.

**Result:** MLLMs demonstrated performance comparable to human experts and outperformed untrained non-experts, with certain prompt engineering approaches yielding better-than-human results.

**Limitations:** The study focuses on specific tasks and may not generalize across all perception scenarios.

**Conclusion:** MLLMs' reliance on visual proxies suggests an emergent, human-like perception mechanism, mimicking the explanations provided by human participants.

**Abstract:** In this paper, we test whether Multimodal Large Language Models (MLLMs) can match human-subject performance in tasks involving the perception of properties in network layouts. Specifically, we replicate a human-subject experiment about perceiving quality (namely stress) in network layouts using GPT-4o and Gemini-2.5. Our experiments show that giving MLLMs exactly the same study information as trained human participants results in a similar performance to human experts and exceeds the performance of untrained non-experts. Additionally, we show that prompt engineering that deviates from the human-subject experiment can lead to better-than-human performance in some settings. Interestingly, like human subjects, the MLLMs seem to rely on visual proxies rather than computing the actual value of stress, indicating some sense or facsimile of perception. Explanations from the models provide descriptions similar to those used by the human participants (e.g., even distribution of nodes and uniform edge lengths).

</details>


### [15] [How Viable are Energy Savings in Smart Homes? A Call to Embrace Rebound Effects in Sustainable HCI](https://arxiv.org/abs/2506.14653)

*Christina Bremer, Harshit Gujral, Michelle Lin, Lily Hinkers, Christoph Becker, Vlad C. Coroamă*

**Main category:** cs.HC

**Keywords:** rebounds effects, HCI, energy efficiency, smart homes, literature mapping

**Relevance Score:** 4

**TL;DR:** This paper explores the consideration of rebound effects in HCI and smart home research, revealing limited attention and proposing actions for HCI.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the extent to which rebound effects, which may reduce energy efficiency savings, have been considered in smart home and HCI research.

**Method:** Conducted a literature mapping using four scientific databases and a SIGCHI corpus.

**Key Contributions:**

	1. Identification of limited attention to rebound effects in smart home research.
	2. Creation of a taxonomy of actions for HCI to address these effects.
	3. Recommendations for future research directions in energy efficiency.

**Result:** Findings indicate limited consideration of rebound effects in relevant literature, highlighting opportunities for enhancement in HCI.

**Limitations:** Study focuses primarily on literature mapping, which may not capture all practical applications or ongoing research activities.

**Conclusion:** The paper offers a taxonomy of actions for HCI to address rebound effects, which could improve the viability of energy efficiency projects.

**Abstract:** As part of global climate action, digital technologies are seen as a key enabler of energy efficiency savings. A popular application domain for this work is smart homes. There is a risk, however, that these efficiency gains result in rebound effects, which reduce or even overcompensate the savings. Rebound effects are well-established in economics, but it is less clear whether they also inform smart energy research in other disciplines. In this paper, we ask: to what extent have rebound effects and their underlying mechanisms been considered in computing, HCI and smart home research? To answer this, we conducted a literature mapping drawing on four scientific databases and a SIGCHI corpus. Our results reveal limited consideration of rebound effects and significant opportunities for HCI to advance this topic. We conclude with a taxonomy of actions for HCI to address rebound effects and help determine the viability of energy efficiency projects.

</details>


### [16] [StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery](https://arxiv.org/abs/2506.14670)

*Jina Kim, Leeje Jang, Yao-Yi Chiang, Guanyu Wang, Michelle Pasco*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Vision-Language Models, Neighborhood Studies

**Relevance Score:** 8

**TL;DR:** StreetLens is a configurable workflow that integrates social science expertise into a vision-language model (VLM) to automate neighborhood environmental assessments, enabling scalable analysis and flexibility across diverse research designs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the traditional, manually intensive methods used in neighborhood studies by integrating vision-language models (VLMs) to improve efficiency and adaptability.

**Method:** StreetLens utilizes socially-informed prompting within a VLM to analyze street view imagery (SVI) based on established protocols, generating semantic annotations ranging from objective features to subjective perceptions.

**Key Contributions:**

	1. Integration of social science expertise into VLM workflows
	2. Configurable analysis framework for neighborhood studies
	3. Access to a Google Colab notebook for ease of use

**Result:** StreetLens allows researchers to customize the role of the VLM and integrate previous survey data, leading to enhanced analysis and applicability across various contexts.

**Limitations:** 

**Conclusion:** StreetLens represents a paradigm shift in neighborhood studies, promoting flexible, collaborative AI systems that streamline research processes while maintaining scholarly rigor.

**Abstract:** Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.

</details>


### [17] [Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach](https://arxiv.org/abs/2506.14677)

*Yingchao Li*

**Main category:** cs.HC

**Keywords:** sign language, user-adaptive AI, transformer, natural language processing, real-time communication

**Relevance Score:** 6

**TL;DR:** This paper introduces a real-time, user-adaptive speech-to-sign language system using Transformer-based motion generation, focusing on user-editable JSON for improved expressiveness and trust.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility and user agency in sign language technologies by allowing users to inspect and modify sign segments directly.

**Method:** The system uses a streaming Conformer encoder and an autoregressive Transformer-MDN decoder to synchronize spoken input with 3D avatar animation, providing an editable interface for users.

**Key Contributions:**

	1. Real-time speech-to-sign conversion
	2. User-editable JSON for customization
	3. Integration of participatory feedback loops for continual improvement

**Result:** Experiments show significant improvements in comprehension, naturalness, usability, and lowered cognitive load for 20 deaf signers and 5 interpreters.

**Limitations:** 

**Conclusion:** The combination of technical advancements and user participation results in a more accessible and explainable AI for sign language applications.

**Abstract:** This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.

</details>


### [18] [How Warm-Glow Alters the Usability of Technology](https://arxiv.org/abs/2506.14720)

*Antonios Saravanos*

**Main category:** cs.HC

**Keywords:** warm-glow, usability, technology design, intrinsic, extrinsic

**Relevance Score:** 7

**TL;DR:** This study explores how the warm-glow phenomenon influences perceived usability, finding that intrinsic warm-glow enhances usability dimensions, while extrinsic warm-glow affects perceived effectiveness and satisfaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of the warm-glow phenomenon in shaping perceptions of usability beyond traditional models focused on functionality.

**Method:** An experimental approach where participants evaluated technology under conditions evoking intrinsic or extrinsic warm-glow dimensions. Multivariate Analysis of Variance was used for analysis.

**Key Contributions:**

	1. Demonstrated the impact of warm-glow on usability perception
	2. Differentiated between intrinsic and extrinsic dimensions of warm-glow
	3. Proposed strategic design considerations for technology to enhance usability

**Result:** Intrinsic warm-glow significantly enhances all usability dimensions, while extrinsic warm-glow selectively impacts perceived effectiveness and satisfaction.

**Limitations:** 

**Conclusion:** Designers should incorporate warm-glow into technology design to align with users' values and enhance perceived usability.

**Abstract:** As technology increasingly aligns with users' personal values, traditional models of usability, focused on functionality and specifically effectiveness, efficiency, and satisfaction, may not fully capture how people perceive and evaluate it. This study investigates how the warm-glow phenomenon, the positive feeling associated with doing good, shapes perceived usability. An experimental approach was taken in which participants evaluated a hypothetical technology under conditions designed to evoke either the intrinsic (i.e., personal fulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A Multivariate Analysis of Variance as well as subsequent follow-up analyses revealed that intrinsic warm-glow significantly enhances all dimensions of perceived usability, while extrinsic warm-glow selectively influences perceived effectiveness and satisfaction. These findings suggest that perceptions of usability extend beyond functionality and are shaped by how technology resonates with users' broader sense of purpose. We conclude by proposing that designers consider incorporating warm-glow into technology as a strategic design decision.

</details>


### [19] [Gaze-informed Signatures of Trust and Collaboration in Human-Autonomy Teams](https://arxiv.org/abs/2409.19139)

*Anthony J. Ries, Stéphane Aroca-Ouellette, Alessandro Roncone, Ewart J. de Visser*

**Main category:** cs.HC

**Keywords:** human-autonomy teaming, adaptive AI, eye tracking, trust, collaboration

**Relevance Score:** 8

**TL;DR:** This paper investigates the collaboration between humans and adaptive AI agents in a gaming environment, focusing on trust and teamwork dynamics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance collaboration and trust between humans and autonomous agents in dynamic teaming scenarios.

**Method:** Utilized the game Overcooked AI to test different AI agent behaviors and environmental complexities, assessing performance and using eye tracking to measure trust and collaboration.

**Key Contributions:**

	1. Development of adaptive AI agents using hierarchical reinforcement learning
	2. Identification of eye-tracking metrics that correlate with trust and contribution
	3. Demonstration of improved teamwork dynamics in high-complexity environments

**Result:** Adaptive AI agents performed better in task distribution and coordination, showing higher trust ratings and revealing eye-tracking metrics related to teamwork effectiveness.

**Limitations:** 

**Conclusion:** Effective autonomous agents should not only excel in tasks but also enhance teamwork by being predictable and reducing cognitive load. Eye-tracking is a viable method for assessing team dynamics.

**Abstract:** In the evolving landscape of human-autonomy teaming (HAT), fostering effective collaboration and trust between human and autonomous agents is increasingly important. To explore this, we used the game Overcooked AI to create dynamic teaming scenarios featuring varying agent behaviors (clumsy, rigid, adaptive) and environmental complexities (low, medium, high). Our objectives were to assess the performance of adaptive AI agents designed with hierarchical reinforcement learning for better teamwork and measure eye tracking signals related to changes in trust and collaboration. The results indicate that the adaptive agent was more effective in managing teaming and creating an equitable task distribution across environments compared to the other agents. Working with the adaptive agent resulted in better coordination, reduced collisions, more balanced task contributions, and higher trust ratings. Reduced gaze allocation, across all agents, was associated with higher trust levels, while blink count, scan path length, agent revisits and trust were predictive of the humans contribution to the team. Notably, fixation revisits on the agent increased with environmental complexity and decreased with agent versatility, offering a unique metric for measuring teammate performance monitoring. These findings underscore the importance of designing autonomous teammates that not only excel in task performance but also enhance teamwork by being more predictable and reducing the cognitive load on human team members. Additionally, this study highlights the potential of eye-tracking as an unobtrusive measure for evaluating and improving human-autonomy teams, suggesting eye gaze could be used by agents to dynamically adapt their behaviors.

</details>


### [20] [Why Interdisciplinary Teams Fail: A Systematic Analysis With Activity Theory in Clinical AI Collaboration](https://arxiv.org/abs/2410.00174)

*Bingsheng Yao, Yao Du, Yue Fu, Xuhai Xu, Yanjun Gao, Hong Yu, Dakuo Wang*

**Main category:** cs.HC

**Keywords:** AI in healthcare, collaboration, Activity Theory, speech-language pathology, interdisciplinary teams

**Relevance Score:** 8

**TL;DR:** This paper investigates collaboration barriers in clinical AI projects, particularly in speech-language pathology, using Activity Theory to enhance interdisciplinary teamwork.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for effective collaboration between clinical and technical experts in developing AI technologies for patient care, yet many interdisciplinary teams struggle due to communication issues.

**Method:** The study involved semi-structured interviews with six clinical and seven technical experts, analyzing collaboration through the lens of Activity Theory.

**Key Contributions:**

	1. Identified barriers in interdisciplinary collaboration for clinical AI development.
	2. Proposed the use of clinical data as boundary objects to bridge communication gaps.
	3. Provided insights on best practices for enhancing collaboration based on Activity Theory.

**Result:** The analysis revealed significant knowledge gaps related to data coding and specialized languages, and identified clinical data as a potential boundary object to facilitate communication.

**Limitations:** 

**Conclusion:** Leveraging analytical frameworks like Activity Theory can improve understanding of collaboration practices in clinical AI teams and enhance future cooperation strategies.

**Abstract:** Advanced AI technologies are increasingly integrated into clinical domains to advance patient care. The design and development of clinical AI technologies necessitate seamless collaboration between clinical and technical experts. Yet, such interdisciplinary teams are often unsuccessful, with a lack of systematic analysis of collaboration barriers and coping strategies. This work examines two clinical AI collaborations in the context of speech-language pathology via semi-structured interviews with six clinical and seven technical experts. Using Activity Theory (AT) as our analytical lens, we systematically investigate persistent knowledge gaps in mismatched data coding themes and specialized languages, and also highlight how clinical data can act as boundary objects and human knowledge brokers to alleviate these challenges. Our work underscores the benefits of leveraging analytical frameworks like AT to systematically examine interdisciplinary teams' collaborative work and provide meaningful insights on best practices in future collaboration.

</details>


### [21] [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/abs/2501.04227)

*Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, Emad Barsoum*

**Main category:** cs.HC

**Keywords:** autonomous LLM, research automation, human feedback

**Relevance Score:** 9

**TL;DR:** Agent Laboratory is an LLM-based framework that automates the research process from literature review to report writing, significantly reducing research costs and improving quality through human feedback.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To accelerate scientific discovery, reduce research costs, and enhance research quality by automating the entire research process.

**Method:** Agent Laboratory employs a three-stage process: literature review, experimentation, and report writing, while integrating human feedback at each stage.

**Key Contributions:**

	1. Introduction of an autonomous LLM-based research framework
	2. Demonstration of significant cost reduction in research
	3. Showing the importance of human feedback in enhancing research quality

**Result:** Agent Laboratory, particularly when driven by o1-preview, produces superior research outcomes, with machine learning code achieving state-of-the-art performance and costs reduced by 84% compared to existing methods.

**Limitations:** 

**Conclusion:** The use of Agent Laboratory allows researchers to focus on creative ideation rather than low-level tasks, ultimately speeding up the scientific discovery process.

**Abstract:** Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.

</details>


### [22] [Bridging Voting and Deliberation with Algorithms: Field Insights from vTaiwan and Kultur Komitee](https://arxiv.org/abs/2502.05017)

*Joshua C. Yang, Fynn Bachmann*

**Main category:** cs.HC

**Keywords:** democratic processes, voting, deliberation, algorithmic trust, participatory decision-making

**Relevance Score:** 4

**TL;DR:** This paper presents methods that integrate online voting with face-to-face deliberation to improve democratic decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work addresses the challenge of reconciling individual preferences with collective decision-making in democratic processes.

**Method:** The authors introduce algorithms for integrating online voting with deliberation, including Preference-based Clustering for Deliberation, Human-in-the-loop Method of Equal Shares, and ReadTheRoom deliberation method.

**Key Contributions:**

	1. Preference-based Clustering for Deliberation (PCD)
	2. Human-in-the-loop Method of Equal Shares (MES)
	3. ReadTheRoom deliberation method

**Result:** The methods were tested in real-world scenarios, demonstrating their effectiveness in enhancing transparency and collaboration in decision-making.

**Limitations:** 

**Conclusion:** These frameworks provide scalable digital solutions to improve in-person deliberation in participatory processes.

**Abstract:** Democratic processes increasingly aim to integrate large-scale voting with face-to-face deliberation, addressing the challenge of reconciling individual preferences with collective decision-making. This work introduces new methods that use algorithms and computational tools to bridge online voting with face-to-face deliberation, tested in two real-world scenarios: Kultur Komitee 2024 (KK24) and vTaiwan. These case studies highlight the practical applications and impacts of the proposed methods.   We present three key contributions: (1) Preference-based Clustering for Deliberation (PCD), which enables both in-depth and broad discussions in deliberative settings by computing homogeneous and heterogeneous group compositions with balanced and adjustable group sizes; (2) Human-in-the-loop MES, a practical method that enhances the Method of Equal Shares (MES) algorithm with real-time digital feedback. This builds algorithmic trust by giving participants full control over how much decision-making is delegated to the voting aggregation algorithm as compared to deliberation; and (3) the ReadTheRoom deliberation method, which uses opinion space mapping to identify agreement and divergence, along with spectrum-based preference visualisation to track opinion shifts during deliberation. This approach enhances transparency by clarifying collective sentiment and fosters collaboration by encouraging participants to engage constructively with differing perspectives. By introducing these actionable frameworks, this research extends in-person deliberation with scalable digital methods that address the complexities of modern decision-making in participatory processes.

</details>


### [23] [Carbon and Silicon, Coexist or Compete? A Survey on Human-AI Interactions in Agent-based Modeling and Simulation](https://arxiv.org/abs/2502.18145)

*Ziyue Lin, Siqi Shen, Zichen Cheng, Cheok Lam Lai, Siming Chen*

**Main category:** cs.HC

**Keywords:** human-AI interaction, agent-based modeling, taxonomy, large language models, emergent phenomena

**Relevance Score:** 9

**TL;DR:** This paper investigates human-AI interactions in agent-based modeling and simulation (ABMS) using large language models (LLMs), proposing a novel taxonomy to categorize these interactions.

**Read time:** 36 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid integration of LLMs into ABMS requires understanding human interactions to better meet research demands and improve model adaptability.

**Method:** A systematic review of existing works on human-AI interactions in ABMS, leading to the development of a novel taxonomy based on five interaction dimensions.

**Key Contributions:**

	1. Novel taxonomy categorizing human-AI interactions in ABMS
	2. Systematic review of existing research on interactions
	3. Summary of interaction patterns that aid future research

**Result:** The paper identifies patterns in human-AI interactions within ABMS and provides comprehensive guidance for researchers developing such interactions.

**Limitations:** Limited to existing literature; does not cover all potential interactions.

**Conclusion:** The proposed taxonomy lays a foundation for further research into unexplored interaction types between humans and AI in ABMS.

**Abstract:** Recent interest in human-AI interactions in agent-based modeling and simulation (ABMS) has grown rapidly due to the widespread utilization of large language models (LLMs). ABMS is an intelligent approach that simulates autonomous agents' behaviors within a defined environment to research emergent phenomena. Integrating LLMs into ABMS enables natural language interaction between humans and models. Meanwhile, it introduces new challenges that rely on human interaction to address. Human involvement can assist ABMS in adapting to flexible and complex research demands. However, systematic reviews of interactions that examine how humans and AI interact in ABMS are lacking. In this paper, we investigate existing works and propose a novel taxonomy to categorize the interactions derived from them. Specifically, human users refer to researchers who utilize ABMS tools to conduct their studies in our survey. We decompose interactions into five dimensions: the goals that users want to achieve (Why), the phases that users are involved (When), the components of the system (What), the roles of users (Who), and the means of interactions (How). Our analysis summarizes the findings that reveal existing interaction patterns. They provide researchers who develop interactions with comprehensive guidance on how humans and AI interact. We further discuss the unexplored interactions and suggest future research directions.

</details>


### [24] [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/abs/2501.04227)

*Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, Emad Barsoum*

**Main category:** cs.HC

**Keywords:** autonomous research, LLM, scientific discovery, human feedback, machine learning

**Relevance Score:** 9

**TL;DR:** Agent Laboratory is an autonomous LLM-based framework designed to accelerate scientific discovery by automating the entire research process, significantly reducing costs and improving quality through human feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To accelerate scientific discovery, reduce research costs, and improve research quality by automating the research process.

**Method:** Agent Laboratory progresses through literature review, experimentation, and report writing stages based on human-provided research ideas, utilizing state-of-the-art LLMs.

**Key Contributions:**

	1. Introduction of an autonomous LLM-based research framework that covers the entire research process.
	2. Demonstrated significant cost reduction in research expenditures (84%).
	3. Proven enhancement of research quality through structured human feedback.

**Result:** Agent Laboratory driven by o1-preview generates the best research outcomes, with its generated machine learning code achieving state-of-the-art performance. Human involvement improves quality and reduces research expenses by 84%.

**Limitations:** 

**Conclusion:** Agent Laboratory aims to shift researchers' focus from coding and writing to creative ideation, thus speeding up scientific discovery.

**Abstract:** Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [25] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)

*Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai*

**Main category:** cs.CL

**Keywords:** climate change, large language models, instruction data, automated generation, fine-tuning

**Relevance Score:** 4

**TL;DR:** This study presents an automated method for constructing instruction data to improve climate change-related LLMs, resulting in a dataset named ClimateChat-Corpus and an enhanced model, ClimateChat.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing problem of climate change necessitates improved research and application of LLMs to provide critical information to decision-makers and the public.

**Method:** The proposed method automates the generation of instruction data by extracting facts and background knowledge from documents, while enhancing diversity through web scraping and seed instruction collection.

**Key Contributions:**

	1. Introduction of an automated method for constructing climate change instruction data.
	2. Creation of the ClimateChat-Corpus dataset for fine-tuning LLMs.
	3. Demonstration of improved performance in climate change question-and-answer tasks with the ClimateChat model.

**Result:** The constructed ClimateChat-Corpus significantly improves the performance of climate change-related tasks when fine-tuning LLMs, particularly demonstrated by the model ClimateChat.

**Limitations:** 

**Conclusion:** Selecting an appropriate base model for instruction tuning is crucial for effectively using LLMs in climate change research, providing empirical support for further development in this area.

**Abstract:** As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [26] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)

*Antara Raaghavi Bhattacharya, Isabel Papadimitriou, Kathryn Davidson, David Alvarez-Melis*

**Main category:** cs.CL

**Keywords:** numerals, language models, linguistic structure, mathematical reasoning, cross-linguistic

**Relevance Score:** 4

**TL;DR:** Large language models struggle with cross-linguistic numeral systems, unable to solve numerical puzzles unless explicitly marked with known symbols. This paper investigates the difficulties faced by LLMs in understanding implicit numeral structure compared to humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why LLMs perform poorly on linguistic-mathematical puzzles involving cross-linguistic numeral systems despite humans succeeding in such tasks.

**Method:** A series of experiments were conducted to separate the linguistic and mathematical aspects of numbers, examining how explicit marking and parameter variations of numeral construction affect performance in LLMs.

**Key Contributions:**

	1. Identification of LLM limitations in reasoning with numeral systems
	2. Experiments demonstrating the importance of explicit marking
	3. Insights into human vs. LLM processing of numeral structures

**Result:** LLMs fail to consistently solve numerical problems without explicit mathematical operations being marked, unlike humans who leverage implicit numeral structures.

**Limitations:** This study focuses only on numeral systems and does not explore other linguistic challenges or the broader context of LLM performance

**Conclusion:** The findings indicate that LLMs lack the ability to infer compositional rules from implicit patterns, highlighting a significant challenge for reasoning models in processing numerals across different languages.

**Abstract:** Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [27] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)

*Jipeng Zhang, Kehao Miao, Renjie Pi, Zhaowei Wang, Runtao Liu, Rui Pan, Tong Zhang*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Reinforcement Fine-Tuning, Multimodal Reasoning, Hallucination Detection, Training Framework

**Relevance Score:** 7

**TL;DR:** This paper introduces an iterative training framework for aligning Vision-Language models using Reinforcement Fine-Tuning (RFT) with a focus on improving the quality of Vision-Language Reward Models (VL-RMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of aligning Vision-Language models, particularly the biases and inefficiencies in existing training methods and data generation processes.

**Method:** An iterative training framework is proposed that incorporates vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling to refine preference datasets and enhance structured critiques.

**Key Contributions:**

	1. Introduction of an iterative training framework for VL models
	2. Use of vision experts and CoT rationales to refine training data
	3. Improved performance in hallucination detection and reasoning tasks

**Result:** Experiments show that the proposed method improves detection of hallucinations and performance in multimodal reasoning tasks, surpassing existing benchmarks.

**Limitations:** Potential reliance on the quality of vision experts and the effectiveness of CoT rationales in diverse contexts is not fully explored.

**Conclusion:** The iterative approach not only advances the performance of VL models but also promotes better alignment through structured feedback mechanisms in reinforcement learning.

**Abstract:** Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


### [28] [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)

*Ryuki Matsuura, Shikhar Bharadwaj, Jiarui Liu, Dhatchi Kunde Govindarajan*

**Main category:** cs.CL

**Keywords:** emotional dialogue system, task-oriented systems, large language models, text-to-speech, engagement

**Relevance Score:** 8

**TL;DR:** Development of an emotional spoken dialogue system for empathetic news conversations, leveraging LLMs and emotional TTS.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more empathetic task-oriented spoken dialogue system for news conversations by regulating emotional speech based on context.

**Method:** Developed a spoken dialogue system utilizing a large language model for sentiment analysis and PromptTTS for synthesizing emotional speech. Introduced a new subjective evaluation scale for assessing emotional dialogue systems.

**Key Contributions:**

	1. Introduction of an emotional SDS for news conversations.
	2. Utilization of LLM-based sentiment analysis for emotional speech synthesis.
	3. Development of a subjective evaluation scale for emotional SDSs.

**Result:** The emotional SDS significantly outperformed a baseline system in terms of emotion regulation and user engagement during conversations.

**Limitations:** Potential need for further refinement in emotional context detection and synthesis quality.

**Conclusion:** Emotional regulation in speech is essential for enhancing engagement in conversations, and our system demonstrates this capability effectively.

**Abstract:** We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

</details>


### [29] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)

*Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, Amitava Das*

**Main category:** cs.CL

**Keywords:** alignment, large language models, safety auditing, LITMUS dataset, human values

**Relevance Score:** 9

**TL;DR:** The paper introduces the Alignment Quality Index (AQI), a metric for assessing the alignment of large language models (LLMs) with human values, addressing current evaluation shortcomings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly deployed in critical sectors like healthcare and education, ensuring their alignment with human values and safety measures has become crucial.

**Method:** The AQI analyzes safe and unsafe activations in latent space using clustering metrics such as the Davies-Bouldin Score, Dunn Index, and others, to identify hidden misalignments and jailbreak risks.

**Key Contributions:**

	1. Introduction of the Alignment Quality Index (AQI) for assessing LLM alignment
	2. Creation of the LITMUS dataset for robust evaluation of alignment
	3. Demonstration of AQI's effectiveness in revealing hidden vulnerabilities in LLMs

**Result:** Empirical tests with the LITMUS dataset show that AQI correlates with external evaluations and effectively reveals vulnerabilities overlooked by traditional refusal-based metrics.

**Limitations:** 

**Conclusion:** AQI is a robust tool for safety auditing of LLM alignment, and the LITMUS dataset enables comprehensive evaluation under tough conditions, with public implementation available for further research.

**Abstract:** Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [30] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)

*Shang-Chi Tsai, Seiya Kawano, Angel Garcia Contreras, Koichiro Yoshino, Yun-Nung Chen*

**Main category:** cs.CL

**Keywords:** robot assistance, multimodal classification, data augmentation

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for enhancing robot intent understanding through visual and linguistic data augmentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve robot assistance in human activities by enhancing user requests with visual cues to better understand intent.

**Method:** The framework uses a large language model to generate simulated dialogues and environmental contexts, followed by employing a stable diffusion model to create corresponding images.

**Key Contributions:**

	1. Introduction of a novel data augmentation framework for robotic assistance
	2. Utilization of a large language model for simulating dialogues
	3. Implementation of a stable diffusion model for generating environmental imagery

**Result:** Experimental results show significant improvement in the robot's action selection capabilities, achieving state-of-the-art performance based on a real-world dataset.

**Limitations:** 

**Conclusion:** The proposed data augmentation methodology effectively refines multimodal models, allowing for better action determination in robot assistance scenarios.

**Abstract:** When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [31] [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)

*Aleksander Smywiński-Pohl, Tomer Libal, Adam Kaczmarczyk, Magdalena Król*

**Main category:** cs.CL

**Keywords:** legal research, annotation automation, language models, legal concepts, LLM

**Relevance Score:** 4

**TL;DR:** The paper investigates the automation of legal concept annotation using LLMs, addressing the optimal volume of manual annotations and selection processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of legal research by automating the annotation of legal concepts and reducing the reliance on manual annotation.

**Method:** Experiments conducted to assess the optimal number of annotations needed for legal concepts, the effectiveness of random versus selective annotation, and the outcomes of LLM-assisted automation of the annotation process.

**Key Contributions:**

	1. Identification of the optimal number of annotations for legal concepts
	2. Analysis of random versus selective annotation for model performance
	3. Exploration of LLM-based automation in legal annotation

**Result:** Initial findings suggest that the automation process can reduce the cost and time associated with manual annotation while maintaining the relevancy of the interpretations retrieved.

**Limitations:** Limited to the scope of legal concepts investigated; further research needed to generalize findings across different legal domains.

**Conclusion:** Automating the annotation process is promising, providing a balance between efficiency and quality in legal research.

**Abstract:** One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.

</details>


### [32] [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)

*Xiuwen Wu, Hao Wang, Zhiang Yan, Xiaohan Tang, Pengfei Xu, Wai-Ting Siok, Ping Li, Jia-Hong Gao, Bingjiang Lyu, Lang Qin*

**Main category:** cs.CL

**Keywords:** human-AI interaction, large language models, emotion representation

**Relevance Score:** 9

**TL;DR:** This paper explores human-AI emotional alignment, particularly how large language models (LLMs) can share and modulate emotional tones akin to human expressions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs can effectively represent human emotions in language and whether their emotional outputs can be controlled.

**Method:** The authors assessed emotional alignment between humans and AI across linguistic-cultural groups and model families, utilizing interpretable LLM features from nuanced emotion categories to analyze emotional outputs.

**Key Contributions:**

	1. Assessment of human-AI emotional alignment across linguistic-cultural groups
	2. Demonstration of LLM emotional representations congruent with human perception
	3. Development of steering vectors for controlled modulation of AI emotional outputs

**Result:** The results indicate that LLM-derived emotion spaces align with human emotional perception, validating essential affective dimensions and enabling controlled modulation of AI emotional expressions.

**Limitations:** 

**Conclusion:** The study concludes that AI can share emotional representations with humans and that its outputs can be directed using grounded human emotion concepts, enhancing human-machine collaboration.

**Abstract:** Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.

</details>


### [33] [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)

*Amr Mohamed, Yang Zhang, Michalis Vazirgiannis, Guokan Shang*

**Main category:** cs.CL

**Keywords:** code-switching, Large Language Models, multilingual discourse, comprehension, fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper evaluates how Large Language Models (LLMs) comprehend code-switching (CSW) in multilingual discourse, demonstrating varying impacts on comprehension based on language interaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs process mixed-language text due to the prevalence of code-switching in online communication.

**Method:** A systematic evaluation by generating code-switched variants of established reasoning and comprehension benchmarks.

**Key Contributions:**

	1. Systematic evaluation of LLMs on code-switched data
	2. Insights on how foreign language tokens affect comprehension
	3. Demonstration that embedding English in other languages can enhance understanding

**Result:** The study finds comprehension degradation when foreign tokens interrupt English text, but embedding English in other languages often enhances understanding. Fine-tuning shows potential for improving comprehension compared to mere prompting.

**Limitations:** The effects observed may vary across different LLM architectures and training data.

**Conclusion:** Understanding LLM interaction with code-switching is crucial for improving their performance in multilingual settings, with fine-tuning proving to be an effective strategy.

**Abstract:** Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

</details>


### [34] [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)

*Xueqing Peng, Lingfei Qian, Yan Wang, Ruoyu Xiang, Yueru He, Yang Ren, Mingyang Jiang, Jeff Zhao, Huan He, Yi Han, Yun Feng, Yuechen Jiang, Yupeng Cao, Haohang Li, Yangyang Yu, Xiaoyu Wang, Penglei Gao, Shengyuan Lin, Keyi Wang, Shanshan Yang, Yilun Zhao, Zhiwei Liu, Peng Lu, Jerry Huang, Suyuchen Wang, Triantafillos Papadopoulos, Polydoros Giannouris, Efstathia Soufleri, Nuo Chen, Guojun Xiong, Zhiyang Deng, Yijia Zhao, Mingquan Lin, Meikang Qiu, Kaleb E Smith, Arman Cohan, Xiao-Yang Liu, Jimin Huang, Alejandro Lopez-Lira, Xi Chen, Junichi Tsujii, Jian-Yun Nie, Sophia Ananiadou, Qianqian Xie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial NLP, Multimodal Benchmark, Cross-lingual Tasks, Complex Reasoning

**Relevance Score:** 8

**TL;DR:** Introduction of MultiFinBen, a multilingual and multimodal benchmark for evaluating LLMs in the financial domain.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks in financial NLP that focus on monolingual and unimodal tasks, which do not capture the complexity of real-world financial communication.

**Method:** Development of the MultiFinBen benchmark, including novel tasks PolyFiQA-Easy, PolyFiQA-Expert, EnglishOCR, and SpanishOCR, across different modalities and linguistic settings. Implementation of a dynamic, difficulty-aware selection mechanism for better evaluation.

**Key Contributions:**

	1. First multilingual and multimodal benchmark for financial NLP
	2. Introduction of complex reasoning tasks over mixed-language inputs
	3. Dynamic difficulty-aware selection mechanism for balanced benchmarking

**Result:** Extensive evaluation of 22 state-of-the-art models demonstrated significant struggles in handling complex cross-lingual and multimodal tasks, revealing gaps in current LLM capabilities in the financial domain.

**Limitations:** Focus on financial domain may limit general applicability to other fields.

**Conclusion:** MultiFinBen aims to facilitate progress in financial studies and applications by offering a robust and comprehensive benchmark for LLM evaluation.

**Abstract:** Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.

</details>


### [35] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)

*Md Nazmus Sakib*

**Main category:** cs.CL

**Keywords:** commonsense reasoning, intent detection, natural language understanding, HCI, multilingual models

**Relevance Score:** 9

**TL;DR:** This review analyzes recent progress in commonsense reasoning and intent detection in natural language understanding by examining 28 papers from notable conferences, emphasizing the interdisciplinary relationship between NLP and HCI.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to highlight advances and challenges in commonsense reasoning and intent detection, which are critical to improving natural language understanding systems.

**Method:** Review of 28 papers from ACL, EMNLP, and CHI (2020-2025), organized by methodology and application.

**Key Contributions:**

	1. Analysis of commonsense reasoning and intent detection methodologies.
	2. Identification of emerging trends in model development.
	3. Discussion of gaps in existing research and benchmark designs.

**Result:** Identified emerging trends towards adaptive, multilingual, and context-aware models while addressing gaps in grounding, generalization, and benchmark design.

**Limitations:** 

**Conclusion:** The synthesis of NLP and HCI insights points to the need for more effective evaluation methods and community engagement to tackle pressing issues in the domains of commonsense reasoning and intent detection.

**Abstract:** This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [36] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)

*David Kogan, Max Schumacher, Sam Nguyen, Masanori Suzuki, Melissa Smith, Chloe Sophia Bellows, Jared Bernstein*

**Main category:** cs.CL

**Keywords:** Language Difficulty, Conversational Text, Large Language Models, Dataset, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introduction of Ace-CEFR, a dataset for evaluating language difficulty of conversational passages to enhance training and filtering of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to assess the difficulty level of short conversational texts to improve LLM training and performance.

**Method:** Expert annotation of English conversational texts to create the Ace-CEFR dataset, followed by testing various models for measuring text difficulty.

**Key Contributions:**

	1. Creation of Ace-CEFR dataset for measuring language difficulty
	2. Demonstration of model accuracy surpassing human experts
	3. Public release for broader research applications

**Result:** Models trained on Ace-CEFR demonstrated greater accuracy in measuring text difficulty than human experts while also maintaining production-level latency.

**Limitations:** 

**Conclusion:** The public release of the Ace-CEFR dataset will aid in research and development efforts related to language difficulty assessment.

**Abstract:** There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [37] [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)

*Iona Carslaw, Sivan Milton, Nicolas Navarre, Ciyang Qing, Wataru Uegaki*

**Main category:** cs.CL

**Keywords:** embedded clauses, constituency parsing, natural language processing, linguistics, dataset

**Relevance Score:** 2

**TL;DR:** This paper presents a methodological approach for detecting and annotating naturally-occurring embedded clauses in English using constituency parsing and heuristics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to utilize large language corpora to analyze embedded clauses in English, which have previously been researched using limited examples.

**Method:** The authors developed a tool for detecting and annotating embedded clauses by applying constituency parsing techniques and parsing heuristics to large-scale text data.

**Key Contributions:**

	1. Development of a tool for detecting naturally-occurring embedded clauses
	2. Creation of the Golden Embedded Clause Set (GECS) for evaluation
	3. Extraction of a large-scale dataset from the Dolma corpus.

**Result:** The tool was evaluated using the Golden Embedded Clause Set (GECS) dataset and successfully extracted a large-scale dataset of naturally-occurring English embedded clauses from the Dolma corpus.

**Limitations:** 

**Conclusion:** The research contributes to a deeper understanding of embedded clauses by providing a new dataset and insights gained from leveraging statistical information from large text corpora.

**Abstract:** For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.

</details>


### [38] [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)

*Paul Landes, Sitara Rao, Aaron Jeremy Chaise, Barbara Di Eugenio*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, automatic summarization, healthcare, deep learning

**Relevance Score:** 9

**TL;DR:** This paper addresses hallucination in LLMs by proposing a method that combines language-based graphs and deep learning for generating trustworthy discharge summaries in the clinical domain.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper is motivated by the need to reduce the documentation burden on physicians and improve the trustworthiness of automatically generated medical summaries, especially in light of the challenges posed by hallucinations in LLMs.

**Method:** The proposed method utilizes language-based graphs in conjunction with deep learning models to enhance the provenance and reliability of automatically generated discharge summaries.

**Key Contributions:**

	1. Development of a novel method that integrates language-based graphs with deep learning for automatic summarization in healthcare.
	2. Reliability validation on real clinical data from MIMIC-III and Anonymous Hospital.
	3. Provision of generated output examples, source code, and trained models for further research and application.

**Result:** The method achieved impressive reliability results when tested on the MIMIC-III corpus and clinical notes from Anonymous Hospital, demonstrating the potential for effective application in clinical settings.

**Limitations:** The study primarily focuses on the use of specific datasets and may not account for variations in content across different medical contexts or institutions.

**Conclusion:** The findings indicate that combining language-based graphs with deep learning can significantly improve the quality and trustworthiness of discharge summaries generated by LLMs.

**Abstract:** The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.

</details>


### [39] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)

*Essential AI, :, Andrew Hojel, Michael Pust, Tim Romanski, Yash Vanjani, Ritvik Kapila, Mohit Parmar, Adarsh Chaluvaraju, Alok Tripathy, Anil Thomas, Ashish Tanwer, Darsh J Shah, Ishaan Shah, Karl Stratos, Khoi Nguyen, Kurt Smith, Michael Callahan, Peter Rushton, Philip Monk, Platon Mazarakis, Saad Jamal, Saurabh Srivastava, Somanshu Singla, Ashish Vaswani*

**Main category:** cs.CL

**Keywords:** dataset, language models, taxonomy, annotations, web data

**Relevance Score:** 7

**TL;DR:** Essential-Web v1.0 is a large, annotated dataset designed to improve data accessibility for language models, containing 24 trillion tokens with detailed categorization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by the lack of large, well-organized pre-training datasets for language models, which results in costly data pipelines.

**Method:** The dataset includes 24 trillion tokens, with each document annotated using a twelve-category taxonomy. Taxonomy labels were created using a fine-tuned model, EAI-Distill-0.5b, achieving high annotator agreement.

**Key Contributions:**

	1. Introduction of a 24-trillion-token dataset with comprehensive annotations
	2. Provides a twelve-category taxonomy for document classification
	3. Demonstrates competitive benchmarks in various fields compared to state-of-the-art datasets.

**Result:** Essential-Web v1.0 enables the extraction of competitive web-curated datasets in various fields with SQL-style filters, showing improvements in STEM and medical fields compared to existing datasets.

**Limitations:** 

**Conclusion:** Essential-Web v1.0 is expected to facilitate better training data for language models, thereby enhancing their skill acquisition and performance.

**Abstract:** Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [40] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)

*Jonathan Hayase, Alisa Liu, Noah A. Smith, Sewoong Oh*

**Main category:** cs.CL

**Keywords:** tokenization, language models, Prompt Boundary Problem, model ensembling, proxy-tuning

**Relevance Score:** 8

**TL;DR:** This paper presents a method to unify different tokenizers used in language models, addressing issues like the Prompt Boundary Problem (PBP) and enabling model ensembling.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** Tokenization can distort model generations and cause interoperability issues among language models, particularly with different tokenizers.

**Method:** The authors propose an inference-time method to convert autoregressive language models with BPE tokenizers into character-level or byte-level models without altering their generative distribution.

**Key Contributions:**

	1. Introduction of a method to convert autoregressive LMs with BPE tokenizers to character-level or byte-level LMs.
	2. Solution to the Prompt Boundary Problem (PBP).
	3. Facilitation of model ensembling and proxy-tuning across different tokenizers.

**Result:** The proposed method effectively solves the Prompt Boundary Problem and allows for the unification of vocabularies of models with different tokenizers, improving the performance of ensemble models and enabling proxy-tuning.

**Limitations:** 

**Conclusion:** The ensemble and proxy-tuned models demonstrate improved performance on downstream evaluations compared to individual models.

**Abstract:** Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [41] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)

*Chengyu Huang, Tanya Goyal*

**Main category:** cs.CL

**Keywords:** preference optimization, distance calibrated reward margin, language models, training datasets, machine learning

**Relevance Score:** 7

**TL;DR:** This paper introduces Distance Calibrated Reward Margin (DCRM) as a metric to optimize preference datasets for training models, demonstrating its effectiveness through various empirical evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze how the differences between preferred and dispreferred responses affect learning in LLMs, and to improve preference dataset quality for better optimization performance.

**Method:** The authors develop a new metric, Distance Calibrated Reward Margin (DCRM), which measures the quality of response pairs in preference optimization by quantifying the distance and reward margin between preferred and dispreferred responses. They propose a best-of-$N^2$ pairing method to select the optimal response pairs based on DCRM.

**Key Contributions:**

	1. Introduction of Distance Calibrated Reward Margin (DCRM) as a new metric for preference optimization.
	2. Establishment of a correlation between DCRM and learning outcomes in LLMs.
	3. Development of a best-of-$N^2$ pairing method for selecting high-quality response pairs.

**Result:** The study finds a correlation between higher DCRM values in training sets and improved model performance across various evaluation settings, demonstrating the efficacy of the proposed method on datasets like AlpacaEval, MT-Bench, and Arena-Hard.

**Limitations:** 

**Conclusion:** By leveraging DCRM to optimize preference datasets, the proposed methods can significantly enhance the performance of language models in preference optimization tasks.

**Abstract:** Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [42] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)

*Tao He, Guang Huang, Yu Yang, Tianshi Xu, Sicheng Zhao, Guiguang Ding, Pengyang Wang, Feng Tian*

**Main category:** cs.CL

**Keywords:** large language models, inference latency, speculative sampling, coherence, token generation

**Relevance Score:** 7

**TL;DR:** The S$^4$C framework enhances speculative sampling for LLMs by improving token generation speed and validation efficiency through coherence-aware methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The autoregressive nature of LLMs results in inference latency, hindering real-time applications.

**Method:** The proposed framework, S$^4$C, incorporates multi-head drafting for fast token generation and a continuous verification tree for effective candidate validation and feature reuse.

**Key Contributions:**

	1. Introduction of multi-head drafting for rapid token generation
	2. Development of a continuous verification tree for efficient validation
	3. Significant improvements in efficiency and resource usage over existing methods

**Result:** Experimental results indicate that S$^4$C improves efficiency and parallelism, achieving an acceleration ratio of 2.26x-2.60x on Spec-bench benchmarks compared to state-of-the-art methods.

**Limitations:** 

**Conclusion:** S$^4$C demonstrates superior performance in generating valid tokens while reducing computation costs.

**Abstract:** Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [43] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)

*Md Nazmus Sakib*

**Main category:** cs.CL

**Keywords:** commonsense reasoning, intent detection, natural language understanding, multilingual models, HCI

**Relevance Score:** 8

**TL;DR:** This review analyzes advances in commonsense reasoning and intent detection in natural language understanding, discussing methodologies and applications from recent literature.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to better understand commonsense reasoning and intent detection to improve natural language understanding in various contexts.

**Method:** The review organizes findings from 28 papers from ACL, EMNLP, and CHI by methodology and application, focusing on commonsense reasoning and intent detection.

**Key Contributions:**

	1. Comprehensive review of literature from top conferences
	2. Framework for understanding commonsense reasoning and intent detection
	3. Identification of key gaps and emerging trends in the field

**Result:** Identifies emerging trends towards adaptive, multilingual, and context-aware models, while revealing gaps in grounding, generalization, and benchmark design.

**Limitations:** 

**Conclusion:** The analysis underscores the importance of bridging insights from NLP and HCI to enhance model capabilities in commonsense reasoning and intent detection.

**Abstract:** This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [44] [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)

*Yanlin Li, Hao Liu, Huimin Liu, Yinwei Wei, Yupeng Hu*

**Main category:** cs.CL

**Keywords:** Theory of Mind, implicit bias, Large Language Models, Stereotype Content Model, evaluation framework

**Relevance Score:** 8

**TL;DR:** Proposes an evaluation framework to assess implicit bias in Large Language Models (LLMs) using the Stereotype Content Model (SCM).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating implicit bias in LLMs, which is often obscured by traditional direct-query methods.

**Method:** Introduces two indirect tasks, the Word Association Bias Test (WABT) and the Affective Attribution Test (AAT), to assess implicit lexical associations and covert affective leanings, respectively.

**Key Contributions:**

	1. Development of WABT and AAT for evaluating implicit bias
	2. Reconceptualization of bias as multi-dimensional in ToM
	3. Experimental validation on State-of-the-Art LLMs

**Result:** Extensive experiments reveal complex bias structures in 8 state-of-the-art LLMs, including sociability bias and asymmetric stereotype amplification.

**Limitations:** Framework might not capture all forms of bias and is focused on specific dimensions of ToM.

**Conclusion:** The proposed framework provides a robust methodology for identifying and understanding implicit bias in LLMs.

**Abstract:** Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.

</details>


### [45] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)

*Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren*

**Main category:** cs.CL

**Keywords:** Language Models, Education, Human-Computer Interaction, Pedagogical Capabilities, AI in Education

**Relevance Score:** 8

**TL;DR:** This paper introduces ELI-Why, a benchmark for evaluating language model explanations in education, assessing their fit for various educational levels, and comparing them to human-curated responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well language models can generate tailored explanations for learners with diverse informational needs across different educational grades.

**Method:** The study utilized a benchmark of 13.4K 'Why' questions and conducted two human studies: one with raters as educators evaluating the fit of explanations, and another with raters as learners assessing personal informational needs.

**Key Contributions:**

	1. Introduction of the ELI-Why benchmark for evaluating educational explanations
	2. Comparison of language model explanations versus human-curated responses
	3. Insights into the limitations of current model outputs in educational contexts

**Result:** GPT-4-generated explanations were found to match the intended educational background only 50% of the time, and suffered from a 20% lower fit for learners' informational needs compared to human responses.

**Limitations:** Findings are based on a specific set of language models and may not generalize across all AI systems.

**Conclusion:** The findings indicate a significant gap in the pedagogical effectiveness of language model-generated explanations, highlighting the need for improved adaptability in AI systems for educational use.

**Abstract:** Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [46] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)

*Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao, Chunliang Zhang, Tongran Liu, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** reward models, large language models, unsupervised learning, fine-tuning, label smoothing

**Relevance Score:** 9

**TL;DR:** This paper introduces a generative reward model for training LLMs that utilizes both labeled and unlabeled data, yielding significant improvements in performance across various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve training methods for reward models in large language models by utilizing both unlabeled and labeled data.

**Method:** A generative reward model is trained using large-scale unsupervised learning followed by fine-tuning through supervised learning, incorporating label smoothing to optimize a regularized pairwise ranking loss.

**Key Contributions:**

	1. Introduction of a generative reward model utilizing both unlabeled and labeled data.
	2. Methodology that incorporates label smoothing for optimizing a regularized ranking loss.
	3. Demonstrated significant performance improvements on various tasks with minimal fine-tuning.

**Result:** The generative reward model achieves significant performance improvements over strong baselines in tasks such as response ranking, reinforcement learning from human feedback, and task adaptation with minimal further fine-tuning.

**Limitations:** 

**Conclusion:** The proposed model provides a unifying perspective on training reward models by linking generative and discriminative approaches, creating a foundation model applicable to diverse tasks.

**Abstract:** In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [47] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)

*Tuan Nguyen, Huy-Dat Tran*

**Main category:** cs.CL

**Keywords:** code-switching, automatic speech recognition, synthetic data, multilingual, ASR performance

**Relevance Score:** 6

**TL;DR:** This study proposes a method for building code-switching automatic speech recognition (CS-ASR) systems using synthetic data to improve performance in multilingual settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Code-switching in ASR is challenging due to a lack of transcribed data, making effective recognition difficult in multilingual environments.

**Method:** A phrase-level mixing method is developed to create synthetic code-switching data, which is then used to fine-tune large pretrained ASR models like Whisper, MMS, and SeamlessM4T.

**Key Contributions:**

	1. A novel phrase-level mixing method for synthetic code-switching data generation
	2. Benchmarking of leading ASR models on under-resourced Southeast Asian language pairs
	3. Demonstration of improved ASR performance using the proposed training strategy

**Result:** The proposed method significantly enhances ASR performance across monolingual and code-switching tests, with the best improvements observed for the Malay-English language pair.

**Limitations:** 

**Conclusion:** The approach provides a cost-effective strategy for developing code-switching ASR systems, which is valuable for both research and practical applications in the industry.

**Abstract:** Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [48] [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)

*Tuan Nguyen, Huy-Dat Tran*

**Main category:** cs.CL

**Keywords:** ASR, Code-switching, Machine Learning, Speech Recognition, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents AsyncSwitch, an asynchronous adaptation framework for code-switched ASR systems that enhances performance by utilizing large-scale text data before fine-tuning on corresponding speech-text pairs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The development of code-switched ASR systems is hindered by language ambiguity and the scarcity of multilingual data, making the process costly and complex.

**Method:** AsyncSwitch employs a three-stage process: (1) training decoder layers on code-switched text, (2) aligning decoder and encoder through cross-attention with limited speech-text data, and (3) fully fine-tuning the model.

**Key Contributions:**

	1. Introduction of AsyncSwitch as a novel ASR adaptation framework
	2. Three-stage training process enabling effective code-switching
	3. Demonstrated significant WER reduction with Whisper on real-world code-switched data

**Result:** Experiments with Whisper show a 9.02% reduction in word error rate (WER) for Malay-English code-switching while also enhancing performance on monolingual tasks in various English dialects.

**Limitations:** 

**Conclusion:** The proposed framework effectively improves code-switched ASR performance and maintains high accuracy for monolingual tasks, showing promise for future applications in speech recognition.

**Abstract:** Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.

</details>


### [49] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)

*Lucile Favero, Daniel Frases, Juan Antonio Pérez-Ortiz, Tanja Käser, Nuria Oliver*

**Main category:** cs.CL

**Keywords:** Large Language Models, critical question generation, argument mining, debates, reasoning

**Relevance Score:** 8

**TL;DR:** The paper investigates using LLMs for generating critical questions to enhance reasoning skills in debates, in response to concerns over superficial learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address concerns about LLMs promoting superficial learning and not developing critical thinking skills.

**Method:** A two-step framework using two language models: a Questioner to generate candidate questions and a Judge to select relevant ones.

**Key Contributions:**

	1. Framework for critical question generation using LLMs
	2. Successful implementation demonstrated by ranking first in the shared task
	3. Potential to foster critical thinking in debate contexts

**Result:** The proposed system ranked first in a shared task competition, indicating its effectiveness in promoting critical engagement with argumentative texts.

**Limitations:** 

**Conclusion:** The use of LLMs can foster deeper reasoning and critical analysis when applied in debate interventions.

**Abstract:** The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [50] [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)

*Junghwan Kim, Kieun Park, Sohee Park, Hyunggug Kim, Bongwon Suh*

**Main category:** cs.CL

**Keywords:** Literary Translation, Quality Assessment, Large Language Models, Translation Metrics, Cultural Nuances

**Relevance Score:** 6

**TL;DR:** The paper introduces MAS-LitEval, a multi-agent system using LLMs to evaluate literary translations based on narrative and style, outperforming traditional metrics like BLEU and METEOR.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional translation metrics do not account for the cultural and stylistic nuances vital in literary translations.

**Method:** MAS-LitEval utilizes large language models to analyze translations on the basis of terminology, narrative consistency, and stylistic fidelity.

**Key Contributions:**

	1. Introduction of MAS-LitEval as a novel evaluation system for literary translations
	2. Demonstrated improved performance over traditional metrics
	3. Established a scalable framework for Translation Quality Assessment (TQA)

**Result:** MAS-LitEval outperformed traditional metrics, achieving scores up to 0.890 in capturing literary nuances during tests on various translations of classic literature.

**Limitations:** The study is limited to specific literary texts and may require further validation on diverse datasets.

**Conclusion:** The study presents a scalable framework for Translation Quality Assessment that serves both researchers and translators seeking to maintain literary integrity.

**Abstract:** Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.

</details>


### [51] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)

*Brihi Joshi, Keyu He, Sahana Ramnath, Sadra Sabouri, Kaitlyn Zhou, Souti Chattopadhyay, Swabha Swayamdipta, Xiang Ren*

**Main category:** cs.CL

**Keywords:** language models, education, benchmark, human studies, pedagogy

**Relevance Score:** 8

**TL;DR:** Introducing ELI-Why, a benchmark for evaluating language models' responses to educational questions, revealing significant gaps in the suitability of model-generated explanations compared to human-curated ones.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the pedagogical capabilities of language models in tailoring responses for learners with diverse knowledge backgrounds in education.

**Method:** The study introduces ELI-Why, a benchmark of 13.4K 'Why' questions, followed by two human studies assessing model-generated explanations against human-curated ones across three educational grades.

**Key Contributions:**

	1. Introduction of the ELI-Why benchmark
	2. Human studies comparing GPT-4 explanations to human-curated explanations
	3. Assessment of language models' performance across educational grades

**Result:** GPT-4 explanations matched intended educational backgrounds only 50% of the time compared to 79% for human-curated explanations, and were deemed 20% less suited to learners' informational needs.

**Limitations:** Limitations related to the automated evaluation metrics and the ability to distinguish explanation quality across different model families.

**Conclusion:** The effectiveness of language models in providing pedagogically appropriate explanations is limited, highlighting a need for improved tailoring to diverse informational needs.

**Abstract:** Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [52] [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)

*Jongho Kim, Romain Storaï, Seung-won Hwang*

**Main category:** cs.CL

**Keywords:** language models, anomia, semantic paraphasia, gradient-based augmentation, HCI

**Relevance Score:** 9

**TL;DR:** This paper explores how language models can assist patients with anomia by addressing challenges related to term failures and semantic paraphasia.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve identification of items for patients with anomia, who struggle to name items due to linguistic impairments.

**Method:** The authors propose enhancing the language model by mitigating semantically paraphasic errors and incorporating unseen terms through a gradient-based selective augmentation strategy.

**Key Contributions:**

	1. Proposed a method to robustify language models against paraphasic errors.
	2. Introduced gradient-based selective augmentation for unseen terms.
	3. Demonstrated the effectiveness of the model on real patient data.

**Result:** The model was evaluated on the Tip-of-the-Tongue dataset and real patient data from AphasiaBank, showing strong performance compared to baselines.

**Limitations:** The approach relies on limited domain-specific datasets for evaluation.

**Conclusion:** The findings indicate that the proposed methods significantly aid anomia patients, effectively addressing the challenges of term failure and semantic paraphasia.

**Abstract:** In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.

</details>


### [53] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)

*Jingxu Xie, Dylan Xu, Xuandong Zhao, Dawn Song*

**Main category:** cs.CL

**Keywords:** task synthesis, LLM, computer agents, dataset generation, task complexity

**Relevance Score:** 6

**TL;DR:** AgentSynth is a pipeline for synthesizing diverse and challenging task datasets for computer-use agents using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for scalable and cost-efficient methods to generate high-quality tasks for generalist agents in computer use.

**Method:** Utilizes a two-step process involving an LLM-based task proposer and an execution agent that creates and logs subtasks iteratively.

**Key Contributions:**

	1. Scalable pipeline for task and trajectory generation.
	2. Demonstrates significant task complexity modulation capabilities.
	3. Reduces costs of dataset creation compared to human annotations.

**Result:** Created over 6,000 diverse tasks with empirical evaluations indicating a significant performance drop in agents at higher difficulty levels due to task complexity.

**Limitations:** Limited to the specific use case of computer-use agents; generalizability to other domains not assessed.

**Conclusion:** AgentSynth effectively modulates task complexity and achieves low costs for data generation compared to traditional methods.

**Abstract:** We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth

</details>


### [54] [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)

*Jia-Chen Zhang, Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Fei Dai*

**Main category:** cs.CL

**Keywords:** generative AI, mixed tabular data, diffusion models, causal regularization, data synthesis

**Relevance Score:** 6

**TL;DR:** CausalDiffTab is a diffusion model-based generative model for high-quality mixed tabular data synthesis, addressing challenges of heterogeneous data types and complex inter-variable relationships.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for high-quality training data for generative AI, particularly in the context of mixed tabular data, where existing methods struggle due to complex data characteristics.

**Method:** Introduction of CausalDiffTab, a diffusion model-based generative model designed to synthesize mixed-type tabular data, using a hybrid adaptive causal regularization method based on Hierarchical Prior Fusion.

**Key Contributions:**

	1. Development of CausalDiffTab for mixed tabular data synthesis
	2. Implementation of hybrid adaptive causal regularization
	3. Comprehensive performance evaluation against baseline methods

**Result:** CausalDiffTab demonstrates superior performance over baseline methods across all metrics when tested on seven datasets.

**Limitations:** 

**Conclusion:** The proposed method effectively improves the synthesis of high-quality mixed tabular data while maintaining generative capabilities.

**Abstract:** Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.

</details>


### [55] [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)

*Sina Abdidizaji, Md Kowsher, Niloofar Yousefi, Ivan Garibay*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Implicit Influence, Language Models, Conversation Analysis, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper enhances the detection of implicit influential patterns in conversations, demonstrating significant improvements in identifying such patterns and classifying related techniques and victim vulnerabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shift of malicious actors towards using implicit linguistic strategies in conversations for influencing individuals, as traditional detection models struggle with these subtle threats.

**Method:** The study augments an existing dataset by leveraging the reasoning capabilities of advanced language models to detect implicit influential patterns and their locations in conversations.

**Key Contributions:**

	1. Improved model for detecting implicit influential patterns in conversations
	2. Identification of specific locations of these patterns
	3. Significant enhancements in classification of influence techniques and victim vulnerabilities

**Result:** The proposed model achieved a 6% improvement in detecting implicit influential patterns and enhanced multi-label classification tasks related to influence techniques and victim vulnerabilities by 33% and 43%, respectively.

**Limitations:** 

**Conclusion:** The findings indicate that state-of-the-art language models can effectively improve the identification and classification of implicit influence techniques, thus contributing to enhanced security measures in digital communication.

**Abstract:** In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.

</details>


### [56] [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)

*Jongho Kim, Dohyeon Lee, Minsoo Kim, Seung-won Hwang*

**Main category:** cs.CL

**Keywords:** Temporal Relations, Timeline Reasoning Network, Temporal Reading Comprehension, Relation Extraction, Inductive Reasoning

**Relevance Score:** 6

**TL;DR:** The paper presents a novel Timeline Reasoning Network (TRN) designed to improve temporal understanding in tasks like temporal reading comprehension and relation extraction by predicting time spans of events to resolve ambiguity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate understanding of temporal relations is crucial for tasks such as temporal reading comprehension and relation extraction, which face challenges with answer overlaps leading to unreliable results.

**Method:** The proposed TRN operates in a two-step inductive reasoning process where initial answers are generated based on semantic and syntactic information, followed by chaining multiple questions to predict a timeline that informs the answers.

**Key Contributions:**

	1. Introduction of the Timeline Reasoning Network (TRN) for temporal understanding.
	2. Two-step inductive reasoning process for improved answer reliability.
	3. Demonstrated effectiveness on TRC and TRE tasks via empirical results.

**Result:** The TRN demonstrates superior performance on the TORQUE and TB-dense datasets, effectively resolving issues with spurious overlaps through the predicted timeline.

**Limitations:** 

**Conclusion:** The proposed method outperforms previous state-of-the-art techniques in handling temporal relations by employing a structured reasoning approach.

**Abstract:** Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.

</details>


### [57] [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/abs/2501.10970)

*Nitay Calderon, Roi Reichart, Rotem Dror*

**Main category:** cs.CL

**Keywords:** Large Language Models, annotation, human annotators, statistical procedure, evaluation

**Relevance Score:** 9

**TL;DR:** The paper proposes the Alternative Annotator Test (alt-test), a statistical procedure for assessing the validity of LLM annotations in place of human annotators.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of rigorous procedures to evaluate if LLMs can effectively replace human annotations in various fields.

**Method:** The authors propose the Alternative Annotator Test (alt-test) and develop an interpretable measure to compare LLM annotators and judges. They curated ten diverse datasets and conducted experiments with six LLMs and four prompting techniques.

**Key Contributions:**

	1. Introduction of the Alternative Annotator Test (alt-test) for LLM evaluation
	2. Development of a measure for comparing LLM annotators
	3. Curated diverse datasets for validation of LLM annotation effectiveness

**Result:** The experiments indicate that closed-source LLMs like GPT-4o can outperform humans and open-source LLMs in certain tasks, revealing that prompting techniques yield judges of varying quality.

**Limitations:** The study focuses on a limited selection of datasets and LLMs, which may not be generalizable to all applications.

**Conclusion:** The study aims to promote more rigorous and reliable practices when utilizing LLM annotations in research.

**Abstract:** The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.

</details>


### [58] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)

*Md Tanzib Hosain, Salman Rahman, Md Kishor Morol, Md Rizwan Parvez*

**Main category:** cs.CL

**Keywords:** large language models, multi-agent reasoning, experiential knowledge

**Relevance Score:** 9

**TL;DR:** Xolver is a multi-agent reasoning framework for large language models (LLMs) that enhances problem-solving by integrating experiential knowledge through a persistent memory system, outperforming specialized agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in LLMs' isolated problem-solving by incorporating rich experiences similar to expert problem solvers, leading to improved reasoning capabilities.

**Method:** Xolver utilizes a training-free framework that combines external and self-retrieval mechanisms, tool usage, collaborative interactions, and iterative refinement to enhance reasoning using past experiences during inference.

**Key Contributions:**

	1. Introduction of a persistent, evolving memory for LLMs
	2. Integration of diverse experience modalities for enhanced reasoning
	3. Demonstration of superior performance on multiple reasoning benchmarks

**Result:** Xolver outperforms specialized reasoning agents, achieving new best results on various benchmarks like GSM8K and Math-500, even with lightweight model architectures.

**Limitations:** 

**Conclusion:** Holistic experience learning is crucial for developing generalist language agents capable of expert-level reasoning.

**Abstract:** Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [59] [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)

*Yimin Deng, Yuxia Wu, Yejing Wang, Guoshuai Zhao, Li Zhu, Qidong Liu, Derong Xu, Zichuan Fu, Xian Wu, Yefeng Zheng, Xiangyu Zhao, Xueming Qian*

**Main category:** cs.CL

**Keywords:** temporal knowledge graph, reasoning, structural-semantics, event prediction, multi-expert framework

**Relevance Score:** 6

**TL;DR:** The MESH framework integrates structural and semantic reasoning for temporal knowledge graph reasoning to predict future events effectively.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous methods in temporal knowledge graph reasoning, specifically their failure to integrate structural and semantic reasoning and their inability to differentiate between historical and non-historical events.

**Method:** The proposed Multi-Expert Structural-Semantic Hybrid (MESH) framework utilizes three types of expert modules to combine structural and semantic information for various event prediction scenarios.

**Key Contributions:**

	1. Integration of structural and semantic reasoning for improved prediction.
	2. Utilization of expert modules tailored for different event types.
	3. Demonstrated effectiveness across multiple datasets.

**Result:** Extensive experiments on three datasets demonstrate that the MESH framework outperforms existing methods in predicting future events in temporal contexts.

**Limitations:** 

**Conclusion:** The MESH framework effectively enhances temporal knowledge graph reasoning by employing a hybrid approach, leading to better generalization across different prediction scenarios.

**Abstract:** Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.

</details>


### [60] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)

*Chenghao Li, Liu Liu, Baosheng Yu, Jiayan Qiu, Yibing Zhan*

**Main category:** cs.CL

**Keywords:** large language models, token learning, tool integration, numerical reasoning, machine learning

**Relevance Score:** 9

**TL;DR:** Proposes a novel token learning method to integrate external tools into large language models, improving their problem-solving capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Integrating external tools into LLMs is essential for enhancing their performance on complex tasks like numerical reasoning and planning.

**Method:** Develops a token learning method that aligns tool tokens with the existing word embedding space by initializing tool token embeddings based on tool names or descriptions.

**Key Contributions:**

	1. Novel token learning method for aligning tool tokens with word embeddings
	2. Demonstration of performance improvement on complex tasks
	3. Evaluation across diverse datasets such as GSM8K-XL and FuncQA

**Result:** The proposed method shows improved performance on tasks such as numerical reasoning and plan generation, outperforming recent baselines like CoT and REACT.

**Limitations:** 

**Conclusion:** The approach effectively enhances the integration of tools into LLMs, leading to better performance on various complex tasks.

**Abstract:** Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [61] [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)

*Seongbo Jang, Minjin Jeon, Jaehoon Lee, Seonghyeon Lee, Dongha Lee, Hwanjo Yu*

**Main category:** cs.CL

**Keywords:** dialogue generation, timely responses, language models, temporal context, commonsense knowledge

**Relevance Score:** 8

**TL;DR:** This paper introduces the task of timely dialogue response generation and presents the TimelyChat benchmark for evaluating language models based on temporal context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the lack of exploration in response generation based on temporal context in dialogue systems.

**Method:** The authors propose a novel task called timely dialogue response generation, create the TimelyChat benchmark, and construct a large-scale dataset using a temporal commonsense knowledge graph alongside a large language model.

**Key Contributions:**

	1. Introduction of the timely dialogue response generation task
	2. Creation of the TimelyChat benchmark
	3. Development of the Timer dialogue agent trained on a large-scale dataset

**Result:** Experimental results indicate that the Timer agent exceeds the performance of prompt-based LLMs and other fine-tuned models in generating time-sensitive dialogue responses.

**Limitations:** The work is still in progress and may have limitations in terms of exhaustive evaluation across diverse dialogue scenarios.

**Conclusion:** The proposed Timer agent shows improved capabilities in timely response generation, and the authors have made their data and model publicly available.

**Abstract:** While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.

</details>


### [62] [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)

*Xueyang Feng, Jingsen Zhang, Jiakai Tang, Wei Li, Guohao Cai, Xu Chen, Quanyu Dai, Yue Zhu, Zhenhua Dong*

**Main category:** cs.CL

**Keywords:** Conversational Recommendation Agents, Multi-turn Preference Optimization, Expectation Confirmation Theory

**Relevance Score:** 9

**TL;DR:** This paper introduces a new paradigm called ECPO for multi-turn preference optimization in Conversational Recommendation Agents (CRAs) that leverages user satisfaction modeling to improve dialogue interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues with short-sighted responses from CRAs which fail to meet user expectations in multi-turn dialogues.

**Method:** The proposed ECPO paradigm utilizes Expectation Confirmation Theory to model user satisfaction evolution and incorporates a user simulator called AILO for simulating feedback during recommendations.

**Key Contributions:**

	1. Introduction of the ECPO paradigm for multi-turn preference optimization.
	2. Development of a user simulator AILO for user feedback simulation.
	3. Significant improvement in CRA interaction capabilities compared to existing MTPO methods.

**Result:** Experimental results demonstrate that ECPO significantly enhances CRA capabilities, showing improvements in both efficiency and effectiveness compared to existing methods.

**Limitations:** 

**Conclusion:** ECPO effectively optimizes multi-turn dialogues in CRAs while eliminating significant overhead found in previous methods, leading to better user experiences.

**Abstract:** Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

</details>


### [63] [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)

*Silvia Casola, Yang Janet Liu, Siyao Peng, Oliver Kraus, Albert Gatt, Barbara Plank*

**Main category:** cs.CL

**Keywords:** summarization, evaluation metrics, reference sets, human judgments, LLMs

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of reference set variation on summarization evaluation metrics, revealing significant instability in popular metrics like ROUGE.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability of summarization evaluation by incorporating the diversity of human language production and addressing the inconsistencies in reference-based metrics.

**Method:** Analyzed three multi-reference summarization datasets (SummEval, GUMSum, DUC2004) to assess the sensitivity of reference-based metrics to different reference sets and collected human judgments on LLM outputs for diverse data.

**Key Contributions:**

	1. Systematic investigation of reference set sensitivity on summarization metrics.
	2. Identification of significant instability in popular metrics like ROUGE.
	3. Recommendations for improving evaluation practices in summarization, particularly with LLMs.

**Result:** Many reference-based metrics, particularly n-gram-based ones like ROUGE, exhibit significant instability based on reference set choice, affecting model rankings and comparisons.

**Limitations:** Focus on three specific datasets; results may not generalize to all summarization tasks or metrics.

**Conclusion:** The study recommends incorporating reference set variation in summarization evaluation to improve consistency and correlation with human judgments, especially for LLM evaluations.

**Abstract:** Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.

</details>


### [64] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)

*Bruno Martins, Piotr Szymański, Piotr Gramacki*

**Main category:** cs.CL

**Keywords:** Large Language Models, Geo-temporal reasoning, Deep Research Systems

**Relevance Score:** 9

**TL;DR:** This paper proposes advancements in deep research systems by integrating geo-temporal reasoning capabilities to enhance information access in context-rich domains such as public health and environmental science.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current deep research systems that lack geo-temporal reasoning, which is essential for complex queries in various domains.

**Method:** The authors identify technical, infrastructural, and evaluative challenges and propose augmenting search, retrieval, and synthesis processes with geo-temporal constraints.

**Key Contributions:**

	1. Identifies challenges in geo-temporal reasoning in deep research systems
	2. Proposes a framework for integrating geo-temporal capabilities
	3. Calls for open infrastructures and rigorous evaluation protocols

**Result:** The proposed integration aims to create more advanced deep research systems that are aware of geographic and temporal contexts, enhancing their responsiveness to complex information needs.

**Limitations:** 

**Conclusion:** By successfully integrating these capabilities, the next generation of AI-driven information systems could significantly impact fields that rely on contextual information for decision-making.

**Abstract:** The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [65] [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)

*Amrit Poudel, Yifan Ding, Jurgen Pfeffer, Tim Weninger*

**Main category:** cs.CL

**Keywords:** search engines, algorithmic curation, content visibility, social media, bias

**Relevance Score:** 4

**TL;DR:** This study analyzes how search engines, particularly Google, influence content visibility on Web and social media through algorithmic curation, revealing biases in the promotion and suppression of certain hashtags and subreddits.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the biases in how search engines curate content, which significantly influences public discourse and user engagement.

**Method:** The study compares search engine results with nonsampled data from Reddit and Twitter/X to identify systematic biases in visibility for specific hashtags and subreddits.

**Key Contributions:**

	1. Identification of systematic biases in search engine curation of social content
	2. Analysis of specific types of content that are suppressed or promoted
	3. Insights into the impact of algorithmic decisions on public discourse

**Result:** It was found that Google's algorithms suppress subreddits and hashtags related to sexually explicit content, conspiracy theories, advertisements, and cryptocurrencies, while promoting content that garners higher engagement.

**Limitations:** 

**Conclusion:** The findings highlight the significant role of search engine algorithms in shaping public narratives and the visibility of social media content.

**Abstract:** Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.

</details>


### [66] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)

*Lucile Favero, Daniel Frases, Juan Antonio Pérez-Ortiz, Tanja Käser, Nuria Oliver*

**Main category:** cs.CL

**Keywords:** Large Language Models, Critical Thinking, Question Generation, Argument Mining, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This study explores using LLMs to generate critical questions that enhance reasoning and critical thinking in debates, proposing a two-step system that ranked first in a competition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance critical thinking skills in learning environments using LLMs, as traditional uses may encourage superficial learning.

**Method:** A two-step framework utilizing two small-scale open source language models: a Questioner for generating candidate questions, and a Judge for selecting the most relevant questions.

**Key Contributions:**

	1. Development of a two-step framework for critical question generation using LLMs.
	2. Establishment of first-place ranking in a competitive shared task context.
	3. Insights into critical engagement with argumentation through LLM applications.

**Result:** The proposed system ranked first in the shared task competition at the 12th Workshop on Argument Mining, showing effectiveness in promoting critical engagement.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of LLMs to foster deeper reasoning through effective critical question generation.

**Abstract:** The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [67] [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)

*Yeonkyoung So, Gyuseong Lee, Sungmok Jung, Joonhak Lee, JiA Kang, Sangho Kim, Jaejin Lee*

**Main category:** cs.CL

**Keywords:** Negation, Large Language Models, Benchmark, Semantic Understanding, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** Thunder-NUBench is a new benchmark for evaluating sentence-level negation understanding in LLMs, addressing limitations in existing frameworks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmarks focusing on negation understanding in LLMs, which is crucial for deep semantic comprehension.

**Method:** Introduction of Thunder-NUBench, a multiple-choice dataset with curated sentence-negation pairs that assess various forms of negation beyond standard cues.

**Key Contributions:**

	1. Creation of Thunder-NUBench for negation assessment
	2. Focus on structurally diverse negation forms
	3. In-depth evaluation methodology using curated datasets

**Result:** The benchmark allows for a more thorough evaluation of LLMs in understanding different types of negation, enhancing insights into their semantic capabilities.

**Limitations:** 

**Conclusion:** Thunder-NUBench offers a targeted approach to assessing negation in LLMs, contributing to the development of more robust language models.

**Abstract:** Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.

</details>


### [68] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)

*Zeinab Sadat Taghavi, Ali Modarressi, Yunpu Ma, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** retrieval systems, NLP, reasoning, implicit facts, benchmark

**Relevance Score:** 8

**TL;DR:** ImpliRet is a benchmark designed to evaluate document-side processing in retrieval systems, focusing on reasoning based on implicit facts in documents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate retrieval systems in NLP beyond surface-level signals by introducing reasoning-heavy benchmarks.

**Method:** ImpliRet presents simple queries whose relevance depends on implicit facts from documents, specifically through temporal, arithmetic, and world knowledge relationships.

**Key Contributions:**

	1. Introduction of the ImpliRet benchmark focusing on document-side reasoning
	2. Evaluation of various retrievers highlighting their limitations
	3. Testing long-context models with suboptimal results

**Result:** The evaluation of various sparse and dense retrievers shows poor performance, with the best nDCG@10 being only 15.07%, and long-context models like GPT-4.1 scoring 35.06% with only ten documents.

**Limitations:** The benchmark relies on implicit facts which may not be adequately addressed by existing retrieval techniques.

**Conclusion:** Document-side reasoning presents significant challenges even for advanced models, indicating room for improvement in retrieval systems.

**Abstract:** Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [69] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)

*Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Diffusion LLMs, Long-context performance, Context extrapolation

**Relevance Score:** 7

**TL;DR:** This paper presents a systematic investigation into the long-context capabilities of diffusion LLMs compared to traditional auto-regressive LLMs, revealing unique phenomena and proposing a method for context extrapolation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unexplored long-context capabilities of diffusion LLMs and understand their performance in comparison to traditional auto-regressive LLMs.

**Method:** The authors conduct experiments comparing the long-context performance of diffusion LLMs with auto-regressive models while explaining findings with Rotary Position Embedding (RoPE) scaling theory. They propose a training-free method called LongLLaDA for effective context extrapolation.

**Key Contributions:**

	1. Systematic investigation of long-context capabilities in diffusion LLMs.
	2. Introduction of LongLLaDA for context extrapolation in diffusion LLMs.
	3. Identification of stable perplexity and local perception phenomena in diffusion LLMs.

**Result:** Diffusion LLMs maintain stable perplexity and exhibit local perception phenomena that enable better performance on longer contexts. The proposed LongLLaDA method validates effective extrapolation scaling laws for diffusion LLMs, showing both areas of advantage and disadvantage compared to auto-regressive LLMs.

**Limitations:** The study is labeled as a work in progress, suggesting ongoing developments and potential areas for further research.

**Conclusion:** This study lays the groundwork for context extrapolation methods for diffusion LLMs, contributing both theoretical insights and empirical benchmarks for future research.

**Abstract:** Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.

</details>


### [70] [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)

*Jiayin Wang, Zhiquang Guo, Weizhi Ma, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Test-time Learning, Semantic Games, Human Performance Comparison, Dynamic Learning

**Relevance Score:** 8

**TL;DR:** The paper proposes evaluating large language models (LLMs) on their ability to learn during test time using semantic games, revealing insights on their performance compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the true capabilities of large language models (LLMs) in learning from experience, particularly in reasoning-intensive tasks, which existing benchmarks fail to address.

**Method:** The paper introduces semantic games as testbeds for evaluating test-time learning, along with an objective evaluation framework that measures performance under various experience conditions and compares it to human performance.

**Key Contributions:**

	1. Advocates for the evaluation of Test-time Learning in LLMs.
	2. Introduces semantic games as effective testbeds for this evaluation.
	3. Provides a comparative study between LLMs and human participants in learning tasks.

**Result:** Results indicate that while LLMs can show test-time learning capabilities, their learning under cumulative experience is less stable and slower than that of humans, highlighting a significant gap in intellectual capabilities.

**Limitations:** The evaluation is limited to specific tasks within semantic games and may not generalize to all reasoning-intensive contexts.

**Conclusion:** LLMs have potential as general-purpose learning machines, but their performance lags behind human intelligence in dynamic learning scenarios.

**Abstract:** As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

</details>


### [71] [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)

*Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici*

**Main category:** cs.CL

**Keywords:** watermarking, large language models, LLM training, data privacy, synonym substitution

**Relevance Score:** 7

**TL;DR:** LexiMark is a novel watermarking technique for LLMs that embeds synonym substitutions in high-entropy words to detect unauthorized use of training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of verifying whether LLMs were trained on unauthorized datasets, given existing methods' lack of stealth and ease of removal.

**Method:** LexiMark uses synonym substitutions for selected high-entropy words, enhancing memorization without altering text semantics, making the watermark subtle and resistant to detection.

**Key Contributions:**

	1. Introduction of LexiMark, an innovative watermarking method for LLMs.
	2. Demonstration of significant AUROC score improvements over previous watermarking methods.
	3. Evidence of the method's effectiveness in various LLM training scenarios.

**Result:** Evaluation shows significant improvements in AUROC scores over existing methods across various training settings and models, demonstrating the effectiveness of LexiMark.

**Limitations:** 

**Conclusion:** LexiMark provides a reliable means of watermarking textual data for LLMs, ensuring detection of unauthorized data usage while maintaining performance integrity.

**Abstract:** Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.

</details>


### [72] [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)

*Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, Wenqiang Zhang*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Energy-latency attacks, Part-of-Speech characteristics, Generative Path Pruning, Natural Language Processing

**Relevance Score:** 5

**TL;DR:** The paper introduces LingoLoop, an attack on Multimodal Large Language Models (MLLMs) that exploits token-level Part-of-Speech (POS) characteristics to induce excessive verbosity and resource consumption during inference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the vulnerability of MLLMs to energy-latency attacks by incorporating token-level POS characteristics and structural patterns that influence output counts, aiming to enhance the effectiveness of such attacks.

**Method:** LingoLoop utilizes a POS-Aware Delay Mechanism to adjust the generation of the EOS token based on POS information, and a Generative Path Pruning Mechanism to promote repetitive output loops, increasing verbosity during model inference.

**Key Contributions:**

	1. Introduction of LingoLoop, an innovative attack method for MLLMs.
	2. Development of POS-Aware Delay Mechanism for managing EOS token generation.
	3. Implementation of Generative Path Pruning Mechanism to induce repetitive output.

**Result:** Experiments show that LingoLoop can increase generated tokens by up to 30 times and significantly boost energy consumption on models like Qwen2.5-VL-3B, highlighting the vulnerabilities in MLLMs.

**Limitations:** The methods proposed may only be applicable to certain MLLM architectures, and the long-term impacts on model performance are not assessed.

**Conclusion:** The findings reveal critical weaknesses in MLLMs, presenting challenges for their deployment, and the code will be available post-acceptance.

**Abstract:** Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.

</details>


### [73] [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)

*Can Zheng, Jiguang He, Chung G. Kang, Guofa Cai, Zitong Yu, Merouane Debbah*

**Main category:** cs.CL

**Keywords:** neural networks, beam prediction, mMIMO, multi-modal data, large language models

**Relevance Score:** 4

**TL;DR:** A novel neural network framework, M2BeamLLM, enhances beam prediction in mmWave mMIMO systems by integrating multi-modal sensor data and leveraging LLMs for improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve beam prediction accuracy and robustness in mmWave massive multi-input multi-output communication systems by utilizing multi-modal sensor data.

**Method:** M2BeamLLM integrates data from various sensors (images, radar, LiDAR, GPS) and employs techniques like multimodal alignment and supervised fine-tuning to enhance prediction performance.

**Key Contributions:**

	1. Introduction of M2BeamLLM framework for beam prediction
	2. Integration of multi-modal sensor data for improved accuracy
	3. Demonstrated superior performance over traditional DL models

**Result:** M2BeamLLM achieves significantly higher accuracy and robustness in beam prediction compared to traditional deep learning models, especially in few-shot scenarios, benefiting from increased diversity in sensing modalities.

**Limitations:** 

**Conclusion:** The proposed framework provides an efficient solution for intelligent beam prediction in vehicle-to-infrastructure communication systems, outperforming existing methods.

**Abstract:** This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.

</details>


### [74] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)

*Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin*

**Main category:** cs.CL

**Keywords:** Weight Decay, Large Language Models, Heavy-Tailed Self-Regularization, Spectral Properties, Model Generalization

**Relevance Score:** 8

**TL;DR:** Introducing AlphaDecay, a method for assigning adaptive weight decay rates in large language models based on spectral properties to enhance model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better weight decay strategies in large language models that consider the structural diversity and spectral properties of different modules.

**Method:** AlphaDecay assigns varying weight decay strengths to each module of an LLM based on their empirical spectral density (ESD) analyzed through Heavy-Tailed Self-Regularization theory.

**Key Contributions:**

	1. Introduction of AlphaDecay for adaptive weight decay in LLMs
	2. Use of Heavy-Tailed Self-Regularization theory for decay assignment
	3. Demonstrated performance improvements across varying model sizes.

**Result:** AlphaDecay shows improved perplexity and generalization on pre-training tasks across various model sizes, outperforming uniform decay and other adaptive methods.

**Limitations:** 

**Conclusion:** Tailored weight decay assignments substantially improve performance by addressing module-wise differences in spectral properties in LLMs.

**Abstract:** Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [75] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)

*David Wan, Eran Hirsch, Elias Stengel-Eskin, Ido Dagan, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** large language models, attribution, interperability, text generation, modular framework

**Relevance Score:** 8

**TL;DR:** The paper introduces GenerationPrograms, a modular framework for improving attribution in LLM-generated texts by decomposing the generation process into programmatic stages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recent large language models struggle with providing clear and accurate attributions for their text outputs, affecting verifiability and trust. Existing methods are limited in interpretability.

**Method:** GenerationPrograms separates text generation into two stages: creating a program plan with modular text operations tailored to the query, and executing these operations to produce the final output.

**Key Contributions:**

	1. Introduction of a two-stage modular generation framework
	2. Enhanced attribution quality through localized refinement
	3. Outperformance of conventional attribution techniques

**Result:** Empirical evaluations show that GenerationPrograms enhances attribution quality at both document and sentence levels across various tasks, and it outperforms traditional attribution methods.

**Limitations:** 

**Conclusion:** The framework not only improves attribution quality but also allows for modular refinements, enhancing the interpretability of the generation process.

**Abstract:** Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [76] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)

*Ahmed Heakl, Sarim Hashmi, Chaimaa Abi, Celine Lee, Abdulrahman Mahmoud*

**Main category:** cs.CL

**Keywords:** transpilation, large language models, ISA, CISC, RISC

**Relevance Score:** 4

**TL;DR:** Introducing GG, an ISA-centric transpilation pipeline that leverages large language models for effective CISC to RISC code translation, achieving high correctness and performance metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for efficient transpilation of low-level programs across different instruction set architectures (ISAs) to enhance code portability and longevity.

**Method:** The GG pipeline combines the power of pre-trained LLMs with a software testing framework to generate and validate translations between ISAs, specifically targeting CISC and RISC architectures.

**Key Contributions:**

	1. Development of the GG transpilation pipeline
	2. High code coverage and functional correctness on diverse benchmarks
	3. Demonstrated performance improvements over existing frameworks

**Result:** GG achieves over 98% code coverage in unit tests and semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, outperforming the state-of-the-art Rosetta 2 framework in performance and resource efficiency.

**Limitations:** 

**Conclusion:** The GG approach demonstrates significant improvements in runtime, energy efficiency, and memory usage for transpiled code, providing a robust foundation for future ISA-level code translation research.

**Abstract:** The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [77] [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)

*Junghyun Min, Xiulin Yang, Shira Wein*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Abstract Meaning Representation, pretrained language models, GPT-4o, semantic reasoning

**Relevance Score:** 6

**TL;DR:** This paper investigates the impact of adding Abstract Meaning Representation (AMR) to pretrained language models for Natural Language Inference (NLI).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether integrating AMR can improve generalization in NLI tasks for pretrained language models.

**Method:** The authors conducted experiments using AMR in both fine-tuning and prompting scenarios with pretrained models like GPT-4o.

**Key Contributions:**

	1. Demonstrated AMR's negative impact on fine-tuning NLI tasks.
	2. Showed that prompting with AMR can lead to minor performance gains.
	3. Identified the risks of models misinterpreting relevance due to superficial amplification.

**Result:** Incorporating AMR during fine-tuning hindered generalization, while prompting with AMR yielded slight improvements; however, these improvements were attributed to surface-level differences.

**Limitations:** The results indicate that improvements are tied to surface-level distinctions rather than deeper semantic understanding, which is a limitation of using AMR in this context.

**Conclusion:** The study concludes that the addition of AMR can mislead models in NLI, as they may rely on superficial differences rather than true semantic reasoning.

**Abstract:** Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.

</details>


### [78] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)

*Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** Moral reasoning, Large Language Models, Model alignment, Collective moral judgment, Embedding optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework to improve moral reasoning in LLMs by aggregating their judgments and realigning misaligned models, leading to more consistent AI systems.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To address discrepancies in moral judgments by large language models when faced with complex dilemmas, aiming for improved consistency and reliability.

**Method:** The framework synthesizes multiple LLMs' moral judgments into a collective moral judgment, incorporating a mechanism that weights contributions based on model reliability, and employs embedding-optimization for misaligned models.

**Key Contributions:**

	1. Framework for synthesizing moral judgments from multiple LLMs
	2. Embedding-optimization procedure for aligning diverging models
	3. Demonstrated improvements in moral judgment consistency through experiments

**Result:** Experiments demonstrated that the proposed method effectively builds consensus among LLMs and enhances the fidelity of individual models, showcasing a reduction in divergence in moral judgments.

**Limitations:** The framework's effectiveness may be limited by the diversity of training data and the inherent biases present in the individual models.

**Conclusion:** The approach underscores the importance of data-driven moral alignment across models, suggesting it leads to safer and more consistent AI systems.

**Abstract:** Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [79] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)

*Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessika Daikeler*

**Main category:** cs.CL

**Keywords:** Large Language Models, Survey Research, Open-ended Responses, Coding, German Data

**Relevance Score:** 7

**TL;DR:** This study investigates the effectiveness of various LLMs in coding open-ended survey responses, particularly using German data, and compares their performance to established methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs as efficient alternatives for manually coding open-ended survey responses, particularly in contexts beyond English and in complex topics.

**Method:** The study compares various state-of-the-art LLMs and prompting approaches against human expert codings while analyzing performance differences based on different prompting techniques and LLMs used.

**Key Contributions:**

	1. Comparison of multiple LLMs on German open-ended survey data
	2. Insights into the performance variation of LLMs based on prompting methods
	3. Discussion of implications for methodological research and practical application in survey research.

**Result:** Performance varies widely among LLMs; only a fine-tuned model achieves satisfactory results in predictive performance. Performance differences in prompting affect the output depending on the LLM.

**Limitations:** Performance variability may not generalize across other languages or less common topics; the necessity of fine-tuning raises questions about accessibility.

**Conclusion:** This research underscores the need for careful consideration of LLM selection and prompting strategies for coding open-ended survey responses, highlighting significant trade-offs for researchers implementing automated methods.

**Abstract:** The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [80] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)

*Xiang Cheng, Chengyan Pan, Minjun Zhao, Deyang Li, Fangchao Liu, Xinyu Zhang, Xiao Zhang, Yong Liu*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Chain-of-Thought, Large Language Models, mathematical reasoning, exemplars

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of Chain-of-Thought (CoT) exemplars in enhancing reasoning performance of recent Large Language Models (LLMs), finding that they do not improve performance compared to Zero-Shot CoT.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the impact of Chain-of-Thought (CoT) exemplars on the reasoning capabilities of advanced LLMs, especially in mathematical tasks, amidst continuous advancements in model capabilities.

**Method:** Systematic experiments are conducted using strong models like Qwen2.5 series and enhanced CoT exemplars constructed from outputs of advanced models, comparing their performances in reasoning tasks.

**Key Contributions:**

	1. Analysis of the effectiveness of traditional and enhanced CoT exemplars in recent LLMs.
	2. Demonstration that CoT exemplars primarily serve to align output format rather than improve reasoning.
	3. Call for a re-examination of the ICL paradigm and exemplar definitions in the context of advanced LLMs.

**Result:** The experiments show that traditional CoT exemplars do not improve reasoning performance over Zero-Shot CoT, and enhanced exemplars also fail to provide observable gains as models tend to ignore exemplars in favor of direct instructions.

**Limitations:** The study highlights that models often ignore exemplars and focuses more on instructions, which limits the potential of CoT exemplars to improve reasoning skills.

**Conclusion:** The findings emphasize the limitations of the current ICL+CoT framework in mathematical reasoning, suggesting a need for a re-evaluation of the ICL paradigm and how exemplars are defined.

**Abstract:** In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [81] [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)

*. Pazzaglia, V. Vendetti, L. D. Comencini, F. Deriu, V. Modugno*

**Main category:** cs.CL

**Keywords:** Large Language Models, Polarization, Political Discourse, Ethics in AI, Disinformation

**Relevance Score:** 8

**TL;DR:** This study investigates how fine-tuned large language models (LLMs) can generate polarized content, using Reddit discussions as a dataset, revealing ethical implications for AI in political discourse.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses concerns about LLMs exacerbating ideological polarization and the risks associated with their persuasive capabilities in political contexts.

**Method:** The authors fine-tune an open-source LLM using a dataset of politically charged discussions from Reddit, analyzing the model's outputs through linguistic analysis, sentiment scoring, and human annotation.

**Key Contributions:**

	1. Investigation of LLMs' role in polarizing discourse
	2. Demonstration of LLMs' capability to produce human-like partisan content
	3. Discussion of ethical implications in AI-driven political communication

**Result:** The findings show that LLMs trained on partisan data can produce content that is highly plausible and indistinguishable from human-generated comments, raising ethical concerns regarding their role in disinformation.

**Limitations:** 

**Conclusion:** The paper highlights the need for AI governance and platform regulation to mitigate risks associated with adversarial fine-tuning of LLMs for political manipulation.

**Abstract:** The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

</details>


### [82] [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)

*Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Wenyue Li, Hayden Kwok-Hay So, Ngai Wong*

**Main category:** cs.CL

**Keywords:** Parameter-efficient fine-tuning, Low-Rank Adaptation, Mixture-of-Experts, Guided Selection Vectors, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper introduces GuiLoMo, a new strategy for fine-tuning large language models by optimizing expert numbers and ranks using guided selection vectors, enhancing the efficiency of parameter-efficient fine-tuning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current PEFT methods, specifically LoRA and LoRA-MoE, to improve adaptation of large language models while maintaining representational diversity and efficiency.

**Method:** GuiLoMo utilizes a bilevel optimization process to learn Guided Selection Vectors, which inform the layer-wise allocation of expert numbers and ranks in adapting large language models.

**Key Contributions:**

	1. Introduction of GuiLoMo for fine-grained expert allocation in language models
	2. Demonstration of superior performance against existing fine-tuning baselines
	3. Insights into the layer-wise variation of expert configuration

**Result:** Experiments show that GuiLoMo achieves superior or comparable performance to existing baselines across multiple models and benchmarks, revealing insights into adaptive configurations of experts.

**Limitations:** 

**Conclusion:** The findings demonstrate that tailored expert configurations across layers and tasks lead to improved performance and highlight the potential of adaptive expert numbers and ranks.

**Abstract:** Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

</details>


### [83] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)

*Yuto Harada, Yusuke Yamauchi, Yusuke Oda, Yohei Oseki, Yusuke Miyao, Yu Takagi*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Large Language Models, Dataset Properties, Model-Specific Strategies, Performance Prediction

**Relevance Score:** 9

**TL;DR:** This paper investigates supervised fine-tuning (SFT) of large language models (LLMs), analyzing the effects of dataset properties and layer-wise modifications on model performance across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the nuances of supervised fine-tuning (SFT) in aligning LLMs with human instructions and values is crucial for improving their effectiveness.

**Method:** A variety of base models were trained on numerous datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in over 1,000 SFT models, with a focus on dataset properties and layer-wise changes during SFT.

**Key Contributions:**

	1. Identification of critical dataset properties for SFT effectiveness.
	2. Revelation of synergies and variances in training-task performance across models.
	3. Establishment of perplexity as a significant predictor of SFT success.

**Result:** The study identifies which dataset properties are critical, reveals persistent training-task synergies across models, and establishes that perplexity is a reliable predictor of SFT effectiveness.

**Limitations:** 

**Conclusion:** The findings highlight the need for model-specific fine-tuning strategies and the correlation between mid-layer weight changes and performance improvements; over 1,000 SFT models and benchmark results will be shared to facilitate future research.

**Abstract:** Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.

</details>


### [84] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)

*Daniel D'souza, Julia Kreutzer, Adrien Morisot, Ahmet Üstün, Sara Hooker*

**Main category:** cs.CL

**Keywords:** machine learning, long-tail problem, training protocols, inference, underrepresented features

**Relevance Score:** 8

**TL;DR:** This paper addresses the challenge of improving machine learning model performance on rare and underrepresented features by optimizing training protocols and implementing a taxonomy for control over generative attributes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Modern machine learning struggles with performing well on long-tail features, which are underrepresented in training datasets, leading to poor model adaptability and output quality.

**Method:** The authors propose an optimized training approach that includes a taxonomy of data characteristics and fine-tuning to help models better adapt at inference time, with a focus on enhancing performance for underrepresented use cases.

**Key Contributions:**

	1. Creation of a detailed taxonomy for controlling generative attributes
	2. Development of an automatic inference marker system
	3. Significant performance improvements for long-tail use cases

**Result:** The approach yields an average lift of 5.7% in open-ended generation quality and over 9.1% in underrepresented domains, with relative lifts of up to 14.1% on specific tasks like CodeRepair.

**Limitations:** 

**Conclusion:** The findings suggest that improved training protocols can significantly enhance model performance and controllability on rare task components, effectively bridging the gap between training and inference.

**Abstract:** One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [85] [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)

*Anton Changalidis, Aki Härmä*

**Main category:** cs.CL

**Keywords:** generative transformers, memorization capacity, model architecture, SNOMED, activation functions

**Relevance Score:** 7

**TL;DR:** This paper examines how model architecture and data configurations affect the memorization capacity of generative transformers, revealing key factors like embedding size and activation functions in optimizing model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and optimize the memorization mechanisms of generative transformers in the context of structured real-world data.

**Method:** The models are trained on synthetic text datasets derived from the SNOMED knowledge graph, using triplets and sequences to evaluate empirical memorization capacity.

**Key Contributions:**

	1. Identification of embedding size as a key factor in learning speed and capacity
	2. Demonstration of limited benefits from additional layers
	3. Insights on the role of activation functions, particularly Softmax, in model performance

**Result:** Embedding size significantly influences learning speed and capacity; additional layers have limited advantages and can hinder simpler datasets. Softmax is highlighted for its stability and capacity in performance.

**Limitations:** 

**Conclusion:** The findings provide insights into transformer memory mechanisms and a framework for optimizing model design, suggesting that more complex datasets enhance memorization capabilities.

**Abstract:** This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.

</details>


### [86] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)

*Ring Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, Zujie Wen*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, large language model, reinforcement learning, parameter efficiency, multi-domain training

**Relevance Score:** 7

**TL;DR:** Introduction of Ring-lite, a Mixture-of-Experts model optimized via reinforcement learning for efficient reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning capabilities in large language models while maintaining efficiency in parameter activation and computational throughput.

**Method:** A joint training pipeline combining distillation with reinforcement learning (RL), employing a novel approach called Constrained Contextual Computation Policy Optimization (C3PO) for training stability and performance.

**Key Contributions:**

	1. Introduction of C3PO for stable RL training
	2. Successful integration of distillation checkpoints based on entropy loss
	3. Two-stage training paradigm for multi-domain data harmonization

**Result:** Ring-lite achieves state-of-the-art performance on reasoning tasks while activating only one-third of the parameters compared to similar models.

**Limitations:** Challenges in optimization stability during RL training and domain conflicts in mixed dataset training.

**Conclusion:** The proposed methods improve training stability and efficiency in large language models, addressing challenges in reinforcement learning and multi-domain data integration.

**Abstract:** We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [87] [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)

*Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Language Models, Exploratory Reasoning, Entropy, AI

**Relevance Score:** 9

**TL;DR:** This paper proposes a minimal modification to standard reinforcement learning that enhances exploratory reasoning in language models by incorporating an entropy-based term into the advantage function, resulting in significant performance improvements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance plateaus in language models due to over-exploitation in reinforcement learning, we revisit the concept of entropy as a signal for exploration.

**Method:** We introduce a modification to standard RL that adds an entropy-based term to the advantage function, promoting longer and deeper reasoning chains instead of merely increasing uncertainty.

**Key Contributions:**

	1. Introduces an entropy-based term to RL advantage functions to enhance exploration in LM reasoning.
	2. Identifies strong correlations between exploratory reasoning and high-entropy regions in outputs.
	3. Achieves significant performance improvements in LM capabilities, particularly on the Pass@K metric.

**Result:** The proposed method shows significant performance gains measured by the Pass@K metric, especially with very large K values, enhancing LM reasoning capabilities.

**Limitations:** 

**Conclusion:** Incorporating entropy into RL helps in balancing exploration and exploitation, yielding substantial improvements in LM reasoning capabilities.

**Abstract:** Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.

</details>


### [88] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)

*Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, David Lopez-Paz*

**Main category:** cs.CL

**Keywords:** autoregressive U-Net, dynamic tokenization, language modeling

**Relevance Score:** 7

**TL;DR:** This paper introduces an autoregressive U-Net model that learns to embed tokens dynamically during training, allowing for flexible tokenization and improved performance in various language tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of fixed tokenization methods like BPE that restrict how language models operate on data and predict future tokens.

**Method:** The proposed autoregressive U-Net processes raw bytes into a hierarchical structure of tokens, enabling it to make predictions at different granularities, from individual bytes to groups of words.

**Key Contributions:**

	1. Introduces a dynamic tokenization method within a U-Net architecture.
	2. Demonstrates the ability to predict at multiple granularity levels in language tasks.
	3. Shows potential for improved performance in low-resource language settings.

**Result:** The model achieves performance comparable to strong BPE baselines, with deeper hierarchies showing promising trends in broader semantic pattern recognition.

**Limitations:** 

**Conclusion:** The dynamic tokenization approach offers advantages for character-level tasks and supports knowledge transfer across low-resource languages.

**Abstract:** Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


### [89] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)

*Li-Wei Chen, Takuya Higuchi, Zakaria Aldeneh, Ahmed Hussen Abdelaziz, Alexander Rudnicky*

**Main category:** cs.CL

**Keywords:** speech modeling, large language models, variational approach, paralinguistic attributes, speech generation

**Relevance Score:** 8

**TL;DR:** This paper presents an end-to-end variational approach to improve speech generation by automatically encoding paralinguistic attributes into semantic tokens, enhancing the naturalness of generated speech without manual feature engineering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models have demonstrated success in text processing, prompting their application in speech modeling. However, traditional methods often ignore vital prosodic aspects of speech, leading to unnatural output.

**Method:** The proposed method employs a variational approach to automatically learn continuous speech attributes, which are integrated with semantic tokens to improve speech generation.

**Key Contributions:**

	1. Introduces an end-to-end variational approach for encoding paralinguistic attributes in speech generation.
	2. Eliminates the need for manual extraction of speech features.
	3. Achieves superior speech naturalness as validated by human raters.

**Result:** The approach enhances the quality of generated speech, yielding more natural continuations preferred by human raters compared to existing methods that rely solely on pitch features.

**Limitations:** 

**Conclusion:** This end-to-end model offers a more efficient and effective way to generate speech, without the drawbacks of manual feature extraction and selection, leading to more human-like speech outputs.

**Abstract:** The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


### [90] [Compression of enumerations and gain](https://arxiv.org/abs/2304.03030)

*George Barmpalias, Xiaoyan Zhang, Bohua Zhan*

**Main category:** cs.CL

**Keywords:** compressibility, Kolmogorov complexity, computably enumerable sets

**Relevance Score:** 2

**TL;DR:** This paper explores compressibility in enumerations through Kolmogorov complexity, demonstrating forms of compression for computably enumerable sets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the compressibility of enumerations in the context of Kolmogorov complexity and its implications on auxiliary information.

**Method:** The study involves analyzing strong and weak forms of compression for computably enumerable (c.e.) sets and using enumeration games to investigate their properties.

**Key Contributions:**

	1. Existence of strong compression for c.e. sets
	2. Establishment of weak gainless compression
	3. Reduction of density problem to well-compressibility question

**Result:** Demonstrates the existence of strong compression and weak gainless compression for any c.e. set, and relates the density problem of c.e. sets to their compressibility.

**Limitations:** 

**Conclusion:** The findings offer insights into the compressibility of c.e. sets, suggesting that the concept of well-compressibility is central to understanding their prefix complexity.

**Abstract:** We study the compressibility of enumerations in the context of Kolmogorov complexity, focusing on strong and weak forms of compression and their gain: the amount of auxiliary information embedded in the compressed enumeration. The existence of strong compression and weak gainless compression is shown for any computably enumerable (c.e.) set. The density problem of c.e. sets with respect to their prefix complexity is reduced to the question of whether every c.e. set is well-compressible, which we study via enumeration games.

</details>


### [91] [FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback](https://arxiv.org/abs/2307.10867)

*Ashish Singh, Ashutosh Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nesreen K. Ahmed, Puneet Mathur, Erik Learned-Miller, Franck Dernoncourt, Ryan A. Rossi*

**Main category:** cs.CL

**Keywords:** figure captioning, human feedback, reinforcement learning, benchmark dataset

**Relevance Score:** 7

**TL;DR:** A new framework, FigCaps-HF, for generating high-quality figure captions in scientific documents using reinforcement learning with human feedback (RLHF) to optimize for reader preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for generating figure captions from scientific documents often produce subpar results concerning helpfulness, explainability, and visual-descriptiveness, leading to misalignment with reader preferences.

**Method:** The FigCaps-HF framework includes an automatic evaluation method for figure-caption quality and a RLHF approach to enhance generative models based on expert feedback.

**Key Contributions:**

	1. Introduction of FigCaps-HF framework for figure-caption generation.
	2. Development of an automatic evaluation method for figure-caption pairs.
	3. Release of a large-scale benchmark dataset for further research.

**Result:** When BLIP is used as the base model, the RLHF framework shows a significant improvement: 35.7% in ROUGE, 16.9% in BLEU, and 9% in Meteor over standard fine-tuning methods.

**Limitations:** 

**Conclusion:** The framework demonstrates improved caption generation performance and provides a benchmark dataset with human feedback, paving the way for better evaluation of RLHF techniques in figure-caption generation.

**Abstract:** Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.

</details>


### [92] [Exploring news intent and its application: A theory-driven approach](https://arxiv.org/abs/2312.16490)

*Zhengjia Wang, Danding Wang, Qiang Sheng, Juan Cao, Siyuan Ma, Haonan Cheng*

**Main category:** cs.CL

**Keywords:** news intent, intent perception, fake news detection, computational social science, action-based cognition

**Relevance Score:** 4

**TL;DR:** This paper introduces a framework for understanding news intent and presents a dataset to aid in news-related tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of structured investigations into perceived news intent and its importance in public discourse.

**Method:** The authors review interdisciplinary studies on intentional action and propose a deconstruction-based framework called the News Intent Understanding Framework (NINT), which includes a new intent perception dataset.

**Key Contributions:**

	1. Introduction of the News Intent Understanding Framework (NINT).
	2. Development of a new intent perception dataset.
	3. Demonstration of improved fake news detection using intent assistance.

**Result:** The study shows that using the intent framework can significantly enhance performance in tasks such as fake news detection by +2.2% macF1.

**Limitations:** 

**Conclusion:** The findings aim to provide insights into intent cognition and computational social science applications, particularly in news dissemination and interpretation.

**Abstract:** Understanding the intent behind information is crucial. However, news as a medium of public discourse still lacks a structured investigation of perceived news intent and its application. To advance this field, this paper reviews interdisciplinary studies on intentional action and introduces a conceptual deconstruction-based news intent understanding framework (NINT). This framework identifies the components of intent, facilitating a structured representation of news intent and its applications. Building upon NINT, we contribute a new intent perception dataset. Moreover, we investigate the potential of intent assistance on news-related tasks, such as significant improvement (+2.2% macF1) in the task of fake news detection. We hope that our findings will provide valuable insights into action-based intent cognition and computational social science.

</details>


### [93] [Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification](https://arxiv.org/abs/2402.10735)

*John Dougrez-Lewis, Mahmud Elahi Akhter, Federico Ruggeri, Sebastian Löbbers, Yulan He, Maria Liakata*

**Main category:** cs.CL

**Keywords:** LLM, claim verification, deductive reasoning, abductive reasoning, RECV benchmark

**Relevance Score:** 8

**TL;DR:** The paper examines the reasoning capabilities of LLMs in claim verification, proposing a framework and benchmark (RECV) to evaluate deductive and abductive reasoning across multiple datasets.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the open problem of LLM reasoning capabilities beyond Mathematics and Coding by focusing on claim verification.

**Method:** A framework is proposed to break down claims and evidence into atomic reasoning types, leading to the creation of the RECV benchmark for evaluating LLMs.

**Key Contributions:**

	1. Introduction of a claim verification framework for LLMs
	2. Creation of the RECV benchmark with real-world claims
	3. Evaluation of deductive vs. abductive reasoning capabilities in LLMs

**Result:** LLMs perform well on deductive reasoning but fail in abductive reasoning, and enhancing LLMs with generated rationales does not always yield benefits. Generated rationales are semantically similar to human ones in deductive cases.

**Limitations:** The effectiveness of the proposed framework and benchmark in different domains is yet to be fully tested.

**Conclusion:** While LLMs can handle deductive reasoning, improvements are needed for abductive reasoning; the utility of rationale generation varies by context.

**Abstract:** Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create RECV, the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of increasing complexity. We evaluate three state-of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases.

</details>


### [94] [Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora](https://arxiv.org/abs/2406.13677)

*Erik Derner, Sara Sansalvador de la Fuente, Yoan Gutiérrez, Paloma Moreda, Nuria Oliver*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, NLP, multilingual, corpus analysis

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel method for detecting and quantifying gender representation bias in LLM training data for gendered languages, revealing significant male-dominant imbalances and demonstrating that small-scale training on datasets biased towards the opposite gender can mitigate such biases.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the often overlooked issue of gender representation bias in LLM training data, emphasizing how imbalances can propagate through the model lifecycle.

**Method:** The proposed method leverages the contextual understanding of LLMs to automatically identify and classify person-referencing words within gendered language corpora.

**Key Contributions:**

	1. Novel LLM-based method for detecting gender representation bias
	2. Reveals male-dominant imbalances in Spanish-English and Valencian corpora
	3. Findings advocate for corpus-level bias analysis in multilingual NLP

**Result:** The analysis on four Spanish-English benchmarks and five Valencian corpora reveals substantial male-dominant biases in training data, which subsequently affect model outputs.

**Limitations:** 

**Conclusion:** The results underline the necessity of conducting corpus-level gender bias analysis in multilingual NLP, stressing the potential for mitigation through targeted training.

**Abstract:** Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias - the association of specific roles or traits with a particular gender - in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias - the unequal frequency of references to individuals of different genders - in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.

</details>


### [95] [ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models](https://arxiv.org/abs/2407.07313)

*Benjamin G. Ascoli, Yasoda Sai Ram Kandikonda, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Natural Language Processing, Large Language Models, Evaluation Metrics, Enhanced Tree Matching

**Relevance Score:** 8

**TL;DR:** Introduction of a new metric, Enhanced Tree Matching (ETM), for evaluating Text-to-SQL tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation metrics for Text-to-SQL, namely Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM), have significant limitations in accurately assessing performance, especially for LLM-based models.

**Method:** The paper presents Enhanced Tree Matching (ETM), which considers both syntactic and semantic elements in query comparison to overcome the shortcomings of ESM and EXE.

**Key Contributions:**

	1. Introduction of Enhanced Tree Matching (ETM) metric for Text-to-SQL evaluation
	2. Demonstration of significant improvements in evaluation accuracy over existing metrics
	3. Release of ETM script as open source for community use

**Result:** ETM demonstrates significantly reduced false positive and negative rates in evaluation compared to EXE and ESM, with rates of 0.3% and 2.7% versus their counterparts reaching as high as 23.0% and 28.9%.

**Limitations:** 

**Conclusion:** The introduction of ETM provides the community with a more effective metric for evaluating Text-to-SQL systems, facilitating better performance assessment of models, particularly those leveraging LLMs.

**Abstract:** The task of Text-to-SQL enables anyone to retrieve information from SQL databases using natural language. While this task has made substantial progress, the two primary evaluation metrics - Execution Accuracy (EXE) and Exact Set Matching Accuracy (ESM) - suffer from inherent limitations that can misrepresent performance. Specifically, ESM's rigid matching overlooks semantically correct but stylistically different queries, whereas EXE can overestimate correctness by ignoring structural errors that yield correct outputs. These shortcomings become especially problematic when assessing outputs from large language model (LLM)-based approaches without fine-tuning, which vary more in style and structure compared to their fine-tuned counterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM), which mitigates these issues by comparing queries using both syntactic and semantic elements. Through evaluating nine LLM-based models, we show that EXE and ESM can produce false positive and negative rates as high as 23.0% and 28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release our ETM script as open source, offering the community a more robust and reliable approach to evaluating Text-to-SQL.

</details>


### [96] [Uncovering Overfitting in Large Language Model Editing](https://arxiv.org/abs/2410.07819)

*Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Large Language Models, Editing Overfit, Inference Constraints, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper addresses the issue of Editing Overfit in knowledge editing for Large Language Models, proposing a new plug-and-play strategy, Learn the Inference, to improve knowledge recall.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve knowledge editing methods for Large Language Models (LLMs), particularly in complex tasks where existing methods struggle due to Editing Overfit.

**Method:** The authors introduce the EVOKE benchmark for evaluating Editing Overfit and propose the Learn the Inference (LTI) strategy, which utilizes a Multi-stage Inference Constraint module to enhance knowledge recall.

**Key Contributions:**

	1. Introduction of the term Editing Overfit related to knowledge editing in LLMs.
	2. Development of the EVOKE benchmark for evaluating knowledge editing performance.
	3. Proposal of the Learn the Inference (LTI) strategy to enhance knowledge recall in edited models.

**Result:** Experiments show that Editing Overfit is common in current editing methods, and the proposed LTI strategy significantly mitigates this issue across various tasks.

**Limitations:** The proposed methods need further evaluation across more complex scenarios and diverse LLM architectures.

**Conclusion:** The paper concludes that existing overfitting mitigation strategies fail in knowledge editing, but the LTI approach effectively improves the generalization of new knowledge.

**Abstract:** Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs' knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn the Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit.

</details>


### [97] [Beyond Browsing: API-Based Web Agents](https://arxiv.org/abs/2410.16464)

*Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig*

**Main category:** cs.CL

**Keywords:** AI agents, APIs, web browsing, hybrid agents, WebArena

**Relevance Score:** 8

**TL;DR:** This paper explores the performance of AI agents that utilize APIs as opposed to traditional web browsing, proposing two types of agents, and demonstrating superior results for API-based interactions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the potential of AI agents to access online content via APIs rather than just through web browsers, aiming to improve task performance in digital environments.

**Method:** The research introduces two types of agents: API-calling agents, which perform tasks solely through APIs, and Hybrid Agents, which use both browsing and APIs. Experiments were conducted on WebArena, a benchmark for web navigation tasks.

**Key Contributions:**

	1. Proposed API-calling and Hybrid Agents for online task execution
	2. Demonstrated superior performance of API-based interactions over traditional web browsing
	3. Achieved state-of-the-art performance metrics among task-agnostic agents.

**Result:** API-Based Agents outperformed Browsing Agents, while Hybrid Agents consistently achieved better results than both, with a success rate of 38.9%—marking a 24% improvement over browsing alone.

**Limitations:** 

**Conclusion:** The findings indicate that leveraging APIs can significantly enhance task success rates for AI agents, suggesting that APIs should be prioritized when available instead of relying solely on web browsing.

**Abstract:** Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by Browsing Agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-Based Agents outperform web Browsing Agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 24.0% absolute improvement over web browsing alone, achieving a success rate of 38.9%, the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.

</details>


### [98] [Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework](https://arxiv.org/abs/2410.18653)

*Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias Aßenmacher*

**Main category:** cs.CL

**Keywords:** text generation, evaluation metrics, decoding strategies, natural language processing, large language models

**Relevance Score:** 8

**TL;DR:** This paper proposes novel methods for multicriteria evaluation of open-ended text generation, addressing challenges in decoding strategy assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for a more effective evaluation framework for open-ended text generation, due to the limitations of existing metrics in assessing quality.

**Method:** The paper introduces benchmarking approaches based on partial orderings and a new summary metric to evaluate decoding methods holistically.

**Key Contributions:**

	1. Novel multicriteria evaluation methods for text generation
	2. Benchmarking approaches using partial orderings
	3. A holistic summary metric for decoding strategies

**Result:** The proposed evaluation strategies allow for robust comparisons of decoding approaches, aiding in model selection for text generation tasks.

**Limitations:** 

**Conclusion:** The authors recommend further improvements in evaluation methodologies and share their resources publicly for community use.

**Abstract:** Open-ended text generation has become a prominent task in natural language processing due to the rise of powerful (large) language models. However, evaluating the quality of these models and the employed decoding strategies remains challenging due to trade-offs among widely used metrics such as coherence, diversity, and perplexity. This paper addresses the specific problem of multicriteria evaluation for open-ended text generation, proposing novel methods for both relative and absolute rankings of decoding methods. Specifically, we employ benchmarking approaches based on partial orderings and present a new summary metric to balance existing automatic indicators, providing a more holistic evaluation of text generation quality. Our experiments demonstrate that the proposed approaches offer a robust way to compare decoding strategies and serve as valuable tools to guide model selection for open-ended text generation tasks. We suggest future directions for improving evaluation methodologies in text generation and make our code, datasets, and models publicly available.

</details>


### [99] [Ensemble Watermarks for Large Language Models](https://arxiv.org/abs/2411.19563)

*Georg Niess, Roman Kern*

**Main category:** cs.CL

**Keywords:** watermarking, large language models, text detection, accountability, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method for AI-generated text watermarking that combines multiple features, achieving high detection rates even after paraphrasing attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Reliable distinction between AI-generated and human-authored text is vital as LLMs achieve human-like fluency; existing watermarks are inflexible and vulnerable to paraphrasing.

**Method:** A multi-feature ensemble watermarking method combines acrostica, sensorimotor norms, and red-green watermarking techniques to enhance detection capabilities.

**Key Contributions:**

	1. Proposed an ensemble method for watermarking AI-generated text.
	2. Demonstrated high detection rates before and after paraphrasing attacks.
	3. Provided flexibility in addressing different watermarking requirements.

**Result:** Achieved a 98% detection rate across several LLMs with the ensemble watermark, maintaining 95% detection rate after paraphrasing attacks.

**Limitations:** 

**Conclusion:** The ensemble method offers flexibility for addressing various detection requirements while ensuring high performance across configurations.

**Abstract:** As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.

</details>


### [100] [BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English](https://arxiv.org/abs/2412.04726)

*Dipankar Srirag, Aditya Joshi, Jordan Painter, Diptesh Kanojia*

**Main category:** cs.CL

**Keywords:** sentiment analysis, language variety, LLM, bias, sarcasm classification

**Relevance Score:** 9

**TL;DR:** BESSTIE is a benchmark dataset for sentiment and sarcasm classification across three English varieties: Australian, Indian, and British, aimed at improving LLM fairness and performance across diverse language varieties.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of labelled datasets for sentiment analysis in different English varieties, aiming to enhance LLM performance and fairness across these varieties.

**Method:** We collected datasets using location-based scraping from Google Places reviews and topic-based filtering from Reddit comments, followed by manual and automatic validation by native speakers.

**Key Contributions:**

	1. Introduction of the BESSTIE benchmark dataset for English sentiment and sarcasm classification.
	2. Demonstration of LLM performance disparity across different English varieties.
	3. Insights into the necessity for language variety-specific datasets for improved model training.

**Result:** Our study reveals that models perform better on inner-circle English varieties (en-AU, en-UK) than on en-IN, especially in sarcasm classification, thus revealing challenges in cross-variety generalization.

**Limitations:** Challenges in cross-variety generalization were noted, indicating a need for more diverse data sources and further validation.

**Conclusion:** BESSTIE provides a necessary benchmark for future research on equitable LLMs, highlighting the importance of variety-specific datasets for improving model performance across diverse English dialects.

**Abstract:** Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.

</details>


### [101] [Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression](https://arxiv.org/abs/2412.05693)

*Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh*

**Main category:** cs.CL

**Keywords:** KV cache, inference, GPU memory, batch size, token generation

**Relevance Score:** 6

**TL;DR:** This paper presents a novel approach to KV cache management that compresses the cache during input processing to enhance throughput in AI inference.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in key-value (KV) caching for improved inference speed and GPU memory utilization in settings with long input contexts.

**Method:** The authors propose an eviction policy that compresses the KV cache during the input processing phase to allow for larger batch sizes.

**Key Contributions:**

	1. Introduction of a dual-phase KV cache compression technique
	2. Demonstration of increased throughput with larger batch sizes
	3. Validation of model accuracy maintenance alongside performance improvements.

**Result:** This method enables significantly higher throughput for token generation while preserving the original accuracy of the model.

**Limitations:** 

**Conclusion:** Compressing the KV cache during input processing enhances inference performance in limited GPU memory contexts without sacrificing accuracy.

**Abstract:** Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

</details>


### [102] [ClusterChat: Multi-Feature Search for Corpus Exploration](https://arxiv.org/abs/2412.14533)

*Ashish Chouhan, Saifeldin Mandour, Michael Gertz*

**Main category:** cs.CL

**Keywords:** Corpus Exploration, Textual Embeddings, Question Answering, Cluster-based Organization, Scalability

**Relevance Score:** 7

**TL;DR:** ClusterChat is an open-source system designed for large-scale text corpus exploration, particularly useful in biomedical, finance, and legal domains.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional keyword-based search methods fail to facilitate effective exploration of vast document collections, necessitating innovative solutions for better insight extraction.

**Method:** ClusterChat integrates cluster-based document organization using text embeddings, along with features like lexical and semantic search, timeline-driven exploration, and multi-feature question answering (QA).

**Key Contributions:**

	1. Open-source design of ClusterChat for corpus exploration
	2. Integration of cluster-based organization and advanced search capabilities
	3. Demonstration of effectiveness through case studies on a large dataset

**Result:** Validation through case studies on a four million abstract PubMed dataset shows that ClusterChat significantly enhances corpus exploration by offering context-aware insights while being scalable and responsive.

**Limitations:** 

**Conclusion:** ClusterChat addresses key limitations in existing corpus exploration methods, empowering users to better navigate and understand large document collections.

**Abstract:** Exploring large-scale text corpora presents a significant challenge in biomedical, finance, and legal domains, where vast amounts of documents are continuously published. Traditional search methods, such as keyword-based search, often retrieve documents in isolation, limiting the user's ability to easily inspect corpus-wide trends and relationships. We present ClusterChat (The demo video and source code are available at: https://github.com/achouhan93/ClusterChat), an open-source system for corpus exploration that integrates cluster-based organization of documents using textual embeddings with lexical and semantic search, timeline-driven exploration, and corpus and document-level question answering (QA) as multi-feature search capabilities. We validate the system with two case studies on a four million abstract PubMed dataset, demonstrating that ClusterChat enhances corpus exploration by delivering context-aware insights while maintaining scalability and responsiveness on large-scale document collections.

</details>


### [103] [Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models](https://arxiv.org/abs/2501.05478)

*Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, Dong An, Ian Reid*

**Main category:** cs.CL

**Keywords:** Vision-and-Language Navigation, Large Language Models, Arabic language processing

**Relevance Score:** 8

**TL;DR:** This study explores the integration of Arabic language in Vision-and-Language Navigation using Large Language Models, demonstrating the challenges and capabilities of various models in navigation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The underexplored integration of Arabic language in the Vision-and-Language Navigation domain, particularly in robotics, necessitates evaluation of language models' performance.

**Method:** Utilizing the NavGPT framework to assess zero-shot sequential action prediction for navigation, evaluating multilingual Small Language Models (SLMs) including GPT-4o mini, Llama 3 8B, Phi-3 medium 14B, and Arabic-centric LLM Jais.

**Key Contributions:**

	1. First-ever integration of Arabic in VLN for robotics
	2. Evaluation of state-of-the-art multilingual SLMs
	3. Identification of limitations for reasoning in Arabic language models

**Result:** The framework showed high-level planning capabilities for navigation tasks with English and Arabic instructions, although some models faced challenges in reasoning and planning in Arabic.

**Limitations:** Certain models struggled with reasoning and planning in Arabic due to inherent limitations and parsing issues.

**Conclusion:** Enhancements in planning and reasoning capabilities of language models are essential for effective navigation, highlighting the potential of Arabic-language models for practical applications.

**Abstract:** Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.

</details>


### [104] [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/abs/2501.10970)

*Nitay Calderon, Roi Reichart, Rotem Dror*

**Main category:** cs.CL

**Keywords:** Large Language Models, annotation, statistical procedure, human evaluation, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces the Alternative Annotator Test (alt-test), a novel statistical procedure for validating the use of LLM annotations over human annotators in various fields.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and establish rigorous standards for using Large Language Models as annotators and judges, particularly in fields such as NLP, medicine, psychology, and social science.

**Method:** The authors propose the Alternative Annotator Test (alt-test), which requires a modest subset of annotated examples to assess the validity of LLM annotations. They compare LLM performance using a curated collection of ten datasets across language and vision-language tasks with six LLMs and four prompting techniques.

**Key Contributions:**

	1. Introduction of the Alternative Annotator Test (alt-test) for validating LLM annotations.
	2. Development of a new measure for comparing LLM annotators and judges.
	3. Empirical demonstration using diverse datasets showcasing LLMs' potential to outperform humans.

**Result:** Results indicate that closed-source LLMs like GPT-4o can outperform some human annotators, while prompting techniques lead to varying quality levels in LLM judges.

**Limitations:** The study is limited by its dependency on a subset of annotated examples and may not generalize across all domains.

**Conclusion:** The study aims to promote more reliable and rigorous practices in using LLMs for annotation and judgment tasks, suggesting that they can in some cases effectively replace human sources.

**Abstract:** The "LLM-as-an-annotator" and "LLM-as-a-judge" paradigms employ Large Language Models (LLMs) as annotators, judges, and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure, the Alternative Annotator Test (alt-test), that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM annotators and judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming the open-source LLMs we examine, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.

</details>


### [105] [Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models](https://arxiv.org/abs/2502.11425)

*Jongho Kim, Seung-won Hwang*

**Main category:** cs.CL

**Keywords:** large language models, temporal reasoning, counterfactual prompting

**Relevance Score:** 9

**TL;DR:** This paper addresses the underdeveloped temporal reasoning ability of large language models (LLMs) by proposing a counterfactual prompting method to enhance temporal consistency in understanding events.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with maintaining temporal consistency, often confusing temporal relations like 'before' and 'after', leading to inconsistent event predictions.

**Method:** The authors propose a novel counterfactual prompting approach that generates counterfactual questions and enforces constraints to improve the model's temporal reasoning.

**Key Contributions:**

	1. Introduction of a counterfactual prompting method for LLMs.
	2. Demonstration of improved temporal consistency in event prediction.
	3. Evaluation across multiple datasets showing significant performance gains.

**Result:** The method shows significant improvements in event ordering for both explicit and implicit events, as well as in temporal commonsense understanding, on multiple evaluated datasets.

**Limitations:** The study is focused on temporal reasoning and does not explore other reasoning capabilities of LLMs.

**Conclusion:** The counterfactual prompting technique effectively addresses the issue of temporal inconsistencies in LLMs, enabling better event understanding.

**Abstract:** Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.

</details>


### [106] [Towards Geo-Culturally Grounded LLM Generations](https://arxiv.org/abs/2502.13497)

*Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin Jr., Sunipa Dev*

**Main category:** cs.CL

**Keywords:** generative LLMs, cultural awareness, retrieval augmented generation, search grounding, human evaluation

**Relevance Score:** 9

**TL;DR:** The study investigates how retrieval augmented generation and search-grounding impact large language models' (LLMs) cultural awareness, revealing search grounding improves propositional knowledge but increases stereotype risks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine gaps in cultural awareness in generative LLMs and evaluate how different grounding techniques influence their performance in this area.

**Method:** The paper compares standard LLMs, LLMs augmented with retrievals from a knowledge base, and those with web search retrievals on cultural awareness benchmarks.

**Key Contributions:**

	1. Comparison of retrieval augmented generation techniques for LLMs
	2. Identification of risks associated with search grounding in LLMs
	3. Clarification of the distinction between propositional knowledge and cultural fluency in LLM evaluations

**Result:** Search grounding significantly enhances LLM performance on benchmarks testing cultural knowledge, but also risks reinforcing stereotypes and does not improve human judges' evaluations of cultural familiarity.

**Limitations:** KB grounding's effectiveness is constrained by knowledge base coverage and a less effective retriever.

**Conclusion:** The study highlights the difference between propositional cultural knowledge and cultural fluency, showing that while search grounding can improve factual knowledge, it may not enhance actual cultural understanding.

**Abstract:** Generative large language models (LLMs) have demonstrated gaps in diverse cultural awareness across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on LLMs' ability to display familiarity with various national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on multiple cultural awareness benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., cultural norms, artifacts, and institutions), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models and fails to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional cultural knowledge and open-ended cultural fluency when it comes to evaluating LLMs' cultural awareness.

</details>


### [107] [PredictaBoard: Benchmarking LLM Score Predictability](https://arxiv.org/abs/2502.14445)

*Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando Martínez-Plumed, José Hernández-Orallo, Lexin Zhou, Wout Schellaert*

**Main category:** cs.CL

**Keywords:** Large Language Models, predictability, benchmarking, human-computer interaction, AI safety

**Relevance Score:** 8

**TL;DR:** This paper presents PredictaBoard, a benchmarking framework to evaluate LLM error predictability and improve their safe deployment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs demonstrate unpredictable errors, which challenges their safe deployment; thus, a method is needed to evaluate and enhance their predictability.

**Method:** The framework assesses pairs of LLMs and score predictors (assessors) based on their rejection rates at different error tolerances.

**Key Contributions:**

	1. Introduction of PredictaBoard for benchmarking LLM predictability
	2. Evaluation of LLMs with baseline assessors
	3. Highlighting the importance of error anticipation in AI systems

**Result:** Illustrative experiments revealed the efficacy of PredictaBoard in stimulating research for better assessors and LLM predictability.

**Limitations:** 

**Conclusion:** PredictaBoard emphasizes the importance of anticipating and mitigating LLM errors, moving beyond just performance metrics.

**Abstract:** Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable "safe zone" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard

</details>


### [108] [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2502.15910)

*Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multimodal Learning, Neuron Unlearning, Privacy, Ethics

**Relevance Score:** 9

**TL;DR:** Proposes MANU, a novel framework for selective neuron unlearning in MLLMs to address privacy concerns by effectively removing sensitive information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical and privacy issues related to the memorization of sensitive information by MLLMs, specifically focusing on the challenge of unlearning due to the entangled nature of knowledge across modalities.

**Method:** The framework consists of two stages: important neuron selection that identifies influential neurons related to the sensitive data, and selective pruning that removes these neurons while preserving non-sensitive knowledge.

**Key Contributions:**

	1. Introduction of Modality Aware Neuron Unlearning (MANU) framework for MLLMs
	2. Two-stage process for identifying and pruning neurons related to sensitive information
	3. Demonstrated effectiveness across various MLLM architectures

**Result:** Experiments demonstrate that MANU achieves balanced and comprehensive unlearning across different modalities without significantly impacting the model's utility.

**Limitations:** Further exploration needed on the long-term effects of unlearning and generalizability across different tasks.

**Conclusion:** MANU provides a robust solution for the unlearning of sensitive data in MLLMs, highlighting its effectiveness in isolating and removing problematic knowledge while maintaining overall model performance.

**Abstract:** Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.

</details>


### [109] [LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification](https://arxiv.org/abs/2502.17421)

*Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An*

**Main category:** cs.CL

**Keywords:** large language models, speculative decoding, long context, machine learning, efficient inference

**Relevance Score:** 9

**TL;DR:** LongSpec introduces a framework to enhance speculative decoding for long-context processing in LLMs, achieving significant speed and efficiency improvements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of inference over long contexts in Large Language Models, a crucial requirement for applications like LLM agents.

**Method:** Introduces a memory-efficient draft model, novel position indices to reduce training-inference mismatch, and an attention aggregation strategy combining prefix computation with tree attention.

**Key Contributions:**

	1. Memory-efficient draft model with constant-sized KV cache
	2. Novel position indices for training-inference consistency
	3. Efficient attention aggregation strategy combining prefix computation and tree attention

**Result:** LongSpec achieves up to a 3.26x speedup over strong Flash Attention baselines and a 2.25x reduction in wall-clock time on the AIME24 long reasoning task.

**Limitations:** 

**Conclusion:** LongSpec significantly enhances the performance and speed of long-context understanding tasks, making it suitable for real-world applications.

**Abstract:** As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.

</details>


### [110] [Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning](https://arxiv.org/abs/2502.20620)

*Ayana Niwa, Masahiro Kaneko, Kentaro Inui*

**Main category:** cs.CL

**Keywords:** large language models, spurious beliefs, belief rectification, unlearning, generalization

**Relevance Score:** 9

**TL;DR:** This paper introduces a method to correct spurious beliefs in large language models (LLMs) that lead to incorrect answers, improving their reliability and generalization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address errors in large language models caused by spurious beliefs that are considered true but are incorrect.

**Method:** The method involves identifying incorrect beliefs through textual explanations generated by the model, using a technique called Forward-Backward Beam Search (FBBS), followed by applying unlearning to suppress spurious beliefs while enhancing true ones.

**Key Contributions:**

	1. Introduction of a method to rectify spurious beliefs in LLMs
	2. Utilization of FBBS for identifying beliefs
	3. Demonstrated improvements in QA performance and generalization

**Result:** Empirical results demonstrate that the proposed method corrects misanswered questions on multiple QA datasets without harming model performance and improves generalization on unseen data.

**Limitations:** 

**Conclusion:** Rectifying a model's belief space is a promising direction for reducing errors and enhancing reliability in LLMs.

**Abstract:** Large language models (LLMs) can exhibit advanced reasoning yet still generate incorrect answers. We hypothesize that such errors frequently stem from spurious beliefs, propositions the model internally considers true but are incorrect. To address this, we propose a method to rectify the belief space by suppressing these spurious beliefs while simultaneously enhancing true ones, thereby enabling more reliable inferences. Our approach first identifies the beliefs that lead to incorrect or correct answers by prompting the model to generate textual explanations, using our Forward-Backward Beam Search (FBBS). We then apply unlearning to suppress the identified spurious beliefs and enhance the true ones, effectively rectifying the model's belief space. Empirical results on multiple QA datasets and LLMs show that our method corrects previously misanswered questions without harming overall model performance. Furthermore, our approach yields improved generalization on unseen data, suggesting that rectifying a model's belief space is a promising direction for mitigating errors and enhancing overall reliability.

</details>


### [111] [SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling](https://arxiv.org/abs/2503.04619)

*Xin Zhang, Qiyu Wei, Yingjie Zhu, Linhai Zhang, Deyu Zhou, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** sentiment analysis, streaming reviews, data sparsity, LLM, dynamic graph

**Relevance Score:** 7

**TL;DR:** This paper presents SynGraph, a framework for improving sentiment analysis on streaming reviews by addressing data sparsity using LLM-augmented dynamic graphs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of traditional sentiment analysis methods that fail to capture the evolving nature of user sentiment in dynamic contexts.

**Method:** The SynGraph framework categorizes users into different scenarios (mid-tail, long-tail, extreme) and utilizes LLM-augmented enhancements within a dynamic graph structure to analyze streaming reviews.

**Key Contributions:**

	1. Introduction of SynGraph framework for sentiment analysis on streaming reviews
	2. Incorporation of LLM-augmented techniques to mitigate data sparsity
	3. Dynamic graph-based approach that categorizes user sentiment scenarios

**Result:** Experiments show that SynGraph effectively addresses data sparsity and enhances sentiment modeling in streaming reviews using real-world datasets.

**Limitations:** 

**Conclusion:** SynGraph provides a significant improvement in sentiment analysis for e-commerce platforms by adapting to the dynamic nature of user reviews.

**Abstract:** User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.

</details>


### [112] [Effect of Selection Format on LLM Performance](https://arxiv.org/abs/2503.06926)

*Yuchen Han, Yucheng Wu, Jeffrey Willard*

**Main category:** cs.CL

**Keywords:** large language models, prompt formatting, classification tasks

**Relevance Score:** 8

**TL;DR:** Investigates the impact of option formatting in prompts for classification tasks in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine how the formatting of classification task options affects the performance of large language models.

**Method:** An experimental study comparing two formats: bullet points and plain English.

**Key Contributions:**

	1. Comparison of bullet point and plain English formats for LLM prompts
	2. Identification of performance differences based on formatting
	3. Call for further exploration of prompt formatting

**Result:** Bullet points generally resulted in better model performance, though exceptions were noted.

**Limitations:** Limited to two formats; further exploration needed for other options.

**Conclusion:** Optimal formatting of options can enhance LLM performance, necessitating further research in this area.

**Abstract:** This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.

</details>


### [113] [SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints](https://arxiv.org/abs/2503.08669)

*Zekun Li, Shinda Huang, Jiangtian Wang, Nathan Zhang, Antonis Antoniades, Wenyue Hua, Kaijie Zhu, Sirui Zeng, Chi Wang, William Yang Wang, Xifeng Yan*

**Main category:** cs.CL

**Keywords:** language agents, SOP compliance, automated evaluation, customer service, agent performance

**Relevance Score:** 8

**TL;DR:** This paper introduces SOPBench, an automated evaluation pipeline for assessing language agents' adherence to domain-specific standard operating procedures (SOPs) across customer service domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well language agents comply with domain-specific SOPs and policies in critical task automation, addressing the lack of rigorous assessment tools in this area.

**Method:** The approach involves creating executable environments with 167 tools/functions and generating over 900 verified test cases. It uses directed graphs of executable functions based on SOP descriptions to evaluate agent compliance with rule-based verifiers.

**Key Contributions:**

	1. Development of SOPBench for automated evaluation in language agents
	2. Introduction of a directed graph model for executing SOP functions
	3. Release of substantial resources including code, data, and agent trajectories for further research.

**Result:** Evaluation of 18 leading models revealed that adherence to SOPs is challenging, with top models showing pass rates of only 30%-50%. Reasoning models performed better, while smaller models fared poorly, revealing a tendency for agents to be jailbroken and ignore constraints.

**Limitations:** The evaluation's reliance on specific customer service domains may limit generalizability, and there's an identified vulnerability of agents to be jailbroken.

**Conclusion:** The paper demonstrates the need for more robust evaluation of language agents in adherence to SOPs, emphasizing the variability in performance across different models and domains.

**Abstract:** As language agents increasingly automate critical tasks, their ability to follow domain-specific standard operating procedures (SOPs), policies, and constraints when taking actions and making tool calls becomes essential yet remains underexplored. To address this gap, we develop an automated evaluation pipeline SOPBench with: (1) executable environments containing 167 tools/functions across seven customer service domains with service-specific SOPs and rule-based verifiers, (2) an automated test generation framework producing over 900 verified test cases, and (3) an automated evaluation framework to rigorously assess agent adherence from multiple dimensions. Our approach transforms each service-specific SOP code program into a directed graph of executable functions and requires agents to call these functions based on natural language SOP descriptions. The original code serves as oracle rule-based verifiers to assess compliance, reducing reliance on manual annotations and LLM-based evaluations. We evaluate 18 leading models, and results show the task is challenging even for top-tier models (like GPT-4o, Claude-3.7-Sonnet), with variances across domains. Reasoning models like o4-mini-high show superiority while other powerful models perform less effectively (pass rates of 30%-50%), and small models (7B, 8B) perform significantly worse. Additionally, language agents can be easily jailbroken to overlook SOPs and constraints. Code, data, and over 24k agent trajectories are released at https://github.com/Leezekun/SOPBench.

</details>


### [114] [Do Construction Distributions Shape Formal Language Learning In German BabyLMs?](https://arxiv.org/abs/2503.11593)

*Bastian Bunzeck, Daniel Duran, Sina Zarrieß*

**Main category:** cs.CL

**Keywords:** language models, child-directed speech, syntactic competence, semantic competence, language learning

**Relevance Score:** 4

**TL;DR:** This paper analyzes how different distributions of utterance constructions in German child-directed speech affect language learning in small language models (LMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how the distribution of utterance-level constructions in child-directed speech influences word-level, syntactic, and semantic competence in language learning models.

**Method:** Training small language models on a novel collection of developmentally plausible German language data and analyzing their learning trajectories.

**Key Contributions:**

	1. Robustness of learning trajectories in small LMs
	2. Impact of utterance complexity on learning outcomes
	3. Provision of developmentally plausible language data for research

**Result:** Learning trajectories in the models are robust across various distributions of training constructions, indicating that training data composition has little effect on overall learning outcomes.

**Limitations:** 

**Conclusion:** Models benefit from complex utterances for syntax but perform better with simpler, fragmentary utterances for word-level learning; they can inform discussions on effective linguistic stimuli for learning.

**Abstract:** We analyze the influence of utterance-level construction distributions in German child-directed/child-available speech on the resulting word-level, syntactic and semantic competence (and their underlying learning trajectories) in small LMs, which we train on a novel collection of developmentally plausible language data for German. We find that trajectories are surprisingly robust for markedly different distributions of constructions in the training data, which have little effect on final accuracies and almost no effect on global learning trajectories. While syntax learning benefits from more complex utterances, word-level learning culminates in better scores with more fragmentary utterances. We argue that LMs trained on developmentally plausible data can contribute to debates on how conducive different kinds of linguistic stimuli are to language learning.

</details>


### [115] [Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information](https://arxiv.org/abs/2504.07738)

*Andrea Loreti, Kesi Chen, Ruby George, Robert Firth, Adriano Agnello, Shinnosuke Tanaka*

**Main category:** cs.CL

**Keywords:** knowledge graph, large language models, automated construction, named entity recognition, retrieval-augmented generation

**Relevance Score:** 6

**TL;DR:** This paper details a multi-step method for automated knowledge graph construction, focusing on the nuclear fusion energy domain and leveraging large language models for named entity recognition and resolution, while also evaluating a retrieval-augmented generation system for complex queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To structure and represent complex, domain-specific knowledge from large document corpora, exemplified by the nuclear fusion energy field.

**Method:** A multi-step approach is employed for automated construction of a knowledge graph, utilizing pre-trained large language models for tasks like named entity recognition and entity resolution, validated against Zipf's law.

**Key Contributions:**

	1. Development of a novel multi-step approach for knowledge graph construction.
	2. Application to nuclear fusion energy knowledge graph.
	3. Introduction of a retrieval-augmented generation system for enhanced query handling.

**Result:** The knowledge graph was successfully constructed, demonstrating effective integration of language models for entity-related tasks and providing contextually relevant answers to complex queries through a novel retrieval-augmented generation system.

**Limitations:** Focused specifically on the nuclear fusion domain, which may limit generalizability to other fields; performance evaluation is based on specific language model capabilities.

**Conclusion:** The research shows that leveraging large language models can significantly enhance the automated construction of knowledge graphs and improve the retrieval of answers for complex queries.

**Abstract:** In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.

</details>


### [116] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)

*Iustin Sirbu, Robert-Adrian Popovici, Cornelia Caragea, Stefan Trausan-Matu, Traian Rebedea*

**Main category:** cs.CL

**Keywords:** semi-supervised learning, pseudo-labeling, natural language processing

**Relevance Score:** 8

**TL;DR:** MultiMatch is a semi-supervised learning algorithm that combines co-training and consistency regularization with pseudo-labeling to enhance classification performance and robustness.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved techniques in semi-supervised learning, particularly in the context of natural language processing and imbalanced data.

**Method:** MultiMatch employs a three-fold pseudo-label weighting module that incorporates heads agreement, self-adaptive thresholds, and Average Pseudo-Margins for enhanced label selection and filtering.

**Key Contributions:**

	1. Introduction of a novel three-fold pseudo-label weighting module
	2. Combines existing SSL techniques for improved performance
	3. Demonstrates state-of-the-art results in challenging NLP datasets

**Result:** Experimental results demonstrate that MultiMatch achieves state-of-the-art performance on 9 out of 10 setups across 5 NLP datasets and shows increased robustness in imbalanced scenarios.

**Limitations:** 

**Conclusion:** The introduction of MultiMatch significantly improves upon existing SSL techniques, offering a unified approach that effectively handles classification difficulties and data imbalance.

**Abstract:** We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.

</details>
