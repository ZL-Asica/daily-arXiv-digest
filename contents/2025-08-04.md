# 2025-08-04

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 57]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [ReVise: A Human-AI Interface for Incremental Algorithmic Recourse](https://arxiv.org/abs/2508.00002)

*Kaustav Bhattacharjee, Jun Yuan, Aritra Dasgupta*

**Main category:** cs.HC

**Keywords:** algorithmic recourse, visual analytic workflow, interactive visualization, socio-technical systems, user decision support

**Relevance Score:** 8

**TL;DR:** This paper presents a visual analytic workflow aimed at assisting individuals in understanding and navigating the incremental steps required for algorithmic recourse in decision-making systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing concern over the black-box nature of AI decisions in socio-technical systems, which can lead to adverse outcomes for individuals seeking recourse.

**Method:** We developed an interactive visualization interface designed to help users explore incremental recourse options based on real-life constraints and model artifacts.

**Key Contributions:**

	1. Development of a visual analytic workflow for recourse planning
	2. An interactive interface to aid users in decision-making
	3. Feedback from real-world scenarios validates the approach

**Result:** Observational studies with twelve graduate students indicated that the workflow is effective in helping users make informed decisions regarding their recourse options.

**Limitations:** The study involved a limited sample size and focused on a specific dataset, which may affect the generalizability of the results.

**Conclusion:** The proposed visual analytic workflow is a valuable tool for individuals to understand and effectively navigate the steps necessary for achieving favorable outcomes in algorithmic decision-making.

**Abstract:** The recent adoption of artificial intelligence in socio-technical systems raises concerns about the black-box nature of the resulting decisions in fields such as hiring, finance, admissions, etc. If data subjects -- such as job applicants, loan applicants, and students -- receive an unfavorable outcome, they may be interested in algorithmic recourse, which involves updating certain features to yield a more favorable result when re-evaluated by algorithmic decision-making. Unfortunately, when individuals do not fully understand the incremental steps needed to change their circumstances, they risk following misguided paths that can lead to significant, long-term adverse consequences. Existing recourse approaches focus exclusively on the final recourse goal but neglect the possible incremental steps to reach the goal with real-life constraints, user preferences, and model artifacts. To address this gap, we formulate a visual analytic workflow for incremental recourse planning in collaboration with AI/ML experts and contribute an interactive visualization interface that helps data subjects efficiently navigate the recourse alternatives and make an informed decision. We present a usage scenario and subjective feedback from observational studies with twelve graduate students using a real-world dataset, which demonstrates that our approach can be instrumental for data subjects in choosing a suitable recourse path.

</details>


### [2] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)

*Guilherme Guerino, Luiz Rodrigues, Luana Bianchiniand Mariana Alves, Marcelo Marinho, Thomaz Veloso, Valmir Macario, Diego Dermeval, Thales Vieira, Ig Bittencourt, Seiji Isotani*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence in Education, Intelligent Tutoring Systems, Augmented Intelligence, User-centered design, Mathematics education

**Relevance Score:** 7

**TL;DR:** This study presents MathAIde, an Intelligent Tutoring System (ITS) designed to enhance mathematics education by integrating Augmented Intelligence (AuI), utilizing computer vision for feedback on student work.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve learning experiences in education through personalized technologies while addressing the challenges faced by Artificial Intelligence in Education.

**Method:** The methodology involved brainstorming with users, high-fidelity prototyping, A/B testing, and real-world case studies in classrooms to assess the effectiveness of the system.

**Key Contributions:**

	1. Development of MathAIde, an ITS that uses computer vision for feedback
	2. Integration of Augmented Intelligence to support teachers without replacing them
	3. Emphasis on a teacher-centered design approach throughout the development process.

**Result:** The research found several design possibilities for integrating AuI in ITSs and demonstrated the practicality of the proposed solution in real classroom environments, leading to a higher adoption potential.

**Limitations:** 

**Conclusion:** A user-centered design approach, which includes teachers in the design process, greatly enhances the effectiveness and adoption of AIED systems, particularly in settings with limited resources.

**Abstract:** Integrating Artificial Intelligence in Education (AIED) aims to enhance learning experiences through technologies like Intelligent Tutoring Systems (ITS), offering personalized learning, increased engagement, and improved retention rates. However, AIED faces three main challenges: the critical role of teachers in the design process, the limitations and reliability of AI tools, and the accessibility of technological resources. Augmented Intelligence (AuI) addresses these challenges by enhancing human capabilities rather than replacing them, allowing systems to suggest solutions. In contrast, humans provide final assessments, thus improving AI over time. In this sense, this study focuses on designing, developing, and evaluating MathAIde, an ITS that corrects mathematics exercises using computer vision and AI and provides feedback based on photos of student work. The methodology included brainstorming sessions with potential users, high-fidelity prototyping, A/B testing, and a case study involving real-world classroom environments for teachers and students. Our research identified several design possibilities for implementing AuI in ITSs, emphasizing a balance between user needs and technological feasibility. Prioritization and validation through prototyping and testing highlighted the importance of efficiency metrics, ultimately leading to a solution that offers pre-defined remediation alternatives for teachers. Real-world deployment demonstrated the usefulness of the proposed solution. Our research contributes to the literature by providing a usable, teacher-centered design approach that involves teachers in all design phases. As a practical implication, we highlight that the user-centered design approach increases the usefulness and adoption potential of AIED systems, especially in resource-limited environments.

</details>


### [3] [Decoupling Data and Tooling in Interactive Visualization](https://arxiv.org/abs/2508.00107)

*Jan Simson*

**Main category:** cs.HC

**Keywords:** data visualization, data wrangling, user experience, HCI, modular design

**Relevance Score:** 6

**TL;DR:** This paper proposes a modular approach for interactive data visualization that separates data wrangling and loading from visualization components, aiming to improve user experience and tool interoperability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the redundancy and learning curve challenges faced by users and developers of current interactive data visualization tools, which often neglect data handling.

**Method:** A modular architecture is proposed that decouples data wrangling and loading from visualization, allowing tools to focus on their core strengths, and demonstrated through a prototype using web technologies.

**Key Contributions:**

	1. Modular architecture for data handling in visualization tools
	2. Prototype demonstrating the feasibility of this approach
	3. Future research directions targeting tool integration and technology incorporation

**Result:** The prototype successfully encapsulates visualization tools and manages data flow between them, indicating the feasibility of the modular approach.

**Limitations:** 

**Conclusion:** The proposed architecture aims to enhance user experience and facilitate interoperability among visualization tools, with opportunities for future research and integration with other technologies.

**Abstract:** Interactive data visualization is a major part of modern exploratory data analysis, with web-based technologies enabling a rich ecosystem of both specialized and general tools. However, current visualization tools often lack support for transformation or wrangling of data and are forced to re-implement their own solutions to load and ingest data. This redundancy creates substantial development overhead for tool creators, steeper learning curves for users who must master different data handling interfaces across tools and a degraded user experience as data handling is usually seen as an after-thought.   We propose a modular approach that separates data wrangling and loading capabilities from visualization components. This architecture allows visualization tools to concentrate on their core strengths while providing the opportunity to develop a unified, powerful interface for data handling. An additional benefit of this approach is that it allows for multiple tools to exist and be used side by side. We demonstrate the feasibility of this approach by building an early prototype using web technologies to encapsulate visualization tools and manage data flow between them.   We discuss future research directions, including downstream integrations with other tooling, such as IDEs, literate programming notebooks and applications, as well as incorporation of new technologies for efficient data transformations. We seek input from the community to better understand the requirements towards this approach.

</details>


### [4] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)

*Zhanna Kaufman, Madeline Endres, Cindy Xiong Bearfield, Yuriy Brun*

**Main category:** cs.HC

**Keywords:** machine learning, explainability, trust, bias perception, visualization

**Relevance Score:** 9

**TL;DR:** This paper explores the relationships between explainability visualizations, comprehension, bias perception, and trust in machine learning systems through user studies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing concern about biased behavior in machine learning systems, which affects stakeholder trust. Understanding how models' behaviors are explained is crucial for users with different backgrounds to build trust.

**Method:** The authors surveyed existing explainability visualizations and created a taxonomy of design characteristics. They conducted user studies to evaluate five visualization tools (LIME, SHAP, CP, Anchors, and ELI5) and measured their impact on comprehension, bias perception, and trust.

**Key Contributions:**

	1. Creation of a taxonomy of explainability visualization design characteristics.
	2. User study findings showing the inverse relationship between comprehension and trust in ML.
	3. Demonstrated that adjusting visualizations can manage perceived bias and trust levels.

**Result:** The study found an inverse relationship between user comprehension and trust; better understanding led to higher bias perception, reducing trust. They confirmed this causal relationship through manipulation of explainability visualizations.

**Limitations:** 

**Conclusion:** Visualization design significantly influences comprehension, bias perception, and trust in machine learning systems; enhancing model fairness and visualization design can increase trust even when understanding is high.

**Abstract:** Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders' trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models' behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people's perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p < 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization's role in facilitating responsible ML applications.

</details>


### [5] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)

*Ziqing Xu, Nick Bryan-Kinns*

**Main category:** cs.HC

**Keywords:** AI music generation, Explainability, Human-Computer Interaction, Deformable interface, User experience

**Relevance Score:** 6

**TL;DR:** DeformTune is a prototype system for AI-assisted music generation that uses a tactile interface to simplify interaction for non-musicians.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create more intuitive and explainable AI music generation tools for users without musical training.

**Method:** The paper introduces DeformTune, a prototype combining a tactile deformable interface with the MeasureVAE model, and presents findings from a study with 11 participants.

**Key Contributions:**

	1. Introduction of DeformTune, an intuitive AI music generation tool for non-musicians
	2. Identification of user challenges in AI music creation
	3. Design opportunities for enhancing explainability in AI music systems

**Result:** Thematic analysis of user feedback identified challenges such as unclear control mappings and limited expressiveness, highlighting design opportunities for improved explainability.

**Limitations:** The study involves a small sample size and focuses on initial user experiences.

**Conclusion:** The findings suggest that enhancing multimodal feedback and providing progressive interaction support can make AI music systems more accessible and empowering for novice users.

**Abstract:** Many existing AI music generation tools rely on text prompts, complex interfaces, or instrument-like controls, which may require musical or technical knowledge that non-musicians do not possess. This paper introduces DeformTune, a prototype system that combines a tactile deformable interface with the MeasureVAE model to explore more intuitive, embodied, and explainable AI interaction. We conducted a preliminary study with 11 adult participants without formal musical training to investigate their experience with AI-assisted music creation. Thematic analysis of their feedback revealed recurring challenge--including unclear control mappings, limited expressive range, and the need for guidance throughout use. We discuss several design opportunities for enhancing explainability of AI, including multimodal feedback and progressive interaction support. These findings contribute early insights toward making AI music systems more explainable and empowering for novice users.

</details>


### [6] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)

*Brian Houck, Travis Lowdermilk, Cody Beyer, Steven Clarke, Ben Hanrahan*

**Main category:** cs.HC

**Keywords:** AI in software development, developer productivity, SPACE framework

**Relevance Score:** 7

**TL;DR:** This paper explores the impact of AI tools on developer productivity and experience through a mixed-methods study, revealing that AI enhances efficiency and satisfaction, particularly for routine tasks, while emphasizing the importance of team culture and support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the true influence of AI tools on software developers' productivity and overall experience.

**Method:** Mixed-methods study utilizing surveys from over 500 developers alongside qualitative insights from interviews and observational studies.

**Key Contributions:**

	1. Empirical evidence on AI's impact using the SPACE framework
	2. Identification of factors influencing AI adoption and productivity
	3. Practical recommendations for integrating AI tools in software development

**Result:** AI is broadly adopted, enhancing productivity primarily for routine tasks, with variations based on task complexity and team adoption; developers report improved efficiency and satisfaction, but less impact on collaboration.

**Limitations:** Results may vary across different programming languages and environments; focus primarily on routine tasks may not generalize to all development scenarios.

**Conclusion:** Effective integration of AI in development teams is contingent on team culture and support structures, alongside the tools themselves; recommendations are offered for maximizing AI's value.

**Abstract:** As artificial intelligence (AI) tools become increasingly embedded in software development workflows, questions persist about their true impact on developer productivity and experience. This paper presents findings from a mixed-methods study examining how developers perceive AI's influence across the dimensions of the SPACE framework: Satisfaction, Performance, Activity, Collaboration and Efficiency. Drawing on survey responses from over 500 developers and qualitative insights from interviews and observational studies, we find that AI is broadly adopted and widely seen as enhancing productivity, particularly for routine tasks. However, the benefits vary, depending on task complexity, individual usage patterns, and team-level adoption. Developers report increased efficiency and satisfaction, with less evidence of impact on collaboration. Organizational support and peer learning play key roles in maximizing AI's value. These findings suggest that AI is augmenting developers rather than replacing them, and that effective integration depends as much on team culture and support structures as on the tools themselves. We conclude with practical recommendations for teams, organizations and researchers seeking to harness AI's potential in software engineering.

</details>


### [7] [HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking](https://arxiv.org/abs/2508.00211)

*Esen K. T端t端nc端, Mar Gonzalez-Franco, Eric J. Gonzalez*

**Main category:** cs.HC

**Keywords:** Extended Reality, Human-Computer Interaction, Hand-Tracking, 3D Interaction, User Study

**Relevance Score:** 8

**TL;DR:** HandOver is an XR interaction technique combining mouse precision with hand-tracking for efficient 3D object manipulation, showing improved ergonomics and performance in user studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance interaction in XR environments by blending the precision of traditional mouse input with the expressiveness of hand-tracking for object manipulation.

**Method:** A user study compared the HandOver technique against traditional raycasting and a hybrid method in a 3D docking task, measuring task errors and user comfort.

**Key Contributions:**

	1. Introduction of HandOver as a novel XR interaction technique
	2. Demonstration of lower task errors using HandOver
	3. Validation of ergonomic benefits through RULA and NASA-TLX measures

**Result:** HandOver demonstrated lower task errors across distances and improved ergonomics, as seen in posture analyses and self-reported comfort measures.

**Limitations:** The study is limited by its specific task scenario and may not generalize to all types of XR interactions.

**Conclusion:** The study supports the effectiveness of integrating different input modalities, leading to better user performance and comfort in immersive environments.

**Abstract:** We present HandOver, an extended reality (XR) interaction technique designed to unify the precision of traditional mouse input for object selection with the expressiveness of hand-tracking for object manipulation. With HandOver, the mouse is used to drive a depth-aware 3D cursor enabling precise and restful targeting -by hovering their hand over the mouse, the user can then seamlessly transition into direct 3D manipulation of the target object. In a formal user study, we compare HandOver against two raybased techniques: traditional raycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results show HandOver yields lower task errors across all distances, and moreover improves interaction ergonomics as highlighted by a RULA posture analysis and self-reported measures (NASA-TLX). These findings illustrate the benefits of blending traditional precise input devices with the expressive gestural inputs afforded by hand-tracking in XR, leading to improved user comfort and task performance. This blended paradigm yields a unified workflow allowing users to leverage the best of each input modality as they interact in immersive environments.

</details>


### [8] [Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism](https://arxiv.org/abs/2508.00233)

*Douglas Markant, Subham Sah, Alireza Karduni, Milad Rogha, My Thai, Wenwen Dou*

**Main category:** cs.HC

**Keywords:** Political sectarianism, Data visualizations, Correction effects

**Relevance Score:** 2

**TL;DR:** The study examines how data visualizations affect perceptions of political extremism across party lines, highlighting the effectiveness of different visualization formats in correcting misperceptions.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Political sectarianism is exacerbated by misperceptions about the opposition's views, which can lead to increased support for extremism. This study investigates the potential of correcting these misperceptions through data visualizations.

**Method:** An experiment was conducted with 483 U.S. participants (Democrats and Republicans) who predicted outparty support for political violence and undemocratic practices after viewing different representations of data on outparty views.

**Key Contributions:**

	1. Demonstrated the effectiveness of data visualizations in correcting political misperceptions.
	2. Investigated the impact of different visualization methods on correction effectiveness.
	3. Provided empirical evidence on the relationship between visualization formats and recall accuracy of outparty views.

**Result:** Participants shown average responses (Mean-Only) and full distributions (Mean+Points) showed strong correction effects, while those presented with the range of views (Mean+Interval) exhibited weaker effects. Full distribution viewers were more accurate in later recall of outparty support levels.

**Limitations:** Findings are limited to U.S. participants and may not generalize across different political contexts or cultures.

**Conclusion:** Data visualizations are effective in correcting beliefs about outparty views, but the method of representation significantly impacts how corrections are interpreted and applied.

**Abstract:** Political sectarianism is fueled in part by misperceptions of political opponents: People commonly overestimate the support for extreme policies among members of the other party. Research suggests that correcting partisan misperceptions by informing people about the actual views of outparty members may reduce one's own expressed support for political extremism, including partisan violence and anti-democratic actions. The present study investigated how correction effects depend on different representations of outparty views communicated through data visualizations. We conducted an experiment with U.S. based participants from Prolific (N=239 Democrats, N=244 Republicans). Participants made predictions about support for political violence and undemocratic practices among members of their political outparty. They were then presented with data from an earlier survey on the actual views of outparty members. Some participants viewed only the average response (Mean-Only condition), while other groups were shown visual representations of the range of views from 75% of the outparty (Mean+Interval condition) or the full distribution of responses (Mean+Points condition). Compared to a control group that was not informed about outparty views, we observed the strongest correction effects among participants in the Mean-only and Mean+Points condition, while correction effects were weaker in the Mean+Interval condition. In addition, participants who observed the full distribution of out-party views (Mean+Points condition) were most accurate at later recalling the degree of support among the outparty. Our findings suggest that data visualizations can be an important tool for correcting pervasive distortions in beliefs about other groups. However, the way in which variability in outparty views is visualized can significantly shape how people interpret and respond to corrective information.

</details>


### [9] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)

*Jacqueline Elise Bruen, Myounghoon Jeon*

**Main category:** cs.HC

**Keywords:** Generative AI, Art perception, Dance performance, User interpretation, Social context

**Relevance Score:** 7

**TL;DR:** This study explores perceptions of generative AI in art, revealing that individuals attribute more artistic merit to AI-created works when unaware of AI's involvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mixed opinions surrounding art created by generative AI and its implications on artistic merit.

**Method:** Developed two versions of a dance performance, one augmented by GenAI and one without. Audiences were surveyed on their perceptions before or after being informed of GenAI's use.

**Key Contributions:**

	1. Empirical evidence on perceptions of AI-generated art
	2. Comparison of audience responses based on awareness of AI use
	3. Discussion on the importance of social context in interpreting GenAI works

**Result:** Participants attributed more artistic merit to GenAI art when they were unaware of its involvement, highlighting the influence of social context on perceptions.

**Limitations:** 

**Conclusion:** The study calls for addressing the role of social context in understanding generative AI's contributions to art, to foster better discussions about its implications.

**Abstract:** With the development of generative artificial intelligence (GenAI) tools to create art, stakeholders cannot come to an agreement on the value of these works. In this study we uncovered the mixed opinions surrounding art made by AI. We developed two versions of a dance performance augmented by technology either with or without GenAI. For each version we informed audiences of the performance's development either before or after a survey on their perceptions of the performance. There were thirty-nine participants (13 males, 26 female) divided between the four performances. Results demonstrated that individuals were more inclined to attribute artistic merit to works made by GenAI when they were unaware of its use. We present this case study as a call to address the importance of utilizing the social context and the users' interpretations of GenAI in shaping a technical explanation, leading to a greater discussion that can bridge gaps in understanding.

</details>


### [10] [TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices](https://arxiv.org/abs/2508.00252)

*Wataru Kawabe, Hiroto Fukuda, Akihisa Shitara, Yuri Nakao, Yusuke Sugano*

**Main category:** cs.HC

**Keywords:** TofuML, machine learning, human-computer interaction, user engagement, interactive systems

**Relevance Score:** 8

**TL;DR:** TofuML is an interactive system for making machine learning concepts more accessible through a physical interface, enhancing user engagement and creativity in ML applications.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind TofuML is to make machine learning concepts more accessible and engaging for non-expert users by employing an interactive, physical, and spatial interface.

**Method:** TofuML employs a small device and a paper mat for users to train and evaluate sound classification models using intuitive interactions. This was tested through two user studies: a comparative study with a GUI-based version and a deployment during a public event.

**Key Contributions:**

	1. Introduction of a physical interface for ML training
	2. Enhanced user engagement through intuitive interactions
	3. Insights on user creativity in ML applications

**Result:** Results showed that TofuML significantly enhanced user engagement compared to traditional GUI systems, helping non-experts engage with ML and demonstrating creativity in conceptualizing diverse ML applications.

**Limitations:** 

**Conclusion:** TofuML provides insights into optimizing user engagement and conceptual understanding in interactive ML systems, encouraging the design of frameworks that cater to a wider audience.

**Abstract:** We introduce TofuML, an interactive system designed to make machine learning (ML) concepts more accessible and engaging for non-expert users. Unlike conventional GUI-based systems, TofuML employs a physical and spatial interface consisting of a small device and a paper mat, allowing users to train and evaluate sound classification models through intuitive, toy-like interactions. Through two user studies -- a comparative study against a GUI-based version and a public event deployment -- we investigated how TofuML impacts users' engagement in the ML model creation process, their ability to provide appropriate training data, and their conception of potential applications. Our results indicated that TofuML enhanced user engagement compared to a GUI while lowering barriers for non-experts to engage with ML. Users demonstrated creativity in conceiving diverse ML applications, revealing opportunities to optimize between conceptual understanding and user engagement. These findings contribute to developing interactive ML systems/frameworks designed for a wide range of users.

</details>


### [11] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)

*Shruthi Chari, Oshani Seneviratne, Prithwish Chakraborty, Pablo Meyer, Deborah L. McGuinness*

**Main category:** cs.HC

**Keywords:** explainable AI, user-centered explanations, neuro-symbolic framework, large language models, machine learning

**Relevance Score:** 8

**TL;DR:** MetaExplainer is a neuro-symbolic framework that generates user-centered explanations by decomposing user questions, generating recommendations, and synthesizing natural language explanations using an Explanation Ontology and large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between model-provided explanations and user needs in AI systems, enhancing trustworthiness and interpretability.

**Method:** A three-stage process involving question decomposition using LLMs, delegating recommendations to explainers, and synthesizing natural language summaries, guided by an Explanation Ontology.

**Key Contributions:**

	1. Introduction of a neuro-symbolic framework for generating user-centered AI explanations.
	2. Utilization of an Explanation Ontology to guide LLMs and explainers in the explanation generation process.
	3. Demonstration of high performance and user satisfaction in providing diverse types of explanations.

**Result:** Achieved high performance with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in synthesis. User studies showed positive reception of the explanations.

**Limitations:** 

**Conclusion:** MetaExplainer effectively generates tailored explanations and shows promise for enhancing AI explainability in diverse applications beyond the tested scenarios.

**Abstract:** Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Throughout this process, we utilize an Explanation Ontology to guide the language models and explainer methods. By leveraging LLMs and a structured approach to explanation generation, MetaExplainer aims to enhance the interpretability and trustworthiness of AI systems across various applications, providing users with tailored, question-driven explanations that better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate a step towards evaluating and utilizing current state-of-the-art explanation frameworks. Our results show high performance across all stages, with a 59.06% F1-score in question reframing, 70% faithfulness in model explanations, and 67% context-utilization in natural language synthesis. User studies corroborate these findings, highlighting the creativity and comprehensiveness of generated explanations. Tested on the Diabetes (PIMA Indian) tabular dataset, MetaExplainer supports diverse explanation types, including Contrastive, Counterfactual, Rationale, Case-Based, and Data explanations. The framework's versatility and traceability from using ontology to guide LLMs suggest broad applicability beyond the tested scenarios, positioning MetaExplainer as a promising tool for enhancing AI explainability across various domains.

</details>


### [12] [Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes](https://arxiv.org/abs/2508.00321)

*Shuning Zhang, Ying Ma, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** privacy, smart homes, large language models, visual sensors, dynamic policies

**Relevance Score:** 9

**TL;DR:** This paper explores the use of Large Language Models (LLMs) to create dynamic privacy policies for visual sensors in smart home environments, achieving notable performance in policy generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The advent of wearable visual sensors such as smart glasses presents complex privacy challenges that current static privacy controls cannot address effectively.

**Method:** The paper proposes a framework where visual data is classified based on sensitivity, context, and social presence. An LLM reasons over this data to enforce adaptive privacy rules in real-time.

**Key Contributions:**

	1. Introduction of a dynamic privacy policy engine using LLMs
	2. Development of a multi-dimensional schema for visual data classification
	3. Demonstration of effective real-time privacy rule enforcement in simulated environments

**Result:** The LLM-based privacy policy engine demonstrated effective real-time privacy management in simulated settings, with high evaluation scores from both machine and human assessments.

**Limitations:** 

**Conclusion:** The approach shows promise for implementing fine-grained, context-aware privacy controls in smart home environments using LLMs.

**Abstract:** The proliferation of visual sensors in smart home environments, particularly through wearable devices like smart glasses, introduces profound privacy challenges. Existing privacy controls are often static and coarse-grained, failing to accommodate the dynamic and socially nuanced nature of home environments. This paper investigates the viability of using Large Language Models (LLMs) as the core of a dynamic and adaptive privacy policy engine. We propose a conceptual framework where visual data is classified using a multi-dimensional schema that considers data sensitivity, spatial context, and social presence. An LLM then reasons over this contextual information to enforce fine-grained privacy rules, such as selective object obfuscation, in real-time. Through a comparative evaluation of state-of-the-art Vision Language Models (including GPT-4o and the Qwen-VL series) in simulated home settings , our findings show the feasibility of this approach. The LLM-based engine achieved a top machine-evaluated appropriateness score of 3.99 out of 5, and the policies generated by the models received a top human-evaluated score of 4.00 out of 5.

</details>


### [13] [From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations](https://arxiv.org/abs/2508.00328)

*Shuning Zhang, Ying Ma, Yongquan `Owen' Hu, Ting Dang, Hong Jia, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** privacy, online consultation, anonymization, local LLM, health informatics

**Relevance Score:** 9

**TL;DR:** The paper investigates privacy concerns in online medical consultation platforms and introduces SafeShare, a technique utilizing localized LLM for real-time consultation redaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant privacy risks in online medical consultation platforms that undermine user trust.

**Method:** Conducted in-depth semi-structured interviews with 12 users to understand their security and privacy perceptions, followed by technical evaluation of the SafeShare technique for real-time redaction.

**Key Contributions:**

	1. Developed SafeShare for real-time redaction of consultations.
	2. Identified user expectations vs. platform realities regarding privacy.
	3. Demonstrated high accuracy of PII detection in evaluation.

**Result:** SafeShare's PII detection module achieved 89.64% accuracy on the IMCS21 dataset, demonstrating high efficacy in balancing privacy and utility.

**Limitations:** Limited to the datasets evaluated; further study needed on broader applications.

**Conclusion:** SafeShare effectively addresses the disconnect between user needs for privacy and the realities of medical consultation platforms by employing localized LLM for real-time data redaction.

**Abstract:** Online medical consultation platforms, while convenient, are undermined by significant privacy risks that erode user trust. We first conducted in-depth semi-structured interviews with 12 users to understand their perceptions of security and privacy landscapes on online medical consultation platforms, as well as their practices, challenges and expectation. Our analysis reveals a critical disconnect between users' desires for anonymity and control, and platform realities that offload the responsibility of ``privacy labor''. To bridge this gap, we present SafeShare, an interaction technique that leverages localized LLM to redact consultations in real-time. SafeShare balances utility and privacy through selectively anonymize private information. A technical evaluation of SafeShare's core PII detection module on 3 dataset demonstrates high efficacy, achieving 89.64\% accuracy with Qwen3-4B on IMCS21 dataset.

</details>


### [14] [HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification](https://arxiv.org/abs/2508.00439)

*Subin Park, Jeonghyun Kim, Jeanne Choi, Joseph Seering, Uichin Lee, Sung-Ju Lee*

**Main category:** cs.HC

**Keywords:** Hate speech, Content moderation, Mental well-being, User study, Text modification

**Relevance Score:** 7

**TL;DR:** This paper presents HateBuffer, a tool designed to mitigate the mental burden on content moderators dealing with hate speech by anonymizing targets and paraphrasing offensive language.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the persistent challenge of hate speech in online platforms and the associated mental burden on content moderators.

**Method:** The study involved 80 participants who tested HateBuffer in a simulated moderation task, followed by semi-structured interviews to gather qualitative feedback.

**Key Contributions:**

	1. Introduction of HateBuffer as a moderation tool
	2. Empirical evaluation of its impact on user perception and emotional response
	3. Insights into the effectiveness of text modification in moderation practices

**Result:** While HateBuffer decreased the perceived severity of hate comments, it did not improve moderators' emotions or reduce fatigue compared to the control group. Participants found it helpful against emotional contagion.

**Limitations:** The tool did not significantly improve moderators' emotional state or reduce fatigue despite qualitative feedback suggesting it had benefits.

**Conclusion:** HateBuffer shows promise as a tool to enhance the well-being of content moderators without compromising moderation accuracy, although its effectiveness on mental well-being remains mixed.

**Abstract:** Hate speech remains a persistent and unresolved challenge in online platforms. Content moderators, working on the front lines to review user-generated content and shield viewers from hate speech, often find themselves unprotected from the mental burden as they continuously engage with offensive language. To safeguard moderators' mental well-being, we designed HateBuffer, which anonymizes targets of hate speech, paraphrases offensive expressions into less offensive forms, and shows the original expressions when moderators opt to see them. Our user study with 80 participants consisted of a simulated hate speech moderation task set on a fictional news platform, followed by semi-structured interviews. Although participants rated the hate severity of comments lower while using HateBuffer, contrary to our expectations, they did not experience improved emotion or reduced fatigue compared with the control group. In interviews, however, participants described HateBuffer as an effective buffer against emotional contagion and the normalization of biased opinions in hate speech. Notably, HateBuffer did not compromise moderation accuracy and even contributed to a slight increase in recall. We explore possible explanations for the discrepancy between the perceived benefits of HateBuffer and its measured impact on mental well-being. We also underscore the promise of text-based content modification techniques as tools for a healthier content moderation environment.

</details>


### [15] [Pull Requests From The Classroom: Co-Developing Curriculum And Code](https://arxiv.org/abs/2508.00646)

*Dennis Zyska, Ilia Kuznetsov, Florian M端ller, Iryna Gurevych*

**Main category:** cs.HC

**Keywords:** peer feedback system, educational technology, co-development

**Relevance Score:** 6

**TL;DR:** The paper presents a case study on developing a peer feedback system alongside a university course on scientific writing, highlighting the importance of co-development in aligning technology with pedagogical goals and its resulting challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misalignment between educational technologies and instructors' pedagogical goals, which undermines teaching efficacy.

**Method:** A case study approach was utilized, examining the iterative co-development of a custom-built peer feedback system alongside a scientific writing course.

**Key Contributions:**

	1. Presentation of a co-development model for educational technologies.
	2. Insights into the alignment of software features with pedagogical objectives.
	3. Identification of usability limitations in educational technology.

**Result:** The study revealed that co-development improved alignment between the system's features and the course goals but also highlighted usability issues and frustrations related to infrastructure.

**Limitations:** Focused on a single university course, which may limit generalizability.

**Conclusion:** Stronger collaboration between teaching and technical teams is necessary to enhance educational technology's effectiveness and usability.

**Abstract:** Educational technologies often misalign with instructors' pedagogical goals, forcing adaptations that compromise teaching efficacy. In this paper, we present a case study on the co-development of curriculum and technology in the context of a university course on scientific writing. Specifically, we examine how a custom-built peer feedback system was iteratively developed alongside the course to support annotation, feedback exchange, and revision. Results show that while co-development fostered stronger alignment between software features and course goals, it also exposed usability limitations and infrastructure-related frustrations, emphasizing the need for closer coordination between teaching and technical teams.

</details>


### [16] [The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech](https://arxiv.org/abs/2508.00652)

*Shuning Zhang, Han Chen, Yabo Wang, Yiqun Xu, Jiaqi Bai, Yuanyuan Wu, Shixuan Li, Xin Yi, Chunhui Wang, Hewu Li*

**Main category:** cs.HC

**Keywords:** voice interaction, dark patterns, synthetic voices, Mandarin Chinese, user perception

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of female synthetic voices on user behavior in Mandarin Chinese, revealing substantial behavioral manipulation through voice characteristics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the empirical investigation gap regarding voice characteristic-based manipulation in non-English contexts, particularly Mandarin Chinese.

**Method:** The study systematically evaluates voice characteristic manipulations (five characteristics, three intensities) across scenarios (shopping vs. question-answering) with a preliminary study (N=24) and a main study (N=36).

**Key Contributions:**

	1. First systematic study of voice characteristic-based manipulation in Mandarin Chinese
	2. Demonstrated significant behavioral manipulation linked to voice characteristics
	3. Provided insights for ethical design considerations.

**Result:** The main study revealed significant behavioral manipulation, with effectiveness variations up to +2027.6%, influenced by voice characteristics, scenario, and user perception.

**Limitations:** Limited demographic impact on user perception; small sample sizes.

**Conclusion:** The findings highlight the ethical considerations in design due to the significant impact of voice characteristics on user behavior.

**Abstract:** Pervasive voice interaction enables deceptive patterns through subtle voice characteristics, yet empirical investigation into this manipulation lags behind, especially within major non-English language contexts. Addressing this gap, our study presents the first systematic investigation into voice characteristic-based dark patterns employing female synthetic voices in Mandarin Chinese. This focus is crucial given the prevalence of female personas in commercial assistants and the prosodic significance in the Chinese language. Guided by the conceptual framework identifying key influencing factors, we systematically evaluate effectiveness variations by manipulating voice characteristics (five characteristics, three intensities) across different scenarios (shopping vs. question-answering) with different commercial aims. A preliminary study (N=24) validated the experimental materials and the main study (N=36) revealed significant behavioral manipulation (up to +2027.6%). Crucially, the analysis showed that effectiveness varied significantly with voice characteristics and scenario, mediated by user perception (of tone, intonation, timbre) and user demographics (individual preferences, though limited demographic impact). These interconnected findings offer evidence-based insights for ethical design.

</details>


### [17] [Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption](https://arxiv.org/abs/2508.00723)

*Rebecca Yu, Valerie Chen, Ameet Talwalkar, Hoda Heidari*

**Main category:** cs.HC

**Keywords:** AI adoption, decision-making, human-computer interaction, cross-domain analysis, ethical AI

**Relevance Score:** 7

**TL;DR:** The paper examines how decision-makers in various fields adopt AI tools, identifying factors influencing this adoption and offering a framework for understanding inter-domain differences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how human decision-makers interact with AI systems and understand the factors that influence their adoption of AI tools.

**Method:** Conducted interviews with experts across four domains (medicine, law, journalism, and public sector) to explore AI use cases and perceptions of adoption.

**Key Contributions:**

	1. Identification of factors influencing AI adoption decisions
	2. Development of an AI adoption sheet to analyze adoption choices
	3. Cross-domain case studies highlighting differences in AI adoption

**Result:** Identified key factors influencing decision-maker adoption of AI: decision-maker's background, perceptions of AI, consequences for decision-makers, and implications for stakeholders.

**Limitations:** 

**Conclusion:** The study provides practical guidance for the responsible deployment of AI by considering decision-makers' perspectives.

**Abstract:** Growing excitement around deploying AI across various domains calls for a careful assessment of how human decision-makers interact with AI-powered systems. In particular, it is essential to understand when decision-makers voluntarily choose to consult AI tools, which we term decision-maker adoption. We interviewed experts across four domains -- medicine, law, journalism, and the public sector -- to explore current AI use cases and perceptions of adoption. From these interviews, we identify key factors that shape decision-maker adoption of AI tools: the decision-maker's background, perceptions of the AI, consequences for the decision-maker, and perceived implications for other stakeholders. We translate these factors into an AI adoption sheet to analyze how decision-makers approach adoption choices through comparative, cross-domain case studies, highlighting how our factors help explain inter-domain differences in adoption. Our findings offer practical guidance for supporting the responsible and context-aware deployment of AI by better accounting for the decision-maker's perspective.

</details>


### [18] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)

*S端eda zkaya, Santiago Berrezueta-Guzman, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** Large Language Models, Virtual Reality, AI in Gaming, NPC Interactions, Multimodal AI

**Relevance Score:** 9

**TL;DR:** This paper reviews the integration of Large Language Models (LLMs) into Virtual Reality (VR) games, highlighting their impact on narrative generation and NPC interactions while addressing challenges and future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can transform the design and experience of VR games through enhanced interactivity and personalization.

**Method:** Comprehensive review of 62 peer-reviewed studies focusing on the intersection of LLMs and VR from 2018 to 2025.

**Key Contributions:**

	1. Comprehensive review of LLMs in VR
	2. Identification of key application areas
	3. Discussion of challenges and future directions

**Result:** Identification of key application domains including emotionally intelligent NPCs and adaptive AI systems, alongside discussions of challenges like real-time performance and ethical concerns.

**Limitations:** Memory limitations and real-time performance constraints.

**Conclusion:** Successful deployment of LLMs in VR requires robust design strategies, which support multimodal interactions and address ethical considerations, with future research aimed at fostering intelligent VR systems.

**Abstract:** The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.

</details>


### [19] [Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors](https://arxiv.org/abs/2409.18203)

*Michelle S. Lam, Fred Hohman, Dominik Moritz, Jeffrey P. Bigham, Kenneth Holstein, Mary Beth Kery*

**Main category:** cs.HC

**Keywords:** AI policy, large language models, policy maps, interactive tools, AI safety

**Relevance Score:** 7

**TL;DR:** Introduction of policy maps for AI policy design, enabling navigation through LLM behavior space with interactive tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective AI policy design for LLMs, which face challenges in ensuring comprehensive behavior coverage.

**Method:** Development of Policy Projector, an interactive tool that allows AI practitioners to create policy maps by defining regions and applying if-then rules for LLM outputs.

**Key Contributions:**

	1. Introduction of policy maps for AI policies
	2. Development of the Policy Projector tool
	3. Interactive authoring of LLM policies using map visualization

**Result:** Evaluation with AI safety experts indicates that Policy Projector aids policy designers in addressing model behaviors like incorrect gender assumptions and safety threat management.

**Limitations:** Still requires refinement for broader applicability across different AI domains.

**Conclusion:** Policy maps provide an innovative approach to AI policy design, enhancing the capability to manage LLM outputs effectively.

**Abstract:** AI policy sets boundaries on acceptable behavior for AI models, but this is challenging in the context of large language models (LLMs): how do you ensure coverage over a vast behavior space? We introduce policy maps, an approach to AI policy design inspired by the practice of physical mapmaking. Instead of aiming for full coverage, policy maps aid effective navigation through intentional design choices about which aspects to capture and which to abstract away. With Policy Projector, an interactive tool for designing LLM policy maps, an AI practitioner can survey the landscape of model input-output pairs, define custom regions (e.g., "violence"), and navigate these regions with if-then policy rules that can act on LLM outputs (e.g., if output contains "violence" and "graphic details," then rewrite without "graphic details"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the AI practitioner's work. In an evaluation with 12 AI safety experts, our system helps policy designers craft policies around problematic model behaviors such as incorrect gender assumptions and handling of immediate physical safety threats.

</details>


### [20] [Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors](https://arxiv.org/abs/2409.18203)

*Michelle S. Lam, Fred Hohman, Dominik Moritz, Jeffrey P. Bigham, Kenneth Holstein, Mary Beth Kery*

**Main category:** cs.HC

**Keywords:** AI policy, large language models, policy maps, human-computer interaction, AI safety

**Relevance Score:** 8

**TL;DR:** This paper introduces policy maps for AI policy design in LLMs, focusing on intentional design to navigate model behaviors.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of setting appropriate boundaries for AI behavior in large language models by designing effective policies.

**Method:** The authors propose policy maps, allowing practitioners to create custom regions for model behavior (e.g., 'violence') and implement if-then policy rules using the Policy Projector tool.

**Key Contributions:**

	1. Introduction of policy maps for AI policy design
	2. Development of the Policy Projector tool for interactive policy authoring
	3. Evaluation demonstrating effectiveness with AI safety experts

**Result:** Evaluation with AI safety experts showed that Policy Projector aids in crafting policies to mitigate problematic model behaviors like gender biases and safety threats.

**Limitations:** 

**Conclusion:** The tool enhances policy design for AI models by providing an interactive and structured way to navigate complex behaviors.

**Abstract:** AI policy sets boundaries on acceptable behavior for AI models, but this is challenging in the context of large language models (LLMs): how do you ensure coverage over a vast behavior space? We introduce policy maps, an approach to AI policy design inspired by the practice of physical mapmaking. Instead of aiming for full coverage, policy maps aid effective navigation through intentional design choices about which aspects to capture and which to abstract away. With Policy Projector, an interactive tool for designing LLM policy maps, an AI practitioner can survey the landscape of model input-output pairs, define custom regions (e.g., "violence"), and navigate these regions with if-then policy rules that can act on LLM outputs (e.g., if output contains "violence" and "graphic details," then rewrite without "graphic details"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the AI practitioner's work. In an evaluation with 12 AI safety experts, our system helps policy designers craft policies around problematic model behaviors such as incorrect gender assumptions and handling of immediate physical safety threats.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)

*Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan*

**Main category:** cs.CL

**Keywords:** Physics, LLM, Natural Language Reasoning, Multi-Agent Framework, Benchmark

**Relevance Score:** 4

**TL;DR:** Evaluation of LLM performance on solving physics problems using innovative multi-agent frameworks and new benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Physics is essential for advancing technology and understanding the universe, necessitating effective natural language reasoning for solving physics problems.

**Method:** We assess frontier LLMs on physics problems and apply various inference-time techniques, including a multi-agent approach where smaller LLMs verify solutions cumulatively.

**Key Contributions:**

	1. Evaluation of LLMs on physics problems
	2. Introduction of a multi-agent framework for solution verification
	3. Development of a new benchmark, P{}YSICSVAL.

**Result:** The multi-agent framework significantly enhances performance on challenging problems, revealing substantial improvements over standard methods.

**Limitations:** 

**Conclusion:** A new evaluation benchmark, P{}YSICSVAL, consisting of over 19,000 problems, is introduced to facilitate further research and model assessment in this domain.

**Abstract:** The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.

</details>


### [22] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)

*Kelly Kendro, Jeffrey Maloney, Scott Jarvis*

**Main category:** cs.CL

**Keywords:** Large Language Models, Lexical Diversity, ChatGPT, Human-Computer Interaction, Language Pedagogy

**Relevance Score:** 8

**TL;DR:** This study investigates the lexical diversity of texts generated by different ChatGPT models compared to human-written texts across various education levels.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To determine the degree to which LLMs produce texts that resemble human writing, specifically in terms of lexical diversity.

**Method:** The study measured six dimensions of lexical diversity in texts generated by four ChatGPT models and compared the results with texts written by L1 and L2 English participants (n = 240) across four education levels using statistical analysis methods, including one-way MANOVAs, ANOVAs, and Support Vector Machines.

**Key Contributions:**

	1. Analysis of lexical diversity in LLM-generated texts
	2. Comparative study across different ChatGPT models
	3. Insights into implications for language pedagogy

**Result:** LLM-generated texts significantly differ from human-written texts in all measured lexical diversity variables, with ChatGPT-4.5 showing higher diversity but producing fewer tokens than earlier models. Human writers' lexical diversity showed no significant difference across educational backgrounds or language status.

**Limitations:** 

**Conclusion:** LLMs produce less human-like texts concerning lexical diversity, and newer models are less human-like than older ones, which has implications for language pedagogy and related applications.

**Abstract:** The degree to which LLMs produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini, and -4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and Support Vector Machines revealed that the LLM-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that LLMs do not produce human-like texts in relation to lexical diversity, and the newer LLMs produce less human-like texts than older models. We discuss the implications of these results for language pedagogy and related applications.

</details>


### [23] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)

*Zachary K. Stine, James E. Deitrick*

**Main category:** cs.CL

**Keywords:** computational humanities, semiotic complexity, interpretive transparency, translation errors, modeling practices

**Relevance Score:** 2

**TL;DR:** The paper discusses the need for greater theorizing in computational humanities to enhance clarity and prevent translation errors between cultural and computational domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide epistemological clarity in computational humanities and improve the maturity of the field by understanding translation processes.

**Method:** The paper develops the concept of semiotic complexity and critiques current modeling practices that oversimplify data interpretation.

**Key Contributions:**

	1. Introduction of the concept of semiotic complexity
	2. Critique of current modeling practices in computational humanities
	3. Recommendations for avoiding epistemological errors in translation work

**Result:** Highlights the risks involved in treating complex data as simple, which leads to significant translation errors and misinterpretations.

**Limitations:** 

**Conclusion:** Recommendations are provided for researchers to enhance their understanding of epistemological issues in their work.

**Abstract:** Greater theorizing of methods in the computational humanities is needed for epistemological and interpretive clarity, and therefore the maturation of the field. In this paper, we frame such modeling work as engaging in translation work from a cultural, linguistic domain into a computational, mathematical domain, and back again. Translators benefit from articulating the theory of their translation process, and so do computational humanists in their work -- to ensure internal consistency, avoid subtle yet consequential translation errors, and facilitate interpretive transparency. Our contribution in this paper is to lay out a particularly consequential dimension of the lack of theorizing and the sorts of translation errors that emerge in our modeling practices as a result. Along these lines we introduce the idea of semiotic complexity as the degree to which the meaning of some text may vary across interpretive lenses, and make the case that dominant modeling practices -- especially around evaluation -- commit a translation error by treating semiotically complex data as semiotically simple when it seems epistemologically convenient by conferring superficial clarity. We then lay out several recommendations for researchers to better account for these epistemological issues in their own work.

</details>


### [24] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)

*Mingda Chen, Yang Li, Xilun Chen, Adina Williams, Gargi Ghosh, Scott Yih*

**Main category:** cs.CL

**Keywords:** long-form factuality, language models, benchmark evaluation

**Relevance Score:** 9

**TL;DR:** The paper introduces FACTORY, a human-verified benchmark for evaluating long-form factuality in language models, showing higher rates of factual inaccuracies in SOTA models compared to other datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of human verification in existing benchmarks for evaluating models' factual accuracy in response to prompts.

**Method:** Developed a large-scale prompt set named FACTORY using a model-in-the-loop approach, refined by human evaluations.

**Key Contributions:**

	1. Introduction of a large-scale, human-verified dataset for long-form factuality evaluation
	2. Demonstration of substantial factual inaccuracies in existing state-of-the-art models using FACTORY
	3. Highlighting the importance of human verification in model evaluations.

**Result:** FACTORY revealed that approximately 40% of claims from state-of-the-art models were not factual, compared to only 10% from other datasets, indicating its reliability as a benchmark.

**Limitations:** The analysis is limited to six state-of-the-art models; further studies could expand the scope.

**Conclusion:** The study highlights the strengths of FACTORY while emphasizing the need for models to effectively reason across long-tailed facts.

**Abstract:** Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.

</details>


### [25] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)

*Xiao Zhang, Johan bos*

**Main category:** cs.CL

**Keywords:** neural semantic parsing, verb phrase ellipsis, context sensitivity, linguistic phenomena, data augmentation

**Relevance Score:** 6

**TL;DR:** Neural semantic parsers excel in general but struggle with verb phrase ellipsis, a context-sensitive linguistic phenomenon.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the performance of neural semantic parsers on context-sensitive phenomena, particularly verb phrase ellipsis in English.

**Method:** Constructed a corpus of 120 cases of verb phrase ellipsis with their fully resolved meaning representations to test a variety of neural semantic parsers.

**Key Contributions:**

	1. Creation of a specialized corpus for testing ellipsis handling in semantic parsers.
	2. Evaluation results showing significant performance drops on ellipsis cases compared to standard ones.
	3. Insight into the limitations of current neural semantic parsing technology regarding context sensitivity.

**Result:** Neural semantic parsers demonstrated high performance on standard datasets but failed to handle the challenging instances of ellipsis in the test set.

**Limitations:** Only focused on verb phrase ellipsis; results may not generalize to all context-sensitive phenomena.

**Conclusion:** The performance gap indicates that while neural semantic parsers are robust, they need improvement in dealing with strongly context-sensitive linguistic constructs like ellipsis.

**Abstract:** Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation

</details>


### [26] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)

*Alper Yaman, Jannik Schwab, Christof Nitsche, Abhirup Sinha, Marco Huber*

**Main category:** cs.CL

**Keywords:** Large Language Models, LLM selection, AI models, comparative analysis

**Relevance Score:** 9

**TL;DR:** This paper presents a comparative list of foundational and domain-specific Large Language Models (LLMs) to assist in model selection for researchers and companies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Navigating the rapidly evolving landscape of LLMs, which presents challenges in selection due to licensing and hardware requirements.

**Method:** A comparative analysis of foundational and domain-specific LLMs focusing on features such as release year, licensing, and hardware requirements.

**Key Contributions:**

	1. Development of a comparative list of LLMs
	2. Focus on licensing and hardware requirements
	3. Continuous updates to the resource

**Result:** The paper provides a continuously updated list of LLMs to aid researchers and companies in selecting optimal models.

**Limitations:** The list may not encompass all models available as the field is rapidly evolving, and some models may have unforeseen limitations not covered.

**Conclusion:** The comparative list of LLMs will serve as a valuable resource for model selection amidst the complexities of the current AI landscape.

**Abstract:** Large Language Models (LLMs), such as Generative Pre-trained Transformers (GPTs) are revolutionizing the generation of human-like text, producing contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these Artificial Intelligence (AI) models excel in tasks, such as content creation, translation, and code generation. Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address these issues. Over the past two years, numerous open-source foundational and fine-tuned models have been introduced, complicating the selection of the optimal LLM for researchers and companies regarding licensing and hardware requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM selection, we present a comparative list of foundational and domain-specific models, focusing on features, such as release year, licensing, and hardware requirements. This list is published on GitLab and will be continuously updated.

</details>


### [27] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)

*Xiaofeng Wu, Alan Ritter, Wei Xu*

**Main category:** cs.CL

**Keywords:** table understanding, large language models, taxonomy, tabular data, multimodal models

**Relevance Score:** 8

**TL;DR:** This paper introduces a taxonomy for tabular inputs and discusses challenges in table understanding tasks for LLMs and MLLMs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities and diverse structures of tables in LLMs and MLLMs, which have not been effectively managed by current methods.

**Method:** The paper presents a taxonomy of tabular representations and outlines various table understanding tasks, analyzing the current landscape and identifying key challenges.

**Key Contributions:**

	1. Introduction of a taxonomy for tabular input representations
	2. Identification of critical gaps in current research and model capabilities
	3. Highlighting the need for diverse reasoning abilities in table processing

**Result:** Identified gaps in the literature highlighting the limitations of existing models in processing complex tabular data and the need for more comprehensive approaches.

**Limitations:** The study primarily focuses on existing gaps without providing specific solutions for the identified challenges.

**Conclusion:** The findings stress the necessity for enhanced reasoning capabilities in LLMs and broader generalization across diverse table structures to improve table understanding.

**Abstract:** Tables have gained significant attention in large language models (LLMs) and multimodal large language models (MLLMs) due to their complex and flexible structure. Unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. This diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. To address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. We highlight several critical gaps in the field that indicate the need for further research: (1) the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; (2) significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and (3) the limited generalization of models across different tabular representations and formats.

</details>


### [28] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)

*Rana Aref Salama, Abdou Youssef, Mona Diab*

**Main category:** cs.CL

**Keywords:** Discrete Wavelet Transforms, NLP, semantic similarity, embeddings, dimensionality reduction

**Relevance Score:** 8

**TL;DR:** This paper explores the use of Discrete Wavelet Transforms (DWT) to enhance word and sentence embeddings in NLP, demonstrating significant dimensionality reduction and improved performance in semantic similarity tasks and downstream applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The use of Wavelet transforms to improve data representation and feature extraction in NLP applications, particularly in semantic similarity tasks.

**Method:** Empirical application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings, assessing their effectiveness in various embedding models, including large language models.

**Key Contributions:**

	1. Demonstration of DWT's capability to compress embeddings while preserving quality.
	2. Empirical validation of DWT on various embedding models and downstream tasks.
	3. Introduction of a novel approach to semantic similarity tasks using wavelet transforms.

**Result:** DWT can reduce embedding dimensionality by 50-93% with minimal performance loss, achieving superior accuracy in most downstream tasks compared to traditional methods.

**Limitations:** 

**Conclusion:** The application of DWT shows promise for enhancing NLP tasks by consolidating important semantic information in embeddings and facilitating improved performance in various applications.

**Abstract:** Wavelet transforms, a powerful mathematical tool, have been widely used in different domains, including Signal and Image processing, to unravel intricate patterns, enhance data representation, and extract meaningful features from data. Tangible results from their application suggest that Wavelet transforms can be applied to NLP capturing a variety of linguistic and semantic properties. In this paper, we empirically leverage the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase the capabilities of DWT in analyzing embedding representations at different levels of resolution and compressing them while maintaining their overall quality. We assess the effectiveness of DWT embeddings on semantic similarity tasks to show how DWT can be used to consolidate important semantic information in an embedding vector. We show the efficacy of the proposed paradigm using different embedding models, including large language models, on downstream tasks. Our results show that DWT can reduce the dimensionality of embeddings by 50-93% with almost no change in performance for semantic similarity tasks, while achieving superior accuracy in most downstream tasks. Our findings pave the way for applying DWT to improve NLP applications.

</details>


### [29] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)

*Bryce Anderson, Riley Galpin, Tom S. Juzek*

**Main category:** cs.CL

**Keywords:** Large Language Models, lexical shifts, human language system, AI ethics, language change

**Relevance Score:** 8

**TL;DR:** This paper investigates lexical shifts in spoken language due to the influence of Large Language Models (LLMs), revealing significant changes post-2022 linked to LLM-associated words.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether shifts in word usage in written language reflect broader changes in the human language system or are merely a result of AI exposure through LLMs.

**Method:** Analysis of a dataset comprising 22.1 million words from conversational science and technology podcasts, focusing on lexical trends before and after the release of ChatGPT.

**Key Contributions:**

	1. Construction of a large dataset analyzing spoken language shifts in relation to LLMs.
	2. Identification of significant lexical trends associated with LLM use.
	3. Discussion of the implications of AI on language and ethical concerns regarding model misalignment.

**Result:** A moderate yet significant increase in the usage of LLM-associated words after 2022, indicating potential convergence between human language and LLM outputs.

**Limitations:** The study is limited by its short time frame and focuses only on spoken language from a specific context (podcasts).

**Conclusion:** The findings suggest an ongoing shift in language use that may involve both natural language change and the influence of AI, raising questions about the implications for social and moral beliefs.

**Abstract:** In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs.

</details>


### [30] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)

*Peixian Li, Yu Tian, Ruiqi Tu, Chengkai Wu, Jingjing Ren, Jingsong Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Reasoning, Medical Diagnosis, Attention Mechanism, Deep Learning

**Relevance Score:** 9

**TL;DR:** The paper presents a framework to enhance diagnostic accuracy and clinical reasoning in LLM-based medical diagnosis, improving reliability in complex clinical scenarios.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limited diagnostic reliability of LLMs in complex clinical scenarios and improve their performance in medical diagnostics.

**Method:** The study proposes an Etiology-Aware Attention Steering Framework, which incorporates structured clinical reasoning through Clinical Reasoning Scaffolding and applies a Reasoning-Guided Loss function for fine-tuning.

**Key Contributions:**

	1. Etiology-Aware Attention Steering Framework
	2. Integration of Clinical Reasoning Scaffolding
	3. Reasoning-Guided Parameter-Efficient Fine-tuning

**Result:** The framework improves average diagnostic accuracy by 15.65% and the Reasoning Focus Score by 31.6% on the Consistent Diagnosis Cohort, and shows effectiveness in external validation on the Discrepant Diagnosis Cohort.

**Limitations:** 

**Conclusion:** The study demonstrates a practical method for enhancing clinical reasoning in LLM-based diagnosis, offering a promising approach for developing interpretable AI diagnostic systems in complex settings.

**Abstract:** Objective: Large Language Models (LLMs) demonstrate significant capabilities in medical text understanding and generation. However, their diagnostic reliability in complex clinical scenarios remains limited. This study aims to enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We propose an Etiology-Aware Attention Steering Framework to integrate structured clinical reasoning into LLM-based diagnosis. Specifically, we first construct Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines for three representative acute abdominal emergencies: acute appendicitis, acute pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head Identification algorithm to pinpoint attention heads crucial for the model's etiology reasoning. To ensure reliable clinical reasoning alignment, we introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds etiological reasoning cues into input representations and steers the selected Etiology-Aware Heads toward critical information through a Reasoning-Guided Loss function. Result: On the Consistent Diagnosis Cohort, our framework improves average diagnostic accuracy by 15.65% and boosts the average Reasoning Focus Score by 31.6% over baselines. External validation on the Discrepant Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic accuracy. Further assessments via Reasoning Attention Frequency indicate that our models exhibit enhanced reliability when faced with real-world complex scenarios. Conclusion: This study presents a practical and effective approach to enhance clinical reasoning in LLM-based diagnosis. By aligning model attention with structured CRS, the proposed framework offers a promising paradigm for building more interpretable and reliable AI diagnostic systems in complex clinical settings.

</details>


### [31] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)

*Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Optimization, Efficiency, Text Generation, Benchmarking

**Relevance Score:** 8

**TL;DR:** This paper benchmarks optimizations for large language models, assessing their impact on efficiency and text generation quality, particularly in long-context scenarios.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the resource demands and limited context windows of large language models and evaluate the effectiveness of existing optimization techniques in long-context scenarios.

**Method:** The study systematically benchmarks individual and combined optimization techniques for large language models, analyzing memory usage, latency, throughput, and text generation quality across two LLM architectures.

**Key Contributions:**

	1. Systematic benchmarking of LLM optimizations focusing on long context scenarios.
	2. Insight into negative effects of naive combination of optimization methods in larger models.
	3. Recommendations for balancing efficiency and accuracy in LLM implementations.

**Result:** The analysis reveals that naive combinations of optimization techniques can negatively impact performance in larger models due to compounded errors, and that relying on the F1 score alone may obscure trade-offs in precision and recall.

**Limitations:** The study primarily focuses on specific LLM architectures and may not generalize to all model types or tasks.

**Conclusion:** Integrating system-level profiling with task-specific insights, this study aids LLM practitioners in finding a balance between efficiency, accuracy, and scalability across various tasks and hardware.

**Abstract:** Large language models (LLMs) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two LLM architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps LLM practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.

</details>


### [32] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)

*Kaiyan Zhao, Zhongtao Miao, Yoshimasa Tsuruoka*

**Main category:** cs.CL

**Keywords:** multimodal embeddings, object-phrase alignment, contrastive learning

**Relevance Score:** 5

**TL;DR:** MCSEO enhances multimodal sentence embeddings by improving object-phrase alignment, using segmentation and object detection to optimize a contrastive learning objective.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the noise in image-caption pairs which can contain irrelevant information, and to improve the accuracy of multimodal sentence embeddings.

**Method:** MCSEO incorporates fine-grained object-phrase alignment alongside traditional image-caption alignment, utilizing segmentation and object detection models for accurate object-phrase pairs.

**Key Contributions:**

	1. Development of MCSEO that leverages fine-grained object-phrase alignment
	2. Demonstration of the effectiveness of MCSEO in multimodal embeddings
	3. Highlighting the importance of object-phrase correspondence in multimodal learning

**Result:** Experimental results indicate that MCSEO outperforms strong baselines in semantic textual similarity tasks across various backbone models.

**Limitations:** 

**Conclusion:** Precise object-phrase alignment is crucial for effective multimodal representation learning.

**Abstract:** Multimodal sentence embedding models typically leverage image-caption pairs in addition to textual data during training. However, such pairs often contain noise, including redundant or irrelevant information on either the image or caption side. To mitigate this issue, we propose MCSEO, a method that enhances multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. Specifically, MCSEO utilizes existing segmentation and object detection models to extract accurate object-phrase pairs, which are then used to optimize a contrastive learning objective tailored to object-phrase correspondence. Experimental results on semantic textual similarity (STS) tasks across different backbone models demonstrate that MCSEO consistently outperforms strong baselines, highlighting the significance of precise object-phrase alignment in multimodal representation learning.

</details>


### [33] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)

*Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-Agent Interaction, Reinforcement Learning, Decision-Making, Planning

**Relevance Score:** 8

**TL;DR:** This paper introduces AdaPlan, an adaptive global plan-based agent paradigm, and PilotRL, a training framework for LLM agents that enhances long-term decision-making and coordination between planning and execution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing agent paradigms limit LLM effectiveness in complex tasks requiring long-term planning and struggle with generalization due to reliance on supervised fine-tuning.

**Method:** Introduce AdaPlan for high-level explicit guidance in decision-making and develop PilotRL to optimize planning with progressive reinforcement learning.

**Key Contributions:**

	1. Introduction of the AdaPlan paradigm for enhanced decision-making in agent tasks
	2. Development of PilotRL for global planning-guided training
	3. Demonstrated improvements over current state-of-the-art LLM models in performance.

**Result:** PilotRL achieves state-of-the-art performance, outperforming GPT-4o by 3.60% and GPT-4o-mini by 55.78% in related tasks.

**Limitations:** 

**Conclusion:** The proposed methodologies enhance LLM agents' abilities in long-horizon decision-making and coordination, addressing key challenges in agent design.

**Abstract:** Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [34] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)

*Alan Dao, Dinh Bach Vu, Alex Nguyen, Norapat Buppodom*

**Main category:** cs.CL

**Keywords:** small language models, dynamic task vector, reinforcement learning, knowledge-intensive tasks, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces Lucy, a 1.7B-parameter small language model that utilizes a dynamic task vector machine for enhanced reasoning, achieving competitive performance with larger models on the SimpleQA benchmark.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of small language models (SLMs) in knowledge-intensive tasks by developing a method that enhances their reasoning capabilities through dynamic task vector construction.

**Method:** The authors optimize a dynamic task vector machine through RLVR, interpreting the model's internal reasoning as an evolving process rather than a fixed one.

**Key Contributions:**

	1. Introduction of a dynamic task vector machine for SLMs
	2. Development of the Lucy model with enhanced reasoning capabilities
	3. Demonstration of SLMs' competitive performance on knowledge-intensive tasks

**Result:** Lucy achieves 78.3% accuracy on the SimpleQA benchmark, which is comparable to larger models like DeepSeek-V3.

**Limitations:** 

**Conclusion:** Structured, self-constructed task reasoning allows small models to rival larger counterparts, suggesting a paradigm shift in how we utilize SLMs.

**Abstract:** Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process. In this work, we propose a new paradigm: viewing the model's internal reasoning, delimited by <think> and </think> tags, as a dynamic task vector machine. Rather than treating the content inside these tags as a mere trace of thought, we interpret the generation process itself as a mechanism through which the model \textbf{constructs and refines its own task vectors} on the fly. We developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model. We present Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning.

</details>


### [35] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)

*Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun*

**Main category:** cs.CL

**Keywords:** Transformer, Large Language Models, Edge Computing, Fine-tuning, Post-training Quantization

**Relevance Score:** 9

**TL;DR:** This paper presents EdgeInfinite-Instruct, a fine-tuned model for deploying Transformer-based LLMs on resource-constrained edge devices, focusing on efficiency improvements for long-sequence tasks like summarization and question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Deploying LLMs on edge devices is challenging due to computational costs and efficiency issues. Existing optimizations are insufficient for reducing time to first token and maintaining performance.

**Method:** The paper proposes EdgeInfinite-Instruct, which utilizes a Segmented Supervised Fine-Tuning strategy and fine-grained post-training quantization to enhance deployment efficiency on edge NPUs.

**Key Contributions:**

	1. Introduction of Segmented Supervised Fine-Tuning strategy for long-sequence tasks
	2. Optimization for NPU-accelerated edge devices through post-training quantization
	3. Maintaining computational efficiency while improving model performance

**Result:** Experiments demonstrate that EdgeInfinite-Instruct improves domain-specific performance while retaining efficiency on long-context benchmarks and real-world mobile tasks.

**Limitations:** Instruction-following ability is limited and lack of mobile-specific optimizations.

**Conclusion:** EdgeInfinite-Instruct effectively addresses limitations of previous models, offering optimized deployment for LLMs on edge devices without compromising performance.

**Abstract:** Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.

</details>


### [36] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)

*Dingzirui Wang, Xuangliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng*

**Main category:** cs.CL

**Keywords:** in-context learning, demonstration selection, gradient flow, LLMs, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the ineffectiveness of demonstrations in in-context learning (ICL), proposing a novel selection method, GradS, that improves performance by leveraging gradient flow.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing studies on in-context learning assume all demonstrations are effective, but many are not, necessitating investigation into the reasons behind demonstration ineffectiveness.

**Method:** The authors analyze gradient flow and linear self-attention models, proposing GradS, which selects demonstrations based on the gradient flow magnitude with respect to the user query.

**Key Contributions:**

	1. Investigation of demonstration ineffectiveness in ICL
	2. Introduction of GradS for effective demonstration selection using gradient flow
	3. Empirical validation on four LLMs across five datasets.

**Result:** GradS proves to significantly improve ICL effectiveness, achieving a relative improvement of 6.8% over the strongest baselines and confirming the amplification of demonstration effectiveness disparity in deeper models.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of considering both the relevance to the user query and the information already learned by the model in demonstration selection for ICL.

**Abstract:** Numerous studies have investigated the underlying mechanisms of in-context learning (ICL) effectiveness to inspire the design of related methods. However, existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness. Our analysis is based on gradient flow and linear self-attention models. By setting the gradient flow to zero, we deduce that a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones. Considering that current demonstration selection methods primarily focus on the relevance to the user query while overlooking the information that the model has already assimilated, we propose a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones. We validate our derivation and GradS on four prominent LLMs across five mainstream datasets. The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on average over the strongest baselines, demonstrating its effectiveness.

</details>


### [37] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)

*Hengxing Cai, Jinhan Dong, Yijie Rao, Jingcheng Deng, Jingjun Tan, Qien Chen, Haidong Wang, Zhen Wang, Shiyu Huang, Agachai Sumalee, Renxin Zhong*

**Main category:** cs.CL

**Keywords:** Unmanned Aerial Vehicle, Vision-Language Navigation, Reinforcement Learning, Curriculum Learning, Training Efficiency

**Relevance Score:** 7

**TL;DR:** This paper presents a training framework called Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS) that enhances Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) by improving training efficiency through a systematic integration of Curriculum Learning in reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the training efficiency and performance of UAV Vision-Language Navigation (VLN) due to challenges like inefficient data usage and slow convergence in existing reinforcement learning methods.

**Method:** The SA-GCS framework employs a Semantic-Aware Difficulty Estimator to measure sample complexity and a Gaussian Curriculum Scheduler to optimize task progression from easy to hard.

**Key Contributions:**

	1. Introduction of Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS) for RL in UAV VLN.
	2. Implementation of a Semantic-Aware Difficulty Estimator for training sample complexity assessment.
	3. Demonstration of enhanced training efficiency and performance through extensive benchmarking.

**Result:** SA-GCS significantly outperforms strong baselines in training efficiency, convergence speed, and overall model performance as demonstrated in extensive experiments on the CityNav benchmark.

**Limitations:** 

**Conclusion:** The proposed SA-GCS framework achieves faster and more stable convergence while generalizing well across different model scales, showing robustness and scalability, and is publicly available for use.

**Abstract:** Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available.

</details>


### [38] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)

*Rana Salama, Abdou Youssef, Mona Diab*

**Main category:** cs.CL

**Keywords:** Discrete Wavelet Transforms, Natural Language Processing, Word Embeddings

**Relevance Score:** 7

**TL;DR:** This paper explores the application of Discrete Wavelet Transforms (DWT) to enhance word and sentence embeddings for Natural Language Processing, demonstrating effective dimensionality reduction and information consolidation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage wavelets in NLP tasks to capture linguistic properties and improve the representation of word and sentence embeddings.

**Method:** The authors apply DWT to word vectors and combine it with Discrete Cosine Transform (DCT) to create a non-parameterized model that compresses sentences while retaining key information.

**Key Contributions:**

	1. Application of DWT in NLP embeddings
	2. Development of a non-parameterized model for sentence compression
	3. Demonstration of superior performance compared to original embeddings

**Result:** The proposed model shows effectiveness on downstream applications, yielding results that are comparable or superior to original embeddings in some cases.

**Limitations:** 

**Conclusion:** Wavelets, through the implementation of DWT and DCT, can significantly enhance the representation of NLP embeddings, leading to better performance in various tasks.

**Abstract:** Wavelets have emerged as a cutting edge technology in a number of fields. Concrete results of their application in Image and Signal processing suggest that wavelets can be effectively applied to Natural Language Processing (NLP) tasks that capture a variety of linguistic properties. In this paper, we leverage the power of applying Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We first evaluate, intrinsically and extrinsically, how wavelets can effectively be used to consolidate important information in a word vector while reducing its dimensionality. We further combine DWT with Discrete Cosine Transform (DCT) to propose a non-parameterized model that compresses a sentence with a dense amount of information in a fixed size vector based on locally varying word features. We show the efficacy of the proposed paradigm on downstream applications models yielding comparable and even superior (in some tasks) results to original embeddings.

</details>


### [39] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)

*Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang*

**Main category:** cs.CL

**Keywords:** Graph Neural Networks, Agent-based planning, Retrieval-augmented generation, Node-level decision-making, Few-shot learning

**Relevance Score:** 8

**TL;DR:** ReaGAN is an innovative framework that enhances graph learning using autonomous, node-level decision-making and retrieval-augmented generation, addressing limitations of traditional GNNs.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the limitations of fixed aggregation mechanisms in GNNs, which struggle with node informativeness imbalance and capture of global semantic relationships.

**Method:** ReaGAN employs an agent-based approach where each node functions as an independent agent planning its actions based on internal memory, complemented by retrieval-augmented generation for accessing relevant content.

**Key Contributions:**

	1. Introduction of Retrieval-augmented Graph Agentic Network (ReaGAN) framework.
	2. Empowerment of nodes with autonomous decision-making capabilities.
	3. Incorporation of retrieval-augmented generation for enhancing global semantic understanding.

**Result:** ReaGAN demonstrates competitive performance in few-shot settings using a frozen LLM backbone without requiring fine-tuning, indicating the efficacy of agentic planning and retrieval in enhancing graph learning.

**Limitations:** The paper is a work in progress, indicating that further validation and refinement of the approach may be needed.

**Conclusion:** The proposed framework showcases significant potential in addressing limitations of existing GNN methods and enhances the ability to model complex relationships within graphs.

**Abstract:** Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.

</details>


### [40] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)

*Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding*

**Main category:** cs.CL

**Keywords:** Large Language Models, Dialogue Evaluation, HCI, Machine Learning, Efficiency

**Relevance Score:** 9

**TL;DR:** This paper introduces an efficient multi-turn dialogue evaluator that aggregates the feedback of multiple LLM judges into a single model, addressing biases and reducing computational costs in dialogue quality assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of dialogue quality while mitigating biases inherent in traditional LLM-as-a-judge approaches and reducing computational overhead.

**Method:** The paper proposes an efficient multi-turn dialogue evaluator that combines preference knowledge from multiple LLM judges into a single model for dialogue assessment.

**Key Contributions:**

	1. Introduction of a multi-turn dialogue evaluator that aggregates multiple LLM judges' feedback
	2. Reduction of computational overhead compared to traditional multi-judge approaches
	3. Empirical validation showing superior performance over existing methods

**Result:** The proposed method outperforms existing baselines in evaluation benchmarks while significantly reducing inference costs.

**Limitations:** 

**Conclusion:** The paper demonstrates that the new multi-turn dialogue evaluator provides robust and efficient dialogue quality assessments, making it a valuable tool in evaluating conversational abilities of LLMs.

**Abstract:** Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.

</details>


### [41] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)

*Jeongwoo Kang, Markarit Vartampetian, Felix Herron, Yongxin Zhou, Diandra Fabre, Gabriela Gonzalez-Saez*

**Main category:** cs.CL

**Keywords:** question-answering, meeting transcripts, retrieval augmented generation, Abstract Meaning Representation, SIGDial

**Relevance Score:** 7

**TL;DR:** The paper presents GETALP's approach to question-answering from meeting transcripts using a RAG system and AMR.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved question-answering techniques in the context of meeting transcripts.

**Method:** The authors developed three systems that combine retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR) to enhance question-answering.

**Key Contributions:**

	1. Introduction of a RAG system combined with AMR for question-answering
	2. Demonstration of improved performance in distinguishing questions among participants
	3. Empirical evaluation showing 35% high-quality response rate

**Result:** The proposed methods achieved high-quality responses for about 35% of questions, particularly excelling in questions requiring differentiation among participants.

**Limitations:** 

**Conclusion:** Incorporating AMR significantly improves the quality of responses in question-answering tasks from meeting transcripts.

**Abstract:** This paper documents GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts. Our method is based on a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR). We propose three systems combining these two approaches. Our results show that incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions).

</details>


### [42] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)

*Yixuan Tang, Jincheng Wang, Anthony K. H. Tung*

**Main category:** cs.CL

**Keywords:** half-truth detection, fact verification, misinformation, contextual reasoning, TRACER

**Relevance Score:** 6

**TL;DR:** This paper introduces the task of half-truth detection and presents TRACER, a framework for identifying omission-based misinformation in political claims.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Many claims are factually correct but misleading due to omitted context; existing fact verification models do not account for this.

**Method:** TRACER is a modular re-assessment framework that aligns evidence, infers claim intent, and estimates the impact of hidden content.

**Key Contributions:**

	1. Introduction of half-truth detection as a distinct task
	2. Creation of the PolitiFact-Hidden benchmark with annotated political claims
	3. Development of TRACER framework for detecting omission-based misinformation

**Result:** TRACER improves Half-True classification F1 by up to 16 points across multiple strong baselines, demonstrating the need to model omissions.

**Limitations:** 

**Conclusion:** Incorporating context and omitted content is crucial for enhancing the reliability of fact verification systems.

**Abstract:** Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about what is left unsaid. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification.

</details>


### [43] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)

*Jiaxin Deng, Qingcheng Zhu, Junbiao Pang, Linlin Yang, Zhongqian Fu, Baochang Zhang*

**Main category:** cs.CL

**Keywords:** Low-Rank Adaptation, Sharpness-Aware Minimization, Model Generalization, Deep Learning, Transformers

**Relevance Score:** 7

**TL;DR:** This paper presents Flat-LoRA and its efficient variant EFlat-LoRA, aimed at optimizing low-rank adaptation while enhancing model generalization through exploring sharpness-aware techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the correlation between expressive ability and generalization ability of low-rank adaptations (LoRA) and to improve general model performance.

**Method:** The authors propose Flat-LoRA and EFlat-LoRA to achieve flat minima through theoretically demonstrating perturbation transfer in the low-rank subspace, along with extensive empirical experiments.

**Key Contributions:**

	1. Introduction of Flat-LoRA and EFlat-LoRA for seeking flat minima in low-rank adaptation.
	2. Theoretical demonstration of perturbation transfer in low-rank subspace enhances understanding of generalization.
	3. Empirical validation of significant performance improvements across various language and vision-language models.

**Result:** EFalt-LoRA outperformed LoRA and full fine-tuning on various benchmarks, including 1.0% on GLUE dataset with RoBERTa-large and improvements on vision-language models such as Qwen-VL-Chat.

**Limitations:** 

**Conclusion:** The findings suggest that improving generalization for LoRA via sharpness-aware techniques is effective and reveals a significant connection that has been overlooked by previous research.

**Abstract:** Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware Minimization (SAM) improves model generalization for both Convolutional Neural Networks (CNNs) and Transformers by encouraging convergence to locally flat minima. However, the connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods. In this work, we propose Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for LoRA. Concretely, we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace. This approach eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace. Our extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively. These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [44] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)

*Giulio Zhou, Tsz Kin Lam, Alexandra Birch, Barry Haddow*

**Main category:** cs.CL

**Keywords:** prosody, emojis, speech, communication, interpretation

**Relevance Score:** 7

**TL;DR:** This study explores the relationship between emojis and prosodic features in speech, showing how emojis influence spoken delivery and listener interpretation of prosodic cues.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how emojis affect prosodic realisation in speech and how they help listeners interpret meaning in the absence of verbal cues.

**Method:** The study involved analyzing human speech data collected from structured production and perception tasks, linking prosody with emoji use directly.

**Key Contributions:**

	1. Empirical evidence linking emoji semantics with prosodic delivery
	2. Demonstration of listener ability to discern emoji meanings from prosody
	3. Insights into the communicative role of emojis in digitally mediated interactions

**Result:** Speakers adapt their prosody based on emoji cues; listeners can identify intended emojis from prosodic variations alone, and greater semantic differences in emojis lead to more significant prosodic divergence.

**Limitations:** 

**Conclusion:** Emojis serve as meaningful carriers of prosodic intent, providing insights into their role in communication in digital contexts.

**Abstract:** Prosodic features such as pitch, timing, and intonation are central to spoken communication, conveying emotion, intent, and discourse structure. In text-based settings, where these cues are absent, emojis act as visual surrogates that add affective and pragmatic nuance. This study examines how emojis influence prosodic realisation in speech and how listeners interpret prosodic cues to recover emoji meanings. Unlike previous work, we directly link prosody and emoji by analysing actual human speech data, collected through structured but open-ended production and perception tasks. This provides empirical evidence of how emoji semantics shape spoken delivery and perception. Results show that speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence. These findings suggest that emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts.

</details>


### [45] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)

*Joonas Tapaninaho, Mourad Oussala*

**Main category:** cs.CL

**Keywords:** large-language models, transformer architecture, training efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces PaPaformer, a decoder-only transformer-based model that trains in hours by using lower-dimensional parallel paths.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the computation power and time required for training large language models, specifically decoder-only transformers.

**Method:** PaPaformer employs lower-dimensional parallel paths that can be trained individually and then combined into a larger model.

**Key Contributions:**

	1. Introduction of the PaPaformer architecture
	2. Demonstration of training lower-dimensional paths individually
	3. Method to combine paths into a larger model for enhanced performance

**Result:** The approach enables significant reductions in total model parameters and training time while improving performance.

**Limitations:** 

**Conclusion:** The parallel path structure allows for customization to meet specific task requirements, enhancing versatility and efficiency.

**Abstract:** The training of modern large-language models requires an increasingly amount of computation power and time. Even smaller variants, such as small-language models (SLMs), take several days to train in the best-case scenarios, often requiring multiple GPUs. This paper explores methods to train and evaluate decoder-only transformer-based language models in hours instead of days/weeks. We introduces \textit{PaPaformer}, a decoder-only transformer architecture variant, whose lower-dimensional parallel paths are combined into larger model. The paper shows that these lower-dimensional paths can be trained individually with different types of training data and then combined into one larger model. This method gives the option to reduce the total number of model parameters and the training time with increasing performance. Moreover, the use of parallel path structure opens interesting possibilities to customize paths to accommodate specific task requirements.

</details>


### [46] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)

*Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, synthetic CCoT, difficulty classifier, LLM, efficiency

**Relevance Score:** 8

**TL;DR:** Introducing SynAdapt, an efficient reasoning framework that utilizes synthetic continuous Chain-of-Thought (CCoT) to improve LLM alignment and performance on complex questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of Chain-of-Thought reasoning in large language models (LLMs) while addressing the limitations of existing continuous CCoT methods.

**Method:** The proposed SynAdapt framework generates synthetic CCoT as a precise alignment target and integrates a difficulty classifier to identify challenging questions, prompting the LLM for improved responses.

**Key Contributions:**

	1. Development of SynAdapt framework for efficient reasoning
	2. Integration of a difficulty classifier to identify hard questions
	3. Achievement of improved accuracy-efficiency trade-off in LLM performance

**Result:** Experimental results across various benchmarks indicate that SynAdapt achieves the best accuracy-efficiency trade-off compared to previous methods.

**Limitations:** 

**Conclusion:** SynAdapt effectively improves LLM performance on complex tasks by utilizing synthetic CCoT and an adaptive difficulty assessment approach.

**Abstract:** While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.

</details>


### [47] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)

*Mingruo Yuan, Shuyi Zhang, Ben Kao*

**Main category:** cs.CL

**Keywords:** confidence estimation, large language models, contextual information, machine learning, health informatics

**Relevance Score:** 9

**TL;DR:** This paper introduces CRUX, a novel framework for improving confidence estimation in large language models by incorporating contextual information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of relevance between LLM responses and contextual information in current confidence estimation methods, which is crucial for assessing output quality.

**Method:** CRUX combines two metrics: contextual entropy reduction, which measures data uncertainty using contrastive sampling, and unified consistency examination, which evaluates model uncertainty through the consistency of answers with and without context.

**Key Contributions:**

	1. Introduction of a context-aware framework for confidence estimation in LLMs
	2. Development of two novel metrics: contextual entropy reduction and unified consistency examination
	3. Demonstration of CRUX's superior performance on benchmark datasets compared to existing baselines.

**Result:** Experimental results show that CRUX outperforms existing methods, achieving the highest AUROC on various benchmark and domain-specific datasets.

**Limitations:** 

**Conclusion:** CRUX provides a significant advancement in confidence estimation for LLMs, enhancing their deployment in safety-critical applications.

**Abstract:** Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines.

</details>


### [48] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)

*Farhana Haque, Md. Abdur Rahman, Sumon Ahmed*

**Main category:** cs.CL

**Keywords:** topic modeling, Graph Convolutional Network, Bengali, NLP, Non-negative Matrix Factorization

**Relevance Score:** 4

**TL;DR:** This paper proposes a novel Graph Convolutional Network-based topic modeling method for Bengali text, addressing the deficiencies in existing resources and techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Topic modeling in Bengali presents unique challenges due to morphological complexity and resource scarcity, creating a need for advanced models.

**Method:** The proposed GHTM model uses Graph Convolutional Networks to derive semantically rich document embeddings that are then decomposed with Non-negative Matrix Factorization for topic representation.

**Key Contributions:**

	1. Introduction of the GHTM model combining GCN and NMF for topic modeling in Bengali.
	2. Development of the NCTBText dataset sourced from Bengali textbooks.
	3. Comparative analysis showcasing superior performance in topic coherence and diversity.

**Result:** Experimental results show that GHTM outperforms traditional and contemporary Bengali topic modeling techniques in terms of topic coherence and diversity.

**Limitations:** The study focuses solely on Bengali, limiting its applicability to other languages and contexts.

**Conclusion:** The introduction of GHTM and the new NCTBText dataset significantly advances the field of topic modeling in Bengali, demonstrating improved performance over existing methods.

**Abstract:** Topic modeling is a Natural Language Processing (NLP) technique that is used to identify latent themes and extract topics from text corpora by grouping similar documents based on their most significant keywords. Although widely researched in English, topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives. In this contribution, a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus. This study compares the proposed model against a wide range of Bengali topic modeling techniques, from traditional methods such as LDA, LSA, and NMF to contemporary frameworks such as BERTopic and Top2Vec on three Bengali datasets. The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity. In addition, we introduce a novel Bengali dataset called "NCTBText" sourced from Bengali textbook materials to enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [49] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)

*Lennart Meincke, Ethan Mollick, Lilach Mollick, Dan Shapiro*

**Main category:** cs.CL

**Keywords:** AI performance, prompting techniques, model evaluation

**Relevance Score:** 6

**TL;DR:** This report tests beliefs about improving AI performance through tipping and threatening AI models, finding no significant overall effect but notable variability in results per question.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To help leaders understand the technical aspects of working with AI through rigorous testing of common prompting beliefs.

**Method:** The study tests the effects of prompting tactics (tipping and threatening) on model performance using benchmarks GPQA and MMLU-Pro.

**Key Contributions:**

	1. Empirical testing of prompting techniques using specific AI benchmarks.
	2. Demonstration that general tactics like threatening or tipping do not significantly affect performance.
	3. Insight into the variability of model responses to different prompts.

**Result:** The investigation shows that threatening or tipping a model does not generally impact overall benchmark performance, but prompt variations can significantly affect performance on specific questions.

**Limitations:** The effects of prompting can only be determined on a per-question basis, making it difficult to generalize the effectiveness.

**Conclusion:** The findings suggest that the effectiveness of simple prompting variations is uncertain, especially for challenging questions, highlighting the variability in model responses.

**Abstract:** This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).   We demonstrate two things:   - Threatening or tipping a model generally has no significant effect on benchmark performance.   - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.   Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.

</details>


### [50] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)

*Shantanu Thorat, Andrew Caines*

**Main category:** cs.CL

**Keywords:** AIG text detection, machine learning, dataset, classification, language models

**Relevance Score:** 7

**TL;DR:** This paper introduces the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a dataset aimed at improving AI-generated text detectors, which struggle in practical applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing AI-generated text detectors perform well under test conditions but fail in real-world scenarios, indicating a need for better robustness.

**Method:** We developed a dataset (DACTYL) focusing on one-shot/few-shot generations and used two methods to train classifiers: binary cross-entropy (BCE) and deep X-risk optimization (DXO).

**Key Contributions:**

	1. Introduction of DACTYL dataset for AIG text detection
	2. Comparison of BCE and DXO optimization approaches
	3. Findings on performance discrepancies between training methods

**Result:** BCE-trained classifiers outperformed DXO classifiers on our DACTYL test set, but DXO classifiers showed superior performance on out-of-distribution texts, significantly improving essay detection metrics.

**Limitations:** 

**Conclusion:** The study demonstrates the inadequacies in current AIG text detectors, proposes enhancements, and emphasizes the need for training on diverse datasets.

**Abstract:** Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.

</details>


### [51] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)

*Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical reasoning, taxonomy, evaluation benchmarks, multimodal reasoning

**Relevance Score:** 10

**TL;DR:** A systematic review of Large Language Models (LLMs) in medicine focusing on their reasoning capabilities, categorizing enhancement techniques, and addressing critical challenges in clinical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in LLMs' ability to perform systematic, transparent, and verifiable reasoning in clinical practice.

**Method:** The paper reviews 60 seminal studies from 2022-2025 and proposes a taxonomy of reasoning enhancement techniques categorized into training-time strategies and test-time mechanisms, analyzed across various data modalities and clinical applications.

**Key Contributions:**

	1. First systematic review of LLMs designed for medical reasoning.
	2. Proposed taxonomy of reasoning enhancement techniques.
	3. Identified critical challenges and future research directions for medical AI.

**Result:** Development of a taxonomy for reasoning enhancement techniques, analysis of application across data modalities, and evolution of evaluation benchmarks for LLMs in medical contexts.

**Limitations:** The study is limited to the analysis of 60 studies from 2022-2025 and may not encompass the entire landscape of medical reasoning enhancement techniques.

**Conclusion:** Identifies critical challenges such as the faithfulness-plausibility gap and emphasizes the need for native multimodal reasoning in medical AI, suggesting future directions for improving reasoning capabilities.

**Abstract:** The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

</details>


### [52] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)

*Farhan Farsi, Farnaz Aghababaloo, Shahriar Shariati Motlagh, Parsa Ghofrani, MohammadAli SadraeiJavaheri, Shayan Bali, Amirhossein Shabani, Farbod Bijary, Ghazal Zamaninejad, AmirMohammad Salehoof, Saeedeh Momtazi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Persian Language, Evaluation Datasets, Cultural Context, Benchmarking

**Relevance Score:** 8

**TL;DR:** This paper introduces 19 evaluation datasets for assessing large language models (LLMs) in the Persian language and Iranian culture, aimed at addressing cultural evaluation gaps.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the evaluation gap for LLMs in non-Western languages, specifically Persian, and to improve their cultural context understanding.

**Method:** The authors created 19 new evaluation datasets focused on Iranian law, Persian grammar, idioms, and entrance exam topics, and benchmarked these against 41 prominent LLMs.

**Key Contributions:**

	1. Introduction of 19 new evaluation datasets for Persian LLMs
	2. Benchmarking of 41 prominent LLMs against these datasets
	3. Highlighting cultural biases in LLMs trained predominantly on Western data.

**Result:** The benchmarking revealed significant performance insights of LLMs when faced with Persian language nuances and Iranian cultural contexts.

**Limitations:** Limited to Persian language and Iranian culture; findings may not generalize to other non-Western languages.

**Conclusion:** The study highlights the need for more culturally aware benchmarks and provides a framework for evaluating LLMs in underrepresented languages and cultures.

**Abstract:** As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.

</details>


### [53] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)

*Gleb Schmidt, Johannes R旦misch, Mariia Halchynska, Svetlana Gorovaia, Ivan P. Yamshchikov*

**Main category:** cs.CL

**Keywords:** style change detection, computational authorship analysis, pre-trained language models

**Relevance Score:** 6

**TL;DR:** This paper addresses style change detection in documents at the sentence level using a Sequential Sentence Pair Classifier (SSPC) model based on pre-trained language models and BiLSTM.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Style change detection is a challenging problem in computational authorship analysis, particularly for pinpointing style shifts at the level of individual sentences.

**Method:** The proposed method utilizes a Sequential Sentence Pair Classifier (SSPC) which leverages pre-trained language models to represent sentences, which are then contextualized using a BiLSTM and combined into a multi-layer perceptron for prediction.

**Key Contributions:**

	1. Introduced a Sequential Sentence Pair Classifier (SSPC) for style change detection at the sentence level.
	2. Showed strong performance on PAN-2025 datasets, surpassing established benchmarks.
	3. Demonstrated effectiveness in handling stylistically shallow short sentences.

**Result:** The model achieved strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD datasets of the PAN-2025 challenge, outperforming both random baselines and a more advanced reference model.

**Limitations:** 

**Conclusion:** The SSPC model effectively captures contextual information in detecting style shifts, even in the presence of stylistically shallow sentences, indicating its potential for further applications in authorship analysis.

**Abstract:** Style change detection - identifying the points in a document where writing style shifts - remains one of the most important and challenging problems in computational authorship analysis. At PAN 2025, the shared task challenges participants to detect style switches at the most fine-grained level: individual sentences. The task spans three datasets, each designed with controlled and increasing thematic variety within documents. We propose to address this problem by modeling the content of each problem instance - that is, a series of sentences - as a whole, using a Sequential Sentence Pair Classifier (SSPC). The architecture leverages a pre-trained language model (PLM) to obtain representations of individual sentences, which are then fed into a bidirectional LSTM (BiLSTM) to contextualize them within the document. The BiLSTM-produced vectors of adjacent sentences are concatenated and passed to a multi-layer perceptron for prediction per adjacency. Building on the work of previous PAN participants classical text segmentation, the approach is relatively conservative and lightweight. Nevertheless, it proves effective in leveraging contextual information and addressing what is arguably the most challenging aspect of this year's shared task: the notorious problem of "stylistically shallow", short sentences that are prevalent in the proposed benchmark data. Evaluated on the official PAN-2025 test datasets, the model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively, outperforming not only the official random baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot performance.

</details>


### [54] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)

*Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya*

**Main category:** cs.CL

**Keywords:** legal retrieval, precedent, BM25, machine learning, natural language processing

**Relevance Score:** 2

**TL;DR:** TraceRetriever is a legal precedent retrieval system designed to work with limited case information, improving traditional methods by extracting rhetorically significant segments from documents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity and volume of legal documents create challenges for traditional legal retrieval methods, necessitating an improved approach that can operate effectively with partial information.

**Method:** The approach utilizes BM25, Vector Database, and Cross-Encoder models, combining results through Reciprocal Rank Fusion and ultimately re-ranking them. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian court judgments.

**Key Contributions:**

	1. Introduces a retrieval method that operates with limited case information
	2. Combines multiple models for improved search results
	3. Generates rhetorical annotations to enhance relevance of retrieved legal precedents

**Result:** Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever shows improved retrieval performance, successfully addressing challenges posed by the increasing volume of legal documents.

**Limitations:** 

**Conclusion:** TraceRetriever provides a reliable and scalable solution for legal precedent retrieval, enhancing legal research capabilities even when only partial case knowledge is available.

**Abstract:** Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.

</details>


### [55] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)

*Johannes R旦misch, Svetlana Gorovaia, Mariia Halchynska, Gleb Schmidt, Ivan P. Yamshchikov*

**Main category:** cs.CL

**Keywords:** large language models, style change detection, authorship analysis, zero-shot learning

**Relevance Score:** 9

**TL;DR:** The study examines the zero-shot performance of large language models on sentence-level style change detection in authorship analysis, benchmarking them on recent datasets and revealing their sensitivity to writing style variations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the zero-shot capabilities of large language models in detecting style changes at the sentence level in authorship analysis, particularly given their potential applications in various fields.

**Method:** Benchmarking four state-of-the-art large language models on the PAN 2024 and 2025 Multi-Author Writing Style Analysis datasets to evaluate their performance on sentence-level style change detection tasks.

**Key Contributions:**

	1. First-time evaluation of LLMs on sentence-level style change detection
	2. Identification of model sensitivity to writing style variations
	3. Establishment of new performance baselines for future authorship analysis tasks

**Result:** The generative models demonstrated sensitivity to variations in writing style, establishing a challenging baseline that outperformed previous PAN competition baselines.

**Limitations:** 

**Conclusion:** The latest LLMs show increased sensitivity to content-independent and stylistic signals, which may influence their predictive performance in authorship analysis.

**Abstract:** This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.

</details>


### [56] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)

*Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya*

**Main category:** cs.CL

**Keywords:** Legal Judgment Prediction, Retrieval-Augmented Generation, Legal reasoning, Machine Learning, Indian legal system

**Relevance Score:** 4

**TL;DR:** The paper presents NyayaRAG, a framework for Legal Judgment Prediction in the Indian legal context, which integrates factual case descriptions with relevant legal statutes and prior cases to enhance predictive accuracy and explanation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of legal judgment prediction models by incorporating a broader range of inputs, including statutory provisions and judicial precedents, often overlooked in prior work.

**Method:** NyayaRAG is a Retrieval-Augmented Generation framework that simulates courtroom scenarios by using case descriptions, legal statutes, and retrieved past cases combined with a domain-specific pipeline.

**Key Contributions:**

	1. Introduction of NyayaRAG for LJP incorporating legal statutes and prior cases.
	2. Demonstration of improved predictive accuracy and explanation quality through empirical evaluation.
	3. Utilization of domain-specific metrics alongside LLM-based evaluators.

**Result:** The study demonstrates that the integration of structured legal knowledge with factual inputs significantly enhances the predictive accuracy and quality of legal explanations in court outcomes.

**Limitations:** 

**Conclusion:** The findings indicate that enhancing legal judgment prediction with augmented, structured inputs leads to better performance in predicting decisions and generating explanations.

**Abstract:** Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.

</details>


### [57] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)

*Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Question Answering, Monte Carlo Tree Search, Large Language Models, Path Evaluation, Transformer

**Relevance Score:** 8

**TL;DR:** This paper presents Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel approach for Knowledge Graph Question Answering (KGQA) that combines Monte Carlo Tree Search with an LLM-based planner for improved efficacy and context-awareness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current KGQA methods in adaptability and computational cost, this paper introduces a new framework for more efficient and accurate path evaluation in KGQA.

**Method:** DAMR employs a Monte Carlo Tree Search framework guided by an LLM-based planner, which selects relevant relations and incorporates a lightweight Transformer-based scorer for context-aware plausibility estimation. It also includes a dynamic pseudo-path refinement mechanism for continuous adaptation to training signals.

**Key Contributions:**

	1. Introduction of a novel DAMR framework for KGQA.
	2. Integration of MCTS and LLM-based planning for path selection.
	3. Development of a lightweight Transformer scorer for context-aware evaluation.

**Result:** Experiments show that DAMR outperforms state-of-the-art KGQA methods on multiple benchmarks, demonstrating improved accuracy and efficiency.

**Limitations:** 

**Conclusion:** The proposed DAMR framework addresses key challenges in KGQA, offering a more adaptable and accurate solution compared to existing methods.

**Abstract:** Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.

</details>


### [58] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)

*Sohaib Imran, Rob Lamb, Peter M. Atkinson*

**Main category:** cs.CL

**Keywords:** Large Language Models, Machine Learning, AI Safety, Out-of-Context Abduction, Situational Awareness

**Relevance Score:** 9

**TL;DR:** This paper investigates the ability of large language models (LLMs) to reason about their training data through out-of-context abduction, focusing on OpenAI's GPT 4o model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore whether LLMs can infer plausible explanations for observations based on relevant facts within their training data, contributing to understanding their reasoning capabilities and implications for AI safety.

**Method:** Experiments were conducted with LLMs trained on fictitious chatbot names and behavior descriptions, without including dialogue examples, to assess inferencing based on response patterns.

**Key Contributions:**

	1. Investigates out-of-context abduction in LLMs.
	2. Demonstrates GPT 4o's ability to infer names and behaviors from observations.
	3. Highlights implications for situational awareness and AI safety.

**Result:** The experiments demonstrated that GPT 4o could infer at least one chatbot's name and exhibited behavior consistent with training when prompted appropriately, showing an ability to leverage training information effectively.

**Limitations:** The research is limited to specific training contexts and may not generalize to all inference scenarios in LLMs.

**Conclusion:** The findings suggest that LLMs can exhibit situational awareness by integrating training data insights into their responses, raising important questions regarding AI safety and reasoning capabilities.

**Abstract:** Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [59] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)

*Sarah Mercer, Daniel P. Martin, Phil Swatton*

**Main category:** cs.CL

**Keywords:** generative agents, Large Language Models, HEXACO personality inventory, social science research, personality profiling

**Relevance Score:** 9

**TL;DR:** The paper examines the use of generative agents powered by LLMs like GPT-4 in social science research, specifically assessing their ability to represent human personality traits through the HEXACO personality inventory experiment.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the validity of persona-based agents as substitutes for human participants in social science research.

**Method:** The study recreates the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents and conducting factor analysis on their responses, comparing results with original findings from 2004.

**Key Contributions:**

	1. Validation of LLM-powered agents in social science applications
	2. Identification of model-specific biases in personality profiling
	3. Guidance on designing effective persona-based agents for research

**Result:** A coherent and reliable personality structure was found in the agents' responses, demonstrating partial alignment to the HEXACO framework, and revealing model-specific biases in personality profiling.

**Limitations:** Model-specific biases and challenges in creating curated populations for reliable results.

**Conclusion:** The study highlights the potential and limitations of using generative agents in social science research and provides guidance for designing agent personas to better represent human personality traits.

**Abstract:** Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.

</details>


### [60] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)

*Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald K旦stler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh*

**Main category:** cs.CL

**Keywords:** radiology, large language models, RAG, clinical decision-making, AI

**Relevance Score:** 9

**TL;DR:** The paper presents an agentic RAG framework for radiology question answering that improves diagnostic accuracy and reduces hallucinations in LLMs, particularly in mid-sized models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance clinical decision-making in radiology by addressing limitations of traditional retrieval-augmented generation systems in handling complex reasoning tasks.

**Method:** An agentic retrieval-augmented generation (RAG) framework that decomposes questions and retrieves relevant clinical evidence iteratively from Radiopaedia.

**Key Contributions:**

	1. Proposing a novel agentic RAG framework for radiology QA
	2. Demonstrating significant improvements in diagnostic accuracy
	3. Reducing hallucinations and enhancing context retrieval.

**Result:** Agentic retrieval improved diagnostic accuracy significantly compared to zero-shot prompting (73% vs. 64%) and conventional online RAG (73% vs. 68%). Mid-sized models showed the greatest improvement, while larger models exhibited minimal changes.

**Limitations:** The study evaluated only 24 LLMs and relied on specific datasets for assessment.

**Conclusion:** The agentic framework enhances factuality and diagnostic accuracy in radiology QA, particularly in mid-sized LLMs, suggesting a need for future studies to confirm clinical utility.

**Abstract:** Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.

</details>


### [61] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)

*Robin Armingaud, Romaric Besan巽on*

**Main category:** cs.CL

**Keywords:** Relation Extraction, Natural Language Processing, GLiDRE, Zero-shot learning, Few-shot learning

**Relevance Score:** 7

**TL;DR:** GLiDRE is a new model for document-level relation extraction that outperforms existing models in few-shot scenarios, building on ideas from GLiNER.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of document-level relation extraction and improve performance in zero-shot or few-shot settings.

**Method:** GLiDRE utilizes a compact NER architecture inspired by GLiNER and is evaluated against state-of-the-art models on the Re-DocRED dataset.

**Key Contributions:**

	1. Introduction of GLiDRE model for document-level relation extraction
	2. Performance benchmarking on Re-DocRED dataset
	3. State-of-the-art results in few-shot scenarios

**Result:** GLiDRE achieves state-of-the-art performance in few-shot scenarios, showing better results than larger models.

**Limitations:** 

**Conclusion:** GLiDRE demonstrates the effectiveness of compact models for complex NLP tasks like relation extraction.

**Abstract:** Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. Current approaches, largely based on the ATLOP architecture, are commonly evaluated on benchmarks like DocRED and Re-DocRED. However, their performance in zero-shot or few-shot settings remains largely underexplored due to the task's complexity. Recently, the GLiNER model has shown that a compact NER model can outperform much larger Large Language Models. With a similar motivation, we introduce GLiDRE, a new model for document-level relation extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against state-of-the-art models across various data settings on the Re-DocRED dataset. Our results demonstrate that GLiDRE achieves state-of-the-art performance in few-shot scenarios. Our code is publicly available.

</details>


### [62] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)

*Qiyao Xue, Yuchen Dou, Ryan Shi, Xiang Lorraine Li, Wei Gao*

**Main category:** cs.CL

**Keywords:** hate speech detection, multimodal, Chinese social networks, Mixture-of-Experts, BERT

**Relevance Score:** 5

**TL;DR:** MMBERT is a novel multimodal framework improving hate speech detection on Chinese social networks by integrating text, speech, and visual modalities through a Mixture-of-Experts architecture.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for hate speech detection primarily focus on English datasets, overlooking the unique challenges faced in Chinese social networks, particularly with cloaking techniques.

**Method:** MMBERT employs a progressive three-stage training paradigm, combining modality-specific experts and a shared self-attention mechanism to improve robustness and detection capabilities.

**Key Contributions:**

	1. Introduction of the MMBERT multimodal framework for hate speech detection.
	2. Development of a three-stage training paradigm to enhance model robustness.
	3. Integration of text, speech, and visual modalities using a Mixture-of-Experts architecture.

**Result:** MMBERT outperforms fine-tuned BERT-based models and LLMs on multiple Chinese hate speech datasets, demonstrating its effectiveness in multimodal integration.

**Limitations:** 

**Conclusion:** The proposed MMBERT framework significantly advances hate speech detection for Chinese social networks through innovative architecture and training methods.

**Abstract:** Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.

</details>


### [63] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)

*Atakan Site, Emre Hakan Erdemir, G端len Eryiit*

**Main category:** cs.CL

**Keywords:** tabular data, question answering, large language models, code generation, Pandas

**Relevance Score:** 8

**TL;DR:** The paper describes a system developed for the SemEval-2025 Task 8 focusing on question-answering over tabular data using LLM-based code generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address question answering from tabular datasets in diverse domains, improving efficiency and accuracy in generating executable code for data manipulation.

**Method:** A zero-shot framework utilizing LLMs to generate Pandas code for tabular data, employing optimized prompting strategies for effective code generation.

**Key Contributions:**

	1. Development of a zero-shot solution for tabular data QA
	2. Optimized prompting strategies for LLMs to generate executable code
	3. Demonstration of superior performance of Python code generation over alternative methods.

**Result:** The system ranked eighth in Subtask I and sixth in Subtask II among 30 competitors, demonstrating better performance in Python code generation for tabular question answering compared to other methods.

**Limitations:** 

**Conclusion:** The results indicate that LLM-based code generation is effective for tabular question answering, with promise for future applications and improvements.

**Abstract:** This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category.

</details>


### [64] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)

*Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang*

**Main category:** cs.CL

**Keywords:** large language models, pronoun usage, MISGENDERED+, inclusive AI, gender-neutral pronouns

**Relevance Score:** 9

**TL;DR:** This study presents MISGENDERED+, an updated benchmark for evaluating large language models on their pronoun usage, particularly focusing on gender-neutral and neopronouns, and finds gaps in performance for neopronouns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of earlier benchmarks in evaluating large language models' handling of inclusive pronouns, especially in sensitive contexts requiring fairness and inclusivity.

**Method:** The study benchmarks five large language models using MISGENDERED+ across zero-shot, few-shot, and gender identity inference tasks.

**Key Contributions:**

	1. Introduction of the MISGENDERED+ benchmark
	2. Evaluation of five state-of-the-art language models
	3. Insights into performance gaps for neopronouns

**Result:** The results indicate significant improvements in accuracy for binary and gender-neutral pronouns, but inconsistent performance for neopronouns and reverse inference tasks.

**Limitations:** Inconsistent accuracy on neopronouns and reverse inference tasks remains.

**Conclusion:** The study highlights the need for ongoing research and development of inclusive AI tools, indicating existing gaps in identity-sensitive reasoning in LLMs.

**Abstract:** Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.

</details>


### [65] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)

*Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Dynamic Length, Large Language Models, AI Efficiency, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces DAEDAL, a novel training-free strategy for Dynamic Adaptive Length Expansion in Diffusion Large Language Models, addressing the constraints of static generation lengths and improving performance and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of static predefined generation lengths in Diffusion Large Language Models, which affect performance and computational efficiency.

**Method:** DAEDAL operates in two phases: first, it expands a short initial generation length based on a sequence completion metric; second, it dynamically adjusts insufficient regions during the denoising process using mask token insertion.

**Key Contributions:**

	1. Introduction of a training-free denoising strategy for length adaptation
	2. Demonstration of superior performance compared to fixed-length baselines
	3. Improvement in computational efficiency through effective token usage

**Result:** DAEDAL achieves performance comparable or superior to fixed-length baselines while increasing computational efficiency by attaining a higher effective token ratio.

**Limitations:** 

**Conclusion:** By addressing the critical architectural constraint of static lengths, DAEDAL enhances the capabilities of DLLMs, promoting their efficiency and performance in generative tasks.

**Abstract:** Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.

</details>


### [66] [Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge](https://arxiv.org/abs/2412.10207)

*Xiao Zhang, Qianru Meng, Johan Bos*

**Main category:** cs.CL

**Keywords:** semantic parsing, large language models, open-domain parsing, retrieval-augmented, unseen concepts

**Relevance Score:** 9

**TL;DR:** This paper presents Retrieval-Augmented Semantic Parsing (RASP), leveraging large language models (LLMs) to enhance semantic parsing, particularly for unseen concepts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges faced by neural models in open-domain semantic parsing, particularly their reliance on heuristics and difficulties with unseen concepts.

**Method:** The authors propose a method called Retrieval-Augmented Semantic Parsing (RASP), which integrates external symbolic knowledge into the parsing process, using large language models.

**Key Contributions:**

	1. Introduction of Retrieval-Augmented Semantic Parsing (RASP)
	2. Demonstration of enhanced performance of LLMs on semantic parsing tasks
	3. Significant improvement in handling unseen concepts compared to previous models

**Result:** Experiments demonstrate that LLMs surpass previous encoder-decoder models for semantic parsing, and RASP nearly doubles performance for out-of-distribution concepts.

**Limitations:** 

**Conclusion:** The findings suggest that combining large language models with retrieval mechanisms significantly improves robust and open-domain semantic parsing capabilities.

**Abstract:** Open-domain semantic parsing remains a challenging task, as neural models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external symbolic knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.

</details>


### [67] [IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance](https://arxiv.org/abs/2502.08395)

*Paul R旦ttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** large language models, issue bias, benchmarking, human-computer interaction, political issues

**Relevance Score:** 9

**TL;DR:** IssueBench is a benchmarking tool for measuring issue bias in large language models, constructed from 2.49m prompts based on real user interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about issue bias in LLMs and provide a means to measure the biases they may introduce in user interactions.

**Method:** Constructed a dataset called IssueBench containing 2.49 million prompts based on 3.9k templates and 212 political issues, derived from real user interactions.

**Key Contributions:**

	1. Creation of IssueBench dataset for measuring LLM biases
	2. Demonstrated common biases across different LLMs
	3. Showed political alignment of LLM outputs with US Democrat views

**Result:** Found common and persistent issue biases in state-of-the-art LLMs, which tend to align more with US Democrat opinions than Republican on certain issues.

**Limitations:** 

**Conclusion:** IssueBench can facilitate more realistic and robust measurement of biases in LLMs, potentially guiding efforts to address these biases.

**Abstract:** Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.

</details>


### [68] [Better Embeddings with Coupled Adam](https://arxiv.org/abs/2502.08441)

*Felix Stollenwerk, Tobias Stollenwerk*

**Main category:** cs.CL

**Keywords:** LLMs, Adam optimizer, anisotropy, Coupled Adam, embeddings

**Relevance Score:** 8

**TL;DR:** This paper introduces Coupled Adam, a modified optimizer aimed at reducing anisotropy in embeddings produced by LLMs, leading to improved performance on various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often produce anisotropic word representations, which negatively affect their performance; understanding the causes and finding solutions motivates this research.

**Method:** The paper identifies the role of the second moment in the Adam optimizer as a cause for anisotropy and proposes a new optimizer, Coupled Adam, to address this issue.

**Key Contributions:**

	1. Introduction of Coupled Adam optimizer
	2. Demonstration of the link between second moment in Adam and anisotropy
	3. Empirical results showing improvements in embedding quality and overall performance

**Result:** Experiments show that Coupled Adam mitigates anisotropic embeddings and results in better performance across different applications.

**Limitations:** The approach may require larger datasets to see significant improvements and may not universally apply to all types of LLMs.

**Conclusion:** Coupled Adam improves embedding quality and enhances both upstream and downstream performance on sufficiently large datasets.

**Abstract:** Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.

</details>


### [69] [SEFL: Enhancing Educational Assignment Feedback with LLM Agents](https://arxiv.org/abs/2502.12927)

*Mike Zhang, Amalie Pernille Dilling, L辿on Gondelman, Niels Erik Ruan Lyngdorf, Euan D. Lindsay, Johannes Bjerva*

**Main category:** cs.CL

**Keywords:** Synthetic Educational Feedback, Large Language Models, Higher Education Feedback, Machine Learning, Educational Technology

**Relevance Score:** 8

**TL;DR:** Introducing Synthetic Educational Feedback Loops (SEFL), a framework leveraging LLMs to generate synthetic feedback for student assignments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** High-quality feedback is essential for student success but often limited by time and costs; SEFL aims to address these constraints.

**Method:** Two LLMs simulate a teacher-student interaction to generate synthetic student work and feedback pairs, which are used to fine-tune smaller LLMs for feedback generation.

**Key Contributions:**

	1. Development of the SEFL framework for generating synthetic educational feedback.
	2. Demonstrated improvements in feedback quality with fine-tuned LLMs.
	3. Evaluation and validation of SEFL's potential impact on higher education feedback mechanisms.

**Result:** SEFL-tuned models demonstrate superior feedback quality compared to non-tuned models and existing baselines, validated through evaluations by experts.

**Limitations:** The framework's reliance on synthetic data may not fully capture all nuances of real student feedback processes.

**Conclusion:** SEFL has significant potential to enhance feedback processes in higher education, as indicated by positive qualitative feedback from human stakeholders.

**Abstract:** Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.

</details>


### [70] [Lost in Space: Finding the Right Tokens for Structured Output](https://arxiv.org/abs/2502.14969)

*Sil Hamilton, David Mimno*

**Main category:** cs.CL

**Keywords:** language models, structured output, NLP benchmarks, classification, best practices

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of structured output formats on the performance of language models in NLP tasks, revealing best practices for improving accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how structured output formats affect the performance of general-purpose language models in tasks like annotation and classification.

**Method:** Testing four popular model families with five different output formats across four common NLP benchmarks.

**Key Contributions:**

	1. Identified key structured output formats that improve model performance.
	2. Showed the effectiveness of leading whitespace in outputs for better accuracy.
	3. Provided best practices for using language models as zero-shot classifiers.

**Result:** Models perform most accurately with conventional formats and show a 5%-10% improvement when using leading whitespace in outputs, with the largest gains in smaller models.

**Limitations:** 

**Conclusion:** Guiding language models to use proper output formats enhances accuracy, and using leading whitespace can prevent structural deficiencies in token representations.

**Abstract:** General-purpose language models are trained to produce varied natural language outputs, but for some tasks, like annotation or classification, we need more specific output formats. LLM systems increasingly support structured output, which enforces formats by sampling tokens according to a grammar -- but also unpredictably reduces downstream performance. Are there systematic differences between grammars that appear semantically (and often visually) similar to humans? To answer this, we test four popular model families with five varying output formats on four common NLP benchmarks. We find all models perform most accurately when guided to use formats respecting convention, such as letters for multiple choice and real numbers for numerical prediction. Performance also improves by 5%-10% when guiding models to return tokens incorporating leading whitespace, with smaller models benefiting the most. We find leading whitespace helps models avoid structural deficiencies in subword token representations. We finally present best practices for researchers using language models as zero-shot classifiers with structured output.

</details>


### [71] [Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning](https://arxiv.org/abs/2502.17407)

*Guijin Son, Jiwoo Hong, Hyunwoo Ko, James Thorne*

**Main category:** cs.CL

**Keywords:** multilingual, large language models, test-time scaling, math benchmark, evaluation

**Relevance Score:** 7

**TL;DR:** This paper introduces MCLM, a multilingual math benchmark, and evaluates three test-time scaling methods for LLMs, revealing limitations in multilingual performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of test-time scaling methods for multilingual large language models (LLMs) in solving competition-level math problems across different languages.

**Method:** The authors tested three test-time scaling methods: Outcome Reward Modeling (ORM), Process Reward Modeling (PRM), and Budget Forcing (BF) on two multilingual LLMs: Qwen2.5-1.5B Math and MR1-1.5B.

**Key Contributions:**

	1. Introduction of MCLM, a multilingual math benchmark for assessing LLM performance.
	2. Evaluation of three novel test-time scaling methods for LLMs in multilingual contexts.
	3. Release of multilingual LLM models and comprehensive evaluation results.

**Result:** Using ORM with Qwen2.5-1.5B Math achieved a score of 35.8 on MCLM, while MR1-1.5B with BF scored 35.2. However, test-time scaling demonstrated limited improvement across diverse languages.

**Limitations:** Test-time scaling methods showed limited generalization to multilingual tasks, with minimal gains across languages compared to English.

**Conclusion:** Test-time scaling methods such as BF did not generalize effectively to multilingual tasks, despite significant improvements in English tasks. The study highlights the need for continued research in multilingual LLM capabilities.

**Abstract:** Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.

</details>


### [72] [Do Large Language Models Know How Much They Know?](https://arxiv.org/abs/2502.19573)

*Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Shagun Sodhani, Sarath Chandar*

**Main category:** cs.CL

**Keywords:** Large Language Models, knowledge awareness, benchmark, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper investigates whether Large Language Models (LLMs) can recognize the scope of their own knowledge through a benchmark designed to evaluate their knowledge recall.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid deployment of LLMs necessitates a better understanding of their internal mechanisms and knowledge recognition capabilities.

**Method:** A benchmark was developed to challenge LLMs to enumerate all information they possess on specific topics, evaluating their knowledge recall accuracy.

**Key Contributions:**

	1. Development of a benchmark for evaluating LLMs' awareness of knowledge
	2. Findings support that LLMs exhibit knowledge scope recognition
	3. Identification of varying capabilities across different LLM architectures

**Result:** All tested LLMs show an understanding of their knowledge scope, with varying rates of capability emergence across architectures.

**Limitations:** Further research is necessary to confirm the underlying mechanisms of knowledge awareness in LLMs.

**Conclusion:** Awareness of knowledge may be a generalizable attribute of LLMs, requiring further research to confirm its mechanisms.

**Abstract:** Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.

</details>


### [73] [AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation](https://arxiv.org/abs/2503.19693)

*Itay Nakash, Nitay Calderon, Eyal Ben David, Elad Hoffer, Roi Reichart*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vocabulary Adaptation, Domain-Specific, N-gram, Efficiency

**Relevance Score:** 9

**TL;DR:** AdaptiVocab is an end-to-end vocabulary adaptation approach for large language models that enhances efficiency in low-resource domains by reducing token usage without compromising performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of large language models by adapting their vocabulary to specific domains, reducing unnecessary computational overhead.

**Method:** AdaptiVocab modifies existing tokenizers to replace general tokens with domain-specific n-gram-based tokens, which decreases the number of tokens needed for processing and generation. It initializes new n-token embeddings from existing embeddings and employs a lightweight fine-tuning process on a single GPU.

**Key Contributions:**

	1. Introduction of AdaptiVocab for domain-specific vocabulary adaptation
	2. Demonstrated more than 25% reduction in token usage
	3. Efficient fine-tuning methodology applicable on a single GPU

**Result:** Evaluation of two 7B LLMs in three niche domains demonstrated a reduction in token usage by over 25%, while maintaining the quality of generated content and overall task performance.

**Limitations:** Potential limitations include reliance on existing embedding quality and the need for domain-specific data for training.

**Conclusion:** AdaptiVocab successfully enhances the efficiency of large language models for specific domains, showcasing a significant reduction in token usage and computational cost.

**Abstract:** Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance

</details>


### [74] [MemInsight: Autonomous Memory Augmentation for LLM Agents](https://arxiv.org/abs/2503.21760)

*Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba*

**Main category:** cs.CL

**Keywords:** large language model, memory augmentation, semantic data representation, LLM agents, contextual performance

**Relevance Score:** 9

**TL;DR:** The paper presents MemInsight, an autonomous memory augmentation approach for large language model agents that improves semantic data representation and retrieval, thereby enhancing their contextual performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of integrating long-term memory capabilities in LLM agents and improve their interaction quality through improved semantic data management.

**Method:** MemInsight leverages autonomous augmentation to historical interactions, focusing on enhancing data representation and retrieval mechanisms within LLM agents.

**Key Contributions:**

	1. Proposed MemInsight for memory augmentation in LLM agents
	2. Empirical validation across multiple task scenarios
	3. Demonstrated significant improvements in recommendation persuasiveness and recall metrics.

**Result:** Empirical validation shows that MemInsight significantly improves the persuasiveness of recommendations by up to 14% and outperforms a RAG baseline by 34% in recall for LoCoMo retrieval across three task scenarios: conversational recommendation, question answering, and event summarization.

**Limitations:** 

**Conclusion:** MemInsight demonstrates the potential to improve contextual performance of LLM agents, suggesting its applicability in various interactive AI applications.

**Abstract:** Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.

</details>


### [75] [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)

*Weiqi Wang, Jiefu Ou, Yangqiu Song, Benjamin Van Durme, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** literature review, LLM, benchmark, information retrieval, table generation

**Relevance Score:** 8

**TL;DR:** This paper addresses the challenges in generating effective literature review tables from scientific papers using LLMs and human annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective literature review tables that accurately summarize and compare scientific papers based on user needs.

**Method:** Combining LLM-based methods and human annotations to generate literature review tables, while introducing a benchmark for evaluation.

**Key Contributions:**

	1. Introduction of ARXIV2TABLE benchmark for realistic evaluation
	2. Enhanced methods for literature review table generation
	3. Identification of specific challenges in the task of table generation

**Result:** Experiments reveal that LLMs, both open-weight and proprietary, face significant difficulties in generating effective literature review tables.

**Limitations:** The approach may still produce irrelevant content and requires careful task evaluation improvements.

**Conclusion:** The findings indicate a pressing need for further advancements in literature review table generation and evaluation methodologies.

**Abstract:** Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.

</details>


### [76] [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)

*Zihao Xu, Junchen Ding, Yiling Lou, Kun Zhang, Dong Gong, Yuekang Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, logical reasoning, benchmark, Reddit data, automated framework

**Relevance Score:** 9

**TL;DR:** Introducing SmartyPat-Bench, a new benchmark for evaluating logical reasoning in LLMs using real-world data annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing datasets that are overly simplistic and not contextually rich, there is a need for a more challenging benchmark that reflects real-world reasoning scenarios.

**Method:** The paper presents SmartyPat-Bench, created from high-quality Reddit posts with subtle logical fallacies, and introduces SmartyPat, an automated framework using logic programming to generate and refine logically fallacious statements into natural language.

**Key Contributions:**

	1. Introduction of SmartyPat-Bench, a novel benchmark for logical reasoning evaluation in LLMs.
	2. Development of SmartyPat, an automated framework for generating high-quality fallacies.
	3. Comprehensive evaluation highlighting the nuanced abilities of LLMs in detecting logical fallacies.

**Result:** SmartyPat-Bench has more detailed annotations and diverse fallacy data compared to existing datasets. SmartyPat outperforms baseline methods in producing logically fallacious statements that match the quality of human-generated content.

**Limitations:** 

**Conclusion:** The framework provides valuable insights into the capabilities of LLMs in logical reasoning, showing that structured reasoning improves fallacy categorization, while too many reasoning steps can reduce accuracy.

**Abstract:** Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.

</details>


### [77] [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)

*Rui Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, Zhengwen Qiu*

**Main category:** cs.CL

**Keywords:** AI agents, financial research, evaluation framework, logic tree, Agent-as-a-Judge

**Relevance Score:** 3

**TL;DR:** The paper introduces FinResearchBench, a novel evaluation framework for financial research AI agents using a logic tree based Agent-as-a-Judge system.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of systematic evaluation frameworks and benchmarks for evaluating the capabilities of AI research agents in the complex domain of financial research.

**Method:** FinResearchBench uses a logic tree structure to assess and evaluate AI research agents on a range of tasks typical in financial research, specifically designed for long-horizon tasks.

**Key Contributions:**

	1. Innovative Agent-as-a-Judge system using logic trees for evaluation.
	2. Covers 70 typical financial research questions.
	3. Systematic evaluation across 7 types of financial research tasks.

**Result:** The framework provides a comprehensive assessment of research agents across 7 key task types in finance, addressing 70 typical financial research questions.

**Limitations:** 

**Conclusion:** FinResearchBench offers a reliable and systematic way to evaluate the effectiveness of financial research AI agents, paving the way for better understanding their capabilities.

**Abstract:** Recently, AI agents are rapidly evolving in intelligence and widely used in professional research applications, such as STEM, software development, finance, etc. Among these AI agents, deep research agent is a key category as it can perform long-horizon tasks and solve problems of greater complexity. However, there are few evaluation frameworks and benchmarks that systematically and automatically investigate the capabilities of these research agents. Furthermore, financial research problems have distinct complexity and subtlety. To fill in the gap, we propose FinResearchBench, which is a logic tree based Agent-as-a-Judge and targets specifically for the financial research agents. It provides a comprehensive and automatic assessment of the research agents across 7 key types of tasks in the financial research domain. The contributions of this work are two-folded: (1) the first and innovative Agent-as-a-Judge system that extracts the logic tree of the research outcome and uses it as the intermediate information to present a comprehensive, reliable and robust evaluation; (2) finance oriented that it covers 70 typical financial research questions, spreading across 7 frequently encountered types of tasks in the domain.

</details>
