# 2025-05-28

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 199]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)

*Samuel Rhys Cox, Rune MÃ¸berg Jacobsen, Niels van Berkel*

**Main category:** cs.HC

**Keywords:** self-disclosure, chatbots, human-computer interaction, relationships, privacy

**Relevance Score:** 7

**TL;DR:** The paper explores how the framing of chatbot relationships affects self-disclosure, comparing Familiar and Stranger chatbots in emotional and factual contexts.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of chatbot relationships on users' willingness to share personal thoughts and feelings, particularly in the context of self-disclosure.

**Method:** A mixed factorial design was employed where participants interacted with either a Familiar or Stranger chatbot over two sessions, focusing on Emotional and Factual self-disclosure.

**Key Contributions:**

	1. Investigated the effect of chatbot relationship framing on self-disclosure.
	2. Demonstrated nuanced impacts in emotional vs. factual disclosure contexts.
	3. Provided qualitative insights into user perceptions of anonymity and judgement.

**Result:** Stranger-condition participants felt more comfortable disclosing emotions initially, while the Familiar-condition participants enjoyed the interaction more when factual disclosure was prioritized.

**Limitations:** The study may not generalize outside the contexts tested or account for all factors influencing self-disclosure.

**Conclusion:** The study reveals that the type of relationship framed by chatbots affects self-disclosure, with anonymity offered by Stranger chatbots enhancing comfort and Familiar chatbots potentially feeling intrusive without prior rapport.

**Abstract:** Self-disclosure, the sharing of one's thoughts and feelings, is affected by the perceived relationship between individuals. While chatbots are increasingly used for self-disclosure, the impact of a chatbot's framing on users' self-disclosure remains under-explored. We investigated how a chatbot's description of its relationship with users, particularly in terms of ephemerality, affects self-disclosure. Specifically, we compared a Familiar chatbot, presenting itself as a companion remembering past interactions, with a Stranger chatbot, presenting itself as a new, unacquainted entity in each conversation. In a mixed factorial design, participants engaged with either the Familiar or Stranger chatbot in two sessions across two days, with one conversation focusing on Emotional- and another Factual-disclosure. When Emotional-disclosure was sought in the first chatting session, Stranger-condition participants felt more comfortable self-disclosing. However, when Factual-disclosure was sought first, these differences were replaced by more enjoyment among Familiar-condition participants. Qualitative findings showed Stranger afforded anonymity and reduced judgement, whereas Familiar sometimes felt intrusive unless rapport was built via low-risk Factual-disclosure.

</details>


### [2] [HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings](https://arxiv.org/abs/2505.20585)

*Ben Rahman*

**Main category:** cs.HC

**Keywords:** digital health, LMIC, evaluation framework, community engagement, policy compliance

**Relevance Score:** 7

**TL;DR:** The paper introduces HOT-FIT-BR, a contextual evaluation framework for digital health systems in low-middle-income countries, addressing infrastructure, policy, and community factors.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Digital health system implementations in LMICs often fail due to inadequate evaluations that do not consider local contexts, necessitating the development of a more holistic evaluation framework.

**Method:** The HOT-FIT-BR framework integrates an Infrastructure Index, a Policy Compliance Layer, and a Community Engagement Fit, validated through simulations in Indonesian Health Centers.

**Key Contributions:**

	1. Introduction of the Infrastructure Index for evaluating digital health systems.
	2. Inclusion of a Policy Compliance Layer to address regulatory factors.
	3. Demonstration of increased sensitivity in evaluations, especially in rural areas.

**Result:** HOT-FIT-BR demonstrated a 58% increase in sensitivity for detecting problems compared to the original HOT-FIT model, particularly in rural settings with low infrastructure scores.

**Limitations:** 

**Conclusion:** The framework is adaptable to various LMIC contexts, as shown by local parameter adjustments in countries like India and Kenya.

**Abstract:** Implementation of digital health systems in low-middle-income countries (LMICs) often fails due to a lack of evaluations that take into account infrastructure limitations, local policies, and community readiness. We introduce HOT-FIT-BR, a contextual evaluation framework that expands the HOT-FIT model with three new dimensions: (1) Infrastructure Index to measure electricity/internet availability, (2) Policy Compliance Layer to ensure regulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community Engagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR is 58% more sensitive to detecting problems than HOT-FIT, especially in rural areas with an Infra Index <3. The framework has also proven adaptive to the context of other LMICs such as India and Kenya through local parameter adjustments.

</details>


### [3] [Institutionalizing Folk Theories of Algorithms: How Multi-Channel Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry](https://arxiv.org/abs/2505.20623)

*Qing Xiao, Rongyi Chen, Jingjia Xiao, Tianyang Fu, Alice Qian Zhang, Xianzhe Fan, Bingbing Zhang, Zhicong Lu, Hong Shen*

**Main category:** cs.HC

**Keywords:** algorithmic systems, folk theories, platform labor, Multi-Channel Networks, ethnographic study

**Relevance Score:** 7

**TL;DR:** This study examines how intermediary organizations like Multi-Channel Networks (MCNs) in China construct and operationalize folk theories of algorithms to manage platform labor, revealing the dual narratives they promote.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of intermediary organizations in shaping workers' understanding of algorithms in platform labor environments.

**Method:** The study is based on nine months of ethnographic fieldwork and 37 interviews with live-streamers and MCN staff in China.

**Key Contributions:**

	1. Investigates the role of intermediary organizations in platform labor
	2. Reveals the dual narratives of algorithmic theories promulgated by MCNs
	3. Demonstrates the impact of institutionalized folk theories on labor management

**Result:** MCNs create dual algorithmic theories, recognizing algorithm volatility internally while presenting simplified theories externally that promote transparency and fairness, thus influencing labor management.

**Limitations:** 

**Conclusion:** Institutionalized folk theories can act as infrastructures of soft control, affecting how workers view platform algorithms and structuring their labor.

**Abstract:** As algorithmic systems increasingly structure platform labor, workers often rely on informal "folk theories", experience-based beliefs about how algorithms work, to navigate opaque and unstable algorithmic environments. Prior research has largely treated these theories as bottom-up, peer-driven strategies for coping with algorithmic opacity and uncertainty. In this study, we shift analytical attention to intermediary organizations and examine how folk theories of algorithms can be institutionally constructed and operationalized by those organizations as tools of labor management. Drawing on nine months of ethnographic fieldwork and 37 interviews with live-streamers and staff at Multi-Channel Networks (MCNs) in China, we show that MCNs develop and circulate dual algorithmic theories: internally, they acknowledge the volatility of platform systems and adopt probabilistic strategies to manage risk; externally, they promote simplified, prescriptive theories portraying the algorithm as transparent, fair, and responsive to individual effort. They have further operationalize those folk theories for labor management, encouraging streamers to self-discipline and invest in equipment, training, and routines, while absolving MCNs of accountability. We contribute to CSCW and platform labor literature by demonstrating how informal algorithmic knowledge, once institutionalized, can become infrastructures of soft control -- shaping not only how workers interpret platform algorithms, but also how their labor is structured, moralized and governed.

</details>


### [4] [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)

*Saharsh Barve, Andy Mao, Jiayue Melissa Shi, Prerna Juneja, Koustuv Saha*

**Main category:** cs.HC

**Keywords:** Generative AI, Text-to-Image Generation, Bias Detection, Social Stereotypes, User Perception

**Relevance Score:** 8

**TL;DR:** This paper examines societal biases in text-to-image (T2I) generation models and proposes a rubric for bias detection and a Social Stereotype Index (SSI) to evaluate T2I outputs, revealing that bias can be reduced with prompt refinement but may impact contextual alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical concerns arising from societal stereotypes in T2I models that can amplify biases related to gender, race, and culture.

**Method:** The paper introduces a bias detection rubric and a Social Stereotype Index (SSI) to assess the outputs of major T2I models. It includes an audit of DALL-E-3, Midjourney-6.1, and Stability AI Core using 100 queries across three categories and employs LLMs for targeted prompt refinement.

**Key Contributions:**

	1. Introduction of a bias detection rubric for T2I outputs.
	2. Development of a Social Stereotype Index (SSI) for systematic evaluation of biases.
	3. Empirical findings on the user perception of AI-generated biased imagery and the impact of prompt refinement.

**Result:** The analysis showed that T2I models often produce outputs with stereotypical visual cues, with significant reductions in bias (SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries) through refined prompts.

**Limitations:** The study may not cover all biases present in varied cultural contexts and relies on user perception, which can be subjective.

**Conclusion:** While prompt refinement can reduce biases, it may also limit contextual alignment; there's a tension between ethical debiasing and the representation of social realities, highlighting the need for inclusive T2I systems.

**Abstract:** Recent advances in generative AI have enabled visual content creation through text-to-image (T2I) generation. However, despite their creative potential, T2I models often replicate and amplify societal stereotypes -- particularly those related to gender, race, and culture -- raising important ethical concerns. This paper proposes a theory-driven bias detection rubric and a Social Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs. We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and Stability AI Core -- using 100 queries across three categories -- geocultural, occupational, and adjectival. Our analysis reveals that initial outputs are prone to include stereotypical visual cues, including gendered professions, cultural markers, and western beauty norms. To address this, we adopted our rubric to conduct targeted prompt refinement using LLMs, which significantly reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries. We complemented our quantitative analysis through a user study examining perceptions, awareness, and preferences around AI-generated biased imagery. Our findings reveal a key tension -- although prompt refinement can mitigate stereotypes, it can limit contextual alignment. Interestingly, users often perceived stereotypical images to be more aligned with their expectations. We discuss the need to balance ethical debiasing with contextual relevance and call for T2I systems that support global diversity and inclusivity while not compromising the reflection of real-world social complexity.

</details>


### [5] [Automating eHMI Action Design with LLMs for Automated Vehicle Communication](https://arxiv.org/abs/2505.20711)

*Ding Xia, Xinyue Gui, Fan Gao, Dongyuan Li, Mark Colley, Takeo Igarashi*

**Main category:** cs.HC

**Keywords:** Automated Vehicles, Human-Machine Interfaces, Large Language Models, Action Design, Ubiquitous Computing

**Relevance Score:** 8

**TL;DR:** This paper explores the use of large language models (LLMs) to enhance external Human-Machine Interfaces (eHMIs) for automated vehicles (AVs), proposing a pipeline for creating adaptable action designs based on user-rated datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective communication between automated vehicles and road users, particularly in uncertain scenarios, necessitates adaptable external Human-Machine Interfaces.

**Method:** The paper proposes a pipeline integrating LLMs with 3D renderers to generate executable actions for eHMIs, validated by a user-rated Action-Design Scoring dataset and automated raters to benchmark the performance of different LLMs.

**Key Contributions:**

	1. Proposed an LLM-based pipeline for generating eHMI actions and sequences.
	2. Collected a user-rated dataset of action sequences to validate LLM capability in action design.
	3. Introduced automated raters for benchmarking LLM performance in eHMI message-action conversion.

**Result:** LLMs can translate intended messages into actions with human-like competency, particularly for reasoning-enabled models, as evidenced by the collected dataset and automated rating methods.

**Limitations:** The applicability of the findings may vary across diverse eHMI modalities, and further exploration is needed in real-world scenarios.

**Conclusion:** The integration of LLMs in eHMI design demonstrates significant potential for scalable and adaptable communication approaches in AVs, with varying effectiveness across different modalities.

**Abstract:** The absence of explicit communication channels between automated vehicles (AVs) and other road users requires the use of external Human-Machine Interfaces (eHMIs) to convey messages effectively in uncertain scenarios. Currently, most eHMI studies employ predefined text messages and manually designed actions to perform these messages, which limits the real-world deployment of eHMIs, where adaptability in dynamic scenarios is essential. Given the generalizability and versatility of large language models (LLMs), they could potentially serve as automated action designers for the message-action design task. To validate this idea, we make three contributions: (1) We propose a pipeline that integrates LLMs and 3D renderers, using LLMs as action designers to generate executable actions for controlling eHMIs and rendering action clips. (2) We collect a user-rated Action-Design Scoring dataset comprising a total of 320 action sequences for eight intended messages and four representative eHMI modalities. The dataset validates that LLMs can translate intended messages into actions close to a human level, particularly for reasoning-enabled LLMs. (3) We introduce two automated raters, Action Reference Score (ARS) and Vision-Language Models (VLMs), to benchmark 18 LLMs, finding that the VLM aligns with human preferences yet varies across eHMI modalities.

</details>


### [6] [What Shapes Writers' Decisions to Disclose AI Use?](https://arxiv.org/abs/2505.20727)

*Jingchao Fang, Mina Lee*

**Main category:** cs.HC

**Keywords:** AI Disclosure, Human-AI Interaction, Co-Creation, Transparency, Behavioral Science

**Relevance Score:** 6

**TL;DR:** This paper explores the factors influencing writers' decisions to disclose their use of AI in co-created content, aiming to promote transparency in human-AI collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the reasons behind writers' choices to disclose or withhold AI involvement in their content is essential for advocating for transparency in human-AI co-creation.

**Method:** The study synthesizes findings and theoretical frameworks from human-AI interaction and behavioral science to identify factors that affect disclosure decisions.

**Key Contributions:**

	1. Identification of factors influencing writers' decisions regarding AI disclosure
	2. Synthesis of study findings within human-AI interaction frameworks
	3. Contribution to the discourse on transparency in AI-generated content

**Result:** A curated list of factors influencing the decision to disclose AI involvement in content creation has been identified, which can inform future transparency initiatives.

**Limitations:** 

**Conclusion:** Promoting transparency in AI-assisted writing content requires a better understanding of the diverse factors influencing writers' decisions to disclose AI use.

**Abstract:** Have you ever read a blog or social media post and suspected that it was written--at least in part--by artificial intelligence (AI)? While transparently acknowledging contributors to writing is generally valued, why some writers choose to disclose or withhold AI involvement remains unclear. In this work, we ask what factors shape writers' decisions to disclose their AI use as a starting point to effectively advocate for transparency. To shed light on this question, we synthesize study findings and theoretical frameworks in human-AI interaction and behavioral science. Concretely, we identify and curate a list of factors that could affect writers' decisions regarding disclosure for human-AI co-created content.

</details>


### [7] [Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset](https://arxiv.org/abs/2505.20788)

*Robin Burchard, Kristof Van Laerhoven*

**Main category:** cs.HC

**Keywords:** human activity recognition, acoustic data, privacy, wearable technology, tap water detection

**Relevance Score:** 7

**TL;DR:** This paper introduces a new label for human activity recognition using acoustic data, specifically focusing on tap water detection to improve hand washing detection in wearable devices without compromising privacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of acoustic data in wearable human activity recognition can provide valuable context, but recording audio poses privacy challenges. The study aims to utilize contextual information from water flow detection while ensuring ethical considerations are met.

**Method:** The authors created a new label, 'tap water', in the HD-Epic dataset with 717 hand-labeled annotations and analyzed its relation to water flow. They trained and evaluated two lightweight classifiers to assess the classification performance of the new label.

**Key Contributions:**

	1. Introduction of the 'tap water' label in the HD-Epic dataset
	2. Creation of a novel dataset with 717 hand-labeled samples
	3. Evaluation of lightweight classifiers for improved context-aware recognition

**Result:** The presence of the tap water label facilitates easier learning by classifiers, indicating significant improvements in detecting hand washing activities using localized acoustic data processing.

**Limitations:** The study is bounded by the limitations of the HD-Epic dataset and localized processing constraints on wearable devices.

**Conclusion:** Processing and classifying tap water sounds locally on wearable devices can enhance human activity recognition while addressing privacy concerns.

**Abstract:** Wearable human activity recognition has been shown to benefit from the inclusion of acoustic data, as the sounds around a person often contain valuable context. However, due to privacy concerns, it is usually not ethically feasible to record and save microphone data from the device, since the audio could, for instance, also contain private conversations. Rather, the data should be processed locally, which in turn requires processing power and consumes energy on the wearable device. One special use case of contextual information that can be utilized to augment special tasks in human activity recognition is water flow detection, which can, e.g., be used to aid wearable hand washing detection. We created a new label called tap water for the recently released HD-Epic data set, creating 717 hand-labeled annotations of tap water flow, based on existing annotations of the water class. We analyzed the relation of tap water and water in the dataset and additionally trained and evaluated two lightweight classifiers to evaluate the newly added label class, showing that the new class can be learned more easily.

</details>


### [8] [Describe Me Something You Do Not Remember - Challenges and Risks of Exposure Design Using Generative Artificial Intelligence for Therapy of Complex Post-traumatic Disorder](https://arxiv.org/abs/2505.20796)

*Annalisa Degenhard, Stefan TschÃ¶ke, Michael Rietzler, Enrico Rukzio*

**Main category:** cs.HC

**Keywords:** Generative AI, Complex PTSD, Trauma therapy, Visualization, Patient participation

**Relevance Score:** 9

**TL;DR:** The paper discusses the use of Generative Artificial Intelligence (GAI) to create individualized exposure visualizations in trauma therapy, particularly for Complex PTSD (CPTSD).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of cost-efficient solutions for creating individualized exposure visualizations in trauma therapy, which is crucial for treating Complex PTSD.

**Method:** The paper outlines the roles of the patient, system, and therapist in using GAI for generating exposure visualizations, emphasizing safe and effective communication.

**Key Contributions:**

	1. Introduces GAI as a tool for individualized trauma therapy visualizations.
	2. Highlights the role of patient participation in the therapy process.
	3. Discusses the need for safety in the therapeutic environment using GAI.

**Result:** GAI enables the creation of tailored therapeutic tools that allow patients to engage in the recall process, potentially improving access to trauma therapy for CPTSD.

**Limitations:** Concerns regarding feasibility and safety in integrating GAI into trauma therapy, particularly for CPTSD.

**Conclusion:** While GAI presents exciting opportunities for trauma therapy, it also poses significant challenges and risks that must be carefully managed.

**Abstract:** Post-traumatic stress disorder (PTSD) is associated with sudden, uncontrollable, and intense flashbacks of traumatic memories. Trauma exposure psychotherapy has proven effective in reducing the severity of trauma-related symptoms. It involves controlled recall of traumatic memories to train coping mechanisms for flashbacks and enable autobiographical integration of distressing experiences. In particular, exposure to visualizations of these memories supports successful recall. Although this approach is effective for various trauma types, it remains available for only a few. This is due to the lack of cost-efficient solutions for creating individualized exposure visualizations. This issue is particularly relevant for the treatment of Complex PTSD (CPTSD), where traumatic memories are highly individual and generic visualizations do not meet therapeutic needs. Generative Artificial Intelligence (GAI) offers a flexible and cost-effective alternative. GAI enables the creation of individualized exposure visualizations during therapy and, for the first time, allows patients to actively participate in the visualization process. While GAI opens new therapeutic perspectives and may improve access to trauma therapy, especially for CPTSD, it also introduces significant challenges and risks. The extreme uncertainty and lack of control that define both CPTSD and GAI raise concerns about feasibility and safety. To support safe and effective three-way communication, it is essential to understand the roles of patient, system, and therapist in exposure visualization and how each can contribute to safety. This paper outlines perspectives, challenges, and risks associated with the use of GAI in trauma therapy, with a focus on CPTSD.

</details>


### [9] [Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and Mitigation of Risks](https://arxiv.org/abs/2505.20916)

*Kyzyl Monteiro, Yuchen Wu, Sauvik Das*

**Main category:** cs.HC

**Keywords:** image privacy, AI, user study, image-sharing, obfuscation techniques

**Relevance Score:** 8

**TL;DR:** Imago Obscura is an AI image-editing copilot designed to help users identify and mitigate privacy risks when sharing images online.

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** Address users' struggles with navigating the privacy/publicity boundary and improve their awareness of image privacy risks.

**Method:** The system employs user inputs on sharing intent and privacy concerns to recommend effective obfuscation techniques based on prior literature. Evaluated through a lab study with 15 end-users.

**Key Contributions:**

	1. Introduction of an AI-powered tool to assist with image privacy
	2. Evaluation based on a lab study with end-users
	3. Recommendations for effective image obfuscation techniques

**Result:** Imago Obscura significantly improved users' awareness of privacy risks and their management capabilities, leading to more informed image-sharing decisions.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of AI tools like Imago Obscura in enhancing user autonomy and safety when sharing images.

**Abstract:** Users often struggle to navigate the privacy / publicity boundary in sharing images online: they may lack awareness of image privacy risks and/or the ability to apply effective mitigation strategies. To address this challenge, we introduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that enables users to identify and mitigate privacy risks with images they intend to share. Driven by design requirements from a formative user study with 7 image-editing experts, Imago Obscura enables users to articulate their image-sharing intent and privacy concerns. The system uses these inputs to surface contextually pertinent privacy risks, and then recommends and facilitates application of a suite of obfuscation techniques found to be effective in prior literature -- e.g., inpainting, blurring, and generative content replacement. We evaluated Imago Obscura with 15 end-users in a lab study and found that it greatly improved users' awareness of image privacy risks and their ability to address those risks, allowing them to make more informed sharing decisions.

</details>


### [10] [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)

*Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung-yi Lee, Yun-Nung Chen*

**Main category:** cs.HC

**Keywords:** multi-agent systems, creativity, large language models, text generation, image generation

**Relevance Score:** 9

**TL;DR:** This survey focuses on creativity within multi-agent systems (MAS) driven by large language models (LLMs), examining how agents generate and evaluate novel ideas in collaboration with humans.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of focus on creativity in existing surveys on multi-agent systems, which predominantly emphasize infrastructure over the creative process.

**Method:** The authors provide a taxonomy of agent proactivity and persona design, an overview of generation techniques, and a discussion on challenges in the field.

**Key Contributions:**

	1. Taxonomy of agent proactivity and persona design
	2. Overview of generation techniques and evaluation metrics
	3. Identification of challenges in creativity in MAS

**Result:** The survey presents a structured framework for understanding creative workflows in MAS, highlighting techniques like divergent exploration and issues such as evaluation standards and bias mitigation.

**Limitations:** The paper does not fully explore all domain-specific applications of creativity in MAS due to the breadth of the topic.

**Conclusion:** This work lays the groundwork for future research and development in the creative capabilities of multi-agent systems, aiming for improved standardization and evaluation.

**Abstract:** Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.

</details>


### [11] [Learning Annotation Consensus for Continuous Emotion Recognition](https://arxiv.org/abs/2505.21196)

*Ibrahim Shoer, Engin Erzin*

**Main category:** cs.HC

**Keywords:** affective computing, emotion recognition, multi-annotator, machine learning, data aggregation

**Relevance Score:** 6

**TL;DR:** This paper introduces a multi-annotator training method for continuous emotion recognition that aggregates diverse annotations into a unified label, improving predictive accuracy over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods in affective computing often lose valuable information by merging multiple annotations into a single gold standard label, which may not reflect the diversity of inputs from different annotators.

**Method:** The proposed method employs a consensus network that aggregates annotations from multiple annotators to form a unified representation, which is then used to guide the primary arousal-valence predictor.

**Key Contributions:**

	1. Introduction of a consensus network for emotion recognition
	2. Demonstrated enhancement in predictive performance using multi-annotator data
	3. Implications for fields with diverse annotation challenges

**Result:** The method was tested on the RECOLA and COGNIMUSE datasets and demonstrated improved performance over conventional methods that rely on single reference labels.

**Limitations:** 

**Conclusion:** The study demonstrates the advantages of utilizing multi-annotator data in emotion recognition, making it applicable in various fields where annotations are often inconsistent.

**Abstract:** In affective computing, datasets often contain multiple annotations from different annotators, which may lack full agreement. Typically, these annotations are merged into a single gold standard label, potentially losing valuable inter-rater variability. We propose a multi-annotator training approach for continuous emotion recognition (CER) that seeks a consensus across all annotators rather than relying on a single reference label. Our method employs a consensus network to aggregate annotations into a unified representation, guiding the main arousal-valence predictor to better reflect collective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach outperforms traditional methods that unify annotations into a single label. This underscores the benefits of fully leveraging multi-annotator data in emotion recognition and highlights its applicability across various fields where annotations are abundant yet inconsistent.

</details>


### [12] [Dynamic Vision from EEG Brain Recordings: How much does EEG know?](https://arxiv.org/abs/2505.21385)

*Prajwal Singh, Anupam Sharma, Pankaj Pandey, Krishna Miyapuram, Shanmuganathan Raman*

**Main category:** cs.HC

**Keywords:** EEG, video synthesis, contrastive learning, StyleGAN, dynamic visual information

**Relevance Score:** 5

**TL;DR:** A framework to reconstruct dynamic visual stimuli from EEG data using triplet-based contrastive learning and modified StyleGAN-ADA.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of reconstructing dynamic visual information from EEG signals due to their non-stationary nature and low SNR.

**Method:** A feature extraction network is trained with a triplet-based contrastive learning strategy, followed by video synthesis using a modified StyleGAN-ADA that incorporates temporal information.

**Key Contributions:**

	1. Proposed a novel framework for reconstructing dynamic visuals from EEG signals.
	2. Utilized triplet-based contrastive learning for feature extraction.
	3. Modified StyleGAN-ADA to integrate temporal dynamics in video synthesis.

**Result:** The framework effectively synthesizes dynamic visual stimuli from EEG, revealing how different brain regions contribute to processing such information.

**Limitations:** Limited availability of EEG-Video stimulus datasets may restrict further validation and application of the model.

**Conclusion:** The findings highlight the potential of deriving dynamic visual information from EEG data, providing insights for future research.

**Abstract:** Reconstructing and understanding dynamic visual information (video) from brain EEG recordings is challenging due to the non-stationary nature of EEG signals, their low signal-to-noise ratio (SNR), and the limited availability of EEG-Video stimulus datasets. Most recent studies have focused on reconstructing static images from EEG recordings. In this work, we propose a framework to reconstruct dynamic visual stimuli from EEG data and conduct an in-depth study of the information encoded in EEG signals. Our approach first trains a feature extraction network using a triplet-based contrastive learning strategy within an EEG-video generation framework. The extracted EEG features are then used for video synthesis with a modified StyleGAN-ADA, which incorporates temporal information as conditioning. Additionally, we analyze how different brain regions contribute to processing dynamic visual stimuli. Through several empirical studies, we evaluate the effectiveness of our framework and investigate how much dynamic visual information can be inferred from EEG signals. The inferences we derive through our extensive studies would be of immense value to future research on extracting visual dynamics from EEG.

</details>


### [13] [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)

*Samuel Rhys Cox, Rune MÃ¸berg Jacobsen, Niels van Berkel*

**Main category:** cs.HC

**Keywords:** Self-disclosure, Chatbots, Human-Computer Interaction, Emotional disclosure, Factual disclosure

**Relevance Score:** 7

**TL;DR:** This study examines how a chatbot's framing, as either Familiar or Stranger, influences users' self-disclosure during interactions, finding that the type of disclosure sought affects user comfort and enjoyment levels.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the relationship between chatbot framing and users' self-disclosure behavior, particularly focusing on the perceived relationship and its impact on users' willingness to share personal thoughts and feelings.

**Method:** Participants interacted with either a Familiar chatbot (recalling past interactions) or a Stranger chatbot (with no prior connection) across two sessions, one focusing on Emotional-disclosure and the other on Factual-disclosure, in a mixed factorial design.

**Key Contributions:**

	1. Investigates the effect of chatbot relationship framing on self-disclosure.
	2. Identifies differences in comfort and disclosure based on the type of interactions with chatbots.
	3. Provides insights into how familiarity impacts user experience and emotional engagement.

**Result:** Stranger-condition participants felt more comfortable self-disclosing when Emotional-disclosure was sought first, but Familiar-condition participants reported more enjoyment when Factual-disclosure was prioritized.

**Limitations:** The study is limited to interactions with fictional chatbots and may not generalize to real-world contexts.

**Conclusion:** The findings indicate that chatbot framing significantly affects self-disclosure, with anonymity provided by Stranger chatbots benefiting emotional sharing, while Familiar chatbots can enhance enjoyment in factual discussions if rapport is established first.

**Abstract:** Self-disclosure, the sharing of one's thoughts and feelings, is affected by the perceived relationship between individuals. While chatbots are increasingly used for self-disclosure, the impact of a chatbot's framing on users' self-disclosure remains under-explored. We investigated how a chatbot's description of its relationship with users, particularly in terms of ephemerality, affects self-disclosure. Specifically, we compared a Familiar chatbot, presenting itself as a companion remembering past interactions, with a Stranger chatbot, presenting itself as a new, unacquainted entity in each conversation. In a mixed factorial design, participants engaged with either the Familiar or Stranger chatbot in two sessions across two days, with one conversation focusing on Emotional- and another Factual-disclosure. When Emotional-disclosure was sought in the first chatting session, Stranger-condition participants felt more comfortable self-disclosing. However, when Factual-disclosure was sought first, these differences were replaced by more enjoyment among Familiar-condition participants. Qualitative findings showed Stranger afforded anonymity and reduced judgement, whereas Familiar sometimes felt intrusive unless rapport was built via low-risk Factual-disclosure.

</details>


### [14] [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)

*Saharsh Barve, Andy Mao, Jiayue Melissa Shi, Prerna Juneja, Koustuv Saha*

**Main category:** cs.HC

**Keywords:** text-to-image generation, bias detection, social stereotypes, LLM-supported refinement, AI ethics

**Relevance Score:** 8

**TL;DR:** This paper presents a bias detection rubric and Social Stereotype Index (SSI) to evaluate and mitigate societal biases in text-to-image (T2I) models, finding significant improvements through LLM-supported prompt refinement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical concerns arising from societal stereotypes in generative AI, particularly in text-to-image outputs.

**Method:** The study involved auditing outputs from three T2I models using 100 queries and applying a bias detection rubric alongside targeted prompt refinement with LLMs.

**Key Contributions:**

	1. Development of a theory-driven bias detection rubric
	2. Creation of the Social Stereotype Index (SSI) for evaluating biases
	3. Identifying the tension between bias mitigation and contextual alignment in AI-generated imagery.

**Result:** The application of the bias detection rubric led to a significant reduction in bias as measured by the SSI: 61% for geocultural, 69% for occupational, and 51% for adjectival queries.

**Limitations:** The study is limited to three T2I models and their outputs, potentially not generalizable to all T2I systems.

**Conclusion:** The need for T2I systems to balance ethical debiasing with contextual relevance was emphasized, advocating for global diversity without sacrificing social complexity.

**Abstract:** Recent advances in generative AI have enabled visual content creation through text-to-image (T2I) generation. However, despite their creative potential, T2I models often replicate and amplify societal stereotypes -- particularly those related to gender, race, and culture -- raising important ethical concerns. This paper proposes a theory-driven bias detection rubric and a Social Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs. We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and Stability AI Core -- using 100 queries across three categories -- geocultural, occupational, and adjectival. Our analysis reveals that initial outputs are prone to include stereotypical visual cues, including gendered professions, cultural markers, and western beauty norms. To address this, we adopted our rubric to conduct targeted prompt refinement using LLMs, which significantly reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries. We complemented our quantitative analysis through a user study examining perceptions, awareness, and preferences around AI-generated biased imagery. Our findings reveal a key tension -- although prompt refinement can mitigate stereotypes, it can limit contextual alignment. Interestingly, users often perceived stereotypical images to be more aligned with their expectations. We discuss the need to balance ethical debiasing with contextual relevance and call for T2I systems that support global diversity and inclusivity while not compromising the reflection of real-world social complexity.

</details>


### [15] [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)

*Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung-yi Lee, Yun-Nung Chen*

**Main category:** cs.HC

**Keywords:** multi-agent systems, creativity, large language models, evaluation metrics, collaborative generation

**Relevance Score:** 9

**TL;DR:** This paper surveys creativity in large language model-driven multi-agent systems, focusing on idea generation, evaluation methods, and agent persona design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of creativity in multi-agent systems (MAS) and how it influences collaboration and output generation between humans and AIs.

**Method:** The survey categorizes agent proactivity and persona design, details generation techniques, and identifies key challenges in creativity-focused MAS.

**Key Contributions:**

	1. Taxonomy of agent proactivity and persona design
	2. Overview of generation techniques and relevant datasets
	3. Discussion of challenges in MAS creativity

**Result:** The survey presents a new taxonomy of creativity in MAS, including divergent exploration and collaborative synthesis techniques, while discussing significant challenges like evaluation standards and bias mitigation.

**Limitations:** Existing inconsistent evaluation standards and a lack of unified benchmarks.

**Conclusion:** The paper provides a structured framework aimed at advancing the development, evaluation, and standardization of creativity in MAS, paving the way for improved and standardized creative workflows.

**Abstract:** Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)

*Amr Hegazy, Mostafa Elhoushi, Amr Alanwar*

**Main category:** cs.CL

**Keywords:** Large Language Models, activation steering, safety control, inference-time intervention, adaptive mechanisms

**Relevance Score:** 9

**TL;DR:** Introducing a trainable controller network for fine-grained control over Large Language Model (LLM) behaviors during inference, enhancing safety without the need for fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To control undesirable LLM behaviors such as generating unsafe content without incurring the costs of fine-tuning.

**Method:** A lightweight, trainable controller network observes LLM activations and predicts a global scaling factor and layer-specific weights that modulate a steering patch during generation.

**Key Contributions:**

	1. Introduction of a trainable controller network for LLMs
	2. Ability to dynamically modulate LLM behavior during inference
	3. Demonstrated improvements on safety benchmarks compared to base models.

**Result:** The controller significantly increases refusal rates on safety benchmarks compared to the base LLM, achieving targeted behavioral modification without altering model parameters.

**Limitations:** 

**Conclusion:** This method demonstrates efficient, adaptive control of LLM behavior at inference time, outperforming existing techniques.

**Abstract:** Controlling undesirable Large Language Model (LLM) behaviors, such as the generation of unsafe content or failing to adhere to safety guidelines, often relies on costly fine-tuning. Activation steering provides an alternative for inference-time control, but existing methods typically lack fine-grained, adaptive mechanisms. We introduce a novel approach using a lightweight, trainable controller network integrated during inference. This controller network observes specific intermediate LLM activations and predicts both a global scaling factor and layer-specific weights. The predicted global scaling factor and layer-specific weights then dynamically modulate the intensity of a steering patch, derived from a pre-computed "refusal direction" vector, applied across the LLM's layers during generation. Trained on activations from both harmful and benign prompts, our controller learns to discriminatively apply nuanced, layer-aware interventions, activating steering primarily for harmful inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild Jailbreak Prompts demonstrate that our weighted steering controller significantly increases refusal rates compared to the base LLM, achieving targeted behavioral modification without altering the original model parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show our approach outperforms existing methods, presenting an efficient and adaptive method for fine-grained control over LLM behavior at inference time.

</details>


### [17] [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)

*Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He*

**Main category:** cs.CL

**Keywords:** natural language processing, SQL generation, reinforcement learning, large language models, Test2SQL

**Relevance Score:** 8

**TL;DR:** Introducing Arctic-Text2SQL-R1, a reinforcement learning framework for generating accurate SQL from natural language queries, showing superior performance on multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of translating natural language into executable SQL queries, especially for complex queries where traditional methods struggle.

**Method:** The proposed Arctic-Text2SQL-R1 uses a reinforcement learning framework focused on execution correctness as a reward signal, avoiding intermediary supervision.

**Key Contributions:**

	1. Introduction of a reinforcement learning framework for SQL generation
	2. State-of-the-art performance on diverse Test2SQL benchmarks
	3. Demonstration of scalability with a smaller model outperforming larger counterparts

**Result:** The method achieves state-of-the-art execution accuracy on six Test2SQL benchmarks and demonstrates that a 7B model surpasses larger 70B models in performance.

**Limitations:** 

**Conclusion:** Arctic-Text2SQL-R1's design enhances scalability and efficiency in generating SQL from natural language, also providing insights for future research.

**Abstract:** Translating natural language into SQL (Test2SQL) is a longstanding challenge at the intersection of natural language understanding and structured data access. While large language models (LLMs) have significantly improved fluency in SQL generation, producing correct and executable SQL--particularly for complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a reinforcement learning (RL) framework and model family designed to generate accurate, executable SQL using a lightweight reward signal based solely on execution correctness. Our approach avoids brittle intermediate supervision and complex reward shaping, promoting stable training and alignment with the end task. Combined with carefully curated data, strong supervised initialization, and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art execution accuracy across six diverse Test2SQL benchmarks, including the top position on the BIRD leaderboard. Notably, our 7B model outperforms prior 70B-class systems, highlighting the framework's scalability and efficiency. We further demonstrate inference-time robustness through simple extensions like value retrieval and majority voting. Extensive experiments and ablation studies offer both positive and negative insights, providing practical guidance for future Test2SQL research.

</details>


### [18] [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)

*Wang Cai, Hsiu-Yuan Huang, Zhixiang Wang, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** Dynamic Vector, In-Context Learning, Latent Representation, Reinforcement Learning, Task Adaptation

**Relevance Score:** 9

**TL;DR:** DyVec introduces a novel method for extracting and injecting task-relevant representations from LLMs, outperforming existing few-shot learning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing ICV methods are limited by sensitivity to ICL-specific factors and suboptimal representation injection positions, necessitating a more robust and adaptable approach for inference.

**Method:** The method employs an Exhaustive Query Rotation strategy to extract aggregated representations, Dynamic Latent Segmentation for partitioning based on task complexity, and REINFORCE-based optimization for selecting optimal injection positions.

**Key Contributions:**

	1. Introduction of Dynamic Vector (DyVec) method
	2. Utilization of Exhaustive Query Rotation for robust representation extraction
	3. Application of Dynamic Latent Segmentation for task-based representation partitioning

**Result:** DyVec demonstrated improved performance compared to few-shot ICL and prior ICV methods, proving the efficacy of its dynamic representation approach.

**Limitations:** 

**Conclusion:** DyVec offers a lightweight, data-efficient solution for enhancing task adaptation during inference, addressing key limitations of traditional methods.

**Abstract:** In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.   To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.   Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.

</details>


### [19] [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)

*Satya Narayana Cheetirala, Ganesh Raut, Dhavalkumar Patel, Fabio Sanatana, Robert Freeman, Matthew A Levin, Girish N. Nadkarni, Omar Dawkins, Reba Miller, Randolph M. Steinhagen, Eyal Klang, Prem Timsina*

**Main category:** cs.CL

**Keywords:** Long Text Classification, Retrieval Augmented Generation, Clinical Notes, Large Language Models, Health Informatics

**Relevance Score:** 9

**TL;DR:** This study evaluates a Retrieval Augmented Generation (RAG) approach for long text classification in clinical notes, demonstrating its efficiency without loss of accuracy compared to full context LLM processing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs.

**Method:** The approach involves splitting clinical documents into chunks, converting them into vector embeddings, and storing them in a FAISS index to retrieve the most relevant text segments for classification.

**Key Contributions:**

	1. Introduction of a RAG approach to reduce token usage
	2. Demonstrated equivalence in performance for classification tasks between RAG and LLMs processing full texts
	3. Provided insights on efficient LLM applications in health informatics.

**Result:** Evaluated three LLMs (GPT4o, LLaMA, and Mistral) showed no statistically significant differences in classification performance between the RAG approach and whole-text processing.

**Limitations:** The study has limitations in terms of the scope of evaluated models and the specific clinical task focused on.

**Conclusion:** RAG can significantly reduce token usage while maintaining classification accuracy, offering a scalable and cost-effective solution for lengthy clinical document analysis.

**Abstract:** Long text classification is challenging for Large Language Models (LLMs) due to token limits and high computational costs. This study explores whether a Retrieval Augmented Generation (RAG) approach using only the most relevant text segments can match the performance of processing entire clinical notes with large context LLMs. We begin by splitting clinical documents into smaller chunks, converting them into vector embeddings, and storing these in a FAISS index. We then retrieve the top 4,000 words most pertinent to the classification query and feed these consolidated segments into an LLM. We evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication identification task. Metrics such as AUC ROC, precision, recall, and F1 showed no statistically significant differences between the RAG based approach and whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can significantly reduce token usage without sacrificing classification accuracy, providing a scalable and cost effective solution for analyzing lengthy clinical documents.

</details>


### [20] [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)

*Mathew J. Koretsky, Maya Willey, Adi Asija, Owen Bianchi, Chelsea X. Alvarado, Tanay Nayak, Nicole Kuznetsov, Sungwon Kim, Mike A. Nalls, Daniel Khashabi, Faraz Faghri*

**Main category:** cs.CL

**Keywords:** BiomedSQL, text-to-SQL, biomedical databases, large language models, scientific reasoning

**Relevance Score:** 8

**TL;DR:** BiomedSQL is a benchmark for evaluating scientific reasoning in text-to-SQL generation in biomedical databases, consisting of 68,000 question/SQL query/answer triples.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current text-to-SQL systems struggle with translating qualitative scientific questions into SQL, especially when domain reasoning is involved.

**Method:** The dataset BiomedSQL includes 68,000 question/SQL query/answer triples and is evaluated using various LLMs against a baseline of expert accuracy.

**Key Contributions:**

	1. Introduction of the BiomedSQL benchmark for scientific reasoning in text-to-SQL systems.
	2. In-depth evaluation of various LLMs and their performance against a robust expert baseline.
	3. Public accessibility of the dataset and code for further research.

**Result:** LLMs showed significant performance gaps, with the best model achieving 62.6% execution accuracy, below the expert baseline of 90%.

**Limitations:** Performance of models remains significantly lower than expert human benchmarks, indicating room for improvement.

**Conclusion:** BiomedSQL establishes a new standard for advancing text-to-SQL systems that can facilitate scientific exploration through intelligent reasoning.

**Abstract:** Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.

</details>


### [21] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)

*Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** language models, safety, control precision, Steering Target Atoms, disentangled knowledge

**Relevance Score:** 9

**TL;DR:** The paper introduces Steering Target Atoms (STA), a method for improving control in language models by isolating and manipulating disentangled knowledge components, enhancing safety and robustness, particularly in adversarial scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve control precision in language model generation and ensure safety and reliability, especially amid intertwined internal representations that limit steerability.

**Method:** The paper proposes a novel method called Steering Target Atoms (STA) that isolates and manipulates disentangled knowledge components within language models to enhance their controllability and safety.

**Key Contributions:**

	1. Introduction of Steering Target Atoms (STA) for disentangled knowledge manipulation
	2. Demonstrated effectiveness of STA through comprehensive experiments
	3. Confirmed robustness and flexibility in adversarial contexts

**Result:** Experimental results show that the STA method allows for precise manipulation of knowledge components, improving robustness and flexibility of steering in both general and adversarial scenarios.

**Limitations:** 

**Conclusion:** The STA approach enables enhanced control over language models, confirming its effectiveness in ensuring safety during model operation, especially while dealing with complex reasoning tasks.

**Abstract:** Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.

</details>


### [22] [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)

*Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss*

**Main category:** cs.CL

**Keywords:** temporal dynamics, clinical narratives, dataset, PubMed, machine learning

**Relevance Score:** 9

**TL;DR:** The paper introduces PMOA-TTS, a dataset of 124,699 PubMed case reports converted into structured timelines, aiding in modeling patient trajectories and predictive analytics in healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for large-scale temporally annotated resources to model patient trajectories effectively in clinical narratives.

**Method:** A scalable LLM-based pipeline was developed, combining heuristic filtering and prompt-driven extraction using Llama 3.3 and DeepSeek R1 to generate structured timelines of clinical events.

**Key Contributions:**

	1. Introduction of PMOA-TTS, a large dataset for clinical timeline extraction.
	2. Utilization of LLMs for creating structured timelines in healthcare contexts.
	3. Validation of the dataset against a clinician-curated reference set using multiple metrics.

**Result:** The dataset includes over 5.6 million timestamped clinical events and achieves high-quality metrics for timeline accuracy and predictive tasks.

**Limitations:** 

**Conclusion:** PMOA-TTS is the first openly available dataset for structured timeline extraction in clinical narratives and supports advanced temporal reasoning and modeling in biomedical NLP.

**Abstract:** Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open Access (PMOA) case reports, each converted into structured (event, time) timelines via a scalable LLM-based pipeline. Our approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1, resulting in over 5.6 million timestamped clinical events. To assess timeline quality, we evaluate against a clinician-curated reference set using three metrics: (i) event-level matching (80% match at a cosine similarity threshold of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide diagnostic and demographic coverage. In a downstream survival prediction task, embeddings from extracted timelines achieve time-dependent concordance indices up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally structured narratives. PMOA-TTS provides a scalable foundation for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP. The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .

</details>


### [23] [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)

*Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu*

**Main category:** cs.CL

**Keywords:** Test-Time Scaling, Large Language Models, Reinforcement Learning, Intrinsic Signals, Efficiency

**Relevance Score:** 8

**TL;DR:** Guided by Gut (GG) is a self-guided Test-Time Scaling framework for LLMs that enhances reasoning efficiency by using intrinsic signals instead of external verifier models, achieving high accuracy with reduced computational costs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the substantial computational costs associated with existing TTS methods that rely on external Process Reward Models or sampling methods.

**Method:** GG employs a lightweight tree search guided by intrinsic LLM signals such as token-level confidence and step novelty. It includes a reinforcement learning fine-tuning phase to improve internal confidence estimates.

**Key Contributions:**

	1. Introduction of an efficient self-guided TTS framework without costly external verifier models.
	2. Improved reliability of internal confidence estimates through reinforcement learning fine-tuning.
	3. Significant reductions in memory usage and inference speed compared to existing methods.

**Result:** Empirical evaluations show that GG allows smaller models (1.5B parameters) to match or exceed the performance of much larger models (32B-70B parameters) while reducing GPU memory usage by up to 10x.

**Limitations:** 

**Conclusion:** Guided by Gut achieves PRM-level performance with significantly faster inference speeds, lower memory usage, and more efficient deployment of TTS techniques.

**Abstract:** Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.

</details>


### [24] [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)

*Yukun Zhang, Qi Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, interpretability, semantic alignment, manifold learning, bias detection

**Relevance Score:** 8

**TL;DR:** This paper proposes a Multi_Scale Manifold Alignment framework to enhance interpretability in Large Language Models by structuring their latent space into semantic manifolds.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The internal reasoning of Large Language Models is often opaque, which limits their interpretability and trust in critical applications.

**Method:** The proposed framework decomposes the latent space into global, intermediate, and local semantic manifolds, employing cross_scale mapping functions for geometric alignment and information preservation with curvature regularization.

**Key Contributions:**

	1. Introduction of Multi_Scale Manifold Alignment framework.
	2. Joint enforcement of geometric alignment and information preservation.
	3. Stable optimization through curvature regularization.

**Result:** The theoretical analysis demonstrates that the alignment error can be bounded under mild assumptions, which supports the effectiveness of the framework in improving interpretability.

**Limitations:** 

**Conclusion:** This framework not only enhances the understanding of LLMs' multi-scale semantics but also enables practical applications like bias detection and robustness improvement.

**Abstract:** Recent advances in Large Language Models (LLMs) have achieved strong performance, yet their internal reasoning remains opaque, limiting interpretability and trust in critical applications. We propose a novel Multi_Scale Manifold Alignment framework that decomposes the latent space into global, intermediate, and local semantic manifolds capturing themes, context, and word-level details. Our method introduces cross_scale mapping functions that jointly enforce geometric alignment (e.g., Procrustes analysis) and information preservation (via mutual information constraints like MINE or VIB). We further incorporate curvature regularization and hyperparameter tuning for stable optimization. Theoretical analysis shows that alignment error, measured by KL divergence, can be bounded under mild assumptions. This framework offers a unified explanation of how LLMs structure multi-scale semantics, advancing interpretability and enabling applications such as bias detection and robustness enhancement.

</details>


### [25] [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)

*Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** Large Language Models, Key-Value Cache, Cache Eviction, Machine Learning, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** The paper introduces Lookahead Q-Cache (LAQ), a novel eviction framework for key-value caches in large language models that improves cache eviction consistency and performance during text decoding.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Existing key-value cache eviction methods for large language models become inconsistent under memory constraints, necessitating a more efficient approach that better aligns with real inference scenarios.

**Method:** The paper proposes LAQ, which utilizes low-cost pseudo lookahead queries to estimate token importance for cache eviction, leading to more accurate and consistent results.

**Key Contributions:**

	1. Introduction of Lookahead Q-Cache (LAQ) for efficient key-value cache eviction in LLMs
	2. Demonstrated performance improvement on LongBench and Needle-in-a-Haystack benchmarks
	3. Proposed method is complementary to existing cache approaches.

**Result:** LAQ outperforms existing cache eviction methods, demonstrating a 1 to 4 point improvement on the LongBench benchmark under limited cache budgets while maintaining compatibility with current techniques.

**Limitations:** 

**Conclusion:** The Lookahead Q-Cache framework significantly enhances cache eviction processes in large language models, yielding better performance and flexibility for real-world applications.

**Abstract:** Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.

</details>


### [26] [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)

*Zishun Yu, Shangzhe Li, Xinhua Zhang*

**Main category:** cs.CL

**Keywords:** language model distillation, temporal difference learning, imitation learning, reinforcement learning

**Relevance Score:** 7

**TL;DR:** This paper presents a framework for temporal difference-based distillation of language models that focuses on the distributional sparsity of teacher models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational costs of large language models by exploring efficient distillation methods.

**Method:** A general framework for temporal difference-based distillation is proposed, leveraging the distributional sparsity of teacher models by operating on a reduced action space.

**Key Contributions:**

	1. Introduction of a general framework for temporal difference-based distillation.
	2. Focus on distributional sparsity in language models.
	3. Demonstration of performance improvements using a reduced action space.

**Result:** Demonstrates how practical algorithms can be derived from this framework and shows resulting performance improvements in model efficiency.

**Limitations:** 

**Conclusion:** The proposed framework provides a more efficient way to distill large language models while retaining performance by focusing on a smaller subset of vocabulary tokens.

**Abstract:** Large language models have led to significant progress across many NLP tasks, although their massive sizes often incur substantial computational costs. Distillation has become a common practice to compress these large and highly capable models into smaller, more efficient ones. Many existing language model distillation methods can be viewed as behavior cloning from the perspective of imitation learning or inverse reinforcement learning. This viewpoint has inspired subsequent studies that leverage (inverse) reinforcement learning techniques, including variations of behavior cloning and temporal difference learning methods. Rather than proposing yet another specific temporal difference method, we introduce a general framework for temporal difference-based distillation by exploiting the distributional sparsity of the teacher model. Specifically, it is often observed that language models assign most probability mass to a small subset of tokens. Motivated by this observation, we design a temporal difference learning framework that operates on a reduced action space (a subset of vocabulary), and demonstrate how practical algorithms can be derived and the resulting performance improvements.

</details>


### [27] [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)

*Yu Zhang, Wanli Jiang, Zhengyu Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-objective alignment, Reward model

**Relevance Score:** 9

**TL;DR:** MOSLIM is a novel multi-objective alignment method for LLMs using a single reward model and policy, allowing for flexibility in controlling objectives without the need for preference-specific training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for multi-objective alignment of LLMs to align with diverse human preferences in a resource-efficient manner.

**Method:** MOSLIM employs a single multi-head reward model that classifies question-answer pairs and utilizes a mapping function to optimize a policy model based on scalar rewards derived from classification results.

**Key Contributions:**

	1. Introduction of the MOSLIM method for multi-objective alignment
	2. Utilization of a single reward model to reduce complexity
	3. Demonstration of resource efficiency compared to existing methods

**Result:** MOSLIM demonstrates superior performance to existing multi-objective methods, with a significant reduction in GPU resource requirements.

**Limitations:** 

**Conclusion:** The proposed method provides an efficient way to align LLMs with multiple objectives, enhancing the usability of existing models without extensive retraining.

**Abstract:** The multi-objective alignment of Large Language Models (LLMs) is essential for ensuring foundational models conform to diverse human preferences. Current research in this field typically involves either multiple policies or multiple reward models customized for various preferences, or the need to train a preference-specific supervised fine-tuning (SFT) model. In this work, we introduce a novel multi-objective alignment method, MOSLIM, which utilizes a single reward model and policy model to address diverse objectives. MOSLIM provides a flexible way to control these objectives through prompting and does not require preference training during SFT phase, allowing thousands of off-the-shelf models to be directly utilized within this training framework. MOSLIM leverages a multi-head reward model that classifies question-answer pairs instead of scoring them and then optimize policy model with a scalar reward derived from a mapping function that converts classification results from reward model into reward scores. We demonstrate the efficacy of our proposed method across several multi-objective benchmarks and conduct ablation studies on various reward model sizes and policy optimization methods. The MOSLIM method outperforms current multi-objective approaches in most results while requiring significantly fewer GPU computing resources compared with existing policy optimization methods.

</details>


### [28] [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)

*Cayo Viegas, Rohit Gheyi, MÃ¡rcio Ribeiro*

**Main category:** cs.CL

**Keywords:** Large Language Models, POSCOMP exam, Natural Language Processing, AI in education, Computer science

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of Large Language Models (LLMs) on the POSCOMP exam for computer science graduate admissions, comparing their results with human test-takers over multiple years.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the proficiency of LLMs in specialized domains like computer science and guide future developments in AI applications.

**Method:** The performance of four LLMs (ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large) was evaluated on the POSCOMP exams from 2022 and 2023, focusing on their ability to solve complex text-based and image interpretation questions.

**Key Contributions:**

	1. Evaluation of LLMs on a specialized academic exam
	2. Comparison of LLM performance with human students
	3. Analysis of advancements in LLM capabilities over multiple years

**Result:** LLMs, especially ChatGPT-4, outperformed human students in text-based questions, achieving the highest scores on the 2023 exam. Newer models consistently surpassed human performance across all examined years.

**Limitations:** Performance in image interpretation tasks remains weak; the study focuses only on specific LLMs.

**Conclusion:** ChatGPT-4 and subsequent models demonstrate significant capabilities in text-based tasks, with ongoing improvements in LLMs suggesting promising potential for future applications in computer science education.

**Abstract:** Recent advancements in Large Language Models (LLMs) have significantly expanded the capabilities of artificial intelligence in natural language processing tasks. Despite this progress, their performance in specialized domains such as computer science remains relatively unexplored. Understanding the proficiency of LLMs in these domains is critical for evaluating their practical utility and guiding future developments. The POSCOMP, a prestigious Brazilian examination used for graduate admissions in computer science promoted by the Brazlian Computer Society (SBC), provides a challenging benchmark. This study investigates whether LLMs can match or surpass human performance on the POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP exams. The assessments measured the models' proficiency in handling complex questions typical of the exam. LLM performance was notably better on text-based questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced (49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were observed in the 2023 exam. ChatGPT-4 achieved the highest performance, surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image interpretation remains a challenge. Given the rapid evolution of LLMs, we expanded our analysis to include more recent models - o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams. These newer models demonstrate further improvements and consistently surpass both the average and top-performing human participants across all three years.

</details>


### [29] [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)

*Yukun Zhang, Qi Dong*

**Main category:** cs.CL

**Keywords:** Dynamic Manifold Evolution Theory, language generation, semantic manifold, Lyapunov stability, text fluency

**Relevance Score:** 8

**TL;DR:** Proposes Dynamic Manifold Evolution Theory (DMET) for modeling large language model generation as a dynamical system on a semantic manifold.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a framework that links the dynamics of language generation to semantic properties in a systematic way.

**Method:** Models latent state updates using discrete time Euler approximations; employs Lyapunov stability theory to define three empirical metrics for evaluating text generation.

**Key Contributions:**

	1. Introduction of Dynamic Manifold Evolution Theory (DMET) for language generation.
	2. Connection between dynamical systems theory and language model performance.
	3. Empirical metrics linking latent trajectories to linguistic qualities.

**Result:** Extensive experiments show that DMET's predictions align with linguistic properties such as fluency and coherence, providing guidelines to improve text generation.

**Limitations:** 

**Conclusion:** DMET offers a new lens for understanding language model behavior and balancing creativity with consistency.

**Abstract:** We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework that models large language model generation as a controlled dynamical system evolving on a low_dimensional semantic manifold. By casting latent_state updates as discrete time Euler approximations of continuous dynamics, we map intrinsic energy_driven flows and context_dependent forces onto Transformer components (residual connections, attention, feed-forward networks). Leveraging Lyapunov stability theory We define three empirical metrics (state continuity, clustering quality, topological persistence) that quantitatively link latent_trajectory properties to text fluency, grammaticality, and semantic coherence. Extensive experiments across decoding parameters validate DMET's predictions and yield principled guidelines for balancing creativity and consistency in text generation.

</details>


### [30] [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)

*Sonal Prabhune, Balaji Padmanabhan, Kaushik Dutta*

**Main category:** cs.CL

**Keywords:** gender bias, LLM, debiasing, RealWorldQuestioning, entropic responses

**Relevance Score:** 8

**TL;DR:** This paper investigates gender bias in LLMs, introduces the RealWorldQuestioning dataset, and proposes a prompt-based debiasing method.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address and analyze the gender bias present in popular LLMs, especially in the contexts of business and health.

**Method:** The authors developed the RealWorldQuestioning dataset and tested it on four LLMs, using qualitative and quantitative measures, including an 'LLM-as-judge,' to evaluate responses for bias.

**Key Contributions:**

	1. Introduction of the RealWorldQuestioning dataset
	2. Definition and investigation of entropy bias in LLM responses
	3. Proposed effective debiasing method using prompt-based strategies

**Result:** The study found that while overall bias does not exist at a category level, significant discrepancies were noted at the individual question level. A prompt-based debiasing approach effectively improved the information content of LLM outputs in 78% of cases.

**Limitations:** The study only considers four LLMs and specific domains; further research is needed to generalize findings across more diverse datasets and models.

**Conclusion:** A simple debiasing strategy can enhance the quality of LLM outputs by merging responses for different genders, addressing substantial biases that could affect user decision-making.

**Abstract:** We investigate the existence and persistence of a specific type of gender bias in some of the popular LLMs and contribute a new benchmark dataset, RealWorldQuestioning (released on HuggingFace ), developed from real-world questions across four key domains in business and health contexts: education, jobs, personal financial management, and general health. We define and study entropy bias, which we define as a discrepancy in the amount of information generated by an LLM in response to real questions users have asked. We tested this using four different LLMs and evaluated the generated responses both qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that there is no significant bias in LLM responses for men and women at a category level. However, at a finer granularity (the individual question level), there are substantial differences in LLM responses for men and women in the majority of cases, which "cancel" each other out often due to some responses being better for males and vice versa. This is still a concern since typical users of these tools often ask a specific question (only) as opposed to several varied ones in each of these common yet important areas of life. We suggest a simple debiasing approach that iteratively merges the responses for the two genders to produce a final result. Our approach demonstrates that a simple, prompt-based debiasing strategy can effectively debias LLM outputs, thus producing responses with higher information content than both gendered variants in 78% of the cases, and consistently achieving a balanced integration in the remaining cases.

</details>


### [31] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)

*Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh*

**Main category:** cs.CL

**Keywords:** Mental Manipulation, Multi-turn Dialogues, Large Language Models, Dataset, Self-Perception Theory

**Relevance Score:** 8

**TL;DR:** The paper introduces the MultiManip dataset for detecting mental manipulation in dialogues and proposes a novel prompting framework called SELF-PERCEPT to improve detection with LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the detection of mental manipulation in complex dialogues which is crucial for safeguarding victims of abuse.

**Method:** The authors created the MultiManip dataset containing 220 dialogues classified as manipulative or non-manipulative and evaluated state-of-the-art LLMs using different prompting strategies, culminating in the development of the SELF-PERCEPT framework.

**Key Contributions:**

	1. Introduction of the MultiManip dataset with balanced manipulative dialogue examples
	2. Evaluation of current LLMs on the dataset establishing their limitations
	3. Development of the SELF-PERCEPT two-stage prompting framework for improved detection

**Result:** State-of-the-art LLMs struggle with identifying manipulative language; the proposed SELF-PERCEPT framework performs significantly better in detecting multi-person, multi-turn manipulation.

**Limitations:** The study focuses on dialogues from reality shows, which may not fully represent all types of real-world scenarios.

**Conclusion:** The SELF-PERCEPT framework offers an effective solution to an existing gap in LLM capabilities for understanding complex dialogues involving mental manipulation.

**Abstract:** Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .

</details>


### [32] [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)

*Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Self-play, Instruction Generation, Reward Mechanism

**Relevance Score:** 9

**TL;DR:** This paper presents Self-play Reinforcement Learning (SeRL) to enhance the training of Large Language Models (LLMs) with limited data by generating high-quality instructional data and estimating rewards through a voting mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of Reinforcement Learning in improving LLMs is established, but challenges arise from the need for quality instructions and verifiable rewards, especially in specialized domains.

**Method:** SeRL consists of two modules: self-instruction, which generates new instructions from existing data, and self-rewarding, which uses a majority-voting mechanism to estimate rewards for these instructions, allowing for effective RL training without external annotations.

**Key Contributions:**

	1. Introduction of Self-play Reinforcement Learning (SeRL) framework
	2. Effective self-instruction and self-rewarding modules
	3. Achievement of high performance without reliance on high-quality external data

**Result:** Experiments show that SeRL outperforms existing methods and matches the performance achieved with high-quality datasets and verifiable rewards on various reasoning benchmarks.

**Limitations:** 

**Conclusion:** SeRL effectively boosts LLM training with limited initial data by generating reliable instructional content and automatic reward estimation, ensuring robust performance in reasoning tasks.

**Abstract:** Recent advances have demonstrated the effectiveness of Reinforcement Learning (RL) in improving the reasoning capabilities of Large Language Models (LLMs). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial data. Specifically, SeRL comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, SeRL performs conventional RL based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different LLM backbones demonstrate that the proposed SeRL yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at https://github.com/wantbook-book/SeRL.

</details>


### [33] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)

*Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li*

**Main category:** cs.CL

**Keywords:** Protein generation, Large language models, Evaluation framework, Data leakage, Natural language processing

**Relevance Score:** 6

**TL;DR:** The paper addresses limitations in protein-text models related to data leakage and evaluation metrics, proposing a new framework that outperforms existing LLMs in protein-to-text generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding and generation of proteins through protein-text models by addressing data leakage and inadequate evaluation metrics in current benchmarks.

**Method:** The authors analyze existing architectures and benchmarks, reorganize datasets, and introduce a novel evaluation framework based on biological entities alongside a retrieval-enhanced method.

**Key Contributions:**

	1. Introduction of a novel evaluation framework based on biological entities
	2. Identification of data leakage issues in current benchmarks
	3. Development of a retrieval-enhanced method for better performance in protein generation

**Result:** The proposed retrieval-enhanced method significantly outperforms fine-tuned LLMs in protein-to-text generation, demonstrating improved accuracy and efficiency in training-free scenarios.

**Limitations:** 

**Conclusion:** The novel evaluation framework and retrieval-enhanced method represent significant advancements in the performance and assessment of protein-text models.

**Abstract:** In recent years, protein-text models have gained significant attention for their potential in protein generation and understanding. Current approaches focus on integrating protein-related knowledge into large language models through continued pretraining and multi-modal alignment, enabling simultaneous comprehension of textual descriptions and protein sequences. Through a thorough analysis of existing model architectures and text-based protein understanding benchmarks, we identify significant data leakage issues present in current benchmarks. Moreover, conventional metrics derived from natural language processing fail to accurately assess the model's performance in this domain. To address these limitations, we reorganize existing datasets and introduce a novel evaluation framework based on biological entities. Motivated by our observation, we propose a retrieval-enhanced method, which significantly outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy and efficiency in training-free scenarios. Our code and data can be seen at https://github.com/IDEA-XL/RAPM.

</details>


### [34] [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)

*Xingwei Tan, Marco Valentino, Mahmud Akhter, Maria Liakata, Nikolaos Aletras*

**Main category:** cs.CL

**Keywords:** large language models, symbolic reasoning, logical reasoning, fine-tuning, generalization

**Relevance Score:** 8

**TL;DR:** This paper addresses the limitations of large language models (LLMs) in reasoning, proposing a method that combines symbolic reasoning trajectories with fine-tuning to enhance performance on logical reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of large language models in logical and mathematical reasoning, by addressing issues related to memorization and generalization.

**Method:** The authors propose generating symbolic reasoning trajectories and selecting high-quality ones using a Monte Carlo estimation-tuned reward model, followed by fine-tuning the LLMs with these trajectories.

**Key Contributions:**

	1. Introduction of symbolic reasoning trajectories for LLMs
	2. Development of an automatic tuning process reward model using Monte Carlo estimation
	3. Demonstration of improved generalization and reasoning capabilities on logical benchmarks

**Result:** The proposed method shows significant improvement in logical reasoning benchmarks such as FOLIO and LogicAsker, with noticeable gains on frontier and open-weight models.

**Limitations:** The study is a work in progress and may require further validation through broader testing.

**Conclusion:** Fine-tuning on generated symbolic reasoning trajectories improves the generalizability of LLMs and mitigates the effects of memorization, highlighting the potential of symbolic methods in enhancing LLM reasoning capabilities.

**Abstract:** Large language models (LLMs) have shown promising performance in mathematical and logical reasoning benchmarks. However, recent studies have pointed to memorization, rather than generalization, as one of the leading causes for such performance. LLMs, in fact, are susceptible to content variations, demonstrating a lack of robust symbolic abstractions supporting their reasoning process. To improve reliability, many attempts have been made to combine LLMs with symbolic methods. Nevertheless, existing approaches fail to effectively leverage symbolic representations due to the challenges involved in developing reliable and scalable verification mechanisms. In this paper, we propose to overcome such limitations by generating symbolic reasoning trajectories and select the high-quality ones using a process reward model automatically tuned based on Monte Carlo estimation. The trajectories are then employed via fine-tuning methods to improve logical reasoning and generalization. Our results on logical reasoning benchmarks such as FOLIO and LogicAsker show the effectiveness of the proposed method with large gains on frontier and open-weight models. Moreover, additional experiments on claim verification reveal that fine-tuning on the generated symbolic reasoning trajectories enhances out-of-domain generalizability, suggesting the potential impact of symbolically-guided process supervision in alleviating the effect of memorization on LLM reasoning.

</details>


### [35] [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)

*Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, Nanqing Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Synthetic Data Generation, Knowledge Graphs, Question Answering, Fine-Tuning

**Relevance Score:** 9

**TL;DR:** GraphGen is a framework that generates high-quality synthetic data for fine-tuning large language models (LLMs) by leveraging knowledge graphs and addressing data limitations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind GraphGen is to improve the quality of synthetic data used in fine-tuning LLMs, overcoming challenges related to factual inaccuracies and knowledge coverage.

**Method:** GraphGen constructs a knowledge graph from the source text, identifies knowledge gaps using the expected calibration error metric, prioritizes QA pair generation for long-tail knowledge, and employs multi-hop neighborhood sampling with style-controlled generation.

**Key Contributions:**

	1. Introduces a knowledge graph-guided framework for QA data generation.
	2. Implements multi-hop neighborhood sampling for relational information capture.
	3. Enhances diversity in generated QA pairs through style-controlled generation.

**Result:** Experimental results indicate that GraphGen significantly outperforms existing synthetic data generation methods, providing more reliable training data for supervised fine-tuning.

**Limitations:** 

**Conclusion:** GraphGen presents a comprehensive solution to the limitations of using synthetic data in LLM fine-tuning, improving data quality and coverage.

**Abstract:** Fine-tuning for large language models (LLMs) typically requires substantial amounts of high-quality supervised data, which is both costly and labor-intensive to acquire. While synthetic data generation has emerged as a promising solution, existing approaches frequently suffer from factual inaccuracies, insufficient long-tail coverage, simplistic knowledge structures, and homogenized outputs. To address these challenges, we introduce GraphGen, a knowledge graph-guided framework designed for three key question-answering (QA) scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by constructing a fine-grained knowledge graph from the source text. It then identifies knowledge gaps in LLMs using the expected calibration error metric, prioritizing the generation of QA pairs that target high-value, long-tail knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling to capture complex relational information and employs style-controlled generation to diversify the resulting QA data. Experimental results on knowledge-intensive tasks under closed-book settings demonstrate that GraphGen outperforms conventional synthetic data methods, offering a more reliable and comprehensive solution to the data scarcity challenge in supervised fine-tuning. The code and data are publicly available at https://github.com/open-sciencelab/GraphGen.

</details>


### [36] [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)

*Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam Kumaraguru, Antonio Vergari, Steffen Staab*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Machine Learning, Large Language Models, Inductive Link Prediction, Textual Semantics

**Relevance Score:** 8

**TL;DR:** SEMMA is a dual-module Knowledge Graph Foundation Model that combines textual semantics with graph structure for improved reasoning and link prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge graph reasoning by integrating transferable textual semantics along with graph structure, addressing the limitations of existing models that only consider structure.

**Method:** SEMMA leverages Large Language Models to enrich relation identifiers, generating semantic embeddings that form a textual relation graph, which is then fused with the structural aspect of the model.

**Key Contributions:**

	1. Introduction of SEMMA as a dual-module KGFM
	2. Demonstration of the importance of textual semantics in generalization
	3. Significant performance improvement over purely structural models in challenging settings.

**Result:** SEMMA outperforms purely structural baselines in inductive link prediction across 54 diverse knowledge graphs, particularly excelling in generalization settings with unseen relation vocabulary.

**Limitations:** 

**Conclusion:** Textual semantics play a crucial role in enhancing generalization capabilities in knowledge reasoning, suggesting the need for models that integrate both structural and linguistic information.

**Abstract:** Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.

</details>


### [37] [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)

*Angelina A. Aquino, Lester James V. Miranda, Elsie Marie T. Or*

**Main category:** cs.CL

**Keywords:** Tagalog, treebank, dependency parsing, Universal Dependencies, computational linguistics

**Relevance Score:** 3

**TL;DR:** UD-NewsCrawl is the largest Tagalog treebank with 15.6k trees, focusing on syntactic analysis and baseline evaluations of dependency parsers using transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a robust resource for computational linguistics research in underrepresented languages, specifically Tagalog.

**Method:** The paper details the development process of the UD-NewsCrawl treebank, including data collection, pre-processing, manual annotation, and quality assurance, as well as baseline evaluations using transformer-based models.

**Key Contributions:**

	1. Largest Tagalog treebank to date with 15.6k trees
	2. Implementation of baseline evaluations for dependency parsers
	3. Insights into syntactic challenges and grammatical properties of Tagalog.

**Result:** Baseline evaluations demonstrated the performance of state-of-the-art dependency parsers on Tagalog, addressing unique grammatical challenges.

**Limitations:** 

**Conclusion:** UD-NewsCrawl offers significant resources for advancing research in Tagalog and potentially other underrepresented languages due to its comprehensive treebank.

**Abstract:** This paper presents UD-NewsCrawl, the largest Tagalog treebank to date, containing 15.6k trees manually annotated according to the Universal Dependencies framework. We detail our treebank development process, including data collection, pre-processing, manual annotation, and quality assurance procedures. We provide baseline evaluations using multiple transformer-based models to assess the performance of state-of-the-art dependency parsers on Tagalog. We also highlight challenges in the syntactic analysis of Tagalog given its distinctive grammatical properties, and discuss its implications for the annotation of this treebank. We anticipate that UD-NewsCrawl and our baseline model implementations will serve as valuable resources for advancing computational linguistics research in underrepresented languages like Tagalog.

</details>


### [38] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)

*Shuhao Guan, Moule Lin, Cheng Xu, Xinyi Liu, Jinman Zhao, Jiexin Fan, Qi Xu, Derek Greene*

**Main category:** cs.CL

**Keywords:** OCR, image restoration, text extraction, historical documents, linguistic consistency

**Relevance Score:** 6

**TL;DR:** PreP-OCR is a two-stage pipeline for improving text extraction from degraded historical documents by combining image restoration and post-OCR correction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance text extraction accuracy from degraded historical documents by addressing both image quality and OCR linguistic errors.

**Method:** The approach consists of two stages: first, generating synthetic image pairs for training an image restoration model; second, using a ByT5 model as a post-corrector to fix OCR errors on historical text.

**Key Contributions:**

	1. Development of a two-stage PreP-OCR pipeline
	2. Use of synthetic data for training image restoration model
	3. Demonstrated significant improvement in OCR accuracy on degraded documents

**Result:** PreP-OCR significantly reduces character error rates by 63.9-70.3% when tested on 13,831 pages of real historical documents in multiple languages.

**Limitations:** 

**Conclusion:** Integrating image restoration with linguistic error correction shows promise for effectively digitizing historical archives.

**Abstract:** This paper introduces PreP-OCR, a two-stage pipeline that combines document image restoration with semantic-aware post-OCR correction to improve text extraction from degraded historical documents. Our key innovation lies in jointly optimizing image clarity and linguistic consistency. First, we generate synthetic image pairs with randomized text fonts, layouts, and degradations. An image restoration model is trained on this synthetic data, using multi-directional patch extraction and fusion to process large images. Second, a ByT5 post-corrector, fine-tuned on synthetic historical text training pairs, addresses any remaining OCR errors. Detailed experiments on 13,831 pages of real historical documents in English, French, and Spanish show that PreP-OCR pipeline reduces character error rates by 63.9-70.3\% compared to OCR on raw images. Our pipeline demonstrates the potential of integrating image restoration with linguistic error correction for digitizing historical archives.

</details>


### [39] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)

*Qishuai Zhong, Zongmin Li, Siqi Fan, Aixin Sun*

**Main category:** cs.CL

**Keywords:** large language models, sociodemographic adaptation, multi-turn dialogue, user profiles, reasoning capabilities

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework to evaluate how large language models (LLMs) adapt their responses based on users' sociodemographic characteristics through dialogue history and explicit profiles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective engagement by LLMs that adjusts responses to users' sociodemographic characteristics.

**Method:** A multi-agent pipeline constructs a synthetic dataset that pairs dialogue histories with user profiles, and evaluates model behavior consistency across modalities.

**Key Contributions:**

	1. Proposed framework for evaluating LLM adaptation to sociodemographic attributes
	2. Demonstrated the impact of reasoning capabilities on adaptation consistency
	3. Constructed a synthetic dataset for testing model responses

**Result:** Most LLMs adjust their expressed values in response to demographic changes, with variations in consistency; models with stronger reasoning capabilities show better alignment.

**Limitations:** The study relies on synthetic data and may not fully capture real-world interactions.

**Conclusion:** Reasoning ability in LLMs is crucial for effective adaptation to users' sociodemographic attributes, suggesting a focus for future improvements.

**Abstract:** Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.

</details>


### [40] [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/abs/2505.20438)

*Jingyu Liu, Ce Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Model, LLM optimization, inference efficiency, speculative decoding, KV cache

**Relevance Score:** 9

**TL;DR:** HAMburger is a new model for optimizing Large Language Model (LLM) inference by enhancing computation and memory efficiency through innovative resource allocation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiencies in LLM inference, where each token currently requires a separate forward pass and KV cache, potentially leading to sub-optimal performance.

**Method:** The authors introduce HAMburger, which stacks a compositional embedder and a micro-step decoder to allow multiple tokens to be generated per step and utilize a speculative decoding framework.

**Key Contributions:**

	1. Introduction of HAMburger for efficient LLM inference
	2. Performance improvements in KV cache usage and throughput
	3. Speculative decoding framework for enhanced token generation

**Result:** HAMburger reduces KV cache computation by up to 2x and achieves up to 2x throughput (TPS), while maintaining quality across tasks with varying context lengths.

**Limitations:** 

**Conclusion:** The method provides a hardware-agnostic approach that greatly improves inference efficiency and scalability of LLMs, particularly for challenging computation- and memory-constrained environments.

**Abstract:** The growing demand for efficient Large Language Model (LLM) inference requires a holistic optimization on algorithms, systems, and hardware. However, very few works have fundamentally changed the generation pattern: each token needs one forward pass and one KV cache. This can be sub-optimal because we found that LLMs are extremely capable of self-identifying the exact dose of information that a single KV cache can store, and many tokens can be generated confidently without global context. Based on this insight, we introduce HAMburger, a Hierarchically Auto-regressive Model that redefines resource allocation in LLMs by moving beyond uniform computation and storage per token during inference. Stacking a compositional embedder and a micro-step decoder in between a base LLM, HAMburger smashes multiple tokens into a single KV and generates several tokens per step. Additionally, HAMburger functions as a speculative decoding framework where it can blindly trust self-drafted tokens. As a result, HAMburger shifts the growth of KV cache and forward FLOPs from linear to sub-linear with respect to output length, and adjusts its inference speed based on query perplexity and output structure. Extensive evaluations show that HAMburger reduces the KV cache computation by up to 2$\times$ and achieves up to 2$\times$ TPS, while maintaining quality in both short- and long-context tasks. Our method explores an extremely challenging inference regime that requires both computation- and memory-efficiency with a hardware-agnostic design.

</details>


### [41] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)

*Xiao Liu, Xinyi Dong, Xinyang Gao, Yansong Feng, Xun Pang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Idea Generation, Research Ideas, Metadata, Automatic Validation

**Relevance Score:** 9

**TL;DR:** This paper examines how augmenting LLMs with relevant data during idea generation can enhance the quality of research ideas, finding significant improvements through data incorporation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by LLM-generated research ideas related to feasibility and effectiveness, especially in the social science domain.

**Method:** The study introduces two methods: integrating metadata during the idea generation phase to steer LLMs towards feasible research directions, and employing automatic validation during the selection phase to evaluate the plausibility of the generated ideas.

**Key Contributions:**

	1. Introduced metadata guidance for LLM-generated ideas
	2. Implemented automatic validation for idea selection
	3. Demonstrated improved feasibility and quality of research ideas in a human study

**Result:** Experiments show that using metadata leads to a 20% improvement in the feasibility of generated ideas and a 7% increase in the quality of ideas selected through automatic validation.

**Limitations:** 

**Conclusion:** The incorporation of relevant data during the idea generation and selection processes enhances the quality and feasibility of LLM-generated research ideas, demonstrating its utility in academic settings.

**Abstract:** Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.

</details>


### [42] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)

*Zhaolin Li, Jan Niehues*

**Main category:** cs.CL

**Keywords:** language models, speech recognition, in-context learning, low-resource languages, automatic speech recognition

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of large language models (LLMs) to learn unseen, low-resource languages in speech recognition tasks through in-context learning (ICL), demonstrating that relevant sample provision can enhance performance significantly.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs can learn low-resource languages using in-context learning for speech recognition, addressing the limitation of existing LLMs supporting only a small number of languages.

**Method:** Experiments on four endangered languages not previously seen by LLMs, testing the effectiveness of in-context learning through varying relevant text samples and comparing probability-based and instruction-based approaches.

**Key Contributions:**

	1. Demonstrated LLMs can learn unseen languages via in-context learning for ASR tasks.
	2. Showed that relevant text samples improve performance in language modeling and ASR.
	3. Established that a probability-based approach is more effective than traditional instruction-based methods.

**Result:** LLMs can achieve Automatic Speech Recognition performance comparable to dedicated language models for the tested low-resource languages when utilizing in-context learning and more relevant training samples.

**Limitations:** Focused on a small number of endangered languages, may not generalize to all low-resource languages.

**Conclusion:** In-context learning allows LLMs to effectively learn and perform tasks in low-resource languages, demonstrating that they can surpass traditional models designed specifically for those languages.

**Abstract:** With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs.

</details>


### [43] [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/abs/2505.20451)

*Sahana Ramnath, Anurag Mudgil, Brihi Joshi, Skyler Hallinan, Xiang Ren*

**Main category:** cs.CL

**Keywords:** large language models, judging, dialog acts, conversational maxims, multi-turn interactions

**Relevance Score:** 9

**TL;DR:** Amulet is a framework designed to enhance the accuracy of language model judges in complex, multi-turn conversations by utilizing dialog acts and conversational maxims.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To benchmark and improve large language model judges in real-world conversational contexts, accounting for the diversity of topics, intents, and requirements in human-assistant interactions.

**Method:** Amulet leverages linguistic concepts such as dialog acts and conversational maxims to evaluate preference responses within multi-turn conversational contexts, demonstrating how frequently human intents change during dialogues.

**Key Contributions:**

	1. Introduces a framework for improving LLM-judges based on dialog acts and maxims
	2. Demonstrates significant changes in human intents across conversation turns
	3. Shows strong performance improvements across multiple datasets

**Result:** Amulet improves judgment accuracy on four challenging datasets, revealing that human intents change frequently in conversations and that dialog acts and maxims significantly aid in differentiating preference responses.

**Limitations:** 

**Conclusion:** Amulet can function independently as a judge for a single LLM or as part of a jury of LLM judges, yielding substantial improvements on relevant baseline performances.

**Abstract:** Today, large language models are widely used as judges to evaluate responses from other language models. Hence, it is imperative to benchmark and improve these LLM-judges on real-world language model usage: a typical human-assistant conversation is lengthy, and shows significant diversity in topics, intents, and requirements across turns, e.g. social interactions, task requests, feedback. We present Amulet, a framework that leverages pertinent linguistic concepts of dialog-acts and maxims to improve the accuracy of LLM-judges on preference data with complex, multi-turn conversational context. Amulet presents valuable insights about (a) the communicative structures and intents present in the conversation (dialog acts), and (b) the satisfaction of conversational principles (maxims) by the preference responses, and uses them to make judgments. On four challenging datasets, Amulet shows that (a) humans frequently (60 to 70 percent of the time) change their intents from one turn of the conversation to the next, and (b) in 75 percent of instances, the preference responses can be differentiated via dialog acts and/or maxims, reiterating the latter's significance in judging such data. Amulet can be used either as a judge by applying the framework to a single LLM, or integrated into a jury with different LLM judges; our judges and juries show strong improvements on relevant baselines for all four datasets.

</details>


### [44] [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)

*Vibhor Agarwal, Arjoo Gupta, Suparna De, Nishanth Sastry*

**Main category:** cs.CL

**Keywords:** conversational context, Conversation Kernels, social media analysis

**Relevance Score:** 6

**TL;DR:** The paper proposes a mechanism for understanding individual posts in online conversations by capturing their contextual dependencies. It introduces Conversation Kernels to analyze various aspects of posts, tested on data from slashdot.org.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of social networks, understanding online conversations has become important, yet challenging due to the complexity and short nature of posts.

**Method:** The paper develops two families of Conversation Kernels that explore the neighborhood of a post within a conversation tree to build relevant context for categorizing posts.

**Key Contributions:**

	1. Introduction of Conversation Kernels to capture conversational context.
	2. Application of the model to different labeling tasks on slashdot.org.
	3. Demonstration of general-purpose flexibility in understanding conversations.

**Result:** The findings demonstrate the adaptability of Conversation Kernels for tasks such as identifying if a post is informative, insightful, interesting, or funny.

**Limitations:** 

**Conclusion:** The framework proves flexible for various conversation understanding tasks, validated through experiments on user-labeled data from slashdot.org.

**Abstract:** Understanding online conversations has attracted research attention with the growth of social networks and online discussion forums. Content analysis of posts and replies in online conversations is difficult because each individual utterance is usually short and may implicitly refer to other posts within the same conversation. Thus, understanding individual posts requires capturing the conversational context and dependencies between different parts of a conversation tree and then encoding the context dependencies between posts and comments/replies into the language model.   To this end, we propose a general-purpose mechanism to discover appropriate conversational context for various aspects about an online post in a conversation, such as whether it is informative, insightful, interesting or funny. Specifically, we design two families of Conversation Kernels, which explore different parts of the neighborhood of a post in the tree representing the conversation and through this, build relevant conversational context that is appropriate for each task being considered. We apply our developed method to conversations crawled from slashdot.org, which allows users to apply highly different labels to posts, such as 'insightful', 'funny', etc., and therefore provides an ideal experimental platform to study whether a framework such as Conversation Kernels is general-purpose and flexible enough to be adapted to disparately different conversation understanding tasks.

</details>


### [45] [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)

*Roi Cohen, Russa Biswas, Gerard de Melo*

**Main category:** cs.CL

**Keywords:** factual completeness, large language models, informativeness alignment, factuality, text generation

**Relevance Score:** 8

**TL;DR:** The paper proposes an informativeness alignment mechanism for improving the informativeness of text generated by LLMs while maintaining factuality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often generate factually correct but less informative text. The authors aim to address this issue to enhance the informativeness of generated content.

**Method:** The authors introduce an informativeness alignment mechanism that utilizes recent factual benchmarks to create an objective that prioritizes responses that are both correct and informative.

**Key Contributions:**

	1. Introduction of an informativeness alignment mechanism for LLMs.
	2. Development of an objective that balances factuality and informativeness.
	3. Empirical evidence showing improved performance in both aspects during model training.

**Result:** The study demonstrates that training models under the informativeness alignment objective leads to improvements in both informativeness and factual correctness of generated text.

**Limitations:** 

**Conclusion:** The proposed method effectively aligns the generation process of LLMs towards producing more informative and factually correct text.

**Abstract:** Factual completeness is a general term that captures how detailed and informative a factually correct text is. For instance, the factual sentence ``Barack Obama was born in the United States'' is factually correct, though less informative than the factual sentence ``Barack Obama was born in Honolulu, Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate and generate factually incorrect text, they might also tend to choose to generate factual text that is indeed factually correct and yet less informative than other, more informative choices. In this work, we tackle this problem by proposing an informativeness alignment mechanism. This mechanism takes advantage of recent factual benchmarks to propose an informativeness alignment objective. This objective prioritizes answers that are both correct and informative. A key finding of our work is that when training a model to maximize this objective or optimize its preference, we can improve not just informativeness but also factuality.

</details>


### [46] [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)

*Asif Shahriar, Rifat Shahriyar, M Saifur Rahman*

**Main category:** cs.CL

**Keywords:** Inceptive Transformer, multi-scale feature extraction, token representation

**Relevance Score:** 7

**TL;DR:** Introduces Inceptive Transformer, a modular architecture enhancing transformer token representations for improved task performance and reduced information loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional transformers often lose information by compressing sequences into a single token, which can hinder tasks requiring localized cues.

**Method:** A modular and lightweight architecture that integrates a multi-scale feature extraction module, inspired by inception networks, to dynamically weight tokens based on task relevance.

**Key Contributions:**

	1. Introduction of Inceptive Transformer architecture
	2. Dynamic weighting of tokens for task relevance
	3. Cross-lingual applicability in various domains

**Result:** The Inceptive Transformer consistently outperformed baselines by 1% to 14% in various tasks, demonstrating its efficiency and cross-lingual applicability.

**Limitations:** 

**Conclusion:** The model proves beneficial for enriching transformer representations across diverse domains, improving performance in localized and global tasks.

**Abstract:** Conventional transformer models typically compress the information from all tokens in a sequence into a single \texttt{[CLS]} token to represent global context-- an approach that can lead to information loss in tasks requiring localized or hierarchical cues. In this work, we introduce \textit{Inceptive Transformer}, a modular and lightweight architecture that enriches transformer-based token representations by integrating a multi-scale feature extraction module inspired by inception networks. Our model is designed to balance local and global dependencies by dynamically weighting tokens based on their relevance to a particular task. Evaluation across a diverse range of tasks including emotion recognition (both English and Bangla), irony detection, disease identification, and anti-COVID vaccine tweets classification shows that our models consistently outperform the baselines by 1\% to 14\% while maintaining efficiency. These findings highlight the versatility and cross-lingual applicability of our method for enriching transformer-based representations across diverse domains.

</details>


### [47] [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)

*Naba Rizvi, Harper Strickland, Saleha Ahmedi, Aekta Kallepalli, Isha Khirwadkar, William Wu, Imani N. S. Munyaka, Nedjma Ousidhoum*

**Main category:** cs.CL

**Keywords:** large language models, ableism, autistic individuals, bias detection, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper evaluates the ability of large language models to detect nuanced ableism against autistic individuals, revealing strengths and limitations in their understanding and contextual recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models conceptualize and detect ableism, particularly against autistic individuals, amidst previous findings of disability-related biases in LLMs.

**Method:** Four large language models were evaluated for their ability to identify ableist language directed at autistic individuals, alongside a qualitative comparison with human annotations.

**Key Contributions:**

	1. Identification of limitations in LLMsâ ability to detect nuanced ableism against autistic individuals.
	2. Comparison between LLM and human annotation processes highlighting context consideration.
	3. Suggestions for improving LLM performance in recognizing harmful language through better contextual understanding.

**Result:** The models can recognize autism-related terms but frequently miss harmful connotations, relying on keyword matching rather than contextual understanding.

**Limitations:** The study is limited to four LLMs and their ability to recognize only specific ableist language without deeper contextual analysis.

**Conclusion:** A binary classification scheme is adequate for evaluating LLM performance in recognizing ableism, aligning with human annotators' agreements in classification.

**Abstract:** Large language models (LLMs) are increasingly used in decision-making tasks like r\'esum\'e screening and content moderation, giving them the power to amplify or suppress certain perspectives. While previous research has identified disability-related biases in LLMs, little is known about how they conceptualize ableism or detect it in text. We evaluate the ability of four LLMs to identify nuanced ableism directed at autistic individuals. We examine the gap between their understanding of relevant terminology and their effectiveness in recognizing ableist content in context. Our results reveal that LLMs can identify autism-related language but often miss harmful or offensive connotations. Further, we conduct a qualitative comparison of human and LLM explanations. We find that LLMs tend to rely on surface-level keyword matching, leading to context misinterpretations, in contrast to human annotators who consider context, speaker identity, and potential impact. On the other hand, both LLMs and humans agree on the annotation scheme, suggesting that a binary classification is adequate for evaluating LLM performance, which is consistent with findings from prior studies involving human annotators.

</details>


### [48] [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/abs/2505.20501)

*Rohan Balasubramanian, Nitish Gokulakrishnan, Syeda Jannatus Saba, Steven Skiena*

**Main category:** cs.CL

**Keywords:** lipograms, large language models, text transformation, meaning retention, language adaptability

**Relevance Score:** 4

**TL;DR:** This study examines the transformation of 'The Great Gatsby' into a text without the letter 'e' using various methods, highlighting the adaptability of language under constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of large language models (LLMs) in the context of constrained writing, specifically lipograms.

**Method:** The study involved transforming 'The Great Gatsby' into an 'e'-less text using techniques such as synonym replacement and generative models enhanced with beam search and named entity analysis.

**Key Contributions:**

	1. Demonstration of LLMs' capabilities in constrained writing tasks.
	2. Assessment of the impact of letter exclusion on meaning retention.
	3. Insights into language adaptability under constraints.

**Result:** The findings indicate that excluding up to 3.6% of the most common letters minimally impacts meaning, though translation fidelity declines with more stringent constraints.

**Limitations:** The study primarily focuses on English and results may vary for other languages; performance may depend on the specific LLM used.

**Conclusion:** The research reveals the flexibility of English under strict constraints, emphasizing the adaptability and creativity of language.

**Abstract:** Lipograms are a unique form of constrained writing where all occurrences of a particular letter are excluded from the text, typified by the novel Gadsby, which daringly avoids all usage of the letter 'e'. In this study, we explore the power of modern large language models (LLMs) by transforming the novel F. Scott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented with a range of techniques, from baseline methods like synonym replacement to sophisticated generative models enhanced with beam search and named entity analysis. We show that excluding up to 3.6% of the most common letters (up to the letter 'u') had minimal impact on the text's meaning, although translation fidelity rapidly and predictably decays with stronger lipogram constraints. Our work highlights the surprising flexibility of English under strict constraints, revealing just how adaptable and creative language can be.

</details>


### [49] [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/abs/2505.20505)

*Md Mahadi Hassan, John Salvador, Akond Rahman, Santu Karmaker*

**Main category:** cs.CL

**Keywords:** LLM, IT Automation, Ansible, Benchmark, State Reconciliation

**Relevance Score:** 8

**TL;DR:** The paper presents ITAB, a benchmark to evaluate LLMs' performance in generating Ansible scripts for IT automation tasks, revealing significant limitations in their understanding of state reconciliation and module-specific execution knowledge.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing benchmarks for evaluating LLMs in real-world IT automation tasks, particularly with tools like Ansible.

**Method:** The study introduces the ITAB benchmark, consisting of 126 diverse IT automation tasks designed to test LLMs in generating functional Ansible scripts, evaluated through dynamic execution in controlled environments.

**Key Contributions:**

	1. Introduction of the ITAB benchmark for IT automation tasks
	2. Evaluation of 14 open-source LLMs under real-world task conditions
	3. Identification of common semantic errors affecting LLM performance in generating Ansible scripts.

**Result:** Evaluation of 14 open-source LLMs revealed that none achieved a pass@10 rate beyond 12%, with analysis of 1,411 execution failures highlighting prevalent semantic errors, particularly in state reconciliation and module knowledge.

**Limitations:** The study is limited to the evaluation of open-source LLMs and the tasks defined within the ITAB benchmark.

**Conclusion:** The findings indicate that current LLMs struggle with essential reasoning for IT automation, emphasizing the need for advancements in state reasoning and domain-specific knowledge to improve reliability.

**Abstract:** LLMs show promise in code generation, yet their effectiveness for IT automation tasks, particularly for tools like Ansible, remains understudied. Existing benchmarks rely primarily on synthetic tasks that fail to capture the needs of practitioners who use IT automation tools, such as Ansible. We present ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g., configuring servers, managing files) where each task accounts for state reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs' ability to generate functional Ansible automation scripts via dynamic execution in controlled environments. We evaluate 14 open-source LLMs, none of which accomplish pass@10 at a rate beyond 12%. To explain these low scores, we analyze 1,411 execution failures across the evaluated LLMs and identify two main categories of prevalent semantic errors: failures in state reconciliation related reasoning (44.87% combined from variable (11.43%), host (11.84%), path(11.63%), and template (9.97%) issues) and deficiencies in module-specific execution knowledge (24.37% combined from Attribute and parameter (14.44%) and module (9.93%) errors). Our findings reveal key limitations in open-source LLMs' ability to track state changes and apply specialized module knowledge, indicating that reliable IT automation will require major advances in state reasoning and domain-specific execution understanding.

</details>


### [50] [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)

*Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki*

**Main category:** cs.CL

**Keywords:** speech corpus, Modern Standard Arabic, speech synthesis, voice conversion, diacritization

**Relevance Score:** 4

**TL;DR:** ArVoice is a multi-speaker Modern Standard Arabic speech corpus designed for various speech synthesis tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a high-quality, diacritized speech corpus for improving multi-speaker speech synthesis and related applications.

**Method:** The corpus includes 83.52 hours of professional recordings from six diverse voice talents, a modified subset of the Arabic Speech Corpus, and high-quality synthetic speech from commercial systems. It features three open-source text-to-speech (TTS) and two voice conversion systems for demonstration.

**Key Contributions:**

	1. Launch of a new Arabic speech synthesis corpus with diacritized transcriptions.
	2. Inclusion of diverse voice talents in the dataset.
	3. Demonstration of advanced TTS and voice conversion systems using the corpus.

**Result:** The dataset successfully demonstrates its applicability in speech-based diacritic restoration, voice conversion, and deepfake detection, showcasing high-quality synthesis capabilities.

**Limitations:** 

**Conclusion:** ArVoice is made available for research use, providing a comprehensive resource for advancing Arabic speech synthesis technologies.

**Abstract:** We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech corpus with diacritized transcriptions, intended for multi-speaker speech synthesis, and can be useful for other tasks such as speech-based diacritic restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a new professionally recorded set from six voice talents with diverse demographics, (2) a modified subset of the Arabic Speech Corpus; and (3) high-quality synthetic speech from two commercial systems. The complete corpus consists of a total of 83.52 hours of speech across 11 voices; around 10 hours consist of human voices from 7 speakers. We train three open-source TTS and two voice conversion systems to illustrate the use cases of the dataset. The corpus is available for research use.

</details>


### [51] [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)

*Chengyan Wu, Yiqiang Cai, Yang Liu, Pengxu Zhu, Yun Xue, Ziwei Gong, Julia Hirschberg, Bolei Ma*

**Main category:** cs.CL

**Keywords:** Multimodal, Emotion Recognition, Human-Computer Interaction, Survey, Dialogue Systems

**Relevance Score:** 8

**TL;DR:** A survey on Multimodal Emotion Recognition in Conversations (MERC) discussing its significance, methods, challenges, and future directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance naturalness and emotional understanding in human-computer interaction through improved emotion recognition.

**Method:** Systematic overview of MERC including core tasks, representative methods, and evaluation strategies.

**Key Contributions:**

	1. Systematic overview of MERC
	2. Identification of key challenges in multimodal emotion recognition
	3. Guidance on future research directions

**Result:** Identifies motivations for MERC and discusses recent trends and key challenges in the field.

**Limitations:** 

**Conclusion:** The growth of interest in emotionally intelligent systems creates an urgent need for advancing MERC research.

**Abstract:** While text-based emotion recognition methods have achieved notable success, real-world dialogue systems often demand a more nuanced emotional understanding than any single modality can offer. Multimodal Emotion Recognition in Conversations (MERC) has thus emerged as a crucial direction for enhancing the naturalness and emotional understanding of human-computer interaction. Its goal is to accurately recognize emotions by integrating information from various modalities such as text, speech, and visual signals.   This survey offers a systematic overview of MERC, including its motivations, core tasks, representative methods, and evaluation strategies. We further examine recent trends, highlight key challenges, and outline future directions. As interest in emotionally intelligent systems grows, this survey provides timely guidance for advancing MERC research.

</details>


### [52] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)

*Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, StÃ©phanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, AstroVisBench, scientific computing, data visualization, astronomy

**Relevance Score:** 8

**TL;DR:** AstroVisBench is the first benchmark for evaluating LLMs in scientific computing and visualization, specifically in astronomy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of evaluating LLM-mediated scientific workflows for their ability to derive correct scientific insights.

**Method:** AstroVisBench evaluates LLMs on their ability to create astronomy-specific workflows and visualize results, using a novel LLM-as-a-judge method validated by professional astronomers.

**Key Contributions:**

	1. Introduction of AstroVisBench as a benchmark for LLMs in scientific workflows.
	2. Integration of LLM-as-a-judge for evaluating visualizations validated by experts.
	3. Identification of the limitations of current LLMs in astronomy-related tasks.

**Result:** The evaluation revealed a significant gap in state-of-the-art LLMs' capabilities as assistants in astronomy research.

**Limitations:** Focuses specifically on the astronomy domain, limiting generalizability to other scientific fields.

**Conclusion:** The findings provide a pathway for developing visualization-based workflows critical across various scientific fields.

**Abstract:** Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.

</details>


### [53] [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/abs/2505.20546)

*Meng Lu, Ruochen Zhang, Ellie Pavlick, Carsten Eickhoff*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, factual consistency, mechanistic analysis, vector interventions, language translation

**Relevance Score:** 9

**TL;DR:** The paper investigates factual inconsistency in multilingual LLMs, revealing underlying mechanisms and proposing interventions to improve accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Multilingual LLMs perform better in English than in other languages, leading to inconsistent factual recall. Understanding and addressing these failures is crucial for their effective use.

**Method:** The authors conducted a mechanistic analysis to identify the pipeline LLMs use for multilingual queries and devised two vector interventions to enhance factual consistency.

**Key Contributions:**

	1. Identification of English-centric factual recall mechanisms in multilingual LLMs.
	2. Introduction of two vector interventions to enhance factual consistency across languages.
	3. Demonstration of over 35% improvement in recall accuracy for low-performing languages.

**Result:** The proposed interventions increased factual recall accuracy by over 35% for the lowest-performing language, addressing identified sources of errors in LLMs.

**Limitations:** The interventions are not tested across all LLM architectures or all languages beyond the lowest-performing ones.

**Conclusion:** Mechanistic insights can be leveraged to improve multilingual performance in LLMs, unlocking their potential across languages.

**Abstract:** Multilingual large language models (LLMs) often exhibit factual inconsistencies across languages, with significantly better performance in factual recall tasks in English than in other languages. The causes of these failures, however, remain poorly understood. Using mechanistic analysis techniques, we uncover the underlying pipeline that LLMs employ, which involves using the English-centric factual recall mechanism to process multilingual queries and then translating English answers back into the target language. We identify two primary sources of error: insufficient engagement of the reliable English-centric mechanism for factual recall, and incorrect translation from English back into the target language for the final answer. To address these vulnerabilities, we introduce two vector interventions, both independent of languages and datasets, to redirect the model toward better internal paths for higher factual consistency. Our interventions combined increase the recall accuracy by over 35 percent for the lowest-performing language. Our findings demonstrate how mechanistic insights can be used to unlock latent multilingual capabilities in LLMs.

</details>


### [54] [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)

*Chris Emezue, The NaijaVoices Community, Busayo Awobade, Abraham Owodunni, Handel Emezue, Gloria Monica Tobechukwu Emezue, Nefertiti Nneoma Emezue, Sewade Ogun, Bunmi Akinremi, David Ifeoluwa Adelani, Chris Pal*

**Main category:** cs.CL

**Keywords:** speech technology, African languages, speech datasets, automatic speech recognition, multilingual processing

**Relevance Score:** 7

**TL;DR:** Proposes the NaijaVoices dataset, a large-scale speech-text resource for African languages, to enhance speech technology performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the under-representation of African languages in speech technology, which limits accessibility for millions.

**Method:** Introduced the NaijaVoices dataset, consisting of 1,800 hours of speech collected from 5,000+ speakers, and conducted finetuning experiments on existing speech recognition models.

**Key Contributions:**

	1. Development of the NaijaVoices dataset with significant scale and diversity.
	2. Unique data collection approach that can serve as a model for other languages.
	3. Demonstrated improvements in ASR performance using the dataset.

**Result:** Finetuning with NaijaVoices achieved average WER improvements of 75.86% for Whisper, 52.06% for MMS, and 42.33% for XLSR.

**Limitations:** 

**Conclusion:** NaijaVoices has significant potential to enhance multilingual speech processing capabilities for African languages.

**Abstract:** The development of high-performing, robust, and reliable speech technologies depends on large, high-quality datasets. However, African languages -- including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to insufficient data. Popular voice-enabled technologies do not support any of the 2000+ African languages, limiting accessibility for circa one billion people. While previous dataset efforts exist for the target languages, they lack the scale and diversity needed for robust speech models. To bridge this gap, we introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+ speakers. We outline our unique data collection approach, analyze its acoustic diversity, and demonstrate its impact through finetuning experiments on automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices' potential to advance multilingual speech processing for African languages.

</details>


### [55] [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)

*Bipul Thapa, Gabriel Cofre*

**Main category:** cs.CL

**Keywords:** Customer feedback, Emotion classification, Spanish, Machine learning, Natural language processing

**Relevance Score:** 6

**TL;DR:** This paper presents a novel hybrid approach using TF-IDF and BERT embeddings for classifying customer feedback in Spanish into emotion categories, improving accuracy through a Custom Stacking Ensemble model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Classifying customer feedback into distinct emotion categories is essential for understanding sentiment and improving customer experience, especially in non-English languages.

**Method:** The study employs a hybrid model that combines TF-IDF vectorization with BERT embeddings along with a Custom Stacking Ensemble approach to classify Spanish customer feedback into positive, neutral, and negative emotions.

**Key Contributions:**

	1. Introduction of a hybrid approach that combines TF-IDF with BERT embeddings
	2. Development of a Custom Stacking Ensemble (CSE) model for improved classification accuracy
	3. Empirical results demonstrating significantly better performance compared to traditional methods.

**Result:** The proposed CSE model achieves a test accuracy of 93.3% on a native Spanish dataset, outperforming individual classifiers and models using translated feedback.

**Limitations:** 

**Conclusion:** Combining vectorization techniques like TF-IDF with BERT allows for better preservation of semantic context in emotion classification, providing essential insights for businesses to enhance customer feedback analysis.

**Abstract:** Classifying customer feedback into distinct emotion categories is essential for understanding sentiment and improving customer experience. In this paper, we classify customer feedback in Spanish into three emotion categories--positive, neutral, and negative--using advanced NLP and ML techniques. Traditional methods translate feedback from widely spoken languages to less common ones, resulting in a loss of semantic integrity and contextual nuances inherent to the original language. To address this limitation, we propose a hybrid approach that combines TF-IDF with BERT embeddings, effectively transforming Spanish text into rich numerical representations that preserve the semantic depth of the original language by using a Custom Stacking Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range of models, including Logistic Regression, KNN, Bagging classifier with LGBM, and AdaBoost. The CSE model combines these classifiers as base models and uses a one-vs-all Logistic Regression as the meta-model. Our experimental results demonstrate that CSE significantly outperforms the individual and BERT model, achieving a test accuracy of 93.3% on the native Spanish dataset--higher than the accuracy obtained from the translated version. These findings underscore the challenges of emotion classification in Spanish and highlight the advantages of combining vectorization techniques like TF-IDF with BERT for improved accuracy. Our results provide valuable insights for businesses seeking to leverage emotion classification to enhance customer feedback analysis and service improvements.

</details>


### [56] [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/abs/2505.20591)

*Sairam Gurajada, Eser Kandogan, Sajjadur Rahman*

**Main category:** cs.CL

**Keywords:** NL2SQL, Large Language Models, Exemplar Selection, Multi-Objective Optimization, SQL Generation

**Relevance Score:** 8

**TL;DR:** This paper presents a prompt optimization framework for NL2SQL systems that focuses on selecting a static set of exemplars to enhance both precision and performance in SQL generation, addressing the limitations of existing retrieval-based methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for high-precision and high-performance NL2SQL systems in production scenarios, as current approaches primarily focus on SQL quality rather than context selection and system performance.

**Method:** The authors propose a prompt optimization framework that utilizes multi-objective optimization for selecting a representative set of exemplars, capturing the intricacies of queries, databases, and SQL performance.

**Key Contributions:**

	1. Introduction of a prompt optimization framework for NL2SQL
	2. Focus on static exemplar selection for production scenarios
	3. Demonstration of effectiveness through empirical analysis

**Result:** Empirical analysis shows that the proposed method effectively improves SQL generation performance while maintaining high precision in production settings.

**Limitations:** 

**Conclusion:** The prompt optimization framework effectively addresses the nuanced requirements of NL2SQL systems in production by optimizing both the selection of exemplars and the performance of SQL queries.

**Abstract:** NL2SQL approaches have greatly benefited from the impressive capabilities of large language models (LLMs). In particular, bootstrapping an NL2SQL system for a specific domain can be as simple as instructing an LLM with sufficient contextual information, such as schema details and translation demonstrations. However, building an accurate system still requires the rigorous task of selecting the right context for each query-including identifying relevant schema elements, cell values, and suitable exemplars that help the LLM understand domain-specific nuances. Retrieval-based methods have become the go-to approach for identifying such context. While effective, these methods introduce additional inference-time costs due to the retrieval process.   In this paper, we argue that production scenarios demand high-precision, high-performance NL2SQL systems, rather than simply high-quality SQL generation, which is the focus of most current NL2SQL approaches. In such scenarios, the careful selection of a static set of exemplars-capturing the intricacies of the query log, target database, SQL constructs, and execution latencies-plays a more crucial role than exemplar selection based solely on similarity. The key challenge, however, lies in identifying a representative set of exemplars for a given production setting. To this end, we propose a prompt optimization framework that not only addresses the high-precision requirement but also optimizes the performance of the generated SQL through multi-objective optimization. Preliminary empirical analysis demonstrates the effectiveness of the proposed framework.

</details>


### [57] [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)

*Dancheng Liu, Amir Nassereldine, Chenhui Xu, Jinjun Xiong*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Acoustic Variation, Data Augmentation, Generalization, Robustness

**Relevance Score:** 7

**TL;DR:** This paper explores how acoustic and linguistic diversity in training data impacts ASR model robustness, suggesting that acoustic variation is more crucial for generalization than linguistic richness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the factors influencing the robustness of ASR models, particularly in the context of limited training data.

**Method:** The study investigates the effect of linguistic and acoustic diversity on ASR performance, employing targeted acoustic augmentation methods to evaluate their impact on generalization.

**Key Contributions:**

	1. Demonstrates that acoustic variation is more important than linguistic richness for ASR generalization.
	2. Introduces targeted acoustic augmentation methods that significantly improve ASR performance.
	3. Offers insights into building robust ASR models without relying on massive datasets.

**Result:** Using targeted acoustic augmentation techniques, the paper achieves a reduction in word-error rates by up to 19.24% on unseen datasets when trained on the 960-hour Librispeech dataset.

**Limitations:** The findings may not fully generalize across all languages or dialects due to the specific datasets used.

**Conclusion:** Strategic acoustically focused data augmentation can serve as a feasible alternative to large datasets for enhancing ASR model robustness, especially when data is scarce.

**Abstract:** Whisper's robust performance in automatic speech recognition (ASR) is often attributed to its massive 680k-hour training set, an impractical scale for most researchers. In this work, we examine how linguistic and acoustic diversity in training data affect the robustness of the ASR model and reveal that transcription generalization is primarily driven by acoustic variation rather than linguistic richness. We find that targeted acoustic augmentation methods could significantly improve the generalization ability of ASR models, reducing word-error rates by up to 19.24 percent on unseen datasets when training on the 960-hour Librispeech dataset. These findings highlight strategic acoustically focused data augmentation as a promising alternative to massive datasets for building robust ASR models, offering a potential solution to future foundation ASR models when massive human speech data is lacking.

</details>


### [58] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)

*Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong*

**Main category:** cs.CL

**Keywords:** theorem proving, Lean 4, large language model, formal verification, mathematics

**Relevance Score:** 4

**TL;DR:** The paper presents REAL-Prover, an advanced theorem prover for Lean 4 that utilizes a fine-tuned large language model to enhance performance on college-level mathematics problems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To extend the capabilities of theorem provers from high-school mathematics to more advanced college-level mathematics.

**Method:** REAL-Prover is developed using a fine-tuned large language model (REAL-Prover-v1) integrated with a retrieval system (Leansearch-PS), alongside a data extraction pipeline (HERALD-AF) for converting natural language math problems into formal statements and a new interactive environment (Jixia-interactive) for data collection.

**Key Contributions:**

	1. Introduction of REAL-Prover, a stepwise theorem prover for advanced mathematics.
	2. Development of HERALD-AF for converting natural language math problems into formal statements.
	3. Introduction of the FATE-M benchmark for evaluating algebraic problem-solving capabilities.

**Result:** REAL-Prover achieves a 23.7% success rate on the ProofNet dataset and a state-of-the-art 56.7% success rate on the new FATE-M benchmark focused on algebraic problems.

**Limitations:** 

**Conclusion:** The proposed system significantly enhances the ability of theorem provers to solve more complex mathematical problems and establishes new benchmarks for performance assessment.

**Abstract:** Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [59] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)

*Ting Xu, Zhichao Huang, Jiankai Sun, Shanbo Cheng, Wai Lam*

**Main category:** cs.CL

**Keywords:** Machine Translation, Reinforcement Learning, Sequential Decision Making, Simultaneous Translation, Language Models

**Relevance Score:** 7

**TL;DR:** A new framework called SeqPO-SiMT for simultaneous machine translation is introduced, optimizing translation quality and reducing latency via sequential decision making and tailored rewards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of simultaneous machine translation (SiMT) as a sequential decision making problem, improving upon traditional methods that focus on single-step tasks.

**Method:** The framework defines the SiMT task as a multi-step decision making problem, incorporating a tailored reward mechanism to enhance translation quality and reduce latency.

**Key Contributions:**

	1. Introduction of the SeqPO-SiMT framework for simultaneous machine translation.
	2. Utilization of a tailored reward mechanism to enhance translation quality and efficiency.
	3. Demonstrated superior performance on several datasets compared to traditional supervised fine-tuning approaches.

**Result:** Experiments on six datasets demonstrate that SeqPO-SiMT achieves higher translation quality and lower latency compared to supervised fine-tuning models, with significant improvements in metrics like COMET and Average Lagging.

**Limitations:** 

**Conclusion:** SeqPO-SiMT can rival the performance of high-performing offline translation LLMs despite operating with significantly less context in SiMT settings.

**Abstract:** We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


### [60] [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/abs/2505.20624)

*Usman Naseem, Juan Ren, Saba Anwar, Sarah Kohail, Rudy Alexandro Garrido Veliz, Robert Geislinger, Aisha Jabr, Idris Abdulmumin, Laiba Qureshi, Aarushi Ajay Borkar, Maryam Ibrahim Mukhtar, Abinew Ali Ayele, Ibrahim Said Ahmad, Adem Ali, Martin Semmann, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

**Main category:** cs.CL

**Keywords:** polarization, multilingual dataset, NLP, language models, computational social science

**Relevance Score:** 4

**TL;DR:** Introduces POLAR, a multilingual and multicultural dataset for detecting online polarization, with experiments on language models showing variable success.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of online polarization and the limitations of existing computational social science research.

**Method:** Introduced a dataset (POLAR) with over 23k instances across seven languages; fine-tuned multilingual language models and evaluated LLMs in few-shot and zero-shot scenarios.

**Key Contributions:**

	1. Creation of a multilingual, multicultural dataset for polarization
	2. Fine-tuning of multilingual pretrained language models
	3. Evaluation of LLMs in few-shot and zero-shot scenarios

**Result:** Most models perform well in binary polarization detection but struggle with predicting types and manifestations of polarization.

**Limitations:** Limited focus on the complexity of polarization types beyond binary detection.

**Conclusion:** Highlights the complex nature of polarization and the need for adaptable approaches in NLP and computational social science.

**Abstract:** Online polarization poses a growing challenge for democratic discourse, yet most computational social science research remains monolingual, culturally narrow, or event-specific. We introduce POLAR, a multilingual, multicultural, and multievent dataset with over 23k instances in seven languages from diverse online platforms and real-world events. Polarization is annotated along three axes: presence, type, and manifestation, using a variety of annotation platforms adapted to each cultural context. We conduct two main experiments: (1) we fine-tune six multilingual pretrained language models in both monolingual and cross-lingual setups; and (2) we evaluate a range of open and closed large language models (LLMs) in few-shot and zero-shot scenarios. Results show that while most models perform well on binary polarization detection, they achieve substantially lower scores when predicting polarization types and manifestations. These findings highlight the complex, highly contextual nature of polarization and the need for robust, adaptable approaches in NLP and computational social science. All resources will be released to support further research and effective mitigation of digital polarization globally.

</details>


### [61] [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)

*Sibo Xiao, Zixin Lin, Wenyang Gao, Yue Zhang*

**Main category:** cs.CL

**Keywords:** long-context processing, multi-agent framework, large language models

**Relevance Score:** 8

**TL;DR:** XpandA is a novel multi-agent framework for processing long contexts in LLMs, improving speed and effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better long-context processing in LLMs due to limitations in existing agent-based methods.

**Method:** A multi-agent framework coupled with dynamic partitioning and question-driven workflows.

**Key Contributions:**

	1. Dynamic partitioning of long texts
	2. Question-guided protocol for consistent knowledge sharing
	3. Selective replay of partitions to resolve structured information

**Result:** XpandA achieves 20% improvements and 1.5x inference speedup over existing methods on long-context benchmarks.

**Limitations:** 

**Conclusion:** XpandA significantly enhances long-context capabilities of LLMs and addresses existing limitations effectively.

**Abstract:** Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.

</details>


### [62] [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)

*Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan*

**Main category:** cs.CL

**Keywords:** Test-Time Learning, Large Language Models, Low-Rank Adaptation

**Relevance Score:** 9

**TL;DR:** This paper introduces a Test-Time Learning (TTL) framework for enhancing Large Language Models (LLMs) by dynamically adapting them to target domains using unlabeled test data, resulting in improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the limitations of LLMs in generalizing to specialized domains and handling linguistic variations, which are significant challenges in their practical applications.

**Method:** The authors propose a Test-Time Learning (TTL) paradigm (TLM) that minimizes input perplexity of unlabeled test data during testing to enhance LLM performance without requiring labeled data.

**Key Contributions:**

	1. Introduction of Test-Time Learning (TTL) paradigm for LLMs
	2. Innovative input perplexity minimization approach for performance enhancement
	3. Development of low-parameter updates through Low-Rank Adaptation (LoRA)

**Result:** The experiments demonstrate that TLM improves LLM performance by at least 20% in domain knowledge adaptation, leveraging high-perplexity samples for optimization and employing Low-Rank Adaptation (LoRA) for model updates.

**Limitations:** 

**Conclusion:** The study concludes that TLM provides a significant advancement in the adaptability of LLMs to specialized domains while mitigating issues like catastrophic forgetting.

**Abstract:** While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.

</details>


### [63] [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)

*Kai Chen, Zihao He, Taiwei Shi, Kristina Lerman*

**Main category:** cs.CL

**Keywords:** steerability, large language models, community norms, benchmark, Reddit

**Relevance Score:** 9

**TL;DR:** Steer-Bench evaluates the steerability of LLMs using community norms from Reddit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well large language models adapt their outputs to align with diverse community-specific norms and perspectives.

**Method:** Introduction of Steer-Bench, a benchmark with 30 subreddit pairs and over 10,000 instruction-response pairs to evaluate LLMs' alignment with community norms.

**Key Contributions:**

	1. Introduction of Steer-Bench benchmark for LLM evaluation
	2. In-depth analysis of 13 popular LLMs across various domains
	3. Identification of significant gaps in community-sensitive steerability among LLMs

**Result:** The best-performing LLMs achieved only around 65% accuracy, significantly lower than human experts' 81%, indicating gaps in steerability across communities.

**Limitations:** Limited to specific subreddit pairs and does not cover all community norms; accuracy levels vary widely depending on domain and model configuration.

**Conclusion:** Steer-Bench systematically measures LLMs' effectiveness in understanding community-specific instructions and their ability to represent diverse perspectives.

**Abstract:** Steerability, or the ability of large language models (LLMs) to adapt outputs to align with diverse community-specific norms, perspectives, and communication styles, is critical for real-world applications but remains under-evaluated. We introduce Steer-Bench, a benchmark for assessing population-specific steering using contrasting Reddit communities. Covering 30 contrasting subreddit pairs across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs and validated 5,500 multiple-choice question with corresponding silver labels to test alignment with diverse community norms. Our evaluation of 13 popular LLMs using Steer-Bench reveals that while human experts achieve an accuracy of 81% with silver labels, the best-performing models reach only around 65% accuracy depending on the domain and configuration. Some models lag behind human-level alignment by over 15 percentage points, highlighting significant gaps in community-sensitive steerability. Steer-Bench is a benchmark to systematically assess how effectively LLMs understand community-specific instructions, their resilience to adversarial steering attempts, and their ability to accurately represent diverse cultural and ideological perspectives.

</details>


### [64] [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)

*Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Xiao-Yang Liu, Jimin Huang, Qianqian Xie*

**Main category:** cs.CL

**Keywords:** XBRL, financial reporting, LLMs, semantic alignment, information extraction

**Relevance Score:** 7

**TL;DR:** FinTagging is a benchmark for evaluating LLMs in XBRL-based financial reporting focusing on structured information extraction and semantic alignment.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLM capabilities in handling XBRL tagging that includes both financial entity extraction and taxonomy alignment for improved financial reporting accuracy.

**Method:** The paper introduces FinTagging, decomposing XBRL tagging into FinNI (financial entity extraction) and FinCL (taxonomy alignment) and evaluates several LLMs under zero-shot conditions.

**Key Contributions:**

	1. First comprehensive benchmark for LLMs in XBRL tagging
	2. Decomposes tagging into financial extraction and concept alignment tasks
	3. Analyzes performance of diverse LLMs under realistic testing conditions.

**Result:** LLMs show strong generalization in extracting information but struggle with precise concept alignment, particularly with similar taxonomy entries.

**Limitations:** LLMs exhibit limitations in disambiguating closely related taxonomy entries in XBRL tagging.

**Conclusion:** Current LLMs are insufficient for fully automating XBRL tagging, highlighting the need for better semantic reasoning and schema-aware models.

**Abstract:** We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.

</details>


### [65] [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)

*Yi Zhu, Xin Zou, Xindong Wu*

**Main category:** cs.CL

**Keywords:** cyberbullying, dataset, incident detection, natural language processing, Chinese

**Relevance Score:** 6

**TL;DR:** Introduction of a novel cyberbullying dataset and detection method focusing on incident-based organization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Previous benchmarks for cyberbullying detection do not adequately capture the incident-based nature of real-world cyberbullying.

**Method:** A novel annotation method for constructing a cyberbullying dataset organized by incidents, combining three detection methods with human validation of pseudo labels.

**Key Contributions:**

	1. Introduction of the CHNCI dataset for cyberbullying incident detection
	2. First study focusing on incident-based cyberbullying in Chinese contexts
	3. Proposes an ensemble method for pseudo labeling in dataset construction

**Result:** The CHNCI dataset consists of 220,676 comments across 91 incidents and serves as a benchmark for cyberbullying detection.

**Limitations:** 

**Conclusion:** This study is the first to address incident detection in Chinese cyberbullying, providing a valuable resource for future research.

**Abstract:** Existing cyberbullying detection benchmarks were organized by the polarity of speech, such as "offensive" and "non-offensive", which were essentially hate speech detection. However, in the real world, cyberbullying often attracted widespread social attention through incidents. To address this problem, we propose a novel annotation method to construct a cyberbullying dataset that organized by incidents. The constructed CHNCI is the first Chinese cyberbullying incident detection dataset, which consists of 220,676 comments in 91 incidents. Specifically, we first combine three cyberbullying detection methods based on explanations generation as an ensemble method to generate the pseudo labels, and then let human annotators judge these labels. Then we propose the evaluation criteria for validating whether it constitutes a cyberbullying incident. Experimental results demonstrate that the constructed dataset can be a benchmark for the tasks of cyberbullying detection and incident prediction. To the best of our knowledge, this is the first study for the Chinese cyberbullying incident detection task.

</details>


### [66] [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)

*Yue Fang, Zhi Jin, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan*

**Main category:** cs.CL

**Keywords:** Signal Temporal Logic, Natural Language Processing, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper presents a novel dataset for transforming natural language into Signal Temporal Logic and introduces a framework for this transformation, showing improved accuracy and diversity compared to existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges faced in automatically transforming natural language (NL) into Signal Temporal Logic (STL) due to the lack of datasets and the limitations of manual transformation.

**Method:** The authors created the STL-Diversity-Enhanced (STL-DivEn) dataset comprising 16,000 NL-STL samples. They developed a Knowledge-Guided STL Transformation (KGST) framework that utilizes a generate-then-refine approach based on LLMs and external knowledge for accurate transformation.

**Key Contributions:**

	1. Introduction of the STL-Diversity-Enhanced dataset containing 16,000 NL-STL samples.
	2. Development of the Knowledge-Guided STL Transformation framework to enhance transformation accuracy.
	3. Validation of improvements in diversity and accuracy over existing datasets and baseline methods.

**Result:** Statistical analyses indicate that STL-DivEn has greater diversity than existing datasets. The KGST framework demonstrates superior transformation accuracy compared to baseline models on both STL-DivEn and DeepSTL datasets.

**Limitations:** 

**Conclusion:** The proposed dataset and transformation framework significantly improve the process of converting natural language to STL, contributing to better formal specifications in cyber-physical systems.

**Abstract:** Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise formal specification, making it widely used in cyber-physical systems such as autonomous driving and robotics. Automatically transforming NL into STL is an attractive approach to overcome the limitations of manual transformation, which is time-consuming and error-prone. However, due to the lack of datasets, automatic transformation currently faces significant challenges and has not been fully explored. In this paper, we propose an NL-STL dataset named STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched with diverse patterns. To develop the dataset, we first manually create a small-scale seed set of NL-STL pairs. Next, representative examples are identified through clustering and used to guide large language models (LLMs) in generating additional NL-STL pairs. Finally, diversity and accuracy are ensured through rigorous rule-based filters and human validation. Furthermore, we introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel approach for transforming natural language into STL, involving a generate-then-refine process based on external knowledge. Statistical analysis shows that the STL-DivEn dataset exhibits more diversity than the existing NL-STL dataset. Moreover, both metric-based and human evaluations indicate that our KGST approach outperforms baseline models in transformation accuracy on STL-DivEn and DeepSTL datasets.

</details>


### [67] [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)

*Qinzhuo Wu, Pengzhi Gao, Wei Liu, Jian Luan*

**Main category:** cs.CL

**Keywords:** GUI agents, Backtracking, Error recovery, Task completion, Machine learning

**Relevance Score:** 7

**TL;DR:** BacktrackAgent is a new framework for GUI agents that incorporates a backtracking mechanism for error detection and recovery, improving task completion efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of GUI agents by addressing errors in task execution and improving their efficiency in completing tasks.

**Method:** BacktrackAgent utilizes a backtracking mechanism with three components: verifier, judger, and reflector, supplemented by judgment rewards and a specially designed training dataset for better performance.

**Key Contributions:**

	1. Introduction of a backtracking mechanism for GUI agents
	2. Development of a training dataset tailored for backtracking
	3. Demonstrated performance improvements on key benchmarks.

**Result:** Experimental results demonstrate that BacktrackAgent significantly improves both task success rates and step accuracy on the Mobile3M and Auto-UI benchmarks.

**Limitations:** 

**Conclusion:** The incorporation of backtracking mechanisms in GUI agents leads to more efficient task completion and enhanced performance, with data and code to be shared upon acceptance.

**Abstract:** Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.

</details>


### [68] [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)

*Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** dynamic reasoning, large language models, self-route

**Relevance Score:** 9

**TL;DR:** The paper introduces Self-Route, a dynamic reasoning framework that optimizes reasoning-augmented large language models (RLLMs) by reducing unnecessary token consumption while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the inefficiencies in resource usage caused by reasoning-augmented large language models (RLLMs) when applied to simpler problems.

**Method:** Self-Route utilizes a lightweight pre-inference stage to assess model capabilities and dynamically switches between reasoning and general modes based on capability-aware embeddings.

**Key Contributions:**

	1. Introduction of Self-Route for dynamic reasoning mode selection
	2. Development of Gradient-10K dataset for model difficulty estimation
	3. Demonstration of token consumption reduction without loss of accuracy

**Result:** Self-Route reduces token consumption by 30-55% while achieving comparable accuracy to traditional reasoning models across various benchmarks.

**Limitations:** 

**Conclusion:** The framework is effective across models of different sizes and reasoning methods, showcasing its broad applicability in optimizing RLLMs.

**Abstract:** While reasoning-augmented large language models (RLLMs) significantly enhance complex task performance through extended reasoning chains, they inevitably introduce substantial unnecessary token consumption, particularly for simpler problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking phenomenon leads to inefficient resource usage without proportional accuracy gains. To address this issue, we propose Self-Route, a dynamic reasoning framework that automatically selects between general and reasoning modes based on model capability estimation. Our approach introduces a lightweight pre-inference stage to extract capability-aware embeddings from hidden layer representations, enabling real-time evaluation of the model's ability to solve problems. We further construct Gradient-10K, a model difficulty estimation-based dataset with dense complexity sampling, to train the router for precise capability boundary detection. Extensive experiments demonstrate that Self-Route achieves comparable accuracy to reasoning models while reducing token consumption by 30-55\% across diverse benchmarks. The proposed framework demonstrates consistent effectiveness across models with different parameter scales and reasoning paradigms, highlighting its general applicability and practical value.

</details>


### [69] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)

*Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** language models, self-supervised learning, NLP, pondering process, benchmark evaluations

**Relevance Score:** 9

**TL;DR:** This paper introduces a new method for integrating a pondering process into language models, enabling deeper cognitive processing and improved performance on language modeling tasks without needing human annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance cognitive processing in language models by mimicking the human pondering process before sentence articulation, thus improving model output quality.

**Method:** The model employs a self-supervised learning approach to yield a weighted sum of token embeddings during a pondering phase, integrating this feedback into the subsequent forward pass.

**Key Contributions:**

	1. Introduction of the pondering process into language models
	2. Demonstration of effectiveness across multiple architectures
	3. Code availability for replication and further research

**Result:** Pondering language models demonstrate performance on par with conventional models with double the parameters, achieving substantial improvements on multiple downstream benchmarks, particularly with the Pythia models.

**Limitations:** 

**Conclusion:** The integration of pondering into language models is effective and can significantly enhance performance, suggesting a promising direction for future research in self-supervised learning for NLP.

**Abstract:** Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Our method is straightforward and can be seamlessly integrated with various existing language models. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.

</details>


### [70] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)

*Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh*

**Main category:** cs.CL

**Keywords:** mental manipulation, dialogue dataset, self-perception, language models, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces the MultiManip dataset for detecting mental manipulation in dialogues and proposes a novel two-stage prompting framework, SELF-PERCEPT, which enhances detection performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective detection of mental manipulation in interpersonal communication to protect potential victims.

**Method:** The authors introduce the MultiManip dataset with 220 dialogues and evaluate state-of-the-art LLMs, proposing a self-perception based prompting framework called SELF-PERCEPT.

**Key Contributions:**

	1. Introduction of the MultiManip dataset for mental manipulation in dialogues.
	2. Development of SELF-PERCEPT, a two-stage prompting framework for improved manipulation detection.
	3. Evaluation of LLMs in detecting manipulation, revealing their limitations.

**Result:** Performance evaluations revealed that current LLMs struggled with manipulation detection, but SELF-PERCEPT showed strong capabilities.

**Limitations:** The paper primarily focuses on dialogues from reality shows, which may not fully generalize to other contexts.

**Conclusion:** SELF-PERCEPT significantly improves the detection of manipulative language in multi-turn, multi-person dialogues.

**Abstract:** Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .

</details>


### [71] [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)

*Praveen Srinivasa Varadhan, Srija Anand, Soma Siddhartha, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, Polyglot Fluency, Fine-tuning, Low-resource Languages, Synthetic Data Generation

**Relevance Score:** 5

**TL;DR:** The study investigates the adaptation of an English TTS model to 11 Indian languages through various fine-tuning strategies, leading to near-human performance in polyglot speech synthesis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of adapting English TTS technology for use in low-resource Indian languages.

**Method:** The research compares different fine-tuning methods: training from scratch, fine-tuning solely on Indian data, and fine-tuning on both Indian and English data.

**Key Contributions:**

	1. Development of IN-F5 for polyglot fluency in Indian languages
	2. Evidence that English pretraining aids low-resource TTS
	3. Human-in-the-loop approach for synthesizing unseen languages

**Result:** Fine-tuning with only Indian data yields the most effective model, IN-F5, achieving near-human polyglot fluency across multiple Indian languages, including the ability to synthesize previously unseen languages.

**Limitations:** 

**Conclusion:** The study demonstrates that English pretraining significantly enhances TTS capabilities for low-resource languages and proposes a compute-optimal strategy for further advancements in this field.

**Abstract:** What happens when an English Fairytaler is fine-tuned on Indian languages? We evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare: (i) training from scratch, (ii) fine-tuning English F5 on Indian data, and (iii) fine-tuning on both Indian and English data to prevent forgetting. Fine-tuning with only Indian data proves most effective and the resultant IN-F5 is a near-human polyglot; that enables speakers of one language (e.g., Odia) to fluently speak in another (e.g., Hindi). Our results show English pretraining aids low-resource TTS in reaching human parity. To aid progress in other low-resource languages, we study data-constrained setups and arrive at a compute optimal strategy. Finally, we show IN-F5 can synthesize unseen languages like Bhojpuri and Tulu using a human-in-the-loop approach for zero-resource TTS via synthetic data generation.

</details>


### [72] [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/abs/2505.20700)

*Yong Wu, Weihang Pan, Ke Li, Chen Binhui, Ping Li, Binbin Lin*

**Main category:** cs.CL

**Keywords:** language models, reasoning alignment, data efficiency, selective imitation, machine learning

**Relevance Score:** 7

**TL;DR:** A novel framework, DART, improves reasoning alignment for small language models (SLMs) by selectively imitating expert reasoning steps and adapting to their capabilities, leading to better performance and efficiency.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning the reasoning capabilities of small language models (SLMs) with those of larger models remains difficult due to distributional differences and the SLMs' limited capacity.

**Method:** DART employs a selective imitation strategy that uses step-wise adaptability estimation via solution simulation to guide SLMs when imitating expert reasoning steps, allowing exploration of alternative paths when necessary.

**Key Contributions:**

	1. Introduction of the DART framework for reasoning alignment.
	2. Selective imitation guided by adaptability estimation.
	3. Improvement in generalization and data efficiency for small language models.

**Result:** DART significantly enhances generalization and data efficiency across various reasoning benchmarks, outperforming static fine-tuning methods.

**Limitations:** 

**Conclusion:** The DART framework offers a scalable approach for improving reasoning alignment in resource-constrained models by aligning training signals with the capabilities of the SLMs.

**Abstract:** Large language models (LLMs) have shown remarkable reasoning capabilities, yet aligning such abilities to small language models (SLMs) remains a challenge due to distributional mismatches and limited model capacity. Existing reasoning datasets, typically designed for powerful LLMs, often lead to degraded performance when directly applied to weaker models. In this work, we introduce Dynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation framework that bridges the capability gap between expert reasoning trajectories and diverse SLMs. Instead of uniformly imitating expert steps, DART employs a selective imitation strategy guided by step-wise adaptability estimation via solution simulation. When expert steps surpass the student's capacity -- signaled by an Imitation Gap -- the student autonomously explores alternative reasoning paths, constrained by outcome consistency. We validate DART across multiple reasoning benchmarks and model scales, demonstrating that it significantly improves generalization and data efficiency over static fine-tuning. Our method enhances supervision quality by aligning training signals with the student's reasoning capabilities, offering a scalable solution for reasoning alignment in resource-constrained models.

</details>


### [73] [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)

*Nicy Scaria, Silvester John Joseph Kennedy, Diksha Seth, Deepak Subramani*

**Main category:** cs.CL

**Keywords:** Small Language Models, educational applications, physics reasoning, cultural contextualization, LLM evaluation

**Relevance Score:** 7

**TL;DR:** This study examines the reasoning capabilities of Small Language Models (SLMs) in high school physics, revealing strengths in answer accuracy but weaknesses in reasoning quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Exploring the potential of Small Language Models (SLMs) for educational use in physics while identifying their limitations in reasoning capabilities.

**Method:** Developed a comprehensive physics dataset from the OpenStax High School Physics textbook and evaluated SLMs using a novel cultural contextualization approach and an LLM-as-a-judge framework.

**Key Contributions:**

	1. Created a culturally contextualized physics dataset for SLM evaluation.
	2. Introduced LLM-as-a-judge framework for evaluating reasoning correctness.
	3. Identified significant gaps in reasoning quality of SLMs despite high answer accuracy.

**Result:** Qwen 3 1.7B achieved 85% answer accuracy, but only 38% in fully correct reasoning; performance varied across topics, and reasoning quality deteriorated with complexity.

**Limitations:** Study focused on a specific domain (high school physics) and may not generalize to other subjects or levels.

**Conclusion:** SLMs can provide correct answers but often rely on flawed reasoning; future development should focus on enhancing understanding and reasoning chains.

**Abstract:** Small Language Models (SLMs) offer computational efficiency and accessibility, making them promising for educational applications. However, their capacity for complex reasoning, particularly in domains such as physics, remains underexplored. This study investigates the high school physics reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters), including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series. We developed a comprehensive physics dataset from the OpenStax High School Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and plaintext mathematical notations. A novel cultural contextualization approach was applied to a subset, creating culturally adapted problems for Asian, African, and South American/Australian contexts while preserving core physics principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash, we evaluated answer and reasoning chain correctness, along with calculation accuracy. The results reveal significant differences between the SLMs. Qwen 3 1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was substantially low (38%). The format of the mathematical notation had a negligible impact on performance. SLMs exhibited varied performance across the physics topics and showed a decline in reasoning quality with increasing cognitive and knowledge complexity. In particular, the consistency of reasoning was largely maintained in diverse cultural contexts, especially by better performing models. These findings indicate that, while SLMs can often find correct answers, their underlying reasoning is frequently flawed, suggesting an overreliance on pattern recognition. For SLMs to become reliable educational tools in physics, future development must prioritize enhancing genuine understanding and the generation of sound, verifiable reasoning chains over mere answer accuracy.

</details>


### [74] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)

*Hanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, Wenjie Li*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Stepwise Progress Attribution, Intermediate Rewards, Agent Training, Multi-step Tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces Stepwise Progress Attribution (SPA), a novel framework for enhancing reinforcement learning (RL) agent training by providing intermediate rewards based on cumulative progress in multi-step tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of delayed rewards in reinforcement learning tasks, which complicate the training of agents by providing insufficient feedback signals during multi-step interactions.

**Method:** The authors propose a reward redistribution framework called Stepwise Progress Attribution (SPA), which decomposes final rewards into incremental contributions reflecting per-step progress toward task completion. A progress estimator is trained to accumulate these contributions, integrating them with grounding signals during policy optimization.

**Key Contributions:**

	1. Introduction of Stepwise Progress Attribution (SPA) framework for RL.
	2. Demonstration of improved agent performance in benchmarks through SPA.
	3. Effective intermediate rewards based on per-step progress.

**Result:** SPA consistently outperforms state-of-the-art methods on common agent benchmarks, achieving an average increase of 2.5% in success rates and 1.9% in grounding accuracy across experiments.

**Limitations:** 

**Conclusion:** The proposed method shows promise for providing more effective intermediate rewards for RL training, potentially improving the learning process and overall performance of RL agents in complex tasks.

**Abstract:** Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\% on average) and grounding accuracy (+1.9\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [75] [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/abs/2505.20738)

*Peiwen Yuan, Yiwei Li, Shaoxiong Feng, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

**Main category:** cs.CL

**Keywords:** LLM, self-bias, benchmarking, performance evaluation, AI

**Relevance Score:** 8

**TL;DR:** This paper introduces Silencer, a framework to reduce biases in LLM-generated benchmarks used for model evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of inflated performance results due to biases in models evaluated on self-generated benchmarks, termed self-bias.

**Method:** The proposed Silencer framework utilizes the diversity of multiple benchmark generators to mitigate self-bias at both the sample and benchmark levels.

**Key Contributions:**

	1. Definition and validation of self-bias in LLM evaluations
	2. Introduction of the Silencer framework for bias mitigation
	3. Demonstrated improvement in benchmark evaluation effectiveness

**Result:** The application of Silencer demonstrates a reduction of self-bias to near zero and an improvement in evaluation effectiveness of generated benchmarks, indicated by an increase in Pearson correlation from 0.655 to 0.833 compared with human-annotated benchmarks.

**Limitations:** 

**Conclusion:** Silencer effectively neutralizes self-bias and enhances the quality of model evaluations, showcasing strong generalizability across different scenarios.

**Abstract:** LLM-as-Benchmark-Generator methods have been widely studied as a supplement to human annotators for scalable evaluation, while the potential biases within this paradigm remain underexplored. In this work, we systematically define and validate the phenomenon of inflated performance in models evaluated on their self-generated benchmarks, referred to as self-bias, and attribute it to sub-biases arising from question domain, language style, and wrong labels. On this basis, we propose Silencer, a general framework that leverages the heterogeneity between multiple generators at both the sample and benchmark levels to neutralize bias and generate high-quality, self-bias-silenced benchmark. Experimental results across various settings demonstrate that Silencer can suppress self-bias to near zero, significantly improve evaluation effectiveness of the generated benchmark (with an average improvement from 0.655 to 0.833 in Pearson correlation with high-quality human-annotated benchmark), while also exhibiting strong generalizability.

</details>


### [76] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)

*Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie*

**Main category:** cs.CL

**Keywords:** faithfulness hallucination, Large Language Models, cognitive statements

**Relevance Score:** 8

**TL;DR:** This paper addresses faithfulness hallucination in Large Language Models (LLMs) through a new framework and benchmark dataset, CogniBench-L, for assessing cognitive statements and training detection models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issue of faithfulness hallucination in LLMs by providing a clearer assessment methodology for cognitive statements.

**Method:** The authors designed a rigorous framework inspired by legislative evidence assessment and created a benchmark dataset, CogniBench-L, complemented by an annotation pipeline for large-scale benchmarks.

**Key Contributions:**

	1. A rigorous framework for assessing the faithfulness of cognitive statements in LLMs.
	2. Development of the CogniBench-L benchmark dataset for facilitating research in cognitive hallucination detection.
	3. An annotation pipeline that enables the automatic creation of large-scale benchmarks for LLM evaluation.

**Result:** The paper presents a new dataset that reveals insightful statistics about cognitive statements' faithfulness and can be used to train models that detect cognitive hallucinations in LLMs.

**Limitations:** 

**Conclusion:** The proposed framework and dataset aim to improve the evaluation and optimization of cognitive statements generated by LLMs.

**Abstract:** Faithfulness hallucination are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standard, existing benchmarks only contain "factual statements" that rephrase source materials without marking "cognitive statements" that make inference from the given context, making the consistency evaluation and optimization of cognitive statements difficult. Inspired by how an evidence is assessed in the legislative domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and create a benchmark dataset where we reveal insightful statistics. We design an annotation pipeline to create larger benchmarks for different LLMs automatically, and the resulting larger-scale CogniBench-L dataset can be used to train accurate cognitive hallucination detection model. We release our model and dataset at: https://github.com/FUTUREEEEEE/CogniBench

</details>


### [77] [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)

*Jungyoub Cha, Hyunjong Kim, Sungzoon Cho*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, efficient attention mechanisms, long sequences, cross-model retrieval

**Relevance Score:** 8

**TL;DR:** SpecExtend enhances speculative decoding for long sequences in LLMs by integrating efficient attention mechanisms and a novel KV cache update strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance degradation of speculative decoding in LLMs on long inputs due to increased attention cost and reduced draft accuracy.

**Method:** SpecExtend implements efficient attention mechanisms, including FlashAttention and Hybrid Tree Attention, while introducing Cross-model Retrieval to optimize KV cache updates based on attention scores.

**Key Contributions:**

	1. Introduction of SpecExtend as a drop-in enhancement for speculative decoding.
	2. Integration of efficient attention mechanisms for performance improvement.
	3. Proposition of Cross-model Retrieval for dynamic context selection.

**Result:** SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for input sizes up to 16K tokens, demonstrating its effectiveness on long-context understanding datasets.

**Limitations:** 

**Conclusion:** SpecExtend provides a significant improvement in the speed and accuracy of speculative decoding for long sequences without needing additional training.

**Abstract:** Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models, reducing latency across all stages. To improve draft accuracy and speed, we propose Cross-model Retrieval, a novel KV cache update strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. The code is available at https://github.com/jycha98/SpecExtend .

</details>


### [78] [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)

*Noy Sternlicht, Tom Hope*

**Main category:** cs.CL

**Keywords:** recombination, knowledge base, machine learning, hypothesis generation, scientific literature

**Relevance Score:** 8

**TL;DR:** CHIMERA is a large-scale knowledge base for mining scientific recombination examples to enhance creativity in research by integrating concepts from different domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how scientists create new ideas by recombining existing concepts and mechanisms.

**Method:** The paper presents a novel task for extracting recombination from scientific abstracts, collects a manually annotated dataset, and trains an LLM-based extraction model on a large corpus of AI papers.

**Key Contributions:**

	1. Development of a large-scale KB for recombination examples
	2. Introduction of a novel information extraction task
	3. Training of a model to predict new creative directions

**Result:** CHIMERA contains over 28K examples of recombination and provides insights into properties of recombination in various AI subfields.

**Limitations:** 

**Conclusion:** The research demonstrates how CHIMERA can be used to inspire new research directions and supports the hypothesis generation in science.

**Abstract:** A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA

</details>


### [79] [Improved Representation Steering for Language Models](https://arxiv.org/abs/2505.20809)

*Zhengxuan Wu, Qinan Yu, Aryaman Arora, Christopher D. Manning, Christopher Potts*

**Main category:** cs.CL

**Keywords:** language models, steering, preference optimization, interpretability, prompting

**Relevance Score:** 9

**TL;DR:** This paper introduces Reference-free Preference Steering (RePS), a novel method for steering language models that improves both concept steering and suppression compared to existing methods and prompting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective steering methods in language models that provide interpretable control over model generations without relying on traditional prompting techniques.

**Method:** RePS is a bidirectional preference-optimization objective that adjusts model behavior by steering and suppressing concepts through a more interpretable framework.

**Key Contributions:**

	1. Introduction of a new preference-optimization method for steering language models.
	2. Demonstrated improved performance on concept steering and suppression tasks.
	3. Enhanced interpretability and reduced parameter requirements compared to existing methods.

**Result:** RePS outperforms existing steering methods in a large-scale benchmark, achieving results comparable to prompting while maintaining a lower parameter count and being resilient to prompt-based attacks.

**Limitations:** 

**Conclusion:** RePS offers an interpretable and robust alternative to existing prompting strategies for steering language models.

**Abstract:** Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression.

</details>


### [80] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)

*Junsik Kim, Jinwook Park, Kangil Kim*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Embedding, Entity Transformation, Relation-Semantics Consistency

**Relevance Score:** 6

**TL;DR:** This paper introduces the Relation-Semantics Consistent Filter (RSCF) method for knowledge graph embedding, which improves consistency in entity-transformation and enhances performance in knowledge graph completion tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for consistent embedding differences in relation-specific entity-transformation methods to retain inductive biases in knowledge graphs.

**Method:** The proposed RSCF method employs shared affine transformations across relations, rooted entity-transformations, and normalization to ensure consistency in embeddings while integrating additional relation transformation and prediction modules.

**Key Contributions:**

	1. Introduction of the Relation-Semantics Consistent Filter (RSCF) method
	2. Improvement of consistency in entity-transformation
	3. Demonstrated superior performance over existing KGE methods

**Result:** RSCF outperforms state-of-the-art knowledge graph embedding methods in various tasks, demonstrating robustness across different relation types and frequencies.

**Limitations:** 

**Conclusion:** The RSCF method provides enhanced consistency and semantics in knowledge graph embeddings, leading to improved completion performance.

**Abstract:** In knowledge graph embedding, leveraging relation-specific entity-transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity-embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF), containing more consistent entity-transformation characterized by three features: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity-transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.

</details>


### [81] [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/abs/2505.20816)

*Krishna Singh Rajput, Tejas Anvekar, Chitta Baral, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** multimodal question answering, multi-agent framework, cross-modal reasoning

**Relevance Score:** 8

**TL;DR:** MAMMQA is a multi-agent framework for multimodal question answering that improves accuracy and interpretability by using specialized agents for text, images, and tables.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of current multimodal question answering approaches that rely on generalized reasoning strategies and do not leverage unique modal characteristics.

**Method:** A framework with two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent to decompose queries, retrieve partial answers, and synthesize results through cross-modal reasoning.

**Key Contributions:**

	1. Development of a multi-agent QA framework for multimodal inputs.
	2. Enhanced interpretability through transparent reasoning processes.
	3. Improved performance on multimodal QA benchmarks compared to existing methods.

**Result:** MAMMQA consistently outperforms existing baselines in terms of accuracy and robustness across diverse multimodal QA benchmarks.

**Limitations:** 

**Conclusion:** The modular design of MAMMQA enhances interpretability and allows each agent to focus on its domain expertise, leading to improved performance in multimodal question answering tasks.

**Abstract:** Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.

</details>


### [82] [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/abs/2505.20819)

*Paul Youssef, Zhixue Zhao, Christin Seifert, JÃ¶rg SchlÃ¶tterer*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, adversarial detection, model editing

**Relevance Score:** 8

**TL;DR:** This work investigates the traceability and reversibility of knowledge editing in LLMs, focusing on the ROME method, and proposes techniques to detect and reverse adversarial edits.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To defend against the malicious use of knowledge editing methods that can implant misinformation or bias in large language models.

**Method:** The study analyzes the distributional patterns in the weight matrices of the ROME method to locate edited weights, predict modified facts, and reverse edits.

**Key Contributions:**

	1. Identification of unique distributional patterns in ROME weight matrices
	2. Development of a method for accurate prediction of edited factual relations
	3. Demonstration of effective reversal of knowledge edits

**Result:** Achieved over 95% accuracy in inferring edited entities and over 80% accuracy in recovering original outputs after reversing edits.

**Limitations:** 

**Conclusion:** The findings provide a robust framework for detecting, tracing, and reversing knowledge edits in LLMs, enhancing their resistance against adversarial manipulations.

**Abstract:** Knowledge editing methods (KEs) are a cost-effective way to update the factual content of large language models (LLMs), but they pose a dual-use risk. While KEs are beneficial for updating outdated or incorrect information, they can be exploited maliciously to implant misinformation or bias. In order to defend against these types of malicious manipulation, we need robust techniques that can reliably detect, interpret, and mitigate adversarial edits. This work investigates the traceability and reversibility of knowledge edits, focusing on the widely used Rank-One Model Editing (ROME) method. We first show that ROME introduces distinctive distributional patterns in the edited weight matrices, which can serve as effective signals for locating the edited weights. Second, we show that these altered weights can reliably be used to predict the edited factual relation, enabling partial reconstruction of the modified fact. Building on this, we propose a method to infer the edited object entity directly from the modified weights, without access to the editing prompt, achieving over 95% accuracy. Finally, we demonstrate that ROME edits can be reversed, recovering the model's original outputs with $\geq$ 80% accuracy. Our findings highlight the feasibility of detecting, tracing, and reversing edits based on the edited weights, offering a robust framework for safeguarding LLMs against adversarial manipulations.

</details>


### [83] [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/abs/2505.20825)

*Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** long-form question answering, reinforcement learning, retrieval-augmented generation, natural language processing, evaluation metrics

**Relevance Score:** 9

**TL;DR:** This paper presents RioRAG, a novel reinforcement learning framework for improving long-form question answering in large language models by optimizing informativeness and using a hierarchical reward system.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing long-form question answering systems face challenges such as lack of quality data, hallucination risks, and poor evaluation metrics.

**Method:** The proposed RioRAG utilizes reinforced informativeness optimization through a new RL training paradigm and a nugget-centric hierarchical reward model to assess factual alignment in long-form answers.

**Key Contributions:**

	1. Introduction of a RL training paradigm for informativeness optimization
	2. Nugget-centric hierarchical reward modeling for assessing answers
	3. Demonstrated effectiveness on LFQA benchmarks LongFact and RAGChecker

**Result:** Extensive experiments on LFQA benchmarks show that RioRAG effectively improves informativeness and factual accuracy compared to traditional methods.

**Limitations:** 

**Conclusion:** RioRAG represents a significant advancement in long-form question answering, showcasing both practical improvements and theoretical insights into the optimization process.

**Abstract:** Long-form question answering (LFQA) presents unique challenges for large language models, requiring the synthesis of coherent, paragraph-length answers. While retrieval-augmented generation (RAG) systems have emerged as a promising solution, existing research struggles with key limitations: the scarcity of high-quality training data for long-form generation, the compounding risk of hallucination in extended outputs, and the absence of reliable evaluation metrics for factual completeness. In this paper, we propose RioRAG, a novel reinforcement learning (RL) framework that advances long-form RAG through reinforced informativeness optimization. Our approach introduces two fundamental innovations to address the core challenges. First, we develop an RL training paradigm of reinforced informativeness optimization that directly optimizes informativeness and effectively addresses the slow-thinking deficit in conventional RAG systems, bypassing the need for expensive supervised data. Second, we propose a nugget-centric hierarchical reward modeling approach that enables precise assessment of long-form answers through a three-stage process: extracting the nugget from every source webpage, constructing a nugget claim checklist, and computing rewards based on factual alignment. Extensive experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the effectiveness of the proposed method. Our codes are available at https://github.com/RUCAIBox/RioRAG.

</details>


### [84] [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/abs/2505.20826)

*Soichiro Murakami, Peinan Zhang, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura*

**Main category:** cs.CL

**Keywords:** ad text, paraphrasing, advertising, linguistic analysis, human preference

**Relevance Score:** 4

**TL;DR:** This study introduces AdParaphrase v2.0, a large dataset for ad text paraphrasing aimed at enhancing ad attractiveness through linguistic analysis.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to identify factors that contribute to the attractiveness of ad text, which is critical for successful advertising.

**Method:** Developed a dataset, AdParaphrase v2.0, consisting of 16,460 annotated ad text paraphrase pairs derived from human preference data, and conducted experiments to analyze linguistic features of engaging ad texts.

**Key Contributions:**

	1. Introduction of a significantly larger dataset for ad text paraphrasing
	2. Identification of new linguistic features contributing to ad engagement
	3. Validation of reference-free metrics using large language models for ad evaluation

**Result:** Identified new linguistic features linked to engaging ad texts and established connections between human preference and ad performance; demonstrated the utility of reference-free metrics from large language models for evaluating ad attractiveness.

**Limitations:** 

**Conclusion:** The study suggests that AdParaphrase v2.0 can advance the understanding of ad text attractiveness and supports the development of better ad generation methods.

**Abstract:** Identifying factors that make ad text attractive is essential for advertising success. This study proposes AdParaphrase v2.0, a dataset for ad text paraphrasing, containing human preference data, to enable the analysis of the linguistic factors and to support the development of methods for generating attractive ad texts. Compared with v1.0, this dataset is 20 times larger, comprising 16,460 ad text paraphrase pairs, each annotated with preference data from ten evaluators, thereby enabling a more comprehensive and reliable analysis. Through the experiments, we identified multiple linguistic features of engaging ad texts that were not observed in v1.0 and explored various methods for generating attractive ad texts. Furthermore, our analysis demonstrated the relationships between human preference and ad performance, and highlighted the potential of reference-free metrics based on large language models for evaluating ad text attractiveness. The dataset is publicly available at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.

</details>


### [85] [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)

*Xinbo Wu, Abhishek Umrawal, Lav R. Varshney*

**Main category:** cs.CL

**Keywords:** large language models, adversarial prompting, game theory, defense mechanisms

**Relevance Score:** 8

**TL;DR:** This paper presents a new attack strategy on large language models involving intent-hiding adversarial prompting and proposes a defense mechanism against it.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing capabilities of large language models, there are growing concerns about their potential misuse, prompting the need for better defense mechanisms against adversarial prompts.

**Method:** A game-theoretic framework is developed to model the interaction between intent-hiding adversarial prompting and filtering defense systems. The framework identifies equilibrium points and structural advantages for attackers.

**Key Contributions:**

	1. Introduction of intent-hiding adversarial prompting as a scalable attack strategy.
	2. Development of a game-theoretic framework for modeling attack-defense interactions.
	3. Proposal of a specific defense mechanism against intent-hiding attacks.

**Result:** The study empirically demonstrates the effectiveness of the intent-hiding attack across multiple real-world LLMs, showcasing its supremacy over existing adversarial prompting techniques.

**Limitations:** The paper primarily focuses on adversarial prompting and does not address potential defenses against other types of attacks.

**Conclusion:** A defense mechanism tailored for intent-hiding attacks is proposed and analyzed, addressing the vulnerabilities of current methods.

**Abstract:** As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.

</details>


### [86] [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/abs/2505.20871)

*Xin Sun, Jianan Xie, Zhongqi Chen, Qiang Liu, Shu Wu, Yuehe Chen, Bowen Song, Weiqiang Wang, Zilei Wang, Liang Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Models, Divide-Then-Align, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces Divide-Then-Align (DTA), a method that improves the reliability of retrieval-augmented systems by enabling them to respond with 'I don't know' when beyond their knowledge limits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability of Retrieval-Augmented Models (RAG) in high-stakes environments by allowing them to properly decline answering when unsure.

**Method:** The method divides data samples into four knowledge quadrants, creating a specialized dataset for Direct Preference Optimization (DPO) to guide responses.

**Key Contributions:**

	1. Introduction of Divide-Then-Align (DTA) for RAG systems
	2. Development of a new dataset for Direct Preference Optimization
	3. Demonstrated improvements in accuracy and reliability through experimental results

**Result:** Experimental results indicate that DTA effectively balances the accuracy of responses while allowing the model to abstain when uncertain, showcasing improvements in trustworthiness.

**Limitations:** 

**Conclusion:** DTA enhances the robustness of retrieval-augmented systems by instilling a systematic approach to acknowledge uncertainty, crucial for high-stakes applications.

**Abstract:** Large language models (LLMs) augmented with retrieval systems have significantly advanced natural language processing tasks by integrating external knowledge sources, enabling more accurate and contextually rich responses. To improve the robustness of such systems against noisy retrievals, Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method. However, RAFT conditions models to generate answers even in the absence of reliable knowledge. This behavior undermines their reliability in high-stakes domains, where acknowledging uncertainty is critical. To address this issue, we propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG systems with the ability to respond with "I don't know" when the query is out of the knowledge boundary of both the retrieved passages and the model's internal knowledge. DTA divides data samples into four knowledge quadrants and constructs tailored preference data for each quadrant, resulting in a curated dataset for Direct Preference Optimization (DPO). Experimental results on three benchmark datasets demonstrate that DTA effectively balances accuracy with appropriate abstention, enhancing the reliability and trustworthiness of retrieval-augmented systems.

</details>


### [87] [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/abs/2505.20874)

*Sirui Xia, Aili Chen, Xintao Wang, Tinghui Zhu, Yikai Zhang, Jiangjie Chen, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** Large Language Models, spatial cognition, navigation, trajectory data, urban environment

**Relevance Score:** 9

**TL;DR:** This study explores how Large Language Models (LLMs) can integrate spatial knowledge to understand global spatial layouts and improve navigation tasks through local observations and trajectory data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The potential of LLMs in understanding structured spatial knowledge is largely unexplored, especially in relation to human spatial perception and navigation.

**Method:** The paper conducts experiments in a simulated urban environment to test LLMs on spatial perception and navigation tasks using relational descriptions and trajectory data.

**Key Contributions:**

	1. Investigation of LLMs in spatial cognition
	2. Demonstration of generalization to unseen spatial relationships
	3. Ability to learn road connectivity for path planning

**Result:** LLMs were able to generalize spatial relationships between points of interest and demonstrated the ability to learn road connectivity from trajectory descriptions, allowing for accurate path planning.

**Limitations:** The experiments are limited to simulated environments and may not fully capture real-world complexities.

**Conclusion:** LLMs can construct coherent global spatial cognition and maintain dynamic spatial awareness, which enhances their utility in navigation tasks.

**Abstract:** Recent advances in Large Language Models (LLMs) have demonstrated strong capabilities in tasks such as code and mathematics. However, their potential to internalize structured spatial knowledge remains underexplored. This study investigates whether LLMs, grounded in locally relative human observations, can construct coherent global spatial cognition by integrating fragmented relational descriptions. We focus on two core aspects of spatial cognition: spatial perception, where models infer consistent global layouts from local positional relationships, and spatial navigation, where models learn road connectivity from trajectory data and plan optimal paths between unconnected locations. Experiments conducted in a simulated urban environment demonstrate that LLMs not only generalize to unseen spatial relationships between points of interest (POIs) but also exhibit latent representations aligned with real-world spatial distributions. Furthermore, LLMs can learn road connectivity from trajectory descriptions, enabling accurate path planning and dynamic spatial awareness during navigation.

</details>


### [88] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)

*Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi*

**Main category:** cs.CL

**Keywords:** Language Models, English Varieties, Linguistic Robustness, Fairness, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces Trans-EnV, a framework for evaluating LLMs on various English varieties, revealing significant performance disparities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address fairness concerns in LLM performance across different English varieties, which are often overlooked in standard evaluations.

**Method:** Trans-EnV transforms SAE datasets into multiple English varieties using linguistic expert knowledge and LLM-based transformations to evaluate robustness.

**Key Contributions:**

	1. Introduction of the Trans-EnV framework for linguistic robustness evaluation across English varieties.
	2. Transformation of six benchmark datasets into 38 English varieties.
	3. Validation of results through statistical testing and expert consultation.

**Result:** Significant performance disparities were found, with accuracy decreasing by up to 46.3% on non-standard varieties across seven LLMs evaluated.

**Limitations:** 

**Conclusion:** Comprehensive evaluations across diverse English varieties are crucial for understanding LLM performance and ensuring fairness for global users.

**Abstract:** Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our \href{https://github.com/jiyounglee-0523/TransEnV}{code} and \href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets} are publicly available.

</details>


### [89] [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/abs/2505.20880)

*Baraa Hikal, Ahmed Nasreldin, Ali Hamdi*

**Main category:** cs.CL

**Keywords:** hallucination detection, Large Language Models, multilingual, ensemble verification, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper presents a submission for the SemEval-2025 Task 3 focused on detecting hallucinations in text generated by instruction-tuned LLMs across languages, utilizing an ensemble verification mechanism with a strong ranking in various languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hallucinations in text generated by instruction-tuned LLMs, the paper develops a method to automatically detect these errors across multiple languages.

**Method:** The method utilizes task-specific prompt engineering combined with an LLM ensemble that verifies hallucinated spans through probability-based voting from multiple models, simulating human annotation processes.

**Key Contributions:**

	1. Development of a novel ensemble verification mechanism for detecting hallucinations in LLM outputs.
	2. First-place performance in multiple languages, showcasing the method's effectiveness.
	3. Simulation of human annotation workflows to enhance detection accuracy.

**Result:** The system achieved 1st place in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French in the shared task rankings.

**Limitations:** 

**Conclusion:** The framework demonstrates effective detection of hallucinations in generated texts and shows promise in multilingual settings.

**Abstract:** This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The task involves detecting hallucinated spans in text generated by instruction-tuned Large Language Models (LLMs) across multiple languages. Our approach combines task-specific prompt engineering with an LLM ensemble verification mechanism, where a primary model extracts hallucination spans and three independent LLMs adjudicate their validity through probability-based voting. This framework simulates the human annotation workflow used in the shared task validation and test data. Additionally, fuzzy matching refines span alignment. Our system ranked 1st in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.

</details>


### [90] [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)

*Chengyu Wang, Junbing Yan, Wenrui Cai, Yuanhao Yue, Jun Huang*

**Main category:** cs.CL

**Keywords:** knowledge distillation, large language models, toolkit, NLP, Alibaba Cloud

**Relevance Score:** 9

**TL;DR:** EasyDistill is a toolkit for knowledge distillation of large language models, offering functionalities like data synthesis and fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To make advanced knowledge distillation techniques for large language models more accessible and impactful in the NLP community.

**Method:** The framework includes features for data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning, designed for both fast and analytical models.

**Key Contributions:**

	1. Comprehensive toolkit for black-box and white-box knowledge distillation of LLMs.
	2. Modular design that supports various KD functionalities and use cases.
	3. Integration with Alibaba Cloud's Platform for AI, enabling wider accessibility.

**Result:** EasyDistill provides a user-friendly interface and robust distilled models along with open-sourced datasets for various applications.

**Limitations:** 

**Conclusion:** The toolkit facilitates seamless experimentation and implementation of state-of-the-art knowledge distillation strategies, integrating with Alibaba Cloud's AI platform.

**Abstract:** In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.

</details>


### [91] [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/abs/2505.20899)

*Jeongsoo Choi, Jaehun Kim, Joon Son Chung*

**Main category:** cs.CL

**Keywords:** cross-lingual dubbing, speech translation, discrete diffusion model

**Relevance Score:** 5

**TL;DR:** The paper presents a cross-lingual dubbing system that translates speech while maintaining characteristics like duration and speaker identity using a novel model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing speech translation methods often fail to preserve speech patterns during translation, which limits their application in dubbing.

**Method:** A discrete diffusion-based speech-to-unit translation model with explicit duration control is proposed, supplemented by a conditional flow matching model for synthesizing speech and a unit-based speed adaptation mechanism.

**Key Contributions:**

	1. Introduction of a discrete diffusion-based speech-to-unit translation model.
	2. Explicit duration control for time-aligned translation.
	3. Unit-based speed adaptation mechanism for consistent speech rate.

**Result:** The proposed system generates natural translations that align with the original speech's duration and speaking pace while achieving competitive translation performance.

**Limitations:** 

**Conclusion:** The framework effectively addresses the limitations of traditional speech translation methods, making it suitable for dubbing applications.

**Abstract:** This paper introduces a cross-lingual dubbing system that translates speech from one language to another while preserving key characteristics such as duration, speaker identity, and speaking speed. Despite the strong translation quality of existing speech translation approaches, they often overlook the transfer of speech patterns, leading to mismatches with source speech and limiting their suitability for dubbing applications. To address this, we propose a discrete diffusion-based speech-to-unit translation model with explicit duration control, enabling time-aligned translation. We then synthesize speech based on the predicted units and source identity with a conditional flow matching model. Additionally, we introduce a unit-based speed adaptation mechanism that guides the translation model to produce speech at a rate consistent with the source, without relying on any text. Extensive experiments demonstrate that our framework generates natural and fluent translations that align with the original speech's duration and speaking pace, while achieving competitive translation performance.

</details>


### [92] [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)

*Junhyuk Choi, Minju Kim, Yeseon Hong, Bugeun Kim*

**Main category:** cs.CL

**Keywords:** vision language models, stereotypes, social biases, evaluation metrics, BASIC

**Relevance Score:** 8

**TL;DR:** This study introduces new evaluation metrics and a benchmark called BASIC to assess stereotypes in large vision language models (LVLMs), revealing interactions between model characteristics and stereotype representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing measurements and datasets regarding social biases and stereotypes in LVLMs, particularly the oversight of content words and color effects.

**Method:** The study employs new metrics based on the Stereotype Content Model (SCM) and introduces BASIC, a benchmark for evaluating gender, race, and color stereotypes across eight LVLMs.

**Key Contributions:**

	1. Development of new SCM-based evaluation metrics for stereotypes
	2. Introduction of BASIC benchmark for assessing stereotypes in LVLMs
	3. Identification of interaction effects between model architecture and stereotype expression

**Result:** The SCM-based evaluation effectively captures stereotypes; LVLMs show color stereotypes along with gender and race biases; the interaction between model architecture and parameter sizes influences these stereotypes.

**Limitations:** 

**Conclusion:** The findings emphasize the need for improved measures in assessing biases in LVLMs and the importance of considering color in such evaluations.

**Abstract:** As large vision language models(LVLMs) rapidly advance, concerns about their potential to learn and generate social biases and stereotypes are increasing. Previous studies on LVLM's stereotypes face two primary limitations: metrics that overlooked the importance of content words, and datasets that overlooked the effect of color. To address these limitations, this study introduces new evaluation metrics based on the Stereotype Content Model (SCM). We also propose BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes. As a result, we found three findings. (1) The SCM-based evaluation is effective in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output along with gender and race ones. (3) Interaction between model architecture and parameter sizes seems to affect stereotypes. We release BASIC publicly on [anonymized for review].

</details>


### [93] [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/abs/2505.20903)

*Ziming Wang, Zeyu Shi, Haoyi Zhou, Shiqi Gao, Qingyun Sun, Jianxin Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Calibration, CogCalib, Fine-tuning, Human-AI Interaction

**Relevance Score:** 9

**TL;DR:** This paper addresses the poor calibration of fine-tuned Large Language Models (LLMs) due to prior knowledge, proposing a new framework called CogCalib to improve calibration without sacrificing performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often have misaligned confidence scores, affecting their reliability, particularly in domains requiring precise human-AI interaction.

**Method:** The authors propose CogCalib, which employs targeted learning strategies based on the model's prior knowledge to enhance calibration during fine-tuning.

**Key Contributions:**

	1. Introduction of the CogCalib framework for cognition-aware calibration
	2. Demonstration of significant ECE reduction in multiple tasks
	3. Improvement in out-of-domain task performance

**Result:** Experiments show CogCalib reduces Expected Calibration Error (ECE) by an average of 57% across tasks using Llama3-8B, while maintaining performance.

**Limitations:** 

**Conclusion:** The proposed method enhances the reliability of LLMs in human-AI interactions by improving calibration while leveraging the models' existing knowledge.

**Abstract:** Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration, with their confidence scores misaligned with actual performance. While calibration has been extensively studied in models trained from scratch, the impact of LLMs' prior knowledge on calibration during fine-tuning remains understudied. Our research reveals that LLMs' prior knowledge causes potential poor calibration due to the ubiquitous presence of known data in real-world fine-tuning, which appears harmful for calibration. Specifically, data aligned with LLMs' prior knowledge would induce overconfidence, while new knowledge improves calibration. Our findings expose a tension: LLMs' encyclopedic knowledge, while enabling task versatility, undermines calibration through unavoidable knowledge overlaps. To address this, we propose CogCalib, a cognition-aware framework that applies targeted learning strategies according to the model's prior knowledge. Experiments across 7 tasks using 3 LLM families prove that CogCalib significantly improves calibration while maintaining performance, achieving an average 57\% reduction in ECE compared to standard fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain tasks, enhancing the objectivity and reliability of domain-specific LLMs, and making them more trustworthy for critical human-AI interaction applications.

</details>


### [94] [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)

*Hang Zeng, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Shaojie Tang, Guihai Chen*

**Main category:** cs.CL

**Keywords:** privacy detection, large language models, user queries

**Relevance Score:** 8

**TL;DR:** This paper addresses the privacy risks users face when interacting with large language models (LLMs) by developing a dataset and evaluation metrics for privacy detection in LLM queries.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Users risk disclosing private information while interacting with LLMs under real identities, highlighting the need for effective privacy detection.

**Method:** Constructed a large multilingual dataset with 249K user queries and 154K annotated privacy phrases; developed an automated annotation pipeline and established evaluation metrics for privacy leakage.

**Key Contributions:**

	1. Large-scale multilingual dataset for LLM privacy detection
	2. Automated privacy annotation pipeline
	3. Evaluation metrics for privacy leakage and phrase extraction

**Result:** Baseline methods were established using light-weight LLMs; the evaluation revealed a performance gap in current privacy detection systems compared to real-world requirements.

**Limitations:** The gap in performance indicates that more effective methods are necessary for practical applications.

**Conclusion:** The study emphasizes the need for improved local privacy detection methods and sets a foundation for future research using the provided dataset.

**Abstract:** Users interacting with large language models (LLMs) under their real identifiers often unknowingly risk disclosing private information. Automatically notifying users whether their queries leak privacy and which phrases leak what private information has therefore become a practical need. Existing privacy detection methods, however, were designed for different objectives and application scenarios, typically tagging personally identifiable information (PII) in anonymous content. In this work, to support the development and evaluation of privacy detection models for LLM interactions that are deployable on local user devices, we construct a large-scale multilingual dataset with 249K user queries and 154K annotated privacy phrases. In particular, we build an automated privacy annotation pipeline with cloud-based strong LLMs to automatically extract privacy phrases from dialogue datasets and annotate leaked information. We also design evaluation metrics at the levels of privacy leakage, extracted privacy phrase, and privacy information. We further establish baseline methods using light-weight LLMs with both tuning-free and tuning-based methods, and report a comprehensive evaluation of their performance. Evaluation results reveal a gap between current performance and the requirements of real-world LLM applications, motivating future research into more effective local privacy detection methods grounded in our dataset.

</details>


### [95] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)

*Injae Na, Keonwoong Noh, Woohwan Jung*

**Main category:** cs.CL

**Keywords:** LLM, Automatic Transmission, NLP, Cost-Performance Optimization, Accuracy Estimator

**Relevance Score:** 9

**TL;DR:** The LLM Automatic Transmission (LLM-AT) framework automates the selection of suitable LLM tiers for NLP tasks, balancing cost and performance without training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** As NLP tasks grow in complexity, selecting the appropriate LLM tier for subtasks has become crucial for optimizing costs and performance.

**Method:** LLM-AT employs a three-component system: a Starter selects an initial LLM tier, a Generator produces responses, and a Judge evaluates those responses to ensure validity. If a response is invalid, the tier is upgraded and re-evaluated until a valid response is achieved. An accuracy estimator helps in selecting the appropriate initial tier based on past inference records.

**Key Contributions:**

	1. Introduction of LLM Automatic Transmission (LLM-AT) for automatic tier selection
	2. Incorporation of an accuracy estimator for initial tier selection
	3. Demonstration of superior performance and cost reduction in LLM usage

**Result:** LLM-AT demonstrates enhanced performance in generating valid responses while significantly reducing costs in comparison to traditional approaches.

**Limitations:** 

**Conclusion:** The framework presents a practical solution for effectively selecting LLM tiers in real-world applications, making it both cost-efficient and robust for various NLP tasks.

**Abstract:** LLM providers typically offer multiple LLM tiers, varying in performance and price. As NLP tasks become more complex and modularized, selecting the suitable LLM tier for each subtask is a key challenge to balance between cost and performance. To address the problem, we introduce LLM Automatic Transmission (LLM-AT) framework that automatically selects LLM tiers without training. LLM-AT consists of Starter, Generator, and Judge. The starter selects the initial LLM tier expected to solve the given question, the generator produces a response using the LLM of the selected tier, and the judge evaluates the validity of the response. If the response is invalid, LLM-AT iteratively upgrades to a higher-tier model, generates a new response, and re-evaluates until a valid response is obtained. Additionally, we propose accuracy estimator, which enables the suitable initial LLM tier selection without training. Given an input question, accuracy estimator estimates the expected accuracy of each LLM tier by computing the valid response rate across top-k similar queries from past inference records. Experiments demonstrate that LLM-AT achieves superior performance while reducing costs, making it a practical solution for real-world applications.

</details>


### [96] [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)

*Zhuo Li, Guodong Du, Weiyang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, Jing Li*

**Main category:** cs.CL

**Keywords:** large language models, alignment, Hierarchical Mixture-of-Experts, parameter-efficient, machine learning

**Relevance Score:** 8

**TL;DR:** The paper presents HoE, a lightweight and parameter-efficient approach for aligning large language models to diverse human preferences without retraining.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of aligning LLMs to effectively balance multiple and often conflicting human preferences.

**Method:** Introducing a Hierarchical Mixture-of-Experts (HoE) approach that consists of three components: LoRA Experts, Router Experts, and Preference Routing, allowing adaptation across the entire Pareto frontier without model training.

**Key Contributions:**

	1. Introduction of a Hierarchical Mixture-of-Experts framework for LLM alignment.
	2. Demonstration of superior performance on multiple objectives and preferences without retraining.
	3. Framework is lightweight and parameter-efficient, optimized for diverse user preferences.

**Result:** HoE was evaluated on 14 objectives and 200 different preferences across 6 benchmarks, showing superior performance compared to 15 recent baselines.

**Limitations:** 

**Conclusion:** HoE effectively reaches optimal Pareto frontiers while balancing parameter size, training cost, and performance, providing a new strategy for LLM alignment.

**Abstract:** Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a \textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play} approach that eliminates the need for model training, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, \textit{HoE} consists of three hierarchical components: LoRA Experts, Router Experts and Preference Routing, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate \textit{HoE} across various tasks on 14 objectives and 200 different preferences among 6 benchmarks, demonstrating superior performance over 15 recent baselines. Code is available in the supplementary materials.

</details>


### [97] [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/abs/2505.20933)

*Duzhen Zhang, Yong Ren, Chenxing Li, Dong Yu, Tielin Zhang*

**Main category:** cs.CL

**Keywords:** Continual Learning, Text Classification, Machine Learning, Human-Computer Interaction, Information Theory

**Relevance Score:** 6

**TL;DR:** This paper introduces InfoComp, a novel approach for Continual Text Classification that minimizes catastrophic forgetting by learning task-specific and task-invariant knowledge through distinct prompt spaces and an information-theoretic framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of Continual Text Classification (CTC) which suffers from catastrophic forgetting, particularly regarding task-invariant knowledge.

**Method:** The approach utilizes two distinct prompt spaces, P-Prompt for task-specific knowledge and S-Prompt for task-invariant knowledge, and employs novel loss functions to maximize mutual information between parameters while learning.

**Key Contributions:**

	1. Introduction of two distinct prompt spaces for task-specific and task-invariant knowledge in CTC.
	2. Development of an information-theoretic framework to optimize learning across tasks.
	3. Novel loss functions designed to mitigate catastrophic forgetting and enhance knowledge retention.

**Result:** Extensive experiments demonstrate that InfoComp significantly outperforms previous state-of-the-art methods in CTC benchmarks, indicating effective retention of both task-specific and task-invariant knowledge.

**Limitations:** 

**Conclusion:** InfoComp provides a robust framework for CTC, allowing models to learn new classification tasks effectively while preserving previously acquired knowledge.

**Abstract:** Continual Text Classification (CTC) aims to continuously classify new text data over time while minimizing catastrophic forgetting of previously acquired knowledge. However, existing methods often focus on task-specific knowledge, overlooking the importance of shared, task-agnostic knowledge. Inspired by the complementary learning systems theory, which posits that humans learn continually through the interaction of two systems -- the hippocampus, responsible for forming distinct representations of specific experiences, and the neocortex, which extracts more general and transferable representations from past experiences -- we introduce Information-Theoretic Complementary Prompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two distinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These respectively encode task-specific and task-invariant knowledge, enabling models to sequentially learn classification tasks without relying on data replay. To promote more informative prompt learning, InfoComp uses an information-theoretic framework that maximizes mutual information between different parameters (or encoded representations). Within this framework, we design two novel loss functions: (1) to strengthen the accumulation of task-specific knowledge in P-Prompt, effectively mitigating catastrophic forgetting, and (2) to enhance the retention of task-invariant knowledge in S-Prompt, improving forward knowledge transfer. Extensive experiments on diverse CTC benchmarks show that our approach outperforms previous state-of-the-art methods.

</details>


### [98] [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/abs/2505.20937)

*Deepesh Gavit, Debajyoti Mazumder, Samiran Das, Jasabanta Patro*

**Main category:** cs.CL

**Keywords:** vision-language models, meme classification, fine-tuning, language models, textual understanding

**Relevance Score:** 6

**TL;DR:** The paper presents a systematic analysis of vision-language models (VLMs) for meme classification tasks, introducing a novel method that enhances LLM understanding of meme text to improve classification performance.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of meme classification tasks using vision-language models (VLMs) and language models (LLMs).

**Method:** The study benchmarks VLMs with different prompting strategies, evaluates LoRA fine-tuning across VLM components, and proposes using detailed meme interpretations generated by VLMs to train smaller LLMs.

**Key Contributions:**

	1. Benchmarking VLMs with diverse prompting strategies for disparate meme classification tasks
	2. Evaluating LoRA fine-tuning on VLM components for performance assessment
	3. Proposing a novel method for training smaller LLMs using detailed meme interpretations from VLMs

**Result:** Combining VLMs with LLMs yielded performance improvements of 8.34% for sarcasm, 3.52% for offensive content, and 26.24% for sentiment classification.

**Limitations:** 

**Conclusion:** The proposed approach reveals the strengths and limitations of VLMs and improves meme understanding significantly.

**Abstract:** In this paper, we present a comprehensive and systematic analysis of vision-language models (VLMs) for disparate meme classification tasks. We introduced a novel approach that generates a VLM-based understanding of meme images and fine-tunes the LLMs on textual understanding of the embedded meme text for improving the performance. Our contributions are threefold: (1) Benchmarking VLMs with diverse prompting strategies purposely to each sub-task; (2) Evaluating LoRA fine-tuning across all VLM components to assess performance gains; and (3) Proposing a novel approach where detailed meme interpretations generated by VLMs are used to train smaller language models (LLMs), significantly improving classification. The strategy of combining VLMs with LLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm, offensive and sentiment classification, respectively. Our results reveal the strengths and limitations of VLMs and present a novel strategy for meme understanding.

</details>


### [99] [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)

*Bertram HÃ¸jer, Terne Sasha Thorn Jakobsen, Anna Rogers, Stefan Heinrich*

**Main category:** cs.CL

**Keywords:** artificial intelligence, natural language processing, machine learning

**Relevance Score:** 7

**TL;DR:** A survey of 303 researchers reveals differing opinions on the definition of 'intelligence' in AI, particularly in NLP, with only a minority viewing current systems as 'intelligent.'

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand varying interpretations of 'intelligence' among researchers in fields related to NLP and AI, and its implication on research goals.

**Method:** A survey was conducted with 303 researchers from disciplines including NLP, ML, Cognitive Science, Linguistics, and Neuroscience, focusing on their definitions and criteria for intelligence.

**Key Contributions:**

	1. Identification of three criteria for intelligence in NLP: generalization, adaptability, and reasoning.
	2. Evidence that the current perception of NLP systems as intelligent is a minority view.
	3. Insights into researchers' priorities regarding the development of intelligent AI systems.

**Result:** The survey identified three key criteria for intelligence: generalization, adaptability, and reasoning. It was found that only 29% of researchers view current NLP systems as intelligent and only 16.2% prioritize developing intelligent systems.

**Limitations:** 

**Conclusion:** The findings indicate that the notion of intelligence in NLP is complex and that most researchers do not align with seeing current systems as truly intelligent, suggesting a need for clearer definitions in research agendas.

**Abstract:** Despite the widespread use of ''artificial intelligence'' (AI) framing in Natural Language Processing (NLP) research, it is not clear what researchers mean by ''intelligence''. To that end, we present the results of a survey on the notion of ''intelligence'' among researchers and its role in the research agenda. The survey elicited complete responses from 303 researchers from a variety of fields including NLP, Machine Learning (ML), Cognitive Science, Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the community agrees on the most: generalization, adaptability, & reasoning. Our results suggests that the perception of the current NLP systems as ''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the respondents see developing intelligent systems as a research goal, and these respondents are more likely to consider the current systems intelligent.

</details>


### [100] [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)

*Felix Krejca, Tobias Kietreiber, Alexander Buchelt, Sebastian Neumaier*

**Main category:** cs.CL

**Keywords:** content moderation, hate speech detection, machine learning, context-aware models, German newspaper forums

**Relevance Score:** 6

**TL;DR:** This paper develops binary classification models for automatic content moderation in German newspapers, focusing on the impact of contextual information.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research on hate speech detection in German-language newspaper forums and improve automatic content moderation.

**Method:** The paper develops and evaluates LSTM, CNN, and ChatGPT-3.5 Turbo models, utilizing the One Million Posts Corpus to assess the impact of contextual information in moderation tasks.

**Key Contributions:**

	1. Development of context-aware models for hate speech detection in German forums.
	2. Evaluation of LSTM, CNN, and ChatGPT-3.5 Turbo for content moderation.
	3. Use of the One Million Posts Corpus to improve model training and evaluation.

**Result:** Context-aware models, specifically CNN and LSTM, show improvements in performance over traditional methods, while ChatGPT's performance does not benefit from added context.

**Limitations:** Study is limited to German-language newspaper forums; results may not generalize to other platforms or languages.

**Conclusion:** Incorporating contextual information enhances moderation model performance for German newspaper forums, highlighting a gap in existing literature related to platform-specific contexts.

**Abstract:** The increasing volume of online discussions requires advanced automatic content moderation to maintain responsible discourse. While hate speech detection on social media is well-studied, research on German-language newspaper forums remains limited. Existing studies often neglect platform-specific context, such as user history and article themes. This paper addresses this gap by developing and evaluating binary classification models for automatic content moderation in German newspaper forums, incorporating contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging the One Million Posts Corpus from the Austrian newspaper Der Standard, we assess the impact of context-aware models. Results show that CNN and LSTM models benefit from contextual information and perform competitively with state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification does not improve with added context and underperforms.

</details>


### [101] [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/abs/2505.20966)

*Zhibo Wang, Xiaoze Jiang, Zhiheng Qin, Enyun Yu, Han Li*

**Main category:** cs.CL

**Keywords:** Query auto-completion, Personalization, Detoxification, Machine learning, Search systems

**Relevance Score:** 8

**TL;DR:** The paper presents LaD, a novel query auto-completion model that captures hierarchical personalized representations and incorporates adaptive detoxification to improve user experience and relevance in search systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for improved personalized representations in query auto-completion (QAC) systems and a method to detoxify generated content to enhance user experience and mitigate public relations issues.

**Method:** LaD captures personalized information hierarchically at both coarse-grained and fine-grained levels and employs an online training method using Reject Preference Optimization (RPO) with a special token [Reject] for adaptive detoxification.

**Key Contributions:**

	1. Hierarchical personalized representations for query auto-completion
	2. Adaptive detoxification through Reject Preference Optimization
	3. Deployment and real-world performance improvements in Kuaishou search.

**Result:** Experiments show that LaD delivers the largest single-experiment metric improvement in nearly two years, enhancing the user experience for hundreds of millions of active users in Kuaishou search.

**Limitations:** 

**Conclusion:** LaD effectively balances personalized content generation with the need to produce non-toxic outputs, representing a significant advancement in QAC technologies.

**Abstract:** Query auto-completion (QAC) plays a crucial role in modern search systems. However, in real-world applications, there are two pressing challenges that still need to be addressed. First, there is a need for hierarchical personalized representations for users. Previous approaches have typically used users' search behavior as a single, overall representation, which proves inadequate in more nuanced generative scenarios. Additionally, query prefixes are typically short and may contain typos or sensitive information, increasing the likelihood of generating toxic content compared to traditional text generation tasks. Such toxic content can degrade user experience and lead to public relations issues. Therefore, the second critical challenge is detoxifying QAC systems.   To address these two limitations, we propose a novel model (LaD) that captures personalized information from both long-term and short-term interests, incorporating adaptive detoxification. In LaD, personalized information is captured hierarchically at both coarse-grained and fine-grained levels. This approach preserves as much personalized information as possible while enabling online generation within time constraints. To move a futher step, we propose an online training method based on Reject Preference Optimization (RPO). By incorporating a special token [Reject] during both the training and inference processes, the model achieves adaptive detoxification. Consequently, the generated text presented to users is both non-toxic and relevant to the given prefix. We conduct comprehensive experiments on industrial-scale datasets and perform online A/B tests, delivering the largest single-experiment metric improvement in nearly two years of our product. Our model has been deployed on Kuaishou search, driving the primary traffic for hundreds of millions of active users. The code is available at https://github.com/JXZe/LaD.

</details>


### [102] [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)

*Xiangqing Shen, Fanfan Wang, Rui Xia*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Question Answering, Probabilistic Modeling, Machine Learning

**Relevance Score:** 9

**TL;DR:** RAR is a framework that combines LLM reasoning with knowledge graphs for Knowledge Graph Question Answering (KGQA), enhancing performance and interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with hallucinations and factual grounding, while KGs lack reasoning flexibility; RAR merges their strengths for improved KGQA.

**Method:** RAR comprises a Reasoner for generating reasoning chains, an Aligner for mapping to KG paths, and a Responser for synthesizing answers, modeled probabilistically and optimized via Expectation-Maximization.

**Key Contributions:**

	1. Introduction of the RAR framework for KGQA.
	2. Integration of LLM reasoning with knowledge graphs.
	3. Demonstrated state-of-the-art performance in multiple benchmarks.

**Result:** RAR achieved state-of-the-art performance on benchmarks with Hit@1 scores of 93.3% on WebQSP and 91.0% on CWQ, with strong human-validated reasoning quality.

**Limitations:** 

**Conclusion:** The RAR framework significantly enhances KGQA by providing interpretable reasoning in line with KGs while demonstrating efficient inference and zero-shot generalization.

**Abstract:** LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference.

</details>


### [103] [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/abs/2505.20976)

*Peiming Guo, Meishan Zhang, Jianling Li, Min Zhang, Yue Zhang*

**Main category:** cs.CL

**Keywords:** Cross-domain Parsing, Constituency Treebank, Large Language Models, Contrastive Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper proposes a novel method for automatic treebank generation using large language models (LLMs) to address the challenge of cross-domain constituency parsing, reporting state-of-the-art results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Cross-domain constituency parsing is challenging due to the limited availability of multi-domain constituency treebanks.

**Method:** The paper introduces a method called LLM back generation, which fills in missing words in incomplete cross-domain constituency trees, and employs a span-level contrastive learning pre-training strategy to enhance the generated treebank's utility.

**Key Contributions:**

	1. Proposal of LLM back generation for treebank generation
	2. Introduction of span-level contrastive learning for pre-training
	3. Achieving state-of-the-art results in cross-domain parsing

**Result:** The method achieves state-of-the-art performance on average results in five target domains compared to various baselines.

**Limitations:** 

**Conclusion:** The proposed LLM back generation approach, along with the contrastive learning strategy, significantly improves performance in cross-domain constituency parsing tasks.

**Abstract:** Cross-domain constituency parsing is still an unsolved challenge in computational linguistics since the available multi-domain constituency treebank is limited. We investigate automatic treebank generation by large language models (LLMs) in this paper. The performance of LLMs on constituency parsing is poor, therefore we propose a novel treebank generation method, LLM back generation, which is similar to the reverse process of constituency parsing. LLM back generation takes the incomplete cross-domain constituency tree with only domain keyword leaf nodes as input and fills the missing words to generate the cross-domain constituency treebank. Besides, we also introduce a span-level contrastive learning pre-training strategy to make full use of the LLM back generation treebank for cross-domain constituency parsing. We verify the effectiveness of our LLM back generation treebank coupled with contrastive learning pre-training on five target domains of MCTB. Experimental results show that our approach achieves state-of-the-art performance on average results compared with various baselines.

</details>


### [104] [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)

*Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, modality preference, representation engineering

**Relevance Score:** 8

**TL;DR:** This paper investigates modality preference in multimodal large language models (MLLMs) and proposes methods to control this preference in various tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how MLLMs exhibit modality preference when processing multimodal contexts and to improve their performance in tasks that require multimodal evidence.

**Method:** The authors created the MCÂ² benchmark to evaluate modality preference under controlled conflict scenarios and introduced a probing and steering method based on representation engineering to adjust this preference without fine-tuning.

**Key Contributions:**

	1. Development of the MCÂ² benchmark for evaluating modality preference
	2. Introduction of a method to control modality preference using representation engineering
	3. Demonstration of improved performance in multimodal tasks through the proposed methods

**Result:** Evaluation of 18 MLLMs showed clear modality bias influenced by external factors, and the proposed method successfully enhanced the preferred modality in downstream tasks like hallucination mitigation and multimodal translation.

**Limitations:** 

**Conclusion:** The findings highlight the existence of modality preference in MLLMs, and the proposed methods provide a means to effectively control this preference, demonstrating improved performance in applications.

**Abstract:** Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a \textbf{MC\textsuperscript{2}} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.

</details>


### [105] [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)

*Jie Shao, Jianxin Wu*

**Main category:** cs.CL

**Keywords:** large language models, output projection module, reasoning capabilities

**Relevance Score:** 8

**TL;DR:** This paper investigates the role of the output projection module in large language models' reasoning abilities using diagnostic tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how reasoning capabilities are manifested in large language models, particularly whether they arise from the entire model or specific components.

**Method:** The authors introduce a tool called Stethoscope for Networks (SfN) to analyze the internal behaviors of LLMs, focusing on the output projection module of the Transformer architecture.

**Key Contributions:**

	1. Introduction of Stethoscope for Networks (SfN) for analyzing LLMs
	2. Hypothesis that reasoning is primarily attributed to the output projection module
	3. Evidence supporting targeted training strategies for LLMs

**Result:** Evidence indicates that the output projection module is central to reasoning capabilities, while other components are more important for fluent dialogue.

**Limitations:** 

**Conclusion:** The findings provide insights into LLM interpretability and suggest directions for more effective training strategies for specialized models.

**Abstract:** Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.

</details>


### [106] [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/abs/2505.20995)

*Justin J. H. Lo, Patrycja Strycharczuk, Sam Kirkham*

**Main category:** cs.CL

**Keywords:** articulation, speaker discrimination, tongue shape, vowel production, Generalised Procrustes Analyses

**Relevance Score:** 2

**TL;DR:** This study investigates whether individual articulatory strategies in vowel production can be used for speaker discrimination, revealing that tongue size and anterior tongue shape are key factors.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if articulatory strategies in vowel production can allow for speaker discrimination based on anatomical and biomechanical variability.

**Method:** Generalised Procrustes Analyses of tongue shape data from 40 English speakers, assessing speaker-discriminatory potential through likelihood ratios.

**Key Contributions:**

	1. Demonstrated the relevance of tongue size and shape in speaker discrimination
	2. Introduced a statistical framework for analyzing speaker-specific articulation
	3. Highlighted the importance of anterior tongue shape variation in speaker discrimination

**Result:** Tongue size was found to be the most significant factor for discrimination, with anterior tongue shape variation outperforming posterior variation.

**Limitations:** Potential limitations include the sample size and speaker homogeneity, which may affect generalizability.

**Conclusion:** Shape-only information may provide speaker specificity similar to size-and-shape information, depending on the features' co-variation across speakers.

**Abstract:** The way speakers articulate is well known to be variable across individuals while at the same time subject to anatomical and biomechanical constraints. In this study, we ask whether articulatory strategy in vowel production can be sufficiently speaker-specific to form the basis for speaker discrimination. We conducted Generalised Procrustes Analyses of tongue shape data from 40 English speakers from the North West of England, and assessed the speaker-discriminatory potential of orthogonal tongue shape features within the framework of likelihood ratios. Tongue size emerged as the individual dimension with the strongest discriminatory power, while tongue shape variation in the more anterior part of the tongue generally outperformed tongue shape variation in the posterior part. When considered in combination, shape-only information may offer comparable levels of speaker specificity to size-and-shape information, but only when features do not exhibit speaker-level co-variation.

</details>


### [107] [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/abs/2505.21003)

*Yifei Wang, Yu Sheng, Linjing Li, Daniel Zeng*

**Main category:** cs.CL

**Keywords:** in-context learning, predictive uncertainty, epistemic uncertainty, long-context sequences, trustworthiness

**Relevance Score:** 8

**TL;DR:** This paper investigates how the number of in-context examples influences predictive uncertainty in long-context in-context learning (ICL), revealing that more examples can enhance trustworthiness by reducing epistemic uncertainty.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored relationship between the quantity of in-context examples and the trustworthiness of generated responses in ICL.

**Method:** The authors systematically quantify the uncertainty of ICL with varying shot counts and conduct uncertainty decomposition to analyze the effects on performance and epistemic uncertainty.

**Key Contributions:**

	1. Introduction of a novel perspective on performance enhancement through uncertainty decomposition.
	2. Demonstration of the role of example quantity in reducing epistemic uncertainty.
	3. Analysis of internal confidence evolution across layers.

**Result:** The results show that additional examples reduce total uncertainty and epistemic uncertainty, enhancing performance, particularly for complex tasks after managing increased noise.

**Limitations:** 

**Conclusion:** Increased in-context examples improve performance by injecting task-specific knowledge, ultimately enhancing the trustworthiness of predictions.

**Abstract:** Recent advances in handling long sequences have facilitated the exploration of long-context in-context learning (ICL). While much of the existing research emphasizes performance improvements driven by additional in-context examples, the influence on the trustworthiness of generated responses remains underexplored. This paper addresses this gap by investigating how increased examples influence predictive uncertainty, an essential aspect in trustworthiness. We begin by systematically quantifying the uncertainty of ICL with varying shot counts, analyzing the impact of example quantity. Through uncertainty decomposition, we introduce a novel perspective on performance enhancement, with a focus on epistemic uncertainty (EU). Our results reveal that additional examples reduce total uncertainty in both simple and complex tasks by injecting task-specific knowledge, thereby diminishing EU and enhancing performance. For complex tasks, these advantages emerge only after addressing the increased noise and uncertainty associated with longer inputs. Finally, we explore the evolution of internal confidence across layers, unveiling the mechanisms driving the reduction in uncertainty.

</details>


### [108] [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/abs/2505.21011)

*Liang Cheng, Zhaowei Wang, Mark Steedman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Inference, Frequency Bias, Textual Entailment, Fine-tuning

**Relevance Score:** 9

**TL;DR:** This work investigates the improvements in LLMs' inferential performance through fine-tuning on NLI corpora, focusing on the role of frequency bias in premises and hypotheses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand what LLMs learn during fine-tuning on Natural Language Inference (NLI) datasets and the mechanisms behind improved inferential performance.

**Method:** The study involved analyzing predicate frequencies across NLI datasets and assessing LLM performance on both bias-consistent and bias-adversarial instances.

**Key Contributions:**

	1. Analyzed predicate frequencies in NLI datasets to identify consistent frequency bias.
	2. Evaluated impact of frequency bias on LLMs' performance under varying conditions.
	3. Revealed correlation between frequency bias of words and model performance in entailment tasks.

**Result:** The results indicate that LLMs leverage frequency bias for inference, showing poor performance on adversarial instances and increased reliance on this bias when fine-tuned.

**Limitations:** The study primarily focuses on frequency bias and does not explore other potential factors affecting LLM inferential performance.

**Conclusion:** The findings suggest that LLMs learn frequency patterns that enhance their performance on inference tasks, providing insights into the biases present in NLI datasets.

**Abstract:** While fine-tuning LLMs on NLI corpora improves their inferential performance, the underlying mechanisms driving this improvement remain largely opaque. In this work, we conduct a series of experiments to investigate what LLMs actually learn during fine-tuning. We begin by analyzing predicate frequencies in premises and hypotheses across NLI datasets and identify a consistent frequency bias, where predicates in hypotheses occur more frequently than those in premises for positive instances. To assess the impact of this bias, we evaluate both standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial cases. We find that LLMs exploit frequency bias for inference and perform poorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit significantly increased reliance on this bias, suggesting that they are learning these frequency patterns from datasets. Finally, we compute the frequencies of hyponyms and their corresponding hypernyms from WordNet, revealing a correlation between frequency bias and textual entailment. These findings help explain why learning frequency patterns can enhance model performance on inference tasks.

</details>


### [109] [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/abs/2505.21033)

*Seungmin Lee, Yongsang Yoo, Minhwa Jung, Min Song*

**Main category:** cs.CL

**Keywords:** Dialogue Topic Segmentation, Large Language Models, NLP, Deductive Reasoning, Intent Classification

**Relevance Score:** 8

**TL;DR:** This paper presents Def-DTS, a method utilizing LLM-based deductive reasoning to improve dialogue topic segmentation (DTS).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Dialogue Topic Segmentation (DTS) is essential for various NLP tasks but faces issues such as data shortage and complexity, which this paper addresses using LLMs.

**Method:** The proposed method employs multi-step deductive reasoning through structured prompting for summarizing context, classifying intent, and detecting topic shifts.

**Key Contributions:**

	1. Introduction of Def-DTS for enhanced dialogue topic segmentation
	2. Generalizable intent list for domain-agnostic intent classification
	3. Significant performance improvements demonstrated in experiments

**Result:** Def-DTS outperforms traditional and state-of-the-art DTS methods, reducing type 2 errors significantly across multiple dialogue settings.

**Limitations:** 

**Conclusion:** The findings highlight the potential of using LLM reasoning techniques for enhancing dialogue topic segmentation and suggest avenues for autolabeling.

**Abstract:** Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent segments. DTS plays a crucial role in various NLP downstream tasks, but suffers from chronic problems: data shortage, labeling ambiguity, and incremental complexity of recently proposed solutions. On the other hand, Despite advances in Large Language Models (LLMs) and reasoning strategies, these have rarely been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step deductive reasoning to enhance DTS performance and enable case study using intermediate result. Our method employs a structured prompting approach for bidirectional context summarization, utterance intent classification, and deductive topic shift detection. In the intent classification process, we propose the generalizable intent list for domain-agnostic dialogue intent classification. Experiments in various dialogue settings demonstrate that Def-DTS consistently outperforms traditional and state-of-the-art approaches, with each subtask contributing to improved performance, particularly in reducing type 2 error. We also explore the potential for autolabeling, emphasizing the importance of LLM reasoning techniques in DTS.

</details>


### [110] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)

*Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu, Fuzhen Zhuang*

**Main category:** cs.CL

**Keywords:** targeted sentiment analysis, cross-task knowledge transfer, aspect extraction

**Relevance Score:** 6

**TL;DR:** This paper presents FCKT, a fine-grained cross-task knowledge transfer framework for targeted sentiment analysis, emphasizing the importance of aspect extraction and contextual sentiment differentiation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of existing targeted sentiment analysis methods that rely on coarse-grained knowledge transfer, leading to negative transfer when predicting sentiments.

**Method:** The authors propose the FCKT framework, which incorporates aspect-level information into sentiment prediction to enable fine-grained knowledge transfer.

**Key Contributions:**

	1. Introduction of the FCKT framework for targeted sentiment analysis
	2. Achievement of fine-grained knowledge transfer
	3. Demonstration of improved performance over existing approaches and baselines

**Result:** Experiments conducted on three datasets show that FCKT effectively mitigates negative transfer and enhances the overall performance of targeted sentiment analysis compared to various baselines and large language models.

**Limitations:** 

**Conclusion:** FCKT improves sentiment prediction by addressing the oversimplified assumptions about aspect-sentiment relationships, ultimately leading to better task performance.

**Abstract:** In this paper, we address the task of targeted sentiment analysis (TSA), which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding sentiments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer. While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer. To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, FCKT achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance. Experiments on three datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [111] [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/abs/2505.21043)

*Sam O'Connor Russell, Naomi Harte*

**Main category:** cs.CL

**Keywords:** multimodal interaction, turn-taking prediction, human-robot interaction, speech alignment, facial cues

**Relevance Score:** 9

**TL;DR:** We present MM-VAP, a multimodal predictive turn-taking model that incorporates visual cues alongside speech, outperforming audio-only models in predicting turn-taking in videoconferencing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-robot interaction through accurate turn-taking prediction by integrating visual cues beyond just speech.

**Method:** MM-VAP, a multimodal predictive turn-taking model, uses speech and visual cues (facial expression, head pose, gaze) to predict when participants will hold or shift turns during conversation.

**Key Contributions:**

	1. Introduction of MM-VAP, a multimodal PTTM incorporating visual cues
	2. Demonstrated superior performance over audio-only models in turn-taking prediction
	3. Conducted an ablation study identifying the importance of facial expressions in model accuracy.

**Result:** MM-VAP achieved 84% prediction accuracy compared to 79% for state-of-the-art audio-only models, especially improving across different durations of speaker transitions.

**Limitations:** 

**Conclusion:** Incorporating visual cues significantly enhances turn-taking prediction performance, indicating their importance in settings where interlocutors can see each other.

**Abstract:** Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs) facilitate naturalistic human-robot interaction, yet most rely solely on speech. We introduce MM-VAP, a multimodal PTTM which combines speech with visual cues including facial expression, head pose and gaze. We find that it outperforms the state-of-the-art audio-only in videoconferencing interactions (84% vs. 79% hold/shift prediction accuracy). Unlike prior work which aggregates all holds and shifts, we group by duration of silence between turns. This reveals that through the inclusion of visual features, MM-VAP outperforms a state-of-the-art audio-only turn-taking model across all durations of speaker transitions. We conduct a detailed ablation study, which reveals that facial expression features contribute the most to model performance. Thus, our working hypothesis is that when interlocutors can see one another, visual cues are vital for turn-taking and must therefore be included for accurate turn-taking prediction. We additionally validate the suitability of automatic speech alignment for PTTM training using telephone speech. This work represents the first comprehensive analysis of multimodal PTTMs. We discuss implications for future work and make all code publicly available.

</details>


### [112] [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)

*Anil Batra, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller*

**Main category:** cs.CL

**Keywords:** Semantic Role Labeling, multimodal models, cooking procedures, implicit arguments, LLM

**Relevance Score:** 8

**TL;DR:** This paper presents the Implicit-VidSRL dataset to enhance AI's understanding of procedural texts in cooking contexts by improving Semantic Role Labeling, particularly for implicit argument inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding procedural instructions requires recognizing both explicit and implicit arguments, which current benchmarks fail to address effectively.

**Method:** Introduction of the Implicit-VidSRL dataset to evaluate multimodal models on their ability to infer implicit semantic roles within cooking procedures.

**Key Contributions:**

	1. Development of the Implicit-VidSRL dataset for multimodal cooking procedures
	2. Benchmarking of recent multimodal LLMs on implicit semantic role prediction
	3. Introduction of the iSRL-Qwen2-VL model with improved prediction accuracy

**Result:** Multimodal LLMs were tested and shown to struggle with predicting implicit arguments; the proposed model, iSRL-Qwen2-VL, demonstrated significant improvements in F1-scores for implicit semantic roles.

**Limitations:** The study focuses specifically on cooking procedures, limiting the generalizability of findings across other procedural domains.

**Conclusion:** The study illustrates the challenges of current models in understanding implicit elements in multimodal procedural data and offers a dataset to address these challenges.

**Abstract:** Procedural texts help AI enhance reasoning about context and action sequences. Transforming these into Semantic Role Labeling (SRL) improves understanding of individual steps by identifying predicate-argument structure like {verb,what,where/with}. Procedural instructions are highly elliptic, for instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second step's where argument is inferred from the context, referring to where the cucumber was placed. Prior SRL benchmarks often miss implicit arguments, leading to incomplete understanding. To address this, we introduce Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit arguments from contextual information in multimodal cooking procedures. Our proposed dataset benchmarks multimodal models' contextual reasoning, requiring entity tracking through visual changes in recipes. We study recent multimodal LLMs and reveal that they struggle to predict implicit arguments of what and where/with from multi-modal procedural data given the verb. Lastly, we propose iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.

</details>


### [113] [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/abs/2505.21072)

*Ekaterina Fadeeva, Aleksandr Rubashevskii, Roman Vashurin, Shehzaad Dhuliawala, Artem Shelmanov, Timothy Baldwin, Preslav Nakov, Mrinmaya Sachan, Maxim Panov*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, hallucination detection, Uncertainty Quantification

**Relevance Score:** 9

**TL;DR:** Introducing FRANQ, a method for hallucination detection in Retrieval-Augmented Generation (RAG) outputs by using Uncertainty Quantification techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** RAG systems face issues with hallucinations that result in factually incorrect outputs. Current methods mistakenly conflate factuality with faithfulness to retrieved context.

**Method:** FRANQ utilizes various Uncertainty Quantification techniques to assess factuality based on the faithfulness of statements to retrieved context.

**Key Contributions:**

	1. Introduction of FRANQ for hallucination detection in RAG outputs
	2. Development of a new QA dataset annotated for factuality and faithfulness
	3. Demonstration of improved accuracy in detecting errors in RAG responses

**Result:** FRANQ demonstrates more accurate detection of factual errors in RAG-generated outputs compared to existing methods through extensive experiments on QA tasks.

**Limitations:** 

**Conclusion:** The proposed FRANQ method significantly enhances the ability to determine factuality in RAG outputs, providing a clearer distinction between factuality and faithfulness.

**Abstract:** Large Language Models (LLMs) enhanced with external knowledge retrieval, an approach known as Retrieval-Augmented Generation (RAG), have shown strong performance in open-domain question answering. However, RAG systems remain susceptible to hallucinations: factually incorrect outputs that may arise either from inconsistencies in the model's internal knowledge or incorrect use of the retrieved context. Existing approaches often conflate factuality with faithfulness to the retrieved context, misclassifying factually correct statements as hallucinations if they are not directly supported by the retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval Augmented UNcertainty Quantification), a novel method for hallucination detection in RAG outputs. FRANQ applies different Uncertainty Quantification (UQ) techniques to estimate factuality based on whether a statement is faithful to the retrieved context or not. To evaluate FRANQ and other UQ techniques for RAG, we present a new long-form Question Answering (QA) dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging examples. Extensive experiments on long- and short-form QA across multiple datasets and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing methods.

</details>


### [114] [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)

*Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, Dongha Lee*

**Main category:** cs.CL

**Keywords:** large language models, personalization, reasoning-level, human-computer interaction, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents RPM, a framework for reasoning-level personalization in black-box large language models (LLMs), enhancing model outputs by aligning them with user-specific reasoning processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve personalization in black-box LLMs, which typically produce generalized outputs that don't consider individual user preferences and reasoning styles.

**Method:** RPM constructs statistical user-specific factors from user history, builds personalized reasoning paths, and retrieves reasoning-aligned examples to condition inference on user-specific logic.

**Key Contributions:**

	1. Introduction of RPM for reasoning-level personalization in black-box LLMs
	2. Construction of user-specific reasoning paths
	3. Demonstration of improved predictive accuracy and interpretability over traditional methods.

**Result:** Extensive experiments show that RPM outperforms existing response-level personalization methods, improving both predictive accuracy and interpretability.

**Limitations:** 

**Conclusion:** The reasoning-level personalization using RPM allows LLMs to produce outputs grounded in user-specific logic, thereby enhancing performance and user satisfaction.

**Abstract:** Large language models (LLMs) have recently achieved impressive performance across a wide range of natural language tasks and are now widely used in real-world applications. Among them, black-box LLMs--served via APIs without access to model internals--are especially dominant due to their scalability and ease of deployment. Despite their strong capabilities, these models typically produce generalized responses that overlook personal preferences and reasoning styles. This has led to growing interest in black-box LLM personalization, which aims to tailor model outputs to user-specific context without modifying model parameters. However, existing approaches primarily focus on response-level personalization, attempting to match final outputs without modeling personal thought process. To address this limitation, we propose RPM, a framework for reasoning-level personalization that aligns the model's reasoning process with a user's personalized logic. RPM first constructs statistical user-specific factors by extracting and grouping response-influential features from user history. It then builds personalized reasoning paths that reflect how these factors are used in context. In the inference stage, RPM retrieves reasoning-aligned examples for new queries via feature-level similarity and performs inference conditioned on the structured factors and retrieved reasoning paths, enabling the model to follow user-specific reasoning trajectories. This reasoning-level personalization enhances both predictive accuracy and interpretability by grounding model outputs in user-specific logic through structured information. Extensive experiments across diverse tasks show that RPM consistently outperforms response-level personalization methods, demonstrating the effectiveness of reasoning-level personalization in black-box LLMs.

</details>


### [115] [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)

*Daeen Kabir, Minhajur Rahman Chowdhury Mahim, Sheikh Shafayat, Adnan Sadik, Arian Ahmed, Eunsu Kim, Alice Oh*

**Main category:** cs.CL

**Keywords:** Bengali, LLM, dataset, evaluation, cultural knowledge

**Relevance Score:** 5

**TL;DR:** BLUCK is a new dataset for evaluating LLMs on Bengali linguistic understanding and cultural knowledge, consisting of 2366 MCQs across various categories.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To measure LLM performance in Bengali language and culture, filling a gap for mid-resource languages.

**Method:** Created a dataset of 2366 multiple-choice questions from exams, covering Bengali culture, history, and linguistics; benchmarked on several LLMs.

**Key Contributions:**

	1. Introduction of the BLUCK dataset for Bengali LLM evaluation
	2. First MCQ benchmark focused on Bengali cultural and linguistic content
	3. Benchmarking results highlighting performance gaps in Bengali phonetics

**Result:** The benchmarking results indicate reasonable performance of LLMs, but notable struggles in Bengali phonetics.

**Limitations:** 

**Conclusion:** BLUCK highlights the need for better LLM performance on mid-resource languages, specifically Bengali, and serves as a foundational evaluation tool.

**Abstract:** In this work, we introduce BLUCK, a new dataset designed to measure the performance of Large Language Models (LLMs) in Bengali linguistic understanding and cultural knowledge. Our dataset comprises 2366 multiple-choice questions (MCQs) carefully curated from compiled collections of several college and job level examinations and spans 23 categories covering knowledge on Bangladesh's culture and history and Bengali linguistics. We benchmarked BLUCK using 6 proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that while these models perform reasonably well overall, they, however, struggles in some areas of Bengali phonetics. Although current LLMs' performance on Bengali cultural and linguistic contexts is still not comparable to that of mainstream languages like English, our results indicate Bengali's status as a mid-resource language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark that is centered around native Bengali culture, history, and linguistics.

</details>


### [116] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)

*Stephen Chung, Wenyu Du, Jie Fu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Question Answering, Dual Process Theory, Inference Efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces a modified question-answering task inspired by Dual Process Theory to enhance the reasoning capabilities of LLMs through a structured multi-stage process, resulting in improved accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the imprecise and redundant responses generated by LLMs in QA tasks, the authors aim to improve reasoning capabilities by applying a structured process inspired by psychological theories.

**Method:** The proposed method involves a four-stage QA task: Fast Thinking (quick response within a token budget), Verification (evaluating the initial response), Slow Thinking (refining the response), and Summarization (condensing the refined response into precise steps).

**Key Contributions:**

	1. Introduction of a multi-stage QA task based on Dual Process Theory
	2. Demonstration of improved accuracy and inference efficiency in LLM responses
	3. Revelation of the benefits of structured reasoning processes for enhancing LLM performance.

**Result:** The modification improved average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. The Fast Thinking mode alone achieved 26.8% accuracy with fewer than 1000 tokens.

**Limitations:** 

**Conclusion:** The findings indicate that distinct systems of intuition and deliberative reasoning in LLMs can be enhanced through structured training methods, leading to improved performance in QA tasks.

**Abstract:** Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.

</details>


### [117] [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)

*Bogdan Bogachov, Yaoyao Fiona Zhao*

**Main category:** cs.CL

**Keywords:** Small Language Graph, language models, adaptation, fine-tuning, generative AI

**Relevance Score:** 8

**TL;DR:** The Small Language Graph (SLG) offers a lightweight adaptation approach for large language models, reducing computational demands while improving performance and addressing hallucination issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the computational resources required for fine-tuning and inference of language models, and to mitigate hallucination problems that affect the generation of structured text.

**Method:** The SLG is structured as a graph with nodes representing lightweight expert language models fine-tuned on specific texts, enabling efficient adaptation.

**Key Contributions:**

	1. Introduction of the Small Language Graph (SLG) for language model adaptation
	2. Demonstrated significant performance improvements in fine-tuning
	3. Potential for widespread adoption in engineering contexts due to reduced resource demands

**Result:** SLG surpassed conventional fine-tuning methods on the Exact Match metric by 3 times, and the fine-tuning process was 1.7 times faster than that of larger models.

**Limitations:** Not specified in the abstract.

**Conclusion:** SLG enables small to medium-sized engineering companies to use generative AI technologies without heavy computational investments, potentially paving the way for distributed AI systems.

**Abstract:** Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.

</details>


### [118] [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115)

*Sergey Pletenev, Maria Marina, Nikolay Ivanov, Daria Galimzianova, Nikita Krayko, Mikhail Salnikov, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii*

**Main category:** cs.CL

**Keywords:** Large Language Models, question answering, temporality, multilingual dataset, classifier

**Relevance Score:** 9

**TL;DR:** Introduces EverGreenQA, a multilingual QA dataset focusing on question temporality, and benchmarks LLM performance on this aspect.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucination in LLMs during QA tasks, particularly by exploring the concept of question temporality and its impact on answer reliability.

**Method:** The study presents EverGreenQA, a dataset labeled with evergreen and mutable question characteristics, and benchmarks 12 modern LLMs using explicit and implicit measures of temporality. Additionally, it trains EG-E5, a multilingual classifier for this classification task.

**Key Contributions:**

	1. Creation of EverGreenQA dataset with evergreen labels.
	2. Benchmarking of LLMs on question temporality.
	3. Development of EG-E5, a lightweight classifier with SoTA performance.

**Result:** Benchmarked LLMs show varying capabilities in recognizing question temporality, and the EG-E5 classifier achieves state-of-the-art performance in classifying evergreen vs. mutable questions.

**Limitations:** Focused on the classification of question temporality, leaving other factors affecting LLM performance unaddressed.

**Conclusion:** The introduction of EverGreenQA and EG-E5 provides valuable tools for enhancing the reliability of QA systems in understanding question temporality, with implications for real-world applications.

**Abstract:** Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.

</details>


### [119] [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2505.21137)

*Mengjie Qian, Rao Ma, Stefano BannÃ², Kate M. Knill, Mark J. F. Gales*

**Main category:** cs.CL

**Keywords:** Spoken Grammatical Error Correction, end-to-end models, feedback generation, pseudo-labelling, Whisper model

**Relevance Score:** 8

**TL;DR:** This paper explores the application of end-to-end speech models for Spoken Grammatical Error Correction (SGEC) and feedback generation, introducing a pseudo-labelling method to enhance training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve Spoken Grammatical Error Correction and feedback mechanisms for second language learners, teachers, and test takers using modern speech foundation models.

**Method:** The authors employed a pseudo-labelling process to increase the size of the training dataset significantly and utilized an E2E Whisper-based model for SGEC and feedback generation, analyzing the effects of model size and data augmentation.

**Key Contributions:**

	1. Introduced a pseudo-labelling technique to enhance training data for SGEC tasks.
	2. Employed an end-to-end Whisper-based model for grammatical error correction and feedback generation.
	3. Demonstrated the effectiveness of prompting with fluent transcriptions for performance improvement.

**Result:** The introduction of pseudo-labelling expanded the training data from 77 hours to approximately 2500 hours, resulting in improved performance. Prompting the Whisper model with fluent transcriptions yielded slight performance gains in SGEC and more significant improvements in feedback generation.

**Limitations:** 

**Conclusion:** While increasing model size did not provide performance benefits when using pseudo-labelled data, the addition of fluent prompts proved to enhance model performance in targeted tasks.

**Abstract:** Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial for second language learners, teachers and test takers. Traditional SGEC systems rely on a cascaded pipeline consisting of an ASR, a module for disfluency detection (DD) and removal and one for GEC. With the rise of end-to-end (E2E) speech foundation models, we investigate their effectiveness in SGEC and feedback generation. This work introduces a pseudo-labelling process to address the challenge of limited labelled data, expanding the training data size from 77 hours to approximately 2500 hours, leading to improved performance. Additionally, we prompt an E2E Whisper-based SGEC model with fluent transcriptions, showing a slight improvement in SGEC performance, with more significant gains in feedback generation. Finally, we assess the impact of increasing model size, revealing that while pseudo-labelled data does not yield performance gain for a larger Whisper model, training with prompts proves beneficial.

</details>


### [120] [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)

*Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei*

**Main category:** cs.CL

**Keywords:** ASR, self-supervised learning, Chinese dialects, large language models, speech recognition

**Relevance Score:** 8

**TL;DR:** This paper explores enhancing ASR performance for Chinese dialects using self-supervised learning and large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Chinese accents and dialects pose challenges for ASR models due to limited data availability. The paper aims to improve ASR models in this context by leveraging self-supervised pre-training with LLMs.

**Method:** The authors pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect speech data and align it with a supervised dataset of 40,000 hours to evaluate the impacts of different projectors and LLMs on speech recognition performance.

**Key Contributions:**

	1. Introduction of a pre-training approach using self-supervised learning for ASR in dialects
	2. Demonstration of state-of-the-art performance with the Data2vec2 model on dialect datasets
	3. Open-sourcing of research to facilitate reproducibility

**Result:** The proposed method achieved state-of-the-art results on several dialect datasets, including Kespeech.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of self-supervised learning combined with LLMs for ASR in low-resource scenarios, particularly for Chinese dialects and accents.

**Abstract:** Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre- training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research

</details>


### [121] [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/abs/2505.21148)

*Rao Ma, Mengjie Qian, Siyuan Tang, Stefano BannÃ², Kate M. Knill, Mark J. F. Gales*

**Main category:** cs.CL

**Keywords:** spoke language assessment, large language models, L2 proficiency grading

**Relevance Score:** 9

**TL;DR:** This paper explores the use of multi-modal large language models for assessing L2 oral proficiency, comparing various training strategies that enhance performance over traditional models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing need for automatic graders for spoken language assessment due to the rise in L2 English speakers and the limitations of traditional grading systems.

**Method:** The paper compares different training strategies with regression and classification targets using large language models specifically designed for speech recognition and assessment.

**Key Contributions:**

	1. Introduction of multi-modal LLMs for spoken language assessment
	2. Demonstration of superior performance over existing models
	3. Validation of strong generalization capabilities across tasks

**Result:** The speech LLMs demonstrated superior performance compared to all previous baselines on two datasets and showed strong generalization capabilities in evaluations across different tasks.

**Limitations:** The study may be limited by the specific datasets used and the focus on English-language learners only.

**Conclusion:** Multi-modal LLMs can effectively serve as automatic graders for L2 oral proficiency, overcoming the limitations of past methodologies.

**Abstract:** The growing population of L2 English speakers has increased the demand for developing automatic graders for spoken language assessment (SLA). Historically, statistical models, text encoders, and self-supervised speech models have been utilised for this task. However, cascaded systems suffer from the loss of information, while E2E graders also have limitations. With the recent advancements of multi-modal large language models (LLMs), we aim to explore their potential as L2 oral proficiency graders and overcome these issues. In this work, we compare various training strategies using regression and classification targets. Our results show that speech LLMs outperform all previous competitive baselines, achieving superior performance on two datasets. Furthermore, the trained grader demonstrates strong generalisation capabilities in the cross-part or cross-task evaluation, facilitated by the audio understanding knowledge acquired during LLM pre-training.

</details>


### [122] [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)

*Rochelle Choenni, Ivan Titov*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Pruning, M-Wanda, Language-aware activation, Sparse models

**Relevance Score:** 9

**TL;DR:** The paper presents M-Wanda, a novel pruning method aimed at maintaining multilingual performance in large language models while reducing their size by dynamically adjusting sparsity based on cross-lingual importance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recent efforts in one-shot pruning of multilingual LLMs reveal serious trade-offs between model size and performance, necessitating a better understanding of these dynamics.

**Method:** The authors propose M-Wanda, a pruning method that integrates language-aware activation statistics into its sparsity criteria, allowing for dynamic layerwise adjustments based on cross-lingual importance.

**Key Contributions:**

	1. Introduction of M-Wanda, a language-aware pruning method
	2. Demonstration of substantial performance improvements with minimal sparsity
	3. First explicit optimization of pruning for multilingual model performance

**Result:** M-Wanda demonstrates improved multilingual performance with minimal cost, indicating the effectiveness of optimizing pruning for multilingual models.

**Limitations:** The study mainly focuses on moderate sparsity levels; extreme levels of pruning and their impacts are not evaluated.

**Conclusion:** The study shows that careful pruning can retain performance while reducing model size, paving the way for future research in multilingual pruning techniques.

**Abstract:** Multilingual LLM performance is often critically dependent on model size. With an eye on efficiency, this has led to a surge in interest in one-shot pruning methods that retain the benefits of large-scale pretraining while shrinking the model size. However, as pruning tends to come with performance loss, it is important to understand the trade-offs between multilinguality and sparsification. In this work, we study multilingual performance under different sparsity constraints and show that moderate ratios already substantially harm performance. To help bridge this gap, we propose M-Wanda, a pruning method that models cross-lingual variation by incorporating language-aware activation statistics into its pruning criterion and dynamically adjusts layerwise sparsity based on cross-lingual importance. We show that M-Wanda consistently improves performance at minimal additional costs. We are the first to explicitly optimize pruning to retain multilingual performance, and hope to inspire future advances in multilingual pruning.

</details>


### [123] [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/abs/2505.21172)

*Zheng Li, Mao Zheng, Mingyang Song, Wenjie Yang*

**Main category:** cs.CL

**Keywords:** Terminology Translation, Reinforcement Learning, Machine Translation

**Relevance Score:** 7

**TL;DR:** This paper introduces TAT-R1, a terminology-aware translation model that uses reinforcement learning to improve the accuracy of terminology translation in machine translation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in deep reasoning LLMs concerning terminology translation, which is a crucial aspect of machine translation that has not been thoroughly explored.

**Method:** The authors propose TAT-R1, which employs a word alignment model to extract keyword translation pairs and implements three rule-based alignment rewards to enhance the training of the translation model via reinforcement learning.

**Key Contributions:**

	1. Introduction of TAT-R1, a novel terminology-aware translation model.
	2. Developed specific rule-based alignment rewards tailored for terminology translation.
	3. Conducted ablation studies to evaluate the impact of the training paradigm.

**Result:** Experimental results demonstrate that TAT-R1 significantly boosts terminology translation accuracy compared to baseline models, while still performing well on general translation tasks.

**Limitations:** 

**Conclusion:** The findings suggest that with specific rewards focused on terminology, a reinforcement learning approach can effectively enhance translation models, especially for key information in texts.

**Abstract:** Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have made significant progress in tasks such as mathematics and coding. Inspired by this, several studies have employed reinforcement learning(RL) to enhance models' deep reasoning capabilities and improve machine translation(MT) quality. However, the terminology translation, an essential task in MT, remains unexplored in deep reasoning LLMs. In this paper, we propose \textbf{TAT-R1}, a terminology-aware translation model trained with reinforcement learning and word alignment. Specifically, we first extract the keyword translation pairs using a word alignment model. Then we carefully design three types of rule-based alignment rewards with the extracted alignment relationships. With those alignment rewards, the RL-trained translation model can learn to focus on the accurate translation of key information, including terminology in the source text. Experimental results show the effectiveness of TAT-R1. Our model significantly improves terminology translation accuracy compared to the baseline models while maintaining comparable performance on general translation tasks. In addition, we conduct detailed ablation studies of the DeepSeek-R1-like training paradigm for machine translation and reveal several key findings.

</details>


### [124] [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178)

*Mingyang Song, Mao Zheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought, Reinforcement Learning, Conciseness, Reasoning

**Relevance Score:** 9

**TL;DR:** The paper proposes a two-stage reinforcement learning framework, ConciseR, to enhance reasoning capabilities in LLMs by generating more concise Chain-of-Thought responses while reducing redundancy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of overthinking and redundancy in long Chain-of-Thought responses generated by Large Language Models (LLMs).

**Method:** The method involves a two-stage reinforcement learning framework: the first stage (GRPO++) incentivizes reasoning capabilities, while the second stage (L-GRPO) enforces conciseness and efficiency in responses.

**Key Contributions:**

	1. Introduction of ConciseR as a novel framework for LLMs.
	2. First stage incentivizing better reasoning capabilities through GRPO++.
	3. Second stage enforcing response conciseness and efficiency with L-GRPO.

**Result:** ConciseR outperforms recent state-of-the-art reasoning models on various benchmarks by generating more concise Chain-of-Thought reasoning responses.

**Limitations:** 

**Conclusion:** ConciseR effectively addresses the challenge of excessive redundancy in reasoning outputs of LLMs, contributing to improved reasoning performance.

**Abstract:** As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model's reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.

</details>


### [125] [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)

*Gleb Mezentsev, Ivan Oseledets*

**Main category:** cs.CL

**Keywords:** large language models, multi-token generation, embedding space

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of frozen large language models to generate multiple tokens in a single forward pass without autoregression, using two learned embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of large language models (LLMs) in multi-token generation without iterative decoding, which has not been thoroughly studied.

**Method:** The authors examine the performance of frozen LLMs to generate hundreds of tokens from just two learned embeddings in a single forward pass.

**Key Contributions:**

	1. Demonstration of non-autoregressive multi-token generation by frozen LLMs.
	2. Insights into the properties of learned embeddings.
	3. Potential for developing dedicated encoders into embedding space.

**Result:** The study demonstrates that LLMs can generate sequences of accurate text without relying on autoregressive processes, revealing an unexpected capability of these models.

**Limitations:** The embeddings are not unique for a given text, which may limit their application.

**Conclusion:** The findings suggest that while these embeddings are not unique to specific texts, they occupy connected and local areas in the embedding space, indicating the possibility of developing a specialized encoder for such representations.

**Abstract:** A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.

</details>


### [126] [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)

*Jong Hak Moon, Geon Choi, Paloma Rabaey, Min Gwan Kim, Hyuk Gi Hong, Jung-Oh Lee, Hangyul Yoon, Eun Woo Doe, Jiyoun Kim, Harshita Sharma, Daniel C. Castro, Javier Alvarez-Valle, Edward Choi*

**Main category:** cs.CL

**Keywords:** radiology, report generation, evaluation metric, longitudinal assessment, structured representation

**Relevance Score:** 7

**TL;DR:** The paper introduces LUNGUAGE, a benchmark dataset and evaluation methods for structured radiology report generation that focus on temporal dependencies and fine-grained clinical semantics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing evaluation methods for radiology reports that do not capture temporal dependencies and detailed clinical observations across multiple studies.

**Method:** A two-stage framework transforms generated reports into schema-aligned structured representations, while LUNGUAGESCORE serves as an interpretable metric for comparing structured outputs.

**Key Contributions:**

	1. Development of the LUNGUAGE benchmark dataset for structured radiology reports.
	2. Creation of a two-stage framework for longitudinal interpretation of reports.
	3. Introduction of LUNGUAGESCORE as a novel evaluation metric for structured output comparison.

**Result:** The LUNGUAGE dataset consists of 1,473 annotated chest X-ray reports, allowing for both single-report evaluation and longitudinal assessment, with LUNGUAGESCORE demonstrating effectiveness in structured report evaluation.

**Limitations:** 

**Conclusion:** The introduction of the LUNGUAGE dataset and LUNGUAGESCORE establishes a new standard for evaluating sequential radiology reporting, supporting better clinical interpretation over time.

**Abstract:** Radiology reports convey detailed clinical observations and capture diagnostic reasoning that evolves over time. However, existing evaluation methods are limited to single-report settings and rely on coarse metrics that fail to capture fine-grained clinical semantics and temporal dependencies. We introduce LUNGUAGE,a benchmark dataset for structured radiology report generation that supports both single-report evaluation and longitudinal patient-level assessment across multiple studies. It contains 1,473 annotated chest X-ray reports, each reviewed by experts, and 80 of them contain longitudinal annotations to capture disease progression and inter-study intervals, also reviewed by experts. Using this benchmark, we develop a two-stage framework that transforms generated reports into fine-grained, schema-aligned structured representations, enabling longitudinal interpretation. We also propose LUNGUAGESCORE, an interpretable metric that compares structured outputs at the entity, relation, and attribute level while modeling temporal consistency across patient timelines. These contributions establish the first benchmark dataset, structuring framework, and evaluation metric for sequential radiology reporting, with empirical results demonstrating that LUNGUAGESCORE effectively supports structured report evaluation. The code is available at: https://github.com/SuperSupermoon/Lunguage

</details>


### [127] [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)

*Junyan Zhang, Yubo Gao, Yibo Yan, Jungang Li, Zhaorui Hou, Sicheng Tao, Shuliang Liu, Song Dai, Yonghua Hei, Junzhuo Li, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Instruction-following, Sparse components, Computational mechanisms

**Relevance Score:** 9

**TL;DR:** This study explores the effects of fine-tuning on Large Language Models (LLMs), focusing on how it alters their computational mechanisms by examining instruction-specific sparse components.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to understand the computational mechanisms that enhance the instruction-following capabilities of LLMs after finetuning, which remain poorly understood.

**Method:** The authors introduce HexaInst, a dataset with six distinct instructional categories, and SPARCOM, an analytical framework that includes methods for identifying sparse components, evaluating their generality, and comparing alterations across models.

**Key Contributions:**

	1. Introduction of HexaInst, a balanced instructional dataset
	2. Proposal of SPARCOM, a novel analytical framework for studying LLMs
	3. Demonstration of the functional generality and uniqueness of sparse components during instruction execution

**Result:** Experiments show that the identified sparse components exhibit functional generality and uniqueness, highlighting their critical role in effective instruction execution within LLMs post-finetuning.

**Limitations:** 

**Conclusion:** The findings provide insights into the relationship between fine-tuning adaptations and the underlying computational structures, contributing to a more trustworthy LLM landscape.

**Abstract:** The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.

</details>


### [128] [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)

*Roi Cohen, Omri Fahn, Gerard de Melo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty, Hallucinations, Correctness Prediction, Instruction-Tuning

**Relevance Score:** 9

**TL;DR:** This paper investigates how large language models (LLMs) capture uncertainty in their outputs and the implications for predicting correctness in tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capability of LLMs in capturing different types of uncertainty and its correlation with correct and incorrect outputs, especially in the context of hallucinations.

**Method:** The study analyzes the latent space of LLMs to evaluate how they represent uncertainty without explicit training, using various benchmarks to assess performance.

**Key Contributions:**

	1. Identification of multiple types of uncertainty inherent in LLMs
	2. Correlation between uncertainty representation and accuracy of outputs
	3. Recommendation for unifying uncertainty types to improve correctness predictions

**Result:** The results indicate that LLMs capture multiple uncertainty types, which are useful for predicting task correctness and demonstrate a correlation with misinformation avoidance.

**Limitations:** 

**Conclusion:** Unifying the different types of uncertainty through techniques like instruction-tuning enhances the model's ability to predict output correctness.

**Abstract:** Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we study how well LLMs capture uncertainty, without explicitly being trained for that. We show that, if considering uncertainty as a linear concept in the model's latent space, it might indeed be captured, even after only pretraining. We further show that, though unintuitive, LLMs appear to capture several different types of uncertainty, each of which can be useful to predict the correctness for a specific task or benchmark. Furthermore, we provide in-depth results such as demonstrating a correlation between our correction prediction and the model's ability to abstain from misinformation using words, and the lack of impact of model scaling for capturing uncertainty. Finally, we claim that unifying the uncertainty types as a single one using instruction-tuning or [IDK]-token tuning is helpful for the model in terms of correctness prediction.

</details>


### [129] [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/abs/2505.21224)

*Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis*

**Main category:** cs.CL

**Keywords:** NLP, Robustness, Grammatical Error Detection, Machine Translation, Attention Mechanism

**Relevance Score:** 7

**TL;DR:** The paper analyzes how internal model representations evolve in NLP systems, particularly in machine translation, focusing on robustness concerning grammatical errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how robustness manifests in the model representations of NLP systems, particularly for machine translation.

**Method:** The study employs Grammatical Error Detection probing and representational similarity analysis to examine how model representations change in response to ungrammatical inputs.

**Key Contributions:**

	1. Introduces the concept of Robustness Heads in NLP models
	2. Demonstrates how attentional mechanisms aid in correcting grammatical errors
	3. Analyzes representational changes in response to ungrammatical inputs

**Result:** The encoder detects grammatical errors and corrects them by adjusting representations toward the correct form, with specific attention given to Robustness Heads that focus on linguistic units during this process.

**Limitations:** 

**Conclusion:** Robustness Heads play a critical role in adapting representations in response to grammatical errors, and fine-tuning for robustness enhances their importance.

**Abstract:** Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term Robustness Heads. We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on Robustness Heads for updating the ungrammatical word representation.

</details>


### [130] [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/abs/2505.21239)

*Yu He, Zihan Yao, Chentao Song, Tianyu Qi, Jun Liu, Ming Li, Qing Huang*

**Main category:** cs.CL

**Keywords:** Cognitive Diagnosis, Large Language Models, Personalized Learning, Cold-start Challenges, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper proposes LMCD, a novel framework for Cognitive Diagnosis that enhances cold-start performance by leveraging large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Cognitive Diagnosis is essential for personalized learning, but traditional models struggle in cold-start situations due to insufficient data.

**Method:** LMCD utilizes large language models to facilitate Knowledge Diffusion and Semantic-Cognitive Fusion, allowing for better integration of textual information and cognitive states.

**Key Contributions:**

	1. Introduces LMCD as a framework for zero-shot cognitive diagnosis
	2. Employs Knowledge Diffusion and Semantic-Cognitive Fusion techniques
	3. Demonstrates superior performance over state-of-the-art methods.

**Result:** Experiments show that LMCD significantly outperforms existing methods in both exercise-cold and domain-cold settings on real-world datasets.

**Limitations:** Currently a work in progress; practical application may require further validation.

**Conclusion:** LMCD offers a promising approach to enhancing Cognitive Diagnosis through improved semantic understanding and cognitive profiling.

**Abstract:** Cognitive Diagnosis (CD) has become a critical task in AI-empowered education, supporting personalized learning by accurately assessing students' cognitive states. However, traditional CD models often struggle in cold-start scenarios due to the lack of student-exercise interaction data. Recent NLP-based approaches leveraging pre-trained language models (PLMs) have shown promise by utilizing textual features but fail to fully bridge the gap between semantic understanding and cognitive profiling. In this work, we propose Language Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel framework designed to handle cold-start challenges by harnessing large language models (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion, where LLMs generate enriched contents of exercises and knowledge concepts (KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion, where LLMs employ causal attention mechanisms to integrate textual information and student cognitive states, creating comprehensive profiles for both students and exercises. These representations are efficiently trained with off-the-shelf CD models. Experiments on two real-world datasets demonstrate that LMCD significantly outperforms state-of-the-art methods in both exercise-cold and domain-cold settings. The code is publicly available at https://github.com/TAL-auroraX/LMCD

</details>


### [131] [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/abs/2505.21242)

*Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical text summarization, vocabulary adaptation, natural language processing, out-of-vocabulary words

**Relevance Score:** 9

**TL;DR:** This study investigates vocabulary adaptation strategies for enhancing LLM performance in medical text summarization, particularly under challenging conditions with out-of-vocabulary words.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs have shown success in medical text summarization, but evaluations often overlook performance drops in difficult conditions, particularly with out-of-vocabulary words.

**Method:** The paper employs a benchmarking study with extensive experimentation of various vocabulary adaptation strategies and continual pretraining methods across three medical summarization datasets.

**Key Contributions:**

	1. Benchmarking of LLM performance under challenging conditions in medical summarization.
	2. Demonstration of effective vocabulary adaptation strategies for LLMs.
	3. Publicly available codebase for further research and experimentation.

**Result:** Vocabulary adaptation significantly improves LLM performance in summarizing medical texts, especially under challenging conditions, as confirmed by both quantitative and qualitative evaluations.

**Limitations:** 

**Conclusion:** The study highlights the importance of vocabulary adaptation in customizing LLMs for the medical domain, leading to better summarization outcomes as assessed by medical experts.

**Abstract:** Large Language Models (LLMs) recently achieved great success in medical text summarization by simply using in-context learning. However, these recent efforts do not perform fine-grained evaluations under difficult settings where LLMs might fail. They typically report performance scores over the entire dataset. Through our benchmarking study, we show that LLMs show a significant performance drop for data points with high concentration of out-of-vocabulary (OOV) words or with high novelty. Vocabulary adaptation is an intuitive solution to this vocabulary mismatch issue where the LLM vocabulary gets updated with certain expert domain (here, medical) words or subwords. An interesting finding from our study is that Llama-3.1, even with a vocabulary size of around 128K tokens, still faces over-fragmentation issue with medical words. To that end, we show vocabulary adaptation helps improve the LLM summarization performance even in difficult settings. Through extensive experimentation of multiple vocabulary adaptation strategies, two continual pretraining strategies, and three benchmark medical summarization datasets, we gain valuable insights into the role of vocabulary adaptation strategies for customizing LLMs to the medical domain. We also performed a human evaluation study with medical experts where they found that vocabulary adaptation results in more relevant and faithful summaries. Our codebase is made publicly available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.

</details>


### [132] [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/abs/2505.21250)

*Dosung Lee, Wonjun Oh, Boyoung Kim, Minyoung Kim, Joonsuk Park, Paul Hongsuck Seo*

**Main category:** cs.CL

**Keywords:** Multi-hop question answering, Dense retrievers, Large language models, Retrieval performance, Natural language processing

**Relevance Score:** 7

**TL;DR:** Introduces ReSCORE, a method for training dense retrievers in multi-hop question answering (MHQA) without needing labeled documents, using large language models for relevance and consistency assessment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of training dense retrievers for MHQA due to the lack of labeled query-document pairs, which is exacerbated by the variability in reformulated questions.

**Method:** ReSCORE uses large language models to determine each document's relevance to a question and its consistency with the correct answer, training a retriever within an iterative question-answering framework.

**Key Contributions:**

	1. Introduction of ReSCORE for MHQA training without labeled documents.
	2. Utilization of large language models for assessing relevance and consistency.
	3. Demonstration of improved state-of-the-art performance on MHQA benchmarks.

**Result:** Experiments on three MHQA benchmarks show that ReSCORE significantly improves retrieval performance and establishes state-of-the-art results in MHQA tasks.

**Limitations:** 

**Conclusion:** ReSCORE effectively trains dense retrievers for MHQA without labeled documents, demonstrating its potential in enhancing retrieval and question answering.

**Abstract:** Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings; however, they require labeled query-document pairs for fine-tuning. This poses a significant challenge in MHQA due to the high variability of queries (reformulated) questions throughout the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without labeled documents. ReSCORE leverages large language models to capture each documents relevance to the question and consistency with the correct answer and use them to train a retriever within an iterative question-answering framework. Experiments on three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval, and in turn, the state-of-the-art MHQA performance. Our implementation is available at: https://leeds1219.github.io/ReSCORE.

</details>


### [133] [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)

*Ilker Kesen, Jonas F. Lotz, Ingo Ziegler, Phillip Rust, Desmond Elliott*

**Main category:** cs.CL

**Keywords:** pixel language models, multilingual pretraining, semantic tasks, syntactic tasks, cross-lingual transfer

**Relevance Score:** 6

**TL;DR:** PIXEL-M4 is a pixel language model pretrained on English, Hindi, Ukrainian, and Simplified Chinese, showing improved performance in multilingual tasks over an English-only model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore multilingual pretraining in pixel language models and enhance their capabilities for diverse languages beyond fixed vocabulary systems.

**Method:** The PIXEL-M4 model was pretrained on images of rendered text from four diverse languages, followed by evaluations on semantic and syntactic tasks to measure its effectiveness.

**Key Contributions:**

	1. Introduction of the PIXEL-M4 model for multilingual pretraining
	2. Demonstrated superior performance on non-Latin scripts in evaluations
	3. Insights into semantic embedding alignment across languages during pretraining

**Result:** PIXEL-M4 outperformed an English-only model on non-Latin scripts and demonstrated rich feature capture for languages not included in pretraining.

**Limitations:** 

**Conclusion:** Multilingual pretraining significantly improves pixel language model performance, offering better support for a variety of languages.

**Abstract:** Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.

</details>


### [134] [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/abs/2505.21297)

*Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, Mao Yang*

**Main category:** cs.CL

**Keywords:** code reasoning, large language models, dataset, competitive programming, verification

**Relevance Score:** 8

**TL;DR:** The paper presents rStar-Coder, a large-scale verified dataset that enhances code reasoning in LLMs by providing high-difficulty coding challenges and long-reasoning solutions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance code reasoning in large language models, there is a need for high-difficulty datasets with verified input-output test cases that support rigorous solution validation.

**Method:** The methodology involves curating a dataset of 418K competitive programming problems, synthesizing new solvable problems, and implementing a three-step input generation method with a mutual verification mechanism for output labeling.

**Key Contributions:**

	1. Curated dataset of competitive coding problems
	2. Input-output test case synthesis pipeline
	3. Augmented long-reasoning solutions with verification

**Result:** Experiments show that the rStar-Coder dataset significantly improves performance, enhancing various models on key benchmarks, with notable improvements in LiveCodeBench and USA Computing Olympiad results.

**Limitations:** 

**Conclusion:** The rStar-Coder dataset demonstrates substantial effectiveness in boosting the code reasoning capabilities of smaller LLMs, rivaling larger models, and is set to be publicly released.

**Abstract:** Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.

</details>


### [135] [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)

*Andrea Pedrotti, Giulia Rambelli, Caterina Villani, Marianna Bolognesi*

**Main category:** cs.CL

**Keywords:** exemplar generation, category induction, typicality judgment, psycholinguistics, language models

**Relevance Score:** 8

**TL;DR:** This study examines category organization by analyzing subordinate-level exemplars and evaluates the alignment of LLMs with human categorization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the lack of exploration into subordinate-level categorizations and seeks to compare human and LLM-generated exemplars.

**Method:** We created an Italian dataset of human-generated exemplars for 187 concrete words and used it to assess LLMs on tasks of exemplar generation, category induction, and typicality judgment.

**Key Contributions:**

	1. New Italian psycholinguistic dataset of human-generated categorization exemplars.
	2. Evaluation of LLMs across three tasks related to category organization.
	3. Analysis of the alignment between human and LLM-generated exemplars.

**Result:** The study found low alignment between human and LLM-generated exemplars, with performance varying across semantic domains.

**Limitations:** The alignment discrepancies suggest limitations in current LLMs' understanding of human category structures.

**Conclusion:** This research underscores the potential and limitations of AI-generated exemplars in the context of psychological and linguistic studies.

**Abstract:** People can categorize the same entity at multiple taxonomic levels, such as basic (bear), superordinate (animal), and subordinate (grizzly bear). While prior research has focused on basic-level categories, this study is the first attempt to examine the organization of categories by analyzing exemplars produced at the subordinate level. We present a new Italian psycholinguistic dataset of human-generated exemplars for 187 concrete words. We then use these data to evaluate whether textual and vision LLMs produce meaningful exemplars that align with human category organization across three key tasks: exemplar generation, category induction, and typicality judgment. Our findings show a low alignment between humans and LLMs, consistent with previous studies. However, their performance varies notably across different semantic domains. Ultimately, this study highlights both the promises and the constraints of using AI-generated exemplars to support psychological and linguistic research.

</details>


### [136] [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)

*Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow*

**Main category:** cs.CL

**Keywords:** NLP, African languages, large language models, digital divide, inclusivity

**Relevance Score:** 7

**TL;DR:** This survey analyzes 734 research papers on NLP for African languages, providing an overview of progress, trends, and future directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the exclusion of African languages in state-of-the-art NLP systems and LLMs, which risks widening the digital divide.

**Method:** The paper surveys and analyzes 734 research papers published over the past five years in the field of NLP for African languages.

**Key Contributions:**

	1. Comprehensive overview of recent research on NLP for African languages.
	2. Identification of key trends shaping the field.
	3. Suggestions for fostering inclusive NLP research.

**Result:** The analysis identifies key trends in the field and highlights the progress made in core NLP tasks for African languages.

**Limitations:** 

**Conclusion:** The paper calls for more inclusive and sustainable NLP research for African languages and outlines promising future directions.

**Abstract:** With over 2,000 languages and potentially millions of speakers, Africa represents one of the richest linguistic regions in the world. Yet, this diversity is scarcely reflected in state-of-the-art natural language processing (NLP) systems and large language models (LLMs), which predominantly support a narrow set of high-resource languages. This exclusion not only limits the reach and utility of modern NLP technologies but also risks widening the digital divide across linguistic communities. Nevertheless, NLP research on African languages is active and growing. In recent years, there has been a surge of interest in this area, driven by several factors-including the creation of multilingual language resources, the rise of community-led initiatives, and increased support through funding programs. In this survey, we analyze 734 research papers on NLP for African languages published over the past five years, offering a comprehensive overview of recent progress across core tasks. We identify key trends shaping the field and conclude by outlining promising directions to foster more inclusive and sustainable NLP research for African languages.

</details>


### [137] [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/abs/2505.21324)

*Yuxin Zhu, Yuting Guo, Noah Marchuck, Abeed Sarker, Yun Wang*

**Main category:** cs.CL

**Keywords:** ADHD classification, ensemble learning, large language models, machine learning, psychiatric applications

**Relevance Score:** 9

**TL;DR:** This study presents an ensemble framework that combines LLMs and traditional ML techniques to classify ADHD diagnoses from narrative transcripts, demonstrating improved performance over individual models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of LLMs with traditional ML techniques for psychiatric applications remains underexplored despite advances in technology, particularly for complex narrative data.

**Method:** The framework integrates three models: LLaMA3 (an LLM), RoBERTa (a pre-trained transformer), and an SVM classifier, using a majority voting mechanism to enhance classification performance.

**Key Contributions:**

	1. Introduction of a novel ensemble framework for ADHD diagnosis classification
	2. Integration of LLaMA3, RoBERTa, and SVM for enhanced predictive performance
	3. Empirical evidence demonstrating improved recall in ADHD identification

**Result:** The ensemble achieved an F1 score of 0.71, outperforming individual models, particularly improving recall while maintaining competitive precision.

**Limitations:** 

**Conclusion:** Combining LLMs with traditional ML approaches shows promise for robust psychiatric text classification, highlighting the value of hybrid architectures.

**Abstract:** Despite rapid advances in large language models (LLMs), their integration with traditional supervised machine learning (ML) techniques that have proven applicability to medical data remains underexplored. This is particularly true for psychiatric applications, where narrative data often exhibit nuanced linguistic and contextual complexity, and can benefit from the combination of multiple models with differing characteristics. In this study, we introduce an ensemble framework for automatically classifying Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using narrative transcripts. Our approach integrates three complementary models: LLaMA3, an open-source LLM that captures long-range semantic structure; RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical narratives; and a Support Vector Machine (SVM) classifier trained using TF-IDF-based lexical features. These models are aggregated through a majority voting mechanism to enhance predictive robustness. The dataset includes 441 instances, including 352 for training and 89 for validation. Empirical results show that the ensemble outperforms individual models, achieving an F$_1$ score of 0.71 (95\% CI: [0.60-0.80]). Compared to the best-performing individual model (SVM), the ensemble improved recall while maintaining competitive precision. This indicates the strong sensitivity of the ensemble in identifying ADHD-related linguistic cues. These findings demonstrate the promise of hybrid architectures that leverage the semantic richness of LLMs alongside the interpretability and pattern recognition capabilities of traditional supervised ML, offering a new direction for robust and generalizable psychiatric text classification.

</details>


### [138] [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)

*Valentin Knappich, Annemarie Friedrich, Anna HÃ¤tty, Simon Razniewski*

**Main category:** cs.CL

**Keywords:** patent claims, indefiniteness, natural language processing, machine learning, large language models

**Relevance Score:** 5

**TL;DR:** The paper introduces PEDANTIC, a novel dataset of 14k annotated US patent claims, aimed at automating the examination of patent claim definiteness, critical for improving patent application efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the frequent rejections of patent claims due to indefiniteness and the lack of annotated datasets to facilitate automatic examination methods, which could enhance patent drafting and assessment processes.

**Method:** PEDANTIC was constructed using an automated pipeline that retrieves office action documents from the USPTO and employs Large Language Models (LLMs) to extract reasons for indefiniteness, followed by a human validation study to ensure annotation accuracy.

**Key Contributions:**

	1. Introduction of PEDANTIC dataset with 14k US patent claims
	2. Fully automated construction and annotation pipeline using LLMs
	3. Validation of LLM's effectiveness compared to traditional methods in patent claim definiteness prediction.

**Result:** The LLM agents, specifically based on Qwen 2.5 models, were shown to struggle against logistic regression baselines in definiteness prediction, indicating limitations of the models in this specific task.

**Limitations:** LLM agents struggled to outperform simpler logistic regression models in definiteness prediction despite correctly identifying reasons, indicating potential limitations in the LLM's reasoning capabilities for this task.

**Conclusion:** The dataset and code will be publicly released, providing a valuable resource for advancing AI research in patent examination.

**Abstract:** Patent claims define the scope of protection for an invention. If there are ambiguities in a claim, it is rejected by the patent office. In the US, this is referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most frequent reasons for patent application rejection. The development of automatic methods for patent definiteness examination has the potential to make patent drafting and examination more efficient, but no annotated dataset has been published to date.   We introduce PEDANTIC (\underline{P}at\underline{e}nt \underline{D}efiniteness Ex\underline{a}mi\underline{n}a\underline{ti}on \underline{C}orpus), a novel dataset of 14k US patent claims from patent applications relating to Natural Language Processing (NLP), annotated with reasons for indefiniteness. We construct PEDANTIC using a fully automatic pipeline that retrieves office action documents from the USPTO and uses Large Language Models (LLMs) to extract the reasons for indefiniteness. A human validation study confirms the pipeline's accuracy in generating high-quality annotations. To gain insight beyond binary classification metrics, we implement an LLM-as-Judge evaluation that compares the free-form reasoning of every model-cited reason with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B and 72B struggle to outperform logistic regression baselines on definiteness prediction, even though they often correctly identify the underlying reasons. PEDANTIC provides a valuable resource for patent AI researchers, enabling the development of advanced examination models. We will publicly release the dataset and code.

</details>


### [139] [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)

*Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah*

**Main category:** cs.CL

**Keywords:** Bengali, Math Word Problems, Natural Language Processing, Large Language Models, Chain of Thought

**Relevance Score:** 3

**TL;DR:** This paper introduces SOMADHAN, a dataset of 8792 complex Bengali Math Word Problems designed to support NLP reasoning-focused tasks and model development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of human-annotated Bengali datasets for Math Word Problems has hindered progress in applying NLP techniques to this language, which is classified as low-resource.

**Method:** A dataset of 8792 Bengali MWPs was created with manually written solutions; various large language models were evaluated using standard and Chain of Thought (CoT) prompting techniques, along with fine-tuning via Low-Rank Adaptation (LoRA).

**Key Contributions:**

	1. Introduction of SOMADHAN dataset for Bengali MWPs
	2. Demonstration of CoT prompting effectiveness for improved model performance
	3. Application of LoRA for efficient model fine-tuning

**Result:** LLaMA-3.3 70B achieved the highest accuracy at 88% using few-shot CoT prompting, demonstrating enhanced reasoning capabilities.

**Limitations:** 

**Conclusion:** This work provides a necessary dataset and framework to advance research in Bengali NLP, particularly in mathematical reasoning, and aims to promote equitable access to educational technologies.

**Abstract:** Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.

</details>


### [140] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)

*Qishuai Zhong, Zongmin Li, Siqi Fan, Aixin Sun*

**Main category:** cs.CL

**Keywords:** large language models, sociodemographics, multi-turn dialogue, reasoning, value expression

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework for evaluating how large language models (LLMs) adapt their responses based on users' sociodemographic attributes across different interaction modalities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how LLMs can effectively engage with users by adapting their responses according to sociodemographic characteristics, an area that has not been fully explored in existing evaluations.

**Method:** The study employs a multi-agent pipeline to construct a synthetic dataset that pairs dialogue histories with distinct user profiles. Responses are analyzed when user attributes are introduced explicitly through profiles and implicitly via multi-turn dialogue history.

**Key Contributions:**

	1. Proposes a new framework for evaluating LLM adaptation to user sociodemographics
	2. Constructs a synthetic dataset for testing LLM responses across modalities
	3. Finds that reasoning capabilities influence LLMs' consistency in responses to demographic changes.

**Result:** Most LLMs adjust their values in response to demographic changes, particularly regarding age and education level, although the consistency of this adaptation varies across models.

**Limitations:** 

**Conclusion:** The research highlights that LLMs with stronger reasoning capabilities show greater alignment in their responses to sociodemographic changes, indicating reasoning is crucial for effective adaptation.

**Abstract:** Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.

</details>


### [141] [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/abs/2505.21378)

*Jules Watson, Xi Wang, Raymond Liu, Suzanne Stevenson, Barend Beekhuizen*

**Main category:** cs.CL

**Keywords:** LLM, gendered language, sociolinguistics, feminism, trans inclusivity

**Relevance Score:** 7

**TL;DR:** This paper explores how LLMs revise gendered role nouns, assessing their justifications against feminist and trans-inclusive language reforms, and evaluates LLM sensitivity to contextual effects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand LLMs' ability to revise gendered language and ensure alignment with contemporary language reforms in support of feminism and trans inclusivity.

**Method:** The study utilizes sociolinguistic insights to analyze LLMs' revisions of gendered nouns and their justifications, evaluating contextual sensitivity comparable to human responses.

**Key Contributions:**

	1. Analysis of LLMs' gendered language revision capabilities
	2. Evaluation of justifications for revisions from a sociolinguistic perspective
	3. Evidence of contextual sensitivity in LLM responses

**Result:** The findings reveal that LLMs demonstrate evidence of sensitivity to contextual effects in gendered language revision, aligning with feminist and trans-inclusive reforms.

**Limitations:** 

**Conclusion:** The study highlights important implications for the alignment of LLMs with societal values around gender and inclusivity in language.

**Abstract:** Within the common LLM use case of text revision, we study LLMs' revision of gendered role nouns (e.g., outdoorsperson/woman/man) and their justifications of such revisions. We evaluate their alignment with feminist and trans-inclusive language reforms for English. Drawing on insight from sociolinguistics, we further assess if LLMs are sensitive to the same contextual effects in the application of such reforms as people are, finding broad evidence of such effects. We discuss implications for value alignment.

</details>


### [142] [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/abs/2505.21380)

*Byungjun Kim, Minju Kim, Hyeonchu Park, Bugeun Kim*

**Main category:** cs.CL

**Keywords:** hate speech detection, phonetic substitution, Korean language, adversarial behavior, machine learning

**Relevance Score:** 4

**TL;DR:** The paper addresses the challenges of hate speech detection in the Korean language by proposing novel methods that leverage phonetic characteristics to improve detection robustness against evasion tactics.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Malicious users employ phonetic substitutions to evade hate speech detection, with a gap in prior research regarding the Korean language and the need for architectural defenses.

**Method:** The paper proposes PHonetic-Informed Substitution for Hangul (PHISH) and Mixed Encoding of Semantic-pHonetic features (MESH), which utilize the phonological traits of Korean to enhance detection systems.

**Key Contributions:**

	1. Introduction of PHISH to leverage phonetic characteristics of Hangul for detection.
	2. Development of MESH to incorporate phonetic information in the detector architecture.
	3. Empirical validation of the proposed methods' effectiveness against both perturbed and unperturbed data.

**Result:** The proposed methods show improved detection performance on both perturbed and unperturbed datasets, validating their effectiveness in combating adversarial evasion tactics.

**Limitations:** 

**Conclusion:** The findings indicate that the methods not only enhance detection capabilities but also align with realistic adversarial behaviors employed by users.

**Abstract:** As malicious users increasingly employ phonetic substitution to evade hate speech detection, researchers have investigated such strategies. However, two key challenges remain. First, existing studies have overlooked the Korean language, despite its vulnerability to phonetic perturbations due to its phonographic nature. Second, prior work has primarily focused on constructing datasets rather than developing architectural defenses. To address these challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH) that exploits the phonological characteristics of the Korean writing system, and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the detector's robustness by incorporating phonetic information at the architectural level. Our experimental results demonstrate the effectiveness of our proposed methods on both perturbed and unperturbed datasets, suggesting that they not only improve detection performance but also reflect realistic adversarial behaviors employed by malicious users.

</details>


### [143] [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/abs/2505.21389)

*Xuanwen Ding, Chengjun Pan, Zejun Li, Jiwen Zhang, Siyuan Wang, Zhongyu Wei*

**Main category:** cs.CL

**Keywords:** multimodal large language models, benchmarking, Item Response Theory

**Relevance Score:** 8

**TL;DR:** AutoJudger is an agent-driven framework designed for efficient and adaptive benchmarking of multimodal large language models (MLLMs), significantly reducing evaluation costs while maintaining high accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating MLLMs is becoming increasingly expensive due to the complexity and size of benchmarks.

**Method:** AutoJudger employs Item Response Theory (IRT) to estimate question difficulty and uses an autonomous evaluation agent to select informative test questions based on real-time model performance, along with a semantic-aware retrieval mechanism and a dynamic memory for question selection.

**Key Contributions:**

	1. Introduction of AutoJudger for adaptive benchmarking of MLLMs
	2. Use of Item Response Theory for difficulty estimation
	3. Innovative retrieval mechanism and dynamic memory for question selection

**Result:** AutoJudger dramatically reduces evaluation expenses, using only 4% of the data to achieve over 90% ranking accuracy compared to full benchmark evaluations.

**Limitations:** 

**Conclusion:** The proposed framework provides a cost-effective solution for efficiently evaluating MLLMs while maintaining high accuracy in assessments.

**Abstract:** Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model's real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench.

</details>


### [144] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)

*Xiao Liu, Xinyi Dong, Xinyang Gao, Yansong Feng, Xun Pang*

**Main category:** cs.CL

**Keywords:** large language models, idea generation, metadata, automatic validation, social sciences

**Relevance Score:** 9

**TL;DR:** This paper investigates how enhancing LLMs with data can improve the feasibility and quality of generated research ideas, specifically in social sciences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve the feasibility and effectiveness of research ideas generated by LLMs.

**Method:** The paper introduces two methods: 1) metadata to guide LLMs during idea generation and 2) automatic validation for assessing the plausibility of ideas.

**Key Contributions:**

	1. Demonstrated that metadata enhances idea feasibility in LLM outputs.
	2. Showed that automatic validation improves the quality assessment of LLM-generated ideas.
	3. Provided empirical evidence supporting data-driven idea generation in research settings.

**Result:** Experiments show that metadata improves idea feasibility by 20% and automatic validation enhances selected idea quality by 7%.

**Limitations:** Experiments were conducted in a specific domain (climate negotiation) which may limit generalizability.

**Conclusion:** The integration of data into LLM-assisted ideation significantly boosts the quality of research ideas in practical academic contexts.

**Abstract:** Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.

</details>


### [145] [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)

*Xiusi Chen, Shanyong Wang, Cheng Qian, Hongru Wang, Peixuan Han, Heng Ji*

**Main category:** cs.CL

**Keywords:** decision-making, explainable AI, language models, structured reasoning, symbolic reasoning

**Relevance Score:** 9

**TL;DR:** DecisionFlow is a new framework that enhances decision-making in high-stakes domains by promoting structured reasoning and explainability in language models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Effective decision-making in healthcare and finance requires both accurate outcomes and transparent reasoning, which current language models often lack.

**Method:** DecisionFlow guides models to reason over structured representations, building a semantically grounded decision space and inferring a latent utility function to evaluate trade-offs.

**Key Contributions:**

	1. Introduction of the DecisionFlow framework for structured decision-making
	2. Empirical results demonstrating significant accuracy improvements over existing methods
	3. Enhanced interpretability of decisions through grounded rationales

**Result:** DecisionFlow achieves up to 30% accuracy gains and improves alignment in outcomes on two high-stakes benchmarks compared to strong prompting baselines.

**Limitations:** 

**Conclusion:** This work integrates symbolic reasoning with LLMs, aiming for more accountable, explainable, and reliable LLM decision support systems.

**Abstract:** In high-stakes domains such as healthcare and finance, effective decision-making demands not just accurate outcomes but transparent and explainable reasoning. However, current language models often lack the structured deliberation needed for such tasks, instead generating decisions and justifications in a disconnected, post-hoc manner. To address this, we propose DecisionFlow, a novel decision modeling framework that guides models to reason over structured representations of actions, attributes, and constraints. Rather than predicting answers directly from prompts, DecisionFlow builds a semantically grounded decision space and infers a latent utility function to evaluate trade-offs in a transparent, utility-driven manner. This process produces decisions tightly coupled with interpretable rationales reflecting the model's reasoning. Empirical results on two high-stakes benchmarks show that DecisionFlow not only achieves up to 30% accuracy gains over strong prompting baselines but also enhances alignment in outcomes. Our work is a critical step toward integrating symbolic reasoning with LLMs, enabling more accountable, explainable, and reliable LLM decision support systems. We release the data and code at https://github.com/xiusic/DecisionFlow.

</details>


### [146] [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)

*Hovhannes Tamoyan, Subhabrata Dutta, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Large Language Models, Factual Correctness, Self-Monitoring, Interpretability, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the internal mechanisms of large language models (LLMs) that enable them to assess the factual correctness of their generated content, suggesting the existence of self-monitoring capabilities within these models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Factual incorrectness in generated content from LLMs is a significant concern, necessitating a better understanding of how these models assess correctness during generation.

**Method:** The authors examine the linear features encoded in the Transformerâs residual stream of LLMs at the time of generation, which indicate the model's ability to recall correct attributes related to entity-relation-attribute triplets. They explore the impact of different contexts and select various examples for experiments across model sizes.

**Key Contributions:**

	1. Demonstration of LLMs' internal compass for factual correctness during content generation
	2. Identification of linear features in the Transformer's residual stream associated with correct recall
	3. Insights into self-awareness emergence and training dynamics of LLMs.

**Result:** The study provides evidence that LLMs possess an internal compass for factual recall, with self-awareness capabilities emerging quickly during training and peaking in intermediate layers.

**Limitations:** Further exploration is needed on how this self-monitoring ability can be generalized across various domains and types of content generation.

**Conclusion:** These findings enhance the interpretability and reliability of LLMs by uncovering their intrinsic self-monitoring capabilities.

**Abstract:** Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.

</details>


### [147] [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)

*Dario Satriani, Enzo Veltri, Donatello Santoro, Paolo Papotti*

**Main category:** cs.CL

**Keywords:** Large Language Models, factuality, RelationalFactQA, tabular outputs, knowledge retrieval

**Relevance Score:** 8

**TL;DR:** The paper introduces RelationalFactQA, a benchmark for evaluating the ability of Large Language Models (LLMs) to generate structured, multi-record tabular outputs from factual knowledge, highlighting their limitations in this area.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of factuality in LLMs, especially their ability to generate structured outputs rather than just short factual answers, which current benchmarks often overlook.

**Method:** Introduces the RelationalFactQA benchmark featuring diverse natural language questions paired with SQL and gold-standard tabular answers to evaluate relational fact retrieval capabilities.

**Key Contributions:**

	1. Development of the RelationalFactQA benchmark
	2. Identification of challenges faced by LLMs in generating structured tabular outputs
	3. Provision of insights into the limitations of current LLMs' factual accuracy

**Result:** Even state-of-the-art LLMs struggle to achieve over 25% factual accuracy in generating relational outputs, with performance decreasing as the dimensionality of output increases.

**Limitations:** The paper does not explore the potential for improving LLM performance on relational output generation.

**Conclusion:** The findings reveal critical limitations in current LLMs' ability to synthesize structured factual knowledge and position RelationalFactQA as an essential tool for future research in this area.

**Abstract:** Factuality in Large Language Models (LLMs) is a persistent challenge. Current benchmarks often assess short factual answers, overlooking the critical ability to generate structured, multi-record tabular outputs from parametric knowledge. We demonstrate that this relational fact retrieval is substantially more difficult than isolated point-wise queries, even when individual facts are known to the model, exposing distinct failure modes sensitive to output dimensionality (e.g., number of attributes or records). To systematically evaluate this under-explored capability, we introduce RelationalFactQA, a new benchmark featuring diverse natural language questions (paired with SQL) and gold-standard tabular answers, specifically designed to assess knowledge retrieval in a structured format. RelationalFactQA enables analysis across varying query complexities, output sizes, and data characteristics. Our experiments reveal that even state-of-the-art LLMs struggle significantly, not exceeding 25% factual accuracy in generating relational outputs, with performance notably degrading as output dimensionality increases. These findings underscore critical limitations in current LLMs' ability to synthesize structured factual knowledge and establish RelationalFactQA as a crucial resource for measuring future progress in LLM factuality.

</details>


### [148] [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)

*Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, load balancing, large language models, Ascend NPUs, inference performance

**Relevance Score:** 5

**TL;DR:** Introduces Mixture of Grouped Experts (MoGE) for improved load balancing in large language models, enhancing throughput especially during inference.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in Mixture of Experts (MoE) due to unequal activation of experts, leading to computational load imbalances in a parallel processing environment.

**Method:** Developed MoGE that groups experts and constrains token activation to balance expert workloads, implemented on Pangu Pro MoE optimized for Ascend NPUs.

**Key Contributions:**

	1. Introduction of Mixture of Grouped Experts (MoGE) for load balancing
	2. Development and optimization of Pangu Pro MoE on Ascend NPUs
	3. Significantly improved inference performance compared to existing dense models.

**Result:** Pangu Pro MoE shows enhanced expert load balancing and improved execution efficiency, achieving up to 1528 tokens/s per card during inference, outperforming existing models.

**Limitations:** 

**Conclusion:** MoGE enhances the performance and efficiency of large language models by ensuring balanced computational loads across devices, making it a leading approach for training large models.

**Abstract:** The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo.Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.

</details>


### [149] [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)

*Xiao Liu, Da Yin, Zirui Wu, Yansong Feng*

**Main category:** cs.CL

**Keywords:** large language models, tool creation, human-computer interaction

**Relevance Score:** 8

**TL;DR:** RefTool is a framework that enhances LLMs' problem-solving abilities by generating tools from structured external materials, ensuring better tool creation and utilization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address limitations of LLMs in generating tools for complex problem-solving tasks when predefined tools are unavailable.

**Method:** RefTool uses a two-module system: tool creation generates executable tools from reference content, and tool utilization involves navigating a toolbox to apply appropriate tools for problem-solving.

**Key Contributions:**

	1. Introduction of RefTool framework for automatic tool creation using external references
	2. Demonstration of improved accuracy and generalizability in problem-solving
	3. Utilization of a hierarchical toolbox structure for effective tool selection

**Result:** RefTool outperforms existing methods by an average accuracy of 11.3% in causality, physics, and chemistry benchmarks while being cost-efficient.

**Limitations:** 

**Conclusion:** Grounding tool creation in structured external references allows LLMs to surpass their knowledge limitations and enhances generalizable reasoning.

**Abstract:** Tools enhance the reasoning capabilities of large language models (LLMs) in complex problem-solving tasks, but not all tasks have available tools. In the absence of predefined tools, prior works have explored instructing LLMs to generate tools on their own. However, such approaches rely heavily on the models' internal knowledge and would fail in domains beyond the LLMs' knowledge scope. To address this limitation, we propose RefTool, a reference-guided framework for automatic tool creation that leverages structured external materials such as textbooks. RefTool consists of two modules: (1) tool creation, where LLMs generate executable tools from reference content, validate them using illustrative examples, and organize them hierarchically into a toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to select and apply the appropriate tools to solve problems. Experiments on causality, physics, and chemistry benchmarks demonstrate that RefTool outperforms existing tool-creation and domain-specific reasoning methods by 11.3% on average accuracy, while being cost-efficient and broadly generalizable. Analyses reveal that grounding tool creation in references produces accurate and faithful tools, and that the hierarchical structure facilitates effective tool selection. RefTool enables LLMs to overcome knowledge limitations, demonstrating the value of grounding tool creation in external references for enhanced and generalizable reasoning.

</details>


### [150] [Towards Better Instruction Following Retrieval Models](https://arxiv.org/abs/2505.21439)

*Yuchen Zhuang, Aaron Trinh, Rushi Qiang, Haotian Sun, Chao Zhang, Hanjun Dai, Bo Dai*

**Main category:** cs.CL

**Keywords:** Retrieval Models, Embedding, Instruction-Following IR

**Relevance Score:** 8

**TL;DR:** Introducing InF-IR, a new training corpus for enhancing instruction-following in information retrieval systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Modern IR models struggle to interpret and follow explicit user instructions effectively, necessitating improved training corpora.

**Method:** The paper presents InF-IR, a dataset of over 38,000 <instruction, query, passage> triplets along with hard negative examples generated by poisoning inputs, aimed at training an embedding model called InF-Embed.

**Key Contributions:**

	1. Development of the InF-IR dataset tailored for instruction-following IR
	2. Introduction of hard negative examples to improve model robustness
	3. Training of InF-Embed that aligns retrieval outcomes with user intents effectively.

**Result:** InF-Embed is trained using contrastive learning and instruction-query attention mechanisms, achieving an 8.1% improvement in p-MRR over competitive baselines across five instruction-based retrieval benchmarks.

**Limitations:** 

**Conclusion:** The InF-IR corpus allows for more effective instruction-following capabilities in retrieval models, enhancing both efficiency and accuracy of information retrieval.

**Abstract:** Modern information retrieval (IR) models, trained exclusively on standard <query, passage> pairs, struggle to effectively interpret and follow explicit user instructions. We introduce InF-IR, a large-scale, high-quality training corpus tailored for enhancing retrieval models in Instruction-Following IR. InF-IR expands traditional training pairs into over 38,000 expressive <instruction, query, passage> triplets as positive samples. In particular, for each positive triplet, we generate two additional hard negative examples by poisoning both instructions and queries, then rigorously validated by an advanced reasoning model (o3-mini) to ensure semantic plausibility while maintaining instructional incorrectness. Unlike existing corpora that primarily support computationally intensive reranking tasks for decoder-only language models, the highly contrastive positive-negative triplets in InF-IR further enable efficient representation learning for smaller encoder-only models, facilitating direct embedding-based retrieval. Using this corpus, we train InF-Embed, an instruction-aware Embedding model optimized through contrastive learning and instruction-query attention mechanisms to align retrieval outcomes precisely with user intents. Extensive experiments across five instruction-based retrieval benchmarks demonstrate that InF-Embed significantly surpasses competitive baselines by 8.1% in p-MRR, measuring the instruction-following capabilities.

</details>


### [151] [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)

*Jocelyn Shen, Akhila Yerukola, Xuhui Zhou, Cynthia Breazeal, Maarten Sap, Hae Won Park*

**Main category:** cs.CL

**Keywords:** Conversational breakdowns, Nonviolent communication, Large language models, Human-computer interaction, Conflict detection

**Relevance Score:** 8

**TL;DR:** This paper studies conversational breakdowns in close relationships, using nonviolent communication theory and a new dataset to evaluate LLM performance in detecting conflict influenced by personal backstories.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the gap in NLP research regarding how personal histories and emotional context shape perceptions of conversational breakdowns in close relationships.

**Method:** The authors created the PersonaConflicts Corpus, comprising 5,772 simulated dialogues reflecting various conflict situations. A controlled human study was conducted for fine-grained labeling of communication breakdowns, assessing the influence of relationship backstories on conflict perception.

**Key Contributions:**

	1. Introduction of the PersonaConflicts Corpus for studying conversational conflicts
	2. Evidence that relationship backstories influence human perceptions of breakdowns
	3. Insights into LLM limitations in leveraging relationship context in conflict detection

**Result:** The study found that relationship backstories significantly affect human perceptions of communication breakdowns and social partner impressions, while models struggled to utilize backstories effectively and tended to overestimate positive listener reactions to messages.

**Limitations:** The dataset is simulated and may not fully capture real-world conversational dynamics.

**Conclusion:** The findings highlight the importance of personalizing LLM interactions based on relationship contexts to improve their effectiveness as mediators in human communication.

**Abstract:** Conversational breakdowns in close relationships are deeply shaped by personal histories and emotional context, yet most NLP research treats conflict detection as a general task, overlooking the relational dynamics that influence how messages are perceived. In this work, we leverage nonviolent communication (NVC) theory to evaluate LLMs in detecting conversational breakdowns and assessing how relationship backstory influences both human and model perception of conflicts. Given the sensitivity and scarcity of real-world datasets featuring conflict between familiar social partners with rich personal backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772 naturalistic simulated dialogues spanning diverse conflict scenarios between friends, family members, and romantic partners. Through a controlled human study, we annotate a subset of dialogues and obtain fine-grained labels of communication breakdown types on individual turns, and assess the impact of backstory on human and model perception of conflict in conversation. We find that the polarity of relationship backstories significantly shifted human perception of communication breakdowns and impressions of the social partners, yet models struggle to meaningfully leverage those backstories in the detection task. Additionally, we find that models consistently overestimate how positively a message will make a listener feel. Our findings underscore the critical role of personalization to relationship contexts in enabling LLMs to serve as effective mediators in human communication for authentic connection.

</details>


### [152] [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/abs/2505.21458)

*Shintaro Ozaki, Tatsuya Hiraoka, Hiroto Otake, Hiroki Ouchi, Masaru Isonuma, Benjamin Heinzerling, Kentaro Inui, Taro Watanabe, Yusuke Miyao, Yohei Oseki, Yu Takagi*

**Main category:** cs.CL

**Keywords:** Large Language Models, latent language, task performance, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** This study investigates the effect of latent language consistency on downstream task performance in Large Language Models (LLMs), revealing that while consistency is hypothesized to enhance performance, it may not always be necessary across diverse tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the discrepancy between latent language and input/output languages affects the performance of LLMs in downstream tasks, which has been largely unexamined in existing literature.

**Method:** The research varies input prompt languages across multiple downstream tasks, analyzing the correlation between latent language consistency and task performance. Datasets include diverse questions from areas such as translation and geo-culture.

**Key Contributions:**

	1. Investigates the impact of latent language consistency on task performance in LLMs.
	2. Presents experimental results across diverse domains, showing adaptability of models.
	3. Challenges the assumption that consistency in latent language always enhances performance.

**Result:** Experimental results on multiple LLMs indicate that consistency in latent language does not always lead to optimal downstream task performance, as the models can adapt their internal representations to suit the target language.

**Limitations:** The analysis primarily focuses on specific tasks (translation and geo-culture) and may not generalize to all LLM applications.

**Conclusion:** Maintaining consistency in latent language is not necessary for optimal performance on tasks sensitive to language choice, due to model adaptability at the final layers.

**Abstract:** Large Language Models (LLMs) are known to process information using a proficient internal language consistently, referred to as latent language, which may differ from the input or output languages. However, how the discrepancy between the latent language and the input and output language affects downstream task performance remains largely unexplored. While many studies research the latent language of LLMs, few address its importance in influencing task performance. In our study, we hypothesize that thinking in latent language consistently enhances downstream task performance. To validate this, our work varies the input prompt languages across multiple downstream tasks and analyzes the correlation between consistency in latent language and task performance. We create datasets consisting of questions from diverse domains such as translation and geo-culture, which are influenced by the choice of latent language. Experimental results across multiple LLMs on translation and geo-culture tasks, which are sensitive to the choice of language, indicate that maintaining consistency in latent language is not always necessary for optimal downstream task performance. This is because these models adapt their internal representations near the final layers to match the target language, reducing the impact of consistency on overall performance.

</details>


### [153] [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)

*Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta*

**Main category:** cs.CL

**Keywords:** diffusion language models, autoregressive models, inference optimization

**Relevance Score:** 8

**TL;DR:** This paper presents methods to enhance the efficiency of diffusion language models by introducing FreeCache and Guided Diffusion, achieving significantly faster inference while maintaining accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the slow inference times and high computational costs of current state-of-the-art diffusion language models compared to autoregressive approaches.

**Method:** Two training-free techniques are proposed: FreeCache for Key-Value approximation caching to reuse KV projections across denoising steps, and Guided Diffusion which utilizes a pretrained autoregressive model to supervise token unmasking.

**Key Contributions:**

	1. Introduction of FreeCache for efficient KV caching during denoising steps.
	2. Development of Guided Diffusion to reduce denoising iterations using an autoregressive model.
	3. Demonstration of a 34x speedup in inference times for diffusion language models.

**Result:** The proposed methods result in up to a 34x end-to-end speedup in inference times without compromising accuracy, enabling diffusion language models to achieve competitive latency with autoregressive models.

**Limitations:** 

**Conclusion:** The introduction of FreeCache and Guided Diffusion has the potential to expand the applicability of diffusion language models across various domains due to improved performance.

**Abstract:** Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver up to a 34x end-to-end speedup without compromising accuracy. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains.

</details>


### [154] [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/abs/2505.21471)

*Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu*

**Main category:** cs.CL

**Keywords:** multi-agent framework, large language models, knowledge integration, information retrieval, scalability

**Relevance Score:** 8

**TL;DR:** This paper presents ExtAgents, a multi-agent framework that improves scalability in LLM inference-time knowledge integration without requiring longer-context training.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The limited context window of LLMs hinders their ability to utilize external knowledge effectively, which is crucial for complex tasks that require significant information.

**Method:** The authors developed a multi-agent framework that addresses knowledge synchronization and reasoning bottlenecks in existing methods, enabling better handling of external knowledge inputs.

**Key Contributions:**

	1. Development of a multi-agent framework (ExtAgents) for LLMs.
	2. Enhanced performance in multi-hop question answering tasks over existing non-training methods.
	3. High efficiency through parallelism allowing better scalability.

**Result:** ExtAgents significantly outperforms existing methods on benchmark tests, including enhanced multi-hop question answering, while maintaining high efficiency through parallelism.

**Limitations:** Information loss in existing context window extension methods; the framework's effectiveness is tested in specific scenarios and may require further exploration in real-world applications.

**Conclusion:** The study suggests that improving coordination among LLM agents to manage larger inputs could have valuable real-world applications.

**Abstract:** With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, $\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $\textbf{$\boldsymbol{\infty}$Bench+}$, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls $\textit{within or exceeds the context window}$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.

</details>


### [155] [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)

*Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard SchÃ¶lkopf, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Moral Reasoning, Ethics, Human-Computer Interaction, Consequentialism

**Relevance Score:** 8

**TL;DR:** This study analyzes moral reasoning in large language models (LLMs) using over 600 trolley problems, revealing a preference for deontological principles in reasoning traces, while explanations tend to align with consequentialist rationales.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI systems handle ethically complex scenarios, particularly in high-stakes applications like healthcare and governance, and to address the gap in analyzing the moral reasoning process of LLMs.

**Method:** A large-scale analysis of the reasoning traces provided by LLMs based on over 600 distinct trolley problems, using a taxonomy of moral rationales tied to consequentialism and deontology.

**Key Contributions:**

	1. Introduced a taxonomy of moral rationales for analyzing LLM reasoning.
	2. Conducted large-scale analysis using over 600 trolley problems.
	3. Demonstrated differing reasoning patterns between chains-of-thought and explanations in LLMs.

**Result:** The analysis shows that LLM chains-of-thought lean toward deontological principles focused on moral obligations, whereas post-hoc explanations shift to consequentialist rationales emphasizing utility.

**Limitations:** 

**Conclusion:** Understanding how LLMs articulate ethical considerations is crucial for their safe deployment in decision-making contexts, and the developed framework serves as a foundation for this understanding.

**Abstract:** As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .

</details>


### [156] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)

*Han Xiao, Guozhi Wang, Yuxiang Chai, Zimu Lu, Weifeng Lin, Hao He, Lue Fan, Liuyang Bian, Rui Hu, Liang Liu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Aojun Zhou, Hongsheng Li*

**Main category:** cs.CL

**Keywords:** GUI agents, self-improvement, reward model

**Relevance Score:** 6

**TL;DR:** Introducing UI-Genie, a self-improving framework for GUI agents that addresses challenges in trajectory verification and scalable training data through a novel reward model and data generation strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle challenges related to the verification of trajectory outcomes and the scalability of high-quality training data for GUI agents.

**Method:** The authors developed a reward model called UI-Genie-RM utilizing an image-text interleaved architecture, along with self-improvement pipelines to enhance agent performance through reward-guided exploration and outcome verification.

**Key Contributions:**

	1. Introduction of a novel UI-Genie framework for self-improving GUI agents
	2. Development of UI-Genie-RM, a reward model with an innovative architecture
	3. Creation of the first reward-specific dataset for GUI agents

**Result:** UI-Genie demonstrates state-of-the-art performance across multiple benchmarks for GUI agents, validated through a series of experiments and the creation of a unique dataset (UI-Genie-RM-517k) for training.

**Limitations:** 

**Conclusion:** The complete framework and generated datasets have been open-sourced to promote further research in the field of GUI agents.

**Abstract:** In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.

</details>


### [157] [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)

*Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng*

**Main category:** cs.CL

**Keywords:** large language models, clinical reasoning, collaborative systems, Catfish Agent, Silent Agreement

**Relevance Score:** 9

**TL;DR:** The paper introduces the Catfish Agent, a role-specialized LLM aimed at addressing the issue of Silent Agreement in multi-agent clinical diagnostic systems, enhancing reasoning through structured dissent.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issue of Silent Agreement in collaborative diagnostic frameworks using LLMs, which can hinder critical analysis and accurate diagnosis.

**Method:** The Catfish Agent employs two mechanisms: a complexity-aware intervention that adjusts agent engagement based on case difficulty, and a tone-calibrated intervention that balances critique with collaboration.

**Key Contributions:**

	1. Introduction of the Catfish Agent concept for structured dissent in LLM frameworks.
	2. Development of complexity-aware and tone-calibrated intervention mechanisms.
	3. Demonstrated improved performance in clinical question answering benchmarks.

**Result:** Evaluations show that the Catfish Agent consistently outperforms existing single and multi-agent LLM frameworks, including advanced models like GPT-4o and DeepSeek-R1 across multiple medical benchmarks.

**Limitations:** 

**Conclusion:** The Catfish Agent enhances diagnostic accuracy by fostering deeper reasoning through structured dissent, improving the collaborative process in clinical settings.

**Abstract:** Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.

</details>


### [158] [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)

*Shimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie She, Xiao Liu, Yeyun Gong, Shujian Huang, Jiajun Chen*

**Main category:** cs.CL

**Keywords:** Multilingual Alignment, LLMs, language-specific neurons, language-agnostic neurons, neuron identification

**Relevance Score:** 9

**TL;DR:** This paper proposes a new algorithm for identifying language-specific and language-agnostic neurons in LLMs, enhancing understanding of multilingual capabilities and alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance multilingual capabilities of LLMs by analyzing language-specific neurons and improving alignment from high-resource to low-resource languages.

**Method:** A new finer-grained neuron identification algorithm is introduced to detect language-specific and agnostic neurons, followed by an analysis of LLM processes during multilingual inference.

**Key Contributions:**

	1. Introduction of a novel neuron identification algorithm for LLMs
	2. Detailed analysis of multilingual inference processes
	3. Insights into spontaneous multilingual alignment phenomenon.

**Result:** Empirical results provide insights into how different neuron types function in multilingual contexts and the phenomenon of spontaneous multilingual alignment.

**Limitations:** 

**Conclusion:** The investigation offers valuable insights into multilingual alignment and capabilities of LLMs, contributing to a deeper understanding of their mechanisms.

**Abstract:** Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.

</details>


### [159] [WizardLM: Empowering large pre-trained language models to follow complex instructions](https://arxiv.org/abs/2304.12244)

*Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang*

**Main category:** cs.CL

**Keywords:** large language model, instruction fine-tune, Evol-Instruct, WizardLM, auto-generated instructions

**Relevance Score:** 9

**TL;DR:** This paper presents Evol-Instruct, a method for automatically generating instruction data using LLMs, which is then used to fine-tune the LLaMA model resulting in WizardLM, outperforming human-generated instructions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of manually creating open-domain instruction data for LLMs and the complexity of human-generated instructions necessitates an automated approach.

**Method:** The authors propose Evol-Instruct, which iteratively rewrites an initial set of simple instructions into more complex ones. This is used to generate a diverse instruction dataset for fine-tuning LLaMA to create WizardLM.

**Key Contributions:**

	1. Introduction of Evol-Instruct for generating complex instructions using LLMs
	2. Development of WizardLM that outperforms human-generated instructions
	3. Public release of the code and data to facilitate further research.

**Result:** WizardLM is shown to produce superior outputs compared to human-created instructions in human evaluations, and it performs at over 90% of ChatGPT's capacity in automatic evaluations on various skills.

**Limitations:** WizardLM still underperforms compared to ChatGPT in certain aspects, indicating room for improvement.

**Conclusion:** The findings indicate that fine-tuning LLMs with AI-generated evolving instructions is a promising strategy for improving model performance, despite some gaps compared to existing models like ChatGPT.

**Abstract:** Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM

</details>


### [160] [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568)

*Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang*

**Main category:** cs.CL

**Keywords:** Large Language model, Code Generation, Instruction fine-tuning

**Relevance Score:** 9

**TL;DR:** Introduction of WizardCoder, a Code LLM with instruction fine-tuning that outperforms existing models in code generation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of Code LLMs by introducing instruction fine-tuning based on the Evol-Instruct method for better handling of code-related tasks.

**Method:** WizardCoder implements complex instruction fine-tuning on Code LLMs and is evaluated on prominent benchmarks including HumanEval, HumanEval+, MBPP, and DS-1000.

**Key Contributions:**

	1. Introduction of instruction fine-tuning for Code LLMs
	2. Significant performance improvements on multiple code generation benchmarks
	3. Public availability of model resources

**Result:** WizardCoder surpasses all existing open-source Code LLMs and even outperforms large closed LLMs like Claude and Bard on specific benchmarks.

**Limitations:** 

**Conclusion:** The model demonstrates superior capabilities in code generation and offers public access to its code, model weights, and data.

**Abstract:** Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM

</details>


### [161] [Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods](https://arxiv.org/abs/2401.16332)

*Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine, Amnon Shashua*

**Main category:** cs.CL

**Keywords:** alignment, representation engineering, language models, helpfulness, AI safety

**Relevance Score:** 9

**TL;DR:** This paper examines the tradeoff between alignment and helpfulness in language models using representation engineering, proposing a theoretical framework and empirical validation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the balance between improving human-model alignment and maintaining the model's helpfulness through representation engineering.

**Method:** A theoretical framework is proposed to analyze the tradeoff between alignment and helpfulness, complemented by empirical validation of these theories.

**Key Contributions:**

	1. Theoretical framework demonstrating tradeoff between alignment and helpfulness.
	2. Empirical validation of alignment increase and helpfulness decrease with representation engineering.
	3. Identification of a regime for efficient use of representation engineering.

**Result:** The study empirically validates that alignment increases linearly while helpfulness decreases quadratically with the norm of the representation engineering vector.

**Limitations:** The study does not explore long-term effects of representation engineering on model performance across diverse tasks.

**Conclusion:** Representation engineering can enhance alignment in LLMs but at the cost of decreasing helpfulness, which can be quantified using the proposed framework.

**Abstract:** Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. First, we find that under the conditions of our framework, alignment can be guaranteed with representation engineering, and at the same time that helpfulness is harmed in the process. Second, we show that helpfulness is harmed quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.

</details>


### [162] [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/abs/2402.14558)

*Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala Mallikarjunarao Garlapati, Srinivasa Rao Chalamala, Rahul Mishra*

**Main category:** cs.CL

**Keywords:** Large Language Models, Industrial Applications, Challenges and Opportunities

**Relevance Score:** 9

**TL;DR:** The paper evaluates the challenges and opportunities of using large language models (LLMs) in industrial applications through a survey of industry practitioners and an analysis of 68 relevant papers.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the obstacles and opportunities in leveraging LLMs within industrial contexts, given their widespread adoption and transformative potential.

**Method:** A survey involving industry practitioners was conducted along with a review of 68 industry papers to derive insights and develop research questions related to LLM usage in industry.

**Key Contributions:**

	1. Survey of industry practitioners to uncover real-world challenges with LLMs
	2. Review of 68 industry papers to inform research questions on LLM implementation
	3. Establishment of a Github repository for ongoing research on LLMs in industry

**Result:** Identified key challenges and opportunities in employing LLMs across various industrial applications, providing valuable conclusions for practitioners.

**Limitations:** The study may be limited by the specific industries involved in the survey and the scope of reviewed papers.

**Conclusion:** The study offers actionable insights into improving LLM utilization in industry, encouraging further research and development.

**Abstract:** Large language models (LLMs) have become the secret ingredient driving numerous industrial applications, showcasing their remarkable versatility across a diverse spectrum of tasks. From natural language processing and sentiment analysis to content generation and personalized recommendations, their unparalleled adaptability has facilitated widespread adoption across industries. This transformative shift driven by LLMs underscores the need to explore the underlying associated challenges and avenues for enhancement in their utilization. In this paper, our objective is to unravel and evaluate the obstacles and opportunities inherent in leveraging LLMs within an industrial context. To this end, we conduct a survey involving a group of industry practitioners, develop four research questions derived from the insights gathered, and examine 68 industry papers to address these questions and derive meaningful conclusions. We maintain the Github repository with the most recent papers in the field.

</details>


### [163] [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963)

*Xuanxin Wu, Yuki Arase*

**Main category:** cs.CL

**Keywords:** Language Models, Simplification, Human Annotation, Evaluation Metrics, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper evaluates the simplification abilities of large language models (LLMs) using a new error-based human annotation framework, revealing strengths and weaknesses in their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is uncertainty regarding the effectiveness of existing evaluation methodologies for LLMs' simplification tasks, necessitating a deeper analysis.

**Method:** The authors designed an error-based human annotation framework for assessing LLM simplification capabilities. They evaluated both closed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and Llama-3.2-3B.

**Key Contributions:**

	1. Proposed an error-based human annotation framework for evaluating LLM simplifications.
	2. Showed that GPT-4 outperforms other models in fewer erroneous outputs for simplification tasks.
	3. Conducted meta-evaluations revealing the limitations of existing automatic metrics in simplification quality assessment.

**Result:** The study found that GPT-4 produces fewer erroneous outputs in simplifications compared to state-of-the-art models, though limitations were observed in lexical paraphrasing performance across models like GPT-4 and Qwen2.5-72B.

**Limitations:** Existing automatic metrics are insufficiently sensitive for high-quality simplification assessments; the study is limited to a small selection of LLMs.

**Conclusion:** Current automatic metrics lack sensitivity in evaluating high-quality simplifications produced by LLMs. The paper suggests that human evaluations could be more reliable with the proposed framework.

**Abstract:** Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliability. To address these problems, this study provides in-depth insights into LLMs' performance while ensuring the reliability of the evaluation. We design an error-based human annotation framework to assess the LLMs' simplification capabilities. We select both closed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and Llama-3.2-3B. We believe that these models offer a representative selection across large, medium, and small sizes of LLMs. Results show that GPT-4 generally generates fewer erroneous simplification outputs compared to the current state-of-the-art. However, LLMs have their limitations, as seen in GPT-4's struggles with lexical paraphrasing. Results show that LLMs generally generate fewer erroneous simplification outputs compared to the previous state-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and Qwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct meta-evaluations on widely used automatic metrics using our human annotations. We find that these metrics lack sufficient sensitivity to assess the overall high-quality simplifications, particularly those generated by high-performance LLMs.

</details>


### [164] [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)

*James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin*

**Main category:** cs.CL

**Keywords:** bias-augmented consistency training, language models, explainability, bias, GPT-3.5-Turbo

**Relevance Score:** 8

**TL;DR:** This paper introduces a method to mitigate biased reasoning in language models using bias-augmented consistency training (BCT).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of systematic biases in language model reasoning and improve their explainability.

**Method:** A new dataset was created to identify 9 different biases in GPT-3.5-Turbo and Llama-8b models, and BCT was developed for unsupervised fine-tuning to ensure consistent reasoning across various prompts.

**Key Contributions:**

	1. Development of a dataset highlighting biases in language models
	2. Introduction of bias-augmented consistency training (BCT)
	3. Demonstrated effectiveness of BCT in reducing biased reasoning.

**Result:** Applying BCT to the models reduced biased reasoning by 86% on held-out tasks and showed a generalization effect, reducing bias by an average of 37% across various bias forms.

**Limitations:** The method may not address all forms of bias and requires further exploration.

**Conclusion:** BCT proves to be an effective method for reducing biased reasoning in language models while being adaptable to unknown biases and tasks without gold labels.

**Abstract:** Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models' behavior -- for example, rationalizing answers in line with a user's opinion.   We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT.   To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.

</details>


### [165] [Predicting drug-gene relations via analogy tasks with word embeddings](https://arxiv.org/abs/2406.00984)

*Hiroaki Yamagiwa, Ryoma Hashimoto, Kiwamu Arakane, Ken Murakami, Shou Soeda, Momose Oyama, Yihua Zhu, Mariko Okada, Hidetoshi Shimodaira*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Embeddings, Drug-Gene Relations, Bioinformatics, Machine Learning

**Relevance Score:** 7

**TL;DR:** This study demonstrates that BioConceptVec embeddings can effectively predict drug-gene relations through analogy computations and outperform conventional methods, showing performance akin to state-of-the-art models like GPT-4.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the utility of BioConceptVec embeddings, specifically in predicting drug-gene relations within biological datasets.

**Method:** The study utilizes analogy computations on BioConceptVec embeddings and their own embeddings trained on PubMed abstracts, applying vector arithmetic to predict target genes from given drugs, and categorizes them using biological pathways.

**Key Contributions:**

	1. Demonstrated effectiveness of BioConceptVec for drug-gene relation prediction
	2. Showcased performance comparable to GPT-4 in a specific biomedical context
	3. Highlighted the importance of biological pathways in enhancing predictive performance

**Result:** The findings indicate that the embeddings can predict target genes accurately and that incorporating biological pathways improves these predictions, achieving results comparable to large language models such as GPT-4.

**Limitations:** 

**Conclusion:** The effectiveness of simple vector arithmetic in analogy tasks suggests a viable approach for predicting biological relations, potentially benefiting applications in health informatics.

**Abstract:** Natural language processing (NLP) is utilized in a wide range of fields, where words in text are typically transformed into feature vectors called embeddings. BioConceptVec is a specific example of embeddings tailored for biology, trained on approximately 30 million PubMed abstracts using models such as skip-gram. Generally, word embeddings are known to solve analogy tasks through simple vector arithmetic. For example, subtracting the vector for man from that of king and then adding the vector for woman yields a point that lies closer to queen in the embedding space. In this study, we demonstrate that BioConceptVec embeddings, along with our own embeddings trained on PubMed abstracts, contain information about drug-gene relations and can predict target genes from a given drug through analogy computations. We also show that categorizing drugs and genes using biological pathways improves performance. Furthermore, we illustrate that vectors derived from known relations in the past can predict unknown future relations in datasets divided by year. Despite the simplicity of implementing analogy tasks as vector additions, our approach demonstrated performance comparable to that of large language models such as GPT-4 in predicting drug-gene relations.

</details>


### [166] [NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human](https://arxiv.org/abs/2406.03749)

*Shuo Huang, William MacLean, Xiaoxi Kang, Qiongkai Xu, Zhuang Li, Xingliang Yuan, Gholamreza Haffari, Lizhen Qu*

**Main category:** cs.CL

**Keywords:** privacy, text sanitization, large language models, dataset, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper explores privacy protection techniques for cloud-based LLM interactions by sanitizing sensitive texts and introduces the NAP^2 dataset for effective text rewriting.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns related to the inadvertent exposure of sensitive information when using cloud-based LLMs.

**Method:** The study utilizes two human-inspired strategies for text sanitization: deleting sensitive expressions and obscuring details through abstraction, and it creates the NAP^2 dataset via crowdsourcing and LLMs.

**Key Contributions:**

	1. Introduction of the NAP^2 dataset for privacy-focused text sanitization
	2. Development of human-inspired strategies for text rewriting
	3. Demonstration of improved balance between privacy and utility in text anonymization

**Result:** The human-inspired methods yield more natural rewrites, improving the balance between privacy protection and data utility compared to prior anonymization techniques.

**Limitations:** The paper does not discuss the potential challenges in scaling the sanitization methods or their applicability in diverse domains.

**Conclusion:** The findings highlight the effectiveness of human-inspired sanitization techniques and the potential for a dataset that supports improving privacy in LLM interactions.

**Abstract:** The widespread use of cloud-based Large Language Models (LLMs) has heightened concerns over user privacy, as sensitive information may be inadvertently exposed during interactions with these services. To protect privacy before sending sensitive data to those models, we suggest sanitizing sensitive text using two common strategies used by humans: i) deleting sensitive expressions, and ii) obscuring sensitive details by abstracting them. To explore the issues and develop a tool for text rewriting, we curate the first corpus, coined NAP^2, through both crowdsourcing and the use of large language models (LLMs). Compared to the prior works on anonymization, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility, as demonstrated by our extensive experiments. Researchers interested in accessing the dataset are encouraged to contact the first or corresponding author via email.

</details>


### [167] [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)

*Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Adaptive Testing, Value Alignment, Evaluation, Ethics

**Relevance Score:** 8

**TL;DR:** GETA is a novel generative evolving testing approach for measuring the value alignment of LLMs, addressing static benchmark limitations by dynamically generating test items.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to measure the value alignment of LLMs due to their propensity to generate harmful and unethical content, and the limitations of static benchmarks in adapting to evolving models.

**Method:** GETA utilizes adaptive testing methods from measurement theory to dynamically create test items based on model capabilities, co-evolving with LLMs by learning a joint distribution of item difficulty and model value conformity.

**Key Contributions:**

	1. Introduction of the GETA approach for adaptive testing of LLMs
	2. Demonstration of GETA's ability to dynamically create tailored test items
	3. Improved evaluation consistency with unseen model performance.

**Result:** GETA can create difficulty-tailored test items dynamically, and its evaluations align better with LLM performance on unseen out-of-distribution (OOD) and independent identically distributed (i.i.d.) items compared to traditional benchmarks.

**Limitations:** 

**Conclusion:** GETA provides a robust foundation for future evaluation paradigms of LLMs, overcoming the challenges posed by static benchmarks.

**Abstract:** Warning: Contains harmful model outputs. Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges. Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.

</details>


### [168] [Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?](https://arxiv.org/abs/2407.00996)

*Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani*

**Main category:** cs.CL

**Keywords:** Small Language Models, noise handling, instruction tuning, pretrained models, real-world applications

**Relevance Score:** 8

**TL;DR:** This study investigates the robustness of Small Language Models (SLMs) in handling various types of noise, revealing differences in adaptability and offering strategies for practical applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient language models in resource-constrained environments has led to the emergence of Small Language Models (SLMs) as alternatives to Large Language Models (LLMs), yet the impact of noise on SLM performance is under-explored.

**Method:** The study tested four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B), focusing on their ability to learn, retain, and eliminate several noise types, using instruction tuning on noise-free and noise-introduced data.

**Key Contributions:**

	1. Investigation of noise handling in small language models.
	2. Comparison of four pretrained small language models' adaptability to noise.
	3. Practical strategies for developing robust SLMs based on empirical findings.

**Result:** The results showed that smaller models like Olmo adapted quickly to noise patterns. Phi2 demonstrated resistance to certain noise types due to its high-quality pretraining, while Gemma effectively managed transliteration noise through its multilingual capability.

**Limitations:** The study focuses on a limited number of SLMs and noise types, which may not fully capture the complexity of real-world scenarios.

**Conclusion:** The findings suggest that careful data curation and subsequent clean data training can enhance the robustness of SLMs, providing valuable strategies for their deployment in practical scenarios.

**Abstract:** With the growing need for efficient language models in resource-constrained environments, Small Language Models (SLMs) have emerged as compact and practical alternatives to Large Language Models (LLMs). While studies have explored noise handling in LLMs, little is known about how SLMs handle noise, a critical factor for their reliable real-world deployment. This study investigates the ability of SLMs with parameters between 1 and 3 billion to learn, retain, and subsequently eliminate different types of noise (word flip, character flip, transliteration, irrelevant content, and contradictory information). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B) were instruction-tuned on noise-free data and tested with in-context examples to assess noise learning. Subsequently, noise patterns were introduced in instruction tuning to assess their adaptability. The results revealed differences in how models handle noise, with smaller models like Olmo quickly adapting to noise patterns. Phi2's carefully curated, structured, and high-quality pretraining data enabled resistance to character level, transliteration, and counterfactual noise, while Gemma adapted successfully to transliteration noise through its multilingual pretraining. Subsequent clean data training effectively mitigated noise effects. These findings provide practical strategies for developing robust SLMs for real-world applications.

</details>


### [169] [Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs](https://arxiv.org/abs/2407.03181)

*Haritz Puerto, Tilek Chubakov, Xiaodan Zhu, Harish Tayyar Madabushi, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Diverse Chains of Thought, large language models, reasoning, fine-tuning, self-improvement

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach for LLMs to generate Diverse Chains of Thought (DCoT) in a single inference step, improving reasoning performance without external feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of LLMs by allowing them to generate diverse reasoning steps within a single inference, improving performance on reasoning tasks.

**Method:** Fine-tuning LLMs to produce a sequence of Diverse Chains of Thought (DCoT) within one inference step, as opposed to generating independent chains separately and combining them post-hoc.

**Key Contributions:**

	1. Introduction of Diverse Chains of Thought (DCoT) for LLMs
	2. Demonstrated self-improvement in reasoning chains
	3. Public availability of code and data for further research

**Result:** Fine-tuning on DCoT improves performance across a variety of tasks compared to conventional Chain of Thought (CoT) approaches, particularly on tasks with a significant result state space.

**Limitations:** 

**Conclusion:** The integration of within-inference refinement through DCoT leads to better performance and self-improvement in reasoning chains among LLMs.

**Abstract:** Requiring a large language model (LLM) to generate intermediary reasoning steps, known as Chain of Thought (CoT), has been shown to be an effective way of boosting performance. Previous approaches have focused on generating multiple independent CoTs, combining them through ensembling or other post-hoc strategies to enhance reasoning. In this work, we introduce a novel approach where LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought (DCoT) within a single inference step, which is fundamentally different from prior work that primarily operate on parallel CoT generations. DCoT allows LLMs to gain the ability to perform within-inference refinement of reasoning chains without requiring external feedback. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT improves performance over the CoT baseline across model families and scales (1.3B to 70B). These improvements are particularly impactful for tasks with a large result state space, such as those involving numeric answers. Our work is also significant because both quantitative analyses and manual evaluations reveal the observed gains stem from the models' ability to refine an initial reasoning chain by generating a second, improved chain within the same inference step, demonstrating previously elusive self-improvement. Our code and data are publicly available at https://github.com/UKPLab/acl2025-diverse-cot.

</details>


### [170] [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/abs/2407.08551)

*Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei*

**Main category:** cs.CL

**Keywords:** text-to-speech synthesis, continuous-valued tokens, variational inference

**Relevance Score:** 6

**TL;DR:** MELLE is a new approach for text-to-speech synthesis that generates continuous mel-spectrogram frames directly from text without vector quantization, improving performance and robustness.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of text-to-speech synthesis methods that use vector quantization, which sacrifices audio fidelity for compression.

**Method:** MELLE uses autoregressive generation of continuous mel-spectrogram frames, applying regression loss and a proposed spectrogram flux loss function, and incorporates variational inference for enhanced sampling diversity.

**Key Contributions:**

	1. Introduction of the MELLE approach for continuous-valued token modeling in TTS.
	2. Use of regression loss with a novel spectrogram flux loss function.
	3. Integration of variational inference to improve output diversity.

**Result:** MELLE outperforms the VALL-E language model and its variants across multiple performance metrics, alleviating robustness issues related to vector quantization.

**Limitations:** 

**Conclusion:** MELLE represents a more efficient and robust paradigm for text-to-speech synthesis that avoids the typical flaws of existing two-stage models.

**Abstract:** We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.

</details>


### [171] [Sentiment Reasoning for Healthcare](https://arxiv.org/abs/2407.21054)

*Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy*

**Main category:** cs.CL

**Keywords:** AI healthcare, Sentiment Reasoning, Multimodal analysis, Large Language Models, Model transparency

**Relevance Score:** 9

**TL;DR:** This paper introduces Sentiment Reasoning, a new task aimed at enhancing model transparency in AI healthcare decision-making by providing rationales for sentiment analysis predictions, demonstrated through a multimodal multitask framework and the largest sentiment analysis dataset to date.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Transparency in AI healthcare decision-making is essential for better user understanding and decision-making, particularly with Large Language Models (LLMs).

**Method:** The study proposes a novel task called Sentiment Reasoning, where models generate both sentiment labels and rationales based on input transcripts, evaluated using a multimodal multitask framework on both human and Automatic Speech Recognition (ASR) transcripts.

**Key Contributions:**

	1. Introduction of the Sentiment Reasoning task for sentiment analysis
	2. Development of a multimodal multitask framework
	3. Creation of the largest multimodal sentiment analysis dataset.

**Result:** The introduction of Sentiment Reasoning improved model classification performance by 2% in accuracy and macro-F1 scores while providing rationales of semantic quality comparable to humans, regardless of whether the transcripts were human-written or generated by ASR systems.

**Limitations:** 

**Conclusion:** Sentiment Reasoning enhances model transparency in sentiment analysis and improves classification performance through rationale-augmented fine-tuning.

**Abstract:** Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)'s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world's largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model's classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: https://github.com/leduckhai/Sentiment-Reasoning

</details>


### [172] [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)

*Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Pengpeng Shao, Jianhua Tao*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, large language models, noisy data, benchmarking, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces a Noise RAG Benchmark to evaluate Retrieval-Augmented Generation (RAG) methods in the presence of various noise types and analyzes their effects on LLM performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucinations in large language models and explore the impact of different noise types on model performance, which has been overlooked in existing research.

**Method:** Defined seven distinct noise types from a linguistic perspective and established the Noise RAG Benchmark (NoiserBench) for evaluation using multiple datasets and reasoning tasks.

**Key Contributions:**

	1. Development of the Noise RAG Benchmark (NoiserBench) for evaluating noisy scenarios in LLMs.
	2. Categorization of noise into beneficial and harmful types with empirical evaluation.
	3. Insights on improving RAG methods to handle diverse retrieval challenges.

**Result:** Contrary to prior assumptions, the paper finds that noise can be categorized into beneficial and harmful types, with some noise actually enhancing LLM performance in specific scenarios.

**Limitations:** 

**Conclusion:** The findings provide insights to improve RAG solutions and mitigate hallucinations in practical retrieval environments by understanding the dual nature of noise.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing more robust, adaptable RAG solutions and mitigating hallucinations across diverse retrieval scenarios.

</details>


### [173] [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/abs/2409.04183)

*Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang*

**Main category:** cs.CL

**Keywords:** Code Language Models, Graph Neural Networks, Finetuning, Cross-modal Alignment, Programming Languages

**Relevance Score:** 9

**TL;DR:** This paper presents GALLa, a model that enhances code language models by integrating graph neural networks to utilize semantic information from code structure during finetuning, showing improved performance across multiple tasks without compromising model compatibility at inference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current code language models that either ignore structural information or require modifications to the Transformer architecture, thus hindering scalability and compatibility with pretrained models.

**Method:** GALLa employs graph neural networks and cross-modal alignment to inject structural code information as an auxiliary task during the finetuning process, maintaining a model-agnostic and task-agnostic approach that only requires structural graph data at training time.

**Key Contributions:**

	1. Introduction of GALLa, a model that integrates graph neural networks with LLMs
	2. Demonstration of consistent performance improvements across multiple coding tasks
	3. Model-agnostic framework that incurs no additional inference costs

**Result:** Experiments demonstrate that GALLa consistently outperforms baseline models across five coding tasks, with significant improvements observed in models like LLaMA3 and Qwen2.5-Coder, regardless of their size.

**Limitations:** 

**Conclusion:** GALLa effectively combines the advantages of structural information modeling and large language models, leading to enhanced performance on code tasks while remaining compatible with existing LLMs.

**Abstract:** Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.

</details>


### [174] [Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints](https://arxiv.org/abs/2409.14469)

*Kaikai An, Shuzheng Si, Helan Hu, Haozhe Zhao, Yuchi Wang, Qingyan Guo, Baobao Chang*

**Main category:** cs.CL

**Keywords:** semantic parsing, large language models, prompting approach, SENSE, NLP

**Relevance Score:** 9

**TL;DR:** The paper investigates the impact of semantic parsing on LLM performance and introduces SENSE, a prompting method that embeds semantic hints to enhance LLM capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how semantic parsing affects the performance of large language models (LLMs) compared to smaller models.

**Method:** The study conducts empirical experiments comparing LLM performance with and without semantic parsing results, followed by the introduction of the SENSE prompting approach.

**Key Contributions:**

	1. Demonstration of the negative impact of directly adding semantic parsing to LLMs.
	2. Introduction of the SENSE prompting approach to effectively integrate semantic hints.
	3. Empirical evidence of improved performance in LLMs when using the SENSE method.

**Result:** Findings show that directly adding semantic parsing outputs reduces LLM performance, but the SENSE approach leads to consistent improvements across tasks.

**Limitations:** The study does not explore all possible dimensions of semantic parsing and its broader implications on various LLM architectures.

**Conclusion:** Integrating semantic information through effective prompting can enhance the capabilities of LLMs significantly.

**Abstract:** Semantic Parsing aims to capture the meaning of a sentence and convert it into a logical, structured form. Previous studies show that semantic parsing enhances the performance of smaller models (e.g., BERT) on downstream tasks. However, it remains unclear whether the improvements extend similarly to LLMs. In this paper, our empirical findings reveal that, unlike smaller models, directly adding semantic parsing results into LLMs reduces their performance. To overcome this, we propose SENSE, a novel prompting approach that embeds semantic hints within the prompt. Experiments show that SENSE consistently improves LLMs' performance across various tasks, highlighting the potential of integrating semantic information to improve LLM capabilities.

</details>


### [175] [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)

*Xiang Hu, Zhihao Teng, Jun Zhao, Wei Wu, Kewei Tu*

**Main category:** cs.CL

**Keywords:** Transformers, Grouped Cross Attention, Long-context handling, Dynamic attention mechanism, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Grouped Cross Attention (GCA), a new attention mechanism for Transformers that allows handling significantly longer contexts (up to 1000 times the pre-training length) efficiently, utilizing a constant attention window size.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of Transformers in managing long contexts due to self-attention's quadratic complexity and the need for larger attention windows, which increase computational and memory costs.

**Method:** The paper proposes a dynamic context attention mechanism that divides input sequences into chunks and learns to retrieve relevant past chunks for text generation in an end-to-end manner, maintaining a fixed-size attention window.

**Key Contributions:**

	1. Novel attention mechanism for long context handling in Transformers.
	2. End-to-end learning for retrieval of past chunks that reduce auto-regressive loss.
	3. Achieves 1000 times pre-training context length with constant attention window.

**Result:** GCA-based models demonstrate near-perfect accuracy in passkey retrieval tasks with context lengths of 16M, showcasing the effectiveness of the proposed method in accessing long-range information efficiently.

**Limitations:** 

**Conclusion:** The proposed GCA mechanism significantly reduces computational and memory costs while enhancing the ability of Transformers to manage long contexts effectively.

**Abstract:** Despite the success of Transformers, handling long contexts remains challenging due to the limited length generalization and quadratic complexity of self-attention. Thus Transformers often require post-training with a larger attention window, significantly increasing computational and memory costs. In this paper, we propose a novel attention mechanism based on dynamic context, Grouped Cross Attention (GCA), which can generalize to 1000 times the pre-training context length while maintaining the ability to access distant information with a constant attention window size. For a given input sequence, we split it into chunks and use each chunk to retrieve top-k relevant past chunks for subsequent text generation. Specifically, unlike most previous works that use an off-the-shelf retriever, our key innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. Such a mechanism accommodates retrieved chunks with a fixed-size attention window to achieve long-range information access, significantly reducing computational and memory costs during training and inference. Experiments show that GCA-based models achieve near-perfect accuracy in passkey retrieval for 16M context lengths, which is 1000 times the training length.

</details>


### [176] [Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing](https://arxiv.org/abs/2410.06638)

*Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical reasoning, preference learning, error mitigation, self-editing

**Relevance Score:** 8

**TL;DR:** This paper introduces eRror-Injected Self-Editing (RISE), a novel preference learning framework designed to improve mathematical reasoning in Large Language Models by injecting subtle errors into solutions to enhance training and error mitigation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with subtle reasoning errors that limit their mathematical capabilities, necessitating improved training methods that address these issues.

**Method:** RISE injects predefined subtle errors into key tokens during reasoning or computation steps, creating hard pairs for training without the need for fine-grained sampling or preference annotation.

**Key Contributions:**

	1. Introduction of RISE framework for error mitigation in LLMs
	2. Effective training with fewer samples through error injection
	3. Demonstrated improvements in performance on mathematical reasoning benchmarks

**Result:** Extensive experiments showed that RISE leads to significant performance improvements of 3.0% on GSM8K and 7.9% on MATH with only 4.5K training samples when applied to Qwen2-7B-Instruct.

**Limitations:** 

**Conclusion:** RISE not only enhances mathematical reasoning but also extends its effectiveness to logical reasoning and code generation tasks.

**Abstract:** Large Language Models (LLMs) have exhibited strong mathematical reasoning prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle yet critical errors, such as miscalculations or incorrect substitutions, limit the LLMs' full potential. Existing studies to improve mathematical ability typically involve applying preference learning to step-wise solution pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook critical subtle errors. In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into pivotal tokens in reasoning or computation steps to construct hard pairs for error mitigation. In detail, RISE uses the LLM itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect of error mitigation extends from mathematical reasoning to logical reasoning and code generation.

</details>


### [177] [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/abs/2410.09829)

*Rimvydas Rubavicius, Antonio Valerio Miceli-Barone, Alex Lascarides, Subramanian Ramamoorthy*

**Main category:** cs.CL

**Keywords:** natural language interface, autonomous vehicles, scenario generation, dialogue system, large language model

**Relevance Score:** 8

**TL;DR:** This paper presents a natural language interface leveraging a large language model to assist non-coding experts in creating autonomous vehicle scenarios for simulation testing.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the testing of autonomous vehicles, enabling domain experts without coding skills to specify scenarios and vehicle behaviors easily.

**Method:** The authors developed a natural language interface that translates verbal instructions into symbolic programs, facilitating scenario generation in simulations.

**Key Contributions:**

	1. Introduction of a natural language interface for scenario synthesis
	2. Demonstration of feasibility with limited data
	3. Established the significance of dialogue in simulation success

**Result:** The approach demonstrated feasibility with a limited training dataset and achieved a 4.5 times higher success rate in simulation generation when utilizing dialogue compared to non-dialogue methods.

**Limitations:** The study is based on a small training dataset, which may affect the generalizability of the results.

**Conclusion:** Engaging in a dialogue is essential for successful scenario generation, highlighting the potential of natural language interfaces in effective simulation testing.

**Abstract:** Cyber-physical systems like autonomous vehicles are tested in simulation before deployment, using domain-specific programs for scenario specification. To aid the testing of autonomous vehicles in simulation, we design a natural language interface, using an instruction-following large language model, to assist a non-coding domain expert in synthesising the desired scenarios and vehicle behaviours. We show that using it to convert utterances to the symbolic program is feasible, despite the very small training dataset. Human experiments show that dialogue is critical to successful simulation generation, leading to a 4.5 times higher success rate than a generation without engaging in extended conversation.

</details>


### [178] [The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph](https://arxiv.org/abs/2410.12458)

*Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari*

**Main category:** cs.CL

**Keywords:** data selection, large language models, supervised fine-tuning, quality and diversity, GraphFilter

**Relevance Score:** 8

**TL;DR:** GraphFilter is a new approach for data selection in supervised fine-tuning of large language models that balances quality and diversity using a bipartite graph model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current data selection methods often prioritize either quality or diversity in data, leading to poor training outcomes for large language models.

**Method:** GraphFilter formulates data selection as a set cover problem, modeling the dataset as a bipartite graph and using a priority function that combines quality and diversity metrics.

**Key Contributions:**

	1. Introduction of GraphFilter for data selection that balances quality and diversity.
	2. Demonstrated superior performance compared to nine baselines.
	3. Provided insights into the interaction between quality and diversity across different subset sizes.

**Result:** GraphFilter outperforms nine existing baselines in model performance and computational efficiency across six benchmarks when tested with three different model backbones.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of balancing quality and diversity in data selection for improving the performance of large language models during fine-tuning.

**Abstract:** The performance of large language models (LLMs) is strongly influenced by the quality and diversity of data used during supervised fine-tuning (SFT). However, current data selection methods often prioritize one aspect over the other, resulting in suboptimal training outcomes. To address this, we formulate data selection as a set cover problem and present GraphFilter, a novel approach that balances both quality and diversity in data selection. GraphFilter models the dataset as a bipartite graph connecting sentences to their constituent n-grams, then employs a priority function that combines quality and diversity metrics multiplicatively. GraphFilter iteratively selects sentences with the highest priority, removes covered n-grams from the bipartite graph, and recomputes priorities to reflect the changing data landscape. We validate GraphFilter using three model backbones across six widely-used benchmarks, demonstrating that it outperforms nine existing baselines in both model performance and computational efficiency. Further analysis shows that our design choices lead to more effective subset selection, underscores the value of instruction diversity, and provides insights into how quality and diversity interact with different subset sizes.

</details>


### [179] [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/abs/2410.13206)

*Shintaro Ozaki, Kazuki Hayashi, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** body language, VideoLLMs, emotion recognition, dataset, bias

**Relevance Score:** 8

**TL;DR:** The paper introduces BQA, a dataset for evaluating Video Large Language Models' ability to interpret body language and emotions, revealing challenges and biases present in their interpretations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the interpretation of nonverbal communication by Video Large Language Models (VideoLLMs), which is essential for accurate human interaction recognition.

**Method:** The authors propose the BQA dataset that contains short clips of body language annotated with 26 emotion labels and evaluate various VideoLLMs against this dataset.

**Key Contributions:**

	1. Introduction of the BQA dataset for body language interpretation
	2. Evaluation of VideoLLMs showing challenges in emotion recognition
	3. Analysis of biases based on demographic factors in VideoLLM responses.

**Result:** The evaluation showed that VideoLLMs struggle with accurately interpreting emotions from body language, and certain models exhibited biases based on the age and ethnicity of individuals in the videos.

**Limitations:** The dataset may not comprehensively cover all possible nonverbal cues and emotions, and biases could be influenced by various factors beyond age and ethnicity.

**Conclusion:** A better understanding of body language interpretation is necessary for VideoLLMs, and the biases observed indicate the need for further research in this area.

**Abstract:** A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding. Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent. To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language. We evaluated various VideoLLMs on BQA and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made significantly biased answers depending on the age group and ethnicity of the individuals in the video. The dataset is available.

</details>


### [180] [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://arxiv.org/abs/2410.13776)

*Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan*

**Main category:** cs.CL

**Keywords:** In-context Learning, Large Language Models, emotion, morality, annotator bias

**Relevance Score:** 7

**TL;DR:** This paper investigates the limitations of In-context Learning (ICL) in LLMs, focusing on annotation artifacts that arise from data aggregation in subjective tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the reliance of LLMs on task priors during In-context Learning, especially in complex domains like emotion and morality.

**Method:** The authors examine the effects of data aggregation and annotator biases on the performance of LLMs in subjective domains, using qualitative and quantitative measures.

**Key Contributions:**

	1. Critical examination of data aggregation in subjective task modeling
	2. Introduction of quantifiable measures for understanding LLM priors
	3. Identification of the impact of minority annotators on LLM alignment

**Result:** Findings indicate that aggregate methods can introduce detrimental noise, affecting model predictions and advocating for a focus on individual annotator insights.

**Limitations:** The study acknowledges that there are other unidentified factors contributing to the ICL gap.

**Conclusion:** The paper concludes that while aggregation affects performance, it does not explain the entire gap between ICL and state-of-the-art performance, highlighting the need for further exploration of other influencing factors.

**Abstract:** In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than "learning" to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified.

</details>


### [181] [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/abs/2410.14641)

*Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu*

**Main category:** cs.CL

**Keywords:** positional bias, large language models, LongPiBench

**Relevance Score:** 8

**TL;DR:** This paper introduces LongPiBench, a benchmark for assessing positional bias in large language models (LLMs) related to processing multiple pieces of relevant information, revealing significant biases in current models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs struggle with the 'lost in the middle' phenomenon, which limits their effectiveness in processing long inputs containing multiple relevant information pieces.

**Method:** The authors developed LongPiBench, a benchmark to evaluate positional bias, conducting thorough experiments across five commercial and six open-source LLMs.

**Key Contributions:**

	1. Introduction of LongPiBench benchmark for positional bias
	2. Demonstrated biases related to spacing of relevant information
	3. Insights into LLM performance with long input processing

**Result:** Most models showed robustness against the 'lost in the middle' issue; however, significant biases were found pertaining to the spacing of relevant information pieces.

**Limitations:** 

**Conclusion:** Evaluating and reducing positional biases in LLMs is crucial for advancing their capabilities in real-world applications.

**Abstract:** Positional bias in large language models (LLMs) hinders their ability to effectively process long inputs. A prominent example is the "lost in the middle" phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LongPiBench, a benchmark designed to assess positional bias involving multiple pieces of relevant information. Thorough experiments are conducted with five commercial and six open-source models. These experiments reveal that while most current models are robust against the "lost in the middle" issue, there exist significant biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases to advance LLM's capabilities.

</details>


### [182] [Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693)

*Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Zhaopeng Tu, Qiaoming Zhu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Dataset Generation, Mathematical Reasoning, Open-source, AI

**Relevance Score:** 8

**TL;DR:** This paper presents ScaleQuest, a new method for generating large-scale mathematical reasoning datasets to enhance LLMs' capabilities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for extensive and diverse reasoning datasets for improving the mathematical reasoning capabilities of Large Language Models (LLMs), particularly for the open-source community.

**Method:** ScaleQuest employs a two-stage question-tuning process: Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to generate diverse questions from 7B-scale models without using proprietary models or seed data.

**Key Contributions:**

	1. Introduction of ScaleQuest for scalable data synthesis for LLMs
	2. Development of a dataset containing 1 million problem-solution pairs
	3. Demonstration of improved model performance on code reasoning tasks.

**Result:** Models trained on the dataset generated by ScaleQuest outperform existing open-source datasets in evaluations, showing improved performance with increasing training data volume.

**Limitations:** 

**Conclusion:** ScaleQuest provides a cost-effective and practical solution for the open-source community to improve LLMs' mathematical reasoning abilities.

**Abstract:** Improving the mathematical reasoning capabilities of Large Language Models (LLMs) is critical for advancing artificial intelligence. However, access to extensive, diverse, and high-quality reasoning datasets remains a significant challenge, particularly for the open-source community. In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs. Our experiments demonstrate that models trained on our data outperform existing open-source datasets in both in-domain and out-of-domain evaluations. Furthermore, our approach shows continued performance improvement as the volume of training data increases, highlighting its potential for ongoing data scaling. The extensive improvements observed in code reasoning tasks demonstrate the generalization capabilities of our proposed method. Our work provides the open-source community with a practical solution to enhance the mathematical reasoning abilities of LLMs.

</details>


### [183] [Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers](https://arxiv.org/abs/2410.21013)

*Akhilesh Kakolu Ramarao, Kevin Tang, Dinah Baer-Henney*

**Main category:** cs.CL

**Keywords:** Morphological processing, Transformer models, Spanish verbs, Inflection

**Relevance Score:** 5

**TL;DR:** Study on morphological processing in Spanish focusing on the Paradigm Cell Filling Problem using transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address how speakers generate inflected forms of words within incomplete paradigms, particularly in the context of Spanish verbal paradigms with irregular patterns.

**Method:** The problem is formulated as a morphological reinflection task where input frequency's role in the acquisition of regular versus irregular patterns is investigated using transformer models.

**Key Contributions:**

	1. Investigates input frequency effects on morphological reinflection
	2. Reveals primacy effects and memorization trends in transformer models
	3. Demonstrates regularization tendencies in verb paradigms

**Result:** Key findings include better model performance on L-shaped verbs under uneven frequency conditions; observation of primacy effects, increased memorization with more L-shaped verbs, and regularization tendencies when consonant pairs are rare in training data.

**Limitations:** 

**Conclusion:** The study provides insights into morphological processing and model behavior regarding inflection patterns and frequency.

**Abstract:** Over the past decade, various studies have addressed how speakers solve the so-called `The Paradigm Cell Filling Problem' (PCFP) \citep{ackerman2009parts} across different languages. The PCFP addresses a fundamental question in morphological processing: how do speakers accurately generate inflected forms of words when presented with incomplete paradigms? This problem is particularly salient when modeling complex inflectional systems. We focus on Spanish verbal paradigms, where certain verbs follow an irregular L-shaped pattern, where the first-person singular present indicative stem matches the stem used throughout the present subjunctive mood. We formulate the problem as a morphological reinflection task. Specifically, we investigate the role of input frequency in the acquisition of regular versus irregular L-shaped patterns in transformer models. By systematically manipulating the input distributions and analyzing model behavior, we reveal four key findings: 1) Models perform better on L-shaped verbs compared to regular verbs, especially in uneven frequency conditions; 2) Robust primacy effects are observed, but no consistent recency effects; 3) Memorization becomes more prominent as the proportion of L-shaped verbs increases; 4) There is a tendency to regularize L-shaped verbs when their consonant alternation pairs are rare or absent in the training data.

</details>


### [184] [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)

*Jiaru Zou, Qing Wang, Pratyush Thakur, Nickvash Kani*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, benchmark dataset

**Relevance Score:** 8

**TL;DR:** STEM-PoM is a benchmark dataset for evaluating LLMs' reasoning with mathematical symbols in STEM texts, highlighting gaps in their classification abilities.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance large language models' reasoning abilities in interpreting abstract mathematical symbols in math-rich STEM documents and address their limitations.

**Method:** Introduction of a comprehensive benchmark dataset, STEM-PoM, sourced from real-world ArXiv documents, containing over 2K classified math symbols with extensive experimental evaluation of LLMs' performance.

**Key Contributions:**

	1. Development of the STEM-PoM benchmark dataset
	2. Insight into LLMs' performance gaps in math symbol classification
	3. Open-source access to code and data for further research

**Result:** State-of-the-art LLMs achieve average accuracy of 20-60% under in-context learning and 50-60% with fine-tuning in classifying mathematical symbols within the dataset.

**Limitations:** 

**Conclusion:** Improving LLMs' classification of mathematical symbols not only addresses existing gaps but also enhances the models' overall mathematical reasoning capabilities.

**Abstract:** Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM (Science, Technology, Engineering, and Mathematics) documents. While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs' reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments demonstrate that state-of-the-art LLMs achieve an average accuracy of 20-60% under in-context learning and 50-60% with fine-tuning, highlighting a substantial gap in their ability to classify mathematical symbols. By improving LLMs' mathematical symbol classification, STEM-PoM further enhances models' downstream mathematical reasoning capabilities. The code and data are available at https://github.com/jiaruzouu/STEM-PoM.

</details>


### [185] [Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2411.01834)

*Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, Reinforcement Learning, Direct Preference Optimization

**Relevance Score:** 8

**TL;DR:** The Align-SLM framework enhances textless Spoken Language Models (SLMs) using preference optimization from Reinforcement Learning with AI Feedback to improve semantic coherence and relevance, achieving state-of-the-art performance on various benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Textless Spoken Language Models (SLMs) show potential but lack semantic coherence compared to text-based Large Language Models (LLMs).

**Method:** The Align-SLM framework generates multiple speech continuations from a prompt and utilizes semantic metrics to develop preference data for Direct Preference Optimization (DPO).

**Key Contributions:**

	1. Introduction of the Align-SLM framework for SLMs
	2. Utilization of preference optimization inspired by RLAIF
	3. Demonstration of state-of-the-art performance on SLM benchmarks

**Result:** The experimental results demonstrate that our method achieves state-of-the-art performance for SLMs on multiple benchmarks, highlighting the effectiveness of preference optimization.

**Limitations:** 

**Conclusion:** Preference optimization is critical for improving the semantic understanding of textless SLMs, as evidenced by the benchmark results.

**Abstract:** While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.

</details>


### [186] [Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection](https://arxiv.org/abs/2411.07446)

*Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, Yangyang Kang*

**Main category:** cs.CL

**Keywords:** prompt engineering, exemplar retrieval, memory mechanism

**Relevance Score:** 9

**TL;DR:** This paper introduces an Exemplar-Guided Reflection with Memory mechanism (ERM) to improve the prompt optimization of large language models by leveraging historical feedback and enhancing exemplar selection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work addresses limitations in existing prompt optimization methods that overlook historical feedback and rely on general semantic relationships for exemplar retrieval, which may not be optimal for task performance.

**Method:** An exemplar-guided reflection mechanism is proposed, where feedback generation is influenced by selected exemplars. Additionally, two memory systems are constructed to utilize historical feedback information and enhance exemplar retrieval.

**Key Contributions:**

	1. Introduction of the ERM framework for prompt optimization
	2. Utilization of historical feedback for exemplar retrieval
	3. Demonstrated performance improvements on benchmark datasets

**Result:** The proposed method significantly outperforms previous approaches, achieving a 10.1 improvement in F1 score on the LIAR dataset and reducing the required optimization steps by half on ProTeGi.

**Limitations:** 

**Conclusion:** The findings demonstrate that incorporating historical feedback and optimized exemplar retrieval leads to better performance in prompt engineering for LLMs.

**Abstract:** Automatic prompt engineering aims to enhance the generation quality of large language models (LLMs). Recent works utilize feedbacks generated from erroneous cases to guide the prompt optimization. During inference, they may further retrieve several semantically-related exemplars and concatenate them to the optimized prompts to improve the performance. However, those works only utilize the feedback at the current step, ignoring historical and unseleccted feedbacks which are potentially beneficial. Moreover, the selection of exemplars only considers the general semantic relationship and may not be optimal in terms of task performance and matching with the optimized prompt. In this work, we propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize more efficient and accurate prompt optimization. Specifically, we design an exemplar-guided reflection mechanism where the feedback generation is additionally guided by the generated exemplars. We further build two kinds of memory to fully utilize the historical feedback information and support more effective exemplar retrieval. Empirical evaluations show our method surpasses previous state-of-the-arts with less optimization steps, i.e., improving F1 score by 10.1 on LIAR dataset, and reducing half of the optimization steps on ProTeGi.

</details>


### [187] [Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation](https://arxiv.org/abs/2411.12719)

*Praveen Srinivasa Varadhan, Amogh Gulati, Ashwin Sankar, Srija Anand, Anirudh Gupta, Anirudh Mukherjee, Shiva Kumar Marepally, Ankur Bhatia, Saloni Jaju, Suvrat Bhooshan, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** TTS, MUSHRA, evaluation, human ratings, Indian languages

**Relevance Score:** 7

**TL;DR:** This paper assesses the MUSHRA test for TTS system evaluation and proposes refined variants to address its shortcomings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation framework for text-to-speech (TTS) models, which currently lacks robustness and consistency.

**Method:** A comprehensive assessment of the MUSHRA test was conducted, involving 492 listeners, focusing on rater variability, listener fatigue, and reference bias with empirical analysis in Hindi and Tamil.

**Key Contributions:**

	1. Identification of reference-matching bias and judgement ambiguity in the MUSHRA test
	2. Proposed refined MUSHRA test variants
	3. Released the MANGO dataset of 246,000 ratings for Indian languages

**Result:** Identified two main issues in the MUSHRA test: reference-matching bias and judgement ambiguity. Proposed two variants: one for fairer ratings beyond human quality and another to reduce ambiguity.

**Limitations:** 

**Conclusion:** The new variants provide more reliable and detailed assessments for TTS systems. A massive dataset, MANGO, is released for research in Indian languages.

**Abstract:** Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems.

</details>


### [188] [DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization](https://arxiv.org/abs/2411.14055)

*Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jing Li, Min Zhang, Zhaopeng Tu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Distribution, Pruning, Performance Bias, Heterogeneous Tasks

**Relevance Score:** 9

**TL;DR:** DRPruning dynamically adjusts data distribution during training to improve LLM performance across diverse tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance degradation in large language models due to structured pruning, which often results in biased outcomes across different domains.

**Method:** The DRPruning method modifies the data distribution during training to ensure balanced performance in heterogeneous and multi-task tasks.

**Key Contributions:**

	1. Introduction of DRPruning for dynamic data distribution adjustment.
	2. Demonstrated improved performance of LLMs in monolingual and multilingual scenarios.
	3. Code availability for reproducibility and further research.

**Result:** Experiments show that DRPruning outperforms similarly sized models in pruning and continued pretraining across various metrics, demonstrating robust performance against distribution shifts.

**Limitations:** 

**Conclusion:** The robustness of DRPruning and its automatic determination of optimal loss references and data ratios indicate its potential for wider applications.

**Abstract:** Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. Code and scripts are available at https://github.com/hexuandeng/DRPruning.

</details>


### [189] [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)

*Anthony Hughes, Ning Ma, Nikolaos Aletras*

**Main category:** cs.CL

**Keywords:** privacy, summarization, language models, medical informatics, legal informatics

**Relevance Score:** 9

**TL;DR:** This paper studies the privacy risks in language model (LM)-based summarization, particularly in sensitive domains like medical and legal, finding that LMs often leak personally identifiable information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of privacy-preserving summarization in sensitive domains where personal data cannot be disclosed.

**Method:** A comprehensive study analyzing two closed- and four open-weight language models of various sizes, evaluating both prompting and fine-tuning strategies across multiple summarization datasets in medical and legal fields.

**Key Contributions:**

	1. Identification of privacy risks in LM-based summarization
	2. Comparative analysis of LM and human-generated summaries
	3. Evaluation of prompting and fine-tuning strategies for privacy preservation

**Result:** The study reveals that language models frequently leak personally identifiable information in their summaries, compared to human-generated summaries which offer superior privacy protection.

**Limitations:** The study is limited to specific language model families and may not encompass all LM architectures or datasets.

**Conclusion:** There is a significant gap between the privacy preservation capabilities of current language models and those of human experts in sensitive summarization tasks.

**Abstract:** In sensitive domains such as medical and legal, protecting sensitive information is critical, with protective laws strictly prohibiting the disclosure of personal data. This poses challenges for sharing valuable data such as medical reports and legal cases summaries. While language models (LMs) have shown strong performance in text summarization, it is still an open question to what extent they can provide privacy-preserving summaries from non-private source documents. In this paper, we perform a comprehensive study of privacy risks in LM-based summarization across two closed- and four open-weight models of different sizes and families. We experiment with both prompting and fine-tuning strategies for privacy-preservation across a range of summarization datasets including medical and legal domains. Our quantitative and qualitative analysis, including human evaluation, shows that LMs frequently leak personally identifiable information in their summaries, in contrast to human-generated privacy-preserving summaries, which demonstrate significantly higher privacy protection levels. These findings highlight a substantial gap between current LM capabilities and expert human expert performance in privacy-sensitive summarization tasks.

</details>


### [190] [Knowledge Boundary of Large Language Models: A Survey](https://arxiv.org/abs/2412.12472)

*Moxin Li, Yong Zhao, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua, Yang Deng*

**Main category:** cs.CL

**Keywords:** large language models, knowledge boundary, taxonomies, open challenges, research directions

**Relevance Score:** 9

**TL;DR:** This paper surveys the concept of knowledge boundaries in large language models (LLMs), proposing a formal definition and taxonomy while reviewing methods and strategies for addressing challenges related to these boundaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in storing and utilizing knowledge, which can lead to inaccuracies and undesired behaviors.

**Method:** The paper provides a formalized definition of LLM knowledge boundaries and categorizes knowledge into four types, reviewing motivation, methods for identifying boundaries, and strategies for mitigation.

**Key Contributions:**

	1. Proposed a formal definition of LLM knowledge boundaries.
	2. Introduced a taxonomy categorizing knowledge types in LLMs.
	3. Systematically reviewed research on knowledge boundaries and mitigation strategies.

**Result:** The survey offers a comprehensive overview of the field, discussing key issues and inspiring advancements in LLM knowledge research.

**Limitations:** The concept of knowledge boundaries in LLMs is still inadequately defined and requires further exploration.

**Conclusion:** Understanding LLM knowledge boundaries is crucial for improving model relevance and accuracy; the survey lays the groundwork for future research.

**Abstract:** Although large language models (LLMs) store vast amount of knowledge in their parameters, they still have limitations in the memorization and utilization of certain knowledge, leading to undesired behaviors such as generating untruthful and inaccurate responses. This highlights the critical need to understand the knowledge boundary of LLMs, a concept that remains inadequately defined in existing research. In this survey, we propose a comprehensive definition of the LLM knowledge boundary and introduce a formalized taxonomy categorizing knowledge into four distinct types. Using this foundation, we systematically review the field through three key lenses: the motivation for studying LLM knowledge boundaries, methods for identifying these boundaries, and strategies for mitigating the challenges they present. Finally, we discuss open challenges and potential research directions in this area. We aim for this survey to offer the community a comprehensive overview, facilitate access to key issues, and inspire further advancements in LLM knowledge research.

</details>


### [191] [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264)

*Xiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng*

**Main category:** cs.CL

**Keywords:** large language models, self-correction, program verification, human-computer interaction, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Program-driven Self-Correction (ProgCo) for enhancing self-verification and self-refinement in large language models (LLMs) through program-driven mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the ability of large language models (LLMs) to self-verify and self-refine their responses without external feedback, especially in complex reasoning tasks where they typically struggle.

**Method:** The paper proposes two main components: program-driven verification (ProgVe) which creates verification pseudo-programs for logic and validation, and program-driven refinement (ProgRe) which refines both responses and verification programs based on feedback from ProgVe.

**Key Contributions:**

	1. Introduction of program-driven self-correction mechanism (ProgCo) for LLMs
	2. Development of ProgVe for complex verification tasks
	3. Implementation of ProgRe for feedback-driven enhancement of responses

**Result:** Experimental results on instruction-following and mathematical tasks show that ProgCo effectively enables self-correction in LLMs, with improved performance when combined with real program tools.

**Limitations:** The approach may be limited by the complexity of reasoning tasks and dependency on the quality of self-generated verification programs.

**Conclusion:** ProgCo represents a significant advancement in the self-correction abilities of LLMs, potentially addressing shortcomings in complex reasoning scenarios.

**Abstract:** Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools. We release our code at https://github.com/songxiaoshuai/progco.

</details>


### [192] [VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models](https://arxiv.org/abs/2501.04962)

*Wenqian Cui, Xiaoqi Jiao, Ziqiao Meng, Irwin King*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, Speech Interaction, VoxEval, Benchmark, Question-Answering

**Relevance Score:** 8

**TL;DR:** VoxEval is a new SpeechQA benchmark designed to evaluate Spoken Language Models (SLMs) through speech interactions, addressing gaps in evaluating their understanding and robustness across diverse audio conditions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of existing QA benchmarks that do not adequately evaluate the knowledge understanding of Spoken Language Models (SLMs) in real-world speech interactions, particularly across varied input audio conditions.

**Method:** Introduction of VoxEval, a novel SpeechQA benchmark that maintains speech input and output formats, assesses model robustness to diverse audio conditions, and evaluates complex tasks in spoken format.

**Key Contributions:**

	1. Development of a SpeechQA benchmark that maintains speech interaction for evaluation.
	2. Evaluation of model robustness across different audio inputs.
	3. Assessment of complex reasoning tasks in spoken language.

**Result:** VoxEval reveals significant challenges for current SLMs, showing their sensitivity to various audio conditions and highlighting deficiencies in their reasoning capabilities.

**Limitations:** 

**Conclusion:** The benchmark aims to guide the development of more advanced and reliable Spoken Language Models in the future.

**Abstract:** With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval

</details>


### [193] [Tuning LLM Judge Design Decisions for 1/1000 of the Cost](https://arxiv.org/abs/2501.17178)

*David Salinas, Omar Swelam, Frank Hutter*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hyperparameter tuning, Cost-efficient evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces a method to evaluate Large Language Model judges by systematically analyzing and tuning their hyperparameters while reducing cost through multi-objective multi-fidelity approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more efficient and cost-effective evaluation methods for Large Language Models (LLMs) without relying on human annotations.

**Method:** Systematic analysis and tuning of hyperparameters of LLM-based judges using multi-objective multi-fidelity techniques to reduce evaluation costs and enhance accuracy.

**Key Contributions:**

	1. Systematic hyperparameter analysis for LLM judges
	2. Introduction of a multi-objective multi-fidelity evaluation method
	3. Utilization of open-weight models for enhanced accessibility

**Result:** The proposed method identifies judges that outperform existing benchmarks in both accuracy and cost-efficiency, utilizing open-weight models for better accessibility and reproducibility.

**Limitations:** 

**Conclusion:** This approach not only improves upon existing evaluation methods for LLMs but also provides a reproducible framework for future research.

**Abstract:** Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention. While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging. In this paper, we propose to systematically analyze and tune the hyperparameters of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trade accuracy for cost and also significantly reduce the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility. The code to reproduce our experiments is available at this repository https://github.com/geoalgo/judgetuning .

</details>


### [194] [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)

*Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan*

**Main category:** cs.CL

**Keywords:** Knowledge Base Question Answering, Large Language Models, Monte Carlo Tree Search

**Relevance Score:** 8

**TL;DR:** This paper presents KBQA-o1, a novel method for Knowledge Base Question Answering that utilizes Monte Carlo Tree Search to improve performance with limited annotated data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** KBQA struggles with weak KB awareness, and the balance between effectiveness and efficiency, especially with reliance on annotated data in large-scale systems.

**Method:** The authors propose an agentic method called KBQA-o1 that incorporates a ReAct-based agent for stepwise logical form generation and employs Monte Carlo Tree Search (MCTS) to optimize exploration and performance.

**Key Contributions:**

	1. Introduction of a novel agentic approach to KBQA using Monte Carlo Tree Search (MCTS).
	2. Significant performance improvements in KBQA with limited data, surpassing prior state-of-the-art methods.
	3. Public availability of the code for community utilization and further research.

**Result:** KBQA-o1 demonstrates significant improvement, achieving a GrailQA F1 score of 78.5% with limited annotated data, outperforming previous methods like GPT-3.5-turbo, which scored 48.5%.

**Limitations:** 

**Conclusion:** The experimental results indicate that KBQA-o1 is effective in enhancing KBQA tasks, making it suitable for low-resource settings and improving model performance with minimal data.

**Abstract:** Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. Our code is publicly available.

</details>


### [195] [Thinking beyond the anthropomorphic paradigm benefits LLM research](https://arxiv.org/abs/2502.09192)

*Lujain Ibrahim, Myra Cheng*

**Main category:** cs.CL

**Keywords:** Anthropomorphism, Large Language Models, Human-Computer Interaction, Research Analysis, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper discusses the prevalence of anthropomorphic terminology in research on large language models (LLMs) and argues for broadening the assumptions underpinning this terminology to improve LLM development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the impact of anthropomorphism in LLM research and challenge existing assumptions that may hinder progress in the field.

**Method:** The authors conducted a comprehensive analysis of hundreds of thousands of research articles to identify the use of anthropomorphic language and its implications.

**Key Contributions:**

	1. Identification of prevalent anthropomorphic assumptions in LLM research.
	2. Empirical evidence challenging the use of anthropomorphic terminology.
	3. Proposal of non-anthropomorphic alternatives for evaluating LLMs.

**Result:** The paper finds that anthropomorphic terminology is widespread in LLM research, impacting various assumptions that influence how LLMs are developed and evaluated.

**Limitations:** 

**Conclusion:** The authors advocate for non-anthropomorphic alternatives and reforms to improve understanding and development of LLMs, proposing five key areas for further exploration.

**Abstract:** Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise. In this position paper, we analyze hundreds of thousands of research articles to present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs). We argue for challenging the deeper assumptions reflected in this terminology -- which, though often useful, may inadvertently constrain LLM development -- and broadening beyond them to open new pathways for understanding and improving LLMs. Specifically, we identify and examine five anthropomorphic assumptions that shape research across the LLM development lifecycle. For each assumption (e.g., that LLMs must use natural language for reasoning, or that they should be evaluated on benchmarks originally meant for humans), we demonstrate empirical, non-anthropomorphic alternatives that remain under-explored yet offer promising directions for LLM research and development.

</details>


### [196] [ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition](https://arxiv.org/abs/2502.11198)

*Bidyarthi Paul, Faika Fairuj Preotee, Shuvashis Sarker, Shamim Rahim Refat, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque, Shahriar Manzoor*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Bangla, Dialect, NLP, Transformer models

**Relevance Score:** 7

**TL;DR:** ANCHOLIK-NER is the first benchmark dataset for Named Entity Recognition (NER) in Bangla regional dialects, addressing gaps in NLP for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant need for NER systems in Bangla regional dialects, as current models do not effectively handle the unique linguistic features of these dialects.

**Method:** A new dataset comprising 17,405 sentences across five Bangla regional dialects was created, and three transformer-based models were evaluated on this dataset.

**Key Contributions:**

	1. Introduction of the first benchmark dataset for NER in Bangla regional dialects
	2. Evaluation of three transformer-based NER models on the new dataset
	3. Identification of performance gaps in specific regions like Chittagong.

**Result:** BERT Base Multilingual Cased performed best, especially in Mymensingh with an F1-score of 82.611%, although performance in Chittagong was lower.

**Limitations:** The dataset is limited to five regions and further expansion is needed to include more dialects and improve model performance.

**Conclusion:** This study provides a foundational step towards improving NER systems for regional dialects in Bangla, with future work aimed at enhancing model performance in underperforming regions and expanding the dataset.

**Abstract:** Named Entity Recognition (NER) in regional dialects is a critical yet underexplored area in Natural Language Processing (NLP), especially for low-resource languages like Bangla. While NER systems for Standard Bangla have made progress, no existing resources or models specifically address the challenge of regional dialects such as Barishal, Chittagong, Mymensingh, Noakhali, and Sylhet, which exhibit unique linguistic features that existing models fail to handle effectively. To fill this gap, we introduce ANCHOLIK-NER, the first benchmark dataset for NER in Bangla regional dialects, comprising 17,405 sentences distributed across five regions. The dataset was sourced from publicly available resources and supplemented with manual translations, ensuring alignment of named entities across dialects. We evaluate three transformer-based models - Bangla BERT, Bangla BERT Base, and BERT Base Multilingual Cased - on this dataset. Our findings demonstrate that BERT Base Multilingual Cased performs best in recognizing named entities across regions, with significant performance observed in Mymensingh with an F1-score of 82.611%. Despite strong overall performance, challenges remain in region like Chittagong, where the models show lower precision and recall. Since no previous NER systems for Bangla regional dialects exist, our work represents a foundational step in addressing this gap. Future work will focus on improving model performance in underperforming regions and expanding the dataset to include more dialects, enhancing the development of dialect-aware NER systems.

</details>


### [197] [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/abs/2502.12134)

*Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Large Language Models, continuous-space reasoning, soft thought tokens, parameter-efficient fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper presents a method for enhancing reasoning performance in Large Language Models (LLMs) through continuous-space reasoning without modifying the LLM itself, using a lightweight assistant model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for Chain-of-Thought reasoning in LLMs face limitations due to hard token decoding and the need for full model fine-tuning, which restricts applicability.

**Method:** The proposed approach uses a lightweight fixed assistant model to generate soft thought tokens, which are then mapped into the LLM's representation space via a trainable projection module.

**Key Contributions:**

	1. Introduces a lightweight fixed assistant model for generating soft thought tokens.
	2. Demonstrates improved reasoning performance on multiple benchmarks without requiring full-model fine-tuning.
	3. Provides a parameter-efficient fine-tuning approach for large language models.

**Result:** Experimental results on five reasoning benchmarks show that the method improves LLM reasoning performance through supervised fine-tuning that is parameter-efficient.

**Limitations:** The method may still depend on the quality and capability of the fixed assistant model used for generating soft thought tokens.

**Conclusion:** The approach effectively enhances LLM reasoning capabilities without the need for extensive modifications to existing models, making it practical for state-of-the-art LLM applications.

**Abstract:** Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often require full-model fine-tuning and suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the LLM. Specifically, we employ a lightweight fixed assistant model to speculatively generate instance-specific soft thought tokens as the initial chain of thoughts, which are then mapped into the LLM's representation space via a trainable projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. Source code is available at https://github.com/xuyige/SoftCoT.

</details>


### [198] [Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues](https://arxiv.org/abs/2502.12187)

*Atsushi Suzuki, Yulan He, Feng Tian, Zhongyuan Wang*

**Main category:** cs.CL

**Keywords:** language models, hallucinations, probability theory, computability theory, training data

**Relevance Score:** 8

**TL;DR:** This paper analyzes hallucinations in language models (LMs), showing that while LMs will invariably generate nonfactual content on an infinite input set, improvements in training data and algorithms can significantly reduce the probability of hallucinations, emphasizing a probabilistic approach over a computability-theoretic perspective.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the practical implications of hallucinations in LMs and to challenge the pessimistic views established by computability theory.

**Method:** The authors use a probabilistic approach to demonstrate that with sufficient quality and quantity of training data, hallucinations can be made statistically negligible.

**Key Contributions:**

	1. Demonstration that training data quality can reduce hallucinations
	2. Establishment of a positive theoretical result from a probabilistic viewpoint
	3. Analysis of contradictions between computability and information theory

**Result:** The study finds that while hallucinations are inevitable in a mathematical sense, their occurrence can be reduced through improved training methodologies.

**Limitations:** 

**Conclusion:** The probabilistic results provided in the study suggest that practical strategies can mitigate hallucinations more effectively than the theoretical bounds imply.

**Abstract:** Hallucinations, a phenomenon where a language model (LM) generates nonfactual content, pose a significant challenge to the practical deployment of LMs. While many empirical methods have been proposed to mitigate hallucinations, recent studies established a computability-theoretic result showing that any LM will inevitably generate hallucinations on an infinite set of inputs, regardless of the quality and quantity of training datasets and the choice of the language model architecture and training and inference algorithms. Although the computability-theoretic result may seem pessimistic, its significance in practical viewpoints has remained unclear. This paper claims that those "innate" inevitability results from computability theory and diagonal argument, in principle, cannot explain practical issues of LLMs. We demonstrate this claim by presenting a positive theoretical result from a probabilistic perspective. Specifically, we prove that hallucinations can be made statistically negligible, provided that the quality and quantity of the training data are sufficient. Interestingly, our positive result coexists with the computability-theoretic result, implying that while hallucinations on an infinite set of inputs cannot be entirely eliminated, their probability can always be reduced by improving algorithms and training data. By evaluating the two seemingly contradictory results through the lens of information theory, we argue that our probability-theoretic positive result better reflects practical considerations than the computability-theoretic negative result.

</details>


### [199] [Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text](https://arxiv.org/abs/2502.12953)

*Andrei Jarca, Florinel Alin Croitoru, Radu Tudor Ionescu*

**Main category:** cs.CL

**Keywords:** language modeling, masking, anti-curriculum learning, text classification, sentiment analysis

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel masking technique for pre-training large language models, proposing a task-informed anti-curriculum learning approach that adjusts the masking ratio dynamically.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of masked language modeling by tailoring the masking process to specific downstream tasks to enhance model focus on relevant features.

**Method:** The authors propose a novel task-informed anti-curriculum masking method (TIACBM) that adjusts the masking ratio over time and utilizes task-specific knowledge to select tokens for masking.

**Key Contributions:**

	1. Introduces a task-informed anti-curriculum learning scheme for token masking.
	2. Proposes a cyclic decaying masking ratio for enhanced training dynamics.
	3. Demonstrates significant performance improvements on multiple NLP tasks.

**Result:** The TIACBM approach demonstrates statistically significant performance improvements in three tasks: sentiment analysis, text classification by topic, and authorship attribution.

**Limitations:** 

**Conclusion:** The findings support that adjusting the masking strategy based on task knowledge improves model performance and highlights the importance of focusing on relevant features during training.

**Abstract:** Masked language modeling has become a widely adopted unsupervised technique to pre-train large language models (LLMs). However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.

</details>


### [200] [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/abs/2502.13010)

*Mohammad Reza Rezaei, Reza Saadati Fard, Rahul G. Krishnan, Milad Lankarany*

**Main category:** cs.CL

**Keywords:** Large Language Models, Medical Knowledge Graphs, Question-Answering, Automated Framework, Evidence Retrieval

**Relevance Score:** 9

**TL;DR:** AMG-RAG framework automates medical knowledge graph construction and updating, enhancing medical question-answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of rapidly evolving medical knowledge and the manual process of updating resources for reliable medical question-answering.

**Method:** Introduces the AMG-RAG framework that automates the construction and continuous updating of medical knowledge graphs while integrating reasoning and retrieving external evidence.

**Key Contributions:**

	1. Introduction of a dynamic framework for updating medical knowledge graphs
	2. Integration of reasoning and external evidence retrieval
	3. Improved accuracy and interpretability in medical queries

**Result:** AMG-RAG achieved an F1 score of 74.1 on MEDQA and an accuracy of 66.34 on MEDMCQA, outperforming larger models without increased computational overhead.

**Limitations:** 

**Conclusion:** Automated knowledge graph generation and retrieval methods are critical for providing up-to-date medical insights.

**Abstract:** Large Language Models (LLMs) have significantly advanced medical question-answering by leveraging extensive clinical data and medical literature. However, the rapid evolution of medical knowledge and the labor-intensive process of manually updating domain-specific resources pose challenges to the reliability of these systems. To address this, we introduce Agentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates the construction and continuous updating of medical knowledge graphs, integrates reasoning, and retrieves current external evidence, such as PubMed and WikiSearch. By dynamically linking new findings and complex medical concepts, AMG-RAG not only improves accuracy but also enhances interpretability in medical queries.   Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of 66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to 100 times larger. Notably, these improvements are achieved without increasing computational overhead, highlighting the critical role of automated knowledge graph generation and external evidence retrieval in delivering up-to-date, trustworthy medical insights.

</details>


### [201] [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132)

*Nadav Borenstein, Greta Warren, Desmond Elliott, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** misinformation, fact-checking, community moderation, social media, language models

**Relevance Score:** 6

**TL;DR:** The paper explores the relationship between professional fact-checking and community moderation on Twitter/X, showing that community notes heavily rely on fact-checking sources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To study the dependencies between fact-checking organizations and community moderation in the context of misinformation on social media platforms like Twitter/X.

**Method:** The authors utilized language models to annotate a large corpus of community notes from Twitter/X, focusing on topics, sources cited, and their relationships to misinformation narratives.

**Key Contributions:**

	1. Demonstrated the heavy reliance of community notes on fact-checking sources.
	2. Provided quantitative insights into the citation frequency of fact-checking in community notes.
	3. Highlighted the interdependence of professional and citizen-led fact-checking efforts.

**Result:** The analysis indicated that community notes cite fact-checking sources up to five times more than previously reported, especially when related to broader misinformation narratives.

**Limitations:** 

**Conclusion:** The findings demonstrate that community moderation is significantly enhanced by professional fact-checking, revealing the interconnectedness of citizen and professional fact-checking.

**Abstract:** Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.

</details>


### [202] [Behavioral Analysis of Information Salience in Large Language Models](https://arxiv.org/abs/2502.14613)

*Jan Trienes, JÃ¶rg SchlÃ¶tterer, Junyi Jessy Li, Christin Seifert*

**Main category:** cs.CL

**Keywords:** Large Language Models, text summarization, information salience, explainable AI, model interpretability

**Relevance Score:** 9

**TL;DR:** This paper proposes an explainable framework to explore information salience in Large Language Models (LLMs) through their summarization behavior, revealing a hierarchical notion of salience consistent across various models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand how LLMs determine the importance of information during text summarization, as the salience these models use is not well understood.

**Method:** The authors employ length-controlled summarization as a behavioral probe and trace the answerability of Questions Under Discussion across different models and datasets to derive a proxy for information salience.

**Key Contributions:**

	1. Introduction of a framework to analyze information salience in LLMs
	2. Use of length-controlled summarization to probe content selection
	3. Demonstration of hierarchical salience in LLMs across multiple models

**Result:** Experimental results show that LLMs exhibit a hierarchical notion of salience that is generally consistent across different model families and sizes, although this notion is not accessible through introspection.

**Limitations:** The derived salience cannot be accessed through introspection and shows weak correlation with human perceptions of information importance.

**Conclusion:** While LLMs display consistent salience patterns in their summarization behavior, these patterns do not align strongly with human perceptions of salience, suggesting limitations in current interpretability methods.

**Abstract:** Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.

</details>


### [203] [Predicting Through Generation: Why Generation Is Better for Prediction](https://arxiv.org/abs/2502.17817)

*Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat, Chun-Nam Yu, Mojtaba Soltanalian, Ivan Garibay, Ozlem Garibay, Chen Chen, Niloofar Yousefi*

**Main category:** cs.CL

**Keywords:** Prediction Tasks, Token Generation, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents PredGen, an end-to-end framework that improves prediction tasks by generating output tokens, addressing issues like exposure bias and format mismatch.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper argues that token-level generation is more effective than pooled representations due to retained mutual information, aligning with how LLMs are trained.

**Method:** PredGen uses scheduled sampling to mitigate exposure bias and incorporates a task adapter for converting generated tokens into structured outputs. It also introduces Writer-Director Alignment Loss (WDAL) for consistency.

**Key Contributions:**

	1. Introduces PredGen framework for improved structured prediction
	2. Uses scheduled sampling to reduce exposure bias
	3. Introduces Writer-Director Alignment Loss (WDAL) to enhance coherence and accuracy

**Result:** PredGen consistently outperforms standard baselines on multiple classification and regression benchmarks, demonstrating improved performance in structured prediction tasks.

**Limitations:** 

**Conclusion:** The proposed framework effectively addresses the challenges faced by autoregressive models, enhancing the prediction capabilities of LLMs.

**Abstract:** This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the tasks required output structure. To address these challenges, we introduce PredGen(Predicting Through Generating), an end to end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.

</details>


### [204] [Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/abs/2502.18874)

*Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation framework, ARJudge, multi-faceted analysis, code-driven evaluation

**Relevance Score:** 9

**TL;DR:** ARJudge is a novel evaluation framework for Large Language Models that improves upon traditional evaluators by combining text-based and code-driven analyses to create adaptive evaluation criteria and robust judgments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for evaluating LLM responses are limited by predefined criteria and often fail to adapt to unseen instructions, prompting the need for a more flexible approach.

**Method:** ARJudge consists of an Analyzer that generates comprehensive evaluation analyses and a Refiner that combines these analyses to produce final judgments, using a Composite Analysis Corpus for training.

**Key Contributions:**

	1. Introduction of ARJudge, a framework for adaptive evaluation of LLM responses
	2. Utilization of both text-based and code-driven analyses for comprehensive evaluations
	3. Development of a Composite Analysis Corpus for training the evaluation components.

**Result:** ARJudge demonstrated superior effectiveness and robustness over existing fine-tuned evaluators in LLM evaluation tasks.

**Limitations:** 

**Conclusion:** The framework highlights the value of integrated multi-faceted evaluations and code-driven analyses in enhancing LLM evaluation capabilities.

**Abstract:** Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.

</details>


### [205] [Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases](https://arxiv.org/abs/2502.19249)

*Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen*

**Main category:** cs.CL

**Keywords:** formal language, transfer learning, language models, pretraining, hierarchical dependencies

**Relevance Score:** 8

**TL;DR:** This paper investigates the effects of pretraining language models on formal languages, particularly their impact on the transfer of learning to natural languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to identify which features of formal languages facilitate effective transfer to natural language acquisition in language models.

**Method:** The research involves pre-pretraining transformers on formal languages that capture hierarchical dependencies before transitioning to natural languages.

**Key Contributions:**

	1. Demonstrated lower loss and better generalization through pre-pretraining.
	2. Provided mechanistic evidence linking formal language training to improvements in natural language tasks.
	3. Highlighted the efficiency gains in training with formal languages compared to natural language.

**Result:** Experiments show that models pretrained on formal languages achieve lower loss and better linguistic generalization in natural language tasks, with pre-pretraining being more efficient than direct training on comparable natural language data.

**Limitations:** The study primarily investigates one type of formal language and relies on the specific architecture of the transformers used; generalizability to other models is uncertain.

**Conclusion:** The findings suggest that utilizing formal languages with hierarchical structures can enhance the training efficiency and performance of language models on natural language, highlighting the importance of the model architecture's computational capabilities.

**Abstract:** Pretraining language models on formal language can improve their acquisition of natural language. Which features of the formal language impart an inductive bias that leads to effective transfer? Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when two conditions are met: the formal language should capture the dependency structures present in natural language, and it should remain within the computational limitations of the model architecture. We experiment with pre-pretraining (training on formal language before natural languages) on transformers and find that formal languages capturing hierarchical dependencies indeed enable language models to achieve lower loss on natural language and better linguistic generalization compared to other formal languages. We also find modest support for the hypothesis that the formal language should fall within the computational limitations of the architecture. Strikingly, pre-pretraining reduces loss more efficiently than training on a matched amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. Finally, we also give mechanistic evidence of transfer from formal to natural language: attention heads acquired during pre-pretraining remain crucial for the model's performance on syntactic evaluations.

</details>


### [206] [Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models](https://arxiv.org/abs/2502.20795)

*Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Guo-Xun Ko, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh*

**Main category:** cs.CL

**Keywords:** Large Language Models, Text Generation, Predictive Planning, Test-time Alignment, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Plan2Align is a test-time alignment framework for long-text generation that uses predictive planning to optimize responses without modifying underlying models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing test-time alignment methods fail to ensure coherence in long responses and slow down inference, necessitating a more efficient solution.

**Method:** Plan2Align formulates text generation as a predictive planning problem using Model Predictive Control (MPC) to iteratively refine outputs by optimizing complete responses.

**Key Contributions:**

	1. Introduces predictive planning for text generation
	2. Improves coherence and efficiency in long-text alignment
	3. Demonstrates superior performance on long-form datasets

**Result:** Experiments show that Plan2Align significantly enhances performance in long-text generation compared to existing methods, achieving superior results with improved inference efficiency.

**Limitations:** 

**Conclusion:** Plan2Align provides a lightweight and effective alternative for aligning LLMs during inference, particularly for long-form text generation.

**Abstract:** Aligning Large Language Models with Preference Fine-Tuning is often resource-intensive. Test-time alignment techniques that do not modify the underlying models, such as prompting and guided decodings, offer a lightweight alternative. However, existing test-time alignment methods primarily improve short responses and fail to ensure coherence over extended contexts due to the myopic nature of token-level alignment. Moreover, these methods often incur a slowdown during inference. To address these challenges, we propose Plan2Align, a test-time alignment framework that formulates text generation as a predictive planning problem. Plan2Align adapts Model Predictive Control (MPC) to iteratively refine output by rolling out multiple complete responses and optimizing each segment. To more rigorously evaluate the effectiveness and efficiency, we focus on the more challenging task of long-text generation. Experiments on the long-form response subset of the HH-RLHF dataset and the WMT'24 Discourse-Level Literary Translation demonstrate that Plan2Align significantly enhances the performance of base LLMs. Compared to existing training-time and test-time alignment methods on LLaMA-3.1 8B, Plan2Align achieves comparable or superior results, while also delivering improved inference efficiency relative to prior test-time alignment approaches.

</details>


### [207] [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/abs/2502.20859)

*Yifan Duan, Yihong Tang, Xuefeng Bai, Kehai Chen, Juntao Li, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, personality traits, problem-solving, creativity, multi-agent systems

**Relevance Score:** 9

**TL;DR:** This paper investigates the influence of personality traits on the performance of large language models in problem-solving and creativity, revealing insights into their capabilities in both individual and multi-agent settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLM capabilities align with human intelligence by examining the role of personality traits in task performance.

**Method:** The research assigns Big Five personality traits to LLM agents and evaluates their performance across single- and multi-agent tasks.

**Key Contributions:**

	1. Systematic investigation of LLM performance through the lens of human personality traits.
	2. Demonstrated influence of personality traits on reasoning and creativity tasks.
	3. New insights into the collective intelligence of multi-agent LLM systems.

**Result:** Specific personality traits significantly influence reasoning accuracy in closed tasks and creative output in open tasks, while multi-agent systems manifest a collective intelligence based on personality combinations.

**Limitations:** 

**Conclusion:** Understanding the role of personality in LLMs provides better insights into their performance, revealing their potential for simulation of human-like intelligence in various tasks.

**Abstract:** Large language models (LLMs) excel in both closed tasks (including problem-solving, and code generation) and open tasks (including creative writing), yet existing explanations for their capabilities lack connections to real-world human intelligence. To fill this gap, this paper systematically investigates LLM intelligence through the lens of ``human simulation'', addressing three core questions: (1) \textit{How do personality traits affect problem-solving in closed tasks?} (2) \textit{How do traits shape creativity in open tasks?} (3) \textit{How does single-agent performance influence multi-agent collaboration?} By assigning Big Five personality traits to LLM agents and evaluating their performance in single- and multi-agent settings, we reveal that specific traits significantly influence reasoning accuracy (closed tasks) and creative output (open tasks). Furthermore, multi-agent systems exhibit collective intelligence distinct from individual capabilities, driven by distinguishing combinations of personalities.

</details>


### [208] [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)

*Weixiang Zhao, Yulin Hu, Yang Deng, Jiahe Guo, Xingyu Sui, Xinyang Han, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu*

**Main category:** cs.CL

**Keywords:** role-playing, large language models, safety risks, fine-tuning, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper assesses safety risks in role-play fine-tuning of LLMs and proposes a method called Safety-Aware Role-Play Fine-Tuning (SaRFT) to mitigate those risks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety risks associated with role-playing in large language models, especially concerning villainous characters.

**Method:** The authors trained 95 role-specific LLMs using RoleBench and assessed the impact of role-play fine-tuning on safety performance.

**Key Contributions:**

	1. First comprehensive assessment of role-play fine-tuning risks in LLMs.
	2. Introduction of Safety-Aware Role-Play Fine-Tuning (SaRFT) method.
	3. Demonstration of enhanced safety performance while maintaining role adaptability.

**Result:** The experiments showed a noticeable decline in safety performance due to role-play fine-tuning, with variations based on character traits. SaRFT was shown to consistently outperform state-of-the-art techniques under various fine-tuning conditions.

**Limitations:** The study focuses primarily on character traits and may not encompass all potential safety risks in every role-playing scenario.

**Conclusion:** There is a critical need for implementing role-adaptive safety measures in role-playing LLMs to minimize safety risks.

**Abstract:** Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs.

</details>


### [209] [MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving](https://arxiv.org/abs/2503.03205)

*Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang*

**Main category:** cs.CL

**Keywords:** theorem proving, Machine Learning, Large Language Models, Lean4, error analysis

**Relevance Score:** 4

**TL;DR:** The paper presents MA-LoT, a framework that enhances Lean4 theorem proving by separating tasks of proof generation and error analysis, achieving a significant accuracy improvement.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing LLM methods in theorem proving, particularly the lack of balance between proof generation and error checking.

**Method:** The MA-LoT framework utilizes a model-collaboration approach with structured interaction between an LLM and a Lean4 verifier, integrating Long Chain-of-Thought processes.

**Key Contributions:**

	1. Introduction of the MA-LoT framework for Lean4 theorem proving
	2. Implementation of LoT-Transfer Learning for enhanced LLM capabilities
	3. Demonstrated performance improvement over existing methods in theorem proving tasks

**Result:** The framework achieves a 61.07% accuracy on the Lean4 MiniF2F-Test dataset, surpassing other models significantly in both whole-proof generation and proof correction tasks.

**Limitations:** 

**Conclusion:** MA-LoT demonstrates the effectiveness of combining Long CoT and formal verification methods for improved theorem proving in Lean4.

**Abstract:** Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted the mathematical and computer science communities. State-of-the-art methods utilize a single Large Language Model (LLM) to generate complete proof or perform tree search, but they fail to balance these tasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long Chain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve this issue. It separates the cognition tasks of general NL for whole-proof generation and error analysis for proof correction using the model-collaboration method. We achieve this by structured interaction of the LLM and Lean4 verifier in Long CoT. To implement the framework, we propose the novel *LoT-Transfer Learning* training-inference pipeline, which enables the Long CoT thinking capability to LLMs without special data annotation. Extensive experiment shows that our framework achieves a **61.07%** accuracy rate on the Lean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3 (33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective.

</details>


### [210] [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/abs/2503.04615)

*Ashok Urlana, Gopichand Kanumolu, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati, Rahul Mishra*

**Main category:** cs.CL

**Keywords:** hallucination detection, language models, consistency patterns, benchmark dataset, AI

**Relevance Score:** 9

**TL;DR:** HalluCounter is a new method for hallucination detection in language models that utilizes consistency patterns and introduces a benchmark dataset for evaluation.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing hallucination detection methods that rely on internal model states and the lack of diverse benchmark datasets.

**Method:** HalluCounter employs response-response and query-response consistency and alignment patterns to train a classifier for hallucination detection.

**Key Contributions:**

	1. Introduction of HalluCounter for reference-free hallucination detection
	2. Use of consistency patterns for improved accuracy
	3. Development of HalluCounterEval dataset for benchmark evaluation

**Result:** HalluCounter achieves over 90% average confidence in hallucination detection, outperforming state-of-the-art methods.

**Limitations:** 

**Conclusion:** The proposed method and the accompanying benchmark dataset significantly improve hallucination detection accuracy and flexibility.

**Abstract:** Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs. However, their inability to capture query-response alignment patterns often results in lower detection accuracy. Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope. To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns. This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries. Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains. Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90\% average confidence in hallucination detection across datasets.

</details>


### [211] [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/abs/2503.09598)

*Ruohao Guo, Wei Xu, Alan Ritter*

**Main category:** cs.CL

**Keywords:** Large Language Models, implicit misinformation, EchoMist, mitigation methods, empirical studies

**Relevance Score:** 9

**TL;DR:** This paper introduces EchoMist, a benchmark for assessing Large Language Models' ability to handle implicit misinformation, revealing significant shortcomings in current models and proposing methods to improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The widespread deployment of Large Language Models raises concerns about their potential to tacitly spread implicit misinformation, a critical issue not adequately addressed by existing evaluations.

**Method:** The authors curated EchoMist, a benchmark containing queries with embedded false assumptions, and conducted empirical studies on 15 state-of-the-art LLMs to evaluate their performance in detecting implicit misinformation.

**Key Contributions:**

	1. Introduction of the EchoMist benchmark for implicit misinformation
	2. Empirical analysis of 15 state-of-the-art LLMs' performance on implicit misinformation
	3. Investigation of mitigation strategies (Self-Alert and RAG)

**Result:** The study finds that current LLMs perform poorly on detecting false premises, frequently generating incorrect counterfactual explanations.

**Limitations:** The study primarily focuses on implicit misinformation without delving deeply into the specific contexts and nuances of various misinformation sources.

**Conclusion:** The findings highlight the ongoing challenge of implicit misinformation and point to the necessity of developing safeguards within LLMs.

**Abstract:** As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world interactions. We curated EchoMist, the first comprehensive benchmark for implicit misinformation, where false assumptions are embedded in the query to LLMs. EchoMist targets circulated, harmful, and ever-evolving implicit misinformation from diverse sources, including realistic human-AI conversations and social media interactions. Through extensive empirical studies on 15 state-of-the-art LLMs, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating counterfactual explanations. We also investigate two mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability to counter implicit misinformation. Our findings indicate that EchoMist remains a persistent challenge and underscore the critical need to safeguard against the risk of implicit misinformation.

</details>


### [212] [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/abs/2503.11985)

*Charaka Vinayak Kumar, Ashok Urlana, Gopichand Kanumolu, Bala Mallikarjunarao Garlapati, Pruthwik Mishra*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias detection, Prompting approaches, Natural language understanding, Evaluation metrics

**Relevance Score:** 9

**TL;DR:** This paper evaluates biases in various LLMs and proposes methods for bias detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs reflecting biases from training data, the study aims to unify the evaluation of LLM biases and provide insights for mitigation.

**Method:** A set of representative small and medium-sized LLMs were evaluated using five different prompting approaches across various benchmarks to detect biases.

**Key Contributions:**

	1. Unified evaluation of biases in LLMs
	2. Proposed five new prompting approaches for bias detection
	3. Insightful research questions formulated to guide future studies on biases.

**Result:** The evaluation showed that all selected LLMs exhibit some form of bias, with the Phi-3.5B model identified as the least biased.

**Limitations:** The study is limited to the selected models and may not represent biases across all LLMs.

**Conclusion:** The paper highlights key challenges in bias detection in LLMs and suggests future research directions in this domain.

**Abstract:** Advancements in Large Language Models (LLMs) have increased the performance of different natural language understanding as well as generation tasks. Although LLMs have breached the state-of-the-art performance in various tasks, they often reflect different forms of bias present in the training data. In the light of this perceived limitation, we provide a unified evaluation of benchmarks using a set of representative small and medium-sized LLMs that cover different forms of biases starting from physical characteristics to socio-economic categories. Moreover, we propose five prompting approaches to carry out the bias detection task across different aspects of bias. Further, we formulate three research questions to gain valuable insight in detecting biases in LLMs using different approaches and evaluation metrics across benchmarks. The results indicate that each of the selected LLMs suffer from one or the other form of bias with the Phi-3.5B model being the least biased. Finally, we conclude the paper with the identification of key challenges and possible future directions.

</details>


### [213] [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/abs/2503.13857)

*Rui Yang, Jiayi Tong, Haoyuan Wang, Hui Huang, Ziyang Hu, Peiyu Li, Nan Liu, Christopher J. Lindsell, Michael J. Pencina, Yong Chen, Chuan Hong*

**Main category:** cs.CL

**Keywords:** preprint publication, automated confidence assessment, LLM, systematic reviews, machine learning

**Relevance Score:** 7

**TL;DR:** A framework, AutoConfidence, is proposed for predicting preprint publication through automated data extraction and multiple predictive features.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by variable quality of preprints in systematic reviews by providing an automated solution for confidence assessment.

**Method:** Leveraging natural language processing for automated data extraction, using semantic embeddings of titles and abstracts, and applying LLM-driven evaluation scores with two models: a random forest classifier and a survival cure model.

**Key Contributions:**

	1. Introduction of AutoConfidence for predicting preprint publication quality
	2. Utilization of natural language processing for automated data extraction
	3. Integration of semantic embeddings and LLM evaluation for improved prediction accuracy

**Result:** The random forest classifier achieved AUROC scores improving from 0.692 to 0.747, while the survival cure model improved from AUROC 0.716 to 0.731 with advanced features.

**Limitations:** 

**Conclusion:** AutoConfidence enhances predictive performance in preprint publication prediction and reduces the manual curation burden, aiding systematic reviews.

**Abstract:** Background. Systematic reviews in comparative effectiveness research require timely evidence synthesis. Preprints accelerate knowledge dissemination but vary in quality, posing challenges for systematic reviews.   Methods. We propose AutoConfidence (automated confidence assessment), an advanced framework for predicting preprint publication, which reduces reliance on manual curation and expands the range of predictors, including three key advancements: (1) automated data extraction using natural language processing techniques, (2) semantic embeddings of titles and abstracts, and (3) large language model (LLM)-driven evaluation scores. Additionally, we employed two prediction models: a random forest classifier for binary outcome and a survival cure model that predicts both binary outcome and publication risk over time.   Results. The random forest classifier achieved AUROC 0.692 with LLM-driven scores, improving to 0.733 with semantic embeddings and 0.747 with article usage metrics. The survival cure model reached AUROC 0.716 with LLM-driven scores, improving to 0.731 with semantic embeddings. For publication risk prediction, it achieved a concordance index of 0.658, increasing to 0.667 with semantic embeddings.   Conclusion. Our study advances the framework for preprint publication prediction through automated data extraction and multiple feature integration. By combining semantic embeddings with LLM-driven evaluations, AutoConfidence enhances predictive performance while reducing manual annotation burden. The framework has the potential to facilitate incorporation of preprint articles during the appraisal phase of systematic reviews, supporting researchers in more effective utilization of preprint resources.

</details>


### [214] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)

*Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li*

**Main category:** cs.CL

**Keywords:** large reasoning models, meta-abilities, reinforcement learning, deduction, induction

**Relevance Score:** 8

**TL;DR:** This paper proposes a method to enhance long-chain reasoning in large models through explicit alignment with meta-abilities using RL, demonstrating significant performance improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unpredictable timing and consistency of emergent reasoning behaviors in large reasoning models (LRMs), and to provide a more scalable and reliable framework for reasoning capabilities.

**Method:** The authors implement a three-stage pipeline involving individual alignment, parameter-space merging, and domain-specific reinforcement learning to explicitly align models with meta-abilities of deduction, induction, and abduction.

**Key Contributions:**

	1. Introduced a method for explicit alignment of LRMs with meta-abilities.
	2. Demonstrated significant performance improvements across various benchmarks.
	3. Provided a scalable framework for enhancing reasoning capabilities.

**Result:** The proposed method boosts performance by over 10% compared to instruction-tuned baselines and yields additional performance gains in domain-specific tasks across multiple benchmarks.

**Limitations:** 

**Conclusion:** Explicit meta-ability alignment offers a scalable and dependable foundation for reasoning in large reasoning models, enhancing both consistency and performance.

**Abstract:** Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional gain in performance ceiling for both 7B and 32B models across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>
