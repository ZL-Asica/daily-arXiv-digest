{"id": "2504.18691", "pdf": "https://arxiv.org/pdf/2504.18691.pdf", "abs": "https://arxiv.org/abs/2504.18691", "title": "From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions", "authors": ["Ali Alfageeh", "Sadegh AlMahdi Kazemi Zarkouei", "Daye Nam", "Daniel Prol", "Matin Amoozadeh", "Souti Chattopadhyay", "James Prather", "Paul Denny", "Juho Leinonen", "Michael Hilton", "Sruti Srinivasa Ragavan", "Mohammad Amin Alipour"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Background and Context. The increasing integration of large language models\n(LLMs) in computing education presents an emerging challenge in understanding\nhow students use LLMs and craft prompts to solve computational tasks. Prior\nresearch has used both qualitative and quantitative methods to analyze\nprompting behavior, but these approaches lack scalability or fail to\neffectively capture the semantic evolution of prompts. Objective. In this\npaper, we investigate whether students prompts can be systematically analyzed\nusing propositional logic constraints. We examine whether this approach can\nidentify patterns in prompt evolution, detect struggling students, and provide\ninsights into effective and ineffective strategies. Method. We introduce\nPrompt2Constraints, a novel method that translates students prompts into\nlogical constraints. The constraints are able to represent the intent of the\nprompts in succinct and quantifiable ways. We used this approach to analyze a\ndataset of 1,872 prompts from 203 students solving introductory programming\ntasks. Findings. We find that while successful and unsuccessful attempts tend\nto use a similar number of constraints overall, when students fail, they often\nmodify their prompts more significantly, shifting problem-solving strategies\nmidway. We also identify points where specific interventions could be most\nhelpful to students for refining their prompts. Implications. This work offers\na new and scalable way to detect students who struggle in solving natural\nlanguage programming tasks. This work could be extended to investigate more\ncomplex tasks and integrated into programming tools to provide real-time\nsupport.", "AI": {"tldr": "This paper investigates the prompting behavior of students using LLMs in computing education through a novel method called Prompt2Constraints, which analyzes prompts as logical constraints to identify patterns and support struggling students.", "motivation": "Understanding how students use large language models (LLMs) in computational tasks is essential for improving educational outcomes in computing education.", "method": "The authors introduce a method called Prompt2Constraints that translates student prompts into logical constraints, allowing for systematic analysis of prompting behavior.", "result": "Analysis of 1,872 prompts from 203 students revealed that while successful and unsuccessful prompts had a similar number of constraints, unsuccessful prompts often involved greater modifications, indicating a shift in problem-solving strategies.", "conclusion": "The findings suggest that Prompt2Constraints enables the identification of students who struggle with natural language programming tasks, offering potential for real-time support in programming tools.", "key_contributions": ["Introduction of Prompt2Constraints method for systematic analysis of student prompts.", "Identification of patterns in prompt evolution and strategies used by students.", "Implications for providing interventions to support students in real-time."], "limitations": "", "future_work": "Future research could explore the application of this method to more complex computational tasks and further integration into educational programming tools.", "keywords": ["large language models", "prompt analysis", "computing education", "natural language programming", "student interventions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18759", "pdf": "https://arxiv.org/pdf/2504.18759.pdf", "abs": "https://arxiv.org/abs/2504.18759", "title": "Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias", "authors": ["Nick von Felten"], "categories": ["cs.HC"], "comment": "Is published at the CHI 2025 Workshop \"Tools for Thought: Research\n  and Design for Understanding, Protecting, and Augmenting Human Cognition with\n  Generative AI\"", "summary": "Isolated perspectives have often paved the way for great scientific\ndiscoveries. However, many breakthroughs only emerged when moving away from\nsingular views towards interactions. Discussions on Artificial Intelligence\n(AI) typically treat human and AI bias as distinct challenges, leaving their\ndynamic interplay and compounding potential largely unexplored. Recent research\nsuggests that biased AI can amplify human cognitive biases, while\nwell-calibrated systems might help mitigate them. In this position paper, I\nadvocate for transcending beyond separate treatment of human and AI biases and\ninstead focus on their interaction effects. I argue that a comprehensive\nframework, one that maps (compound human-AI) biases to mitigation strategies,\nis essential for understanding and protecting human cognition, and I outline\nconcrete steps for its development.", "AI": {"tldr": "This position paper advocates for an integrated approach to understanding the interaction between human and AI biases, proposing a framework to map these biases to mitigation strategies to protect human cognition.", "motivation": "There is a need to explore the dynamic interplay between human and AI biases, which is often overlooked in discussions about AI. Many breakthroughs in science arise from understanding interactions rather than isolated perspectives.", "method": "The paper proposes a comprehensive framework that identifies and maps compound human-AI biases to specific mitigation strategies, calling for an evaluation of how these biases interact.", "result": "The author highlights that biased AI can amplify human cognitive biases, while well-calibrated AI can help in mitigating these biases, emphasizing the importance of examining their interactions.", "conclusion": "A thorough understanding of the interaction between human and AI biases is crucial for the protection of human cognition, and necessary steps for developing a mapping framework are outlined.", "key_contributions": ["Advocates for a comprehensive framework addressing human-AI bias interactions.", "Highlights the potential of calibrated systems in mitigating cognitive biases.", "Outlines concrete steps for future development of bias mitigation strategies."], "limitations": "", "future_work": "Further research is needed to refine the framework and empirically validate the proposed mapping of biases to mitigation strategies.", "keywords": ["Human-AI interaction", "Cognitive bias", "AI bias", "Mitigation strategies", "Human cognition"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.18807", "pdf": "https://arxiv.org/pdf/2504.18807.pdf", "abs": "https://arxiv.org/abs/2504.18807", "title": "Clones in the Machine: A Feminist Critique of Agency in Digital Cloning", "authors": ["Siân Brooke"], "categories": ["cs.HC", "cs.AI"], "comment": "ACM CHI Conference on Human Factors in Computing Systems 2025", "summary": "This paper critiques digital cloning in academic research, highlighting how\nit exemplifies AI solutionism. Digital clones, which replicate user data to\nsimulate behavior, are often seen as scalable tools for behavioral insights.\nHowever, this framing obscures ethical concerns around consent, agency, and\nrepresentation. Drawing on feminist theories of agency, the paper argues that\ndigital cloning oversimplifies human complexity and risks perpetuating systemic\nbiases. To address these issues, it proposes decentralized data repositories\nand dynamic consent models, promoting ethical, context-aware AI practices that\nchallenge the reductionist logic of AI solutionism", "AI": {"tldr": "The paper critiques digital cloning in academic research, emphasizing ethical concerns surrounding consent and representation, while proposing decentralized data repositories as a solution.", "motivation": "The paper aims to address the ethical implications of digital cloning in research, highlighting issues of consent, agency, and representation that are often overlooked.", "method": "The critique is based on feminist theories of agency and provides a framework for understanding the impact of digital cloning.", "result": "The findings reveal that digital cloning can oversimplify human complexity and may reinforce systemic biases due to its reductionist nature.", "conclusion": "The paper concludes that implementing decentralized data repositories and dynamic consent models can promote more ethical AI practices.", "key_contributions": ["Critique of AI solutionism in digital cloning", "Integration of feminist theories to address ethical concerns", "Proposals for decentralized data and dynamic consent models"], "limitations": "The paper does not provide empirical data to support its arguments or proposed solutions.", "future_work": "Future research could explore the practical implementation of the proposed decentralized data repositories and dynamic consent models.", "keywords": ["digital cloning", "AI ethics", "decentralized data", "dynamic consent", "feminist theories"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.18817", "pdf": "https://arxiv.org/pdf/2504.18817.pdf", "abs": "https://arxiv.org/abs/2504.18817", "title": "Understanding Decentralized Social Feed Curation on Mastodon", "authors": ["Yuhan Liu", "Emmy Song", "Owen Xingjian Zhang", "Jewel Merriman", "Lei Zhang", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Accepted at CSCW 2025", "summary": "As centralized social media platforms face growing concerns, more users are\nseeking greater control over their social feeds and turning to decentralized\nalternatives such as Mastodon. The decentralized nature of Mastodon creates\nunique opportunities for customizing feeds, yet user perceptions and curation\nstrategies on these platforms remain unknown. This paper presents findings from\na two-part interview study with 21 Mastodon users, exploring how they perceive,\ninteract with, and manage their current feeds, and how we can better empower\nusers to personalize their feeds on Mastodon. We use the qualitative findings\nof the first part of the study to guide the creation of Braids, a web-based\nprototype for feed curation. Results from the second part of our study, using\nBraids, highlighted opportunities and challenges for future research,\nparticularly in using seamful design to enhance people's acceptance of\nalgorithmic curation and nuanced trade-offs between machine learning-based and\nrule-based curation algorithms. To optimize user experience, we also discuss\nthe tension between creating new apps and building add-ons in the decentralized\nsocial media realm.", "AI": {"tldr": "This paper explores user interactions with decentralized social media platform Mastodon through qualitative interviews and presents a prototype for personalized feed curation.", "motivation": "To understand user perceptions and interactions on decentralized social media platforms like Mastodon and empower personalization of feeds.", "method": "A two-part interview study with 21 Mastodon users followed by the creation of a web-based prototype, Braids, for feed curation.", "result": "The findings reveal user interactions with feeds and highlight challenges and opportunities for using design to improve acceptance of algorithmic curation.", "conclusion": "User experiences can be optimized by addressing the balance between app development and add-ons in decentralized social media.", "key_contributions": ["Identified user perceptions and curation strategies on Mastodon", "Developed a web-based prototype, Braids, for feed personalization", "Discussed implications of algorithmic vs rule-based curation in decentralized platforms"], "limitations": "The study is limited to a small sample size of 21 users and focuses solely on Mastodon.", "future_work": "Future research could explore broader applications of seamful design in decentralized social media and user acceptance of curation algorithms.", "keywords": ["Mastodon", "decentralized social media", "feed curation", "user experience", "algorithmic curation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560.pdf", "abs": "https://arxiv.org/abs/2504.18560", "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination.", "AI": {"tldr": "Introducing MLA-BiTe, a framework for multilingual bias testing in LLMs.", "motivation": "To systematically evaluate and mitigate social biases in large language models due to their training data.", "method": "MLA-BiTe uses automated translation and paraphrasing to conduct comprehensive multilingual bias assessments.", "result": "The effectiveness of MLA-BiTe was demonstrated by testing four state-of-the-art LLMs in six languages, focusing on multiple sensitive discrimination categories.", "conclusion": "MLA-BiTe provides a significant advancement in bias evaluation methods by allowing for a broader exploration of biases across languages.", "key_contributions": ["Development of the MLA-BiTe framework for multilingual bias testing", "Application of automated translation and paraphrasing for bias assessment", "Evaluation of LLMs across multiple languages, including low-resource options."], "limitations": "The performance of translations and the representation of certain languages may affect bias assessment accuracy.", "future_work": "Further exploration of bias assessment across additional languages and improving translation techniques for better accuracy.", "keywords": ["Large Language Models", "Bias Testing", "Multilingual", "Natural Language Processing", "Social Biases"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919.pdf", "abs": "https://arxiv.org/abs/2504.18919", "title": "Clinical knowledge in LLMs does not translate to human interactions", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.", "AI": {"tldr": "A study tests the effectiveness of large language models (LLMs) in assisting the public with medical decision-making, revealing significant gaps in real-world performance despite high accuracy in controlled assessments.", "motivation": "Investigate the potential of LLMs in providing medical advice and support to the public amidst concerns about their real-world application versus clinical testing outcomes.", "method": "In a controlled study with 1,298 participants, individuals received assistance from either LLMs (GPT-4o, Llama 3, Command R+) or a source of their choice (control) in ten medical scenarios.", "result": "LLMs demonstrated high accuracy in identifying conditions (94.9%) and dispositions (56.3%) when evaluated alone, but users aided by LLMs only identified conditions in 34.5% and dispositions in 44.2% of cases, similar to the control group.", "conclusion": "The user interaction with LLMs poses challenges to their effectiveness in medical contexts, indicating a need for rigorous user testing before public deployment.", "key_contributions": ["Demonstrated the gap between LLMs' performance in controlled tests versus real-world applicability.", "Highlighted the importance of user interaction in the effectiveness of LLMs in healthcare advice.", "Recommended systematic human user testing to ensure the readiness of LLMs for public healthcare use."], "limitations": "Study limited to ten medical scenarios and may not represent all potential healthcare advice situations; controlled environment may not reflect real-world dynamics.", "future_work": "Encourage further research on human user testing for assessing interaction capabilities of LLMs in healthcare before deployment.", "keywords": ["Large language models", "Healthcare", "Medical advice", "User interaction", "Systematic testing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639.pdf", "abs": "https://arxiv.org/abs/2504.18639", "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses.", "AI": {"tldr": "This paper presents a framework for detecting hallucinations in LLM-generated answers by integrating Semantic Role Labeling with a DeBERTa-based textual entailment model, demonstrating competitive detection performance in experiments.", "motivation": "Improving factual consistency in LLM-generated answers necessitates effective methods for detecting hallucinations.", "method": "A span-level hallucination detection framework that utilizes Semantic Role Labeling to decompose answers and a DeBERTa-based textual entailment model to evaluate semantic alignment against a reference context.", "result": "Experiments on the Mu-SHROOM dataset show competitive performance in detecting hallucinated spans, verified through fact-checking with GPT-4 and LLaMA.", "conclusion": "The proposed framework contributes significantly to the task of hallucination detection in LLM-generated responses.", "key_contributions": ["Development of a span-level hallucination detection framework for LLM-generated texts.", "Integration of Semantic Role Labeling to enhance contextual evaluation.", "Validation of hallucination detection through advanced fact-checking methods."], "limitations": "The framework currently focuses only on English and Arabic texts and may require adaptation for other languages.", "future_work": "Exploring the application of the framework to additional languages and contexts in LLM-generated content.", "keywords": ["hallucination detection", "Semantic Role Labeling", "LLM-generated texts"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.18932", "pdf": "https://arxiv.org/pdf/2504.18932.pdf", "abs": "https://arxiv.org/abs/2504.18932", "title": "AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression", "authors": ["Dong Whi Yoo", "Jiayue Melissa Shi", "Violeta J. Rodriguez", "Koustuv Saha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advancements in LLMs enable chatbots to interact with individuals on a\nrange of queries, including sensitive mental health contexts. Despite\nuncertainties about their effectiveness and reliability, the development of\nLLMs in these areas is growing, potentially leading to harms. To better\nidentify and mitigate these harms, it is critical to understand how the values\nof people with lived experiences relate to the harms. In this study, we\ndeveloped a technology probe, a GPT-4o based chatbot called Zenny, enabling\nparticipants to engage with depression self-management scenarios informed by\nprevious research. We used Zenny to interview 17 individuals with lived\nexperiences of depression. Our thematic analysis revealed key values:\ninformational support, emotional support, personalization, privacy, and crisis\nmanagement. This work explores the relationship between lived experience\nvalues, potential harms, and design recommendations for mental health AI\nchatbots, aiming to enhance self-management support while minimizing risks.", "AI": {"tldr": "This study examines the development and user insights of a chatbot called Zenny for depression self-management, focusing on values related to harms in mental health AI.", "motivation": "To understand how the values of individuals with lived experiences of depression relate to potential harms caused by AI mental health chatbots.", "method": "A technology probe in the form of a GPT-4o based chatbot was developed and used to interview 17 individuals with lived experiences of depression; thematic analysis was conducted on the responses.", "result": "Key values identified include informational support, emotional support, personalization, privacy, and crisis management, which are crucial for improving chatbot design.", "conclusion": "The study provides insights into lived experience values that can inform the design of more effective and safer AI chatbots for mental health support.", "key_contributions": ["Development of Zenny, a chatbot for depression self-management", "Identification of key values important for mental health AI design", "Recommendations for minimizing risks in AI chatbots for mental health"], "limitations": "The study is limited by its small sample size (17 participants) and could benefit from a broader demographic.", "future_work": "Future research should explore the implementation of these values in wider contexts and with diverse populations.", "keywords": ["mental health", "AI chatbots", "user experience", "depression", "ethical design"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673.pdf", "abs": "https://arxiv.org/abs/2504.18673", "title": "Can Third-parties Read Our Emotions?", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states.", "AI": {"tldr": "The study evaluates the effectiveness of third-party annotations in emotion recognition tasks, finding that LLMs generally outperform human annotators, with improvements possible through demographic similarity and refined annotation practices.", "motivation": "To investigate the reliability of third-party annotations in capturing authors' private emotional states in Natural Language Processing tasks.", "method": "Human subjects experiments were conducted to compare third-party annotations with first-party emotion labels in emotion recognition tasks.", "result": "Findings indicate significant limitations in third-party annotations, with LLMs performing better than human annotators. Improvements in performance were noted when incorporating demographic similarities.", "conclusion": "The study calls for improved annotation practices and a framework for evaluating third-party annotations to better model authors' private states.", "key_contributions": ["Demonstrated limitations of third-party annotations in capturing authors' emotions", "Showed that LLMs outperform human annotators in emotion recognition tasks", "Introduced a framework for evaluating annotation practices."], "limitations": "The study primarily focuses on emotion recognition tasks and may not generalize across other NLP applications.", "future_work": "Recommendations for future research include refining annotation processes and further exploring the role of demographic factors in annotation quality.", "keywords": ["Natural Language Processing", "emotion recognition", "third-party annotations", "large language models", "annotation practices"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.18969", "pdf": "https://arxiv.org/pdf/2504.18969.pdf", "abs": "https://arxiv.org/abs/2504.18969", "title": "Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)", "authors": ["Meisam J. Sekiavandi", "Laurits Dixen", "Jostein Fimland", "Sree Keerthi Desu", "Antonia-Bianca Zserai", "Ye Sul Lee", "Maria Barrett", "Paolo Burre"], "categories": ["cs.HC"], "comment": null, "summary": "Emotion recognition has the potential to play a pivotal role in enhancing\nhuman-computer interaction by enabling systems to accurately interpret and\nrespond to human affect. Yet, capturing emotions in face-to-face contexts\nremains challenging due to subtle nonverbal cues, variations in personal\ntraits, and the real-time dynamics of genuine interactions. Existing emotion\nrecognition datasets often rely on limited modalities or controlled conditions,\nthereby missing the richness and variability found in real-world scenarios.\n  In this work, we introduce Advancing Face-to-Face Emotion Communication\n(AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses\n84 simulated emotional dialogues across six distinct emotions, recorded from 73\nparticipants over more than 5,000 trials and annotated with more than 20,000\nlabels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin\nresponse (GSR), facial videos, and Big Five personality assessments. Crucially,\nAFFEC explicitly distinguishes between felt emotions (the participant's\ninternal affect) and perceived emotions (the observer's interpretation of the\nstimulus).\n  Baseline analyses spanning unimodal features and straightforward multimodal\nfusion demonstrate that even minimal processing yields classification\nperformance significantly above chance, especially for arousal. Incorporating\npersonality traits further improves predictions of felt emotions, highlighting\nthe importance of individual differences. By bridging controlled\nexperimentation with more realistic face-to-face stimuli, AFFEC offers a unique\nresource for researchers aiming to develop context-sensitive, adaptive, and\npersonalized emotion recognition models.", "AI": {"tldr": "The paper presents the AFFEC dataset designed for emotion recognition in human-computer interaction, featuring multimodal data from 73 participants in simulated emotional dialogues.", "motivation": "To improve emotion recognition systems by capturing the complexity of human emotions in real-world interactions, addressing the limitations of current datasets.", "method": "The study introduces the AFFEC dataset, which includes various modalities such as EEG, eye-tracking, GSR, and facial videos, recorded during 84 emotional dialogues involving six emotions. Baseline analyses were conducted to test classification performance.", "result": "Initial analyses show that even simple processing methods can significantly outperform chance-level classification, particularly in identifying arousal, with added personality traits enhancing predictions of felt emotions.", "conclusion": "The AFFEC dataset enables the development of more nuanced emotion recognition models, taking into account individual differences and contextual factors in emotional communication.", "key_contributions": ["Introduction of the AFFEC dataset with extensive multimodal data", "Distinction between felt and perceived emotions", "Demonstration of significant classification improvements with minimal processing"], "limitations": "The dataset is based on simulated interactions, which may not fully replicate the complexity of genuine emotional exchanges in natural settings.", "future_work": "Further exploration of adaptive emotion recognition systems that leverage the AFFEC dataset in more diverse and uncontrolled environments.", "keywords": ["emotion recognition", "multimodal dataset", "human-computer interaction", "affect", "EEG"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715.pdf", "abs": "https://arxiv.org/abs/2504.18715", "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation.", "AI": {"tldr": "The paper presents a novel concept of spatial speech translation for hearables that translates speech in the wearer’s environment while preserving spatial cues and speaker characteristics, achieving effective results in real-world conditions.", "motivation": "To enable users in crowded multilingual environments to understand speech by translating it into their native language while maintaining the spatial characteristics of each speaker.", "method": "The methodology involves addressing technical challenges in blind source separation, localization, real-time translation, and binaural rendering for accurate spatial audio output on Apple M2 silicon.", "result": "The system achieves a BLEU score of up to 22.01 in translating speech even in noisy environments, significantly outperforming existing models.", "conclusion": "This research marks a significant step towards the integration of spatial perception in speech translation, paving the way for more natural interactions between users and multilingual environments.", "key_contributions": ["Introduction of spatial speech translation for hearables", "Real-time translation maintaining spatial cues and speaker characteristics", "Proof-of-concept evaluations demonstrating effectiveness in real-world settings"], "limitations": "The current prototype is a proof-of-concept, and further development is needed for commercial applications and broader language support.", "future_work": "Explore enhancements for broader language pairs, integration with more sophisticated processing algorithms, and improvements in user experience.", "keywords": ["Spatial Speech Translation", "Hearables", "Binaural Rendering", "Real-time Translation", "Speech Localization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research.", "AI": {"tldr": "The paper presents LINC, a multimodal system designed to enhance collaboration among ESL researchers in multilingual teams, addressing communication challenges through real-time support and post-meeting analysis.", "motivation": "The paper addresses the difficulties faced by ESL researchers in communicating and contributing during multilingual collaborative meetings, highlighting the need for effective support systems.", "method": "The authors conducted a survey of 64 ESL researchers to identify design goals and subsequently developed the LINC system, which includes a real-time communication module and a post-meeting dashboard. A two-phased evaluation study was performed with six multilingual team triads to assess user experience and communication effectiveness.", "result": "Participants using LINC reported improved communication in their preferred language, enhanced recall of meeting insights, and better preparedness for future meetings, demonstrating the system's effectiveness in addressing language barriers.", "conclusion": "LINC supports effective collaboration in multilingual settings, suggesting that factors beyond language preferences also affect participation, emphasizing the relevance of multimodal systems in such collaborative environments.", "key_contributions": ["Development of a multimodal collaboration system (LINC) for ESL researchers.", "Real-time communication and post-meeting analysis features that support multilingual discussions.", "Identification of factors impacting multilingual meeting participation beyond language."], "limitations": "The study involved a small sample size and was conducted in specific multilingual contexts, which may limit generalizability.", "future_work": "Future research should explore further integrations of multimodal features and their impact on diverse collaborative contexts beyond research environments.", "keywords": ["multilingual communication", "collaborative research", "language barriers", "ESL researchers", "multimodal systems"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718.pdf", "abs": "https://arxiv.org/abs/2504.18718", "title": "Building UD Cairo for Old English in the Classroom", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance.", "AI": {"tldr": "This paper presents a treebank for Old English based on classroom data collection using LLM prompting and authentic texts, highlighting challenges and suggesting improvements in annotation and parsing performance.", "motivation": "To improve understanding and teaching of Old English through the creation of a treebank that utilizes both LLMs and authentic data.", "method": "The authors collected and annotated a sample of 20 sentences in Old English, involving multiple beginner annotators and comparing their results, while also conducting parsing experiments.", "result": "The study found LLM outputs in Old English to be inaccurate but correctable with post-editing, and showed that beginner annotations, while imperfect, can yield useful results when aggregated.", "conclusion": "While current LLMs struggle with Old English syntax, collaborative annotation and incorporating lessons from authentic data can enhance both learning and data quality.", "key_contributions": ["Creation of a treebank for Old English using classroom data.", "Utilization of LLM prompting alongside authentic data collections.", "Insights into the effectiveness of beginner annotators in producing reliable linguistic data."], "limitations": "The performance of LLMs in generating authentic Old English syntax remains inadequate without significant post-editing.", "future_work": "Further research is needed to improve LLM performance on historical languages and to explore more systematic approaches to training beginner annotators.", "keywords": ["Old English", "treebank", "linguistic annotation", "LLM", "historical linguistics"], "importance_score": 2, "read_time_minutes": 7}}
{"id": "2504.19010", "pdf": "https://arxiv.org/pdf/2504.19010.pdf", "abs": "https://arxiv.org/abs/2504.19010", "title": "Investigating the Prominence and Severity of Bugs and Glitches Within Games and Their Effects on Player Experience", "authors": ["Jessica Backus"], "categories": ["cs.HC"], "comment": "7 pages, 2 figures", "summary": "Different errors that occur in video games are often referred to as glitches\nor bugs. The goal of this exploratory research is to understand how these\nglitches and bugs within video games affect a players experience. To do this, I\nreviewed relevant literature and performed observations of these different\nerrors in different games via Twitch livestreams. I then performed thematic\nanalysis with the observation data and generated themes that tie back into to\nthe relevant literature. Most of the current literature focuses on the what and\nhow behind bugs in games, but very little on the implications of these bugs on\nthe overall experience for the players, and what patterns of behavior may\nemerge because of them.", "AI": {"tldr": "This research explores the impact of video game glitches and bugs on player experience through literature review and thematic analysis of observations.", "motivation": "To understand the implications of glitches and bugs in video games on player experience, filling a gap in existing literature that mainly focuses on the mechanics of bugs rather than their effects on players.", "method": "The methodology involved reviewing relevant literature and conducting observations of different glitches and bugs in games, analyzed through thematic analysis.", "result": "The study generated themes that connect the observed glitches to existing literature, revealing a lack of focus on the player's overall experience and behaviors influenced by these glitches.", "conclusion": "The research suggests that understanding player experience in the context of game glitches and bugs is crucial, as it could inform game design and improve player satisfaction.", "key_contributions": ["Identifies a gap in literature regarding player experience in relation to game glitches.", "Utilizes thematic analysis to connect observed data with theoretical implications.", "Highlights behavioral patterns emerging from player interactions with glitches."], "limitations": "The study is exploratory and based on observational data, which may not capture all player experiences or types of glitches comprehensively.", "future_work": "Future research could explore specific player reactions to different types of glitches and how these insights could influence game development.", "keywords": ["video games", "glitches", "player experience", "thematic analysis", "game design"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736.pdf", "abs": "https://arxiv.org/abs/2504.18736", "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench", "AI": {"tldr": "This paper introduces EvidenceBench, a benchmark for evaluating models that find evidence relevant to hypotheses in biomedical papers, demonstrating its validity with expert annotations and highlighting the performance gap between models and experts.", "motivation": "Finding relevant evidence in biomedical literature is crucial for researchers testing scientific hypotheses.", "method": "The authors developed a novel pipeline involving hypothesis generation and sentence-by-sentence annotation guided by expert judgment to create EvidenceBench and EvidenceBench-100k datasets.", "result": "Evaluation revealed that various language models and retrieval systems underperformed compared to expert annotators in finding relevant evidence.", "conclusion": "The EvidenceBench benchmarks highlight the need for improved models in identifying relevant evidence, with a larger dataset available for advancing research in this area.", "key_contributions": ["Introduction of EvidenceBench and EvidenceBench-100k for evaluating evidence retrieval models.", "Demonstration of significant performance gaps between current models and human experts.", "Creation of a scalable annotation pipeline based on expert judgment."], "limitations": "Current models still do not reach expert-level performance, indicating room for improvement.", "future_work": "Future research should focus on enhancing models to improve their performance in evidence retrieval tasks.", "keywords": ["biomedical", "evidence retrieval", "language models", "machine learning", "hypothesis generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19038", "pdf": "https://arxiv.org/pdf/2504.19038.pdf", "abs": "https://arxiv.org/abs/2504.19038", "title": "Generative AI Literacy: A Comprehensive Framework for Literacy and Responsible Use", "authors": ["Chengzhi Zhang", "Brian Magerko"], "categories": ["cs.HC"], "comment": "14 pages", "summary": "After the release of several AI literacy guidelines, the rapid rise and\nwidespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek,\nhave transformed our lives. Unlike traditional AI algorithms (e.g.,\nconvolutional neural networks, semantic networks, classifiers) captured in\nexisting AI literacy frameworks, generative AI exhibits distinct and more\nnuanced characteristics. However, a lack of robust generative AI literacy is\nhindering individuals ability to evaluate critically and use these models\neffectively and responsibly. To address this gap, we propose a set of\nguidelines with 12 items for generative AI literacy, organized into four key\naspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2)\nGuidelines for Understanding Interaction with Generative AI, (3) Guidelines for\nUnderstanding Interaction with Generative AI, and (4) Guidelines for High Level\nUnderstanding of Generative AI. These guidelines aim to support schools,\ncompanies, educators, and organizations in developing frameworks that empower\ntheir members, such as students, employees, and stakeholders, to use generative\nAI in an efficient, ethical, and informed way.", "AI": {"tldr": "This paper proposes 12 guidelines for generative AI literacy to help individuals evaluate and use generative AI tools responsibly.", "motivation": "The rapid rise of generative AI tools has outpaced existing AI literacy frameworks, resulting in a critical gap in users' ability to evaluate and use these technologies effectively.", "method": "The authors developed a set of guidelines organized into four key aspects related to the selection, interaction, and understanding of generative AI.", "result": "The proposed guidelines aim to empower users in schools, companies, and organizations to engage with generative AI in an informed and ethical manner.", "conclusion": "The guidelines provide a structured approach to enhance generative AI literacy, fostering responsible usage among diverse user groups.", "key_contributions": ["Introduction of 12 specific guidelines for generative AI literacy.", "Categorization of guidelines into four key aspects for clarity.", "Focus on ethical and informed utilization of generative AI tools."], "limitations": "The effectiveness of the guidelines in diverse contexts remains to be empirically tested.", "future_work": "Future research should explore the practical implementation of these guidelines and their impact on user competency.", "keywords": ["Generative AI", "Literacy", "Guidelines", "Ethical Use", "AI Tools"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762.pdf", "abs": "https://arxiv.org/abs/2504.18762", "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI.", "AI": {"tldr": "Introduction of SynLexLM, a novel legal LLM training method utilizing curriculum learning and synthetic data.", "motivation": "To overcome the challenges of fine-tuning LLMs for specialized domains, particularly law, where data scarcity and legal nuances are prevalent.", "method": "The authors propose a curriculum learning approach that starts from simple legal texts and progressively advances to complex materials, supplemented with synthetic data generation functionalities.", "result": "Preliminary results indicate that SynLexLM outperforms traditional models on specific legal benchmarks.", "conclusion": "This approach could enhance legal document analysis and research tools, democratizing access to advanced legal AI.", "key_contributions": ["Introduction of SynLexLM for legal LLM pre-training", "Utilization of curriculum learning for progressive training", "Implementation of synthetic data augmentation for data scarcity mitigation."], "limitations": "The current work is preliminary and further validation on a broader set of legal tasks is required.", "future_work": "Exploration of additional legal domains and larger-scale benchmarks for comprehensive evaluation.", "keywords": ["Large Language Models", "legal AI", "curriculum learning", "synthetic data augmentation", "legal benchmarks"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.19120", "pdf": "https://arxiv.org/pdf/2504.19120.pdf", "abs": "https://arxiv.org/abs/2504.19120", "title": "Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility", "authors": ["Gaojian Huang", "Yantong Jin", "Wei-Hsiang Lo"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The goal of the current study is to introduce a triadic human-AI\ncollaboration framework for the automated vehicle domain. Previous\nclassifications (e.g., SAE Levels of Automation) focus on defining automation\nlevels based on who controls the vehicle. However, it remains unclear how human\nusers and AI should collaborate in real-time, especially in dynamic driving\ncontexts, where roles can shift frequently. To fill the gap, this study\nproposes a triadic human-AI collaboration framework with three AI roles (i.e.,\nAdvisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.\nOverall, the study lays a foundation for developing adaptive, role-based\nhuman-AI collaboration strategies in automated vehicles.", "AI": {"tldr": "This study proposes a triadic human-AI collaboration framework for automated vehicles, focusing on dynamic roles in real-time collaboration.", "motivation": "To address the lack of clarity in how human users and AI should collaborate in dynamic driving contexts, where control roles shift frequently.", "method": "The study introduces a framework comprising three AI roles (Advisor, Co-Pilot, and Guardian) that adapt to the needs of human users in real-time.", "result": "The framework provides a basis for developing adaptive, role-based collaboration strategies in automated vehicles.", "conclusion": "The proposed framework aims to improve human-AI collaboration in dynamic driving scenarios by defining flexible AI roles.", "key_contributions": ["Introduction of a triadic human-AI collaboration framework", "Definition of three adaptive AI roles", "Foundation for role-based collaboration strategies in automated vehicles"], "limitations": "", "future_work": "Future research could explore practical applications of the framework in real-world driving scenarios and investigate user acceptance of the adaptive roles.", "keywords": ["Human-AI collaboration", "Automated vehicles", "Dynamic roles"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805.pdf", "abs": "https://arxiv.org/abs/2504.18805", "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available.", "AI": {"tldr": "The paper presents SciTalk, a multi-LLM framework for generating accurate and engaging short-form videos from scientific papers, addressing challenges in content complexity and feedback loops in video creation.", "motivation": "The motivation behind this work is to improve the generation of engaging and accurate short-form videos from complex scientific content, addressing issues such as factual inaccuracies and visual artifacts in existing methods.", "method": "SciTalk employs a multi-LLM agentic framework that utilizes specialized agents for content summarization, visual scene planning, and editing, incorporating an iterative feedback system where video agents simulate user roles to refine video generation.", "result": "Experimental evaluations reveal that SciTalk significantly outperforms traditional prompting methods in creating scientifically accurate and engaging videos through its iterative refinement process.", "conclusion": "While current results do not yet reach the quality of human creators, SciTalk offers insights into the process and challenges of feedback-driven video generation.", "key_contributions": ["Introduction of a novel multi-LLM framework for video generation from scientific papers.", "Implementation of an iterative feedback mechanism involving simulated user roles to improve video content.", "Demonstration of improved accuracy and engagement in video generation compared to existing methods."], "limitations": "Preliminary results do not match the quality of videos created by human content creators.", "future_work": "Future directions include improving the quality of generated videos further and exploring additional sources and agents to enhance the framework's performance.", "keywords": ["Video Generation", "Multi-LLM Framework", "Scientific Dissemination"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19158", "pdf": "https://arxiv.org/pdf/2504.19158.pdf", "abs": "https://arxiv.org/abs/2504.19158", "title": "SnuggleSense: Empowering Online Harm Survivors Through a Structured Sensemaking Process", "authors": ["Sijia Xiao", "Haodi Zou", "Amy Mathews", "Jingshu Rui", "Coye Cheshire", "Niloufar Salehi"], "categories": ["cs.HC"], "comment": null, "summary": "Online interpersonal harm, such as cyberbullying and sexual harassment,\nremains a pervasive issue on social media platforms. Traditional approaches,\nprimarily content moderation, often overlook survivors' needs and agency. We\nintroduce SnuggleSense, a system that empowers survivors through structured\nsensemaking. Inspired by restorative justice practices, SnuggleSense guides\nsurvivors through reflective questions, offers personalized recommendations\nfrom similar survivors, and visualizes plans using interactive sticky notes. A\ncontrolled experiment demonstrates that SnuggleSense significantly enhances\nsensemaking compared to an unstructured process of making sense of the harm. We\nargue that SnuggleSense fosters community awareness, cultivates a supportive\nsurvivor network, and promotes a restorative justice-oriented approach toward\nrestoration and healing. We also discuss design insights, such as tailoring\ninformational support and providing guidance while preserving survivors'\nagency.", "AI": {"tldr": "SnuggleSense is a system designed to empower survivors of online interpersonal harm through structured sensemaking, enhancing their agency and promoting restorative justice.", "motivation": "To address the inadequacies of traditional content moderation approaches in supporting survivors of online harm such as cyberbullying and sexual harassment.", "method": "A controlled experiment was conducted to compare the effectiveness of SnuggleSense in enhancing survivors' sensemaking to an unstructured approach.", "result": "SnuggleSense significantly enhances sensemaking, fostering community awareness and supportive networks while promoting a restorative justice ethos.", "conclusion": "The system provides valuable design insights for tailoring support to survivors while maintaining their agency, emphasizing the importance of community and restorative practices.", "key_contributions": ["Introduction of SnuggleSense for structured sensemaking of interpersonal harm.", "Empowerment of survivors through personalized recommendations and community support.", "Insights into design principles for supporting survivors' agency during recovery."], "limitations": "", "future_work": "", "keywords": ["SnuggleSense", "cyberbullying", "restorative justice", "online harm", "support networks"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838.pdf", "abs": "https://arxiv.org/abs/2504.18838", "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators.", "AI": {"tldr": "This survey addresses the evaluation challenges posed by the rapid evolution of Large Language Models (LLMs), focusing on the shift from task-specific to capability-based evaluation and from manual to automated evaluation, while highlighting the generalization issue in evaluation.", "motivation": "The rise of LLMs presents significant challenges for evaluation methods, necessitating a survey to address these emerging issues.", "method": "The survey analyzes transitions in evaluation approaches, focusing on capability-based benchmarks and automated evaluation methods, and discusses the generalization issue with respect to evaluation methods, datasets, evaluators, and metrics.", "result": "The survey identifies two major transitions in evaluation methodology and outlines the persistent evaluation generalization issue that limits the scalability of test sets.", "conclusion": "New evaluation methods must evolve alongside LLMs, addressing capability-based evaluations and automating the evaluation process, while the generalization issue remains a core challenge.", "key_contributions": ["Identifies key transitions in evaluation methodology for LLMs", "Analyzes the generalization issue in model evaluation", "Provides a living repository for ongoing updates and contributions."], "limitations": "The persistent evaluation generalization issue hampers the effectiveness of current evaluation practices as LLM capabilities grow.", "future_work": "Future work involves ongoing contributions to the repository and further exploration of scalable evaluation methods.", "keywords": ["Large Language Models", "Evaluation", "Machine Learning", "Automated Evaluation", "Capability-based Evaluation"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2504.19345", "pdf": "https://arxiv.org/pdf/2504.19345.pdf", "abs": "https://arxiv.org/abs/2504.19345", "title": "Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users", "authors": ["Apurv Varshney", "Lucas Nadolskis", "Tobias Höllerer", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Blind individuals face persistent challenges in last-mile navigation,\nincluding locating entrances, identifying obstacles, and navigating complex or\ncluttered spaces. Although wearable cameras are increasingly used in assistive\nsystems, there has been no systematic, vantage-focused comparison to guide\ntheir design. This paper addresses that gap through a two-part investigation.\nFirst, we surveyed ten experienced blind cane users, uncovering navigation\nstrategies, pain points, and technology preferences. Participants stressed the\nimportance of multi-sensory integration, destination-focused travel, and\nassistive tools that complement (rather than replace) the cane's tactile\nutility. Second, we conducted controlled data collection with a blind\nparticipant navigating five real-world environments using synchronized head-\nand cane-mounted cameras, isolating vantage placement as the primary variable.\nTo assess how each vantage supports spatial perception, we evaluated SLAM\nperformance (for localization and mapping) and NeRF-based 3D reconstruction\n(for downstream scene understanding). Head-mounted sensors delivered superior\nlocalization accuracy, while cane-mounted views offered broader ground-level\ncoverage and richer environmental reconstructions. A combined (head+cane)\nconfiguration consistently outperformed both. These results highlight the\ncomplementary strengths of different sensor placements and offer actionable\nguidance for developing hybrid navigation aids that are perceptive, robust, and\nuser-aligned.", "AI": {"tldr": "This paper investigates the effectiveness of wearable cameras for blind navigation by comparing head- and cane-mounted perspectives through user surveys and controlled experiments.", "motivation": "Blind individuals experience significant challenges in last-mile navigation, necessitating the development of effective assistive navigation tools.", "method": "The study involved surveying ten experienced blind cane users to understand their navigation strategies and technology preferences, followed by controlled data collection with cameras mounted on the head and cane of a blind participant navigating various environments.", "result": "Findings revealed that head-mounted cameras provide better localization accuracy, while cane-mounted setups offer superior ground-level coverage. A combined configuration of both cameras yielded the best performance.", "conclusion": "The results emphasize the importance of hybrid navigation aids that leverage the strengths of multiple sensor placements for better navigation assistance.", "key_contributions": ["Systematic comparison of head-mounted vs. cane-mounted cameras for navigation", "Insights from user surveys on assistive technology preferences", "Evaluation of SLAM and 3D reconstruction for navigation aid design"], "limitations": "The study is limited to a small number of participants and specific environments, which may not generalize across all navigation challenges for the blind.", "future_work": "Future research should explore additional environments and further refine sensor integration techniques for improved navigation aids.", "keywords": ["Blind navigation", "Wearable cameras", "Assistive technology", "Human-computer interaction", "Hybrid navigation aids"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839.pdf", "abs": "https://arxiv.org/abs/2504.18839", "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability.", "AI": {"tldr": "This paper addresses the detection and mitigation of dialogue breakdowns in LLM-driven conversational systems by proposing a specialized fine-tuning approach combined with advanced prompting strategies.", "motivation": "To address the issues of incoherent or contradictory responses from large language models (LLMs) that undermine user trust in conversational systems.", "method": "Combining specialized fine-tuning with advanced prompting strategies including few-shot learning, chain-of-thought reasoning, and analogical prompting, while also implementing a real-time deployment architecture.", "result": "Achieved a 7% accuracy improvement over the base model on the BETOLD dataset and showed significant operational cost reductions by selectively using more resource-intensive models only when breakdowns are detected.", "conclusion": "The proposed approach offers a scalable solution for robust conversational AI, enhancing efficiency, interpretability, and reliability in high-impact domains.", "key_contributions": ["Introduction of advanced prompting strategies for dialogue systems", "Real-time deployment architecture for cost-effective operation", "Significant improvement in model performance and breakdown detection"], "limitations": "", "future_work": "Further exploration of generalization capabilities and application to other languages or dialogue scenarios.", "keywords": ["Large Language Models", "Conversational AI", "Dialogue Breakdown Detection", "Fine-tuning Strategies", "Real-time Deployment"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2504.19423", "pdf": "https://arxiv.org/pdf/2504.19423.pdf", "abs": "https://arxiv.org/abs/2504.19423", "title": "MER 2025: When Affective Computing Meets Large Language Models", "authors": ["Zheng Lian", "Rui Liu", "Kele Xu", "Bin Liu", "Xuefei Liu", "Yazhou Zhang", "Xin Liu", "Yong Li", "Zebang Cheng", "Haolin Zuo", "Ziyang Ma", "Xiaojiang Peng", "Xie Chen", "Ya Li", "Erik Cambria", "Guoying Zhao", "Björn W. Schuller", "Jianhua Tao"], "categories": ["cs.HC"], "comment": null, "summary": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub.", "AI": {"tldr": "MER2025 challenge focuses on the intersection of affective computing and large language models, addressing innovative approaches for emotion recognition.", "motivation": "To explore emerging trends in affective computing, particularly in relation to large language models (LLMs).", "method": "The challenge includes four tracks: MER-SEMI for semi-supervised learning of fixed categorical emotions, MER-FG for fine-grained emotion recognition, MER-DES for incorporating multimodal cues, and MER-PR for exploring emotion prediction's impact on personality recognition.", "result": "Participants will work with baseline code and datasets provided for each challenge track, facilitating research in emotion understanding.", "conclusion": "MER2025 represents a shift from traditional emotion taxonomies to LLM-driven generative methods for emotion recognition.", "key_contributions": ["Focus on LLMs in emotion recognition", "Introduction of multimodal cues", "Exploration of fine-grained emotional states"], "limitations": "", "future_work": "Potential future research could extend LLM applications in emotion recognition and investigate additional multimodal approaches.", "keywords": ["Affective Computing", "Large Language Models", "Emotion Recognition", "Multimodal Cues", "Fine-grained Emotions"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851.pdf", "abs": "https://arxiv.org/abs/2504.18851", "title": "When2Call: When (not) to Call Tools", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call.", "AI": {"tldr": "This paper presents the When2Call benchmark for evaluating decision-making in tool-calling for Language Models, highlighting a significant area for improvement in existing models.", "motivation": "The need to assess when Language Models should call external tools, go for follow-up questions, or admit inability to answer, beyond just tool accuracy.", "method": "Development of the When2Call benchmark and a training set that includes a preference optimization training regime for enhanced learning.", "result": "State-of-the-art tool-calling LMs show substantial room for improvement on the When2Call benchmark, proving its significance for future assessments.", "conclusion": "The creation of When2Call emphasizes the need for refined benchmarks and training methods in tool-calling decision-making for LMs.", "key_contributions": ["Introduction of the When2Call benchmark for evaluating tool-calling decisions.", "Development of a preference optimization training regime.", "Release of benchmark data and evaluation scripts."], "limitations": "", "future_work": "Further research could focus on improving model decision-making processes and exploring additional tool-calling scenarios.", "keywords": ["Language Models", "tool calling", "benchmark", "HCI", "preference optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19460", "pdf": "https://arxiv.org/pdf/2504.19460.pdf", "abs": "https://arxiv.org/abs/2504.19460", "title": "A Real-Time Gesture-Based Control Framework", "authors": ["Mahya Khazaei", "Ali Bahrani", "George Tzanetakis"], "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures, 2025 International Computer Music Conference", "summary": "We introduce a real-time, human-in-the-loop gesture control framework that\ncan dynamically adapt audio and music based on human movement by analyzing live\nvideo input. By creating a responsive connection between visual and auditory\nstimuli, this system enables dancers and performers to not only respond to\nmusic but also influence it through their movements. Designed for live\nperformances, interactive installations, and personal use, it offers an\nimmersive experience where users can shape the music in real time.\n  The framework integrates computer vision and machine learning techniques to\ntrack and interpret motion, allowing users to manipulate audio elements such as\ntempo, pitch, effects, and playback sequence. With ongoing training, it\nachieves user-independent functionality, requiring as few as 50 to 80 samples\nto label simple gestures. This framework combines gesture training, cue\nmapping, and audio manipulation to create a dynamic, interactive experience.\nGestures are interpreted as input signals, mapped to sound control commands,\nand used to naturally adjust music elements, showcasing the seamless interplay\nbetween human interaction and machine response.", "AI": {"tldr": "A gesture control framework enhances live audio interactions through human movement analysis using video input.", "motivation": "To create an immersive real-time audio manipulation experience for performers by linking visual and auditory stimuli.", "method": "The framework utilizes computer vision and machine learning to track movements and interpret gestures, enabling audio control based on user actions.", "result": "Users can influence music elements like tempo and pitch with simple gestures, requiring minimal gesture samples for effective training.", "conclusion": "The developed system showcases the potential for intuitive human-machine interactions in live performance and installation settings.", "key_contributions": ["Real-time gesture control for audio manipulation", "Integration of computer vision with machine learning techniques", "User-independent functionality requiring minimal gesture samples"], "limitations": "", "future_work": "Further exploration of gesture diversity and enhanced machine learning models for broader applications.", "keywords": ["Gesture Control", "Human-Computer Interaction", "Machine Learning", "Audio Manipulation", "Interactive Installations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857.pdf", "abs": "https://arxiv.org/abs/2504.18857", "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K.", "AI": {"tldr": "This paper introduces Dimension-Wise Positional Embeddings Manipulation (DPE), a framework for extending the context window of Large Language Models (LLMs) without retraining.", "motivation": "To overcome the limitations of LLMs in processing long input contexts without incurring the high costs of retraining large-scale models.", "method": "DPE manipulates the position indices of key dimensions selectively, optimizing each dimension's effective length for improved context extension.", "result": "DPE allows Llama3-8k 8B to handle context windows of 128k tokens, outperforming baseline methods such as YaRN and Self-Extend, while also enhancing performance on long-context benchmarks by over 18 points.", "conclusion": "DPE significantly improves LLMs' context handling capabilities without the need for continual training, achieving performance comparable to commercial models like GPT-4-128K.", "key_contributions": ["Introduces a novel framework, DPE, for context extension in LLMs without retraining.", "Enhances model performance on long-context benchmarks.", "Demonstrates superior capabilities of Llama3-8k 8B with DPE compared to other models."], "limitations": "", "future_work": "Further exploration of DPE's applicability to other LLM architectures and potential optimizations.", "keywords": ["Large Language Models", "context window", "Dimension-Wise Positional Embeddings", "extrapolation", "training-free"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.19611", "pdf": "https://arxiv.org/pdf/2504.19611.pdf", "abs": "https://arxiv.org/abs/2504.19611", "title": "Scene2Hap: Combining LLMs and Physical Modeling for Automatically Generating Vibrotactile Signals for Full VR Scenes", "authors": ["Arata Jingu", "Easa AliAbbasi", "Paul Strohmeier", "Jürgen Steimle"], "categories": ["cs.HC"], "comment": null, "summary": "Haptic feedback contributes to immersive virtual reality (VR) experiences.\nDesigning such feedback at scale, for all objects within a VR scene and their\nrespective arrangements, remains a time-consuming task. We present Scene2Hap,\nan LLM-centered system that automatically designs object-level vibrotactile\nfeedback for entire VR scenes based on the objects' semantic attributes and\nphysical context. Scene2Hap employs a multimodal large language model to\nestimate the semantics and physical context of each object, including its\nmaterial properties and vibration behavior, from the multimodal information\npresent in the VR scene. This semantic and physical context is then used to\ncreate plausible vibrotactile signals by generating or retrieving audio signals\nand converting them to vibrotactile signals. For the more realistic spatial\nrendering of haptics in VR, Scene2Hap estimates the propagation and attenuation\nof vibration signals from their source across objects in the scene, considering\nthe estimated material properties and physical context, such as the distance\nand contact between virtual objects. Results from two user studies confirm that\nScene2Hap successfully estimates the semantics and physical context of VR\nscenes, and the physical modeling of vibration propagation improves usability,\nperceived materiality, and spatial awareness.", "AI": {"tldr": "Scene2Hap is an LLM-centered system that automates the design of vibrotactile feedback for VR scenes based on object semantics and physical context.", "motivation": "Creating realistic haptic feedback for all objects in a VR environment is labor-intensive and time-consuming, necessitating an automated solution.", "method": "Scene2Hap utilizes a multimodal large language model to analyze and generate haptic feedback by estimating semantic attributes and physical context of objects in VR scenes.", "result": "User studies show that Scene2Hap effectively estimates semantics and physical context while improving usability, perceived materiality, and spatial awareness through effective vibration modeling.", "conclusion": "The approach significantly enhances the immersive experience of VR by providing tailored haptic feedback based on an object's characteristics.", "key_contributions": ["Development of Scene2Hap for automated haptic feedback design in VR", "Integration of multimodal inputs for hardware and software synergy in haptic feedback", "Demonstrated improved user experience through empirical testing."], "limitations": "The effectiveness may vary with more complex scenarios and diverse object types in VR setups.", "future_work": "Further exploration of advanced models for diverse object interactions and integration with more VR platforms is needed.", "keywords": ["Haptic feedback", "Virtual reality", "Large language model", "Multimodal", "Vibrotactile signals"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872.pdf", "abs": "https://arxiv.org/abs/2504.18872", "title": "Latent Adversarial Training Improves the Representation of Refusal", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety.", "AI": {"tldr": "This paper analyzes how Latent Adversarial Training (LAT) influences the representation of refusal behavior in language models, revealing that LAT creates a concentrated representation in latent space that enhances vulnerability to certain attacks.", "motivation": "To evaluate the effectiveness and limitations of Latent Adversarial Training (LAT) in enhancing the robustness of language models against harmful instructions.", "method": "The study examines the Llama 2 7B model by calculating activation differences between harmful and harmless instruction pairs and applies Singular Value Decomposition (SVD) to analyze the representation of refusal behavior under LAT compared to traditional methods.", "result": "LAT significantly alters the refusal representation, concentrating it in the first two SVD components, which account for about 75% of activation variance, leading to improved robustness against attacks from reference models but increased vulnerability to self-generated vectors.", "conclusion": "LAT enhances the representation of refusal behavior but introduces new vulnerabilities, suggesting both potential for improving model safety and the need for caution in its application.", "key_contributions": ["Demonstrates LAT's impact on refusal behavior representation in language models.", "Identifies vulnerabilities associated with improved refusal vectors generated by LAT.", "Highlights the trade-offs between robustness and vulnerability in model training approaches."], "limitations": "The study primarily focuses on the Llama 2 7B model and may not generalize to all language models.", "future_work": "Further research is needed to explore LAT's effectiveness across different architectures and its interaction with various training methods.", "keywords": ["Latent Adversarial Training", "Refusal Behavior", "Language Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19703", "pdf": "https://arxiv.org/pdf/2504.19703.pdf", "abs": "https://arxiv.org/abs/2504.19703", "title": "Interactive Discovery and Exploration of Visual Bias in Generative Text-to-Image Models", "authors": ["Johannes Eschner", "Roberto Labadie-Tamayo", "Matthias Zeppelzauer", "Manuela Waldner"], "categories": ["cs.HC"], "comment": null, "summary": "Bias in generative Text-to-Image (T2I) models is a known issue, yet\nsystematically analyzing such models' outputs to uncover it remains\nchallenging. We introduce the Visual Bias Explorer (ViBEx) to interactively\nexplore the output space of T2I models to support the discovery of visual bias.\nViBEx introduces a novel flexible prompting tree interface in combination with\nzero-shot bias probing using CLIP for quick and approximate bias exploration.\nIt additionally supports in-depth confirmatory bias analysis through visual\ninspection of forward, intersectional, and inverse bias queries. ViBEx is\nmodel-agnostic and publicly available. In four case study interviews, experts\nin AI and ethics were able to discover visual biases that have so far not been\ndescribed in literature.", "AI": {"tldr": "Introduction of Visual Bias Explorer (ViBEx) to analyze visual bias in T2I models.", "motivation": "To systematically analyze and uncover biases present in generative Text-to-Image models.", "method": "ViBEx utilizes a flexible prompting tree interface and zero-shot bias probing with CLIP for bias exploration and supports confirmatory analysis through visual inspection of biases.", "result": "Experts in AI and ethics identified previously undocumented visual biases in T2I outputs using ViBEx.", "conclusion": "ViBEx provides a model-agnostic tool for exploring and analyzing visual bias in generative T2I models.", "key_contributions": ["Development of ViBEx for interactive bias exploration", "Implementation of a novel flexible prompting tree interface", "Support for confirmatory bias analysis using visual inspection"], "limitations": "", "future_work": "Further development and application of ViBEx to explore additional biases and refine bias discovery techniques.", "keywords": ["Visual Bias Explorer", "Text-to-Image", "Bias analysis", "Generative models", "CLIP"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884.pdf", "abs": "https://arxiv.org/abs/2504.18884", "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "authors": ["Junichiro Niimi"], "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.", "AI": {"tldr": "This study presents a new ensemble strategy for sentiment analysis using LLMs, showing improved accuracy over single attempts.", "motivation": "To address the variability and reproducibility issues in the results of large language models (LLMs) for sentiment analysis, leveraging a majority voting method commonly used in human annotation.", "method": "An ensemble strategy was implemented by integrating multiple inferences from medium-sized LLMs, compared against results from a single inference from a large LLM.", "result": "The ensemble approach demonstrated an 18.6% reduction in RMSE, leading to more robust and accurate sentiment analysis results.", "conclusion": "Employing an ensemble strategy significantly enhances the performance of LLMs in sentiment analysis, making it a viable alternative to using a single large model.", "key_contributions": ["Introduction of an ensemble strategy for LLMs in sentiment analysis", "Demonstrated improvement in accuracy and robustness", "Reduction in RMSE by 18.6% compared to single model inference"], "limitations": "", "future_work": "Further exploration of ensemble methods with different configurations and applications in various NLP tasks.", "keywords": ["large language models", "sentiment analysis", "ensemble strategy", "reproducibility", "majority voting"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.19767", "pdf": "https://arxiv.org/pdf/2504.19767.pdf", "abs": "https://arxiv.org/abs/2504.19767", "title": "Crafting a Personal Journaling Practice: Negotiating Ecosystems of Materials, Personal Context, and Community in Analog Journaling", "authors": ["Katherine Lin", "Juna Kawai-Yue", "Adira Sklar", "Lucy Hecht", "Sarah Sterman", "Tiffany Tseng"], "categories": ["cs.HC"], "comment": "Creativity and Cognition 2025", "summary": "Analog journaling has grown in popularity, with journaling on paper\nencompassing a range of motivations, styles, and practices including planning,\nhabit-tracking, and reflecting. Journalers develop strong personal preferences\naround the tools they use, the ideas they capture, and the layout in which they\nrepresent their ideas and memories. Understanding how analog journaling\npractices are individually shaped and crafted over time is critical to\nsupporting the varied benefits associated with journaling, including improved\nmental health and positive support for identity development. To understand this\ndevelopment, we qualitatively analyzed publicly-shared journaling content from\nYouTube and Instagram and interviewed 11 journalers. We report on our\nidentification of the journaling ecosystem in which journaling practices are\nshaped by materials, personal context, and communities, sharing how this\necosystem plays a role in the practices and identities of journalers as they\ncustomize their journaling routine to best suit their personal goals. Using\nthese insights, we discuss design opportunities for how future tools can better\nalign with and reflect the rich affordances and practices of journaling on\npaper.", "AI": {"tldr": "This paper explores analog journaling practices, their personal motivations, and the ecosystems that shape them, based on qualitative analysis of social media content and interviews.", "motivation": "Understanding how analog journaling practices shape mental health and identity development.", "method": "Qualitative analysis of public journaling content on YouTube and Instagram, along with interviews with 11 journalers.", "result": "Identified a journaling ecosystem that influences practices and identities, showcasing how materials, context, and communities shape journaling.", "conclusion": "Insights can inform design opportunities for future journaling tools that better reflect user practices and goals.", "key_contributions": ["Identification of the journaling ecosystem affecting practices and identities.", "Insights into design opportunities for future journaling tools.", "Analysis of qualitative data from social media and interviews."], "limitations": "", "future_work": "Exploration of how future tools can support diverse journaling practices.", "keywords": ["Analog journaling", "Mental health", "Design opportunities", "Qualitative analysis", "User practices"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938.pdf", "abs": "https://arxiv.org/abs/2504.18938", "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "authors": ["Junhong Liang", "Yu Zhou"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks.", "AI": {"tldr": "This paper introduces the MTCSC framework for variable-length Chinese Spelling Correction, enhancing error correction with domain adaptation and length consistency.", "motivation": "To address the limitations of traditional Chinese Spelling Correction methods that require fixed input-output length parity and struggle with domain-specific corrections.", "method": "The proposed MTCSC framework utilizes a retrieval-augmented generation mechanism enhanced with a length reflection mechanism for multi-turn correction.", "result": "The proposed method shows significant improvement in correction quality across diverse domain-specific datasets, particularly for variable-length errors.", "conclusion": "MTCSC effectively extends the capabilities of traditional CSC approaches, allowing for flexible error correction suitable for various domains.", "key_contributions": ["Introduction of MTCSC for variable-length correction scenarios", "Use of a length reflection mechanism for output fidelity", "Implementation of a multi-source combination strategy for enhanced domain adaptation"], "limitations": "The performance may still depend on the quality of domain-specific training data and dictionaries utilized in the retrieval database.", "future_work": "Further exploration of additional error types and refinement of the retrieval-enhanced correction methods.", "keywords": ["Chinese Spelling Correction", "Large Language Models", "Domain Adaptation", "Error Correction", "Multi-Turn CSC"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.19772", "pdf": "https://arxiv.org/pdf/2504.19772.pdf", "abs": "https://arxiv.org/abs/2504.19772", "title": "Memento: Augmenting Personalized Memory via Practical Multimodal Wearable Sensing in Visual Search and Wayfinding Navigation", "authors": ["Indrajeet Ghosh", "Kasthuri Jayarajah", "Nicholas Waytowich", "Nirmalya Roy"], "categories": ["cs.HC", "cs.MM"], "comment": "This work has been accepted to the Proceedings of the ACM UMAP 2025", "summary": "Working memory involves the temporary retention of information over short\nperiods. It is a critical cognitive function that enables humans to perform\nvarious online processing tasks, such as dialing a phone number, recalling\nmisplaced items' locations, or navigating through a store. However, inherent\nlimitations in an individual's capacity to retain information often result in\nforgetting important details during such tasks. Although previous research has\nsuccessfully utilized wearable and assistive technologies to enhance long-term\nmemory functions (e.g., episodic memory), their application to supporting\nshort-term recall in daily activities remains underexplored. To address this\ngap, we present Memento, a framework that uses multimodal wearable sensor data\nto detect significant changes in cognitive state and provide intelligent in\nsitu cues to enhance recall. Through two user studies involving 15 and 25\nparticipants in visual search navigation tasks, we demonstrate that\nparticipants receiving visual cues from Memento achieved significantly better\nroute recall, improving approximately 20-23% compared to free recall.\nFurthermore, Memento reduced cognitive load and review time by 46% while also\nsubstantially reducing computation time (3.86 seconds vs. 15.35 seconds),\noffering an average of 75% effectiveness compared to computer vision-based cue\nselection approaches.", "AI": {"tldr": "Memento is a framework that enhances short-term memory recall using multimodal wearable sensor data, showing significant improvements in recall performance and cognitive load reduction through user studies.", "motivation": "The paper addresses the gap in using technology to support short-term recall in daily activities, as most prior research focuses on enhancing long-term memory.", "method": "Memento utilizes multimodal wearable sensor data to detect changes in cognitive state and provide intelligent cues to assist recall during tasks.", "result": "User studies showed that participants receiving cues from Memento improved recall by 20-23% and reduced cognitive load and review time by 46%.", "conclusion": "Memento demonstrates effective enhancement of short-term recall through intelligent cueing, significantly outperforming existing approaches.", "key_contributions": ["Introduction of Memento framework for short-term memory support", "Significant empirical results showing improvements in recall and cognitive load", "Comparison of effectiveness against computer vision-based approaches"], "limitations": "The study is limited to specific tasks and participant demographics.", "future_work": "Exploration of additional cognitive tasks and broader user demographics for further validation of the framework.", "keywords": ["Working Memory", "Wearable Technology", "Cognitive Load", "Human-Computer Interaction", "Intelligent Cues"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942.pdf", "abs": "https://arxiv.org/abs/2504.18942", "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/).", "AI": {"tldr": "The paper introduces LawFlow, a dataset of complete legal workflows to improve AI's support for legal practitioners, comparing human and LLM-generated workflows, and suggesting design improvements for AI tools in legal practice.", "motivation": "To address the gap in AI support for complex, end-to-end legal decision-making by providing a dataset that reflects real-world legal workflows.", "method": "Introduction of LawFlow dataset, comparison of human and LLM-generated workflows, analysis of structure and reasoning flexibility.", "result": "Human workflows are modular and adaptive, while LLM workflows are sequential and less sensitive to context; findings suggest legal professionals prefer AI in supportive roles rather than full execution of complex tasks.", "conclusion": "Current LLMs have limitations in supporting complex legal workflows; better design suggestions for collaborative AI systems in legal contexts are needed.", "key_contributions": ["Introduction of the LawFlow dataset", "Comparison of human and LLM workflow characteristics", "Design suggestions for improving AI in legal assistance"], "limitations": "The study primarily focuses on specific legal scenarios and may not generalize to all legal practices.", "future_work": "Explore further improvements in LLM capabilities for more collaborative and reasoning-aware legal AI systems.", "keywords": ["Legal workflows", "AI in law", "Human-computer interaction", "LLM", "LawFlow"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.19838", "pdf": "https://arxiv.org/pdf/2504.19838.pdf", "abs": "https://arxiv.org/abs/2504.19838", "title": "LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Liang Liu", "Yaxuan Guo", "Han Xiao", "Weifeng Lin", "Yuxiang Chai", "Yue Han", "Shuai Ren", "Hao Wang", "Xiaoyu Liang", "Wenhao Wang", "Tianze Wu", "Linghao Li", "Hao Wang", "Guanjing Xiong", "Yong Liu", "Hongsheng Li"], "categories": ["cs.HC"], "comment": "37 pages, 10 figures, 7 tables, Project Homepage:\n  https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents", "summary": "With the rapid rise of large language models (LLMs), phone automation has\nundergone transformative changes. This paper systematically reviews LLM-driven\nphone GUI agents, highlighting their evolution from script-based automation to\nintelligent, adaptive systems. We first contextualize key challenges, (i)\nlimited generality, (ii) high maintenance overhead, and (iii) weak intent\ncomprehension, and show how LLMs address these issues through advanced language\nunderstanding, multimodal perception, and robust decision-making. We then\npropose a taxonomy covering fundamental agent frameworks (single-agent,\nmulti-agent, plan-then-act), modeling approaches (prompt engineering,\ntraining-based), and essential datasets and benchmarks. Furthermore, we detail\ntask-specific architectures, supervised fine-tuning, and reinforcement learning\nstrategies that bridge user intent and GUI operations. Finally, we discuss open\nchallenges such as dataset diversity, on-device deployment efficiency,\nuser-centric adaptation, and security concerns, offering forward-looking\ninsights into this rapidly evolving field. By providing a structured overview\nand identifying pressing research gaps, this paper serves as a definitive\nreference for researchers and practitioners seeking to harness LLMs in\ndesigning scalable, user-friendly phone GUI agents.", "AI": {"tldr": "This paper reviews the evolution and challenges of LLM-driven phone GUI agents, proposing a taxonomy and addressing open challenges in the field.", "motivation": "With the rise of large language models, phone automation has changed significantly, necessitating a comprehensive understanding and taxonomy of LLM-driven GUI agents.", "method": "The paper reviews existing literature, identifies key challenges, proposes a taxonomy, and discusses various methodologies including supervised fine-tuning and reinforcement learning for enhancing phone GUI agents.", "result": "It identifies three main challenges in phone GUI automation and how LLMs can address these through advanced language understanding and multimodal perception, paving the way for adaptive and intelligent phone agents.", "conclusion": "The study serves as a reference for researchers, highlighting research gaps and future directions in the use of LLMs in phone GUI automation.", "key_contributions": ["Provides a taxonomy of LLM-driven phone GUI agent frameworks and methodologies.", "Identifies key challenges and how LLMs can overcome them.", "Offers insights into future research directions and open challenges in the field."], "limitations": "The paper may not cover all emerging technologies and approaches as the field is rapidly evolving.", "future_work": "The authors suggest exploring dataset diversity, deployment efficiency, user-centric adaptation, and addressing security concerns as future research areas.", "keywords": ["Large Language Models", "Phone Automation", "GUI Agents", "User Intent", "Machine Learning"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992.pdf", "abs": "https://arxiv.org/abs/2504.18992", "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data.", "AI": {"tldr": "The paper introduces Dynamic Fisher-weighted Merging (DF-Merge), a novel framework for merging task-specific language models, improving performance significantly over existing methods.", "motivation": "There is a growing need for efficient multi-task language models, but existing parameter merging methods lead to performance gaps compared to multi-task fine-tuning. This paper aims to improve model merging to address this issue.", "method": "The authors propose DF-Merge, where candidate models are associated with coefficients that scale their parameters. Bayesian optimization adjusts these coefficients dynamically to enhance performance, while integrating parameter importance using Fisher information.", "result": "DF-Merge demonstrates superior performance against strong baselines across different model sizes and tasks, achieving near-optimal results in few iterations even with limited validation data.", "conclusion": "The unified merging framework of DF-Merge allows for efficient and effective multi-task model creation, revealing the potential for improvements in model merging strategies.", "key_contributions": ["Introduction of a unified framework that merges existing merging strategies.", "Development of DF-Merge, which utilizes Bayesian optimization for dynamic parameter scaling.", "Demonstration of enhanced performance in multi-task settings with minimal additional training data."], "limitations": "", "future_work": "Further exploration of the framework's applicability across additional tasks and evaluation of its performance in real-world scenarios.", "keywords": ["Dynamic Merging", "Language Models", "Bayesian Optimization", "Multi-task Learning", "Parameter Scaling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.20016", "pdf": "https://arxiv.org/pdf/2504.20016.pdf", "abs": "https://arxiv.org/abs/2504.20016", "title": "Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design", "authors": ["Linshi Li", "Hanlin Cai"], "categories": ["cs.HC", "cs.CY", "cs.MM"], "comment": "This paper has been accepted as a Work-in-Progress (WiP) paper in the\n  24th annual ACM Interaction Design and Children (IDC) Conference", "summary": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement.", "AI": {"tldr": "This study explores the use of AI-powered virtual humans in child-centered design to improve engagement in interviews with children.", "motivation": "Engaging children directly in research is essential for understanding their experiences, but traditional methods often prioritize adult perspectives and face unique challenges.", "method": "The study established design guidelines for LLM-powered virtual humans tailored to interviewing children, developed three Human-AI workflows, and conducted a user study with 15 children aged 6 to 12.", "result": "The LLM-Analyze workflow significantly outperformed others by eliciting longer responses, achieving higher user experience ratings, and promoting better engagement with children.", "conclusion": "AI-powered virtual humans can enhance child-centered research by providing a structured and engaging interaction method for interviews.", "key_contributions": ["Key design guidelines for child interviews using AI-powered virtual humans", "Development of three human-AI interaction workflows (LLM-Auto, LLM-Interview, LLM-Analyze)", "Empirical results demonstrating improved child engagement with LLM-Analyze workflow"], "limitations": "The study is limited to a specific age group (6 to 12 years) and involves only a small sample size (15 children).", "future_work": "Further research could explore the adaptability of these workflows for different age groups and contexts.", "keywords": ["Human-AI interaction", "Child-centered design", "Virtual humans"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019.pdf", "abs": "https://arxiv.org/abs/2504.19019", "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.", "AI": {"tldr": "The paper introduces GoAT, a method to generate effective adversarial prompts to test the robustness of Large Language Models (LLMs), achieving high success rates against existing models.", "motivation": "To address vulnerabilities in Large Language Models (LLMs) that allow adversarial jailbreaks, improving model safety and alignment with societal standards.", "method": "GoAT uses a graph structure to dynamically generate and refine adversarial prompts through collaborative exploration of multiple attack paths, enhancing the effectiveness of black-box attacks.", "result": "GoAT achieves up to five times better jailbreak success rates compared to state-of-the-art methods, without needing access to model parameters, producing human-readable prompts.", "conclusion": "GoAT offers a significant advancement in testing LLM robustness, allowing for deeper exploration of adversarial vulnerabilities while maintaining model safety.", "key_contributions": ["Introduction of the GoAT framework for generating adversarial prompts.", "Better jailbreak success rates than current state-of-the-art methods.", "Dynamic integration of multiple reasoning paths to enhance attack effectiveness."], "limitations": "The method currently focuses on LLMs and may not be universally applicable to all AI model types.", "future_work": "Further exploration of the collaborative reasoning framework and its applications in various AI models, as well as enhancing the attack strategies.", "keywords": ["Large Language Models", "adversarial attacks", "robustness testing", "Graph of Thoughts", "black-box attacks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.20035", "pdf": "https://arxiv.org/pdf/2504.20035.pdf", "abs": "https://arxiv.org/abs/2504.20035", "title": "Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality", "authors": ["Brandon Woodard", "Melvin He", "Mose Sakashita", "Jing Qian", "Zainab Iftikhar", "Joseph LaViola Jr"], "categories": ["cs.HC", "H.5.2; H.5.1; H.5.0; H.1.2"], "comment": null, "summary": "Off-the-shelf smartphone-based AR systems typically use a single front-facing\nor rear-facing camera, which restricts user interactions to a narrow field of\nview and small screen size, thus reducing their practicality. We present\n\\textit{Cam-2-Cam}, an interaction concept implemented in three\nsmartphone-based AR applications with interactions that span both cameras.\nResults from our qualitative analysis conducted on 30 participants presented\ntwo major design lessons that explore the interaction space of smartphone AR\nwhile maintaining critical AR interface attributes like embodiment and\nimmersion: (1) \\textit{Balancing Contextual Relevance and Feedback Quality}\nserves to outline a delicate balance between implementing familiar interactions\npeople do in the real world and the quality of multimodal AR responses and (2)\n\\textit{Preventing Disorientation using Simultaneous Capture and Alternating\nCameras} which details how to prevent disorientation during AR interactions\nusing the two distinct camera techniques we implemented in the paper.\nAdditionally, we consider observed user assumptions or natural tendencies to\ninform future implementations of dual-camera setups for smartphone-based AR. We\nenvision our design lessons as an initial pioneering step toward expanding the\ninteraction space of smartphone-based AR, potentially driving broader adoption\nand overcoming limitations of single-camera AR.", "AI": {"tldr": "The paper introduces Cam-2-Cam, a concept for smartphone AR that utilizes both front and rear cameras for enhanced interaction. It discusses key design lessons derived from participant studies, focusing on contextual relevance and user orientation.", "motivation": "To enhance the practicality of smartphone-based AR applications by overcoming limitations of single-camera systems and improving user interaction experiences.", "method": "Implemented the Cam-2-Cam interaction concept in three AR applications; conducted qualitative analysis with 30 participants to gather insights on user interactions.", "result": "Two main design lessons were identified: balancing contextual relevance with feedback quality, and preventing user disorientation through simultaneous capture and alternating camera usage.", "conclusion": "The design lessons aim to expand the interaction space of AR and encourage the adoption of dual-camera setups in future smartphone AR applications.", "key_contributions": ["Introduction of the Cam-2-Cam interaction concept for smartphone AR.", "Identification of design principles that balance user experience with technology limitations.", "Insights into preventing user disorientation during AR interactions."], "limitations": "Findings are based on qualitative analysis with a limited sample size of participants.", "future_work": "Further exploration of dual-camera setups and their impact on user experiences in smartphone AR.", "keywords": ["Augmented Reality", "User Interaction", "Dual-Camera Systems", "Smartphone AR", "User Experience"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021.pdf", "abs": "https://arxiv.org/abs/2504.19021", "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification.", "AI": {"tldr": "This study investigates the effectiveness of fine-tuning pre-trained language models for academic text classification using an augmented dataset from the Web of Science.", "motivation": "The increasing volume of academic publications necessitates efficient text classification techniques.", "method": "The study fine-tunes various pre-trained language models (BERT, SciBERT, BioBERT, BlueBERT) on an augmented dataset sourced from targeted queries in the WoS database, applying a hard-voting strategy for improved accuracy.", "result": "Fine-tuning with dynamic learning rates and early stopping leads to significant accuracy improvements, particularly in specialized domains, with domain-specific models outperforming general-purpose ones.", "conclusion": "The research demonstrates that dataset augmentation, label prediction strategies, and focused fine-tuning can greatly enhance performance in academic text classification.", "key_contributions": ["Evaluation of PLMs for scientific text classification", "Contribution of dataset augmentation to model performance", "Comparison of domain-specific and general-purpose models"], "limitations": "The study may be limited by the focused domain and the specific dataset used for augmentation, potentially affecting generalizability to other fields.", "future_work": "Further research could explore additional datasets and model architectures to enhance generalization across diverse academic fields.", "keywords": ["text classification", "pre-trained language models", "dataset augmentation", "hard-voting strategy", "academic publications"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024.pdf", "abs": "https://arxiv.org/abs/2504.19024", "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch.", "AI": {"tldr": "The paper introduces KETCHUP, a k-step return estimation method for RL-based knowledge distillation in text generation, which reduces gradient variance and enhances performance on LLMs.", "motivation": "The need for improved reinforcement learning optimization in knowledge distillation for large language models.", "method": "The paper uses a k-step return induction based on the Bellman Optimality Equation to estimate multiple steps in reinforcement learning.", "result": "Empirical evaluations across three text generation tasks show that KETCHUP outperforms existing methods in terms of standard metrics and LLM evaluations.", "conclusion": "KETCHUP effectively improves the optimization process in RL-based knowledge distillation, particularly for larger student models.", "key_contributions": ["Introduction of KETCHUP, a novel k-step return estimation method for RL-KD.", "Theoretical analysis showing reduced gradient variance in RL optimization.", "Empirical results demonstrating superior performance in text generation tasks."], "limitations": "", "future_work": "Further exploration of KETCHUP's application in different RL paradigms and other text generation tasks.", "keywords": ["Reinforcement Learning", "Knowledge Distillation", "Text Generation", "K-step Return", "Large Language Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044.pdf", "abs": "https://arxiv.org/abs/2504.19044", "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt.", "AI": {"tldr": "This paper proposes a method to calibrate hypothesis likelihoods in neural machine translation (NMT) systems, aiming to align the decoding process with real-world translation quality by optimizing their Pearson correlation.", "motivation": "Traditional maximum a posteriori (MAP) decoding in NMT often results in low-quality translations, motivating a need for better alignment between decoding objectives and real-world translation quality.", "method": "The proposed method optimizes the Pearson correlation between hypothesis likelihoods and translation quality, using a distribution view for calibration, thus enhancing translation decoding effectiveness.", "result": "Implementing this approach leads to substantial improvements in translations from large language models (LLMs) with limited training, outperforming previous methods across various metrics and human evaluations.", "conclusion": "Calibrating translation likelihoods enhances MAP decoding efficiency and aligns predictions with actual translation quality, with the created state-of-the-art model available to the community.", "key_contributions": ["Introduces a novel calibration method aligning translation likelihoods with quality", "Demonstrates substantial translation quality gains using limited training", "Outperforms existing translation quality estimation models in human evaluations."], "limitations": "The study focuses primarily on calibration without delving into comprehensive algorithmic design of LLMs or other underlying techniques.", "future_work": "Further exploration into the integration of this calibration method with other advanced model architectures and more extensive training datasets is suggested.", "keywords": ["Neural Machine Translation", "Calibration", "Translation Quality", "Large Language Models", "Hypothesis Likelihoods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061.pdf", "abs": "https://arxiv.org/abs/2504.19061", "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization.", "AI": {"tldr": "This paper evaluates open-source large language models for summarizing clinical discharge reports, focusing on key event extraction and hallucination prevalence.", "motivation": "The aim is to improve the accuracy and reliability of clinical summarization using LLMs, essential for patient understanding and care management.", "method": "The study involves numerical simulations to assess the performance of open-source LLMs in extracting key events from discharge reports.", "result": "The evaluation reveals insights into the accuracy of extracted content and identifies the types of hallucinations present in the model-generated summaries.", "conclusion": "Findings highlight the potential of LLMs in clinical settings while emphasizing the need for careful evaluation of hallucinations for maintaining reliability in patient care.", "key_contributions": ["Evaluation of open-source LLMs for clinical discharge summaries", "Analysis of key event extraction effectiveness", "Assessment of hallucination prevalence in LLM output"], "limitations": "The study may be limited by the specific types of discharge reports used and the models evaluated.", "future_work": "Future research should explore additional clinical texts and improve LLM methodologies to reduce hallucinations and enhance summarization quality.", "keywords": ["Clinical Summarization", "Large Language Models", "Discharge Reports", "Hallucination Detection", "Patient Care"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061.pdf", "abs": "https://arxiv.org/abs/2504.19061", "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization.", "AI": {"tldr": "This paper evaluates the effectiveness of open-source large language models in summarizing clinical discharge reports and examines the prevalence of hallucinations in the generated summaries.", "motivation": "Clinical summarization is essential for improving patient understanding and care management by simplifying complex medical data.", "method": "The study investigates the use of open-source LLMs to extract key events from discharge reports and assesses hallucinations in the generated summaries through numerical simulations.", "result": "The performance of open-source LLMs in extracting key clinical information and the prevalence of hallucinations in the summaries are rigorously evaluated and reported.", "conclusion": "The findings highlight the potential of LLMs in clinical summarization while raising concerns about the reliability and accuracy of the generated information due to hallucinations.", "key_contributions": ["Evaluation of open-source LLMs in clinical summarization tasks", "Analysis of hallucinations in model-generated summaries", "Comprehensive numerical simulations to assess model performance"], "limitations": "Focuses on specific types of clinical texts (discharge reports), which may limit generalizability.", "future_work": "Further research could expand to other clinical documentation types and investigate methods to minimize hallucinations.", "keywords": ["Clinical summarization", "Large language models", "Hallucinations", "Discharge reports", "Patient care"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066.pdf", "abs": "https://arxiv.org/abs/2504.19066", "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics.", "AI": {"tldr": "This paper introduces EWRA, an innovative method that enhances small language models for analyzing extreme weather events by integrating structured reasoning paths and a large dataset of related news articles.", "motivation": "Accurate assessments of extreme weather events are vital for research and policy, but a lack of localized data hinders effective decision-making.", "method": "The paper proposes Extreme Weather Reasoning-Aware Alignment (EWRA), which incorporates structured reasoning paths derived from large language models into small language models, utilizing the ExtremeWeatherNews dataset.", "result": "The approach improves small language models' abilities to generate well-grounded, domain-specific responses for extreme weather analytics, outperforming task-specific models.", "conclusion": "The EWRA method and the ClimaEmpact framework enhance the performance and real-world applicability of small language models in extreme weather analytics.", "key_contributions": ["Proposal of EWRA for aligning small language models with structured reasoning", "Creation of the ExtremeWeatherNews dataset to support extreme weather analysis", "Demonstration of improved performance of small language models over task-specific models in extreme weather tasks."], "limitations": "", "future_work": "Further exploration of the framework's applicability to other domains and refining the reasoning alignment processes.", "keywords": ["Extreme Weather", "Language Models", "Data Alignment", "Natural Language Processing", "Analytics"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070.pdf", "abs": "https://arxiv.org/abs/2504.19070", "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency.", "AI": {"tldr": "Development of a sample-efficient Hinglish chatbot using cross-lingual language models and fine-tuning techniques to overcome unique computational challenges.", "motivation": "The goal is to address the challenges posed by Hinglish, a code-mixed language, in generating conversational dialogue efficiently due to data scarcity and lack of standardization.", "method": "The paper evaluates multiple pre-trained cross-lingual models and employs fine-tuning techniques alongside synthetic dialogue generation to enhance performance on Hinglish tasks.", "result": "Models with fewer parameters, when fine-tuned on high-quality, code-mixed datasets, demonstrated competitive performance in conversational generation tasks.", "conclusion": "Efficient fine-tuning methods on limited data can yield effective results for developing conversational AI in code-mixed languages like Hinglish.", "key_contributions": ["Evaluation of pre-trained cross-lingual models for Hinglish", "Integration of synthetic dialogues with existing datasets", "Demonstration of competitive performance with fewer parameters"], "limitations": "The reliance on synthetic data may impact the realism of conversational quality.", "future_work": "Exploration of additional fine-tuning strategies and outside data sources to further improve model performance on Hinglish conversational tasks.", "keywords": ["Hinglish", "conversational AI", "language models", "fine-tuning", "cross-lingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556.pdf", "abs": "https://arxiv.org/abs/2504.19556", "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "authors": ["Kristen Sussman", "Daniel Carter"], "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns.", "AI": {"tldr": "This study analyzes the impact of AI-mediated communication on linguistic patterns by comparing tweets from 2020 and 2024, revealing shifts in sentiment and text complexity.", "motivation": "To understand the effects of AI-mediated communication on language use and sentiment in social media over time.", "method": "The study compares a dataset of 970,919 tweets from 2020 and 20,000 tweets from 2024 using Flesch-Kincaid readability and polarity scores for analysis.", "result": "A significant increase in mean sentiment polarity was found from 0.04 in 2020 to 0.12 in 2024, along with a shift from 54.8% neutral content in 2020 to 39.8% in 2024 and an increase in positive sentiment from 28.6% to 45.9%.", "conclusion": "The results indicate an increasing influence of AI on social media communication, affecting language and emotional expression.", "key_contributions": ["Demonstrates the impact of AI on language use in social media over time.", "Provides statistical evidence of sentiment shifts in social media discourse.", "Highlights the changing nature of emotional expression in the context of AI-mediated communication."], "limitations": "The analysis is limited to tweets that mention Donald Trump, which may not be representative of broader social media trends.", "future_work": "Future research could explore the effects of AI communication in other contexts and with more diverse datasets.", "keywords": ["AI-mediated communication", "social media", "sentiment analysis", "linguistic patterns", "large language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095.pdf", "abs": "https://arxiv.org/abs/2504.19095", "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT.", "AI": {"tldr": "Introduction of Speculative Chain-of-Thought (SCoT) to reduce reasoning latency using model collaboration.", "motivation": "To address the high reasoning costs and latency in large reasoning language models while maintaining accuracy.", "method": "The method involves drafting thoughts with a lightweight model and correcting mistakes with a larger target model, enhancing drafting efficiency and prediction accuracy.", "result": "Experimental results demonstrate that SCoT can reduce reasoning latency by 48% to 66% while achieving performance near the target model on various datasets.", "conclusion": "SCoT presents a novel approach to improve reasoning efficiency without sacrificing accuracy, which could influence future model implementations.", "key_contributions": ["Introduction of SCoT for efficient reasoning in large models.", "Demonstration of reduced reasoning latency with maintained accuracy.", "Application of lightweight and heavyweight model collaboration."], "limitations": "The paper does not address the trade-offs between model sizes in practical applications or robustness of the proposed approach in varied contexts.", "future_work": "Exploration of further enhancing model collaborations and testing across additional domains or datasets.", "keywords": ["Reasoning Language Models", "Speculative Chain-of-Thought", "Model Collaboration"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101.pdf", "abs": "https://arxiv.org/abs/2504.19101", "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection.", "AI": {"tldr": "This paper presents a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG) that addresses data privacy challenges in private Retrieval-Augmented Generation systems by leveraging federated learning and knowledge distillation.", "motivation": "To enhance the accuracy and credibility of Large Language Models in Question & Answer tasks while addressing the challenges of data scarcity and privacy in private RAG systems.", "method": "The paper proposes the FedE4RAG framework, which uses federated learning for collaborative training of RAG retrieval models, securing data privacy through aggregation of model parameters and employing knowledge distillation between server and clients.", "result": "Extensive experiments show that FedE4RAG significantly improves the performance of private RAG systems while ensuring robust data privacy protection, effectively balancing data security with availability.", "conclusion": "The proposed FedE4RAG framework can facilitate better deployment of privacy-preserving RAG systems, making them more effective in real-world applications.", "key_contributions": ["Introduction of the FedE4RAG framework for federated learning in RAG systems", "Application of knowledge distillation to enhance client-side model training", "Integration of homomorphic encryption for safeguarding model parameters"], "limitations": "", "future_work": "Exploration of further optimizations in federated learning for RAG systems and broader applications in different domains.", "keywords": ["Retrieval-Augmented Generation", "Federated Learning", "Data Privacy", "Knowledge Distillation", "Homomorphic Encryption"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2311.10833", "pdf": "https://arxiv.org/pdf/2311.10833.pdf", "abs": "https://arxiv.org/abs/2311.10833", "title": "Generative AI has lowered the barriers to computational social sciences", "authors": ["Yongjun Zhang"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Generative artificial intelligence (AI) has revolutionized the field of\ncomputational social science (CSS), unleashing new possibilities for collecting\nand analyzing multimodal data, especially for scholars who may not have\nextensive programming expertise. This breakthrough carries profound\nimplications for social scientists. First, generative AI can significantly\nenhance the productivity of social scientists by automating the generation,\nannotation, and debugging of code. Second, it empowers researchers to delve\ninto sophisticated data analysis through the innovative use of prompt\nengineering. Last, the educational sphere of CSS stands to benefit immensely\nfrom these tools, given their exceptional ability to annotate and elucidate\ncomplex codes for learners, thereby simplifying the learning process and making\nthe technology more accessible.", "AI": {"tldr": "This paper explores how generative AI enhances computational social science by improving productivity, data analysis, and educational accessibility.", "motivation": "The need for enhancing productivity, data analysis capabilities, and educational accessibility in computational social science, especially for those with limited programming skills.", "method": "The paper discusses the applications of generative AI for automating code generation, annotation, debugging, and advanced data analysis through prompt engineering.", "result": "Generative AI significantly boosts the productivity of social scientists, allows for advanced data analysis, and improves educational tools for learning coding and data interaction.", "conclusion": "Generative AI holds transformative potential for computational social science, making it more accessible and efficient for researchers and students.", "key_contributions": ["Enhancement of productivity in social science research through automation.", "Introduction of prompt engineering for sophisticated data analysis.", "Improvement in educational tools for computational social science."], "limitations": "", "future_work": "Further research on integrating generative AI tools into existing social science methodologies.", "keywords": ["Generative AI", "Computational Social Science", "Data Analysis", "Prompt Engineering", "Educational Tools"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110.pdf", "abs": "https://arxiv.org/abs/2504.19110", "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries.", "AI": {"tldr": "This paper introduces Automated Proof Engineering (APE) to enhance automated proof tasks using large language models (LLMs) and presents APE-Bench I, a benchmark from real-world proof libraries.", "motivation": "To address the limitations of existing benchmarks in formal theorem proving that do not reflect real-world, iterative workflows in formal mathematics.", "method": "Development of APE-Bench I, a benchmark using commit histories from Mathlib4 with tasks described in natural language and verified through a hybrid method using the Lean compiler and LLM-as-a-Judge.", "result": "Empirical testing shows strong performance for localized edits but significant challenges in complex proof engineering tasks.", "conclusion": "This research establishes a foundational framework for future automated proof workflows and suggests directions for more complex multi-file verification benchmarks.", "key_contributions": ["Introduction of Automated Proof Engineering (APE) concept.", "Creation of the APE-Bench I benchmark from real-world data.", "Development of Eleanstic, a scalable verification infrastructure."], "limitations": "Performance degradation on complex proof engineering tasks was noted, indicating existing limitations in current LLM approaches.", "future_work": "Future benchmarks will target improvements in multi-file coordination, project-scale verification, and the creation of autonomous agents for formal libraries.", "keywords": ["Automated Proof Engineering", "Large Language Models", "Formal Theorem Proving"], "importance_score": 5, "read_time_minutes": 12}}
{"id": "2407.02896", "pdf": "https://arxiv.org/pdf/2407.02896.pdf", "abs": "https://arxiv.org/abs/2407.02896", "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality", "authors": ["Portia Wang", "Eugy Han", "Anna C. M. Queiroz", "Cyan DeVeaux", "Jeremy N. Bailenson"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "In networked virtual reality (VR), user behaviors, individual differences,\nand group dynamics can serve as important signals into future speech behaviors,\nsuch as who the next speaker will be and the timing of turn-taking behaviors.\nThe ability to predict and understand these behaviors offers opportunities to\nprovide adaptive and personalized assistance, for example helping users with\nvarying sensory abilities navigate complex social scenes and instantiating\nvirtual moderators with natural behaviors. In this work, we predict turn-taking\nbehaviors using features extracted based on social dynamics literature. We\ndiscuss results from a large-scale VR classroom dataset consisting of 77\nsessions and 1660 minutes of small-group social interactions collected over\nfour weeks. In our evaluation, gradient boosting classifiers achieved the best\nperformance, with accuracies of 0.71--0.78 AUC (area under the ROC curve)\nacross three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking\nbehaviors. In interpreting these models, we found that group size, listener\npersonality, speech-related behavior (e.g., time elapsed since the listener's\nlast speech event), group gaze (e.g., how much the group looks at the speaker),\nas well as the listener's and previous speaker's head pitch, head y-axis\nposition, and left hand y-axis position more saliently influenced predictions.\nResults suggested that these features remain reliable indicators in novel\nsocial VR settings, as prediction performance is robust over time and with\ngroups and activities not used in the training dataset. We discuss theoretical\nand practical implications of the work.", "AI": {"tldr": "This study predicts turn-taking behaviors in virtual reality using social dynamics features, analyzing 77 VR classroom sessions.", "motivation": "To leverage user behaviors and social dynamics in networked VR to enhance understanding and prediction of turn-taking and to provide personalized assistance in social interactions.", "method": "The study analyzed a large-scale VR classroom dataset and employed gradient boosting classifiers to predict turn-taking behaviors across three tasks relevant to social interactions.", "result": "The classifiers achieved accuracies between 0.71 and 0.78 AUC, identifying key features influencing predictions such as group size and listener behavior.", "conclusion": "The findings indicate that specific social dynamics features are reliable indicators of turn-taking in VR, applicable across diverse social settings.", "key_contributions": ["Prediction of turn-taking behaviors in VR environments using social dynamics features", "Identification of salient factors influencing turn-taking predictions", "Demonstration of robust model performance across novel groups and activities"], "limitations": "The approach is limited to social interactions captured in the dataset and may not generalize to all types of interactions outside of the VR classroom context.", "future_work": "Investigating the application of these predictive models in varied VR settings and exploring further personalization of virtual assistant behaviors for users with sensory differences.", "keywords": ["Virtual Reality", "Turn-taking", "Social Dynamics", "Machine Learning", "Adaptative Assistive Technologies"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162.pdf", "abs": "https://arxiv.org/abs/2504.19162", "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models.", "AI": {"tldr": "Introducing Self-Play Critic (SPC), a method that assesses reasoning steps in large language models through adversarial self-play, leading to improved error detection without manual annotations.", "motivation": "The challenge of evaluating LLM reasoning reliability due to the high cost and difficulty of obtaining precise step-level supervision.", "method": "SPC utilizes two copies of a base model, a 'sneaky generator' that creates difficult-to-detect errors and a 'critic' that analyzes these steps, training them through adversarial self-play games using reinforcement learning.", "result": "SPC enhances error detection capabilities, improving accuracy from 70.8% to 77.7% on the ProcessBench benchmark and outperforming strong baseline models.", "conclusion": "SPC effectively trains LLMs to improve their reasoning performance, specifically in mathematical tasks, and demonstrates the ability to reduce reliance on manual supervision.", "key_contributions": ["Introduction of a novel adversarial self-play methodology for model training", "Improvement of error detection capabilities in LLMs without manual annotation", "Significant enhancements in mathematical reasoning performance on benchmark datasets"], "limitations": "", "future_work": "Exploration of further applications of SPC in various reasoning tasks and enhancement of critic model capabilities.", "keywords": ["Large Language Models", "Self-Play Critic", "Reasoning Evaluation", "Reinforcement Learning", "Error Detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.08775", "pdf": "https://arxiv.org/pdf/2409.08775.pdf", "abs": "https://arxiv.org/abs/2409.08775", "title": "What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use", "authors": ["Qianou Ma", "Weirui Peng", "Chenyang Yang", "Hua Shen", "Kenneth Koedinger", "Tongshuang Wu"], "categories": ["cs.HC", "cs.AI"], "comment": "15 pages; TOCHI 2025", "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications.", "AI": {"tldr": "The paper introduces Requirement-Oriented Prompt Engineering (ROPE), designed to improve requirement articulation for better LLM performance in complex tasks, showing significant gains over conventional techniques.", "motivation": "Existing prompt engineering techniques often neglect the importance of clearly articulating customized requirements, which is crucial for effective LLM application.", "method": "ROPE is implemented through a training suite that allows users to practice generating requirements with feedback from LLMs. It was tested in a controlled experiment involving 30 novices.", "result": "Participants trained with ROPE achieved a 20% improvement in task performance compared to a 1% improvement with conventional methods, demonstrating the effectiveness of focusing on requirement quality.", "conclusion": "ROPE empowers end-users to create better LLM applications by enhancing their capability to articulate requirements clearly, indicating a strong link between input quality and output success.", "key_contributions": ["Introduction of the ROPE paradigm for prompt engineering.", "Development of a training suite for practicing requirement articulation.", "Empirical evidence showing significant performance improvements over traditional methods."], "limitations": "", "future_work": "Exploration of other potential areas where ROPE can be applied and how it can further improve interaction with LLMs.", "keywords": ["prompt engineering", "requirement articulation", "LLM applications"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191.pdf", "abs": "https://arxiv.org/abs/2504.19191", "title": "WuNeng: Hybrid State with Attention", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.", "AI": {"tldr": "The WuNeng architecture enhances large language models by integrating RNN-based RWKV-7 with improved attention mechanisms for better contextual coherence and efficiency.", "motivation": "To enhance the expressivity and computational efficiency of large language models while maintaining contextual coherence.", "method": "Integration of RNN-based RWKV-7 with advanced attention mechanisms, using a hybrid-head concept to augment standard multi-head attention and a cross-head interaction technique for dynamic synergy among heads.", "result": "The architecture significantly boosts the expressivity of the model with minimal additional parameters, excelling in complex reasoning and sequence generation tasks.", "conclusion": "WuNeng establishes a new standard for balancing expressivity and computational efficiency in neural architectures.", "key_contributions": ["Introduction of a novel architecture integrating RWKV-7 with advanced attention mechanisms.", "Use of hybrid-head concept to enhance multi-head attention.", "Implementation of multi-token state processing for capturing sequence-wide dependencies."], "limitations": "", "future_work": "", "keywords": ["WuNeng", "large language models", "RWKV-7", "attention mechanisms", "neural architectures"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.01568", "pdf": "https://arxiv.org/pdf/2501.01568.pdf", "abs": "https://arxiv.org/abs/2501.01568", "title": "Interruption Handling for Conversational Robots", "authors": ["Shiye Cao", "Jiwon Moon", "Amama Mahmood", "Victor Nikhil Antony", "Ziang Xiao", "Anqi Liu", "Chien-Ming Huang"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots.", "AI": {"tldr": "The paper presents a system for real-time detection and management of user-initiated interruptions in robotic conversations, improving the robot's conversational efficiency.", "motivation": "To improve the management of user-initiated interruptions in robotic systems, which are often not handled effectively despite advancements in technology.", "method": "The system detects user-initiated interruptions based on the interrupter's intent and manages these interruptions in real-time, utilizing interaction patterns from human-human interactions.", "result": "The system handled 93.69% of user-initiated interruptions effectively during user tests, demonstrating its potential for enhancing robotic conversations.", "conclusion": "Designing conversational robots that manage interruptions effectively can lead to more dynamic and effective human-robot interactions.", "key_contributions": ["Introduced a real-time interruption detection and management system for robotic conversations.", "Integrated the system into an LLM-powered social robot and validated its performance in two tasks.", "Provided insights for designing better interruption-handling behaviors in conversational robots."], "limitations": "The study's findings may be limited by the specific scenarios tested and the sample size of 21 participants.", "future_work": "Explore the application of the developed system in broader contexts and its adaptation to other interaction patterns beyond the tested scenarios.", "keywords": ["human-robot interaction", "interruptions", "LLM-powered systems", "real-time management", "conversational robots"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209.pdf", "abs": "https://arxiv.org/abs/2504.19209", "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search.", "AI": {"tldr": "This paper evaluates dynamic embedded topic model implementations across various historical corpora to enhance applied scholarship utility.", "motivation": "The study aims to improve the Dynamic Embedded Topic Model to better handle diachronic corpora and identify key implementation decisions that enhance its applicability in scholarship.", "method": "The authors apply the Dynamic Embedded Topic Model to five different diachronic corpora, analyzing various implementation choices and their effects.", "result": "Key findings indicate that model performance can be improved through better vocabulary scalability and flexible interval modeling, while certain limitations do not significantly affect performance.", "conclusion": "Enhancing the scalability of vocabulary size and modeling flexibility can maximize the effectiveness of the Dynamic Embedded Topic Model.", "key_contributions": ["Identification of implementation choices that affect the utility of the model", "Recommending practical approaches for vocabulary size scalability", "Highlighting performance aspects that do not significantly hinder application"], "limitations": "The model's application is still impacted by certain decisions, but many hypothesized limiting factors were found not to affect performance significantly.", "future_work": "Further exploration of additional implementation choices and their impact on performance across different types of corpora.", "keywords": ["Dynamic Embedded Topic Model", "diachronic corpora", "implementation choices"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.06009", "pdf": "https://arxiv.org/pdf/2502.06009.pdf", "abs": "https://arxiv.org/abs/2502.06009", "title": "Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage", "authors": ["Jenny S Wang", "Samar Haider", "Amir Tohidi", "Anushkaa Gupta", "Yuxuan Zhang", "Chris Callison-Burch", "David Rothschild", "Duncan J Watts"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Mainstream media, through their decisions on what to cover and how to frame\nthe stories they cover, can mislead readers without using outright falsehoods.\nTherefore, it is crucial to have tools that expose these editorial choices\nunderlying media bias. In this paper, we introduce the Media Bias Detector, a\ntool for researchers, journalists, and news consumers. By integrating large\nlanguage models, we provide near real-time granular insights into the topics,\ntone, political lean, and facts of news articles aggregated to the publisher\nlevel. We assessed the tool's impact by interviewing 13 experts from\njournalism, communications, and political science, revealing key insights into\nusability and functionality, practical applications, and AI's role in powering\nmedia bias tools. We explored this in more depth with a follow-up survey of 150\nnews consumers. This work highlights opportunities for AI-driven tools that\nempower users to critically engage with media content, particularly in\npolitically charged environments.", "AI": {"tldr": "The Media Bias Detector leverages large language models to provide insights into the editorial choices and bias in news articles, helping users critically engage with media content.", "motivation": "To expose media bias caused by editorial choices in mainstream media, which can mislead readers without using explicit falsehoods.", "method": "The tool integrates large language models to analyze the topics, tone, political lean, and facts of news articles aggregated at the publisher level. Usability and functionality were assessed through expert interviews and a survey of news consumers.", "result": "The tool provides near real-time, granular insights into media bias, demonstrating its applicability and potential impact on user engagement with news content.", "conclusion": "AI-driven tools like the Media Bias Detector can empower users to critically engage with media content, especially in politically charged contexts.", "key_contributions": ["Introduction of the Media Bias Detector tool", "Integration of large language models for media analysis", "Insights from expert interviews and consumer surveys on usability and impact"], "limitations": "The study may have limitations in the generalizability of the findings from expert interviews and surveys.", "future_work": "Further research on enhancing the tool's capabilities and expanding its impact across diverse user groups.", "keywords": ["Media Bias", "AI Tools", "Large Language Models", "Journalism", "User Engagement"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254.pdf", "abs": "https://arxiv.org/abs/2504.19254", "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.", "AI": {"tldr": "The paper presents a framework for zero-resource hallucination detection in Large Language Models using uncertainty quantification techniques, showing improved performance in high-stakes applications like healthcare.", "motivation": "The motivation is the persistent issue of hallucinations in Large Language Models, especially as they are increasingly used in high-stakes domains such as healthcare and finance.", "method": "The authors adapt various uncertainty quantification techniques into a standardized framework, creating response-level confidence scores and a tunable ensemble approach for improved detection of hallucinations.", "result": "The tunable ensemble approach typically outperforms both its individual components and existing methods, demonstrating enhanced accuracy and reliability for LLM applications.", "conclusion": "The results highlight the importance of customized hallucination detection strategies to bolster the performance of LLMs in critical applications.", "key_contributions": ["Versatile framework for zero-resource hallucination detection.", "Introduction of a tunable ensemble approach for optimizing confidence scores.", "Provision of a companion Python toolkit, UQLM, for implementation."], "limitations": "", "future_work": "Future work could explore enhancements in uncertainty quantification techniques and expand the toolkit's applications in different high-stakes domains.", "keywords": ["hallucination detection", "Large Language Models", "uncertainty quantification", "healthcare", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13902", "pdf": "https://arxiv.org/pdf/2502.13902.pdf", "abs": "https://arxiv.org/abs/2502.13902", "title": "Grid Labeling: Crowdsourcing Task-Specific Importance from Visualizations", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Andreas Bulling", "Cindy Xiong Bearfield"], "categories": ["cs.HC"], "comment": "6 pages, 4 figures, Accepted to EuroVis 2025 (Short Paper Track)", "summary": "Knowing where people look in visualizations is key to effective design. Yet,\nexisting research primarily focuses on free-viewing-based saliency models -\nalthough visual attention is inherently task-dependent. Collecting\ntask-relevant importance data remains a resource-intensive challenge. To\naddress this, we introduce Grid Labeling - a novel annotation method for\ncollecting task-specific importance data to enhance saliency prediction models.\nGrid Labeling dynamically segments visualizations into Adaptive Grids, enabling\nefficient, low-effort annotation while adapting to visualization structure. We\nconducted a human subject study comparing Grid Labeling with existing\nannotation methods, ImportAnnots, and BubbleView across multiple metrics.\nResults show that Grid Labeling produces the least noisy data and the highest\ninter-participant agreement with fewer participants while requiring less\nphysical (e.g., clicks/mouse movements) and cognitive effort. An interactive\ndemo is available at https://jangsus1.github.io/Grid-Labeling.", "AI": {"tldr": "This paper introduces Grid Labeling, a method for collecting task-specific importance data for visualizations to improve saliency prediction models.", "motivation": "Existing approaches to visual attention analysis are mostly based on free-viewing saliency models, which do not account for task-dependent factors. The authors aim to enhance saliency prediction models by addressing the challenges in collecting task-relevant importance data.", "method": "Grid Labeling segments visualizations into Adaptive Grids for efficient annotation, allowing users to provide task-specific importance data with lower cognitive and physical effort compared to other methods.", "result": "The human subject study demonstrated that Grid Labeling yields the least noisy data and highest inter-participant agreement, with fewer participants and reduced cognitive effort needed.", "conclusion": "Grid Labeling presents a novel solution for collecting more relevant and efficient importance data that can enhance saliency models in task-dependent scenarios.", "key_contributions": ["Introduction of Grid Labeling for efficient data collection", "Demonstration of Adaptive Grids for annotation", "Comparison showing superior performance of Grid Labeling over existing methods"], "limitations": "The study may be limited by the specific tasks tested and may not generalize to all types of visualizations or user groups.", "future_work": "Further research could explore additional tasks and different types of visualizations to broaden the application of Grid Labeling.", "keywords": ["visualization", "task-specific importance", "saliency models", "Grid Labeling", "human subject study"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267.pdf", "abs": "https://arxiv.org/abs/2504.19267", "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.", "AI": {"tldr": "This paper introduces a novel multimodal model for visual storytelling that produces narratives from images, using advanced evaluation metrics.", "motivation": "The aim is to enhance the task of visual storytelling by leveraging recent advancements in multimodal models and addressing the limitations of traditional evaluation metrics.", "method": "The authors present the VIST-GPT model, which employs transformer-based architectures and utilizes the Visual Storytelling (VIST) dataset to generate narratives.", "result": "The proposed model produces visually grounded and contextually appropriate narratives, outperforming traditional models.", "conclusion": "Novel reference-free metrics RoViST and GROOVIST are introduced to better evaluate the quality of visual narratives, focusing on aspects like grounding and coherence.", "key_contributions": ["Introduction of VIST-GPT model for visual storytelling using transformer-based architecture", "Development of RoViST and GROOVIST metrics for narrative evaluation", "Demonstration of improved narrative generation using multimodal models"], "limitations": "Evaluation metrics may still require validation against human judgments across diverse storytelling contexts.", "future_work": "Future research could explore further refinements in narrative generation techniques and enhanced evaluation metrics.", "keywords": ["Visual Storytelling", "Multimodal Models", "Narrative Generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.08539", "pdf": "https://arxiv.org/pdf/2503.08539.pdf", "abs": "https://arxiv.org/abs/2503.08539", "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and Readability of Dictation Interfaces", "authors": ["Zhaohui Liang", "Yonglin Chen", "Naser Al Madi", "Can Liu"], "categories": ["cs.HC"], "comment": null, "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nRAKE guides the reading of self-produced text better than GP-TSM. These\nsurprising findings suggest that dictation interfaces could consider showing\nsummaries or key information to support recall instead of raw transcripts.", "AI": {"tldr": "The study evaluates five dictation interfaces using eye-tracking to understand how users read and review dictated text, revealing preferences for summaries and highlighting methods that improve readability.", "motivation": "To investigate user behavior regarding reading and reviewing text from dictation interfaces, and to enhance the usability of such systems.", "method": "Conducted a controlled eye-tracking experiment with 20 participants comparing five dictation interfaces: PLAIN, AOC, RAKE, GP-TSM, and SUMMARY.", "result": "Participants spent only 7--11% of their time actively reading during composition and preferred SUMMARY for its readability, despite its use of unfamiliar words.", "conclusion": "Dictation interfaces should incorporate summarization and key information display to improve user experience and recall.", "key_contributions": ["Comparison of five dictation interfaces using eye-tracking", "Insights into user reading behavior during dictation and reviewing", "Recommendations for improving dictation interface design with summary features"], "limitations": "Limited sample size of 20 participants may affect generalizability of results.", "future_work": "Further exploration of different dictation methods and user adaptations to varying interface feedback types.", "keywords": ["Dictation Interfaces", "Eye-Tracking", "Human-Computer Interaction", "User Experience", "Text Input"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298.pdf", "abs": "https://arxiv.org/abs/2504.19298", "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen.", "AI": {"tldr": "The paper introduces AndroidGen, a framework to enhance LLM-based agents on mobile devices by addressing data scarcity through trajectory collection and open-source model training.", "motivation": "The paper addresses the gap in using LLMs as agents on mobile devices due to high demands for quality data and challenges in human annotation.", "method": "The authors develop AndroidGen to enhance LLM capabilities by collecting task trajectories and training open-source LLMs without the need for manually labeled data.", "result": "AndroidGen was extensively evaluated alongside AndroidWorld, AitW, and various applications, showing significant improvements in performance.", "conclusion": "The research reveals the potential of AndroidGen in improving LLM-based mobile agents and highlights areas for future exploration and enhancement.", "key_contributions": ["Introduction of AndroidGen framework for LLM-based mobile agents", "Development of open-source mobile agent training without manual annotation", "Extensive evaluation showcasing improvements over existing systems"], "limitations": "The study focuses on specific applications and may not generalize to all mobile environments or tasks.", "future_work": "Further improvements to the trajectory collection process and enhancements to the LLM training methodology are proposed as future exploration.", "keywords": ["Large Language Models", "Mobile Agents", "Data Scarcity", "Human Tasks", "Open-source"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.17620", "pdf": "https://arxiv.org/pdf/2503.17620.pdf", "abs": "https://arxiv.org/abs/2503.17620", "title": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review", "authors": ["Mingyue Yuan", "Jieshan Chen", "Zhenchang Xing", "Gelareh Mohammadi", "Aaron Quigley"], "categories": ["cs.HC"], "comment": "7 pages, GenAICHI: CHI 2025 Workshop on Generative AI and HCI", "summary": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios.", "AI": {"tldr": "The paper presents MCHR, a semi-automated framework enhancing content annotation efficiency by integrating multiple LLMs with human review to reduce annotation time while maintaining high accuracy.", "motivation": "Content annotation at scale is challenging due to the need for substantial human expertise and effort.", "method": "MCHR employs a structured consensus-building mechanism among multiple LLMs, coupled with an adaptive review protocol that involves targeted human review.", "result": "MCHR demonstrates a reduction in annotation time by 32% to 100% compared to manual methods, maintaining accuracy between 85.5% and 98% across various tasks.", "conclusion": "The systematic integration of LLMs and human review via MCHR significantly improves annotation efficiency while preserving accuracy.", "key_contributions": ["Introduction of a novel semi-automated framework (MCHR) for content annotation", "Development of a structured consensus-building mechanism among LLMs", "Implementation of an adaptive review protocol to strategically engage human expertise"], "limitations": "", "future_work": "", "keywords": ["annotation", "LLM", "human review", "automation", "content analysis"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314.pdf", "abs": "https://arxiv.org/abs/2504.19314", "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.", "AI": {"tldr": "BrowseComp-ZH is a benchmark for evaluating large language models on the Chinese web, highlighting their current limitations in reasoning and retrieval tasks.", "motivation": "To evaluate large language models (LLMs) in real-time web browsing, especially on the Chinese web, where existing benchmarks are lacking.", "method": "The benchmark consists of 289 multi-hop questions across 11 domains, with a two-stage quality control process to ensure question difficulty and answer uniqueness.", "result": "Most evaluated models, including OpenAI's DeepResearch, perform poorly, with accuracy rates below 10% for many, and only a few exceeding 20%.", "conclusion": "The findings illustrate the struggle of current models with complex retrieval and reasoning tasks that are necessary for browsing effectively.", "key_contributions": ["Introduction of BrowseComp-ZH for benchmarking LLMs on the Chinese web", "High-difficulty multi-hop questions covering diverse domains", "Public release of dataset and construction guidelines"], "limitations": "The benchmark primarily focuses on the Chinese web and may not address other languages and information systems.", "future_work": "Extending the evaluation framework to other languages and improving model capabilities in reasoning and retrieval.", "keywords": ["Large Language Models", "Benchmarking", "Human-Computer Interaction", "Information Retrieval"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2504.08985", "pdf": "https://arxiv.org/pdf/2504.08985.pdf", "abs": "https://arxiv.org/abs/2504.08985", "title": "Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design", "authors": ["Luna Xingyu Li", "Ray-yuan Chung", "Feng Chen", "Wenyu Zeng", "Yein Jeon", "Oleg Zaslavsky"], "categories": ["cs.HC", "cs.AI"], "comment": "Accepted as Research talk for Considering Cultural and Linguistic\n  Diversity in AI Applications workshop at CALD-AI@ASIS&T 2025", "summary": "Low technology and eHealth literacy among older adults in retirement\ncommunities hinder engagement with digital tools. To address this, we designed\nan LLM-powered chatbot prototype using a human-centered approach for a local\nretirement community. Through interviews and persona development, we\nprioritized accessibility and dual functionality: simplifying internal\ninformation retrieval and improving technology and eHealth literacy. A pilot\ntrial with residents demonstrated high satisfaction and ease of use, but also\nidentified areas for further improvement. Based on the feedback, we refined the\nchatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt\nengineering to deliver concise responses. Accessible features like adjustable\nfont size, interface theme and personalized follow-up responses were\nimplemented. Future steps include enabling voice-to-text function and\nlongitudinal intervention studies. Together, our results highlight the\npotential of LLM-driven chatbots to empower older adults through accessible,\npersonalized interactions, bridging literacy gaps in retirement communities.", "AI": {"tldr": "The paper presents an LLM-powered chatbot prototype aimed at improving digital engagement and eHealth literacy among older adults in retirement communities through a human-centered design.", "motivation": "Address low technology and eHealth literacy among older adults to enhance their engagement with digital tools.", "method": "Designed and refined a chatbot using interviews, persona development, and tailored prompt engineering, followed by a pilot trial for feedback.", "result": "The pilot trial showed high satisfaction and ease of use among residents, indicating the chatbot's potential to empower users while identifying areas for improvement.", "conclusion": "LLM-driven chatbots can enhance accessibility and personalization for older adults, helping bridge literacy gaps in retirement communities.", "key_contributions": ["Human-centered design approach for chatbot development.", "Integration of accessibility features like adjustable font size and personalized responses.", "Demonstrated effectiveness through pilot testing with real users."], "limitations": "Further improvements needed based on pilot feedback; future functionalities are yet to be implemented.", "future_work": "Implement voice-to-text functions and conduct longitudinal intervention studies.", "keywords": ["LLM", "chatbot", "eHealth literacy", "older adults", "human-centered design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333.pdf", "abs": "https://arxiv.org/abs/2504.19333", "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli", "AI": {"tldr": "This paper presents a method for generating task-specific classifiers that outperform state-of-the-art models in safety detection while being significantly smaller and more efficient.", "motivation": "The increasing reliance on large language models for safety measures leads to challenges, such as latency and high resource consumption, which this work aims to address.", "method": "The authors propose task-specific data generation to create fine-tuned classifiers and introduce a model, MultiTaskGuard, pretrained on a synthetically generated dataset. They also present a search-based model merging approach to optimize the combination of policies.", "result": "On 7 public datasets, the proposed classifiers demonstrated an average F1 score improvement of 29.92 points over Aegis-LlamaGuard and 21.62 over gpt-4o, outperforming existing LLMs and guardrail APIs.", "conclusion": "The results indicate that efficient guardrail classifiers can be both compact and highly effective in detecting safe and unsafe behaviors, presenting a viable alternative to current LLMs.", "key_contributions": ["Introduced task-specific data generation for classifier tuning", "Developed MultiTaskGuard model for better generalization", "Proposed a search-based model merging technique for optimal parameter combination"], "limitations": "The study may be limited by the dependency on synthetic data and the potential for overfitting in specific task contexts.", "future_work": "Exploration of more diverse synthetic datasets and further refinement of the merging approach for improved model performance.", "keywords": ["Large Language Models", "Task-Specific Classifiers", "Guardrail Models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.13926", "pdf": "https://arxiv.org/pdf/2504.13926.pdf", "abs": "https://arxiv.org/abs/2504.13926", "title": "A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust", "authors": ["Chameera De Silva", "Thilina Halloluwa", "Dhaval Vyas"], "categories": ["cs.HC", "cs.AI"], "comment": "I am requesting this withdrawal because I believe the current version\n  requires significant revisions and restructuring to better reflect the\n  intended research contributions. I plan to substantially improve the work and\n  may resubmit a revised version in the future. Thank you for your\n  understanding and support", "summary": "The integration of Artificial Intelligence (AI) into high-stakes domains such\nas healthcare, finance, and autonomous systems is often constrained by concerns\nover transparency, interpretability, and trust. While Human-Centered AI (HCAI)\nemphasizes alignment with human values, Explainable AI (XAI) enhances\ntransparency by making AI decisions more understandable. However, the lack of a\nunified approach limits AI's effectiveness in critical decision-making\nscenarios. This paper presents a novel three-layered framework that bridges\nHCAI and XAI to establish a structured explainability paradigm. The framework\ncomprises (1) a foundational AI model with built-in explainability mechanisms,\n(2) a human-centered explanation layer that tailors explanations based on\ncognitive load and user expertise, and (3) a dynamic feedback loop that refines\nexplanations through real-time user interaction. The framework is evaluated\nacross healthcare, finance, and software development, demonstrating its\npotential to enhance decision-making, regulatory compliance, and public trust.\nOur findings advance Human-Centered Explainable AI (HCXAI), fostering AI\nsystems that are transparent, adaptable, and ethically aligned.", "AI": {"tldr": "This paper introduces a three-layered framework merging Human-Centered AI and Explainable AI to improve transparency and trust in high-stakes decision-making contexts such as healthcare and finance.", "motivation": "The integration of AI in critical fields is limited by transparency and trust issues, necessitating a unified approach to enhance decision-making effectiveness.", "method": "A three-layered framework is proposed, including an AI model with built-in explainability, a user-centric explanation layer, and a feedback loop for real-time adaptation.", "result": "The framework was evaluated in healthcare, finance, and software development, showing improved decision-making, regulatory compliance, and enhanced public trust.", "conclusion": "The findings contribute to the field of Human-Centered Explainable AI, illustrating the potential for creating transparent, adaptable, and ethically aligned AI systems.", "key_contributions": ["Novel three-layered framework for explainability", "Integration of human-centered design with AI", "Evaluation across multiple high-stakes domains"], "limitations": "Current version requires significant revisions and restructuring to reflect intended contributions more accurately.", "future_work": "Plans for revision and potential resubmission to improve the work.", "keywords": ["Human-Centered AI", "Explainable AI", "Healthcare", "AI Transparency", "Decision-Making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339.pdf", "abs": "https://arxiv.org/abs/2504.19339", "title": "Explanatory Summarization with Discourse-Driven Planning", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination.", "AI": {"tldr": "This paper presents a plan-based approach to enhance automatic summarization of scientific documents by integrating discourse frameworks to improve clarity in lay summaries.", "motivation": "Current automatic summarization methods lack explicit modeling of explanations, making it hard to align with human-written summaries.", "method": "The authors propose two discourse-driven planning strategies that condition a discourse plan either as part of the input or as part of the output prefix for generating summaries.", "result": "Empirical experiments on three lay summarization datasets demonstrate that the proposed approach significantly outperforms existing state-of-the-art methods in summary quality, model robustness, and controllability.", "conclusion": "The plan-based approach effectively improves the quality of lay summaries by better aligning explanatory content with human expectations.", "key_contributions": ["Introduction of discourse-driven planning strategies for summary generation.", "Demonstrated improvements in summary quality over existing methods.", "Enhanced robustness and controllability in automatic summarization."], "limitations": "", "future_work": "Explore further integration of discourse frameworks in various summarization tasks and develop user-centered evaluation methods.", "keywords": ["automatic summarization", "discourse frameworks", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2504.15647", "pdf": "https://arxiv.org/pdf/2504.15647.pdf", "abs": "https://arxiv.org/abs/2504.15647", "title": "Promoting Real-Time Reflection in Synchronous Communication with Generative AI", "authors": ["Yi Wen", "Meng Xia"], "categories": ["cs.HC"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Real-time reflection plays a vital role in synchronous communication. It\nenables users to adjust their communication strategies dynamically, thereby\nimproving the effectiveness of their communication. Generative AI holds\nsignificant potential to enhance real-time reflection due to its ability to\ncomprehensively understand the current context and generate personalized and\nnuanced content. However, it is challenging to design the way of interaction\nand information presentation to support the real-time workflow rather than\ndisrupt it. In this position paper, we present a review of existing research on\nsystems designed for reflection in different synchronous communication\nscenarios. Based on that, we discuss design implications on how to design\nhuman-AI interaction to support reflection in real time.", "AI": {"tldr": "This paper discusses the importance of real-time reflection in communication, exploring how generative AI can enhance this process while addressing challenges in interaction design and information presentation.", "motivation": "To improve effectiveness in synchronous communication through real-time reflection.", "method": "Review existing research on systems for real-time reflection in synchronous communication and discuss design implications.", "result": "Identified key challenges and potential design solutions for integrating generative AI to support real-time reflection in communication.", "conclusion": "Effective human-AI interaction design can enhance real-time reflection, leading to improved communication outcomes.", "key_contributions": ["Review of existing systems for reflection in synchronous communication", "Discussion of design implications for human-AI interaction", "Insights into integrating generative AI for better reflection support"], "limitations": "", "future_work": "Further exploration of interaction design strategies for real-time AI-supported reflection.", "keywords": ["real-time reflection", "generative AI", "synchronous communication", "human-AI interaction", "design implications"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395.pdf", "abs": "https://arxiv.org/abs/2504.19395", "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs.", "AI": {"tldr": "This paper introduces ICL CIPHERS, a novel approach to study In-Context Learning (ICL) in LLMs that utilizes reversible substitution ciphers to distinguish between task retrieval and task learning.", "motivation": "To better understand the dual modes of In-Context Learning (ICL) in large language models (LLMs) by using a structured approach with cryptography-inspired reformulations.", "method": "The authors developed a class of task reformulations called ICL CIPHERS, which involve substituting tokens in input sentences with irrelevant ones based on a fixed reversible pattern. This allows for the assessment of LLMs' capabilities to decipher the underlying tasks while maintaining abstract task structures.", "result": "Experiments show that LLMs perform better in decoding ICL CIPHERS that employ bijective mappings compared to non-bijective (irreversible) mappings, indicating a novel way to quantify learning in ICL.", "conclusion": "The results suggest that the nature of the mapping in ICL can affect performance, with consistent results across various datasets and models. Insights into LLMs' internal representations were also examined through this approach.", "key_contributions": ["Introduction of ICL CIPHERS to study In-Context Learning in LLMs.", "Demonstration that LLMs can better solve bijective ciphers than non-bijective ones, highlighting their learning capabilities.", "Evidence of LLMs' internal representations aiding in deciphering inputs."], "limitations": "The performance gap is small, suggesting that while there is some distinction in capabilities, it may not be significant across all scenarios.", "future_work": "Further exploration of different cipher constructions and their impacts on LLM performance; potential applications in enhancing understanding of learning paradigms in AI.", "keywords": ["In-Context Learning", "Large Language Models", "Substitution Ciphers", "Task Retrieval", "Task Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406.pdf", "abs": "https://arxiv.org/abs/2504.19406", "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER.", "AI": {"tldr": "This paper presents a novel framework for educational question generation (EQG) from real-world classroom lectures, utilizing large language models to dynamically select and rewrite context from transcripts and videos.", "motivation": "To improve educational question generation from real-world classroom content, addressing the limitations of existing EQG datasets that rely on predefined texts.", "method": "A framework is introduced that selects contexts from lecture transcripts and video keyframes based on answer relevance and temporal proximity, integrating them into answer-containing knowledge statements.", "result": "The approach significantly enhances the quality and relevance of generated questions compared to existing methods.", "conclusion": "The proposed framework overcomes challenges in EQG, leading to better alignment of questions with target answers and timestamps.", "key_contributions": ["Dataset of educational questions generated from real-world classroom lectures", "Novel framework for dynamic context selection and rewriting using large language models", "Improvements in question generation quality and relevance"], "limitations": "Current methods still struggle with contextual selection from extensive transcripts.", "future_work": "Further research could explore more efficient ways to process larger datasets and integrate additional contextual data sources.", "keywords": ["Educational Question Generation", "Large Language Models", "Real-world Data", "Context Selection", "Video Lectures"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413.pdf", "abs": "https://arxiv.org/abs/2504.19413", "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents.", "AI": {"tldr": "Mem0 is a memory-centric architecture designed to enhance long-term coherence in dialogues by dynamically managing memory in large language models (LLMs).", "motivation": "Address the challenges faced by LLMs in maintaining consistency over prolonged multi-session dialogues due to fixed context windows.", "method": "Mem0 employs a scalable memory architecture that extracts, consolidates, and retrieves salient information from ongoing conversations, with a variant using graph-based representations to manage relational structures among conversational elements.", "result": "Empirical evaluations on the LOCOMO benchmark demonstrate Mem0's superiority over existing memory-augmented systems and other baseline models, achieving 26% improvement in LLM-as-a-Judge metric and a 91% reduction in latency and token costs.", "conclusion": "Mem0 enhances conversational coherence and efficiency in LLMs through structured memory mechanisms, suggesting a path forward for more effective AI agents.", "key_contributions": ["Introduction of Mem0 architecture for dynamic memory management in LLMs", "Utilization of graph-based memory representations for complex relational conversation aspects", "Significant performance improvements in memory systems across various query types"], "limitations": "", "future_work": "Explore further improvements in memory representation techniques and their impacts on conversational AI performance.", "keywords": ["Large Language Models", "Memory Management", "Conversational AI", "Graph-based Memory", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436.pdf", "abs": "https://arxiv.org/abs/2504.19436", "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems.", "AI": {"tldr": "The paper presents a dynamic optimization method for Retrieval-Augmented Generation (RAG) that enhances semantic understanding and knowledge access in language models for open-domain question answering and generation tasks.", "motivation": "Address limitations in static RAG structures concerning context adaptation and knowledge access.", "method": "Introduces a state-aware dynamic knowledge retrieval mechanism using a multi-level perceptive retrieval vector and a differentiable document matching path, allowing end-to-end joint training of retrieval and generation modules.", "result": "Significant improvements in BLEU and ROUGE-L scores, and enhanced robustness and generation consistency in tasks with semantic ambiguity and multi-document fusion.", "conclusion": "The proposed method broadens the application potential of RAG architectures in high-quality language generation systems.", "key_contributions": ["State-aware dynamic knowledge retrieval mechanism", "Multi-level perceptive retrieval vector construction", "End-to-end joint training of retrieval and generation modules"], "limitations": "", "future_work": "", "keywords": ["Retrieval-Augmented Generation", "Natural Questions dataset", "knowledge retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445.pdf", "abs": "https://arxiv.org/abs/2504.19445", "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases.", "AI": {"tldr": "This study explores the influence of response format on the judgment biases of large language models in psychological text analyses.", "motivation": "To assess the reliability of large language models (LLMs) in decision-making tasks and understand how response formats can introduce biases.", "method": "The study examined LLM responses in two tasks—a value statement judgment task and a text sentiment analysis task—utilizing binary and continuous response formats across various models.", "result": "The findings showed a consistent negative bias, with LLMs delivering more 'negative' judgments in binary formats than in continuous ones, confirmed by control experiments.", "conclusion": "Small changes in task design, like response format, can significantly impact LLM outputs and introduce systematic biases in decision-making.", "key_contributions": ["Demonstrated the effect of response format on LLM judgment biases.", "Conducted evaluations across multiple models, both open-source and commercial.", "Provided insights for designing LLM application tasks to minimize biases."], "limitations": "Results may vary with different LLM architectures or other untested formats.", "future_work": "Further exploration of response formats and their implications on other decision-making tasks involving LLMs.", "keywords": ["Large Language Models", "Response Format", "Bias", "Decision-Making", "Sentiment Analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457.pdf", "abs": "https://arxiv.org/abs/2504.19457", "title": "Towards Long Context Hallucination Detection", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference.", "AI": {"tldr": "This paper addresses the issue of contextual hallucination in large language models (LLMs) specifically for long-context inputs by proposing a new dataset and a novel architecture for improved detection.", "motivation": "The need to address contextual hallucinations in LLMs, particularly when dealing with long-context inputs, which remains an open problem.", "method": "The authors constructed a dataset aimed at long-context hallucination detection and proposed a novel architecture that employs a decomposition and aggregation mechanism for pre-trained encoder models like BERT to enhance their capability in processing long contexts.", "result": "The proposed architecture significantly outperforms previous models of similar size and LLM-based models across various metrics, and enables faster inference.", "conclusion": "This work lays the groundwork for improved contextual hallucination detection in LLMs using long-context inputs, demonstrating the potential for enhanced performance with the proposed methods.", "key_contributions": ["New dataset for long-context hallucination detection", "Novel architecture for processing long contexts and detecting hallucinations", "Performance improvement over existing models in terms of speed and accuracy"], "limitations": "", "future_work": "Future research could further refine the architecture and explore additional methods for reducing contextual hallucination in various model types.", "keywords": ["Large Language Models", "Contextual Hallucination", "Long-context Inputs", "BERT", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467.pdf", "abs": "https://arxiv.org/abs/2504.19467", "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding.", "AI": {"tldr": "BRIDGE is a comprehensive multilingual benchmark for evaluating large language models (LLMs) in clinical contexts, comprising 87 tasks from real-world electronic health records across nine languages.", "motivation": "There is a lack of comprehensive evaluations of LLMs in clinical contexts, with existing benchmarks limited in scope and generalizability.", "method": "We created BRIDGE, a multilingual benchmark featuring 87 tasks sourced from real-world clinical data, and evaluated 52 state-of-the-art LLMs through 13,572 experiments across various inference strategies.", "result": "Our evaluation revealed significant performance discrepancies among model sizes and clinical specialties, with open-source LLMs performing comparably to proprietary models, while older medically fine-tuned LLMs often underperformed.", "conclusion": "BRIDGE serves as a foundational resource for the evaluation and development of LLMs in understanding real-world clinical text, providing a leaderboard for comparative analysis.", "key_contributions": ["Introduction of the BRIDGE benchmark for multilingual clinical evaluations", "Comprehensive assessment of 52 LLMs across various healthcare applications", "Insights into performance discrepancies in LLMs based on architecture and tuning"], "limitations": "The benchmark may not capture all nuances of EHR data and is limited to the tasks included.", "future_work": "Future research could expand the benchmark to include additional tasks and assess the performance of emerging LLMs in different clinical scenarios.", "keywords": ["large language models", "benchmarking", "healthcare", "electronic health records", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472.pdf", "abs": "https://arxiv.org/abs/2504.19472", "title": "Conflicts in Texts: Data, Implications and Challenges", "authors": ["Siyi Liu", "Dan Roth"], "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively.", "AI": {"tldr": "This survey discusses the issue of conflicting information in NLP models, categorizes these conflicts, and proposes strategies for developing more reliable systems.", "motivation": "The integration of NLP models into real-world applications necessitates addressing the generation and reliance on conflicting information to enhance model reliability and trustworthiness.", "method": "The paper categorizes conflicts into three areas: natural texts on the web, human-annotated data, and model interactions, analyzing each type and discussing their implications.", "result": "The survey identifies key challenges in managing conflicting information and proposes mitigation strategies to develop conflict-aware NLP systems.", "conclusion": "Addressing conflicting information is crucial for the reliability of NLP models, and future research should focus on unifying mitigation strategies.", "key_contributions": ["Unified categorization of conflicts in NLP models", "Analysis of implications of conflicting information", "Proposed strategies for conflict-aware NLP systems"], "limitations": "The survey does not provide empirical results or case studies to validate the proposed strategies.", "future_work": "Future research should explore more effective methods to reason over and reconcile conflicting information in NLP systems.", "keywords": ["NLP", "conflicting information", "reliability", "trustworthiness", "mitigation strategies"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556.pdf", "abs": "https://arxiv.org/abs/2504.19556", "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "authors": ["Kristen Sussman", "Daniel Carter"], "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns.", "AI": {"tldr": "This study analyzes the impact of AI-mediated communication on language patterns in tweets mentioning Donald Trump from 2020 and 2024, revealing increased sentiment positivity and changes in text complexity.", "motivation": "To understand how AI influences linguistic patterns in social media communication over time.", "method": "The study compares 970,919 tweets from 2020 with 20,000 tweets from 2024, analyzing changes in text complexity and sentiment using Flesch-Kincaid readability and polarity scores.", "result": "The analysis shows a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%).", "conclusion": "The findings indicate a greater presence of AI in social media communication and its effect on language and emotional expression.", "key_contributions": ["Demonstrated the influence of AI on linguistic changes on social media.", "Provided quantifiable data on sentiment shifts before and after AI emergence in communication.", "Highlighted the changing dynamics of emotional expression in political discourse on social media."], "limitations": "", "future_work": "", "keywords": ["AI-mediated communication", "social media analysis", "sentiment analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565.pdf", "abs": "https://arxiv.org/abs/2504.19565", "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training.", "AI": {"tldr": "The paper proposes a multi-agent framework for distilling biomedical corpora tailored for LLMs, enhancing their performance in biomedical question-answering tasks.", "motivation": "Existing annotated scientific corpora are inadequate for training LLMs in biomedical research due to quality and quantity limitations.", "method": "A collaborative multi-agent architecture where specialized agents, guided by the MeSH hierarchy, autonomously extract and synthesize data from scientific literature to generate question-answer pairs.", "result": "Language models trained on the proposed distilled datasets significantly outperform existing LLM baselines in biomedical question-answering tasks.", "conclusion": "The study demonstrates the effectiveness of multi-agent collaboration in improving biomedical LLM training results.", "key_contributions": ["Introduction of a multi-agent framework for biomedical corpus distillation", "Development of an AI-Ready dataset that enhances LLM training outcomes", "Validation of the framework's effectiveness through comprehensive experimental results"], "limitations": "", "future_work": "", "keywords": ["Large Language Model", "Agentic AI", "Dataset Distillation", "Multi-agent Collaboration"], "importance_score": 9, "read_time_minutes": 22}}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590.pdf", "abs": "https://arxiv.org/abs/2504.19590", "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "authors": ["Israa Alsiyat"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor.", "AI": {"tldr": "This paper presents a new approach for sentiment classification of the Arabic Metaphor Corpus using semantic tags.", "motivation": "To explore the impact of Arabic online metaphors on sentiment classification and to introduce a novel tool for this purpose.", "method": "The approach involves the development of an automatic tool that applies semantic emotional tags for sentiment classification, evaluated with standard metrics like F-score, recall, and precision.", "result": "The evaluation demonstrated the effectiveness of the tool in classifying sentiment related to Arabic metaphors, providing a new perspective on metaphor analysis in sentiment.", "conclusion": "This approach represents the first sentiment classification for Arabic metaphors using semantic tagging, showing potential for future research in the area.", "key_contributions": ["Introduction of a sentiment classification tool for Arabic metaphors", "Application of semantic emotional tags", "Evaluation using standard classification metrics"], "limitations": "Limited scope as it focuses solely on Arabic metaphors and may not generalize to other languages or metaphor types.", "future_work": "Further exploration of sentiment classification methods for different languages and metaphor types, as well as enhancement of the tool's capabilities.", "keywords": ["Sentiment classification", "Arabic Metaphor Corpus", "Semantic tags"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606.pdf", "abs": "https://arxiv.org/abs/2504.19606", "title": "Coreference Resolution for Vietnamese Narrative Texts", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese.", "AI": {"tldr": "This paper addresses coreference resolution in Vietnamese using a newly developed dataset and evaluates LLM performance for this task.", "motivation": "Coreference resolution is crucial in NLP, especially for low-resource languages like Vietnamese where annotated datasets are scarce.", "method": "Creation of a comprehensive annotated dataset from narrative texts of VnExpress; detailed annotation guidelines were established. Performance of LLMs (GPT-3.5-Turbo and GPT-4) evaluated on this dataset.", "result": "GPT-4 significantly outperformed GPT-3.5-Turbo in accuracy and response consistency for coreference resolution in Vietnamese.", "conclusion": "GPT-4 is a more reliable tool for coreference resolution in Vietnamese compared to GPT-3.5-Turbo.", "key_contributions": ["Development of an annotated dataset for coreference resolution in Vietnamese.", "Evaluation of LLM performance on the dataset with findings favoring GPT-4.", "Establishment of annotation guidelines for entity consistency and accuracy."], "limitations": "Limited to one language (Vietnamese) and its specific context, potential generalizability issues to other languages or datasets.", "future_work": "Further exploration of coreference resolution in other low-resource languages and improvement of dataset diversity.", "keywords": ["coreference resolution", "Vietnamese", "large language models", "NLP", "annotated dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627.pdf", "abs": "https://arxiv.org/abs/2504.19627", "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM.", "AI": {"tldr": "VCM is a self-supervised framework for visual concept modeling that improves efficiency in LVLMs by reducing computational costs while maintaining performance.", "motivation": "Current LVLMs are inefficient as they process full images at the token level, unlike humans who analyze conceptual information. Improving this efficiency is crucial for real-world applications.", "method": "VCM employs an end-to-end self-supervised learning approach, utilizing implicit contrastive learning and vision-language fine-tuning to build a visual concept model without costly annotations.", "result": "VCM demonstrates a significant reduction in computational costs (85% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance in various image understanding tasks.", "conclusion": "The proposed VCM framework enhances the capabilities of visual encoders in perception tasks and validates its effectiveness through extensive experiments.", "key_contributions": ["Introduction of VCM as a novel visual concept modeling framework", "Demonstrated 85% reduction in computational costs for LLaVA-1.5-7B", "Enhanced performance in classic visual concept perception tasks"], "limitations": "", "future_work": "Further explorations on improving visual concept extraction and applying VCM to more real-world tasks.", "keywords": ["Vision-Language Models", "self-supervised learning", "visual concept modeling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645.pdf", "abs": "https://arxiv.org/abs/2504.19645", "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks.", "AI": {"tldr": "This paper proposes a comprehensive and standardized Part-of-Speech (POS) tagset for the low-resourced Central-Kurdish language (CKL) to enhance NLP tasks performance.", "motivation": "To address the lack of resources and standardized POS tagging for low-resourced languages like Central-Kurdish, which hinders the development of NLP applications.", "method": "The study involved gathering various POS tags from existing literature and Kurdish linguistic experts to create a comprehensive POS tagset specifically for CKL. It also included an initial comparison with the Universal Dependencies framework.", "result": "The proposed POS tagset improved the accuracy of CKL NLP tasks and provided a standardized way of annotating a large CKL corpus which can benefit machine translation and text recommendation.", "conclusion": "A standardized POS tagset for CKL can significantly enhance the performance of various NLP applications and facilitate further development in the field for this language.", "key_contributions": ["Development of a comprehensive POS tagset for Central-Kurdish", "Standardization of part-of-speech tags from expert consultations", "Initial performance evaluation against the Universal Dependencies framework"], "limitations": "The paper primarily focuses on POS tagging and does not address other NLP aspects such as machine translation or speech recognition for CKL.", "future_work": "Further studies may enhance NLP applications using the developed POS tagset and explore additional resources for other low-resourced languages.", "keywords": ["Natural Language Processing", "Part-of-Speech tagging", "Central-Kurdish language", "Standardization", "NLP resources"], "importance_score": 4, "read_time_minutes": 25}}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669.pdf", "abs": "https://arxiv.org/abs/2504.19669", "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance.", "AI": {"tldr": "Proposes a multimodal conditioned diffusion model for time series forecasting, leveraging timestamps and text for enhanced predictive accuracy.", "motivation": "To address the limitations of existing diffusion-based approaches in time series forecasting by utilizing rich multimodal information.", "method": "Introduces a multimodal conditioned diffusion model (MCD-TSF) that combines timestamps and texts as guidance for time series modeling.", "result": "The MCD-TSF model outperforms existing methods on real-world benchmark datasets across eight domains, achieving state-of-the-art performance.", "conclusion": "Leveraging multimodal information significantly improves time series forecasting accuracy and opens new avenues for research in this area.", "key_contributions": ["Introduces the MCD-TSF model for time series forecasting using multimodal data.", "Demonstrates superior performance on benchmark datasets compared to existing approaches.", "Establishes temporal and semantic correlations to enhance model predictions."], "limitations": "", "future_work": "Future research can explore further multimodal integrations and applications of the MCD-TSF model.", "keywords": ["multimodal", "time series forecasting", "diffusion model", "MCD-TSF", "machine learning"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675.pdf", "abs": "https://arxiv.org/abs/2504.19675", "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts.", "AI": {"tldr": "The paper presents the Annif system for subject indexing using large language models in SemEval-2025, achieving top rankings in multiple evaluation categories.", "motivation": "To improve subject indexing for multilingual bibliographic records using advanced techniques in natural language processing and machine learning.", "method": "The approach utilizes the Annif toolkit, integrating traditional NLP and ML techniques with LLM-based methods for translation, synthetic data generation, and merging predictions from monolingual models.", "result": "The Annif system ranked first in the all-subjects category, second in the tib-core-subjects category during quantitative evaluation, and fourth in qualitative evaluations.", "conclusion": "Combining traditional XMTC algorithms with modern LLM techniques can significantly enhance the accuracy and efficiency of subject indexing in multilingual contexts.", "key_contributions": ["Introduction of the Annif system leveraging LLMs for subject indexing.", "Top rankings in subject prediction categories showcase its effectiveness.", "Demonstration of improved accuracy in multilingual subject indexing."], "limitations": "", "future_work": "Exploration of further enhancements in LLM integration for indexing tasks.", "keywords": ["Subject Indexing", "Large Language Models", "Natural Language Processing", "Machine Learning", "Multilingual Contexts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720.pdf", "abs": "https://arxiv.org/abs/2504.19720", "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving.", "AI": {"tldr": "This paper surveys advancements in optimizing Large Language Model (LLM) inference services, addressing memory overhead and computational challenges, including instance-level and cluster-level strategies.", "motivation": "To tackle the significant memory and computational challenges posed by LLMs for Generative AI, ensuring low latency and high throughput for inference services.", "method": "The paper reviews instance-level approaches like model placement and request scheduling, and cluster-level strategies such as deployment and load balancing, along with emerging scenarios and critical areas in LLM inference.", "result": "The survey highlights various optimization strategies and frameworks, providing a comprehensive overview of both established and emerging methods in LLM inference.", "conclusion": "The insights gathered in this survey advance the understanding and improvement of LLM inference serving, with suggested future research directions to address existing limitations.", "key_contributions": ["Comprehensive review of instance-level and cluster-level optimization strategies for LLM inference.", "Identification of emerging scenarios and critical areas relevant to LLM serving.", "Outlining future research directions to enhance LLM inference performance."], "limitations": "The paper is a work in progress and may not cover all recent advancements comprehensively.", "future_work": "Further exploration of specific tasks, modules, and auxiliary methods in LLM inference to refine and enhance services.", "keywords": ["Large Language Models", "Generative AI", "Inference optimization", "Memory overhead", "Computational demands"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734.pdf", "abs": "https://arxiv.org/abs/2504.19734", "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "authors": ["Ying Na", "Shihui Feng"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis.", "AI": {"tldr": "This study develops a novel LLM-assisted automated coding framework for dialogue data, addressing contextual challenges to improve accuracy in coding and analysis.", "motivation": "To improve the understanding of learning processes through dialogue data and enhance the accuracy of automated coding using LLMs.", "method": "Developed a framework that predicts codes for utterances based on dialogue-specific characteristics, utilizing multiple LLMs for collaborative code prediction and implementing consistency checking for contextual accuracy.", "result": "Substantial accuracy improvement in coding predictions, with findings indicating that act predictions are more accurate than event predictions.", "conclusion": "The proposed framework enhances the precision of automated coding of dialogue data and presents a scalable solution for tackling contextual challenges in dialogue analysis.", "key_contributions": ["Novel LLM-assisted automated coding approach for dialogue data", "Use of multiple LLMs for collaborative code prediction", "Implementation of consistency checking using GPT-4o to improve accuracy"], "limitations": "Inherent challenges of understanding complex contextual information remain persistent.", "future_work": "Further exploration of LLM capabilities for dialogue analysis and potential applications in other fields.", "keywords": ["Large Language Models", "automated coding", "dialogue analysis", "contextual challenges", "education research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759.pdf", "abs": "https://arxiv.org/abs/2504.19759", "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP.", "AI": {"tldr": "This paper presents the Multilingual Moral Reasoning Benchmark (MMRB) to assess moral reasoning in large language models across various languages and contextual complexities.", "motivation": "To evaluate and understand the moral reasoning capabilities of LLMs in different languages and complexity levels, especially focusing on low-resource languages.", "method": "The paper introduces the MMRB and tests LLMs on moral reasoning tasks across three context levels (sentence, paragraph, document) in five languages. The LLaMA-3-8B model is fine-tuned with curated monolingual data.", "result": "Results indicate that moral reasoning performance declines with increasing context complexity, with low-resource languages like Vietnamese demonstrating significant challenges.", "conclusion": "The findings emphasize the greater impact that low-resource languages have on multilingual reasoning and underline their vital importance in the field of multilingual NLP.", "key_contributions": ["Development of the Multilingual Moral Reasoning Benchmark (MMRB).", "Insights on the decline of moral reasoning performance with increasing contextual complexity.", "Evidence of critical roles played by low-resource languages in multilingual NLP."], "limitations": "The focus on specific languages and the reliance on multilingual datasets may not represent all possible scenarios in moral reasoning tasks.", "future_work": "Future research could explore more diverse languages, improve the robustness of LLMs in low-resource contexts, and enhance the benchmarking methodologies.", "keywords": ["moral reasoning", "multilingual NLP", "large language models", "low-resource languages", "benchmark"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811.pdf", "abs": "https://arxiv.org/abs/2504.19811", "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.", "AI": {"tldr": "The paper introduces a Lineage-Regularized Matrix Factorization (LRMF) framework for forecasting LLM performance by incorporating ancestral ties among models, outperforming previous methods.", "motivation": "The need to accurately predict LLM performance to reduce computational costs and development time, particularly considering lineage relationships among models.", "method": "A novel LRMF framework using a graph Laplacian regularizer that encodes ancestral ties among LLMs and leverages multi-hop parent-child connections.", "result": "LRMF demonstrates consistent performance improvements, achieving 7-10 percentage points higher correlation with actual performance compared to conventional methods in a study of 2,934 models and 21,000+ instances.", "conclusion": "Lineage constraints effectively improve performance predictions and address cold-start problems, offering a resource-efficient strategy for LLM development.", "key_contributions": ["Introduction of the LRMF framework", "Use of a graph Laplacian regularizer to encode ancestral ties", "Improved performance prediction for cold-start models"], "limitations": "", "future_work": "Exploration of additional models and benchmarks to further validate the lineage-guided prediction approach.", "keywords": ["Large Language Models", "performance prediction", "lineage relationships", "matrix factorization", "cold-start problem"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850.pdf", "abs": "https://arxiv.org/abs/2504.19850", "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT", "AI": {"tldr": "This pilot study examines how translation modality affects cognitive load in readers, focusing on creativity and errors in different translation types.", "motivation": "To understand the effects of creativity and errors in translation modalities on reader cognitive load.", "method": "A pilot study involving eight participants who filled in a questionnaire, read a short story using an eye-tracker, and conducted retrospective think-aloud interviews under four translation conditions: machine translation, post-editing, human translation, and original source text.", "result": "Higher units of creative potential increase cognitive load, with human translation showing the highest cognitive load and machine translation the lowest; no effect of error was observed on cognitive load.", "conclusion": "Translation creativity impacts cognitive load, potentially linking higher cognitive load with increased reader enjoyment and immersion.", "key_contributions": ["Introduction of units of creative potential (UCP) in translation studies.", "Novel findings on the relationship between translation creativity and cognitive load.", "Availability of study code and data for further research."], "limitations": "Small sample size of eight participants may limit the generalizability of the results.", "future_work": "Further research into translation creativity effects on other reader experiences and potential applications in translation practices.", "keywords": ["translation modalities", "cognitive load", "creativity", "machine translation", "reader immersion"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856.pdf", "abs": "https://arxiv.org/abs/2504.19856", "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments.", "AI": {"tldr": "Introducing ICL-APT, a method leveraging in-context learning and kNN to enhance domain-adaptive continual pretraining while reducing GPU time and maintaining model performance.", "motivation": "The paper addresses the challenge of obtaining domain-related data for DAPT in languages other than English, specifically in the German language for the process industry.", "method": "ICL-APT utilizes in-context learning and k-nearest neighbors to augment target data with domain-specific texts, improving training efficiency while reducing computational demands.", "result": "The ICL-APT approach outperforms traditional DAPT by 3.5 average IR metrics (mAP, MRR, and nDCG) and requires nearly 4 times less computing time.", "conclusion": "ICL-APT provides a cost-effective solution for low-resource industries, making NLP-based solutions more accessible and feasible.", "key_contributions": ["Development of ICL-APT technique", "Demonstration of significant performance improvement over traditional DAPT", "Reduction in computational time for training"], "limitations": "", "future_work": "Exploration of ICL-APT's applicability to other low-resource industries beyond the process industry.", "keywords": ["domain-adaptive continual pretraining", "in-context learning", "k-nearest neighbors", "NLP", "low-resource industries"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867.pdf", "abs": "https://arxiv.org/abs/2504.19867", "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.", "AI": {"tldr": "This paper introduces semi-PD, a novel LLM serving system that utilizes disaggregated computation with unified storage to improve serving performance, particularly under high request rates.", "motivation": "Existing LLM serving systems suffer from storage inefficiencies and latency issues, especially in disaggregated systems.", "method": "The paper proposes a new architecture, semi-PD, which employs a computation resource controller and a unified memory manager for better resource management and performance optimization.", "result": "semi-PD reduces average end-to-end latency per request by 1.27-2.58 times on DeepSeek models, and increases request handling capacity by 1.55-1.72 times on Llama models while maintaining latency constraints.", "conclusion": "The semi-PD system significantly enhances performance for LLM serving, mitigating the common storage and latency issues associated with existing approaches.", "key_contributions": ["Introduction of semi-PD system characterized by disaggregated computation and unified storage", "Development of a low-overhead resource adjustment mechanism", "Implementation of an SLO-aware dynamic partitioning algorithm"], "limitations": "", "future_work": "", "keywords": ["large language models", "disaggregated computation", "unified storage", "resource management", "latency optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898.pdf", "abs": "https://arxiv.org/abs/2504.19898", "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications.", "AI": {"tldr": "GenCLS++ is a framework for generative text classification that optimizes supervised fine-tuning (SFT) and reinforcement learning (RL) while exploring various strategies, leading to significant accuracy improvements over traditional methods.", "motivation": "To enhance the capabilities of Large Language Models (LLMs) in text classification by addressing shortcomings in traditional discriminative methods and leveraging the strengths of generative approaches.", "method": "The authors propose GenCLS++, a framework that jointly optimizes SFT and RL while exploring five high-level strategy dimensions during both training and inference stages.", "result": "GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline across seven datasets, with an increase to 4.00% on public datasets.", "conclusion": "The findings indicate that classification tasks do not benefit from explicit reasoning processes, offering new insights for future LLM applications.", "key_contributions": ["Introduction of GenCLS++, a unified framework for generative text classification combining SFT and RL.", "Systematic exploration of five high-level strategy dimensions for improved classification performance.", "Empirical evidence showing that classification tasks perform better without reasoning steps, contrasting with reasoning-intensive tasks."], "limitations": "Further exploration into other potential dimensions and strategies in RL and generative classification is needed.", "future_work": "Investigation into alternative training methodologies and their effects on generative classification tasks.", "keywords": ["text classification", "generative models", "reinforcement learning", "large language models", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940.pdf", "abs": "https://arxiv.org/abs/2504.19940", "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems.", "AI": {"tldr": "This paper explores the use of LLM-powered generative agents in crowdsourced fact-checking, demonstrating their superiority over human crowds in truthfulness classification and bias reduction.", "motivation": "The increase in online misinformation necessitates effective, scalable fact-checking solutions. Crowdsourced methods are gaining popularity despite concerns about quality and bias.", "method": "The study simulates crowds composed of generative agents with varied demographic and ideological backgrounds, which evaluate claims and retrieve evidence for fact-checking tasks.", "result": "Generative agents outperform human crowds in truthfulness classification, show higher consistency, and are less biased, using more structured criteria such as Accuracy and Precision.", "conclusion": "Generative agents can be valuable contributors to fact-checking, providing scalability and reduced bias in crowd-based systems.", "key_contributions": ["Demonstration of generative agents enhancing fact-checking tasks.", "Evidence of superior performance of agents over humans in veracity assessments.", "Identification of structured decision-making processes in agents."], "limitations": "The study mainly focuses on simulations rather than real-world applications, limiting its generalizability.", "future_work": "Explore the real-world implementation of LLM-powered agents in fact-checking processes and their integration with existing systems.", "keywords": ["fact-checking", "crowdsourcing", "generative agents", "machine learning", "bias reduction"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982.pdf", "abs": "https://arxiv.org/abs/2504.19982", "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.", "AI": {"tldr": "This paper introduces TD-EVAL, a two-step evaluation framework for task-oriented dialogue systems that combines turn-level analysis and dialogue-level comparisons for improved evaluation accuracy.", "motivation": "The motivation behind this work is the need for better evaluation methodologies for task-oriented dialogue systems as they evolve with the increasing capabilities of Large Language Models, which traditional metrics do not adequately address.", "method": "The paper proposes TD-EVAL, which evaluates dialogue interactions in two steps: first through fine-grained analysis of individual responses using three dimensions, and second through holistic comparisons of dialogues using pairwise assessments.", "result": "Experiments on MultiWOZ 2.4 and τ-Bench show that TD-EVAL identifies critical conversational errors missed by conventional metrics and aligns better with human judgments.", "conclusion": "TD-EVAL presents a new approach for evaluating task-oriented dialogue systems, balancing both the granularity of turn analysis and the holistic view of dialogue quality, offering a versatile framework for future research.", "key_contributions": ["Introduction of TD-EVAL framework for dialogue evaluation", "Fine-grained turn-level analysis alongside holistic assessments", "Demonstrated alignment with human judgment compared to existing metrics"], "limitations": "", "future_work": "Future research can explore applying TD-EVAL in different contexts and integrating it with varying dialogue systems to further validate its applicability and robustness.", "keywords": ["Task-oriented dialogue", "Evaluation framework", "Large Language Models", "Dialogue systems", "Human judgments"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000.pdf", "abs": "https://arxiv.org/abs/2504.20000", "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models.", "AI": {"tldr": "This paper investigates knowledge distillation (KD) in telecom domain QA tasks, focusing on the impact of fine-tuning teacher and student LLMs and vocabulary on model performance.", "motivation": "To understand the effects of domain adaptation through Knowledge Distillation in large language models, particularly in the telecom sector's Question-Answering applications.", "method": "The study conducts experiments comparing Supervised Fine-tuning (SFT) of teacher models, student models, and both, while assessing different vocabularies and KD algorithms (vanilla KD and Dual Space KD).", "result": "The results indicate that fine-tuning the teacher model enhances the performance of the distilled model with the same vocabulary; however, using both teacher and student models for fine-tuning consistently yields better outcomes across all evaluation metrics.", "conclusion": "SFT of both teacher and student models improves performance overall, with the statistical significance influenced by the vocabulary used in the teacher models.", "key_contributions": ["Systematic exploration of fine-tuning strategies for KD in QA tasks.", "Multi-metric evaluation of model performance post-distillation.", "Insights on the influence of vocabulary on the effectiveness of KD."], "limitations": "The study is limited to telecom domain applications, and results might vary with different domains or tasks beyond QA.", "future_work": "Future research can explore additional domains and tasks, as well as variations in KD algorithms and their interactions with different model architectures.", "keywords": ["Knowledge Distillation", "Large Language Models", "Supervised Fine-tuning", "Question-Answering", "Telecom"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013.pdf", "abs": "https://arxiv.org/abs/2504.20013", "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.", "AI": {"tldr": "The study explores the impact of LLM-generated fake news on neural news recommendation systems, revealing a phenomenon termed 'truth decay.'", "motivation": "To understand the challenges posed by LLM-generated fake news on the news ecosystem and its moderation.", "method": "Developed a simulation pipeline and a dataset containing approximately 56,000 generated news articles of various types.", "result": "Real news rankings deteriorate ('truth decay') as LLM-generated news infiltrates news recommendation systems, showing a correlation between perplexity and news ranking.", "conclusion": "LLM-generated fake news poses significant threats to the integrity of news ecosystems, necessitating proactive measures from stakeholders.", "key_contributions": ["Development of a simulation pipeline to analyze LLM-specific impacts on news ecosystems.", "Identification of 'truth decay' in news rankings caused by LLM-generated news.", "Correlation findings between news perplexity and its ranking in recommendation systems."], "limitations": "The study primarily focuses on simulated environments; real-world implications may vary.", "future_work": "Investigation of additional countermeasures against LLM-generated fake news and their efficacy in real-world applications.", "keywords": ["fake news", "large language models", "news recommendation", "truth decay", "news ecosystem"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022.pdf", "abs": "https://arxiv.org/abs/2504.20022", "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs.", "AI": {"tldr": "The study assesses the factual accuracy of multilingual LLMs in English and 19 Indic languages using the IndicQuest dataset, finding better performance in English and increased hallucination in Indic responses.", "motivation": "To investigate the factual accuracy of LLMs in low-resource Indic languages compared to high-resource English, as existing studies have primarily focused on the latter.", "method": "The paper compares the performance of various LLMs (GPT-4o, Gemma-2-9B, Gemma-2-2B, Llama-3.1-8B) on the IndicQuest dataset, which contains question-answer pairs in both English and Indic languages, analyzing responses to the same questions in both language contexts.", "result": "LLMs generally perform better in English than in Indic languages and show a higher tendency for hallucination in responses generated in low-resource languages.", "conclusion": "The findings indicate that current LLMs struggle with factual accuracy in low-resource languages, presenting challenges in multilingual understanding and contextual accuracy.", "key_contributions": ["Assessment of factual accuracy for LLMs across multiple languages including low-resource Indic languages", "Identification of a performance gap in LLMs between English and Indic contexts", "Highlighting the problem of hallucination in language model responses for low-resource languages"], "limitations": "The study is limited to only a subset of LLMs and does not explore more extensive datasets or a wider range of low-resource languages.", "future_work": "Future research could expand the dataset to include more low-resource languages and explore improvements in LLM training for better multilingual support.", "keywords": ["Multilingual LLMs", "Factual accuracy", "Indic languages", "Hallucination", "Language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039.pdf", "abs": "https://arxiv.org/abs/2504.20039", "title": "AutoJudge: Judge Decoding Without Manual Annotation", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks.", "AI": {"tldr": "AutoJudge is a framework for accelerating LLM inference by using task-specific lossy speculative decoding, allowing for faster generation of 'unimportant' tokens while maintaining quality.", "motivation": "To improve the speed of large language model inference by differentiating between important and unimportant tokens in the output.", "method": "The framework utilizes a semi-greedy search algorithm and a lightweight classifier to predict which tokens can be safely accepted or skipped during inference.", "result": "AutoJudge achieved up to 1.5x more accepted tokens per verification cycle in zero-shot GSM8K reasoning with under 1% accuracy degradation, and over 2x speedup with small accuracy loss in programming tasks on the LiveCodeBench benchmark.", "conclusion": "The approach demonstrates significant performance improvements in LLM inference speed while maintaining high accuracy and generalizing well across different tasks.", "key_contributions": ["Introduction of task-specific lossy speculative decoding for LLMs.", "Development of a semi-greedy search algorithm for token acceptance.", "Creation of a lightweight classifier based on LLM embeddings for inference optimization."], "limitations": "The performance may vary with different model architectures and tasks; further validation is needed.", "future_work": "Exploration of broader applications of the AutoJudge framework and improvements on other model architectures.", "keywords": ["AutoJudge", "speculative decoding", "LLM inference", "accuracy", "token generation"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919.pdf", "abs": "https://arxiv.org/abs/2504.18919", "title": "Clinical knowledge in LLMs does not translate to human interactions", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare.", "AI": {"tldr": "This study evaluates the effectiveness of large language models (LLMs) in assisting the public with medical advice, revealing significant gaps between LLM performance and user interaction outcomes.", "motivation": "To explore the effectiveness of large language models in providing accurate medical advice to the public, considering the gap between exam performance and real-world application.", "method": "A controlled study with 1,298 participants tested LLMs against a control group in identifying medical conditions and recommended actions across ten scenarios.", "result": "LLMs accurately identified conditions in 94.9% of cases and disposition in 56.3% when tested alone; however, user interaction resulted in less than 34.5% accuracy for conditions and 44.2% for dispositions.", "conclusion": "The study highlights the challenge of user interactions in deploying LLMs for medical advice and stresses the need for systematic human user testing before public deployment.", "key_contributions": ["Demonstrated that LLMs perform well in controlled environments but not in user interactions.", "Identified significant accuracy gaps in user interaction with LLMs compared to LLMs operating independently.", "Recommended a shift towards human user testing to better evaluate LLMs' interactive capabilities."], "limitations": "The findings suggest limitations in how LLMs interact with users, which were not predicted by standard benchmarks for medical knowledge.", "future_work": "Future research should focus on systematic human user testing and improving LLM interfaces for better user interactions in healthcare settings.", "keywords": ["large language models", "medical advice", "user interaction", "human-computer interaction", "healthcare applications"], "importance_score": 9, "read_time_minutes": 52}}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988.pdf", "abs": "https://arxiv.org/abs/2504.18988", "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research.", "AI": {"tldr": "The paper presents LINC, a multimodal system designed to facilitate collaboration among ESL researchers by improving communication and comprehension in multilingual meetings.", "motivation": "To address communication barriers faced by ESL researchers in collaborative environments due to language differences, which limit their participation and contributions.", "method": "A survey was conducted with 64 ESL researchers to identify design goals, leading to the development of the LINC system, which includes a real-time multilingual communication module and a post-meeting dashboard. The system's effectiveness was evaluated with a study involving six multilingual teams.", "result": "Participants using LINC were able to communicate in their preferred languages, recall actionable insights from discussions, and prepare more effectively for meetings, indicating improved participation and comprehension.", "conclusion": "The use of multimodal systems like LINC can significantly enhance the collaboration experience for multilingual teams in research contexts, addressing language barriers and promoting participation.", "key_contributions": ["Identification of key design goals for multilingual collaboration", "Development and evaluation of the LINC system", "Insights into external factors affecting participation in multilingual meetings"], "limitations": "The study may be limited by the small sample size and specific context of ESL researchers, which may not generalize to all collaborative settings.", "future_work": "Investigating additional multimodal tools and methods to further improve participation and outcomes in multilingual collaborative environments.", "keywords": ["multilingual collaboration", "LINC", "ESL researchers"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2104.02496", "pdf": "https://arxiv.org/pdf/2104.02496.pdf", "abs": "https://arxiv.org/abs/2104.02496", "title": "A Bayesian approach to modeling topic-metadata relationships", "authors": ["P. Schulze", "S. Wiegrebe", "P. W. Thurner", "C. Heumann", "M. Aßenmacher"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "13 pages, 1 table, 5 figures", "summary": "The objective of advanced topic modeling is not only to explore latent\ntopical structures, but also to estimate relationships between the discovered\ntopics and theoretically relevant metadata. Methods used to estimate such\nrelationships must take into account that the topical structure is not directly\nobserved, but instead being estimated itself in an unsupervised fashion,\nusually by common topic models. A frequently used procedure to achieve this is\nthe method of composition, a Monte Carlo sampling technique performing multiple\nrepeated linear regressions of sampled topic proportions on metadata\ncovariates. In this paper, we propose two modifications of this approach:\nFirst, we substantially refine the existing implementation of the method of\ncomposition from the R package stm by replacing linear regression with the more\nappropriate Beta regression. Second, we provide a fundamental enhancement of\nthe entire estimation framework by substituting the current blending of\nfrequentist and Bayesian methods with a fully Bayesian approach. This allows\nfor a more appropriate quantification of uncertainty. We illustrate our\nimproved methodology by investigating relationships between Twitter posts by\nGerman parliamentarians and different metadata covariates related to their\nelectoral districts, using the Structural Topic Model to estimate topic\nproportions.", "AI": {"tldr": "This paper improves topic modeling by refining the method of composition and adopting a fully Bayesian approach for estimating relationships between topics and metadata.", "motivation": "To enhance the estimation of relationships between latent topic structures and relevant metadata in topic modeling.", "method": "The paper refines the method of composition by replacing linear regression with Beta regression and adopts a fully Bayesian framework for stability and uncertainty quantification.", "result": "The proposed methodology demonstrates improved estimation of topic proportions in relation to metadata, specifically applied to Twitter posts by German parliamentarians.", "conclusion": "The enhancements lead to better interpretation of relationships in topic modeling, providing a clearer understanding of how topics relate to covariates.", "key_contributions": ["Refinement of the method of composition by using Beta regression instead of linear regression.", "Full Bayesian approach for improved estimation and uncertainty quantification in topic modeling."], "limitations": "", "future_work": "", "keywords": ["topic modeling", "Bayesian statistics", "social media analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2110.08420", "pdf": "https://arxiv.org/pdf/2110.08420.pdf", "abs": "https://arxiv.org/abs/2110.08420", "title": "Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information", "authors": ["Kawin Ethayarajh", "Yejin Choi", "Swabha Swayamdipta"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2022 (Outstanding Paper)", "summary": "Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. However, this comparison provides little\nunderstanding of how difficult each instance in a given distribution is, or\nwhat attributes make the dataset difficult for a given model. To address these\nquestions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as\nthe lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019),\nwhere a lower value indicates a more difficult dataset for $\\mathcal{V}$. We\nfurther introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for\nmeasuring the difficulty of individual instances w.r.t. a given distribution.\nWhile standard evaluation metrics typically only compare different models for\nthe same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also\npermit the converse: for a given model $\\mathcal{V}$, we can compare different\ndatasets, as well as different instances/slices of the same dataset.\nFurthermore, our framework allows for the interpretability of different input\nattributes via transformations of the input, which we use to discover\nannotation artefacts in widely-used NLP benchmarks.", "AI": {"tldr": "This paper introduces a framework to estimate dataset difficulty by analyzing model-specific usable information and individual instance difficulty, allowing for better understanding and interpretability.", "motivation": "To provide a deeper understanding of dataset difficulty beyond state-of-the-art comparisons, focusing on instance-level attributes for a given model.", "method": "The authors frame dataset difficulty in terms of $\textit{usable information}$ specific to a model, introducing pointwise $\textit{V}$-information (PVI) for assessing individual instances.", "result": "The framework enables the comparison of different datasets and instances for a given model, revealing insights into their difficulty and allowing for the interpretation of input attributes.", "conclusion": "The proposed methods enhance our understanding of dataset difficulties in NLP and enable discovery of potential annotation artefacts in existing benchmarks.", "key_contributions": ["Introduction of $\textit{usable information}$ for model-specific dataset difficulty estimation", "Development of pointwise $\textit{V}$-information (PVI) for individual instance analysis", "Framework allows for interpretability of attributes and insights into dataset characteristics."], "limitations": "The applicability of the framework may vary across different models and datasets not examined in this study.", "future_work": "Exploration of broader applications of the framework to other domains and model comparisons.", "keywords": ["Dataset Difficulty", "Usable Information", "Pointwise Information", "NLP", "Interpretability"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2112.06876", "pdf": "https://arxiv.org/pdf/2112.06876.pdf", "abs": "https://arxiv.org/abs/2112.06876", "title": "Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach", "authors": ["Eugene Yu Ji"], "categories": ["cs.CL"], "comment": "12 Pages; 3 figures", "summary": "In recent years, the field of NLP has seen growing interest in modeling both\nsemantic and pragmatic dimensions. Despite this progress, two key challenges\npersist: firstly, the complex task of mapping and analyzing the interactions\nbetween semantic and pragmatic features; secondly, the insufficient\nincorporation of relevant insights from related disciplines outside NLP.\nAddressing these issues, this study introduces a novel geometric metric that\nutilizes word co-occurrence patterns. This metric maps two fundamental\nproperties - semantic typicality (cognitive) and pragmatic salience\n(socio-cultural) - for basic-level categories within a two-dimensional\nhyperbolic space. Our evaluations reveal that this semantic-pragmatic metric\nproduces mappings for basic-level categories that not only surpass traditional\ncognitive semantics benchmarks but also demonstrate significant socio-cultural\nrelevance. This finding proposes that basic-level categories, traditionally\nviewed as semantics-driven cognitive constructs, should be examined through the\nlens of both semantic and pragmatic dimensions, highlighting their role as a\ncognitive-cultural interface. The broad contribution of this paper lies in the\ndevelopment of medium-sized, interpretable, and human-centric language\nembedding models, which can effectively blend semantic and pragmatic dimensions\nto elucidate both the cognitive and socio-cultural significance of linguistic\ncategories.", "AI": {"tldr": "This paper presents a novel geometric metric for analyzing semantic and pragmatic features in NLP using word co-occurrence patterns, demonstrating significant improvements in mapping basic-level categories pertinent to both cognitive and socio-cultural dimensions.", "motivation": "To address the challenges in modeling semantic and pragmatic dimensions in NLP, particularly the interaction between these features and the lack of interdisciplinary insights.", "method": "The study introduces a geometric metric that utilizes word co-occurrence patterns to map semantic typicality and pragmatic salience in a two-dimensional hyperbolic space.", "result": "The semantic-pragmatic metric developed produces superior mappings for basic-level categories, achieving better performance than traditional cognitive semantics benchmarks and showing strong socio-cultural relevance.", "conclusion": "The findings suggest that basic-level categories should be analyzed through both semantic and pragmatic lenses, emphasizing their dual role in cognitive and cultural contexts, leading to the development of interpretable, human-centric language embedding models.", "key_contributions": ["Novel geometric metric for semantic and pragmatic analysis", "Implementation of interpretable language embedding models", "Demonstration of cognitive-cultural interplay in linguistic categories"], "limitations": "", "future_work": "", "keywords": ["NLP", "Semantic Typicality", "Pragmatic Salience"], "importance_score": 6, "read_time_minutes": 12}}
{"id": "2305.01920", "pdf": "https://arxiv.org/pdf/2305.01920.pdf", "abs": "https://arxiv.org/abs/2305.01920", "title": "Generative Meta-Learning for Zero-Shot Relation Triplet Extraction", "authors": ["Wanli Li", "Tieyun Qian", "Yi Song", "Zeyu Zhang", "Jiawei Li", "Zhuang Chen", "Lixin Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation\ntriplets from texts containing unseen relation types. This capability benefits\nvarious downstream information retrieval (IR) tasks. The primary challenge lies\nin enabling models to generalize effectively to unseen relation categories.\nExisting approaches typically leverage the knowledge embedded in pre-trained\nlanguage models to accomplish the generalization process. However, these\nmethods focus solely on fitting the training data during training, without\nspecifically improving the model's generalization performance, resulting in\nlimited generalization capability. For this reason, we explore the integration\nof bi-level optimization (BLO) with pre-trained language models for learning\ngeneralized knowledge directly from the training data, and propose a generative\nmeta-learning framework which exploits the `learning-to-learn' ability of\nmeta-learning to boost the generalization capability of generative models.\n  Specifically, we introduce a BLO approach that simultaneously addresses data\nfitting and generalization. This is achieved by constructing an upper-level\nloss to focus on generalization and a lower-level loss to ensure accurate data\nfitting. Building on this, we subsequently develop three generative\nmeta-learning methods, each tailored to a distinct category of meta-learning.\nExtensive experimental results demonstrate that our framework performs well on\nthe ZeroRTE task. Our code is available at\nhttps://github.com/leeworry/TGM-MetaLearning.", "AI": {"tldr": "This paper proposes a generative meta-learning framework for Zero-shot Relation Triplet Extraction (ZeroRTE) that enhances generalization capabilities using bi-level optimization and pre-trained language models.", "motivation": "The motivation is to improve the generalization performance of models in ZeroRTE, which aims to extract relation triplets from texts with unseen relation types, leveraging the challenges posed by limited generalization capabilities in existing methods.", "method": "The paper introduces a bi-level optimization (BLO) approach combined with pre-trained language models, employing a generative meta-learning framework that focuses on two loss levels: an upper-level loss for generalization and a lower-level loss for data fitting.", "result": "Extensive experimental results show that the proposed framework significantly improves performance on the ZeroRTE task compared to existing methods.", "conclusion": "The proposed generative meta-learning framework effectively boosts the generalization capabilities of models in ZeroRTE, demonstrating better performance and providing a new approach for relation extraction tasks.", "key_contributions": ["Integration of bi-level optimization with pre-trained language models for improved generalization in ZeroRTE.", "Development of three generative meta-learning methods tailored to different categories of meta-learning.", "Comprehensive experimental results showcasing the framework's effectiveness in targeted tasks."], "limitations": "", "future_work": "Future research directions include exploring further enhancements in generalization techniques and applying the framework to additional natural language processing tasks.", "keywords": ["Zero-shot Relation Extraction", "bi-level optimization", "meta-learning", "generalization", "pre-trained language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2305.16326", "pdf": "https://arxiv.org/pdf/2305.16326.pdf", "abs": "https://arxiv.org/abs/2305.16326", "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations", "authors": ["Qingyu Chen", "Yan Hu", "Xueqing Peng", "Qianqian Xie", "Qiao Jin", "Aidan Gilson", "Maxwell B. Singer", "Xuguang Ai", "Po-Ting Lai", "Zhizheng Wang", "Vipina Kuttichi Keloth", "Kalpana Raja", "Jiming Huang", "Huan He", "Fongci Lin", "Jingcheng Du", "Rui Zhang", "W. Jim Zheng", "Ron A. Adelman", "Zhiyong Lu", "Hua Xu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "The rapid growth of biomedical literature poses challenges for manual\nknowledge curation and synthesis. Biomedical Natural Language Processing\n(BioNLP) automates the process. While Large Language Models (LLMs) have shown\npromise in general domains, their effectiveness in BioNLP tasks remains unclear\ndue to limited benchmarks and practical guidelines.\n  We perform a systematic evaluation of four LLMs, GPT and LLaMA\nrepresentatives on 12 BioNLP benchmarks across six applications. We compare\ntheir zero-shot, few-shot, and fine-tuning performance with traditional\nfine-tuning of BERT or BART models. We examine inconsistencies, missing\ninformation, hallucinations, and perform cost analysis. Here we show that\ntraditional fine-tuning outperforms zero or few shot LLMs in most tasks.\nHowever, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as\nmedical question answering. Open source LLMs still require fine-tuning to close\nperformance gaps. We find issues like missing information and hallucinations in\nLLM outputs. These results offer practical insights for applying LLMs in\nBioNLP.", "AI": {"tldr": "This paper evaluates the performance of four large language models (LLMs) in biomedical natural language processing (BioNLP) tasks and compares them to traditional fine-tuning approaches.", "motivation": "The rapid growth of biomedical literature requires effective automation for knowledge curation and synthesis, where BioNLP plays a critical role. Understanding the effectiveness of LLMs in this domain is essential for future applications.", "method": "A systematic evaluation of four LLMs (GPT and LLaMA), tested on 12 BioNLP benchmarks across six applications, focusing on zero-shot, few-shot, and fine-tuning performance compared to traditional models like BERT and BART.", "result": "Traditional fine-tuning methods outperform zero or few-shot approaches of LLMs in most tasks, but closed-source models like GPT-4 show superior performance in reasoning-related tasks. Open-source models need fine-tuning to perform comparably.", "conclusion": "While traditional models generally outperform LLMs in BioNLP tasks, LLMs like GPT-4 can excel in specific areas, highlighting the need for careful application and further improvement of open-source models.", "key_contributions": ["Systematic evaluation of LLMs in BioNLP tasks", "Comparison of LLM performance against traditional models", "Identification of issues like hallucinations in LLM outputs"], "limitations": "The evaluation is limited to specific LLMs and benchmarks, and results may not generalize across all BioNLP tasks or datasets.", "future_work": "Further investigations into improving open-source LLMs and addressing their limitations in BioNLP applications.", "keywords": ["Biomedical Natural Language Processing", "Large Language Models", "Knowledge Curation"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2309.15217", "pdf": "https://arxiv.org/pdf/2309.15217.pdf", "abs": "https://arxiv.org/abs/2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "categories": ["cs.CL"], "comment": "Reference-free (not tied to having ground truth available) evaluation\n  framework for retrieval agumented generation", "summary": "We introduce Ragas (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With Ragas, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.", "AI": {"tldr": "Ragas is a framework for evaluating Retrieval Augmented Generation (RAG) systems without relying on human annotations.", "motivation": "The paper addresses the challenges in evaluating RAG architectures which combine retrieval systems and LLMs, crucial for reducing hallucinations in generated content.", "method": "The authors propose a set of metrics to evaluate retrieval effectiveness, LLM reliance on retrieved passages, and the quality of the generated text, independent of human annotations.", "result": "Ragas offers a suite of reference-free evaluation metrics, enhancing the speed of evaluation cycles for RAG architectures.", "conclusion": "Implementing Ragas can significantly accelerate the evaluation of RAG architectures, addressing the need for faster assessments as LLM usage grows.", "key_contributions": ["Introduction of a reference-free evaluation framework for RAG", "Development of a suite of metrics for various evaluation dimensions", "Facilitation of faster evaluation cycles for RAG systems."], "limitations": "The framework's effectiveness in diverse real-world applications still needs further investigation.", "future_work": "Further refinement of the proposed metrics and exploration of their application across different types of RAG systems.", "keywords": ["Retrieval Augmented Generation", "Evaluation Framework", "LLM", "Reference-free Metrics", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.13056", "pdf": "https://arxiv.org/pdf/2405.13056.pdf", "abs": "https://arxiv.org/abs/2405.13056", "title": "Large language models for newspaper sentiment analysis during COVID-19: The Guardian", "authors": ["Rohitash Chandra", "Baicheng Zhu", "Qingying Fang", "Eka Shinjikashvili"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "During the COVID-19 pandemic, the news media coverage encompassed a wide\nrange of topics that includes viral transmission, allocation of medical\nresources, and government response measures. There have been studies on\nsentiment analysis of social media platforms during COVID-19 to understand the\npublic response given the rise of cases and government strategies implemented\nto control the spread of the virus. Sentiment analysis can provide a better\nunderstanding of changes in societal opinions and emotional trends during the\npandemic. Apart from social media, newspapers have played a vital role in the\ndissemination of information, including information from the government,\nexperts, and also the public about various topics. A study of sentiment\nanalysis of newspaper sources during COVID-19 for selected countries can give\nan overview of how the media covered the pandemic. In this study, we select The\nGuardian newspaper and provide a sentiment analysis during various stages of\nCOVID-19 that includes initial transmission, lockdowns and vaccination. We\nemploy novel large language models (LLMs) and refine them with expert-labelled\nsentiment analysis data. We also provide an analysis of sentiments experienced\npre-pandemic for comparison. The results indicate that during the early\npandemic stages, public sentiment prioritised urgent crisis response, later\nshifting focus to addressing the impact on health and the economy. In\ncomparison with related studies about social media sentiment analyses, we found\na discrepancy between The Guardian with dominance of negative sentiments (sad,\nannoyed, anxious and denial), suggesting that social media offers a more\ndiversified emotional reflection. We found a grim narrative in The Guardian\nwith overall dominance of negative sentiments, pre and during COVID-19 across\nnews sections including Australia, UK, World News, and Opinion", "AI": {"tldr": "This study conducts a sentiment analysis of The Guardian newspaper during COVID-19, revealing a predominance of negative sentiments and a focus shift from crisis response to health and economic impacts over time.", "motivation": "To understand how media coverage during COVID-19 reflected public sentiment and emotional trends, comparing it with social media sentiment analyses.", "method": "Sentiment analysis of articles from The Guardian newspaper during various COVID-19 stages, utilizing large language models refined with expert-labelled sentiment data.", "result": "The analysis showed a shift in public sentiment from urgent crisis response to concerns about health and the economy, with The Guardian exhibiting a predominance of negative sentiments.", "conclusion": "The study suggests that media narratives during the pandemic often conveyed a grim outlook, contrasting with more diversified sentiments observed in social media analyses.", "key_contributions": ["Conducted an in-depth sentiment analysis of a major newspaper during COVID-19.", "Utilized large language models for sentiment analysis, reflecting nuanced emotional trends.", "Provided a comparison of newspaper sentiment with social media, highlighting differences in emotional representation."], "limitations": "Focused solely on The Guardian newspaper and selected countries, which may limit generalizability.", "future_work": "Expanding sentiment analysis to include more diverse news sources and countries for broader insights into media coverage during health crises.", "keywords": ["COVID-19", "Sentiment Analysis", "Large Language Models", "Media Coverage", "Public Sentiment"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2407.02122", "pdf": "https://arxiv.org/pdf/2407.02122.pdf", "abs": "https://arxiv.org/abs/2407.02122", "title": "Fake News Detection: It's All in the Data!", "authors": ["Soveatin Kuntur", "Anna Wróblewska", "Marcin Paprzycki", "Maria Ganzha"], "categories": ["cs.CL"], "comment": null, "summary": "This comprehensive survey serves as an indispensable resource for researchers\nembarking on the journey of fake news detection. By highlighting the pivotal\nrole of dataset quality and diversity, it underscores the significance of these\nelements in the effectiveness and robustness of detection models. The survey\nmeticulously outlines the key features of datasets, various labeling systems\nemployed, and prevalent biases that can impact model performance. Additionally,\nit addresses critical ethical issues and best practices, offering a thorough\noverview of the current state of available datasets. Our contribution to this\nfield is further enriched by the provision of GitHub repository, which\nconsolidates publicly accessible datasets into a single, user-friendly portal.\nThis repository is designed to facilitate and stimulate further research and\ndevelopment efforts aimed at combating the pervasive issue of fake news.", "AI": {"tldr": "A comprehensive survey focusing on fake news detection, emphasizing dataset quality, diversity, and ethical considerations.", "motivation": "To provide a resource for researchers in fake news detection, highlighting the importance of dataset quality and diversity.", "method": "Survey of dataset features, labeling systems, biases, and ethical issues related to fake news detection.", "result": "Identified key factors affecting the robustness of detection models and presented a consolidated GitHub repository for datasets.", "conclusion": "The paper underscores the need for high-quality datasets to improve fake news detection efforts and offers a consolidated repository for researchers.", "key_contributions": ["Emphasizes dataset quality and diversity in fake news detection", "Documents various labeling systems and biases in datasets", "Provides a GitHub repository of publicly accessible datasets for research."], "limitations": "", "future_work": "Future research could explore innovative labeling systems and further development of detection models using high-quality datasets.", "keywords": ["fake news detection", "dataset quality", "ethical issues", "GitHub repository", "research support"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2408.02239", "pdf": "https://arxiv.org/pdf/2408.02239.pdf", "abs": "https://arxiv.org/abs/2408.02239", "title": "Pula: Training Large Language Models for Setswana", "authors": ["Nathan Brown", "Vukosi Marivate"], "categories": ["cs.CL"], "comment": "NAACL 2025. 10 pages, 5 tables, 1 figure", "summary": "In this work we present Pula, a suite of bilingual language models proficient\nin both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o\nand Gemini 1.5 Pro on English-Setswana translation tasks and achieve\nstate-of-the-art performance on Setswana reasoning tasks for their size. We\nrelease the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and\ntraining and evaluation code. Alongside Pula, we release the largest-ever\nSetswana text corpus, Marothodi, and the first comprehensive Setswana\ninstruction-tuning dataset, Medupi, consisting of reformatted datasets,\ntranslated corpora, and synthetic LLM-generated text. To accompany this data,\nwe release the code used for dataset construction, formatting, filtering, and\nscraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and\nGSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.", "AI": {"tldr": "Pula is a suite of bilingual language models for Setswana and English, achieving state-of-the-art performance on particular tasks and accompanied by the largest Setswana text corpus and various resources for further research.", "motivation": "The need for advanced bilingual models that can effectively handle Setswana and English to improve translation and reasoning tasks.", "method": "Development of bilingual language models Pula 1B, 3B, 8B, and 14B using efficient fine-tuning on a large Setswana text corpus.", "result": "Pula models outperform existing benchmarks on English-Setswana translation and state-of-the-art performance on Setswana reasoning tasks.", "conclusion": "The work contributes significant resources for future research in bilingual AI applications and enhances the understanding of Setswana language processing.", "key_contributions": ["Introduction of Pula models for Setswana and English.", "Release of the Marothodi corpus, the largest Setswana text corpus.", "Creation of the Medupi dataset for instruction tuning in Setswana."], "limitations": "Mainly focused on bilingual capabilities, which may limit generalizability to monolingual contexts.", "future_work": "Future research may explore further enhancements in language understanding and reasoning in bilingual contexts.", "keywords": ["bilingual language models", "Setswana", "language translation", "instruction-tuning", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.05906", "pdf": "https://arxiv.org/pdf/2408.05906.pdf", "abs": "https://arxiv.org/abs/2408.05906", "title": "AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising", "authors": ["Peinan Zhang", "Yusuke Sakai", "Masato Mita", "Hiroki Ouchi", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025", "summary": "With the increase in the fluency of ad texts automatically created by natural\nlanguage generation technology, there is high demand to verify the quality of\nthese creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation\nBenchmark by CyberAgent), the first public benchmark to evaluate ad texts from\nmultiple perspectives within practical advertising operations. Our\ncontributions are as follows: (i) Defining five tasks for evaluating the\nquality of ad texts, as well as building a Japanese dataset based on the\npractical operational experiences of building a Japanese dataset based on the\npractical operational experiences of advertising agencies, which are typically\nkept in-house. (ii) Validating the performance of existing pre-trained language\nmodels (PLMs) and human evaluators on the dataset. (iii) Analyzing the\ncharacteristics and providing challenges of the benchmark. The results show\nthat while PLMs have already reached practical usage level in several tasks,\nhumans still outperform in certain domains, implying that there is significant\nroom for improvement in this area.", "AI": {"tldr": "The paper presents AdTEC, a public benchmark for evaluating ad texts created by natural language generation, highlighting quality assessment tasks and performance comparisons between PLMs and human evaluators.", "motivation": "To address the increasing need for quality evaluation of automatically generated ad texts in real-world advertising contexts.", "method": "The authors define five tasks for evaluating ad text quality and create a Japanese dataset influenced by practical experiences from advertising agencies.", "result": "The evaluation shows that while pre-trained language models (PLMs) are effective in certain tasks, human evaluators still outperform them in specific areas, indicating potential improvements.", "conclusion": "The benchmark lays the groundwork for further advancements in evaluating the quality of automatically generated ad texts.", "key_contributions": ["Definition of five evaluation tasks for ad texts", "Creation of a practical Japanese dataset for evaluation", "Performance analysis of PLMs vs human evaluators"], "limitations": "The dataset is focused on the Japanese language and may not generalize to other languages or contexts.", "future_work": "Future research could explore adaptation of the benchmark for other languages and improvement strategies for PLMs.", "keywords": ["Natural Language Generation", "Ad Text Evaluation", "Pre-trained Language Models", "Japanese Dataset", "Advertising"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2408.08444", "pdf": "https://arxiv.org/pdf/2408.08444.pdf", "abs": "https://arxiv.org/abs/2408.08444", "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering", "authors": ["Jinming Nian", "Zhiyuan Peng", "Qifan Wang", "Yi Fang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In knowledge-intensive tasks such as open-domain question answering (OpenQA),\nlarge language models (LLMs) often struggle to generate factual answers,\nrelying solely on their internal (parametric) knowledge. To address this\nlimitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by\nretrieving relevant information from external sources, thereby positioning the\nretriever as a pivotal component. Although dense retrieval demonstrates\nstate-of-the-art performance, its training poses challenges due to the scarcity\nof ground-truth evidence, largely attributed to the high costs of human\nannotation. In this paper, we propose W-RAG, a method that draws weak training\nsignals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the\nretriever to prioritize passages that most benefit the task. Specifically, we\nrerank the top-$k$ passages retrieved via BM25 by assessing the probability\nthat the LLM will generate the correct answer for a question given each\npassage. The highest-ranking passages are then used as positive fine-tuning\nexamples for dense retrieval. We conduct comprehensive experiments across four\npublicly available OpenQA datasets to demonstrate that our approach enhances\nboth retrieval and OpenQA performance compared to baseline models, achieving\nresults comparable to models fine-tuned with human-labeled data.", "AI": {"tldr": "W-RAG enhances LLMs for open-domain question answering by fine-tuning the retrieval process based on task-specific signals, improving retrieval and answer generation performance with minimal human annotation.", "motivation": "To improve open-domain question answering (OpenQA) performance of LLMs by addressing their reliance on internal knowledge and enhancing retrieval with weak supervision from task data.", "method": "W-RAG uses weak training signals from the OpenQA task to fine-tune the retriever, reranking top passages based on their likelihood of enabling correct answer generation.", "result": "W-RAG achieves improved retrieval and OpenQA performance on four datasets, reaching results akin to human-labeled data fine-tuning while mitigating the need for expensive annotations.", "conclusion": "The proposed method effectively utilizes the link between question answering tasks and retrieval processes, showcasing that weak signals can enhance model performance significantly without extensive labeling efforts.", "key_contributions": ["Introduction of W-RAG for enhancing retrieval in OpenQA tasks", "Leveraging weak training signals from downstream tasks", "Demonstrating effectiveness across multiple datasets without heavy reliance on human annotation"], "limitations": "The reliance on the quality of passage retrieval and the potential need for further evaluation on diverse datasets or tasks beyond OpenQA.", "future_work": "Exploration of additional task-specific signals and further refinement of the retrieval approach in broader contexts.", "keywords": ["Retrieval-Augmented Generation", "Open-domain Question Answering", "Large Language Models", "Dense Retrieval", "Fine-tuning"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2410.08800", "pdf": "https://arxiv.org/pdf/2410.08800.pdf", "abs": "https://arxiv.org/abs/2410.08800", "title": "Data Processing for the OpenGPT-X Model Family", "authors": ["Nicolo' Brandizzi", "Hammam Abdelwahab", "Anirban Bhowmick", "Lennard Helmer", "Benny Jörg Stein", "Pavel Denisov", "Qasid Saleem", "Michael Fromm", "Mehdi Ali", "Richard Rutmann", "Farzad Naderi", "Mohamad Saif Agy", "Alexander Schwirjow", "Fabian Küch", "Luzian Hahn", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Dennis Wegener", "Nicolas Flores-Herr", "Joachim Köhler", "Johannes Leveling"], "categories": ["cs.CL", "H.3.1; I.2.7"], "comment": null, "summary": "This paper presents a comprehensive overview of the data preparation pipeline\ndeveloped for the OpenGPT-X project, a large-scale initiative aimed at creating\nopen and high-performance multilingual large language models (LLMs). The\nproject goal is to deliver models that cover all major European languages, with\na particular focus on real-world applications within the European Union. We\nexplain all data processing steps, starting with the data selection and\nrequirement definition to the preparation of the final datasets for model\ntraining. We distinguish between curated data and web data, as each of these\ncategories is handled by distinct pipelines, with curated data undergoing\nminimal filtering and web data requiring extensive filtering and deduplication.\nThis distinction guided the development of specialized algorithmic solutions\nfor both pipelines. In addition to describing the processing methodologies, we\nprovide an in-depth analysis of the datasets, increasing transparency and\nalignment with European data regulations. Finally, we share key insights and\nchallenges faced during the project, offering recommendations for future\nendeavors in large-scale multilingual data preparation for LLMs.", "AI": {"tldr": "Overview of the data preparation pipeline for the OpenGPT-X project focused on multilingual LLMs.", "motivation": "To create high-performance multilingual LLMs that serve real-world applications in major European languages, adhering to data regulations.", "method": "The paper outlines the data selection, requirement definition, and the distinct pipelines for curated and web data, detailing the filtering and deduplication processes.", "result": "An in-depth analysis of datasets highlighting transparency and alignment with European data regulations, along with specialized algorithmic solutions for both curated and web data pipelines.", "conclusion": "The findings provide insights into the challenges of large-scale multilingual dataset preparation and suggest future directions for similar projects.", "key_contributions": ["Comprehensive methodologies for handling and processing multilingual data.", "Analysis of datasets aligned with European regulations.", "Recommendations for data preparation in LLM projects."], "limitations": "The paper does not address specific algorithmic design aspects or the performance metrics of the LLMs derived from the datasets.", "future_work": "Further studies on enhancing data preparation techniques and addressing challenges in multilingual data contexts.", "keywords": ["data preparation", "multilingual LLMs", "European languages", "data processing pipeline", "OpenGPT-X"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.11325", "pdf": "https://arxiv.org/pdf/2410.11325.pdf", "abs": "https://arxiv.org/abs/2410.11325", "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling", "authors": ["Wenda Xu", "Rujun Han", "Zifeng Wang", "Long T. Le", "Dhruv Madeka", "Lei Li", "William Yang Wang", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL", "cs.AI"], "comment": "ICLR2025", "summary": "Recent advances in knowledge distillation (KD) have enabled smaller student\nmodels to approach the performance of larger teacher models. However, popular\nmethods such as supervised KD and on-policy KD, are adversely impacted by the\nknowledge gaps between teacher-student in practical scenarios. Supervised KD\nsuffers from a distribution mismatch between training with a static dataset and\ninference over final student-generated outputs. Conversely, on-policy KD, which\nuses student-generated samples for training, can suffer from low-quality\ntraining examples with which teacher models are not familiar, resulting in\ninaccurate teacher feedback. To address these limitations, we introduce\nSpeculative Knowledge Distillation (SKD), a novel approach that leverages\ncooperation between student and teacher models to generate high-quality\ntraining data on-the-fly while aligning with the student's inference-time\ndistribution. In SKD, the student proposes tokens, and the teacher replaces\npoorly ranked ones based on its own distribution, transferring high-quality\nknowledge adaptively. We evaluate SKD on various text generation tasks,\nincluding translation, summarization, math, and instruction following, and show\nthat SKD consistently outperforms existing KD methods across different domains,\ndata sizes, and model initialization strategies.", "AI": {"tldr": "Introduction of Speculative Knowledge Distillation (SKD) to improve knowledge transfer between student and teacher models in knowledge distillation.", "motivation": "To address the limitations of current knowledge distillation methods, such as distribution mismatches and low-quality training examples.", "method": "Speculative Knowledge Distillation (SKD) enables student models to generate tokens, which are then refined by the teacher model to produce high-quality training data that aligns with the student's inference-time distribution.", "result": "SKD outperforms existing knowledge distillation methods across various text generation tasks, demonstrating improved performance in translation, summarization, math, and instruction following.", "conclusion": "SKD represents a significant advancement in knowledge distillation techniques, enhancing the cooperation between student and teacher models for better knowledge transfer.", "key_contributions": ["Introduction of Speculative Knowledge Distillation (SKD) methodology.", "Demonstration of superior performance of SKD over existing KD methods.", "Application of SKD in multiple text generation tasks validates its efficiency."], "limitations": "", "future_work": "Exploration of additional domains and improvements in the adaptability of the model cooperation in SKD.", "keywords": ["Knowledge Distillation", "Speculative Knowledge Distillation", "Text Generation", "Machine Learning", "AI Applications"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2410.12311", "pdf": "https://arxiv.org/pdf/2410.12311.pdf", "abs": "https://arxiv.org/abs/2410.12311", "title": "Open Domain Question Answering with Conflicting Contexts", "authors": ["Siyi Liu", "Qiang Ning", "Kishaloy Halder", "Wei Xiao", "Zheng Qi", "Phu Mon Htut", "Yi Zhang", "Neha Anna John", "Bonan Min", "Yassine Benajiba", "Dan Roth"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open domain question answering systems frequently rely on information\nretrieved from large collections of text (such as the Web) to answer questions.\nHowever, such collections of text often contain conflicting information, and\nindiscriminately depending on this information may result in untruthful and\ninaccurate answers. To understand the gravity of this problem, we collect a\nhuman-annotated dataset, Question Answering with Conflicting Contexts (QACC),\nand find that as much as 25% of unambiguous, open domain questions can lead to\nconflicting contexts when retrieved using Google Search. We evaluate and\nbenchmark three powerful Large Language Models (LLMs) with our dataset QACC and\ndemonstrate their limitations in effectively addressing questions with\nconflicting information. To explore how humans reason through conflicting\ncontexts, we request our annotators to provide explanations for their\nselections of correct answers. We demonstrate that by finetuning LLMs to\nexplain their answers, we can introduce richer information into their training\nthat guide them through the process of reasoning with conflicting contexts.", "AI": {"tldr": "The paper addresses the issue of conflicting information in open domain question answering systems, demonstrating the limitations of LLMs when confronted with such contexts, and explores the potential of finetuning these models to improve reasoning through conflict.", "motivation": "Open domain question answering systems depend on vast text collections for information, which can contain conflicting data leading to inaccurate answers.", "method": "A human-annotated dataset, QACC, was collected to evaluate how often unambiguous questions lead to conflicting answers from Google Search. The study benchmarks three LLMs on this dataset and examines human reasoning through conflicting contexts by analyzing annotator explanations.", "result": "The findings reveal that up to 25% of open domain questions can lead to conflicting contexts, and current LLMs struggle to effectively respond in these situations. However, finetuning LLMs to explain their reasoning improves their capability to manage conflicting information.", "conclusion": "Enhancing LLMs with explanation-based finetuning offers a promising direction for developing more reliable question answering systems capable of resolving information conflicts.", "key_contributions": ["Introduction of the QACC dataset for benchmarking conflicting contexts in question answering.", "Evaluation of LLMs' limitations in handling conflicting information.", "Proposal of finetuning LLMs for better reasoning with conflicting contexts."], "limitations": "The study is limited to the capability of current LLMs and may not fully address all aspects of conflicting information in open domain QA systems.", "future_work": "Further research is suggested to improve LLMs through alternative training methods and exploring other approaches in resolving information conflicts.", "keywords": ["Open Domain Question Answering", "Conflicting Information", "Large Language Models", "Human Reasoning", "Dataset QACC"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.13961", "pdf": "https://arxiv.org/pdf/2410.13961.pdf", "abs": "https://arxiv.org/abs/2410.13961", "title": "From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization", "authors": ["Catarina G. Belem", "Pouya Pezeshkpour", "Hayate Iso", "Seiji Maekawa", "Nikita Bhutani", "Estevam Hruschka"], "categories": ["cs.CL"], "comment": "NAACL 2025 - Findings", "summary": "Although many studies have investigated and reduced hallucinations in large\nlanguage models (LLMs) for single-document tasks, research on hallucination in\nmulti-document summarization (MDS) tasks remains largely unexplored.\nSpecifically, it is unclear how the challenges arising from handling multiple\ndocuments (e.g., repetition and diversity of information) affect models\noutputs. In this work, we investigate how hallucinations manifest in LLMs when\nsummarizing topic-specific information from multiple documents. Since no\nbenchmarks exist for investigating hallucinations in MDS, we use existing news\nand conversation datasets, annotated with topic-specific insights, to create\ntwo novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks,\nwe observe that on average, up to 75% of the content in LLM-generated summary\nis hallucinated, with hallucinations more likely to occur towards the end of\nthe summaries. Moreover, when summarizing non-existent topic-related\ninformation, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and\n44% of the time, raising concerns about their tendency to fabricate content. To\nunderstand the characteristics of these hallucinations, we manually evaluate\n700+ insights and find that most errors stem from either failing to follow\ninstructions or producing overly generic insights. Motivated by these\nobservations, we investigate the efficacy of simple post-hoc baselines in\nmitigating hallucinations but find them only moderately effective. Our results\nunderscore the need for more effective approaches to systematically mitigate\nhallucinations in MDS. We release our dataset and code at\ngithub.com/megagonlabs/Hallucination_MDS.", "AI": {"tldr": "This paper investigates hallucinations in large language models during multi-document summarization and introduces new benchmarks to evaluate this issue.", "motivation": "The study aims to explore the under-researched area of hallucinations in LLMs for multi-document summarization tasks, as existing research mainly focuses on single-document tasks.", "method": "The authors created two novel multi-document benchmarks from existing news and conversation datasets, annotated with topic-specific insights, and evaluated five LLMs on these benchmarks.", "result": "Findings reveal that, on average, 75% of the content generated in LLM summaries is hallucinated, particularly more frequently towards summary ends, with notable fabrication of non-existent topic-related information.", "conclusion": "There is a significant need for more effective approaches to mitigate hallucinations in multi-document summarization, as existing simple post-hoc methods were found only moderately effective.", "key_contributions": ["Introduction of two novel benchmarks for MDS hallucination evaluation", "Analysis of hallucination characteristics and common errors in LLM-generated summaries", "Empirical evaluation revealing high rates of hallucinations in popular LLMs"], "limitations": "The post-hoc methods tested were only moderately effective, indicating that more sophisticated mitigation strategies are necessary.", "future_work": "Future research should focus on developing more effective techniques to systematically reduce hallucinations observed in multi-document summarization.", "keywords": ["hallucinations", "multi-document summarization", "large language models", "datasets", "evaluation benchmarks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.00030", "pdf": "https://arxiv.org/pdf/2411.00030.pdf", "abs": "https://arxiv.org/abs/2411.00030", "title": "WikiNER-fr-gold: A Gold-Standard NER Corpus", "authors": ["Danrun Cao", "Nicolas Béchet", "Pierre-François Marteau"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "We address in this article the the quality of the WikiNER corpus, a\nmultilingual Named Entity Recognition corpus, and provide a consolidated\nversion of it. The annotation of WikiNER was produced in a semi-supervised\nmanner i.e. no manual verification has been carried out a posteriori. Such\ncorpus is called silver-standard. In this paper we propose WikiNER-fr-gold\nwhich is a revised version of the French proportion of WikiNER. Our corpus\nconsists of randomly sampled 20% of the original French sub-corpus (26,818\nsentences with 700k tokens). We start by summarizing the entity types included\nin each category in order to define an annotation guideline, and then we\nproceed to revise the corpus. Finally we present an analysis of errors and\ninconsistency observed in the WikiNER-fr corpus, and we discuss potential\nfuture work directions.", "AI": {"tldr": "This paper revises the WikiNER corpus for French Named Entity Recognition, creating a gold-standard version to enhance quality and consistency.", "motivation": "To improve the quality of the WikiNER corpus, which was generated in a semi-supervised manner and lacks manual verification.", "method": "The authors sampled 20% of the original French sub-corpus to create WikiNER-fr-gold, defined annotation guidelines, and revised the corpus based on these guidelines.", "result": "The revised corpus contains 26,818 sentences and provides a more reliable dataset for Named Entity Recognition tasks.", "conclusion": "The study highlights inconsistencies in the original WikiNER-fr corpus and sets the groundwork for improving Named Entity Recognition datasets.", "key_contributions": ["Creation of WikiNER-fr-gold to enhance data quality", "Establishment of annotation guidelines for Named Entity Recognition", "Analysis of errors in the original corpus"], "limitations": "The study focuses solely on the French sub-corpus of WikiNER and may not address issues in other language sub-corpora.", "future_work": "Exploration of additional languages and further refinements in annotation processes.", "keywords": ["WikiNER", "Named Entity Recognition", "Corpus Quality", "Annotation Guidelines", "Silver-Standard"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.17270", "pdf": "https://arxiv.org/pdf/2411.17270.pdf", "abs": "https://arxiv.org/abs/2411.17270", "title": "An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese", "authors": ["Duc-Vu Nguyen", "Thang Chau Phan", "Quoc-Nam Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": "Accepted at SoICT 2024", "summary": "In this paper, we aimed to develop a neural parser for Vietnamese based on\nsimplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora,\nVietTreebank and VnDT, had around 15% of constituency and dependency tree pairs\nthat did not adhere to simplified HPSG rules. To attempt to address the issue\nof the corpora not adhering to simplified HPSG rules, we randomly permuted\nsamples from the training and development sets to make them compliant with\nsimplified HPSG. We then modified the first simplified HPSG Neural Parser for\nthe Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which\ncan encode Vietnamese texts. We conducted experiments on our modified\nVietTreebank and VnDT corpora. Our extensive experiments showed that the\nsimplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82%\nfor constituency parsing when using the same predicted part-of-speech (POS)\ntags as the self-attentive constituency parser. Additionally, it outperformed\nprevious studies in dependency parsing with a higher Unlabeled Attachment Score\n(UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores\nlikely due to our focus on arc permutation without changing the original\nlabels, as we did not consult with a linguistic expert. Lastly, the research\nfindings of this paper suggest that simplified HPSG should be given more\nattention to linguistic expert when developing treebanks for Vietnamese natural\nlanguage processing.", "AI": {"tldr": "Development of a neural parser for Vietnamese using simplified Head-Driven Phrase Structure Grammar, achieving new state-of-the-art results in constituency parsing.", "motivation": "To address issues with existing Vietnamese corpora not adhering to simplified HPSG rules and to improve natural language processing for Vietnamese.", "method": "Modified an existing neural parser by using PhoBERT and XLM-RoBERTa models on compliant training samples to achieve better parsing performance.", "result": "Achieved a new state-of-the-art F-score of 82% for constituency parsing and outperformed previous studies in dependency parsing, though it had lower Labeled Attachment Score.", "conclusion": "Emphasizes the need for collaboration with linguistic experts to enhance treebank development for Vietnamese NLP.", "key_contributions": ["New state-of-the-art F-score for constituency parsing in Vietnamese", "Innovative use of modified models (PhoBERT and XLM-RoBERTa)", "Highlights the importance of expert consultation in linguistic research."], "limitations": "Lower Labeled Attachment Score likely due to neglecting expert consultation during development.", "future_work": "Encouragement of expert involvement in future treebank development for improved accuracy and linguistic adherence.", "keywords": ["Vietnamese NLP", "Neural Parser", "HPSG", "Constituency Parsing", "Dependency Parsing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2412.17592", "pdf": "https://arxiv.org/pdf/2412.17592.pdf", "abs": "https://arxiv.org/abs/2412.17592", "title": "Investigating Length Issues in Document-level Machine Translation", "authors": ["Ziqian Peng", "Rachel Bawden", "François Yvon"], "categories": ["cs.CL"], "comment": "Accepted at the MT Summit 2025", "summary": "Transformer architectures are increasingly effective at processing and\ngenerating very long chunks of texts, opening new perspectives for\ndocument-level machine translation (MT). In this work, we challenge the ability\nof MT systems to handle texts comprising up to several thousands of tokens. We\ndesign and implement a new approach designed to precisely measure the effect of\nlength increments on MT outputs. Our experiments with two representative\narchitectures unambiguously show that (a)~translation performance decreases\nwith the length of the input text; (b)~the position of sentences within the\ndocument matters, and translation quality is higher for sentences occurring\nearlier in a document. We further show that manipulating the distribution of\ndocument lengths and of positional embeddings only marginally mitigates such\nproblems. Our results suggest that even though document-level MT is\ncomputationally feasible, it does not yet match the performance of\nsentence-based MT.", "AI": {"tldr": "This study examines the impact of input text length on machine translation performance using transformer architectures, revealing that longer documents result in decreased translation quality.", "motivation": "To investigate the limitations of machine translation systems in handling very long texts and understand how document length affects translation quality.", "method": "The authors designed experiments with two machine translation architectures to measure the effect of incremental input text lengths on translation outputs.", "result": "The results indicate that translation performance declines as text length increases, and that earlier sentences within documents are translated with higher quality, with some attempts to manipulate input length and positional embeddings showing minimal improvement.", "conclusion": "Document-level machine translation is feasible but does not yet achieve the same performance as sentence-based translation methods.", "key_contributions": ["Demonstrated the negative impact of input length on translation quality.", "Investigated the relevance of sentence positioning within documents for MT quality.", "Provided empirical evidence on the limits of document-level MT compared to sentence-based MT."], "limitations": "The manipulations of document lengths and positional embeddings only marginally improve translation quality.", "future_work": "Further research is needed to develop more effective techniques to enhance translation performance for longer texts.", "keywords": ["Machine Translation", "Transformer", "Document-Level Translation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.17596", "pdf": "https://arxiv.org/pdf/2412.17596.pdf", "abs": "https://arxiv.org/abs/2412.17596", "title": "LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context", "authors": ["Kai Ruan", "Xuan Wang", "Jixiang Hong", "Peng Wang", "Yang Liu", "Hao Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "Updated manuscript and title", "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in\nscientific tasks such as literature analysis and experimental design (e.g.,\naccurately extracting key findings from papers or generating coherent\nexperimental procedures), existing evaluation benchmarks primarily assess\nperformance using rich contextual inputs. We introduce LiveIdeaBench, a\ncomprehensive benchmark evaluating LLMs' scientific idea generation by\nassessing divergent thinking capabilities using single-keyword prompts. Drawing\nfrom Guilford's creativity theory, our benchmark employs a dynamic panel of\nstate-of-the-art LLMs to assess generated ideas across five key dimensions:\noriginality, feasibility, fluency, flexibility, and clarity. Through extensive\nexperimentation with over 40 leading models across 1,180 keywords spanning 22\nscientific domains, we reveal that the scientific idea generation capabilities\nmeasured by our benchmark, are poorly predicted by standard metrics of general\nintelligence. Our results demonstrate that models like QwQ-32B-preview achieve\ncreative performance comparable to top-tier models such as\nclaude-3.7-sonnet:thinking, despite significant gaps in their general\nintelligence scores. These findings highlight the need for specialized\nevaluation benchmarks for scientific idea generation and suggest that enhancing\nthese idea generation capabilities in LLMs may require different training\nstrategies than those used for improving general problem-solving abilities,\npotentially enabling a wider range of AI tools tailored for different stages of\nthe scientific process.", "AI": {"tldr": "LiveIdeaBench is a benchmark for evaluating LLMs' scientific idea generation capabilities based on single-keyword prompts, revealing gaps between creativity and general intelligence metrics.", "motivation": "To assess LLMs' abilities in scientific idea generation beyond traditional evaluation metrics focused on general intelligence.", "method": "The benchmark evaluates LLMs' creativity across five dimensions: originality, feasibility, fluency, flexibility, and clarity using a dynamic panel of state-of-the-art models and single-keyword prompts.", "result": "Experiments with over 40 models show that creativity in scientific idea generation is poorly predicted by general intelligence metrics, with models like QwQ-32B-preview performing comparably to top models despite lower general scores.", "conclusion": "There is a need for specialized benchmarks in scientific idea generation, which may require distinct training strategies for LLMs to enhance their capabilities in this area.", "key_contributions": ["Introduction of the LiveIdeaBench benchmark for evaluating scientific idea generation in LLMs.", "Findings that highlight the disconnect between general intelligence scores and creative performance.", "Recommendations for alternative training strategies to improve LLMs' scientific idea generation capabilities."], "limitations": "The benchmark only assesses performance using single-keyword prompts, which may limit the scope of evaluation.", "future_work": "Investigate training strategies tailored specifically for improving idea generation capabilities and expand the benchmark to include more complex prompts.", "keywords": ["Large Language Models", "scientific idea generation", "benchmark", "creativity", "evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.00090", "pdf": "https://arxiv.org/pdf/2502.00090.pdf", "abs": "https://arxiv.org/abs/2502.00090", "title": "Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora", "authors": ["Logan Born", "M. Willis Monroe", "Kathryn Kelley", "Anoop Sarkar"], "categories": ["cs.CL"], "comment": "Englund 1996 incorrectly reported the relative values of signs in the\n  decimal system. An earlier version of this paper used those values. This\n  update fixes those mistakes and retrains our models using the corrected\n  readings. Our analysis and discussion remain similar to the original, but the\n  performance of the baseline model is now stronger", "summary": "A numeration system encodes abstract numeric quantities as concrete strings\nof written characters. The numeration systems used by modern scripts tend to be\nprecise and unambiguous, but this was not so for the ancient and\npartially-deciphered proto-Elamite (PE) script, where written numerals can have\nup to four distinct readings depending on the system that is used to read them.\nWe consider the task of disambiguating between these readings in order to\ndetermine the values of the numeric quantities recorded in this corpus. We\nalgorithmically extract a list of possible readings for each PE numeral\nnotation, and contribute two disambiguation techniques based on structural\nproperties of the original documents and classifiers learned with the\nbootstrapping algorithm. We also contribute a test set for evaluating\ndisambiguation techniques, as well as a novel approach to cautious rule\nselection for bootstrapped classifiers. Our analysis confirms existing\nintuitions about this script and reveals previously-unknown correlations\nbetween tablet content and numeral magnitude. This work is crucial to\nunderstanding and deciphering PE, as the corpus is heavily accounting-focused\nand contains many more numeric tokens than tokens of text.", "AI": {"tldr": "This paper focuses on disambiguating numeral readings in the proto-Elamite script using algorithmic techniques and classifiers to enhance understanding of numeric quantification in ancient records.", "motivation": "The ancient proto-Elamite script has ambiguous numeral representations, complicating efforts to accurately interpret numeric data; understanding these values is vital for historical and accounting analysis.", "method": "The paper presents two disambiguation techniques that utilize structural properties of original documents and employ classifiers trained through a bootstrapping algorithm, along with the development of a test set for evaluating these techniques.", "result": "The analysis validates previous assumptions about the proto-Elamite script and reveals unknown correlations between the content of tablets and the magnitude of numerals, utilizing improved baseline model performance after correcting prior errors in numeral values.", "conclusion": "This work aids significantly in the understanding and deciphering of proto-Elamite numerals, which are heavily featured in accounting-related tablets, thus enhancing the study of ancient numerical systems.", "key_contributions": ["Development of two disambiguation techniques for proto-Elamite numerals", "Creation of a test set for evaluating disambiguation methods", "Novel cautious rule selection approach for bootstrapped classifiers"], "limitations": "The study focuses exclusively on numeral disambiguation, which may limit its applicability to other aspects of the proto-Elamite script and does not address full script decipherment.", "future_work": "Future research could expand on additional applications of the disambiguation techniques to other historical scripts and explore deeper insights into scripts with highly ambiguous representations.", "keywords": ["proto-Elamite script", "numerical disambiguation", "bootstrapping algorithm", "classifier techniques", "ancient accounting"], "importance_score": 0, "read_time_minutes": 15}}
{"id": "2502.12257", "pdf": "https://arxiv.org/pdf/2502.12257.pdf", "abs": "https://arxiv.org/abs/2502.12257", "title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context", "authors": ["Bryan L. M. de Oliveira", "Luana G. B. Martins", "Bruno Brandão", "Luckeciano C. Melo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models excel at following explicit instructions, but they\noften struggle with ambiguous or incomplete user requests, defaulting to\nverbose, generic responses instead of seeking clarification. We introduce\nInfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents\nhandle hidden context in open-ended user requests. This benchmark presents\nintentionally ambiguous scenarios that require models to engage in\ninformation-seeking dialogue by asking clarifying questions before providing\nappropriate responses. Our evaluation of both open and closed models reveals\nthat, while proprietary models generally perform better, all current assistants\nstruggle to gather critical information effectively. They often require\nmultiple turns to infer user intent and frequently default to generic responses\nwithout proper clarification. We provide a systematic methodology for\ngenerating diverse scenarios and evaluating models' information-seeking\ncapabilities, which can be leveraged to automatically generate data for\nself-improvement. We also offer insights into the current limitations of\nlanguage models in handling ambiguous requests through multi-turn interactions.", "AI": {"tldr": "A benchmark called InfoQuest is introduced to assess how dialogue agents manage ambiguous user requests through multi-turn conversations.", "motivation": "To evaluate the effectiveness of dialogue agents in handling ambiguous or incomplete user requests and to improve their interaction capabilities.", "method": "The paper introduces InfoQuest, a chat benchmark with intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions.", "result": "Evaluation shows that while proprietary models perform better, all dialogue assistants struggle to effectively gather necessary information, often defaulting to generic responses.", "conclusion": "The need for improved models that can initiate clarifying questions and better understand user intent in multi-turn interactions is highlighted.", "key_contributions": ["Introduction of the InfoQuest benchmark", "Systematic methodology for generating and evaluating scenarios", "Insights into limitations of current language models in multi-turn dialogue"], "limitations": "Current models often need multiple turns to infer intent and provide generic responses without clarification.", "future_work": "Further investigation into self-improvement methodologies and development of models that can better handle ambiguity in user requests.", "keywords": ["Dialogue agents", "Ambiguous requests", "Information-seeking dialogue"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13019", "pdf": "https://arxiv.org/pdf/2502.13019.pdf", "abs": "https://arxiv.org/abs/2502.13019", "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation", "authors": ["Sha Li", "Naren Ramakrishnan"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of\nLarge Language Models (LLMs) by retrieving and incorporate external documents\nor chunks prior to generation. However, even improved retriever relevance can\nbrings erroneous or contextually distracting information, undermining the\neffectiveness of RAG in downstream tasks. We introduce a compact, efficient,\nand pluggable module designed to refine retrieved chunks before using them for\ngeneration. The module aims to extract and reorganize the most relevant and\nsupportive information into a concise, query-specific format. Through a\nthree-stage training paradigm - comprising supervised fine - tuning,\ncontrastive multi-task learning, and reinforcement learning-based alignment -\nit prioritizes critical knowledge and aligns it with the generator's\npreferences. This approach enables LLMs to produce outputs that are more\naccurate, reliable, and contextually appropriate.", "AI": {"tldr": "This paper presents a novel module for refining retrieved information in Retrieval-Augmented Generation (RAG) to enhance the accuracy and contextual relevance of outputs from Large Language Models (LLMs).", "motivation": "To tackle the issues of erroneous and distracting information retrieved from external documents during RAG, which affects downstream task effectiveness.", "method": "The authors propose a compact and efficient module that refines retrieved chunks through a three-stage training process involving supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment.", "result": "The implementation of the module leads to LLMs producing outputs that are more accurate, reliable, and contextually appropriate, showing improved performance in downstream tasks.", "conclusion": "The proposed module enhances the effectiveness of RAG by extracting and reorganizing relevant information in a query-specific format, aligning better with LLM generation preferences.", "key_contributions": ["Introduction of a new module for refining retrieved information in RAG", "Three-stage training paradigm for optimizing knowledge extraction and alignment", "Improved performance of LLMs in generating contextually appropriate outputs"], "limitations": "The paper does not detail the computational resources required for implementing the proposed module, which may impact scalability.", "future_work": "Exploration of additional learning paradigms and integration with other types of models or domains to further enhance information retrieval and generation.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2502.14644", "pdf": "https://arxiv.org/pdf/2502.14644.pdf", "abs": "https://arxiv.org/abs/2502.14644", "title": "LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning", "authors": ["Yansheng Mao", "Yufei Xu", "Jiaqi Li", "Fanxu Meng", "Haotong Yang", "Zilong Zheng", "Xiyuan Wang", "Muhan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper presents Long Input Fine-Tuning\n(LIFT), a novel framework for long-context modeling that can improve the\nlong-context performance of arbitrary (short-context) LLMs by dynamically\nadapting model parameters based on the long input. Importantly, LIFT, rather\nthan endlessly extending the context window size to accommodate increasingly\nlonger inputs in context, chooses to store and absorb the long input in\nparameter. By fine-tuning the long input into model parameters, LIFT allows\nshort-context LLMs to answer questions even when the required information is\nnot provided in the context during inference. Furthermore, to enhance LIFT\nperformance while maintaining the original in-context learning (ICL)\ncapabilities, we introduce Gated Memory, a specialized attention adapter that\nautomatically balances long input memorization and ICL. We provide a\ncomprehensive analysis of the strengths and limitations of LIFT on long context\nunderstanding, offering valuable directions for future research.", "AI": {"tldr": "The paper introduces Long Input Fine-Tuning (LIFT), enhancing long-context performance of short-context LLMs by adapting model parameters based on long inputs and utilizing Gated Memory for improved performance.", "motivation": "Addressing the challenge of long context understanding in large language models due to limited context windows.", "method": "LIFT dynamically adapts model parameters for long inputs instead of extending context window sizes, while Gated Memory balances memorization and in-context learning.", "result": "LIFT allows short-context LLMs to answer questions relying on long input information not included in the context, showing improved long-context capabilities.", "conclusion": "LIFT demonstrates effective long-context modeling with short-context LLMs, though it has strengths and limitations that need further exploration.", "key_contributions": ["Introduction of Long Input Fine-Tuning (LIFT) for long-context modeling.", "Development of Gated Memory to enhance ICL alongside long input processing.", "Comprehensive analysis of LIFT's performance and its future research directions."], "limitations": "The paper discusses strengths and limitations regarding LIFT's implementation in long context understanding.", "future_work": "Exploring further enhancements to LIFT and its applications in various scenarios.", "keywords": ["Long Input Fine-Tuning", "Gated Memory", "Long-context modeling", "LLMs", "In-context learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.15147", "pdf": "https://arxiv.org/pdf/2502.15147.pdf", "abs": "https://arxiv.org/abs/2502.15147", "title": "Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision", "authors": ["Zhouhang Xie", "Tushar Khot", "Bhavana Dalvi Mishra", "Harshit Surana", "Julian McAuley", "Peter Clark", "Bodhisattwa Prasad Majumder"], "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Instruction-following LLMs have recently allowed systems to discover hidden\nconcepts from a collection of unstructured documents based on a natural\nlanguage description of the purpose of the discovery (i.e., goal). Still, the\nquality of the discovered concepts remains mixed, as it depends heavily on\nLLM's reasoning ability and drops when the data is noisy or beyond LLM's\nknowledge. We present Instruct-LF, a goal-oriented latent factor discovery\nsystem that integrates LLM's instruction-following ability with statistical\nmodels to handle large, noisy datasets where LLM reasoning alone falls short.\n  Instruct-LF uses LLMs to propose fine-grained, goal-related properties from\ndocuments, estimates their presence across the dataset, and applies\ngradient-based optimization to uncover hidden factors, where each factor is\nrepresented by a cluster of co-occurring properties. We evaluate latent factors\nproduced by Instruct-LF on movie recommendation, text-world navigation, and\nlegal document categorization tasks. These interpretable representations\nimprove downstream task performance by 5-52% than the best baselines and were\npreferred 1.8 times as often as the best alternative, on average, in human\nevaluation.", "AI": {"tldr": "Instruct-LF is a novel system that leverages instruction-following LLMs and statistical models to discover meaningful latent factors from large, noisy datasets, significantly improving task performance in various applications.", "motivation": "To improve the quality of discovered concepts from unstructured documents, particularly when faced with noisy data or limitations in LLM reasoning capabilities.", "method": "Instruct-LF integrates LLMs with statistical models to propose goal-related properties, estimates their presence across datasets, and employs gradient-based optimization to reveal hidden factors represented by clusters of co-occurring properties.", "result": "Instruct-LF enhances performance in tasks such as movie recommendation, text-world navigation, and legal document categorization by 5-52% compared to best baselines and received a higher preference rate in human evaluations.", "conclusion": "The integration of LLMs with statistical techniques enables more reliable concept discovery in challenging datasets, demonstrating practical applications across multiple domains.", "key_contributions": ["Introduction of Instruct-LF for latent factor discovery", "Demonstrated 5-52% performance improvements in various tasks", "Human evaluations show a preference for Instruct-LF's outputs over baselines"], "limitations": "Quality of discovered concepts is still dependent on the underlying data quality and LLM reasoning.", "future_work": "Exploring further enhancements to the model's robustness and effectiveness with varying data characteristics.", "keywords": ["latent factor discovery", "instruction-following LLM", "noisy datasets", "statistical models", "concept discovery"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.20503", "pdf": "https://arxiv.org/pdf/2502.20503.pdf", "abs": "https://arxiv.org/abs/2502.20503", "title": "Protecting multimodal large language models against misleading visualizations", "authors": ["Jonathan Tonglet", "Tinne Tuytelaars", "Marie-Francine Moens", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Preprint. Code and data available at\n  https://github.com/UKPLab/arxiv2025-misleading-visualizations", "summary": "Visualizations play a pivotal role in daily communication in an increasingly\ndata-driven world. Research on multimodal large language models (MLLMs) for\nautomated chart understanding has accelerated massively, with steady\nimprovements on standard benchmarks. However, for MLLMs to be reliable, they\nmust be robust to misleading visualizations, charts that distort the underlying\ndata, leading readers to draw inaccurate conclusions that may support\ndisinformation. Here, we uncover an important vulnerability: MLLM\nquestion-answering accuracy on misleading visualizations drops on average to\nthe level of a random baseline. To address this, we introduce the first\ninference-time methods to improve performance on misleading visualizations,\nwithout compromising accuracy on non-misleading ones. The most effective method\nextracts the underlying data table and uses a text-only LLM to answer the\nquestion based on the table. Our findings expose a critical blind spot in\ncurrent research and establish benchmark results to guide future efforts in\nreliable MLLMs.", "AI": {"tldr": "This paper identifies vulnerabilities in multimodal large language models (MLLMs) when interpreting misleading visualizations and introduces novel methods to improve their accuracy while maintaining performance on accurate charts.", "motivation": "To explore the reliability of MLLMs in the presence of misleading visualizations that can lead to disinformation.", "method": "The authors introduce inference-time methods, including one that extracts the underlying data from visualizations to enhance question-answering accuracy using a text-only LLM.", "result": "The study finds a significant drop in question-answering accuracy for MLLMs on misleading visualizations, equating to random baseline performance, while presenting new methods that effectively counter this issue.", "conclusion": "The paper reveals a major gap in the robustness of current MLLMs and sets benchmark results for future research on reliable automated chart understanding.", "key_contributions": ["Identification of a critical vulnerability in MLLMs regarding misleading visualizations.", "Introduction of the first inference-time methods to enhance accuracy on misleading charts.", "Establishment of benchmark results to guide future research efforts."], "limitations": "The proposed methods may not generalize to all types of misleading visualizations and require further validation in diverse contexts.", "future_work": "Further exploration of more robust models and additional techniques to handle varying forms of misleading data visualizations.", "keywords": ["Multimodal Large Language Models", "Automated Chart Understanding", "Misleading Visualizations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.10617", "pdf": "https://arxiv.org/pdf/2503.10617.pdf", "abs": "https://arxiv.org/abs/2503.10617", "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models", "authors": ["Andy Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICLR 2025 SCOPE", "summary": "Adapting large language models to multiple tasks can cause cross-skill\ninterference, where improvements for one skill degrade another. While methods\nsuch as LoRA impose orthogonality constraints at the weight level, they do not\nfully address interference in hidden-state representations. We propose\nCompositional Subspace Representation Fine-tuning (CS-ReFT), a novel\nrepresentation-based approach that learns multiple orthonormal subspace\ntransformations, each specializing in a distinct skill, and composes them via a\nlightweight router. By isolating these subspace edits in the hidden state,\nrather than weight matrices, CS-ReFT prevents cross-task conflicts more\neffectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B\nachieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring\nonly 0.0098% of model parameters. These findings show that specialized\nrepresentation edits, composed via a simple router, significantly enhance\nmulti-task instruction following with minimal overhead.", "AI": {"tldr": "Proposes CS-ReFT, a method to reduce cross-skill interference in large language models by learning orthonormal subspace transformations for distinct skills, enhancing multi-task performance with minimal parameter overhead.", "motivation": "To address cross-skill interference in large language models, where improvements in one skill can degrade performance in another, especially in hidden-state representations.", "method": "CS-ReFT learns multiple orthonormal subspace transformations and composes them using a lightweight router to isolate subspace edits in the hidden state.", "result": "CS-ReFT applied to Llama-2-7B achieved a 93.94% win rate on the AlpacaEval benchmark, outperforming GPT-3.5 Turbo (86.30%) with only 0.0098% of model parameters used.", "conclusion": "The method demonstrates that specialized representation edits can effectively enhance multi-task instruction following in large language models with minimal additional overhead.", "key_contributions": ["Introduction of Compositional Subspace Representation Fine-tuning (CS-ReFT) to mitigate cross-task conflicts.", "Demonstrated effectiveness on the AlpacaEval benchmark with significant performance improvement and minimal parameter overhead.", "Novel approach focusing on hidden-state representation rather than weight matrices for multi-task learning."], "limitations": "", "future_work": "Further exploration of subspace representation techniques and their application across various language models and tasks.", "keywords": ["large language models", "cross-skill interference", "multi-task learning", "subspace representation", "instruction following"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.06868", "pdf": "https://arxiv.org/pdf/2504.06868.pdf", "abs": "https://arxiv.org/abs/2504.06868", "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games", "authors": ["Seungwon Lim", "Seungbeen Lee", "Dongjun Min", "Youngjae Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments.", "AI": {"tldr": "This paper introduces PANDA, a method for integrating human-like personality traits into AI agents to enhance their decision-making in text-based interactive environments.", "motivation": "Aligning artificial agent behaviors with human values is a key challenge, particularly in complex decision-making tasks.", "method": "The method involves training a personality classifier to identify agent personality types and integrating these profiles into the agent's policy-learning process.", "result": "Agents with distinct personality types were deployed in 25 text-based games, revealing that personality adaptation influences decision-making and performance positively, especially for traits like Openness.", "conclusion": "Personality-adapted agents can improve alignment and effectiveness in decision-making, promoting human-centric interactions in AI applications.", "key_contributions": ["Introduction of a novel personality-adapted decision-making framework (PANDA)", "Demonstration of significant performance advantages for agents with certain personality traits", "Insights into the interaction between personality traits and agent actions in gaming environments."], "limitations": "The study is limited to text-based games, so its applicability to other domains may vary.", "future_work": "Future research could explore broader domains for personality adaptation and investigate the impact of other personality traits.", "keywords": ["Personality Adaptation", "AI Agents", "Decision Making", "Human-Centric AI", "Interactive Environments"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09714", "pdf": "https://arxiv.org/pdf/2504.09714.pdf", "abs": "https://arxiv.org/abs/2504.09714", "title": "Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish", "authors": ["Ayşe Aysu Cengiz", "Ahmet Kaan Sever", "Elif Ecem Ümütlü", "Naime Şeyma Erdem", "Burak Aytan", "Büşra Tufan", "Abdullah Topraksoy", "Esra Darıcı", "Cagri Toraman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reliance on translated or adapted datasets from English or multilingual\nresources introduces challenges regarding linguistic and cultural suitability.\nThis study addresses the need for robust and culturally appropriate benchmarks\nby evaluating the quality of 17 commonly used Turkish benchmark datasets. Using\na comprehensive framework that assesses six criteria, both human and LLM-judge\nannotators provide detailed evaluations to identify dataset strengths and\nshortcomings.\n  Our results reveal that 70% of the benchmark datasets fail to meet our\nheuristic quality standards. The correctness of the usage of technical terms is\nthe strongest criterion, but 85% of the criteria are not satisfied in the\nexamined datasets. Although LLM judges demonstrate potential, they are less\neffective than human annotators, particularly in understanding cultural common\nsense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger\nlabeling capabilities for grammatical and technical tasks, while Llama3.3-70B\nexcels at correctness and cultural knowledge evaluation. Our findings emphasize\nthe urgent need for more rigorous quality control in creating and adapting\ndatasets for low-resource languages.", "AI": {"tldr": "This study evaluates the quality of 17 Turkish benchmark datasets and identifies significant shortcomings in their linguistic and cultural suitability, with 70% failing to meet quality standards.", "motivation": "The study addresses the challenges posed by translated datasets from English or multilingual resources that may lack cultural appropriateness and quality.", "method": "A comprehensive framework assessing six criteria is used to evaluate the datasets, incorporating assessments from both human and LLM judges.", "result": "70% of the examined benchmark datasets do not meet the heuristic quality standards; human annotators outperform LLMs in cultural common sense knowledge.", "conclusion": "There is a critical need for more rigorous quality control in the creation of datasets for low-resource languages to ensure linguistic and cultural relevance.", "key_contributions": ["Evaluation framework for benchmark datasets in low-resource languages", "Comparison of human and LLM performance in dataset evaluation", "Highlighting the need for improved dataset quality control"], "limitations": "The study only examines Turkish datasets and may not generalize to other languages.", "future_work": "Further research is needed to develop quality control mechanisms for datasets and to explore improved methodologies for evaluating linguistic and cultural suitability.", "keywords": ["dataset evaluation", "linguistic quality", "cultural appropriateness", "benchmark datasets", "low-resource languages"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982.pdf", "abs": "https://arxiv.org/abs/2504.10982", "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Zixin Xu", "Xiujie Chen", "Issey Sukeda", "Irene Li"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "AI": {"tldr": "This paper explores a KG-based RAG framework for Japanese medical QA using small open-source LLMs, revealing limited effectiveness and challenges related to retrieved content quality.", "motivation": "To address the limitations of LLMs in Japanese medical QA, particularly due to privacy constraints, and to explore the combination of instruction-tuned open-source LLMs with retrieval-augmented generation (RAG).", "method": "The authors developed a knowledge graph-based RAG framework tailored for small-scale open-source LLMs and conducted experiments to assess its impact on Japanese medical question answering.", "result": "Experimental results indicate that the KG-based RAG has a limited impact on the performance of Japanese medical QA with small-scale open-source LLMs, highly influenced by the quality of external content.", "conclusion": "The findings highlight the sensitivity of the KG-based RAG framework to the relevance of retrieved information and provide insights for optimizing RAG in low-resource language contexts.", "key_contributions": ["First exploration of KG-based RAG in Japanese medical QA.", "Insights into the challenges of applying RAG in low-resource languages.", "Empirical data showing the impact of content quality on RAG performance."], "limitations": "The effectiveness of the RAG framework is constrained by the quality and relevance of the external retrieved content.", "future_work": "Future research could focus on enhancing the retrieval processes and evaluating the framework with different language models and medical domains.", "keywords": ["Japanese medical QA", "knowledge graph", "retrieval-augmented generation", "open-source LLMs", "low-resource languages"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.11975", "pdf": "https://arxiv.org/pdf/2504.11975.pdf", "abs": "https://arxiv.org/abs/2504.11975", "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes", "authors": ["Raúl Vázquez", "Timothee Mickus", "Elaine Zosa", "Teemu Vahtola", "Jörg Tiedemann", "Aman Sinha", "Vincent Segonne", "Fernando Sánchez-Vega", "Alessandro Raganato", "Jindřich Libovický", "Jussi Karlgren", "Shaoxiong Ji", "Jindřich Helcl", "Liane Guillou", "Ona de Gibert", "Jaione Bengoetxea", "Joseph Attieh", "Marianna Apidianaki"], "categories": ["cs.CL"], "comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)", "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.", "AI": {"tldr": "This paper presents the Mu-SHROOM shared task on detecting hallucinations in outputs from instruction-tuned large language models across 14 languages, analyzing over 2,600 submissions.", "motivation": "The paper addresses the challenge of detecting hallucinations and overgeneration mistakes in outputs from large language models, emphasizing the significance of this issue in AI applications.", "method": "The hallucination detection problem is framed as a span-labeling task, supported by empirical analysis of submissions from participating teams.", "result": "The results include various methodologies from 43 teams, highlighting factors that contribute to strong performance and the variability of hallucinations in different languages.", "conclusion": "The study emphasizes the importance of addressing varying hallucination degrees and annotator disagreements in labeling tasks, suggesting areas for future research.", "key_contributions": ["Introduction of the Mu-SHROOM shared task for detecting hallucinations in LLM outputs.", "Analysis of 2,618 submissions to identify factors influencing task performance.", "Insights into language-related challenges in hallucination detection."], "limitations": "Variability of hallucinations across languages and high annotator disagreement when labeling.", "future_work": "Further research on improving detection methods and reducing annotator disagreement.", "keywords": ["hallucination detection", "large language models", "span-labeling", "SemEval-2025"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.12098", "pdf": "https://arxiv.org/pdf/2504.12098.pdf", "abs": "https://arxiv.org/abs/2504.12098", "title": "Gauging Overprecision in LLMs: An Empirical Study", "authors": ["Adil Bahaj", "Hamed Rahimi", "Mohamed Chetouani", "Mounir Ghogho"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is\nno correlation between the length of the interval and the imposed confidence\nlevel, which can be symptomatic of a a) lack of understanding of the concept of\nconfidence or b) inability to adjust self-confidence by following instructions,\n{3) LLM numerical precision differs depending on the task, scale of answer and\nprompting technique 4) Refinement of answers doesn't improve precision in most\ncases. We believe this study offers new perspectives on LLM overconfidence and\nserves as a strong baseline for overprecision in LLMs.", "AI": {"tldr": "This paper introduces a framework to study overconfidence in large language models (LLMs), focusing on the concept of overprecision from cognitive science, and evaluates its effects on numerical tasks.", "motivation": "The study aims to quantify the trustworthiness of LLM outputs by addressing the issue of overconfidence, particularly in numerical generation tasks and its implications for understanding LLM reliability.", "method": "The framework consists of three phases: generation (where LLMs produce answers with specified confidence intervals), refinement (where answers are improved), and evaluation (where the internal workings of LLMs are analyzed).", "result": "The study identifies that LLMs exhibit high levels of uncalibrated responses in numerical tasks, show no correlation between interval length and confidence levels, and demonstrate varying numerical precision based on task and prompting techniques.", "conclusion": "This research provides new insights into LLM overconfidence, highlighting areas that require attention and setting a baseline for future studies on overprecision in LLMs.", "key_contributions": ["Introduces a novel framework for studying overconfidence in LLMs.", "Findings on the lack of calibration of LLMs for numerical tasks.", "Identifies key differences in LLM performance based on task and prompting."], "limitations": "The study primarily focuses on numerical tasks and may not generalize to other types of tasks involving LLMs.", "future_work": "Future research could explore overconfidence in different LLM architectures and tasks, and investigate strategies to improve calibration in LLM outputs.", "keywords": ["Large Language Models", "Overconfidence", "Evaluation Techniques", "Cognitive Science", "Numerical Precision"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828.pdf", "abs": "https://arxiv.org/abs/2504.13828", "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "v3: add the comparison to existing work part; fix some errors", "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations such as\nknowledge latency, shallow reasoning, and constrained cognitive processes.\nDuring this era, prompt engineering emerged as our primary interface with AI,\nenabling dialogue-level communication through natural language. We now witness\nthe emergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering", "AI": {"tldr": "The paper discusses the evolution from traditional Large Language Models (LLMs) to advanced cognition engineering in AI, outlining the shift towards thought-construction engines and the implications for user interaction with AI.", "motivation": "To address the limitations of first-generation LLMs and explore the potential of cognition engineering in improving human-AI interaction.", "method": "The paper presents comprehensive tutorials and implementations for understanding cognition engineering and its application in generative AI.", "result": "The introduction of test-time scaling techniques is highlighted as pivotal in transforming LLMs from knowledge-retrieval to thought-construction engines, fostering deeper cognitive connections with AI.", "conclusion": "The paper emphasizes the critical need for practitioners to engage with cognition engineering to fully leverage the capabilities of advanced AI systems.", "key_contributions": ["Clarification of cognition engineering concepts", "Tutorials for implementing advanced AI techniques", "A repository of papers on test-time scaling"], "limitations": "Potentially limited access to computational resources for practical application of discussed techniques.", "future_work": "Encouragement for ongoing contributions to the field and continuous updates to the shared repository.", "keywords": ["Large Language Models", "cognition engineering", "test-time scaling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.15471", "pdf": "https://arxiv.org/pdf/2504.15471.pdf", "abs": "https://arxiv.org/abs/2504.15471", "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models", "authors": ["Tyler A. Chang", "Benjamin K. Bergen"], "categories": ["cs.CL"], "comment": null, "summary": "In Transformer language models, activation vectors transform from current\ntoken embeddings to next token predictions as they pass through the model. To\nisolate a minimal form of this transformation, we identify language model\nsubnetworks that make bigram predictions, naive next token predictions based\nonly on the current token. We find that bigram subnetworks can be found in\nfully trained language models up to 1B parameters, and these subnetworks are\ncritical for model performance even when they consist of less than 0.2% of\nmodel parameters. Bigram subnetworks are concentrated in the first Transformer\nMLP layer, and they overlap significantly with subnetworks trained to optimally\nprune a given model. Mechanistically, the bigram subnetworks often recreate a\npattern from the full models where the first layer induces a sharp change that\naligns activations with next token predictions rather than current token\nrepresentations. Our results demonstrate that bigram subnetworks comprise a\nminimal subset of parameters that are both necessary and sufficient for basic\nnext token predictions in language models, and they help drive the\ntransformation from current to next token activations in the residual stream.\nThese subnetworks can lay a foundation for studying more complex language model\ncircuits by building up from a minimal circuit.", "AI": {"tldr": "This paper identifies minimal subnetwork structures within Transformer language models that are responsible for bigram predictions and their necessity for model performance.", "motivation": "To explore the minimal transformation processes in language models that are crucial for generating next token predictions.", "method": "The study involved isolating language model subnetworks that focus on bigram predictions, examining their effectiveness within fully trained models with up to 1 billion parameters.", "result": "It was found that bigram subnetworks are crucial for performance, making proper predictions with less than 0.2% of model parameters, primarily located in the first MLP layer of the Transformer model.", "conclusion": "Bigram subnetworks represent a necessary and sufficient component for basic next token predictions, serving as a foundation for future research on more complex language model circuits.", "key_contributions": ["Identification of bigram subnetworks in language models up to 1B parameters.", "Demonstration of their critical role in model performance despite their small size.", "Insight into the structural organization of these subnetworks relating to predictions."], "limitations": "", "future_work": "Further exploration into how these minimal subnetworks can serve as a basis for understanding more complex language model functionalities.", "keywords": ["Transformer models", "bigram predictions", "subnetwork", "language models", "activation transformation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900.pdf", "abs": "https://arxiv.org/abs/2504.15900", "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.", "AI": {"tldr": "This paper extends the GRPO framework to improve audio-language models through structured reasoning and curriculum learning.", "motivation": "To investigate how reinforcement learning can enhance audio-language reasoning capabilities in large language models, building upon prior successes in text-based models.", "method": "The authors applied a two-stage regimen involving supervised fine-tuning on structured and unstructured reasoning, followed by curriculum-guided Group-Relative Policy Optimization (GRPO), to a Large Audio-Language Model (LALM).", "result": "The structured audio reasoning model, SARI, achieved a 16.35% improvement in average accuracy over the baseline Qwen2-Audio-7B-Instruct and reached state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.", "conclusion": "Explicit and structured reasoning with curriculum learning significantly enhances the understanding of audio-language tasks in large language models.", "key_contributions": ["Development of a structured audio reasoning model (SARI)", "Demonstration of significant accuracy improvement over baseline models", "Insights into the importance of structured reasoning and curriculum learning in RL for audio tasks."], "limitations": "", "future_work": "Exploration of additional architectures and deeper implications of curriculum learning in various audio-language tasks.", "keywords": ["Reinforcement Learning", "Audio-Language Models", "Curriculum Learning", "Structured Reasoning", "Large Language Models"], "importance_score": 8, "read_time_minutes": 15}}
