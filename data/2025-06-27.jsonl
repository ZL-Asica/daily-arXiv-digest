{"id": "2506.20748", "pdf": "https://arxiv.org/pdf/2506.20748.pdf", "abs": "https://arxiv.org/abs/2506.20748", "title": "Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots", "authors": ["Jingshu Li", "Zicheng Zhu", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Chatbots are increasingly integrated into people's lives and are widely used\nto help people. Recently, there has also been growing interest in the reverse\ndirection-humans help chatbots-due to a wide range of benefits including better\nchatbot performance, human well-being, and collaborative outcomes. However,\nlittle research has explored the factors that motivate people to help chatbots.\nTo address this gap, we draw on the Computers Are Social Actors (CASA)\nframework to examine how chatbot anthropomorphism-including human-like\nidentity, emotional expression, and non-verbal expression-influences human\nempathy toward chatbots and their subsequent prosocial behaviors and\nintentions. We also explore people's own interpretations of their prosocial\nbehaviors toward chatbots. We conducted an online experiment (N = 244) in which\nchatbots made mistakes in a collaborative image labeling task and explained the\nreasons to participants. We then measured participants' prosocial behaviors and\nintentions toward the chatbots. Our findings revealed that human identity and\nemotional expression of chatbots increased participants' prosocial behavior and\nintention toward chatbots, with empathy mediating these effects. Qualitative\nanalysis further identified two motivations for participants' prosocial\nbehaviors: empathy for the chatbot and perceiving the chatbot as human-like. We\ndiscuss the implications of these results for understanding and promoting human\nprosocial behaviors toward chatbots."}
{"id": "2506.20884", "pdf": "https://arxiv.org/pdf/2506.20884.pdf", "abs": "https://arxiv.org/abs/2506.20884", "title": "\"TikTok, Do Your Thing\": User Reactions to Social Surveillance in the Public Sphere", "authors": ["Meira Gilbert", "Miranda Wei", "Lindah Kotut"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "''TikTok, Do Your Thing'' is a viral trend where users attempt to identify\nstrangers they see in public via information crowd-sourcing. The trend started\nas early as 2021 and users typically engage with it for romantic purposes\n(similar to a ''Missed Connections'' personal advertisement). This practice\nincludes acts of surveillance and identification in the public sphere, although\nby peers rather than governments or corporations. To understand users'\nreactions to this trend we conducted a qualitative analysis of 60 TikTok videos\nand 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were\nsuccessfully identified. We also find that while there were comments expressing\ndisapproval (n=310), more than double the number expressed support (n=883).\nSupportive comments demonstrated genuine interest and empathy, reflecting\nevolving conceptions of community and algorithmic engagement. On the other\nhand, disapproving comments highlighted concerns about inappropriate\nrelationships, stalking, consent, and gendered double standards. We discuss\nthese insights in relation to the normalization of interpersonal surveillance,\nonline stalking, and as an evolution of social surveillance to offer a new\nperspective on user perceptions surrounding interpersonal surveillance and\nidentification in the public sphere."}
{"id": "2506.20952", "pdf": "https://arxiv.org/pdf/2506.20952.pdf", "abs": "https://arxiv.org/abs/2506.20952", "title": "Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment", "authors": ["Kyosuke Ishibashi", "Atsushi Saito", "Zin Y. Tun", "Lucas Ray", "Megan C. Coram", "Akihiro Sakurai", "Allison M. Okamura", "Ko Yamamoto"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Human crowd simulation in virtual reality (VR) is a powerful tool with\npotential applications including emergency evacuation training and assessment\nof building layout. While haptic feedback in VR enhances immersive experience,\nits effect on walking behavior in dense and dynamic pedestrian flows is\nunknown. Through a user study, we investigated how haptic feedback changes user\nwalking motion in crowded pedestrian flows in VR. The results indicate that\nhaptic feedback changed users' collision avoidance movements, as measured by\nincreased walking trajectory length and change in pelvis angle. The\ndisplacements of users' lateral position and pelvis angle were also increased\nin the instantaneous response to a collision with a non-player character (NPC),\neven when the NPC was inside the field of view. Haptic feedback also enhanced\nusers' awareness and visual exploration when an NPC approached from the side\nand back. Furthermore, variation in walking speed was increased by the haptic\nfeedback. These results suggested that the haptic feedback enhanced users'\nsensitivity to a collision in VR environment."}
{"id": "2506.21195", "pdf": "https://arxiv.org/pdf/2506.21195.pdf", "abs": "https://arxiv.org/abs/2506.21195", "title": "Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact", "authors": ["Neha Raghuvanshi"], "categories": ["cs.HC"], "comment": null, "summary": "Have you wondered how cross-functional teams balance between maximizing value\nthat users derive and business growth leading to win-win situations? This case\nstudy shows how User Experience Research (UXR) and Data Science teams used\nmixed methods research to strategically influence Product Led Growth (PLG) for\na Password Manager used by million+ users, thus allowing our users, internal\nteams, and business to win. The audience will take away practical\nlessons/techniques related to leveraging mixed methods to: a. Maximize user\nvalue while meeting business growth goals b. Influence cross-functional teams\nc. Measure user and business impact This case study can be easily tied to the\nUXR Point of view pyramid (POV) [2] that represents a methodological approach\nto construct a POV and further dives into actioning POV to create measurable\nuser and business impact."}
{"id": "2506.20747", "pdf": "https://arxiv.org/pdf/2506.20747.pdf", "abs": "https://arxiv.org/abs/2506.20747", "title": "Towards Probabilistic Question Answering Over Tabular Data", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning."}
{"id": "2506.21201", "pdf": "https://arxiv.org/pdf/2506.21201.pdf", "abs": "https://arxiv.org/abs/2506.21201", "title": "Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices", "authors": ["Zihao You", "Michael Crabb"], "categories": ["cs.HC"], "comment": "3 pages, 1 figure, Access InContext Workshop at CHI 2025 on 26th of\n  April", "summary": "The consumption of subtitles via TVs, laptops and smartphones has the\npotential to marginalize people based on their complex accessibility needs. The\ncurrent one-size-fits-all approach to this accessibility aid is no longer fit\nfor purpose and work is required to look at how it can be adapted to be\npersonalised for individual users based on individual context, content, and\nconsumption habits. People with Aphasia, for example, encounter significant\nchallenges in understanding subtitle texts.\n  We see our work as a call to action for more inclusive practices, focusing on\nhow the thoughts and opinions of people with aphasia can be included in media\nresearch. Our work investigates how to develop future media solutions for\npeople with aphasia to create a more inclusive media viewing environment. We\nbelieve the key to this is appropriate prototyping tools and methods to allow\nequitable inclusion in the system design process."}
{"id": "2506.20793", "pdf": "https://arxiv.org/pdf/2506.20793.pdf", "abs": "https://arxiv.org/abs/2506.20793", "title": "Multi-lingual Functional Evaluation for Large Language Models", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations."}
{"id": "2506.21319", "pdf": "https://arxiv.org/pdf/2506.21319.pdf", "abs": "https://arxiv.org/abs/2506.21319", "title": "Multimodal LLMs for Visualization Reconstruction and Understanding", "authors": ["Can Liu", "Chunlin Da", "Xiaoxiao Long", "Yuxiao Yang", "Yu Zhang", "Yong Wang"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality."}
{"id": "2506.20803", "pdf": "https://arxiv.org/pdf/2506.20803.pdf", "abs": "https://arxiv.org/abs/2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes."}
{"id": "2506.21322", "pdf": "https://arxiv.org/pdf/2506.21322.pdf", "abs": "https://arxiv.org/abs/2506.21322", "title": "\"Who Should I Believe?\": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory", "authors": ["Hong Wang", "Natalia Calvo-Barajas", "Katie Winkle", "Ginevra Castellano"], "categories": ["cs.HC", "cs.RO"], "comment": "8 pages", "summary": "Advancements in robotic capabilities for providing physical assistance,\npsychological support, and daily health management are making the deployment of\nintelligent healthcare robots in home environments increasingly feasible in the\nnear future. However, challenges arise when the information provided by these\nrobots contradicts users' memory, raising concerns about user trust and\ndecision-making. This paper presents a study that examines how varying a\nrobot's level of transparency and sociability influences user interpretation,\ndecision-making and perceived trust when faced with conflicting information\nfrom a robot. In a 2 x 2 between-subjects online study, 176 participants\nwatched videos of a Furhat robot acting as a family healthcare assistant and\nsuggesting a fictional user to take medication at a different time from that\nremembered by the user. Results indicate that robot transparency influenced\nusers' interpretation of information discrepancies: with a low transparency\nrobot, the most frequent assumption was that the user had not correctly\nremembered the time, while with the high transparency robot, participants were\nmore likely to attribute the discrepancy to external factors, such as a partner\nor another household member modifying the robot's information. Additionally,\nparticipants exhibited a tendency toward overtrust, often prioritizing the\nrobot's recommendations over the user's memory, even when suspecting system\nmalfunctions or third-party interference. These findings highlight the impact\nof transparency mechanisms in robotic systems, the complexity and importance\nassociated with system access control for multi-user robots deployed in home\nenvironments, and the potential risks of users' over reliance on robots in\nsensitive domains such as healthcare."}
{"id": "2506.20821", "pdf": "https://arxiv.org/pdf/2506.20821.pdf", "abs": "https://arxiv.org/abs/2506.20821", "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning."}
{"id": "2506.21333", "pdf": "https://arxiv.org/pdf/2506.21333.pdf", "abs": "https://arxiv.org/abs/2506.21333", "title": "A Systematic Review of Human-AI Co-Creativity", "authors": ["Saloni Singh", "Koen Hndriks", "Drik Heylen", "Kim Baraka"], "categories": ["cs.HC", "cs.AI", "I.2.11"], "comment": null, "summary": "The co creativity community is making significant progress in developing more\nsophisticated and tailored systems to support and enhance human creativity.\nDesign considerations from prior work can serve as a valuable and efficient\nfoundation for future systems. To support this effort, we conducted a\nsystematic literature review of 62 papers on co-creative systems. These papers\ncover a diverse range of applications, including visual arts, design, and\nwriting, where the AI acts not just as a tool but as an active collaborator in\nthe creative process. From this review, we identified several key dimensions\nrelevant to system design: phase of the creative process, creative task,\nproactive behavior of the system, user control, system embodiment, and AI model\ntype. Our findings suggest that systems offering high user control lead to\ngreater satisfaction, trust, and a stronger sense of ownership over creative\noutcomes. Furthermore, proactive systems, when adaptive and context sensitive,\ncan enhance collaboration. We also extracted 24 design considerations,\nhighlighting the value of encouraging users to externalize their thoughts and\nof increasing the system's social presence and transparency to foster trust.\nDespite recent advancements, important gaps remain, such as limited support for\nearly creative phases like problem clarification, and challenges related to\nuser adaptation to AI systems."}
{"id": "2506.20822", "pdf": "https://arxiv.org/pdf/2506.20822.pdf", "abs": "https://arxiv.org/abs/2506.20822", "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "authors": ["Quintin Myers", "Yanjun Gao"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology."}
{"id": "2506.21417", "pdf": "https://arxiv.org/pdf/2506.21417.pdf", "abs": "https://arxiv.org/abs/2506.21417", "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation", "authors": ["Yunxiu Xu", "Siyu Wang", "Shoichi Hasegawa"], "categories": ["cs.HC", "H.5.2; I.3.6"], "comment": "14 pages, 15 figures, 2 tables. Published in IEEE Transactions on\n  Haptics (Early Access)", "summary": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability."}
{"id": "2506.20876", "pdf": "https://arxiv.org/pdf/2506.20876.pdf", "abs": "https://arxiv.org/abs/2506.20876", "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "categories": ["cs.CL"], "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process."}
{"id": "2506.21441", "pdf": "https://arxiv.org/pdf/2506.21441.pdf", "abs": "https://arxiv.org/abs/2506.21441", "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail."}
{"id": "2506.20917", "pdf": "https://arxiv.org/pdf/2506.20917.pdf", "abs": "https://arxiv.org/abs/2506.20917", "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "authors": ["Zhengyan Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence."}
{"id": "2506.21456", "pdf": "https://arxiv.org/pdf/2506.21456.pdf", "abs": "https://arxiv.org/abs/2506.21456", "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model."}
{"id": "2506.20920", "pdf": "https://arxiv.org/pdf/2506.20920.pdf", "abs": "https://arxiv.org/abs/2506.20920", "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "categories": ["cs.CL"], "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases."}
{"id": "2506.20795", "pdf": "https://arxiv.org/pdf/2506.20795.pdf", "abs": "https://arxiv.org/abs/2506.20795", "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "comment": null, "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures."}
{"id": "2506.20923", "pdf": "https://arxiv.org/pdf/2506.20923.pdf", "abs": "https://arxiv.org/abs/2506.20923", "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL"], "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters."}
{"id": "2506.20803", "pdf": "https://arxiv.org/pdf/2506.20803.pdf", "abs": "https://arxiv.org/abs/2506.20803", "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes."}
{"id": "2506.20989", "pdf": "https://arxiv.org/pdf/2506.20989.pdf", "abs": "https://arxiv.org/abs/2506.20989", "title": "Can Gradient Descent Simulate Prompting?", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 2 figures", "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning."}
{"id": "2506.20993", "pdf": "https://arxiv.org/pdf/2506.20993.pdf", "abs": "https://arxiv.org/abs/2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines."}
{"id": "2506.20993", "pdf": "https://arxiv.org/pdf/2506.20993.pdf", "abs": "https://arxiv.org/abs/2506.20993", "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines."}
{"id": "2506.21338", "pdf": "https://arxiv.org/pdf/2506.21338.pdf", "abs": "https://arxiv.org/abs/2506.21338", "title": "AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification", "authors": ["Galvin Brice S. Lim", "Brian Godwin S. Lim", "Argel A. Bandala", "John Anthony C. Jose", "Timothy Scott C. Chu", "Edwin Sybingco"], "categories": ["cs.LG", "cs.HC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Brain-computer interface (BCI) technology utilizing electroencephalography\n(EEG) marks a transformative innovation, empowering motor-impaired individuals\nto engage with their environment on equal footing. Despite its promising\npotential, developing subject-invariant and session-invariant BCI systems\nremains a significant challenge due to the inherent complexity and variability\nof neural activity across individuals and over time, compounded by EEG hardware\nconstraints. While prior studies have sought to develop robust BCI systems,\nexisting approaches remain ineffective in capturing the intricate\nspatiotemporal dependencies within multichannel EEG signals. This study\naddresses this gap by introducing the attentive graph-temporal convolutional\nnetwork (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)\nclassification. Specifically, AGTCNet leverages the topographic configuration\nof EEG electrodes as an inductive bias and integrates graph convolutional\nattention network (GCAT) to jointly learn expressive spatiotemporal EEG\nrepresentations. The proposed model significantly outperformed existing MI-EEG\nclassifiers, achieving state-of-the-art performance while utilizing a compact\narchitecture, underscoring its effectiveness and practicality for BCI\ndeployment. With a 49.87% reduction in model size, 64.65% faster inference\ntime, and shorter input EEG signal, AGTCNet achieved a moving average accuracy\nof 66.82% for subject-independent classification on the BCI Competition IV\nDataset 2a, which further improved to 82.88% when fine-tuned for\nsubject-specific classification. On the EEG Motor Movement/Imagery Dataset,\nAGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and\n2-class subject-independent classifications, respectively, with further\nimprovements to 72.13% and 90.54% for subject-specific classifications."}
{"id": "2506.21031", "pdf": "https://arxiv.org/pdf/2506.21031.pdf", "abs": "https://arxiv.org/abs/2506.21031", "title": "Large Language Models Acing Chartered Accountancy", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation."}
{"id": "2506.21490", "pdf": "https://arxiv.org/pdf/2506.21490.pdf", "abs": "https://arxiv.org/abs/2506.21490", "title": "Ad-Hoc Human-AI Coordination Challenge", "authors": ["Tin Dizdarević", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": "Published at ICML 2025", "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}."}
{"id": "2506.21049", "pdf": "https://arxiv.org/pdf/2506.21049.pdf", "abs": "https://arxiv.org/abs/2506.21049", "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by ACL 2025", "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models."}
{"id": "2506.21536", "pdf": "https://arxiv.org/pdf/2506.21536.pdf", "abs": "https://arxiv.org/abs/2506.21536", "title": "PsyLite Technical Report", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments."}
{"id": "2506.21053", "pdf": "https://arxiv.org/pdf/2506.21053.pdf", "abs": "https://arxiv.org/abs/2506.21053", "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection."}
{"id": "2501.09910", "pdf": "https://arxiv.org/pdf/2501.09910.pdf", "abs": "https://arxiv.org/abs/2501.09910", "title": "Chatbot apologies: Beyond bullshit", "authors": ["P. D. Magnus", "Alessandra Buccella", "Jason D'Cruz"], "categories": ["cs.HC"], "comment": null, "summary": "Apologies serve essential functions for moral agents such as expressing\nremorse, taking responsibility, and repairing trust. LLM-based chatbots\nroutinely produce output that has the linguistic form of an apology. However,\nthey do this simply because they are echoing the kinds of things that humans\nsay. Moreover, there are reasons to think that chatbots are not the kind of\nlinguistic or moral agents capable of apology. To put the point bluntly:\nChatbot apologies are bullshit. This paper explores this concern and develops\nit beyond the epithet, drawing on the nature of morally serious apologies, the\nlinguistic agency required to perform them, and the moral agency required for\nthem to matter. We conclude by considering some consequences for how chatbots\nshould be designed and how we ought to think about them."}
{"id": "2506.21096", "pdf": "https://arxiv.org/pdf/2506.21096.pdf", "abs": "https://arxiv.org/abs/2506.21096", "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines."}
{"id": "2506.19268", "pdf": "https://arxiv.org/pdf/2506.19268.pdf", "abs": "https://arxiv.org/abs/2506.19268", "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "categories": ["cs.HC", "cs.CR", "cs.ET", "cs.LG"], "comment": null, "summary": "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing."}
{"id": "2506.21098", "pdf": "https://arxiv.org/pdf/2506.21098.pdf", "abs": "https://arxiv.org/abs/2506.21098", "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations."}
{"id": "2503.18313", "pdf": "https://arxiv.org/pdf/2503.18313.pdf", "abs": "https://arxiv.org/abs/2503.18313", "title": "Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective", "authors": ["Changlun Li", "Yao Shi", "Yuyu Luo", "Nan Tang"], "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.HC"], "comment": "6 pages, 3 figures, perspective paper", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision-making remains\ninadequately evaluated. Current benchmarks primarily assess LLMs' understanding\non financial documents rather than the ability to manage assets or dig out\ntrading opportunities in dynamic market conditions. Despite the release of new\nbenchmarks for evaluating diversified tasks on the financial domain, we\nidentified four major problems in these benchmarks, which are data leakage,\nnavel-gazing, over-intervention, and maintenance-hard. To pave the research\ngap, we introduce DeepFund, a comprehensive arena platform for evaluating\nLLM-based trading strategies in a live environment. Our approach implements a\nmulti-agent framework where they serve as multiple key roles that realize the\nreal-world investment decision processes. Moreover, we provide a web interface\nthat visualizes LLMs' performance with fund investment metrics across different\nmarket conditions, enabling detailed comparative analysis. Through DeepFund, we\naim to provide a more realistic and fair assessment on LLM's capabilities in\nfund investment, offering diversified insights and revealing their potential\napplications in real-world financial markets. Our code is publicly available at\nhttps://github.com/HKUSTDial/DeepFund."}
{"id": "2506.21119", "pdf": "https://arxiv.org/pdf/2506.21119.pdf", "abs": "https://arxiv.org/abs/2506.21119", "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICONIP 2024", "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Under review for KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2506.21170", "pdf": "https://arxiv.org/pdf/2506.21170.pdf", "abs": "https://arxiv.org/abs/2506.21170", "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference."}
{"id": "2506.21182", "pdf": "https://arxiv.org/pdf/2506.21182.pdf", "abs": "https://arxiv.org/abs/2506.21182", "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb"}
{"id": "2506.21191", "pdf": "https://arxiv.org/pdf/2506.21191.pdf", "abs": "https://arxiv.org/abs/2506.21191", "title": "Prompt-Guided Turn-Taking Prediction", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts."}
{"id": "2506.21222", "pdf": "https://arxiv.org/pdf/2506.21222.pdf", "abs": "https://arxiv.org/abs/2506.21222", "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks."}
{"id": "2506.21252", "pdf": "https://arxiv.org/pdf/2506.21252.pdf", "abs": "https://arxiv.org/abs/2506.21252", "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub."}
{"id": "2506.21274", "pdf": "https://arxiv.org/pdf/2506.21274.pdf", "abs": "https://arxiv.org/abs/2506.21274", "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "categories": ["cs.CL"], "comment": "(Submitted for publication)", "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness"}
{"id": "2506.21285", "pdf": "https://arxiv.org/pdf/2506.21285.pdf", "abs": "https://arxiv.org/abs/2506.21285", "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique."}
{"id": "2506.21288", "pdf": "https://arxiv.org/pdf/2506.21288.pdf", "abs": "https://arxiv.org/abs/2506.21288", "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less"}
{"id": "2506.21294", "pdf": "https://arxiv.org/pdf/2506.21294.pdf", "abs": "https://arxiv.org/abs/2506.21294", "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "authors": ["Bram Willemsen", "Gabriel Skantze"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication at XLLM @ ACL 2025", "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches."}
{"id": "2506.21360", "pdf": "https://arxiv.org/pdf/2506.21360.pdf", "abs": "https://arxiv.org/abs/2506.21360", "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "categories": ["cs.CL"], "comment": "Accepted in CogSci 2025", "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement."}
{"id": "2506.21384", "pdf": "https://arxiv.org/pdf/2506.21384.pdf", "abs": "https://arxiv.org/abs/2506.21384", "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries."}
{"id": "2506.21443", "pdf": "https://arxiv.org/pdf/2506.21443.pdf", "abs": "https://arxiv.org/abs/2506.21443", "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)\\-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk\\-sensitive scenarios. To\naddress these challenges, we present a Domain Knowledge (DK)\\-Enhanced LLM\nframework that integrates pretrained LLMs with structured, task\\-specific\ninsights to perform fraud and concept drift detection. The proposed\narchitecture consists of three main components: (1) a DK\\-LLM module to detect\nfake or deceptive conversations; (2) a drift detection unit (OCDD) to determine\nwhether a semantic shift has occurred; and (3) a second DK\\-LLM module to\nclassify the drift as either benign or fraudulent. We first validate the value\nof domain knowledge using a fake review dataset and then apply our full\nframework to SEConvo, a multiturn dialogue dataset that includes various types\nof fraud and spam attacks. Results show that our system detects fake\nconversations with high accuracy and effectively classifies the nature of\ndrift. Guided by structured prompts, the LLaMA\\-based implementation achieves\n98\\% classification accuracy. Comparative studies against zero\\-shot baselines\ndemonstrate that incorporating domain knowledge and drift awareness\nsignificantly improves performance, interpretability, and robustness in\nhigh\\-stakes NLP applications."}
{"id": "2506.21445", "pdf": "https://arxiv.org/pdf/2506.21445.pdf", "abs": "https://arxiv.org/abs/2506.21445", "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages."}
{"id": "2506.21463", "pdf": "https://arxiv.org/pdf/2506.21463.pdf", "abs": "https://arxiv.org/abs/2506.21463", "title": "Aligning Spoken Dialogue Models from User Interactions", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at ICML 2025", "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems."}
{"id": "2506.21468", "pdf": "https://arxiv.org/pdf/2506.21468.pdf", "abs": "https://arxiv.org/abs/2506.21468", "title": "TopK Language Models", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "categories": ["cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability."}
{"id": "2506.21495", "pdf": "https://arxiv.org/pdf/2506.21495.pdf", "abs": "https://arxiv.org/abs/2506.21495", "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types."}
{"id": "2506.21497", "pdf": "https://arxiv.org/pdf/2506.21497.pdf", "abs": "https://arxiv.org/abs/2506.21497", "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs."}
{"id": "2506.21508", "pdf": "https://arxiv.org/pdf/2506.21508.pdf", "abs": "https://arxiv.org/abs/2506.21508", "title": "skLEP: A Slovak General Language Understanding Benchmark", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "ACL 2025 Findings", "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU."}
{"id": "2506.21521", "pdf": "https://arxiv.org/pdf/2506.21521.pdf", "abs": "https://arxiv.org/abs/2506.21521", "title": "Potemkin Understanding in Large Language Models", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations."}
{"id": "2506.21532", "pdf": "https://arxiv.org/pdf/2506.21532.pdf", "abs": "https://arxiv.org/abs/2506.21532", "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat"}
{"id": "2506.21545", "pdf": "https://arxiv.org/pdf/2506.21545.pdf", "abs": "https://arxiv.org/abs/2506.21545", "title": "Data Efficacy for Language Model Training", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "categories": ["cs.CL"], "comment": null, "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining."}
{"id": "2506.20737", "pdf": "https://arxiv.org/pdf/2506.20737.pdf", "abs": "https://arxiv.org/abs/2506.20737", "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving."}
{"id": "2506.20856", "pdf": "https://arxiv.org/pdf/2506.20856.pdf", "abs": "https://arxiv.org/abs/2506.20856", "title": "Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA", "authors": ["Fei Wang", "Baochun Li"], "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Memorization in large language models (LLMs) makes them vulnerable to data\nextraction attacks. While pre-training memorization has been extensively\nstudied, fewer works have explored its impact in fine-tuning, particularly for\nLoRA fine-tuning, a widely adopted parameter-efficient method.\n  In this work, we re-examine memorization in fine-tuning and uncover a\nsurprising divergence from prior findings across different fine-tuning\nstrategies. Factors such as model scale and data duplication, which strongly\ninfluence memorization in pre-training and full fine-tuning, do not follow the\nsame trend in LoRA fine-tuning. Using a more relaxed similarity-based\nmemorization metric, we demonstrate that LoRA significantly reduces\nmemorization risks compared to full fine-tuning, while still maintaining strong\ntask performance."}
{"id": "2506.20949", "pdf": "https://arxiv.org/pdf/2506.20949.pdf", "abs": "https://arxiv.org/abs/2506.20949", "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents."}
{"id": "2506.20990", "pdf": "https://arxiv.org/pdf/2506.20990.pdf", "abs": "https://arxiv.org/abs/2506.20990", "title": "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes", "authors": ["Yifan Yang", "Zhen Zhang", "Rupak Vignesh Swaminathan", "Jing Liu", "Nathan Susanj", "Zheng Zhang"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Fine-tuning vision language models (VLMs) has achieved remarkable performance\nacross various downstream tasks; yet, it requires access to model gradients\nthrough backpropagation (BP), making them unsuitable for memory-constrained,\ninference-only edge devices. To address this limitation, previous work has\nexplored various BP-free fine-tuning methods. However, these approaches often\nrely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)\noptimization, and often fail to achieve satisfactory performance. In this\npaper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)\napproach, specifically designed to enhance the performance of ZO VLM\nfine-tuning via a sharpness-aware warm-up training. SharpZO features a\ntwo-stage optimization process: a sharpness-aware ES stage that globally\nexplores and smooths the loss landscape to construct a strong initialization,\nfollowed by a fine-grained local search via sparse ZO optimization. The entire\noptimization relies solely on forward passes. Detailed theoretical analysis and\nextensive experiments on CLIP models demonstrate that SharpZO significantly\nimproves accuracy and convergence speed, achieving up to 7% average gain over\nstate-of-the-art forward-only methods."}
{"id": "2506.21071", "pdf": "https://arxiv.org/pdf/2506.21071.pdf", "abs": "https://arxiv.org/abs/2506.21071", "title": "Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph", "authors": ["Jingwei Wang", "Zai Zhang", "Hao Qian", "Chunjing Gan", "Binbin Hu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "Bin Shi", "Bo Dong"], "categories": ["cs.LG", "cs.CL"], "comment": "20 pages, 12 figures", "summary": "Teaching large language models (LLMs) to use tools is crucial for improving\ntheir problem-solving abilities and expanding their applications. However,\neffectively using tools is challenging because it requires a deep understanding\nof tool functionalities and user intentions. Previous methods relied mainly on\nLLMs to generate instruction data, but the quality of these data was often\ninsufficient. In this paper, we propose a new method that uses knowledge graphs\nto generate high-quality instruction data for LLMs. Knowledge graphs are\nmanually curated datasets rich in semantic information. We begin by extracting\nvarious query pathways from a given knowledge graph, which are transformed into\na broad spectrum of user queries. We then translate the relationships between\nentities into actionable tools and parse the pathways of each query into\ndetailed solution steps, thereby creating high-quality instruction data. Our\nexperiments show that fine-tuning on just a small sample of this synthetic data\ncan significantly improve the tool utilization and overall capabilities of\nLLMs."}
{"id": "2506.21103", "pdf": "https://arxiv.org/pdf/2506.21103.pdf", "abs": "https://arxiv.org/abs/2506.21103", "title": "Learning to Skip the Middle Layers of Transformers", "authors": ["Tim Lawson", "Laurence Aitchison"], "categories": ["cs.LG", "cs.CL"], "comment": "11 pages, 2 figures", "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle."}
{"id": "2506.21215", "pdf": "https://arxiv.org/pdf/2506.21215.pdf", "abs": "https://arxiv.org/abs/2506.21215", "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "24 pages, accepted at NeurIPS 2024", "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2."}
{"id": "2506.21220", "pdf": "https://arxiv.org/pdf/2506.21220.pdf", "abs": "https://arxiv.org/abs/2506.21220", "title": "Complexity-aware fine-tuning", "authors": ["Andrey Goncharov", "Daniil Vyazhev", "Petr Sychev", "Edvard Khalafyan", "Alexey Zaytsev"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "General-purpose Large Language Models (LLMs) are frequently fine-tuned\nthrough supervised fine-tuning (SFT) to enhance performance in specific\ndomains. Better results can be achieved by distilling the chain-of-thought of a\nlarger model at the cost of numerous expensive calls and a much greater amount\nof data. We propose a novel blueprint for efficient fine-tuning that uses\nreasoning only for complex data identified by entropy. Specifically, across two\nsmall open models ($\\approx 3B$) we split the training data into complexity\ncategories by a single token answer entropy (ROC AUC $0.73$), fine-tune large\nlanguage models (LLMs) via SFT and distillation, and show that our pipeline\nsignificantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average\naccuracy) and provides comparable with distillation performance while using\n$62\\%$ less data ($0.55$ average accuracy for both). We publish our code and\ndata to facilitate further research in this direction."}
{"id": "2506.21263", "pdf": "https://arxiv.org/pdf/2506.21263.pdf", "abs": "https://arxiv.org/abs/2506.21263", "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster", "authors": ["Ji Qi", "WenPeng Zhu", "Li Li", "Ming Wu", "YingJun Wu", "Wu He", "Xun Gao", "Jason Zeng", "Michael Heinrich"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters."}
{"id": "2506.21277", "pdf": "https://arxiv.org/pdf/2506.21277.pdf", "abs": "https://arxiv.org/abs/2506.21277", "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context", "authors": ["Qize Yang", "Shimin Yao", "Weixuan Chen", "Shenghao Fu", "Detao Bai", "Jiaxing Zhao", "Boyuan Sun", "Bowen Yin", "Xihan Wei", "Jingren Zhou"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models."}
{"id": "2506.21298", "pdf": "https://arxiv.org/pdf/2506.21298.pdf", "abs": "https://arxiv.org/abs/2506.21298", "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "comment": "9 pages, 5 figures", "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations."}
{"id": "2506.21328", "pdf": "https://arxiv.org/pdf/2506.21328.pdf", "abs": "https://arxiv.org/abs/2506.21328", "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts", "authors": ["Jiajie Yang"], "categories": ["cs.LG", "cs.CL"], "comment": "15 pages,4 figures", "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing."}
{"id": "2506.21386", "pdf": "https://arxiv.org/pdf/2506.21386.pdf", "abs": "https://arxiv.org/abs/2506.21386", "title": "Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings", "authors": ["Ghazal Al-Shwayyat", "Omer Nezih Gerek"], "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "comment": null, "summary": "Arabic dialect recognition presents a significant challenge in speech\ntechnology due to the linguistic diversity of Arabic and the scarcity of large\nannotated datasets, particularly for underrepresented dialects. This research\ninvestigates hybrid modeling strategies that integrate classical signal\nprocessing techniques with deep learning architectures to address this problem\nin low-resource scenarios. Two hybrid models were developed and evaluated: (1)\nMel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural\nNetwork (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with\na Recurrent Neural Network (RNN). The models were trained on a dialect-filtered\nsubset of the Common Voice Arabic dataset, with dialect labels assigned based\non speaker metadata. Experimental results demonstrate that the MFCC + CNN\narchitecture achieved superior performance, with an accuracy of 91.2% and\nstrong precision, recall, and F1-scores, significantly outperforming the\nWavelet + RNN configuration, which achieved an accuracy of 66.5%. These\nfindings highlight the effectiveness of leveraging spectral features with\nconvolutional models for Arabic dialect recognition, especially when working\nwith limited labeled data. The study also identifies limitations related to\ndataset size, potential regional overlaps in labeling, and model optimization,\nproviding a roadmap for future research. Recommendations for further\nimprovement include the adoption of larger annotated corpora, integration of\nself-supervised learning techniques, and exploration of advanced neural\narchitectures such as Transformers. Overall, this research establishes a strong\nbaseline for future developments in Arabic dialect recognition within\nresource-constrained environments."}
{"id": "2506.21408", "pdf": "https://arxiv.org/pdf/2506.21408.pdf", "abs": "https://arxiv.org/abs/2506.21408", "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference", "authors": ["Colin Samplawski", "Adam D. Cobb", "Manoj Acharya", "Ramneet Kaur", "Susmit Jha"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at UAI 2025", "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work."}
{"id": "2506.21458", "pdf": "https://arxiv.org/pdf/2506.21458.pdf", "abs": "https://arxiv.org/abs/2506.21458", "title": "Spatial Mental Modeling from Limited Views", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Preprint version", "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space."}
{"id": "2506.21474", "pdf": "https://arxiv.org/pdf/2506.21474.pdf", "abs": "https://arxiv.org/abs/2506.21474", "title": "Logios : An open source Greek Polytonic Optical Character Recognition system", "authors": ["Perifanos Konstantinos", "Goutsos Dionisis"], "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "In this paper, we present an Optical Character Recognition (OCR) system\nspecifically designed for the accurate recognition and digitization of Greek\npolytonic texts. By leveraging the combined strengths of convolutional layers\nfor feature extraction and recurrent layers for sequence learning, our system\naddresses the unique challenges posed by Greek polytonic scripts. This approach\naims to overcome the limitations of traditional OCR methods, offering\nsignificant improvements in accuracy and efficiency. We release the underlying\nmodel as an open-source library and make our OCR platform available for\nacademic use."}
{"id": "2506.21506", "pdf": "https://arxiv.org/pdf/2506.21506.pdf", "abs": "https://arxiv.org/abs/2506.21506", "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jiménez Gutiérrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "categories": ["cs.AI", "cs.CL"], "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems."}
{"id": "2506.21546", "pdf": "https://arxiv.org/pdf/2506.21546.pdf", "abs": "https://arxiv.org/abs/2506.21546", "title": "HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation", "authors": ["Xinzhuo Li", "Adheesh Juvekar", "Xingyou Liu", "Muntasir Wahed", "Kiet A. Nguyen", "Ismini Lourentzou"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project webpage: https://plan-lab.github.io/hallusegbench/", "summary": "Recent progress in vision-language segmentation has significantly advanced\ngrounded visual understanding. However, these models often exhibit\nhallucinations by producing segmentation masks for objects not grounded in the\nimage content or by incorrectly labeling irrelevant regions. Existing\nevaluation protocols for segmentation hallucination primarily focus on label or\ntextual hallucinations without manipulating the visual context, limiting their\ncapacity to diagnose critical failures. In response, we introduce\nHalluSegBench, the first benchmark specifically designed to evaluate\nhallucinations in visual grounding through the lens of counterfactual visual\nreasoning. Our benchmark consists of a novel dataset of 1340 counterfactual\ninstance pairs spanning 281 unique object classes, and a set of newly\nintroduced metrics that quantify hallucination sensitivity under visually\ncoherent scene edits. Experiments on HalluSegBench with state-of-the-art\nvision-language segmentation models reveal that vision-driven hallucinations\nare significantly more prevalent than label-driven ones, with models often\npersisting in false segmentation, highlighting the need for counterfactual\nreasoning to diagnose grounding fidelity."}
{"id": "2308.04386", "pdf": "https://arxiv.org/pdf/2308.04386.pdf", "abs": "https://arxiv.org/abs/2308.04386", "title": "Learning Evaluation Models from Large Language Models for Sequence Generation", "authors": ["Chenglong Wang", "Hang Zhou", "Kaiyan Chang", "Tongran Liu", "Chunliang Zhang", "Quan Du", "Tong Xiao", "Yue Zhang", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted by TASLP 2025", "summary": "Automatic evaluation of sequence generation, traditionally reliant on metrics\nlike BLEU and ROUGE, often fails to capture the semantic accuracy of generated\ntext sequences due to their emphasis on n-gram overlap. A promising solution to\nthis problem is to develop model-based metrics, such as BLEURT and COMET.\nHowever, these approaches are typically hindered by the scarcity of labeled\nevaluation data, which is necessary to train the evaluation models. In this\nwork, we build upon this challenge by proposing the Customized Sequence\nEvaluation Metric (CSEM), a three-stage evaluation model training method that\nutilizes large language models to generate labeled data for model-based metric\ndevelopment, thereby eliminating the need for human-labeled data. Additionally,\nwe expand the scope of CSEM to support various evaluation types, including\nsingle-aspect, multi-aspect, reference-free, and reference-based evaluations,\nenabling the customization of metrics to suit diverse real-world scenarios.\nExperimental results on the SummEval benchmark demonstrate that CSEM can\neffectively train an evaluation model without human-labeled data. Further\nexperiments in reinforcement learning and reranking show that metrics developed\nthrough CSEM outperform traditional evaluation metrics, leading to substantial\nimprovements in sequence quality as evaluated by both commonly used metrics and\nChatGPT."}
{"id": "2405.18113", "pdf": "https://arxiv.org/pdf/2405.18113.pdf", "abs": "https://arxiv.org/abs/2405.18113", "title": "MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting", "authors": ["Hongda Sun", "Hongzhan Lin", "Haiyu Yan", "Yang Song", "Xin Gao", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025 Research Track", "summary": "Online recruitment platforms have reshaped job-seeking and recruiting\nprocesses, driving increased demand for applications that enhance person-job\nmatching. Traditional methods generally rely on analyzing textual data from\nresumes and job descriptions, limiting the dynamic, interactive aspects crucial\nto effective recruitment. Recent advances in Large Language Models (LLMs) have\nrevealed remarkable potential in simulating adaptive, role-based dialogues,\nmaking them well-suited for recruitment scenarios. In this paper, we propose\n\\textbf{MockLLM}, a novel framework to generate and evaluate mock interview\ninteractions. The system consists of two key components: mock interview\ngeneration and two-sided evaluation in handshake protocol. By simulating both\ninterviewer and candidate roles, MockLLM enables consistent and collaborative\ninteractions for real-time and two-sided matching. To further improve the\nmatching quality, MockLLM further incorporates reflection memory generation and\ndynamic strategy modification, refining behaviors based on previous experience.\nWe evaluate MockLLM on real-world data Boss Zhipin, a major Chinese recruitment\nplatform. The experimental results indicate that MockLLM outperforms existing\nmethods in matching accuracy, scalability, and adaptability across job domains,\nhighlighting its potential to advance candidate assessment and online\nrecruitment."}
{"id": "2407.13358", "pdf": "https://arxiv.org/pdf/2407.13358.pdf", "abs": "https://arxiv.org/abs/2407.13358", "title": "Capturing Style in Author and Document Representation", "authors": ["Enzo Terreau", "Antoine Gourru", "Julien Velcin"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A wide range of Deep Natural Language Processing (NLP) models integrates\ncontinuous and low dimensional representations of words and documents.\nSurprisingly, very few models study representation learning for authors. These\nrepresentations can be used for many NLP tasks, such as author identification\nand classification, or in recommendation systems. A strong limitation of\nexisting works is that they do not explicitly capture writing style, making\nthem hardly applicable to literary data. We therefore propose a new\narchitecture based on Variational Information Bottleneck (VIB) that learns\nembeddings for both authors and documents with a stylistic constraint. Our\nmodel fine-tunes a pre-trained document encoder. We stimulate the detection of\nwriting style by adding predefined stylistic features making the representation\naxis interpretable with respect to writing style indicators. We evaluate our\nmethod on three datasets: a literary corpus extracted from the Gutenberg\nProject, the Blog Authorship Corpus and IMDb62, for which we show that it\nmatches or outperforms strong/recent baselines in authorship attribution while\ncapturing much more accurately the authors stylistic aspects."}
{"id": "2409.09510", "pdf": "https://arxiv.org/pdf/2409.09510.pdf", "abs": "https://arxiv.org/abs/2409.09510", "title": "Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for Privacy-Preserving Personalization of Large Language Models", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL"], "comment": null, "summary": "Despite its substantial impact on various search, recommendation, and\nquestion answering tasks, privacy-preserving methods for personalizing large\nlanguage models (LLMs) have received relatively limited exploration. There is\none primary approach in this area through retrieval-augmented generation (RAG),\nwhich generates personalized outputs by enriching the input prompt with\ninformation retrieved from the user's personal data. This paper studies an\northogonal approach to RAG that involves learning user-dependent LLM parameters\nthrough parameter-efficient fine-tuning (PEFT). This paper presents the first\nsystematic study for exploration of PEFT for LLM personalization and provides\nan extensive comparisons between RAG- and PEFT-based solutions, across a broad\nset of seven diverse datasets from the LaMP benchmark. Our results demonstrate\nthat, on average, both RAG- and PEFT-based personalization methods yield 14.92%\nand 1.07% improvements over non-personalized LLMs, respectively. When combining\nRAG with PEFT, we observe a further improvement of 15.98%, highlighting the\neffectiveness of their integration in enhancing personalized text generation.\nAdditionally, we identify a positive correlation between the amount of user\ndata available and the effectiveness of PEFT. This finding suggests that RAG is\nparticularly beneficial for cold-start users -- users with limited personal\ndata -- while PEFT performs better when more user-specific data is available."}
{"id": "2410.09942", "pdf": "https://arxiv.org/pdf/2410.09942.pdf", "abs": "https://arxiv.org/abs/2410.09942", "title": "Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and RAG strategy. We introduce an\niterative approach where the search engine generates retrieval results for the\nRAG agents and gathers feedback on the quality of the retrieved documents\nduring an offline phase. This feedback is then used to iteratively optimize the\nsearch engine using an expectation-maximization algorithm, with the goal of\nmaximizing each agent's utility function. Additionally, we adapt this to an\nonline setting, allowing the search engine to refine its behavior based on\nreal-time individual agents feedback to better serve the results for each of\nthem. Experiments on datasets from the Knowledge-Intensive Language Tasks\n(KILT) benchmark demonstrates that our approach significantly on average\noutperforms baselines across 18 RAG models. We demonstrate that our method\neffectively ``personalizes'' the retrieval for each RAG agent based on the\ncollected feedback. Finally, we provide a comprehensive ablation study to\nexplore various aspects of our method."}
{"id": "2410.16155", "pdf": "https://arxiv.org/pdf/2410.16155.pdf", "abs": "https://arxiv.org/abs/2410.16155", "title": "A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns", "authors": ["Tianyi Men", "Pengfei Cao", "Zhuoran Jin", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "With the development of large language models, they are widely used as agents\nin various fields. A key component of agents is memory, which stores vital\ninformation but is susceptible to jailbreak attacks. Existing research mainly\nfocuses on single-agent attacks and shared memory attacks. However, real-world\nscenarios often involve independent memory. In this paper, we propose the\nTroublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale,\nmulti-agent, multi-topology text-based attack evaluation framework. TMCHT\ninvolves one attacker agent attempting to mislead an entire society of agents.\nWe identify two major challenges in multi-agent attacks: (1) Non-complete graph\nstructure, (2) Large-scale systems. We attribute these challenges to a\nphenomenon we term toxicity disappearing. To address these issues, we propose\nan Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes\nthe retrieval suffix to make poisoned samples more easily retrieved and\noptimizes the replication suffix to make poisoned samples have contagious\nability. We demonstrate the superiority of our approach in TMCHT, with 23.51%,\n18.95%, and 52.93% improvements in line topology, star topology, and 100-agent\nsettings. Encourage community attention to the security of multi-agent systems."}
{"id": "2410.21909", "pdf": "https://arxiv.org/pdf/2410.21909.pdf", "abs": "https://arxiv.org/abs/2410.21909", "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent", "authors": ["Xiao Xia", "Dan Zhang", "Zibo Liao", "Zhenyu Hou", "Tianrui Sun", "Jing Li", "Ling Fu", "Yuxiao Dong"], "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": "Accepted to ACL 2025", "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."}
{"id": "2411.02398", "pdf": "https://arxiv.org/pdf/2411.02398.pdf", "abs": "https://arxiv.org/abs/2411.02398", "title": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages", "authors": ["Hoang H Nguyen", "Khyati Mahajan", "Vikas Yadav", "Julian Salazar", "Philip S. Yu", "Masoud Hashemi", "Rishabh Maheshwary"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to NAACL 2025 (Main Conference). This version contains minor\n  improvements to the camera-ready", "summary": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval."}
{"id": "2411.05199", "pdf": "https://arxiv.org/pdf/2411.05199.pdf", "abs": "https://arxiv.org/abs/2411.05199", "title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement", "authors": ["Leitian Tao", "Xiang Chen", "Tong Yu", "Tung Mai", "Ryan Rossi", "Yixuan Li", "Saayan Mitra"], "categories": ["cs.CL"], "comment": "TMLR 2025", "summary": "Large Language Models (LLMs) have revolutionized code generation but require\nsignificant resources and often over-generalize, limiting their task-specific\nefficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective\nalternative. However, standard supervised approaches rely only on correct\nexamples, missing valuable insights from failures. We introduce CodeLutra, a\nframework that leverages both correct and incorrect code attempts. Instead of\nusing only correct solutions, CodeLutra applies iterative preference-based\nrefinement, comparing successful and failed outputs to better approximate\ndesired results. This approach narrows the performance gap with\nstate-of-the-art larger models without requiring massive datasets or auxiliary\nmodels. For instance, on a challenging data science coding task, using only 500\nsamples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's\nlevel. By learning from both successes and mistakes, CodeLutra provides a\nscalable and efficient path to high-quality code generation, making smaller\nopen-source models more competitive with leading closed-source alternatives."}
{"id": "2412.09587", "pdf": "https://arxiv.org/pdf/2412.09587.pdf", "abs": "https://arxiv.org/abs/2412.09587", "title": "OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages", "authors": ["Chester Palen-Michel", "Maxwell Pickering", "Maya Kruse", "Jonne Sälevä", "Constantine Lignos"], "categories": ["cs.CL"], "comment": "Under review", "summary": "We present OpenNER 1.0, a standardized collection of openly-available named\nentity recognition (NER) datasets. OpenNER contains 36 NER corpora that span 52\nlanguages, human-annotated in varying named entity ontologies. We correct\nannotation format issues, standardize the original datasets into a uniform\nrepresentation with consistent entity type names across corpora, and provide\nthe collection in a structure that enables research in multilingual and\nmulti-ontology NER. We provide baseline results using three pretrained\nmultilingual language models and two large language models to compare the\nperformance of recent models and facilitate future research in NER. We find\nthat no single model is best in all languages and that significant work remains\nto obtain high performance from LLMs on the NER task."}
{"id": "2412.14501", "pdf": "https://arxiv.org/pdf/2412.14501.pdf", "abs": "https://arxiv.org/abs/2412.14501", "title": "Do Large Language Models Advocate for Inferentialism?", "authors": ["Yuzuki Arai", "Sho Tsugawa"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of large language models (LLMs) such as ChatGPT and Claude\npresents new challenges for philosophy of language, particularly regarding the\nnature of linguistic meaning and representation. While LLMs have traditionally\nbeen understood through distributional semantics, this paper explores Robert\nBrandom's inferential semantics as an alternative foundational framework for\nunderstanding these systems. We examine how key features of inferential\nsemantics -- including its anti-representationalist stance, logical\nexpressivism, and quasi-compositional approach -- align with the architectural\nand functional characteristics of Transformer-based LLMs. Through analysis of\nthe ISA (Inference, Substitution, Anaphora) approach, we demonstrate that LLMs\nexhibit fundamentally anti-representationalist properties in their processing\nof language. We further develop a consensus theory of truth appropriate for\nLLMs, grounded in their interactive and normative dimensions through mechanisms\nlike RLHF. While acknowledging significant tensions between inferentialism's\nphilosophical commitments and LLMs' sub-symbolic processing, this paper argues\nthat inferential semantics provides valuable insights into how LLMs generate\nmeaning without reference to external world representations. Our analysis\nsuggests that LLMs may challenge traditional assumptions in philosophy of\nlanguage, including strict compositionality and semantic externalism, though\nfurther empirical investigation is needed to fully substantiate these\ntheoretical claims."}
{"id": "2501.19324", "pdf": "https://arxiv.org/pdf/2501.19324.pdf", "abs": "https://arxiv.org/abs/2501.19324", "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning", "authors": ["Baohao Liao", "Yuhui Xu", "Hanze Dong", "Junnan Li", "Christof Monz", "Silvio Savarese", "Doyen Sahoo", "Caiming Xiong"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD."}
{"id": "2502.15680", "pdf": "https://arxiv.org/pdf/2502.15680.pdf", "abs": "https://arxiv.org/abs/2502.15680", "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training", "authors": ["Jaydeep Borkar", "Matthew Jagielski", "Katherine Lee", "Niloofar Mireshghallah", "David A. Smith", "Christopher A. Choquette-Choo"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted at the Findings of the Association for Computational\n  Linguistics (2025)", "summary": "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation."}
{"id": "2503.21004", "pdf": "https://arxiv.org/pdf/2503.21004.pdf", "abs": "https://arxiv.org/abs/2503.21004", "title": "Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters", "authors": ["Mahmoud Alwakeel", "Emory Buck", "Jonathan G. Martin", "Imran Aslam", "Sudarshan Rajagopal", "Jian Pei", "Mihai V. Podgoreanu", "Christopher J. Lindsell", "An-Kwok Ian Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Pulmonary embolism (PE) registries accelerate practice improving research but\nrely on labor intensive manual abstraction of radiology reports. We examined\nwhether openly available large language models (LLMs) can automate concept\nextraction from computed tomography PE (CTPE) reports without loss of data\nquality. Four Llama 3 variants (3.0 8B, 3.1 8B, 3.1 70B, 3.3 70B) and one\nreviewer model, Phi 4 14B, were tested on 250 dual annotated CTPE reports from\neach of MIMIC IV and Duke University. Accuracy, positive predictive value (PPV)\nand negative predictive value (NPV) versus a human gold standard were measured\nacross model size, temperature and shot count. Mean accuracy rose with scale:\n0.83 (3.0 8B), 0.91 (3.1 8B) and 0.96 for both 70B variants; Phi 4 14B reached\n0.98. Accuracy differed by less than 0.03 between datasets, indicating external\nrobustness. In dual model concordance (L3 70B plus Phi 4 14B) PPV for PE\npresence was at least 0.95 and NPV at least 0.98, while location, thrombus\nburden, right heart strain and image quality artifacts each achieved PPV of at\nleast 0.90 and NPV of at least 0.95. Fewer than four percent of individual\nconcept annotations were discordant, and full agreement occurred in more than\nseventy five percent of reports. Large language models therefore provide a\nscalable, accurate solution for PE registry abstraction, and a dual model\nreview workflow can safeguard data quality with minimal human oversight."}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753.pdf", "abs": "https://arxiv.org/abs/2505.00753", "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Dongyuan Li", "Renhe Jiang", "Xue Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems."}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277.pdf", "abs": "https://arxiv.org/abs/2505.11277", "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "authors": ["Yaorui Shi", "Sihang Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942.pdf", "abs": "https://arxiv.org/abs/2505.12942", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379.pdf", "abs": "https://arxiv.org/abs/2505.13379", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657.pdf", "abs": "https://arxiv.org/abs/2505.21657", "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "authors": ["Zeinab Dehghani", "Mohammed Naveed Akram", "Koorosh Aslansefat", "Adil Khan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The submission contains incorrect references that require substantial\n  revision", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495.pdf", "abs": "https://arxiv.org/abs/2506.01495", "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC."}
{"id": "2506.15830", "pdf": "https://arxiv.org/pdf/2506.15830.pdf", "abs": "https://arxiv.org/abs/2506.15830", "title": "Rethinking LLM Training through Information Geometry and Quantum Metrics", "authors": ["Riccardo Di Sipio"], "categories": ["cs.CL", "quant-ph", "I.2; I.7"], "comment": "9 pages, 1 figure(s)", "summary": "Optimization in large language models (LLMs) unfolds over high-dimensional\nparameter spaces with non-Euclidean structure. Information geometry frames this\nlandscape using the Fisher information metric, enabling more principled\nlearning via natural gradient descent. Though often impractical, this geometric\nlens clarifies phenomena such as sharp minima, generalization, and observed\nscaling laws. We argue that curvature-aware approaches deepen our understanding\nof LLM training. Finally, we speculate on quantum analogies based on the\nFubini-Study metric and Quantum Fisher Information, hinting at efficient\noptimization in quantum-enhanced systems."}
{"id": "2506.19750", "pdf": "https://arxiv.org/pdf/2506.19750.pdf", "abs": "https://arxiv.org/abs/2506.19750", "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "categories": ["cs.CL"], "comment": null, "summary": "Symptom Checkers (SCs) provide medical information tailored to user symptoms.\nA critical challenge in SC development is preventing unexpected performance\ndegradation for individual diseases, especially rare diseases, when updating\nalgorithms. This risk stems from the lack of practical pre-deployment\nevaluation methods. For rare diseases, obtaining sufficient evaluation data\nfrom user feedback is difficult. To evaluate the impact of algorithm updates on\nthe diagnostic performance for individual rare diseases before deployment, this\nstudy proposes and validates a novel Synthetic Vignette Simulation Approach.\nThis approach aims to enable this essential evaluation efficiently and at a low\ncost. To estimate the impact of algorithm updates, we generated synthetic\nvignettes from disease-phenotype annotations in the Human Phenotype Ontology\n(HPO), a publicly available knowledge base for rare diseases curated by\nexperts. Using these vignettes, we simulated SC interviews to predict changes\nin diagnostic performance. The effectiveness of this approach was validated\nretrospectively by comparing the predicted changes with actual performance\nmetrics using the R-squared ($R^2$) coefficient. Our experiment, covering eight\npast algorithm updates for rare diseases, showed that the proposed method\naccurately predicted performance changes for diseases with phenotype frequency\ninformation in HPO (n=5). For these updates, we found a strong correlation for\nboth Recall@8 change ($R^2$ = 0.83,$p$ = 0.031) and Precision@8 change ($R^2$ =\n0.78,$p$ = 0.047). Our proposed method enables the pre-deployment evaluation of\nSC algorithm changes for individual rare diseases. This evaluation is based on\na publicly available medical knowledge database created by experts, ensuring\ntransparency and explainability for stakeholders. Additionally, SC developers\ncan efficiently improve diagnostic performance at a low cost."}
{"id": "2506.20081", "pdf": "https://arxiv.org/pdf/2506.20081.pdf", "abs": "https://arxiv.org/abs/2506.20081", "title": "SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization", "authors": ["Dhruv Gupta", "Gayathri Ganesh Lakshmy", "Yiqing Xie"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Code Generation (RACG) is a critical technique for\nenhancing code generation by retrieving relevant information. In this work, we\nconduct an in-depth analysis of code retrieval by systematically masking\nspecific features while preserving code functionality. Our discoveries include:\n(1) although trained on code, current retrievers heavily rely on surface-level\ntextual features (e.g., docstrings, identifier names), and (2) they exhibit a\nstrong bias towards well-documented code, even if the documentation is\nirrelevant. Based on our discoveries, we propose SACL, a framework that\nenriches textual information and reduces bias by augmenting code or structural\nknowledge with semantic information. Extensive experiments show that SACL\nsubstantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on\nHumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation\nperformance (e.g., by 4.88% Pass@1 on HumanEval)."}
{"id": "2506.20409", "pdf": "https://arxiv.org/pdf/2506.20409.pdf", "abs": "https://arxiv.org/abs/2506.20409", "title": "TAPS: Tool-Augmented Personalisation via Structured Tagging", "authors": ["Ekaterina Taktasheva", "Jeff Dalton"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in tool-augmented large language models have enabled them\nto interact with external tools, enhancing their ability to perform complex\nuser tasks. However, existing approaches overlook the role of personalisation\nin guiding tool use. This work investigates how user preferences can be\neffectively integrated into goal-oriented dialogue agents. Through extensive\nanalysis, we identify key weaknesses in the ability of LLMs to personalise tool\nuse. To this end, we introduce TAPS, a novel solution that enhances\npersonalised tool use by leveraging a structured tagging tool and an\nuncertainty-based tool detector. TAPS significantly improves the ability of\nLLMs to incorporate user preferences, achieving the new state-of-the-art for\nopen source models on the NLSI task."}
{"id": "2506.20639", "pdf": "https://arxiv.org/pdf/2506.20639.pdf", "abs": "https://arxiv.org/abs/2506.20639", "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation", "authors": ["Shansan Gong", "Ruixiang Zhang", "Huangjie Zheng", "Jiatao Gu", "Navdeep Jaitly", "Lingpeng Kong", "Yizhe Zhang"], "categories": ["cs.CL"], "comment": "minor update", "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR bias during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder."}
{"id": "2408.17443", "pdf": "https://arxiv.org/pdf/2408.17443.pdf", "abs": "https://arxiv.org/abs/2408.17443", "title": "HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics", "authors": ["Gueter Josmy Faure", "Jia-Fong Yeh", "Min-Hung Chen", "Hung-Ting Su", "Shang-Hong Lai", "Winston H. Hsu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted for ICCV 2025. Project page:\n  https://joslefaure.github.io/assets/html/hermes.html", "summary": "Long-form video understanding presents unique challenges that extend beyond\ntraditional short-video analysis approaches, particularly in capturing\nlong-range dependencies, processing redundant information efficiently, and\nextracting high-level semantic concepts. To address these challenges, we\npropose a novel approach that more accurately reflects human cognition. This\npaper introduces HERMES: temporal-coHERent long-forM understanding with\nEpisodes and Semantics, featuring two versatile modules that can enhance\nexisting video-language models or operate as a standalone system. Our Episodic\nCOmpressor (ECO) efficiently aggregates representations from micro to\nsemi-macro levels, reducing computational overhead while preserving temporal\ndependencies. Our Semantics ReTRiever (SeTR) enriches these representations\nwith semantic information by focusing on broader context, dramatically reducing\nfeature dimensionality while preserving relevant macro-level information. We\ndemonstrate that these modules can be seamlessly integrated into existing SOTA\nmodels, consistently improving their performance while reducing inference\nlatency by up to 43% and memory usage by 46%. As a standalone system, HERMES\nachieves state-of-the-art performance across multiple long-video understanding\nbenchmarks in both zero-shot and fully-supervised settings."}
{"id": "2412.09925", "pdf": "https://arxiv.org/pdf/2412.09925.pdf", "abs": "https://arxiv.org/abs/2412.09925", "title": "Simulating Hard Attention Using Soft Attention", "authors": ["Andy Yang", "Lena Strobl", "David Chiang", "Dana Angluin"], "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": "19 pages", "summary": "We study conditions under which transformers using soft attention can\nsimulate hard attention, that is, effectively focus all attention on a subset\nof positions. First, we examine several subclasses of languages recognized by\nhard-attention transformers, which can be defined in variants of linear\ntemporal logic. We demonstrate how soft-attention transformers can compute\nformulas of these logics using unbounded positional embeddings or temperature\nscaling. Second, we demonstrate how temperature scaling allows softmax\ntransformers to simulate general hard-attention transformers, using a\ntemperature that depends on the minimum gap between the maximum attention\nscores and other attention scores."}
{"id": "2502.13898", "pdf": "https://arxiv.org/pdf/2502.13898.pdf", "abs": "https://arxiv.org/abs/2502.13898", "title": "GroundCap: A Visually Grounded Image Captioning Dataset", "authors": ["Daniel A. P. Oliveira", "Lourenço Teodoro", "David Martins de Matos"], "categories": ["cs.CV", "cs.CL", "I.2.10; I.2.7"], "comment": "37 pages", "summary": "Current image captioning systems lack the ability to link descriptive text to\nspecific visual elements, making their outputs difficult to verify. While\nrecent approaches offer some grounding capabilities, they cannot track object\nidentities across multiple references or ground both actions and objects\nsimultaneously. We propose a novel ID-based grounding system that enables\nconsistent object reference tracking and action-object linking. We present\nGroundCap, a dataset containing 52,016 images from 77 movies, with 344\nhuman-annotated and 52,016 automatically generated captions. Each caption is\ngrounded on detected objects (132 classes) and actions (51 classes) using a tag\nsystem that maintains object identity while linking actions to the\ncorresponding objects. Our approach features persistent object IDs for\nreference tracking, explicit action-object linking, and the segmentation of\nbackground elements through K-means clustering. We propose gMETEOR, a metric\ncombining caption quality with grounding accuracy, and establish baseline\nperformance by fine-tuning Pixtral-12B and Qwen2.5-VL 7B on GroundCap. Human\nevaluation demonstrates our approach's effectiveness in producing verifiable\ndescriptions with coherent object references."}
{"id": "2503.04065", "pdf": "https://arxiv.org/pdf/2503.04065.pdf", "abs": "https://arxiv.org/abs/2503.04065", "title": "PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks", "authors": ["Feng Ni", "Kui Huang", "Yao Lu", "Wenyu Lv", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}."}
{"id": "2506.15928", "pdf": "https://arxiv.org/pdf/2506.15928.pdf", "abs": "https://arxiv.org/abs/2506.15928", "title": "Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues", "authors": ["Myke C. Cohen", "Zhe Su", "Hsien-Te Kao", "Daniel Nguyen", "Spencer Lynch", "Maarten Sap", "Svitlana Volkova"], "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Under review for KDD 2025 Workshop on Evaluation and Trustworthiness\n  of Agentic and Generative AI Models", "summary": "This paper presents an evaluation framework for agentic AI systems in\nmission-critical negotiation contexts, addressing the need for AI agents that\ncan adapt to diverse human operators and stakeholders. Using Sotopia as a\nsimulation testbed, we present two experiments that systematically evaluated\nhow personality traits and AI agent characteristics influence LLM-simulated\nsocial negotiation outcomes--a capability essential for a variety of\napplications involving cross-team coordination and civil-military interactions.\nExperiment 1 employs causal discovery methods to measure how personality traits\nimpact price bargaining negotiations, through which we found that Agreeableness\nand Extraversion significantly affect believability, goal achievement, and\nknowledge acquisition outcomes. Sociocognitive lexical measures extracted from\nteam communications detected fine-grained differences in agents' empathic\ncommunication, moral foundations, and opinion patterns, providing actionable\ninsights for agentic AI systems that must operate reliably in high-stakes\noperational scenarios. Experiment 2 evaluates human-AI job negotiations by\nmanipulating both simulated human personality and AI system characteristics,\nspecifically transparency, competence, adaptability, demonstrating how AI agent\ntrustworthiness impact mission effectiveness. These findings establish a\nrepeatable evaluation methodology for experimenting with AI agent reliability\nacross diverse operator personalities and human-agent team dynamics, directly\nsupporting operational requirements for reliable AI systems. Our work advances\nthe evaluation of agentic AI workflows by moving beyond standard performance\nmetrics to incorporate social dynamics essential for mission success in complex\noperations."}
{"id": "2506.18959", "pdf": "https://arxiv.org/pdf/2506.18959.pdf", "abs": "https://arxiv.org/abs/2506.18959", "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents", "authors": ["Weizhi Zhang", "Yangning Li", "Yuanchen Bei", "Junyu Luo", "Guancheng Wan", "Liangwei Yang", "Chenxuan Xie", "Yuyao Yang", "Wei-Chieh Huang", "Chunyu Miao", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Yankai Chen", "Chunkit Chan", "Peilin Zhou", "Xinyang Zhang", "Chenwei Zhang", "Jingbo Shang", "Ming Zhang", "Yangqiu Song", "Irwin King", "Philip S. Yu"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research."}
