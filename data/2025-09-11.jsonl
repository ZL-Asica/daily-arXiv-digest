{"id": "2509.08108", "pdf": "https://arxiv.org/pdf/2509.08108.pdf", "abs": "https://arxiv.org/abs/2509.08108", "title": "Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya", "authors": ["Lan Xiao", "Maryam Bandukda", "Franklin Mingzhe Li", "Mark Colley", "Catherine Holloway"], "categories": ["cs.HC"], "comment": null, "summary": "Video content creation offers vital opportunities for expression and\nparticipation, yet remains largely inaccessible to creators with sensory\nimpairments, especially in low-resource settings. We conducted interviews with\n20 video creators with visual and hearing impairments in Kenya to examine their\ntools, challenges, and collaborative practices. Our findings show that\naccessibility barriers and infrastructural limitations shape video creation as\na staged, collaborative process involving trusted human partners and emerging\nAI tools. Across workflows, creators actively negotiated agency and trust,\nmaintaining creative control while bridging sensory gaps. We discuss the need\nfor flexible, interdependent collaboration models, inclusive human-AI\nworkflows, and diverse storytelling practices. This work broadens accessibility\nresearch in HCI by examining how technology and social factors intersect in\nlow-resource contexts, suggesting ways to better support disabled creators\nglobally."}
{"id": "2509.08203", "pdf": "https://arxiv.org/pdf/2509.08203.pdf", "abs": "https://arxiv.org/abs/2509.08203", "title": "Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units", "authors": ["Ryan Lingo", "Rajeev Chhajer", "Martin Arroyo", "Luka Brkljacic", "Ben Davis", "Nithin Santhanam"], "categories": ["cs.HC", "cs.AI", "cs.SE", "I.2.7; H.5.2"], "comment": "12 pages, 4 figures", "summary": "Large Language Models (LLMs) often produce monolithic text that is hard to\nedit in parts, which can slow down collaborative workflows. We present\ncomponentization, an approach that decomposes model outputs into modular,\nindependently editable units while preserving context. We describe Modular and\nAdaptable Output Decomposition (MAOD), which segments responses into coherent\ncomponents and maintains links among them, and we outline the Component-Based\nResponse Architecture (CBRA) as one way to implement this idea. Our reference\nprototype, MAODchat, uses a microservices design with state-machine-based\ndecomposition agents, vendor-agnostic model adapters, and real-time component\nmanipulation with recomposition.\n  In an exploratory study with four participants from academic, engineering,\nand product roles, we observed that component-level editing aligned with\nseveral common workflows and enabled iterative refinement and selective reuse.\nParticipants also mentioned possible team workflows. Our contributions are: (1)\na definition of componentization for transforming monolithic outputs into\nmanipulable units, (2) CBRA and MAODchat as a prototype architecture, (3)\npreliminary observations from a small user study, (4) MAOD as an algorithmic\nsketch for semantic segmentation, and (5) example Agent-to-Agent protocols for\nautomated decomposition. We view componentization as a promising direction for\nturning passive text consumption into more active, component-level\ncollaboration."}
{"id": "2509.08213", "pdf": "https://arxiv.org/pdf/2509.08213.pdf", "abs": "https://arxiv.org/abs/2509.08213", "title": "A Priest, a Rabbi, and an Atheist Walk Into an Error Bar: Religious Meditations on Uncertainty Visualization", "authors": ["Michael Correll", "Lane Harrison"], "categories": ["cs.HC"], "comment": null, "summary": "In this provocation, we suggest that much (although not all) current\nuncertainty visualization simplifies the myriad forms of uncertainty into error\nbars around an estimate. This apparent simplification into error bars comes\nonly as a result of a vast metaphysics around uncertainty and probability\nunderlying modern statistics. We use examples from religion to present\nalternative views of uncertainty (metaphysical or otherwise) with the goal of\nenriching our conception of what kind of uncertainties we ought to visualize,\nand what kinds of people we might be visualizing those uncertainties for."}
{"id": "2509.08353", "pdf": "https://arxiv.org/pdf/2509.08353.pdf", "abs": "https://arxiv.org/abs/2509.08353", "title": "An Adaptive Scoring Framework for Attention Assessment in NDD Children via Serious Games", "authors": ["Abdul Rehman", "Ilona Heldal", "Cristina Costescu", "Carmen David", "Jerry Chun-Wei Lin"], "categories": ["cs.HC"], "comment": null, "summary": "This paper introduces an innovative adaptive scoring framework for children\nwith Neurodevelopmental Disorders (NDD) that is attributed to the integration\nof multiple metrics, such as spatial attention patterns, temporal engagement,\nand game performance data, to create a comprehensive assessment of learning\nthat goes beyond traditional game scoring. The framework employs a progressive\ndifficulty adaptation method, which focuses on specific stimuli for each level\nand adjusts weights dynamically to accommodate increasing cognitive load and\nlearning complexity. Additionally, it includes capabilities for temporal\nanalysis, such as detecting engagement periods, providing rewards for sustained\nattention, and implementing an adaptive multiplier framework based on\nperformance levels. To avoid over-rewarding high performers while maximizing\nimprovement potential for students who are struggling, the designed framework\nfeatures an adaptive temporal impact framework that adjusts performance scales\naccordingly. We also established a multi-metric validation framework using Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and\nSpearman correlation, along with defined quality thresholds for assessing\ndeployment readiness in educational settings. This research bridges the gap\nbetween technical eye-tracking metrics and educational insights by explicitly\nmapping attention patterns to learning behaviors, enabling actionable\npedagogical interventions."}
{"id": "2509.07998", "pdf": "https://arxiv.org/pdf/2509.07998.pdf", "abs": "https://arxiv.org/abs/2509.07998", "title": "Bilingual Word Level Language Identification for Omotic Languages", "authors": ["Mesay Gemeda Yigezu", "Girma Yohannis Bade", "Atnafu Lambebo Tonja", "Olga Kolesnikova", "Grigori Sidorov", "Alexander Gelbukh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language identification is the task of determining the languages for a given\ntext. In many real world scenarios, text may contain more than one language,\nparticularly in multilingual communities. Bilingual Language Identification\n(BLID) is the task of identifying and distinguishing between two languages in a\ngiven text. This paper presents BLID for languages spoken in the southern part\nof Ethiopia, namely Wolaita and Gofa. The presence of words similarities and\ndifferences between the two languages makes the language identification task\nchallenging. To overcome this challenge, we employed various experiments on\nvarious approaches. Then, the combination of the BERT based pretrained language\nmodel and LSTM approach performed better, with an F1 score of 0.72 on the test\nset. As a result, the work will be effective in tackling unwanted social media\nissues and providing a foundation for further research in this area."}
{"id": "2509.08357", "pdf": "https://arxiv.org/pdf/2509.08357.pdf", "abs": "https://arxiv.org/abs/2509.08357", "title": "Personalized Inhibition Training with Eye-Tracking: Enhancing Student Learning and Teacher Assessment in Educational Games", "authors": ["Abdul Rehman", "Ilona Heldal", "Diana Stilwell", "Paula Costa Ferreira", "Jerry Chun-Wei Lin"], "categories": ["cs.HC"], "comment": null, "summary": "Eye tracking (ET) can help to understand visual attention and cognitive\nprocesses in interactive environments. This study presents a comprehensive\neye-tracking analysis framework of the Inhibitory Control Game, named the\nReStroop game, which is an educational intervention aimed at improving\ninhibitory control skills in children through a recycling-themed sorting task,\nfor educational assessment that processes raw gaze data through unified\nalgorithms for fixation detection, performance evaluation, and personalized\nintervention planning. The system employs dual-threshold eye movement detection\n(I-VT and advanced clustering), comprehensive Area of Interest (AOI) analysis,\nand evidence-based risk assessment to transform gaze patterns into actionable\neducational insights. We evaluated this framework across three difficulty\nlevels and revealed critical attention deficits, including low task relevance,\nelevated attention scatter, and compromised processing efficiency. The\nmulti-dimensional risk assessment identified high to moderate risk levels,\ntriggering personalized interventions including focus training, attention\nregulation support, and environmental modifications. The system successfully\ndistinguishes between adaptive learning and cognitive overload, providing early\nwarning indicators for educational intervention. Results demonstrate the\nsystem's effectiveness in objective attention assessment, early risk\nidentification, and the generation of evidence-based recommendations for\nstudents, teachers, and specialists, supporting data-driven educational\ndecision-making and personalized learning approaches."}
{"id": "2509.08000", "pdf": "https://arxiv.org/pdf/2509.08000.pdf", "abs": "https://arxiv.org/abs/2509.08000", "title": "AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs", "authors": ["Debdeep Sanyal", "Manodeep Ray", "Murari Mandal"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "The release of open-weight large language models (LLMs) creates a tension\nbetween advancing accessible research and preventing misuse, such as malicious\nfine-tuning to elicit harmful content. Current safety measures struggle to\npreserve the general capabilities of the LLM while resisting a determined\nadversary with full access to the model's weights and architecture, who can use\nfull-parameter fine-tuning to erase existing safeguards. To address this, we\nintroduce AntiDote, a bi-level optimization procedure for training LLMs to be\nresistant to such tampering. AntiDote involves an auxiliary adversary\nhypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA)\nweights conditioned on the defender model's internal activations. The defender\nLLM is then trained with an objective to nullify the effect of these\nadversarial weight additions, forcing it to maintain its safety alignment. We\nvalidate this approach against a diverse suite of 52 red-teaming attacks,\nincluding jailbreak prompting, latent space manipulation, and direct\nweight-space attacks. AntiDote is upto 27.4\\% more robust against adversarial\nattacks compared to both tamper-resistance and unlearning baselines. Crucially,\nthis robustness is achieved with a minimal trade-off in utility, incurring a\nperformance degradation of upto less than 0.5\\% across capability benchmarks\nincluding MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute\nefficient methodology for building open-weight models where safety is a more\nintegral and resilient property."}
{"id": "2509.08404", "pdf": "https://arxiv.org/pdf/2509.08404.pdf", "abs": "https://arxiv.org/abs/2509.08404", "title": "HyperMOOC: Augmenting MOOC Videos with Concept-based Embedded Visualizations", "authors": ["Li Ye", "Lei Wang", "Lihong Cai", "Ruiqi Yu", "Yong Wang", "Yigang Wang", "Wei Chen", "Zhiguang Zhou"], "categories": ["cs.HC"], "comment": "25 pages,10 figures", "summary": "Massive Open Online Courses (MOOCs) have become increasingly popular\nworldwide. However, learners primarily rely on watching videos, easily losing\nknowledge context and reducing learning effectiveness. We propose HyperMOOC, a\nnovel approach augmenting MOOC videos with concept-based embedded\nvisualizations to help learners maintain knowledge context. Informed by expert\ninterviews and literature review, HyperMOOC employs multi-glyph designs for\ndifferent knowledge types and multi-stage interactions for deeper\nunderstanding. Using a timeline-based radial visualization, learners can grasp\ncognitive paths of concepts and navigate courses through hyperlink-based\ninteractions. We evaluated HyperMOOC through a user study with 36 MOOC learners\nand interviews with two instructors. Results demonstrate that HyperMOOC\nenhances learners' learning effect and efficiency on MOOCs, with participants\nshowing higher satisfaction and improved course understanding compared to\ntraditional video-based learning approaches."}
{"id": "2509.08022", "pdf": "https://arxiv.org/pdf/2509.08022.pdf", "abs": "https://arxiv.org/abs/2509.08022", "title": "MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values", "authors": ["Yao Liang", "Dongcheng Zhao", "Feifei Zhao", "Guobin Shen", "Yuwei Wang", "Dongqi Liang", "Yi Zeng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe and effective deployment across diverse user populations.\nHowever, existing benchmarks often neglect cultural and demographic diversity,\nleading to limited understanding of how value alignment generalizes globally.\nIn this work, we introduce MVPBench, a novel benchmark that systematically\nevaluates LLMs' alignment with multi-dimensional human value preferences across\n75 countries. MVPBench contains 24,020 high-quality instances annotated with\nfine-grained value labels, personalized questions, and rich demographic\nmetadata, making it the most comprehensive resource of its kind to date. Using\nMVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs,\nrevealing substantial disparities in alignment performance across geographic\nand demographic lines. We further demonstrate that lightweight fine-tuning\nmethods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization\n(DPO), can significantly enhance value alignment in both in-domain and\nout-of-domain settings. Our findings underscore the necessity for\npopulation-aware alignment evaluation and provide actionable insights for\nbuilding culturally adaptive and value-sensitive LLMs. MVPBench serves as a\npractical foundation for future research on global alignment, personalized\nvalue modeling, and equitable AI development."}
{"id": "2509.08444", "pdf": "https://arxiv.org/pdf/2509.08444.pdf", "abs": "https://arxiv.org/abs/2509.08444", "title": "GlyphWeaver: Unlocking Glyph Design Creativity with Uniform Glyph DSL and AI", "authors": ["Can Liu", "Shiwei Chen", "Zhibang Jiang", "Yong Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Expressive glyph visualizations provide a powerful and versatile means to\nrepresent complex multivariate data through compact visual encodings, but\ncreating custom glyphs remains challenging due to the gap between design\ncreativity and technical implementation. We present GlyphWeaver, a novel\ninteractive system to enable an easy creation of expressive glyph\nvisualizations. Our system comprises three key components: a glyph\ndomain-specific language (GDSL), a GDSL operation management mechanism, and a\nmultimodal interaction interface. The GDSL is a hierarchical container model,\nwhere each container is independent and composable, providing a rigorous yet\npractical foundation for complex glyph visualizations. The operation management\nmechanism restricts modifications of the GDSL to atomic operations, making it\naccessible without requiring direct coding. The multimodal interaction\ninterface enables direct manipulation, natural language commands, and parameter\nadjustments. A multimodal large language model acts as a translator, converting\nthese inputs into GDSL operations. GlyphWeaver significantly lowers the barrier\nfor designers, who often do not have extensive programming skills, to create\nsophisticated glyph visualizations. A case study and user interviews with 13\nparticipants confirm its substantial gains in design efficiency and\neffectiveness of producing creative glyph visualizations."}
{"id": "2509.08025", "pdf": "https://arxiv.org/pdf/2509.08025.pdf", "abs": "https://arxiv.org/abs/2509.08025", "title": "NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment", "authors": ["Hoang-Trung Nguyen", "Tan-Minh Nguyen", "Xuan-Bach Le", "Tuan-Kiet Le", "Khanh-Huyen Nguyen", "Ha-Thanh Nguyen", "Thi-Hai-Yen Vuong", "Le-Minh Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents the methodologies and results of the NOWJ team's\nparticipation across all five tasks at the COLIEE 2025 competition, emphasizing\nadvancements in the Legal Case Entailment task (Task 2). Our comprehensive\napproach systematically integrates pre-ranking models (BM25, BERT, monoT5),\nembedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large\nLanguage Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance\nscoring, and contextual re-ranking. Specifically, in Task 2, our two-stage\nretrieval system combined lexical-semantic filtering with contextualized LLM\nanalysis, achieving first place with an F1 score of 0.3195. Additionally, in\nother tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal\nTextual Entailment, and Legal Judgment Prediction--we demonstrated robust\nperformance through carefully engineered ensembles and effective prompt-based\nreasoning strategies. Our findings highlight the potential of hybrid models\nintegrating traditional IR techniques with contemporary generative models,\nproviding a valuable reference for future advancements in legal information\nprocessing."}
{"id": "2509.08459", "pdf": "https://arxiv.org/pdf/2509.08459.pdf", "abs": "https://arxiv.org/abs/2509.08459", "title": "Printegrated Circuits: Personal Fabrication of 3D Printed Devices with Embedded PCBs", "authors": ["Oliver Child", "Ollie Hanton", "Jack Dawson", "Steve Hodges", "Mike Fraser"], "categories": ["cs.HC"], "comment": null, "summary": "Consumer-level multi-material 3D printing with conductive thermoplastics\nenables fabrication of interactive elements for bespoke tangible devices.\nHowever, large feature sizes, high resistance materials, and limitations of\nprintable control circuitry mean that deployable devices cannot be printed\nwithout post-print assembly steps. To address these challenges, we present\nPrintegrated Circuits, a technique that uses traditional electronics as\nmaterial to 3D print self-contained interactive objects. Embedded PCBs are\nplaced into recesses during a pause in the print, and through a process we term\n\\textit{Prinjection}, conductive filament is injected into their plated-through\nholes. This automatically creates reliable electrical and mechanical contact,\neliminating the need for manual wiring or bespoke connectors. We describe the\ncustom machine code generation that supports our approach, and characterise its\nelectrical and mechanical properties. With our 6 demonstrations, we highlight\nhow the Printegrated Circuits process fits into existing design and prototyping\nworkflows as well as informs future research agendas."}
{"id": "2509.08032", "pdf": "https://arxiv.org/pdf/2509.08032.pdf", "abs": "https://arxiv.org/abs/2509.08032", "title": "SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery", "authors": ["Fengyu She", "Nan Wang", "Hongfei Wu", "Ziyi Wan", "Jingmian Wang", "Chang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific literature is growing exponentially, creating a critical\nbottleneck for researchers to efficiently synthesize knowledge. While\ngeneral-purpose Large Language Models (LLMs) show potential in text processing,\nthey often fail to capture scientific domain-specific nuances (e.g., technical\njargon, methodological rigor) and struggle with complex scientific tasks,\nlimiting their utility for interdisciplinary research. To address these gaps,\nthis paper presents SciGPT, a domain-adapted foundation model for scientific\nliterature understanding and ScienceBench, an open source benchmark tailored to\nevaluate scientific LLMs.\n  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:\n(1) low-cost domain distillation via a two-stage pipeline to balance\nperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention\nmechanism that cuts memory consumption by 55\\% for 32,000-token long-document\nreasoning; and (3) knowledge-aware adaptation integrating domain ontologies to\nbridge interdisciplinary knowledge gaps.\n  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in\ncore scientific tasks including sequence labeling, generation, and inference.\nIt also exhibits strong robustness in unseen scientific tasks, validating its\npotential to facilitate AI-augmented scientific discovery."}
{"id": "2509.08514", "pdf": "https://arxiv.org/pdf/2509.08514.pdf", "abs": "https://arxiv.org/abs/2509.08514", "title": "Bias in the Loop: How Humans Evaluate AI-Generated Suggestions", "authors": ["Jacob Beck", "Stephanie Eckman", "Christoph Kern", "Frauke Kreuter"], "categories": ["cs.HC", "stat.ML"], "comment": null, "summary": "Human-AI collaboration increasingly drives decision-making across industries,\nfrom medical diagnosis to content moderation. While AI systems promise\nefficiency gains by providing automated suggestions for human review, these\nworkflows can trigger cognitive biases that degrade performance. We know little\nabout the psychological factors that determine when these collaborations\nsucceed or fail. We conducted a randomized experiment with 2,784 participants\nto examine how task design and individual characteristics shape human responses\nto AI-generated suggestions. Using a controlled annotation task, we manipulated\nthree factors: AI suggestion quality in the first three instances, task burden\nthrough required corrections, and performance-based financial incentives. We\ncollected demographics, attitudes toward AI, and behavioral data to assess four\nperformance metrics: accuracy, correction activity, overcorrection, and\nundercorrection. Two patterns emerged that challenge conventional assumptions\nabout human-AI collaboration. First, requiring corrections for flagged AI\nerrors reduced engagement and increased the tendency to accept incorrect\nsuggestions, demonstrating how cognitive shortcuts influence collaborative\noutcomes. Second, individual attitudes toward AI emerged as the strongest\npredictor of performance, surpassing demographic factors. Participants\nskeptical of AI detected errors more reliably and achieved higher accuracy,\nwhile those favorable toward automation exhibited dangerous overreliance on\nalgorithmic suggestions. The findings reveal that successful human-AI\ncollaboration depends not only on algorithmic performance but also on who\nreviews AI outputs and how review processes are structured. Effective human-AI\ncollaborations require consideration of human psychology: selecting diverse\nevaluator samples, measuring attitudes, and designing workflows that counteract\ncognitive biases."}
{"id": "2509.08075", "pdf": "https://arxiv.org/pdf/2509.08075.pdf", "abs": "https://arxiv.org/abs/2509.08075", "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "authors": ["Flor Miriam Plaza-del-Arco", "Paul Röttger", "Nino Scherrer", "Emanuele Borgonovo", "Elmar Plischke", "Dirk Hovy"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors."}
{"id": "2509.08539", "pdf": "https://arxiv.org/pdf/2509.08539.pdf", "abs": "https://arxiv.org/abs/2509.08539", "title": "Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning", "authors": ["Lukas Schach", "Christian Rack", "Ryan P. McMahan", "Marc Erich Latoschik"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This paper examines the generalization capacity of two state-of-the-art\nclassification and similarity learning models in reliably identifying users\nbased on their motions in various Extended Reality (XR) applications. We\ndeveloped a novel dataset containing a wide range of motion data from 49 users\nin five different XR applications: four XR games with distinct tasks and action\npatterns, and an additional social XR application with no predefined task sets.\nThe dataset is used to evaluate the performance and, in particular, the\ngeneralization capacity of the two models across applications. Our results\nindicate that while the models can accurately identify individuals within the\nsame application, their ability to identify users across different XR\napplications remains limited. Overall, our results provide insight into current\nmodels generalization capabilities and suitability as biometric methods for\nuser verification and identification. The results also serve as a much-needed\nrisk assessment of hazardous and unwanted user identification in XR and\nMetaverse applications. Our cross-application XR motion dataset and code are\nmade available to the public to encourage similar research on the\ngeneralization of motion-based user identification in typical Metaverse\napplication use cases."}
{"id": "2509.08093", "pdf": "https://arxiv.org/pdf/2509.08093.pdf", "abs": "https://arxiv.org/abs/2509.08093", "title": "Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression", "authors": ["Nathaniel Imel", "Noga Zaslavsky"], "categories": ["cs.CL"], "comment": null, "summary": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages."}
{"id": "2509.08540", "pdf": "https://arxiv.org/pdf/2509.08540.pdf", "abs": "https://arxiv.org/abs/2509.08540", "title": "Formal verification for robo-advisors: Irrelevant for subjective end-user trust, yet decisive for investment behavior?", "authors": ["Alina Tausch", "Magdalena Wischnewski", "Mustafa Yalciner", "Daniel Neider"], "categories": ["cs.HC"], "comment": null, "summary": "This online-vignette study investigates the impact of certification and\nverification as measures for quality assurance of AI on trust and use of a\nrobo-advisor. Confronting 520 participants with an imaginary situation where\nthey were using an online banking service to invest their inherited money, we\nformed 4 experimental groups. EG1 achieved no further information of their\nrobo-advisor, while EG2 was informed that their robo-advisor was certified by a\nreliable agency for unbiased processes, and EG3 was presented with a formally\nverified robo-advisor that was proven to consider their investment preferences.\nA control group was presented a remote certified human financial advisor. All\ngroups had to decide on how much of their 10,000 euros they would give to their\nadvisor to autonomously invest for them and report on trust and perceived\ndependability. A second manipulation happened afterwards, confronting\nparticipants with either a successful or failed investment. Overall, our\nresults show that the level of quality assurance of the advisor had\nsurprisingly near to no effect of any of our outcome variables, except for\npeople's perception of their own mental model of the advisor. Descriptively,\ndifferences between investments show that seem to favor a verified advisor with\na median investment of 65,000 euros (vs. 50,000). Success or failure\ninformation, though influences only partially by advisor quality, has been\nperceived as a more important clue for advisor trustworthiness, leading to\nsubstantially different trust and dependability ratings. The study shows the\nimportance of thoroughly investigating not only trust, but also trusting\nbehavior with objective measures. It also underlines the need for future\nresearch on formal verification, that might be the gold standard in proving AI\nmathematically, but seems not to take full effect as a cue for trustworthiness\nfor end-users."}
{"id": "2509.08105", "pdf": "https://arxiv.org/pdf/2509.08105.pdf", "abs": "https://arxiv.org/abs/2509.08105", "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion", "authors": ["Kosei Uemura", "David Guzmán", "Quang Phuoc Nguyen", "Jesujoba Oluwadara Alabi", "En-shiun Annie Lee", "David Ifeoluwa Adelani"], "categories": ["cs.CL"], "comment": "under submission", "summary": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings."}
{"id": "2509.08548", "pdf": "https://arxiv.org/pdf/2509.08548.pdf", "abs": "https://arxiv.org/abs/2509.08548", "title": "Embedding Empathy into Visual Analytics: A Framework for Person-Centred Dementia Care", "authors": ["Rhiannon Owen", "Jonathan C. Roberts"], "categories": ["cs.HC"], "comment": "7 pages, 10 figures, accepted for publication in Proceedings IEEE VIS\n  2025", "summary": "Dementia care requires healthcare professionals to balance a patient's\nmedical needs with a deep understanding of their personal needs, preferences,\nand emotional cues. However, current digital tools prioritise quantitative\nmetrics over empathetic engagement,limiting caregivers ability to develop a\ndeeper personal understanding of their patients. This paper presents an empathy\ncentred visualisation framework, developed through a design study, to address\nthis gap. The framework integrates established principles of person centred\ncare with empathy mapping methodologies to encourage deeper engagement. Our\nmethodology provides a structured approach to designing for indirect end users,\npatients whose experience is shaped by a tool they may not directly interact\nwith. To validate the framework, we conducted evaluations with healthcare\nprofessinals, including usability testing of a working prototype and a User\nExperience Questionnaire study. Results suggest the feasibility of the\nframework, with participants highlighting its potential to support a more\npersonal and empathetic relationship between medical staff and patients. The\nwork starts to explore how empathy could be systematically embedded into\nvisualisation design, as we contribute to ongoing efforts in the data\nvisualisation community to support human centred, interpretable, and ethically\naligned clinical care, addressing the urgent need to improve dementia patients\nexperiences in hospital settings."}
{"id": "2509.08146", "pdf": "https://arxiv.org/pdf/2509.08146.pdf", "abs": "https://arxiv.org/abs/2509.08146", "title": "Bias after Prompting: Persistent Discrimination in Large Language Models", "authors": ["Nivedha Sivakumar", "Natalie Mackraz", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks."}
{"id": "2509.08554", "pdf": "https://arxiv.org/pdf/2509.08554.pdf", "abs": "https://arxiv.org/abs/2509.08554", "title": "Acceptability of AI Assistants for Privacy: Perceptions of Experts and Users on Personalized Privacy Assistants", "authors": ["Meihe Xu", "Aurelia Tamò-Larrieux", "Arianna Rossi"], "categories": ["cs.HC"], "comment": null, "summary": "Individuals increasingly face an overwhelming number of tasks and decisions.\nTo cope with the new reality, there is growing research interest in developing\nintelligent agents that can effectively assist people across various aspects of\ndaily life in a tailored manner, with privacy emerging as a particular area of\napplication. Artificial intelligence (AI) assistants for privacy, such as\npersonalized privacy assistants (PPAs), have the potential to automatically\nexecute privacy decisions based on users' pre-defined privacy preferences,\nsparing them the mental effort and time usually spent on each privacy decision.\nThis helps ensure that, even when users feel overwhelmed or resigned about\nprivacy, the decisions made by PPAs still align with their true preferences and\nbest interests. While research has explored possible designs of such agents,\nuser and expert perspectives on the acceptability of such AI-driven solutions\nremain largely unexplored. In this study, we conducted five focus groups with\ndomain experts (n = 11) and potential users (n = 26) to uncover key themes\nshaping the acceptance of PPAs. Factors influencing the acceptability of AI\nassistants for privacy include design elements (such as information sources\nused by the agent), external conditions (such as regulation and literacy\neducation), and systemic conditions (e.g., public or market providers and the\nneed to avoid monopoly) to PPAs. These findings provide theoretical extensions\nto technology acceptance models measuring PPAs, insights on design, and policy\nimplications for PPAs, as well as broader implications for the design of AI\nassistants."}
{"id": "2509.08150", "pdf": "https://arxiv.org/pdf/2509.08150.pdf", "abs": "https://arxiv.org/abs/2509.08150", "title": "Verbalized Algorithms", "authors": ["Supriya Lall", "Christian Farrell", "Hari Pathanjaly", "Marko Pavic", "Sarvesh Chezhian", "Masataro Asai"], "categories": ["cs.CL"], "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks."}
{"id": "2509.08589", "pdf": "https://arxiv.org/pdf/2509.08589.pdf", "abs": "https://arxiv.org/abs/2509.08589", "title": "Visual Analysis of Time-Dependent Observables in Cell Signaling Simulations", "authors": ["Lena Cibulski", "Fiete Haack", "Adelinde Uhrmacher", "Stefan Bruckner"], "categories": ["cs.HC"], "comment": null, "summary": "The ability of a cell to communicate with its environment is essential for\nkey cellular functions like replication, metabolism, or cell fate decisions.\nThe involved molecular mechanisms are highly dynamic and difficult to capture\nexperimentally. Simulation studies offer a valuable means for exploring and\npredicting how cell signaling processes unfold. We present a design study on\nthe visual analysis of such studies to support 1) modelers in calibrating model\nparameters such that the simulated signal responses over time reflect reference\nbehavior from cell biology research and 2) cell biologists in exploring the\ninfluence of receptor trafficking on the efficiency of signal transmission\nwithin the cell. We embed time series plots into parallel coordinates to enable\na simultaneous analysis of model parameters and temporal outputs. A usage\nscenario illustrates how our approach assists with typical tasks such as\nassessing the plausibility of temporal outputs or their sensitivity across\nmodel configurations."}
{"id": "2509.08217", "pdf": "https://arxiv.org/pdf/2509.08217.pdf", "abs": "https://arxiv.org/abs/2509.08217", "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions", "authors": ["Eve Fleisig", "Matthias Orlikowski", "Philipp Cimiano", "Dan Klein"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity."}
{"id": "2509.08689", "pdf": "https://arxiv.org/pdf/2509.08689.pdf", "abs": "https://arxiv.org/abs/2509.08689", "title": "Augmenting speech transcripts of VR recordings with gaze, pointing, and visual context for multimodal coreference resolution", "authors": ["Riccardo Bovo", "Frederik Brudy", "George Fitzmaurice", "Fraser Anderson"], "categories": ["cs.HC"], "comment": null, "summary": "Understanding transcripts of immersive multimodal conversations is\nchallenging because speakers frequently rely on visual context and non-verbal\ncues, such as gestures and visual attention, which are not captured in speech\nalone. This lack of information makes coreferences resolution-the task of\nlinking ambiguous expressions like ``it'' or ``there'' to their intended\nreferents-particularly challenging. In this paper we present a system that\naugments VR speech transcript with eye-tracking laser pointing data, and scene\nmetadata to generate textual descriptions of non-verbal communication and the\ncorresponding objects of interest. To evaluate the system, we collected gaze,\ngesture, and voice data from 12 participants (6 pairs) engaged in an open-ended\ndesign critique of a 3D model of an apartment. Our results show a 26.5\\%\nimprovement in coreference resolution accuracy by a GPT model when using our\nmultimodal transcript compared to a speech-only baseline."}
{"id": "2509.08304", "pdf": "https://arxiv.org/pdf/2509.08304.pdf", "abs": "https://arxiv.org/abs/2509.08304", "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection", "authors": ["Yehudit Aperstein", "Alon Gottlib", "Gal Benita", "Alexander Apartsin"], "categories": ["cs.CL"], "comment": "27 pages, 1 figure", "summary": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility."}
{"id": "2509.08010", "pdf": "https://arxiv.org/pdf/2509.08010.pdf", "abs": "https://arxiv.org/abs/2509.08010", "title": "Measuring and mitigating overreliance is necessary for building human-compatible AI", "authors": ["Lujain Ibrahim", "Katherine M. Collins", "Sunnie S. Y. Kim", "Anka Reuel", "Max Lamparth", "Kevin Feng", "Lama Ahmad", "Prajna Soni", "Alia El Kattan", "Merlin Stein", "Siddharth Swaroop", "Ilia Sucholutsky", "Andrew Strait", "Q. Vera Liao", "Umang Bhatt"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) distinguish themselves from previous\ntechnologies by functioning as collaborative \"thought partners,\" capable of\nengaging more fluidly in natural language. As LLMs increasingly influence\nconsequential decisions across diverse domains from healthcare to personal\nadvice, the risk of overreliance - relying on LLMs beyond their capabilities -\ngrows. This position paper argues that measuring and mitigating overreliance\nmust become central to LLM research and deployment. First, we consolidate risks\nfrom overreliance at both the individual and societal levels, including\nhigh-stakes errors, governance challenges, and cognitive deskilling. Then, we\nexplore LLM characteristics, system design features, and user cognitive biases\nthat - together - raise serious and unique concerns about overreliance in\npractice. We also examine historical approaches for measuring overreliance,\nidentifying three important gaps and proposing three promising directions to\nimprove measurement. Finally, we propose mitigation strategies that the AI\nresearch community can pursue to ensure LLMs augment rather than undermine\nhuman capabilities."}
{"id": "2509.08345", "pdf": "https://arxiv.org/pdf/2509.08345.pdf", "abs": "https://arxiv.org/abs/2509.08345", "title": "Toward Subtrait-Level Model Explainability in Automated Writing Evaluation", "authors": ["Alejandro Andrade-Lotero", "Lee Becker", "Joshua Southerland", "Scott Hellman"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "Subtrait (latent-trait components) assessment presents a promising path\ntoward enhancing transparency of automated writing scores. We prototype\nexplainability and subtrait scoring with generative language models and show\nmodest correlation between human subtrait and trait scores, and between\nautomated and human subtrait scores. Our approach provides details to demystify\nscores for educators and students."}
{"id": "2509.08128", "pdf": "https://arxiv.org/pdf/2509.08128.pdf", "abs": "https://arxiv.org/abs/2509.08128", "title": "Signals in the Noise: Decoding Unexpected Engagement Patterns on Twitter", "authors": ["Yulin Yu", "Houming Chen", "Daniel Romero", "Paramveer S. Dhillon"], "categories": ["cs.SI", "cs.CY", "cs.HC"], "comment": "Proceedings of CSCW 2025", "summary": "Social media platforms offer users multiple ways to engage with\ncontent--likes, retweets, and comments--creating a complex signaling system\nwithin the attention economy. While previous research has examined factors\ndriving overall engagement, less is known about why certain tweets receive\nunexpectedly high levels of one type of engagement relative to others. Drawing\non Signaling Theory and Attention Economy Theory, we investigate these\nunexpected engagement patterns on Twitter (now known as \"X\"), developing an\n\"unexpectedness quotient\" to quantify deviations from predicted engagement\nlevels. Our analysis of over 600,000 tweets reveals distinct patterns in how\ncontent characteristics influence unexpected engagement. News, politics, and\nbusiness tweets receive more retweets and comments than expected, suggesting\nusers prioritize sharing and discussing informational content. In contrast,\ngames and sports-related topics garner unexpected likes and comments,\nindicating higher emotional investment in these domains. The relationship\nbetween content attributes and engagement types follows clear patterns:\nsubjective tweets attract more likes while objective tweets receive more\nretweets, and longer, complex tweets with URLs unexpectedly receive more\nretweets. These findings demonstrate how users employ different engagement\ntypes as signals of varying strength based on content characteristics, and how\ncertain content types more effectively compete for attention in the social\nmedia ecosystem. Our results offer valuable insights for content creators\noptimizing engagement strategies, platform designers facilitating meaningful\ninteractions, and researchers studying online social behavior."}
{"id": "2509.08355", "pdf": "https://arxiv.org/pdf/2509.08355.pdf", "abs": "https://arxiv.org/abs/2509.08355", "title": "Automatic Detection of Inauthentic Templated Responses in English Language Assessments", "authors": ["Yashad Samant", "Lee Becker", "Scott Hellman", "Bradley Behan", "Sarah Hughes", "Joshua Southerland"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting", "summary": "In high-stakes English Language Assessments, low-skill test takers may employ\nmemorized materials called ``templates'' on essay questions to ``game'' or fool\nthe automated scoring system. In this study, we introduce the automated\ndetection of inauthentic, templated responses (AuDITR) task, describe a machine\nlearning-based approach to this task and illustrate the importance of regularly\nupdating these models in production."}
{"id": "2509.08494", "pdf": "https://arxiv.org/pdf/2509.08494.pdf", "abs": "https://arxiv.org/abs/2509.08494", "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants", "authors": ["Benjamin Sturgeon", "Daniel Samuelson", "Jacob Haimes", "Jacy Reese Anthis"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets."}
{"id": "2509.08358", "pdf": "https://arxiv.org/pdf/2509.08358.pdf", "abs": "https://arxiv.org/abs/2509.08358", "title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs", "authors": ["Sergey Pletenev", "Daniil Moskovskiy", "Alexander Panchenko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems."}
{"id": "2509.08676", "pdf": "https://arxiv.org/pdf/2509.08676.pdf", "abs": "https://arxiv.org/abs/2509.08676", "title": "Echo Chambers and Information Brokers on Truth Social: A Study of Network Dynamics and Political Discourse", "authors": ["Emelia May Hughes", "Tim Weninger"], "categories": ["cs.SI", "cs.HC"], "comment": null, "summary": "This study examines the structural dynamics of Truth Social, a politically\naligned social media platform, during two major political events: the U.S.\nSupreme Court's overturning of Roe v. Wade and the FBI's search of Mar-a-Lago.\nUsing a large-scale dataset of user interactions based on re-truths\n(platform-native reposts), we analyze how the network evolves in relation to\nfragmentation, polarization, and user influence. Our findings reveal a\nsegmented and ideologically homogenous structure dominated by a small number of\ncentral figures. Political events prompt temporary consolidation around shared\nnarratives, followed by rapid returns to fragmented, echo-chambered clusters.\nCentrality metrics highlight the disproportionate role of key influencers,\nparticularly @realDonaldTrump, in shaping visibility and directing discourse.\nThese results contribute to research on alternative platforms, political\ncommunication, and online network behavior, demonstrating how infrastructure\nand community dynamics together reinforce ideological boundaries and limit\ncross-cutting engagement."}
{"id": "2509.08381", "pdf": "https://arxiv.org/pdf/2509.08381.pdf", "abs": "https://arxiv.org/abs/2509.08381", "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model", "authors": ["Yu Cheng Chih", "Yong Hao Hou"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER", "summary": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments."}
{"id": "2509.08756", "pdf": "https://arxiv.org/pdf/2509.08756.pdf", "abs": "https://arxiv.org/abs/2509.08756", "title": "Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform", "authors": ["Zhaoxun \"Lorenz\" Liu", "Wagner H. Souza", "Jay Han", "Amin Madani"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,\naccurate patient-hospital allocation decisions under extreme pressure. Here, we\ndeveloped and validated a deep reinforcement learning-based decision-support AI\nagent to optimize patient transfer decisions during simulated MCIs by balancing\npatient acuity levels, specialized care requirements, hospital capacities, and\ntransport logistics. To integrate this AI agent, we developed MasTER, a\nweb-accessible command dashboard for MCI management simulations. Through a\ncontrolled user study with 30 participants (6 trauma experts and 24\nnon-experts), we evaluated three interaction approaches with the AI agent\n(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI\nscenarios in the Greater Toronto Area. Results demonstrate that increasing AI\ninvolvement significantly improves decision quality and consistency. The AI\nagent outperforms trauma surgeons (p < 0.001) and enables non-experts to\nachieve expert-level performance when assisted, contrasting sharply with their\nsignificantly inferior unassisted performance (p < 0.001). These findings\nestablish the potential for our AI-driven decision support to enhance both MCI\npreparedness training and real-world emergency response management."}
{"id": "2509.08438", "pdf": "https://arxiv.org/pdf/2509.08438.pdf", "abs": "https://arxiv.org/abs/2509.08438", "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework", "authors": ["Jinzhong Ning", "Paerhati Tulajiang", "Yingying Le", "Yijia Zhang", "Yuanyuan Sun", "Hongfei Lin", "Haifeng Liu"], "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe."}
{"id": "2501.05434", "pdf": "https://arxiv.org/pdf/2501.05434.pdf", "abs": "https://arxiv.org/abs/2501.05434", "title": "GraspR: A Computational Model of Spatial User Preferences for Adaptive Grasp UI Design", "authors": ["Arthur Caetano", "Yunhao Luo", "Adwait Sharma", "Misha Sra"], "categories": ["cs.HC", "H.5.2"], "comment": null, "summary": "Grasp User Interfaces (grasp UIs) enable dual-tasking in XR by allowing\ninteraction with digital content while holding physical objects. However,\ncurrent grasp UI design practices face a fundamental challenge: existing\napproaches either capture user preferences through labor-intensive elicitation\nstudies that are difficult to scale or rely on biomechanical models that\noverlook subjective factors. We introduce GraspR, the first computational model\nthat predicts user preferences for single-finger microgestures in grasp UIs.\nOur data-driven approach combines the scalability of computational methods with\nhuman preference modeling, trained on 1,520 preferences collected via a\ntwo-alternative forced choice paradigm across eight participants and four\nfrequently used grasp variations. We demonstrate GraspR's effectiveness through\na working prototype that dynamically adjusts interface layouts across four\neveryday tasks. We release both the dataset and code to support future research\nin adaptive grasp UIs."}
{"id": "2509.08463", "pdf": "https://arxiv.org/pdf/2509.08463.pdf", "abs": "https://arxiv.org/abs/2509.08463", "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey", "authors": ["Fanzhen Liu", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Jia Wu", "Jian Yang", "Quan Z. Sheng"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks", "summary": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy."}
{"id": "2501.08500", "pdf": "https://arxiv.org/pdf/2501.08500.pdf", "abs": "https://arxiv.org/abs/2501.08500", "title": "Visual Network Analysis in Immersive Environments: A Survey", "authors": ["Lucas Joos", "Maximilian T. Fischer", "Julius Rauscher", "Daniel A. Keim", "Tim Dwyer", "Falk Schreiber", "Karsten Klein"], "categories": ["cs.HC"], "comment": null, "summary": "The increasing complexity and volume of network data demand effective\nanalysis approaches, with visual exploration proving particularly beneficial.\nImmersive technologies, such as augmented reality, virtual reality, and large\ndisplay walls, have enabled the emerging field of immersive analytics, offering\nnew opportunities to enhance user engagement, spatial awareness, and\nproblem-solving. A growing body of work has explored immersive environments for\nnetwork visualisation, ranging from design studies to fully integrated\napplications across various domains. Despite these advancements, the field\nremains fragmented, lacking a clear description of the design space and a\nstructured overview of the aspects that have already been empirically\nevaluated. To address this gap, we present a survey of visual network analysis\nin immersive environments, covering 138 publications retrieved through a\nstructured pipeline. We systematically analyse the key aspects that define the\ndesign space, investigate their coverage in prior applications (n=87), and\nreview user evaluations (n=59) that provide empirical evidence for essential\ndesign-related questions. By synthesising experimental findings and evaluating\nexisting applications, we identify key achievements, highlight research gaps,\nand offer guidance for the design of future approaches. Additionally, we\nprovide an online resource to explore our results interactively, which will be\nupdated as new developments emerge."}
{"id": "2509.08480", "pdf": "https://arxiv.org/pdf/2509.08480.pdf", "abs": "https://arxiv.org/abs/2509.08480", "title": "Acquiescence Bias in Large Language Models", "authors": ["Daniel Braun"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement."}
{"id": "2501.15463", "pdf": "https://arxiv.org/pdf/2501.15463.pdf", "abs": "https://arxiv.org/abs/2501.15463", "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "authors": ["Hua Shen", "Nicholas Clark", "Tanushree Mitra"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Paper", "summary": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps."}
{"id": "2509.08484", "pdf": "https://arxiv.org/pdf/2509.08484.pdf", "abs": "https://arxiv.org/abs/2509.08484", "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text", "authors": ["Pia Sommerauer", "Giulia Rambelli", "Tommaso Caselli"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP Findings 2025", "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group."}
{"id": "2507.00596", "pdf": "https://arxiv.org/pdf/2507.00596.pdf", "abs": "https://arxiv.org/abs/2507.00596", "title": "Gaze3P: Gaze-Based Prediction of User-Perceived Privacy", "authors": ["Mayar Elfares", "Pascal Reisert", "Ralf Küsters", "Andreas Bulling"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Privacy is a highly subjective concept and perceived variably by different\nindividuals. Previous research on quantifying user-perceived privacy has\nprimarily relied on questionnaires. Furthermore, applying user-perceived\nprivacy to optimise the parameters of privacy-preserving techniques (PPT)\nremains insufficiently explored. To address these limitations, we introduce\nGaze3P -- the first dataset specifically designed to facilitate systematic\ninvestigations into user-perceived privacy. Our dataset comprises gaze data\nfrom 100 participants and 1,000 stimuli, encompassing a range of private and\nsafe attributes. With Gaze3P, we train a machine learning model to implicitly\nand dynamically predict perceived privacy from human eye gaze. Through\ncomprehensive experiments, we show that the resulting models achieve high\naccuracy. Finally, we illustrate how predicted privacy can be used to optimise\nthe parameters of differentially private mechanisms, thereby enhancing their\nalignment with user expectations."}
{"id": "2509.08486", "pdf": "https://arxiv.org/pdf/2509.08486.pdf", "abs": "https://arxiv.org/abs/2509.08486", "title": "Too Helpful, Too Harmless, Too Honest or Just Right?", "authors": ["Gautam Siddharth Kashyap", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL"], "comment": "EMNLP'25 Main", "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones."}
{"id": "2508.00300", "pdf": "https://arxiv.org/pdf/2508.00300.pdf", "abs": "https://arxiv.org/abs/2508.00300", "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains."}
{"id": "2509.08541", "pdf": "https://arxiv.org/pdf/2509.08541.pdf", "abs": "https://arxiv.org/abs/2509.08541", "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data."}
{"id": "2509.06776", "pdf": "https://arxiv.org/pdf/2509.06776.pdf", "abs": "https://arxiv.org/abs/2509.06776", "title": "Hue4U: Real-Time Personalized Color Correction in Augmented Reality", "authors": ["Jingwen Qin", "Semen Checherin", "Yue Li", "Berend-Jan van der Zwaag", "Ozlem Durmaz-Incel"], "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent\nof women worldwide. Existing color-correction methods often rely on prior\nclinical diagnosis and static filtering, making them less effective for users\nwith mild or moderate CVD. In this paper, we introduce Hue4U, a personalized,\nreal-time color-correction system in augmented reality using consumer-grade\nMeta Quest headsets. Unlike previous methods, Hue4U requires no prior medical\ndiagnosis and adapts to the user in real time. A user study with 10\nparticipants showed notable improvements in their ability to distinguish\ncolors. The results demonstrated large effect sizes (Cohen's d > 1.4),\nsuggesting clinically meaningful gains for individuals with CVD. These findings\nhighlight the potential of personalized AR interventions to improve visual\naccessibility and quality of life for people affected by CVD."}
{"id": "2509.08596", "pdf": "https://arxiv.org/pdf/2509.08596.pdf", "abs": "https://arxiv.org/abs/2509.08596", "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge", "authors": ["Dima Galat", "Diego Molla-Aliod"], "categories": ["cs.CL"], "comment": "CEUR-WS, CLEF2025", "summary": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering."}
{"id": "2407.03451", "pdf": "https://arxiv.org/pdf/2407.03451.pdf", "abs": "https://arxiv.org/abs/2407.03451", "title": "The Role of Privacy Guarantees in Voluntary Donation of Private Health Data for Altruistic Goals", "authors": ["Ruizhe Wang", "Roberta De Viti", "Aarushi Dubey", "Elissa M. Redmiles"], "categories": ["cs.CR", "cs.HC"], "comment": "Accepted by NDSS 2026", "summary": "The voluntary donation of private health information for altruistic purposes,\nsuch as supporting research advancements, is a common practice. However,\nconcerns about data misuse and leakage may deter people from donating their\ninformation. Privacy Enhancement Technologies (PETs) aim to alleviate these\nconcerns and in turn allow for safe and private data sharing. This study\nconducts a vignette survey (N=494) with participants recruited from Prolific to\nexamine the willingness of US-based people to donate medical data for\ndeveloping new treatments under four general guarantees offered across PETs:\ndata expiration, anonymization, purpose restriction, and access control. The\nstudy explores two mechanisms for verifying these guarantees: self-auditing and\nexpert auditing, and controls for the impact of confounds including\ndemographics and two types of data collectors: for-profit and non-profit\ninstitutions.\n  Our findings reveal that respondents hold such high expectations of privacy\nfrom non-profit entities a priori that explicitly outlining privacy protections\nhas little impact on their overall perceptions. In contrast, offering privacy\nguarantees elevates respondents' expectations of privacy for for-profit\nentities, bringing them nearly in line with those for non-profit organizations.\nFurther, while the technical community has suggested audits as a mechanism to\nincrease trust in PET guarantees, we observe limited effect from transparency\nabout such audits. We emphasize the risks associated with these findings and\nunderscore the critical need for future interdisciplinary research efforts to\nbridge the gap between the technical community's and end-users' perceptions\nregarding the effectiveness of auditing PETs."}
{"id": "2509.08604", "pdf": "https://arxiv.org/pdf/2509.08604.pdf", "abs": "https://arxiv.org/abs/2509.08604", "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications", "authors": ["Anran Li", "Lingfei Qian", "Mengmeng Du", "Yu Yin", "Yan Hu", "Zihao Sun", "Yihang Fu", "Erica Stutz", "Xuguang Ai", "Qianqian Xie", "Rui Zhu", "Jimin Huang", "Yifan Yang", "Siru Liu", "Yih-Chung Tham", "Lucila Ohno-Machado", "Hyunghoon Cho", "Zhiyong Lu", "Hua Xu", "Qingyu Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information."}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582.pdf", "abs": "https://arxiv.org/abs/2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chenyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyu Liu", "Kwan-Liu Ma"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems."}
{"id": "2509.08612", "pdf": "https://arxiv.org/pdf/2509.08612.pdf", "abs": "https://arxiv.org/abs/2509.08612", "title": "OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis", "authors": ["Xinfeng Liao", "Xuanqi Chen", "Lianxi Wang", "Jiahuan Yang", "Zhuowei Chen", "Ziying Rong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics effectively identify aspect sentiment, existing methods\nrelying on syntax trees and aspect-aware attention struggle to model complex\nsemantic relationships. Their dependence on linear dot-product features fails\nto capture nonlinear associations, allowing noisy similarity from irrelevant\nwords to obscure key opinion terms. Motivated by Differentiable Optimal\nMatching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), which introduces a Syntactic-Semantic Collaborative\nAttention. It comprises a Syntactic Graph-Aware Attention for mining latent\nsyntactic dependencies and modeling global syntactic topology, as well as a\nSemantic Optimal Transport Attention designed to uncover fine-grained semantic\nalignments amidst textual noise, thereby accurately capturing sentiment signals\nobscured by irrelevant tokens. A Adaptive Attention Fusion module integrates\nthese heterogeneous features, and contrastive regularization further improves\nrobustness. Experiments demonstrate that OTESGN achieves state-of-the-art\nresults, outperforming previous best models by +1.01% F1 on Twitter and +1.30%\nF1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its\nefficacy in precise localization of opinion words and noise resistance."}
{"id": "2509.08729", "pdf": "https://arxiv.org/pdf/2509.08729.pdf", "abs": "https://arxiv.org/abs/2509.08729", "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates", "authors": ["Hyunjun Kim", "Junwoo Ha", "Sangyoon Yu", "Haon Park"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming."}
{"id": "2509.08753", "pdf": "https://arxiv.org/pdf/2509.08753.pdf", "abs": "https://arxiv.org/abs/2509.08753", "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling", "authors": ["Neil Zeghidour", "Eugene Kharitonov", "Manu Orsini", "Václav Volhejn", "Gabriel de Marmiesse", "Edouard Grave", "Patrick Pérez", "Laurent Mazaré", "Alexandre Défossez"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling"}
{"id": "2509.08778", "pdf": "https://arxiv.org/pdf/2509.08778.pdf", "abs": "https://arxiv.org/abs/2509.08778", "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms", "authors": ["Minyeong Choe", "Haehyun Cho", "Changho Seo", "Hyunil Kim"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "Understanding how Transformer-based language models store and retrieve\nfactual associations is critical for improving interpretability and enabling\ntargeted model editing. Prior work, primarily on GPT-style models, has\nidentified MLP modules in early layers as key contributors to factual recall.\nHowever, it remains unclear whether these findings generalize across different\nautoregressive architectures. To address this, we conduct a comprehensive\nevaluation of factual recall across several models -- including GPT, LLaMA,\nQwen, and DeepSeek -- analyzing where and how factual information is encoded\nand accessed. Consequently, we find that Qwen-based models behave differently\nfrom previous patterns: attention modules in the earliest layers contribute\nmore to factual recall than MLP modules. Our findings suggest that even within\nthe autoregressive Transformer family, architectural variations can lead to\nfundamentally different mechanisms of factual recall."}
{"id": "2509.08809", "pdf": "https://arxiv.org/pdf/2509.08809.pdf", "abs": "https://arxiv.org/abs/2509.08809", "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals", "authors": ["Cheng Chen", "Haiyan Yin", "Ivor Tsang"], "categories": ["cs.CL"], "comment": "11 pages, 10 figures", "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings."}
{"id": "2509.08812", "pdf": "https://arxiv.org/pdf/2509.08812.pdf", "abs": "https://arxiv.org/abs/2509.08812", "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages", "authors": ["Hailay Kidu Teklehaymanot", "Dren Fazlija", "Wolfgang Nejdl"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": "This submission is approximately 10 pages in length and includes 1\n  figure and 6 tables", "summary": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC"}
{"id": "2509.08824", "pdf": "https://arxiv.org/pdf/2509.08824.pdf", "abs": "https://arxiv.org/abs/2509.08824", "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "Helio Pedrini"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development."}
{"id": "2509.08825", "pdf": "https://arxiv.org/pdf/2509.08825.pdf", "abs": "https://arxiv.org/abs/2509.08825", "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation", "authors": ["Joachim Baumann", "Paul Röttger", "Aleksandra Urman", "Albert Wendsjö", "Flor Miriam Plaza-del-Arco", "Johannes B. Gruber", "Dirk Hovy"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant."}
{"id": "2509.08827", "pdf": "https://arxiv.org/pdf/2509.08827.pdf", "abs": "https://arxiv.org/abs/2509.08827", "title": "A Survey of Reinforcement Learning for Large Reasoning Models", "authors": ["Kaiyan Zhang", "Yuxin Zuo", "Bingxiang He", "Youbang Sun", "Runze Liu", "Che Jiang", "Yuchen Fan", "Kai Tian", "Guoli Jia", "Pengfei Li", "Yu Fu", "Xingtai Lv", "Yuchen Zhang", "Sihang Zeng", "Shang Qu", "Haozhan Li", "Shijie Wang", "Yuru Wang", "Xinwei Long", "Fangfu Liu", "Xiang Xu", "Jiaze Ma", "Xuekai Zhu", "Ermo Hua", "Yihao Liu", "Zonglin Li", "Huayu Chen", "Xiaoye Qu", "Yafu Li", "Weize Chen", "Zhenzhao Yuan", "Junqi Gao", "Dong Li", "Zhiyuan Ma", "Ganqu Cui", "Zhiyuan Liu", "Biqing Qi", "Ning Ding", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"}
{"id": "2509.08010", "pdf": "https://arxiv.org/pdf/2509.08010.pdf", "abs": "https://arxiv.org/abs/2509.08010", "title": "Measuring and mitigating overreliance is necessary for building human-compatible AI", "authors": ["Lujain Ibrahim", "Katherine M. Collins", "Sunnie S. Y. Kim", "Anka Reuel", "Max Lamparth", "Kevin Feng", "Lama Ahmad", "Prajna Soni", "Alia El Kattan", "Merlin Stein", "Siddharth Swaroop", "Ilia Sucholutsky", "Andrew Strait", "Q. Vera Liao", "Umang Bhatt"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) distinguish themselves from previous\ntechnologies by functioning as collaborative \"thought partners,\" capable of\nengaging more fluidly in natural language. As LLMs increasingly influence\nconsequential decisions across diverse domains from healthcare to personal\nadvice, the risk of overreliance - relying on LLMs beyond their capabilities -\ngrows. This position paper argues that measuring and mitigating overreliance\nmust become central to LLM research and deployment. First, we consolidate risks\nfrom overreliance at both the individual and societal levels, including\nhigh-stakes errors, governance challenges, and cognitive deskilling. Then, we\nexplore LLM characteristics, system design features, and user cognitive biases\nthat - together - raise serious and unique concerns about overreliance in\npractice. We also examine historical approaches for measuring overreliance,\nidentifying three important gaps and proposing three promising directions to\nimprove measurement. Finally, we propose mitigation strategies that the AI\nresearch community can pursue to ensure LLMs augment rather than undermine\nhuman capabilities."}
{"id": "2509.08182", "pdf": "https://arxiv.org/pdf/2509.08182.pdf", "abs": "https://arxiv.org/abs/2509.08182", "title": "XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols", "authors": ["Faruk Alpay", "Taylan Alpay"], "categories": ["cs.PL", "cs.AI", "cs.CL", "03B70, 06B23, 47H10, 68T27, 68T50", "I.2.7; I.2.8; F.4.1; F.4.3; H.5.2"], "comment": "7 pages, multiple XML prompts", "summary": "Structured prompting with XML tags has emerged as an effective way to steer\nlarge language models (LLMs) toward parseable, schema-adherent outputs in\nreal-world systems. We develop a logic-first treatment of XML prompting that\nunifies (i) grammar-constrained decoding, (ii) fixed-point semantics over\nlattices of hierarchical prompts, and (iii) convergent human-AI interaction\nloops. We formalize a complete lattice of XML trees under a refinement order\nand prove that monotone prompt-to-prompt operators admit least fixed points\n(Knaster-Tarski) that characterize steady-state protocols; under a task-aware\ncontraction metric on trees, we further prove Banach-style convergence of\niterative guidance. We instantiate these results with context-free grammars\n(CFGs) for XML schemas and show how constrained decoding guarantees\nwell-formedness while preserving task performance. A set of multi-layer\nhuman-AI interaction recipes demonstrates practical deployment patterns,\nincluding multi-pass \"plan $\\to$ verify $\\to$ revise\" routines and agentic tool\nuse. We provide mathematically complete proofs and tie our framework to recent\nadvances in grammar-aligned decoding, chain-of-verification, and programmatic\nprompting."}
{"id": "2509.08315", "pdf": "https://arxiv.org/pdf/2509.08315.pdf", "abs": "https://arxiv.org/abs/2509.08315", "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference", "authors": ["Bohan Yu", "Yekun Chai"], "categories": ["cs.LG", "cs.CL", "cs.NE"], "comment": null, "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."}
{"id": "2509.08494", "pdf": "https://arxiv.org/pdf/2509.08494.pdf", "abs": "https://arxiv.org/abs/2509.08494", "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants", "authors": ["Benjamin Sturgeon", "Daniel Samuelson", "Jacob Haimes", "Jacy Reese Anthis"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets."}
{"id": "2509.08653", "pdf": "https://arxiv.org/pdf/2509.08653.pdf", "abs": "https://arxiv.org/abs/2509.08653", "title": "Generative Data Refinement: Just Ask for Better Data", "authors": ["Minqi Jiang", "João G. M. Araújo", "Will Ellsworth", "Sian Gooding", "Edward Grefenstette"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "For a fixed parameter size, the capabilities of large models are primarily\ndetermined by the quality and quantity of its training data. Consequently,\ntraining datasets now grow faster than the rate at which new data is indexed on\nthe web, leading to projected data exhaustion over the next decade. Much more\ndata exists as user-generated content that is not publicly indexed, but\nincorporating such data comes with considerable risks, such as leaking private\ninformation and other undesirable content. We introduce a framework, Generative\nData Refinement (GDR), for using pretrained generative models to transform a\ndataset with undesirable content into a refined dataset that is more suitable\nfor training. Our experiments show that GDR can outperform industry-grade\nsolutions for dataset anonymization, as well as enable direct detoxification of\nhighly unsafe datasets. Moreover, we show that by generating synthetic data\nthat is conditioned on each example in the real dataset, GDR's refined outputs\nnaturally match the diversity of web scale datasets, and thereby avoid the\noften challenging task of generating diverse synthetic data via model\nprompting. The simplicity and effectiveness of GDR make it a powerful tool for\nscaling up the total stock of training data for frontier models."}
{"id": "2509.08755", "pdf": "https://arxiv.org/pdf/2509.08755.pdf", "abs": "https://arxiv.org/abs/2509.08755", "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning", "authors": ["Zhiheng Xi", "Jixuan Huang", "Chenyang Liao", "Baodai Huang", "Honglin Guo", "Jiaqi Liu", "Rui Zheng", "Junjie Ye", "Jiazheng Zhang", "Wenxiang Chen", "Wei He", "Yiwen Ding", "Guanyu Li", "Zehui Chen", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Tao Gui", "Zuxuan Wu", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL", "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents."}
{"id": "2509.08777", "pdf": "https://arxiv.org/pdf/2509.08777.pdf", "abs": "https://arxiv.org/abs/2509.08777", "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles", "authors": ["Eric Slyman", "Mehrab Tanjim", "Kushal Kafle", "Stefan Lee"], "categories": ["cs.CV", "cs.CL"], "comment": "17 pages, 8 figures, Accepted at ICCV 2025", "summary": "Multimodal large language models (MLLMs) are increasingly used to evaluate\ntext-to-image (TTI) generation systems, providing automated judgments based on\nvisual and textual context. However, these \"judge\" models often suffer from\nbiases, overconfidence, and inconsistent performance across diverse image\ndomains. While prompt ensembling has shown promise for mitigating these issues\nin unimodal, text-only settings, our experiments reveal that standard\nensembling methods fail to generalize effectively for TTI tasks. To address\nthese limitations, we propose a new multimodal-aware method called Multimodal\nMixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt\nensemble approach augmented by image clustering, allowing the judge to\ndynamically assign prompt weights based on the visual characteristics of each\nsample. We show that MMB improves accuracy in pairwise preference judgments and\ngreatly enhances calibration, making it easier to gauge the judge's true\nuncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB\noutperforms existing baselines in alignment with human annotations and\ncalibration across varied image content. Our findings highlight the importance\nof multimodal-specific strategies for judge calibration and suggest a promising\npath forward for reliable large-scale TTI evaluation."}
{"id": "2509.08803", "pdf": "https://arxiv.org/pdf/2509.08803.pdf", "abs": "https://arxiv.org/abs/2509.08803", "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking", "authors": ["Ihsan A. Qazi", "Zohaib Khan", "Abdullah Ghani", "Agha A. Raza", "Zafar A. Qazi", "Wassay Sajjad", "Ayesha Ali", "Asher Javaid", "Muhammad Abdullah Sohail", "Abdul H. Azeemi"], "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.CY"], "comment": "65 pages, 26 figures, 6 tables", "summary": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking."}
{"id": "2509.08814", "pdf": "https://arxiv.org/pdf/2509.08814.pdf", "abs": "https://arxiv.org/abs/2509.08814", "title": "Merge-of-Thought Distillation", "authors": ["Zhanming Shen", "Zeyu Qin", "Zenan Huang", "Hao Chen", "Jiaqi Hu", "Yihong Zhuang", "Guoshan Lu", "Gang Chen", "Junbo Zhao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students."}
{"id": "2407.13729", "pdf": "https://arxiv.org/pdf/2407.13729.pdf", "abs": "https://arxiv.org/abs/2407.13729", "title": "Baba Is AI: Break the Rules to Beat the Benchmark", "authors": ["Nathan Cloos", "Meagan Jens", "Michelangelo Naim", "Yen-Ling Kuo", "Ignacio Cases", "Andrei Barbu", "Christopher J. Cueva"], "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Humans solve problems by following existing rules and procedures, and also by\nleaps of creativity to redefine those rules and objectives. To probe these\nabilities, we developed a new benchmark based on the game Baba Is You where an\nagent manipulates both objects in the environment and rules, represented by\nmovable tiles with words written on them, to reach a specified goal and win the\ngame. We test three state-of-the-art multi-modal large language models (OpenAI\nGPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail\ndramatically when generalization requires that the rules of the game must be\nmanipulated and combined."}
{"id": "2410.07473", "pdf": "https://arxiv.org/pdf/2410.07473.pdf", "abs": "https://arxiv.org/abs/2410.07473", "title": "Localizing Factual Inconsistencies in Attributable Text Generation", "authors": ["Arie Cattan", "Paul Roit", "Shiyue Zhang", "David Wan", "Roee Aharoni", "Idan Szpektor", "Mohit Bansal", "Ido Dagan"], "categories": ["cs.CL"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement. This benchmark includes more than 3K instances spanning various\ntasks of attributable text generation. We also show that QASemConsistency\nyields factual consistency scores that correlate well with human judgments.\nFinally, we implement several methods for automatically detecting localized\nfactual inconsistencies, with both supervised entailment models and LLMs."}
{"id": "2412.14161", "pdf": "https://arxiv.org/pdf/2412.14161.pdf", "abs": "https://arxiv.org/abs/2412.14161", "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "authors": ["Frank F. Xu", "Yufan Song", "Boxuan Li", "Yuxuan Tang", "Kritanjali Jain", "Mengxue Bao", "Zora Z. Wang", "Xuhui Zhou", "Zhitong Guo", "Murong Cao", "Mingyang Yang", "Hao Yang Lu", "Amaad Martin", "Zhe Su", "Leander Maben", "Raj Mehta", "Wayne Chi", "Lawrence Jang", "Yiqing Xie", "Shuyan Zhou", "Graham Neubig"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com."}
{"id": "2501.12051", "pdf": "https://arxiv.org/pdf/2501.12051.pdf", "abs": "https://arxiv.org/abs/2501.12051", "title": "MedS$^3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision", "authors": ["Shuyang Jiang", "Yusheng Liao", "Zhe Chen", "Ya Zhang", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": "20 pages;", "summary": "Medical language models face critical barriers to real-world clinical\nreasoning applications. However, mainstream efforts, which fall short in task\ncoverage, lack fine-grained supervision for intermediate reasoning steps, and\nrely on proprietary systems, are still far from a versatile, credible and\nefficient language model for clinical reasoning usage. To this end, we propose\n\\mone, a self-evolving framework that imparts robust reasoning capabilities to\nsmall, deployable models. Starting with 8,000 curated instances sampled via a\ncurriculum strategy across five medical domains and 16 datasets, we use a small\nbase policy model to conduct Monte Carlo Tree Search (MCTS) for constructing\nrule-verifiable reasoning trajectories. Self-explored reasoning trajectories\nranked by node values are used to bootstrap the policy model via reinforcement\nfine-tuning and preference learning. Moreover, we introduce a soft dual process\nreward model that incorporates value dynamics: steps that degrade node value\nare penalized, enabling fine-grained identification of reasoning errors even\nwhen the final answer is correct. Experiments on eleven benchmarks show that\n\\mone outperforms the previous state-of-the-art medical model by +6.45 accuracy\npoints and surpasses 32B-scale general-purpose reasoning models by +8.57\npoints. Additional empirical analysis further demonstrates that \\mone achieves\nrobust and faithful reasoning behavior."}
{"id": "2502.02390", "pdf": "https://arxiv.org/pdf/2502.02390.pdf", "abs": "https://arxiv.org/abs/2502.02390", "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning", "authors": ["Jianfeng Pan", "Senyou Deng", "Shaomang Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 10 figures", "summary": "Research on LLM technologies is rapidly emerging, with most of them employ a\n'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. We validate CoAT's\neffectiveness across a variety of generative and reasoning tasks. Quantitative\nexperiments show that CoAT achieves over 10% performance improvement on\nopen-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15%\ngain on our proprietary CRB dataset."}
{"id": "2502.08395", "pdf": "https://arxiv.org/pdf/2502.08395.pdf", "abs": "https://arxiv.org/abs/2502.08395", "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance", "authors": ["Paul Röttger", "Musashi Hinck", "Valentin Hofmann", "Kobi Hackenburg", "Valentina Pyatkin", "Faeze Brahman", "Dirk Hovy"], "categories": ["cs.CL"], "comment": "accepted at TACL (pre-MIT Press publication version)", "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs manifest in real user interactions, making it difficult to\naddress the risks from biased LLMs. Therefore, we create IssueBench: a set of\n2.49m realistic English-language prompts to measure issue bias in LLM writing\nassistance, which we construct based on 3.9k templates (e.g. \"write a blog\nabout\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in 10 state-of-the-art LLMs. We also show that biases are very\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."}
{"id": "2502.12737", "pdf": "https://arxiv.org/pdf/2502.12737.pdf", "abs": "https://arxiv.org/abs/2502.12737", "title": "Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation", "authors": ["Shengxiang Gao", "Jey Han Lau", "Jianzhong Qi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Knowledge base question answering (KBQA) aims to answer user questions in\nnatural language using rich human knowledge stored in large KBs. As current\nKBQA methods struggle with unseen knowledge base elements at test time,we\nintroduce SG-KBQA: a novel model that injects schema contexts into entity\nretrieval and logical form generation to tackle this issue. It uses the richer\nsemantics and awareness of the knowledge base structure provided by schema\ncontexts to enhance generalizability. We show that SG-KBQA achieves strong\ngeneralizability, outperforming state-of-the-art models on two commonly used\nbenchmark datasets across a variety of test settings. Our source code is\navailable at https://github.com/gaosx2000/SG_KBQA."}
{"id": "2502.16523", "pdf": "https://arxiv.org/pdf/2502.16523.pdf", "abs": "https://arxiv.org/abs/2502.16523", "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data."}
{"id": "2502.16838", "pdf": "https://arxiv.org/pdf/2502.16838.pdf", "abs": "https://arxiv.org/abs/2502.16838", "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction", "authors": ["Omar Sharif", "Joseph Gatto", "Madhusudan Basak", "Sarah M. Preum"], "categories": ["cs.CL"], "comment": "Accepted at EMNLP-2025", "summary": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness."}
{"id": "2503.02682", "pdf": "https://arxiv.org/pdf/2503.02682.pdf", "abs": "https://arxiv.org/abs/2503.02682", "title": "MPO: Boosting LLM Agents with Meta Plan Optimization", "authors": ["Weimin Xiong", "Yifan Song", "Qingxiu Dong", "Bingchan Zhao", "Feifan Song", "Xun Wang", "Sujian Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Findings", "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios."}
{"id": "2503.19498", "pdf": "https://arxiv.org/pdf/2503.19498.pdf", "abs": "https://arxiv.org/abs/2503.19498", "title": "DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts", "authors": ["Yujing Lu", "Ling Zhong", "Jing Yang", "Weiming Li", "Peng Wei", "Yongheng Wang", "Manni Duan", "Qing Zhang"], "categories": ["cs.CL"], "comment": "85 pages, 59 figures", "summary": "Chart Question Answering (CQA) evaluates Multimodal Large Language Models\n(MLLMs) on visual understanding and reasoning over chart data. However,\nexisting benchmarks mostly test surface-level parsing, such as reading labels\nand legends, while overlooking deeper scientific reasoning. We propose\nDomainCQA, a framework for constructing domain-specific CQA benchmarks that\nemphasize both visual comprehension and knowledge-intensive reasoning. It\nintegrates complexity-aware chart selection, multitier QA generation, and\nexpert validation. Applied to astronomy, DomainCQA yields AstroChart, a\nbenchmark of 1,690 QA pairs over 482 charts, exposing persistent weaknesses in\nfine-grained perception, numerical reasoning, and domain knowledge integration\nacross 21 MLLMs. Fine-tuning on AstroChart improves performance across\nfundamental and advanced tasks. Pilot QA sets in biochemistry, economics,\nmedicine, and social science further demonstrate DomainCQA's generality.\nTogether, our results establish DomainCQA as a unified pipeline for\nconstructing and augmenting domain-specific chart reasoning benchmarks."}
{"id": "2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438.pdf", "abs": "https://arxiv.org/abs/2504.02438", "title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "authors": ["Chuanqi Cheng", "Jian Guan", "Wei Wu", "Rui Yan"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLAMP, a hierarchical video-language model that\nprocesses hour-long videos at \"mixed precision\" through two key mechanisms: (1)\ndifferential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLAMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLAMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLAMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance. Code and model are available at\nhttps://github.com/steven-ccq/ViLAMP."}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534.pdf", "abs": "https://arxiv.org/abs/2504.13534", "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning boosts large language models' (LLMs)\nperformance on complex tasks but faces two key limitations: a lack of\nreliability when solely relying on LLM-generated reasoning chains and lower\nreasoning performance from natural language prompts compared with code prompts.\nTo address these issues, we propose CoT-RAG, a novel reasoning framework with\nthree key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo Program Prompting Execution, which promotes greater\nlogical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs.\nEvaluations on nine public datasets spanning three reasoning tasks reveal\nsignificant accuracy gains-ranging from 4.0% to 44.3%-over state-of-the-art\nmethods. Furthermore, tests on four domain-specific datasets demonstrate\nexceptional accuracy and efficient execution, underscoring its practical\napplicability and scalability. Our code and data are available at https:\n//github.com/hustlfy123/CoT-RAG."}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117.pdf", "abs": "https://arxiv.org/abs/2504.21117", "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "11 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)", "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."}
{"id": "2505.14157", "pdf": "https://arxiv.org/pdf/2505.14157.pdf", "abs": "https://arxiv.org/abs/2505.14157", "title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "authors": ["Pittawat Taveekitworachai", "Potsawee Manakul", "Sarana Nutanong", "Kunat Pipatanakul"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025, Main; 26 pages, 42 figures", "summary": "This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT."}
{"id": "2505.15337", "pdf": "https://arxiv.org/pdf/2505.15337.pdf", "abs": "https://arxiv.org/abs/2505.15337", "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors", "authors": ["Hao Fang", "Jiawei Kong", "Tianqu Zhuang", "Yixiang Qiu", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP-2025", "summary": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios."}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104.pdf", "abs": "https://arxiv.org/abs/2506.07104", "title": "How Far Are We from Optimal Reasoning Efficiency?", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge."}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582.pdf", "abs": "https://arxiv.org/abs/2506.21582", "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "authors": ["Sam Yu-Te Lee", "Chenyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyu Liu", "Kwan-Liu Ma"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems."}
{"id": "2507.05714", "pdf": "https://arxiv.org/pdf/2507.05714.pdf", "abs": "https://arxiv.org/abs/2507.05714", "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation", "authors": ["YiHan Jiao", "ZheHao Tan", "Dan Yang", "DuoLin Sun", "Jie Feng", "Yue Shen", "Jian Wang", "Peng Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA."}
{"id": "2508.07286", "pdf": "https://arxiv.org/pdf/2508.07286.pdf", "abs": "https://arxiv.org/abs/2508.07286", "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Yuqi Lu", "Zhou Li"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE."}
{"id": "2508.07976", "pdf": "https://arxiv.org/pdf/2508.07976.pdf", "abs": "https://arxiv.org/abs/2508.07976", "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher."}
{"id": "2508.09016", "pdf": "https://arxiv.org/pdf/2508.09016.pdf", "abs": "https://arxiv.org/abs/2508.09016", "title": "A Survey on Training-free Alignment of Large Language Models", "authors": ["Birong Pan", "Yongqi Li", "Weiyu Zhang", "Wenpeng Lu", "Mayi Xu", "Shen Zhou", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to EMNLP 2025 (findings), camera-ready version", "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs."}
{"id": "2508.13107", "pdf": "https://arxiv.org/pdf/2508.13107.pdf", "abs": "https://arxiv.org/abs/2508.13107", "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research", "authors": ["Figarri Keisha", "Prince Singh", "Pallavi", "Dion Fernandes", "Aravindh Manivannan", "Ilham Wicaksono", "Faisal Ahmad", "Wiem Ben Rim"], "categories": ["cs.CL", "cs.IR", "F.2.2; H.3.3; I.2.7"], "comment": "submitted to NLLP 2025 Workshop", "summary": "Retrieval-Augmented Generation (RAG) has transformed how we approach text\ngeneration tasks by grounding Large Language Model (LLM) outputs in retrieved\nknowledge. This capability is especially critical in the legal domain. In this\nwork, we introduce a novel end-to-end RAG pipeline that improves upon previous\nbaselines using three targeted enhancements: (i) a context-aware query\ntranslator that disentangles document references from natural-language\nquestions and adapts retrieval depth and response style based on expertise and\nspecificity, (ii) open-source retrieval strategies using SBERT and GTE\nembeddings that achieve substantial performance gains while remaining\ncost-efficient, and (iii) a comprehensive evaluation and generation framework\nthat combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic\nalignment and faithfulness across models and prompt designs. Our results show\nthat carefully designed open-source pipelines can rival proprietary approaches\nin retrieval quality, while a custom legal-grounded prompt consistently\nproduces more faithful and contextually relevant answers than baseline\nprompting. Taken together, these contributions demonstrate the potential of\ntask-aware, component-level tuning to deliver legally grounded, reproducible,\nand cost-effective RAG systems for legal research assistance."}
{"id": "2508.15474", "pdf": "https://arxiv.org/pdf/2508.15474.pdf", "abs": "https://arxiv.org/abs/2508.15474", "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "authors": ["Sai Sundaresan", "Harshita Chopra", "Atanu R. Sinha", "Koustava Goswami", "Nagasai Saketh Naidu", "Raghav Karan", "N Anushka"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment."}
{"id": "2509.01053", "pdf": "https://arxiv.org/pdf/2509.01053.pdf", "abs": "https://arxiv.org/abs/2509.01053", "title": "A Dynamic Fusion Model for Consistent Crisis Response", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Eduardo Blanco", "Vanessa Frias-Martinez", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "In response to the urgent need for effective communication with\ncrisis-affected populations, automated responses driven by language models have\nbeen proposed to assist in crisis communications. A critical yet often\noverlooked factor is the consistency of response style, which could affect the\ntrust of affected individuals in responders. Despite its importance, few\nstudies have explored methods for maintaining stylistic consistency across\ngenerated responses. To address this gap, we propose a novel metric for\nevaluating style consistency and introduce a fusion-based generation approach\ngrounded in this metric. Our method employs a two-stage process: it first\nassesses the style of candidate responses and then optimizes and integrates\nthem at the instance level through a fusion process. This enables the\ngeneration of high-quality responses while significantly reducing stylistic\nvariation between instances. Experimental results across multiple datasets\ndemonstrate that our approach consistently outperforms baselines in both\nresponse quality and stylistic uniformity."}
{"id": "2509.01058", "pdf": "https://arxiv.org/pdf/2509.01058.pdf", "abs": "https://arxiv.org/abs/2509.01058", "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Dibakar Barua", "Pengcheng Luo", "Junhua Ding", "Lingzi Hong"], "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Health misinformation spreading online poses a significant threat to public\nhealth. Researchers have explored methods for automatically generating\ncounterspeech to health misinformation as a mitigation strategy. Existing\napproaches often produce uniform responses, ignoring that the health literacy\nlevel of the audience could affect the accessibility and effectiveness of\ncounterspeech. We propose a Controlled-Literacy framework using\nretrieval-augmented generation (RAG) with reinforcement learning (RL) to\ngenerate tailored counterspeech adapted to different health literacy levels. In\nparticular, we retrieve knowledge aligned with specific health literacy levels,\nenabling accessible and factual information to support generation. We design a\nreward function incorporating subjective user preferences and objective\nreadability-based rewards to optimize counterspeech to the target health\nliteracy level. Experiment results show that Controlled-Literacy outperforms\nbaselines by generating more accessible and user-preferred counterspeech. This\nresearch contributes to more equitable and impactful public health\ncommunication by improving the accessibility and comprehension of counterspeech\nto health misinformation"}
{"id": "2509.02492", "pdf": "https://arxiv.org/pdf/2509.02492.pdf", "abs": "https://arxiv.org/abs/2509.02492", "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning", "authors": ["Chenglong Wang", "Yongyu Mu", "Hang Zhou", "Yifu Huo", "Ziming Zhu", "Jiali Zeng", "Murun Yang", "Bei Li", "Tong Xiao", "Xiaoyang Hao", "Chunliang Zhang", "Fandong Meng", "Jingbo Zhu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines."}
{"id": "2509.03867", "pdf": "https://arxiv.org/pdf/2509.03867.pdf", "abs": "https://arxiv.org/abs/2509.03867", "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth", "authors": ["Yang Wang", "Chenghao Xiao", "Chia-Yi Hsiao", "Zi Yan Chang", "Chi-Li Chen", "Tyler Loakman", "Chenghua Lin"], "categories": ["cs.CL"], "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference", "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence."}
{"id": "2509.04373", "pdf": "https://arxiv.org/pdf/2509.04373.pdf", "abs": "https://arxiv.org/abs/2509.04373", "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases", "authors": ["Bufan Gao", "Elisa Kreiss"], "categories": ["cs.CL"], "comment": "To be published at EMNLP 2025 (main conference)", "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that prompts that more clearly align with (gender bias)\nevaluation framing elicit distinct gender output distributions compared to less\nevaluation-framed prompts. Discrete-choice metrics further tend to amplify bias\nrelative to probabilistic measures. These findings do not only highlight the\nbrittleness of LLM gender bias evaluations but open a new puzzle for the NLP\nbenchmarking and development community: To what extent can well-controlled\ntesting designs trigger LLM \"testing mode\" performance, and what does this mean\nfor the ecological validity of future benchmarks."}
{"id": "2509.04903", "pdf": "https://arxiv.org/pdf/2509.04903.pdf", "abs": "https://arxiv.org/abs/2509.04903", "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning", "authors": ["Jianghao Chen", "Wei Sun", "Qixiang Yin", "Lingxing Kong", "Zhixing Tan", "Jiajun Zhang"], "categories": ["cs.CL"], "comment": "Under review, our code is available at https://github.com/ZNLP/ACE-RL", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios."}
{"id": "2509.05230", "pdf": "https://arxiv.org/pdf/2509.05230.pdf", "abs": "https://arxiv.org/abs/2509.05230", "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models", "authors": ["Aysenur Kocak", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2025)", "summary": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems."}
{"id": "2509.06806", "pdf": "https://arxiv.org/pdf/2509.06806.pdf", "abs": "https://arxiv.org/abs/2509.06806", "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."}
{"id": "2509.07188", "pdf": "https://arxiv.org/pdf/2509.07188.pdf", "abs": "https://arxiv.org/abs/2509.07188", "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support."}
{"id": "2509.07730", "pdf": "https://arxiv.org/pdf/2509.07730.pdf", "abs": "https://arxiv.org/abs/2509.07730", "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "categories": ["cs.CL"], "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE."}
{"id": "2509.07801", "pdf": "https://arxiv.org/pdf/2509.07801.pdf", "abs": "https://arxiv.org/abs/2509.07801", "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "categories": ["cs.CL", "cs.DL", "cs.IR"], "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP."}
{"id": "2501.15463", "pdf": "https://arxiv.org/pdf/2501.15463.pdf", "abs": "https://arxiv.org/abs/2501.15463", "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "authors": ["Hua Shen", "Nicholas Clark", "Tanushree Mitra"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Paper", "summary": "Existing research primarily evaluates the values of LLMs by examining their\nstated inclinations towards specific values. However, the \"Value-Action Gap,\" a\nphenomenon rooted in environmental and social psychology, reveals discrepancies\nbetween individuals' stated values and their actions in real-world contexts. To\nwhat extent do LLMs exhibit a similar gap between their stated values and their\nactions informed by those values? This study introduces ValueActionLens, an\nevaluation framework to assess the alignment between LLMs' stated values and\ntheir value-informed actions. The framework encompasses the generation of a\ndataset comprising 14.8k value-informed actions across twelve cultures and\neleven social topics, and two tasks to evaluate how well LLMs' stated value\ninclinations and value-informed actions align across three different alignment\nmeasures. Extensive experiments reveal that the alignment between LLMs' stated\nvalues and actions is sub-optimal, varying significantly across scenarios and\nmodels. Analysis of misaligned results identifies potential harms from certain\nvalue-action gaps. To predict the value-action gaps, we also uncover that\nleveraging reasoned explanations improves performance. These findings\nunderscore the risks of relying solely on the LLMs' stated values to predict\ntheir behaviors and emphasize the importance of context-aware evaluations of\nLLM values and value-action gaps."}
{"id": "2502.06130", "pdf": "https://arxiv.org/pdf/2502.06130.pdf", "abs": "https://arxiv.org/abs/2502.06130", "title": "Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models", "authors": ["Ce Zhang", "Zifu Wan", "Zhehan Kan", "Martin Q. Ma", "Simon Stepputtis", "Deva Ramanan", "Russ Salakhutdinov", "Louis-Philippe Morency", "Katia Sycara", "Yaqi Xie"], "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICLR 2025. Project page:\n  https://zhangce01.github.io/DeGF/", "summary": "While recent Large Vision-Language Models (LVLMs) have shown remarkable\nperformance in multi-modal tasks, they are prone to generating hallucinatory\ntext responses that do not align with the given visual input, which restricts\ntheir practical applicability in real-world scenarios. In this work, inspired\nby the observation that the text-to-image generation process is the inverse of\nimage-conditioned response generation in LVLMs, we explore the potential of\nleveraging text-to-image generative models to assist in mitigating\nhallucinations in LVLMs. We discover that generative models can offer valuable\nself-feedback for mitigating hallucinations at both the response and token\nlevels. Building on this insight, we introduce self-correcting Decoding with\nGenerative Feedback (DeGF), a novel training-free algorithm that incorporates\nfeedback from text-to-image generative models into the decoding process to\neffectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an\nimage from the initial response produced by LVLMs, which acts as an auxiliary\nvisual reference and provides self-feedback to verify and correct the initial\nresponse through complementary or contrastive decoding. Extensive experimental\nresults validate the effectiveness of our approach in mitigating diverse types\nof hallucinations, consistently surpassing state-of-the-art methods across six\nbenchmarks. Code is available at https://github.com/zhangce01/DeGF."}
{"id": "2504.03814", "pdf": "https://arxiv.org/pdf/2504.03814.pdf", "abs": "https://arxiv.org/abs/2504.03814", "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "authors": ["Grgur Kovač", "Jérémy Perez", "Rémy Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": "Accepted to EMNLP 2025 (Oral)", "summary": "Large language models (LLMs) are increasingly used in the creation of online\ncontent, creating feedback loops as subsequent generations of models will be\ntrained on this synthetic data. Such loops were shown to lead to distribution\nshifts - models misrepresenting the true underlying distributions of human data\n(also called model collapse). However, how human data properties affect such\nshifts remains poorly understood. In this paper, we provide the first empirical\nexamination of the effect of such properties on the outcome of recursive\ntraining. We first confirm that using different human datasets leads to\ndistribution shifts of different magnitudes. Through exhaustive manipulation of\ndataset properties combined with regression analyses, we then identify a set of\nproperties predicting distribution shift magnitudes. Lexical diversity is found\nto amplify these shifts, while semantic diversity and data quality mitigate\nthem. Furthermore, we find that these influences are highly modular: data\nscrapped from a given internet domain has little influence on the content\ngenerated for another domain. Finally, experiments on political bias reveal\nthat human data properties affect whether the initial bias will be amplified or\nreduced. Overall, our results portray a novel view, where different parts of\ninternet may undergo different types of distribution shift."}
{"id": "2505.05684", "pdf": "https://arxiv.org/pdf/2505.05684.pdf", "abs": "https://arxiv.org/abs/2505.05684", "title": "Meta-Semantics Augmented Few-Shot Relational Learning", "authors": ["Han Wu", "Jie Yin"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Few-shot relational learning on knowledge graph (KGs) aims to perform\nreasoning over relations with only a few training examples. While existing\nmethods have primarily focused on leveraging specific relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ncritical gap, we propose a novel prompted meta-learning (PromptMeta) framework\nthat seamlessly integrates meta-semantics with relational information for\nfew-shot relational learning. PromptMeta has two key innovations: (1) a\nMeta-Semantic Prompt (MSP) pool that learns and consolidates high-level\nmeta-semantics, enabling effective knowledge transfer and adaptation to rare\nand newly emerging relations; and (2) a learnable fusion token that dynamically\ncombines meta-semantics with task-specific relational information tailored to\ndifferent few-shot tasks. Both components are optimized jointly with model\nparameters within a meta-learning framework. Extensive experiments and analyses\non two real-world KG datasets demonstrate the effectiveness of PromptMeta in\nadapting to new relations with limited data."}
{"id": "2507.23674", "pdf": "https://arxiv.org/pdf/2507.23674.pdf", "abs": "https://arxiv.org/abs/2507.23674", "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses", "authors": ["Muhammad Taha Cheema", "Abeer Aamir", "Khawaja Gul Muhammad", "Naveed Anwar Bhatti", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 9 figures", "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."}
{"id": "2509.00115", "pdf": "https://arxiv.org/pdf/2509.00115.pdf", "abs": "https://arxiv.org/abs/2509.00115", "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems", "authors": ["Manish Shukla"], "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance.\nThird, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication. The code supporting this work\nis available at\nhttps://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring."}
{"id": "2509.01907", "pdf": "https://arxiv.org/pdf/2509.01907.pdf", "abs": "https://arxiv.org/abs/2509.01907", "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events", "authors": ["Zhenyuan Chen", "Chenxi Wang", "Feng Zhang"], "categories": ["cs.CV", "cs.CL"], "comment": "under review", "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC."}
{"id": "2509.07170", "pdf": "https://arxiv.org/pdf/2509.07170.pdf", "abs": "https://arxiv.org/abs/2509.07170", "title": "That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral", "authors": ["Quinten Steenhuis"], "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "Submission to JURIX 2025", "summary": "Each year millions of people seek help for their legal problems by calling a\nlegal aid program hotline, walking into a legal aid office, or using a lawyer\nreferral service. The first step to match them to the right help is to identify\nthe legal problem the applicant is experiencing. Misdirection has consequences.\nApplicants may miss a deadline, experience physical abuse, lose housing or lose\ncustody of children while waiting to connect to the right legal help. We\nintroduce and evaluate the FETCH classifier for legal issue classification and\ndescribe two methods for improving accuracy: a hybrid LLM/ML ensemble\nclassification method, and the automatic generation of follow-up questions to\nenrich the initial problem narrative. We employ a novel data set of 419\nreal-world queries to a nonprofit lawyer referral service. Ultimately, we show\nclassification accuracy (hits@2) of 97.37\\% using a mix of inexpensive models,\nexceeding the performance of the current state-of-the-art GPT-5 model. Our\napproach shows promise in significantly reducing the cost of guiding users of\nthe legal system to the right resource for their problem while achieving high\naccuracy."}
