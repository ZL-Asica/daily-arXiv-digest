{"id": "2504.17792", "pdf": "https://arxiv.org/pdf/2504.17792.pdf", "abs": "https://arxiv.org/abs/2504.17792", "title": "My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data", "authors": ["Hauke Sandhaus", "Angel Hsing-Chi Hwang", "Wendy Ju", "Qian Yang"], "categories": ["cs.HC", "cs.AI", "cs.DB", "E.m; H.2.8; J.1"], "comment": "To appear in Proc. ACM Hum.-Comput. Interact., Computer-Supported\n  Cooperative Work & Social Computing (CSCW), 2025", "summary": "Safety-critical data, such as crash and near-crash records, are crucial to\nimproving autonomous vehicle (AV) design and development. Sharing such data\nacross AV companies, academic researchers, regulators, and the public can help\nmake all AVs safer. However, AV companies rarely share safety-critical data\nexternally. This paper aims to pinpoint why AV companies are reluctant to share\nsafety-critical data, with an eye on how these barriers can inform new\napproaches to promote sharing. We interviewed twelve AV company employees who\nactively work with such data in their day-to-day work. Findings suggest two\nkey, previously unknown barriers to data sharing: (1) Datasets inherently embed\nsalient knowledge that is key to improving AV safety and are\nresource-intensive. Therefore, data sharing, even within a company, is fraught\nwith politics. (2) Interviewees believed AV safety knowledge is private\nknowledge that brings competitive edges to their companies, rather than public\nknowledge for social good. We discuss the implications of these findings for\nincentivizing and enabling safety-critical AV data sharing, specifically,\nimplications for new approaches to (1) debating and stratifying public and\nprivate AV safety knowledge, (2) innovating data tools and data sharing\npipelines that enable easier sharing of public AV safety data and knowledge;\n(3) offsetting costs of curating safety-critical data and incentivizing data\nsharing.", "AI": {"tldr": "This paper investigates barriers to data sharing among autonomous vehicle (AV) companies regarding safety-critical data, emphasizing implications for enhancing sharing practices.", "motivation": "The increasing importance of safety-critical data in improving autonomous vehicle safety necessitates understanding the reluctance of AV companies to share such data.", "method": "The authors conducted interviews with twelve employees from AV companies who handle safety-critical data in their work.", "result": "Two major barriers to safety-critical data sharing were identified: (1) the inherent value and resource intensity of datasets creating internal politics, and (2) the perception of AV safety knowledge as proprietary rather than communal.", "conclusion": "Addressing these barriers through innovative data sharing tools, cost-offset strategies, and public-private knowledge debates could improve AV safety.", "key_contributions": ["Identification of unique barriers to safety-critical data sharing in AV companies", "Insights into the politics of data ownership and confidentiality", "Recommendations for fostering data sharing through innovative tools and incentives"], "limitations": "", "future_work": "Future research could explore specific tools and strategies to facilitate safer data sharing among AV stakeholders.", "keywords": ["autonomous vehicles", "data sharing", "safety-critical data", "human-computer interaction", "competitive knowledge"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934.pdf", "abs": "https://arxiv.org/abs/2504.17934", "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.", "AI": {"tldr": "This position paper discusses the privacy and security risks of LLM-powered GUI agents and advocates for a human-centered evaluation framework.", "motivation": "The paper addresses growing concerns over privacy and security risks associated with LLM-powered GUI agents, highlighting a gap in existing evaluations that prioritize performance over these critical factors.", "method": "The authors review existing evaluation metrics and identify challenges in integrating human evaluators for GUI agent assessments, advocating for a comprehensive human-centered framework.", "result": "Identifies three key risks of GUI agents compared to traditional automation and outlines five challenges in current evaluation practices.", "conclusion": "A shift towards a human-centered evaluation framework is essential for ensuring that privacy and security considerations are integrated into the design and assessment of GUI agents.", "key_contributions": ["Identification of three key risks associated with LLM-powered GUI agents", "Outline of five challenges in the current assessment process", "Proposal of a human-centered evaluation framework emphasizing privacy and security"], "limitations": "The paper primarily focuses on identifying risks and suggesting a framework without empirical evaluation or case studies to support the proposed changes.", "future_work": "Future research should explore the implementation and effectiveness of the proposed human-centered evaluation framework in real-world scenarios.", "keywords": ["Large Language Models", "GUI automation", "privacy", "security", "human-centered evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.17960", "pdf": "https://arxiv.org/pdf/2504.17960.pdf", "abs": "https://arxiv.org/abs/2504.17960", "title": "VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics", "authors": ["Kazi Shahrukh Omar", "Shuaijie Wang", "Ridhuparan Kungumaraju", "Tanvi Bhatt", "Fabio Miranda"], "categories": ["cs.HC", "cs.CY"], "comment": "Accepted for publication in the IEEE Transactions on Visualization\n  and Computer Graphics. VIGMA is available at https://github.com/komar41/VIGMA", "summary": "Gait disorders are commonly observed in older adults, who frequently\nexperience various issues related to walking. Additionally, researchers and\nclinicians extensively investigate mobility related to gait in typically and\natypically developing children, athletes, and individuals with orthopedic and\nneurological disorders. Effective gait analysis enables the understanding of\nthe causal mechanisms of mobility and balance control of patients, the\ndevelopment of tailored treatment plans to improve mobility, the reduction of\nfall risk, and the tracking of rehabilitation progress. However, analyzing gait\ndata is a complex task due to the multivariate nature of the data, the large\nvolume of information to be interpreted, and the technical skills required.\nExisting tools for gait analysis are often limited to specific patient groups\n(e.g., cerebral palsy), only handle a specific subset of tasks in the entire\nworkflow, and are not openly accessible. To address these shortcomings, we\nconducted a requirements assessment with gait practitioners (e.g., researchers,\nclinicians) via surveys and identified key components of the workflow,\nincluding (1) data processing and (2) data analysis and visualization. Based on\nthe findings, we designed VIGMA, an open-access visual analytics framework\nintegrated with computational notebooks and a Python library, to meet the\nidentified requirements. Notably, the framework supports analytical\ncapabilities for assessing disease progression and for comparing multiple\npatient groups. We validated the framework through usage scenarios with experts\nspecializing in gait and mobility rehabilitation. VIGMA is available at\nhttps://github.com/komar41/VIGMA.", "AI": {"tldr": "VIGMA is an open-access visual analytics framework designed for gait analysis in various patient populations, addressing the complexity of gait data interpretation.", "motivation": "Gait disorders affect various populations, including older adults and children, and effective analysis is necessary for improving mobility and developing tailored treatment plans.", "method": "We conducted a requirements assessment with gait practitioners through surveys to identify workflow components and designed VIGMA, integrating computational notebooks and a Python library.", "result": "VIGMA supports analytical capabilities for assessing disease progression and comparing multiple patient groups, validated through expert usage scenarios.", "conclusion": "VIGMA addresses limitations of existing gait analysis tools by providing an open-access framework that meets the needs of practitioners in gait and mobility rehabilitation.", "key_contributions": ["Development of an open-access visual analytics framework for gait analysis", "Integration with computational notebooks and a Python library", "Capability to assess disease progression and compare patient groups"], "limitations": "Existing tools are limited to specific patient groups and tasks, and may require specialized technical skills for use.", "future_work": "Future research may explore further enhancements to VIGMA for broader applications in gait analysis.", "keywords": ["gait analysis", "visual analytics", "mobility", "rehabilitation", "open-access"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.17964", "pdf": "https://arxiv.org/pdf/2504.17964.pdf", "abs": "https://arxiv.org/abs/2504.17964", "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content", "authors": ["Celia Chen", "Alex Leitch"], "categories": ["cs.HC", "cs.AI"], "comment": "Under review at ACM Web Science Conference 2025's Human-GenAI\n  Interactions Workshop, 4 pages", "summary": "This paper examines how graduate students develop frameworks for evaluating\nmachine-generated expertise in web-based interactions with large language\nmodels (LLMs). Through a qualitative study combining surveys, LLM interaction\ntranscripts, and in-depth interviews with 14 graduate students, we identify\npatterns in how these emerging professionals assess and engage with\nAI-generated content. Our findings reveal that students construct evaluation\nframeworks shaped by three main factors: professional identity, verification\ncapabilities, and system navigation experience. Rather than uniformly accepting\nor rejecting LLM outputs, students protect domains central to their\nprofessional identities while delegating others--with managers preserving\nconceptual work, designers safeguarding creative processes, and programmers\nmaintaining control over core technical expertise. These evaluation frameworks\nare further influenced by students' ability to verify different types of\ncontent and their experience navigating complex systems. This research\ncontributes to web science by highlighting emerging human-genAI interaction\npatterns and suggesting how platforms might better support users in developing\neffective frameworks for evaluating machine-generated expertise signals in\nAI-mediated web environments.", "AI": {"tldr": "Graduate students develop frameworks for evaluating AI-generated expertise in web interactions with LLMs, influenced by identity, verification capabilities, and system navigation.", "motivation": "To understand how graduate students assess machine-generated content in the context of large language models.", "method": "Qualitative study using surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students.", "result": "Students construct evaluation frameworks based on identity, verification, and system navigation, showing patterns of protecting certain domains of expertise while engaging with LLM outputs.", "conclusion": "The research sheds light on human-genAI interaction patterns and suggests ways to improve support for users evaluating AI-generated content.", "key_contributions": ["Identification of evaluation framework factors for AI content", "Insights into student interaction with LLMs", "Recommendations for platform improvements in AI content evaluation"], "limitations": "Limited to graduate students and their specific contexts; may not generalize to all user demographics.", "future_work": "Exploration of diverse user groups and development of platform features to support evaluation frameworks.", "keywords": ["Human-Computer Interaction", "Large Language Models", "AI content evaluation", "Graduate students", "Web interactions"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974.pdf", "abs": "https://arxiv.org/abs/2504.17974", "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages.", "AI": {"tldr": "The study introduces PolyHope V2, a multilingual fine-grained hope speech dataset and benchmarks various models to detect nuanced categories of hope in tweets.", "motivation": "To explore the complex emotional state of hope, which is underexplored yet significant in education, mental health, and social interaction, and to improve natural language processing (NLP) detection of this emotion.", "method": "The methodology involves the creation of the PolyHope V2 dataset, which includes over 30,000 annotated tweets in English and Spanish categorized into four hope subtypes. Multiple pretrained transformer models and large language models (LLMs) were benchmarked under zero-shot and few-shot conditions.", "result": "Fine-tuned transformers showed superior performance compared to prompt-based LLMs in distinguishing nuanced hope categories and identifying sarcasm, revealing systematic challenges in separating closely related hope subtypes.", "conclusion": "The study provides a foundational resource for future emotion recognition tasks, emphasizing the need for greater semantic and contextual sensitivity across languages.", "key_contributions": ["Introduction of the PolyHope V2 dataset with nuanced hope categories", "Benchmarking performance of transformers vs LLMs in detecting hope", "Highlighting challenges in NLP emotion recognition, particularly for sarcasm"], "limitations": "The study primarily focuses on tweets, which may limit generalizability to other contexts of emotional expression.", "future_work": "Future research directions include enhancing datasets for other emotions and improving model performance in nuanced emotion detection across diverse contexts.", "keywords": ["hope", "Natural Language Processing", "emotion recognition", "dataset", "transformer models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.17997", "pdf": "https://arxiv.org/pdf/2504.17997.pdf", "abs": "https://arxiv.org/abs/2504.17997", "title": "Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating Adolescent Mobile Interactions", "authors": ["Suwon Yoon", "Seungwon Yang", "Jeongwon Choi", "Wonjeong Park", "Inseok Hwang"], "categories": ["cs.HC"], "comment": "5 pages, Workshop Paper", "summary": "Adolescents' uncontrolled exposure to digital content can negatively impact\ntheir development. Traditional regulatory methods, such as time limits or app\nrestrictions, often take a rigid approach, ignoring adolescents'\ndecision-making abilities. Another issue is the lack of content and services\ntailored for adolescents. To address this, we propose Chatperone, a concept of\na system that provides adaptive scaffolding to support adolescents. Chatperone\nfosters healthy mobile interactions through three key modules: Perception,\nNegotiation, and Moderation. This paper outlines these modules' functionalities\nand discusses considerations for real-world implementation.", "AI": {"tldr": "Chatperone is a proposed system that supports adolescents' healthy mobile interactions through adaptive scaffolding.", "motivation": "To address the negative impact of uncontrolled digital content exposure on adolescents and the limitations of traditional regulatory methods.", "method": "The paper outlines a system with three key modules: Perception, Negotiation, and Moderation, designed to provide adaptive support for adolescents.", "result": "Chatperone's modules aim to foster healthier mobile interactions by adapting to adolescents' decision-making.", "conclusion": "The proposed system is expected to enhance the way adolescents engage with digital content in a supportive manner.", "key_contributions": ["Introduction of the Chatperone system for adolescent digital interactions", "Development of three modules (Perception, Negotiation, Moderation) for adaptive scaffolding", "Discussion of real-world implementation considerations"], "limitations": "", "future_work": "", "keywords": ["adolescents", "digital content", "adaptive scaffolding", "mobile interactions", "moderation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.17993", "pdf": "https://arxiv.org/pdf/2504.17993.pdf", "abs": "https://arxiv.org/abs/2504.17993", "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds", "authors": ["Brihi Joshi", "Xiang Ren", "Swabha Swayamdipta", "Rik Koncel-Kedziorski", "Tim Paek"], "categories": ["cs.CL"], "comment": null, "summary": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales.", "AI": {"tldr": "The paper introduces PB&J, a framework that enhances language model personas by incorporating psychological rationales for user preferences, leading to improved predictions of user behavior.", "motivation": "Existing persona-building methods for language models fail to account for the reasoning behind user judgments, which limits their effectiveness in predicting user preferences.", "method": "The PB&J framework integrates user rationales derived from their experiences, personality traits, and beliefs, utilizing psychological scaffolds rooted in established theories like the Big 5 Personality Traits.", "result": "Experiments reveal that LLM personas enhanced with PB&J rationales consistently outperform traditional methods that rely only on demographics or prior judgments for user preference predictions.", "conclusion": "The integration of psychological rationales significantly improves the predictive accuracy of language model personas, suggesting a need to move beyond simplistic demographic approaches.", "key_contributions": ["Introduction of the PB&J framework for enhancing LLM personas", "Demonstration of the effectiveness of incorporating psychological rationales", "Comparison of PB&J with traditional demographic-based approaches showing superior performance."], "limitations": "", "future_work": "Further exploration of psychological theories that can enhance LLM persona development and testing on diverse user preference tasks.", "keywords": ["Language Model", "User Preferences", "Psychological Rationales", "Personality Traits", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.17999", "pdf": "https://arxiv.org/pdf/2504.17999.pdf", "abs": "https://arxiv.org/abs/2504.17999", "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving", "authors": ["Chang Xiao", "Brenda Yang"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. Our statistical analysis of\ncomputational savings, combined with crowdsourced user studies, provides\ninsights into the trade-offs between service efficiency and user satisfaction,\ndemonstrating that our method can significantly reduce computational\nconsumption up to 16.8\\%. This context-aware computational resource management\nstrategy presents a practical framework for enhancing system efficiency in\ncloud-based conversational AI interfaces without compromising user experience.", "AI": {"tldr": "This paper presents an adaptive streaming method for conversational interfaces powered by LLMs that optimizes output pacing based on users' cognitive load, resulting in substantial computational savings and improved user experience.", "motivation": "The paper addresses the inefficiency in computational resource use when streaming outputs from LLMs faster than users can read, particularly during peak usage in cloud-based services.", "method": "An adaptive streaming method is proposed that dynamically adjusts the speed of LLM output streaming based on inferred cognitive load, slowing down during complex segments to enhance efficiency.", "result": "The method significantly reduces computational consumption by up to 16.8%, as demonstrated through statistical analysis and user studies that examine the balance between efficiency and user satisfaction.", "conclusion": "The proposed strategy offers a practical framework for improving system efficiency in cloud-based conversational AI interfaces while maintaining a positive user experience.", "key_contributions": ["Development of an adaptive streaming method for LLMs based on cognitive load.", "Quantitative analysis demonstrating up to 16.8% reduction in computational resources.", "User studies indicating improved service efficiency without sacrificing user satisfaction."], "limitations": "The method relies on accurate inference of cognitive load, which may vary between users and contexts.", "future_work": "Future research may involve refining cognitive load estimations and exploring integration with various conversational AI systems.", "keywords": ["Generative conversational interfaces", "Large language models", "Cognitive load", "Adaptive streaming", "Computational efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012.pdf", "abs": "https://arxiv.org/abs/2504.18012", "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems.", "AI": {"tldr": "This study explores the role of pre-trained encoders and decoders in Multimodal Machine Translation, demonstrating that pre-trained decoders enhance translation fluency and accuracy, while encoders' effectiveness varies with visual-text alignment.", "motivation": "To investigate the underexplored effectiveness of pre-trained language and vision models in Multimodal Machine Translation (MMT).", "method": "We systematically analyze different training strategies from scratch to pre-trained and partially frozen components in a unified MMT framework, conducting experiments on Multi30K and CoMMuTE datasets for English-German and English-French translation tasks.", "result": "Pre-trained decoders yield more fluent and accurate outputs, while pre-trained encoders show varied effects based on visual-text alignment quality.", "conclusion": "Pre-training is crucial in multimodal settings, and effective modality fusion alongside pre-trained components must be considered in future MMT architecture designs.", "key_contributions": ["Analysis of pre-trained encoders and decoders in MMT", "Insights on modality fusion and pre-trained components", "Guidance for future architecture design in multimodal translation systems."], "limitations": "", "future_work": "Further exploration of architecture designs that incorporate effective modality fusion and pre-trained components.", "keywords": ["Multimodal Machine Translation", "pre-trained models", "visual-text alignment", "translation performance", "modality fusion"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.18189", "pdf": "https://arxiv.org/pdf/2504.18189.pdf", "abs": "https://arxiv.org/abs/2504.18189", "title": "ClassComet: Exploring and Designing AI-generated Danmaku in Educational Videos to Enhance Online Learning", "authors": ["Zipeng Ji", "Pengcheng An", "Jian Zhao"], "categories": ["cs.HC"], "comment": null, "summary": "Danmaku, users' live comments synchronized with, and overlaying on videos,\nhas recently shown potential in promoting online video-based learning. However,\nuser-generated danmaku can be scarce-especially in newer or less viewed videos\nand its quality is unpredictable, limiting its educational impact. This paper\nexplores how large multimodal models (LMM) can be leveraged to automatically\ngenerate effective, high-quality danmaku. We first conducted a formative study\nto identify the desirable characteristics of content- and emotion-related\ndanmaku in educational videos. Based on the obtained insights, we developed\nClassComet, an educational video platform with novel LMM-driven techniques for\ngenerating relevant types of danmaku to enhance video-based learning. Through\nuser studies, we examined the quality of generated danmaku and their influence\non learning experiences. The results indicate that our generated danmaku is\ncomparable to human-created ones, and videos with both content- and\nemotion-related danmaku showed significant improvement in viewers' engagement\nand learning outcome.", "AI": {"tldr": "This paper explores the use of large multimodal models to automatically generate high-quality danmaku comments for educational videos, enhancing viewer engagement and learning outcomes.", "motivation": "The scarcity and unpredictable quality of user-generated danmaku in educational videos limit their potential in promoting effective online learning.", "method": "A formative study was conducted to identify the desirable characteristics of danmaku, leading to the development of ClassComet, a platform utilizing LMM techniques to generate relevant danmaku.", "result": "User studies demonstrated that the quality of the LMM-generated danmaku was comparable to human-created comments, significantly improving viewer engagement and learning outcomes in videos featuring both content- and emotion-related danmaku.", "conclusion": "The integration of LMM-driven danmaku generation can enhance the educational impact of video-based learning environments.", "key_contributions": ["Development of ClassComet platform for generating danmaku", "Insights on desirable characteristics of educational danmaku", "Demonstration of improved engagement and learning outcomes through generated content"], "limitations": "", "future_work": "Further exploration of scaling danmaku generation across various platforms and different educational contexts.", "keywords": ["danmaku", "educational videos", "large multimodal models", "user engagement", "learning outcomes"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.18041", "pdf": "https://arxiv.org/pdf/2504.18041.pdf", "abs": "https://arxiv.org/abs/2504.18041", "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "authors": ["Bang An", "Shiyue Zhang", "Mark Dredze"], "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs.", "AI": {"tldr": "The paper analyzes the safety implications of the Retrieval-Augmented Generation (RAG) framework in relation to large language models (LLMs), demonstrating that RAG can reduce safety and alter models' safety profiles.", "motivation": "To investigate how RAG frameworks differ from standard LLMs in terms of safety, as current safety research largely overlooks RAG use cases.", "method": "Conducted a comparative analysis of eleven LLMs, examining their safety profiles in both RAG and non-RAG contexts.", "result": "RAG can reduce model safety and alter safety profiles, with unsafe generations possible even from safe combinations of models and documents; existing red teaming methods are less effective for RAG settings.", "conclusion": "There is a critical need for safety research and red teaming methods specifically adapted for RAG LLMs.", "key_contributions": ["Detailed comparative analysis of safety profiles for RAG vs non-RAG LLMs", "Demonstration of RAG's potential to decrease model safety", "Evaluation of red teaming methods' effectiveness in RAG settings"], "limitations": "The study examines only eleven LLMs, which may not represent the full spectrum of models.", "future_work": "Develop safety evaluation frameworks and red teaming methods tailored for RAG LLMs.", "keywords": ["Large language models", "Retrieval-Augmented Generation", "Safety", "Red teaming", "AI safety"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18238", "pdf": "https://arxiv.org/pdf/2504.18238.pdf", "abs": "https://arxiv.org/abs/2504.18238", "title": "SecCityVR: Visualization and Collaborative Exploration of Software Vulnerabilities in Virtual Reality", "authors": ["Dennis Wüppelman", "Enes Yigitbas"], "categories": ["cs.HC"], "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 29th International Conference on Evaluation and Assessment\n  in Software Engineering (EASE 2025)", "summary": "Security vulnerabilities in software systems represent significant risks as\npotential entry points for malicious attacks. Traditional dashboards that\ndisplay the results of static analysis security testing often use 2D or 3D\nvisualizations, which tend to lack the spatial details required to effectively\nreveal issues such as the propagation of vulnerabilities across the codebase or\nthe appearance of concurrent vulnerabilities. Additionally, most reporting\nsolutions only treat the analysis results as an artifact that can be reviewed\nor edited asynchronously by developers, limiting real-time, collaborative\nexploration. To the best of our knowledge, no VR-based approach exists for the\nvisualization and interactive exploration of software security vulnerabilities.\nAddressing these challenges, the virtual reality (VR) environment SecCityVR was\ndeveloped as a proof-of-concept implementation that employs the code city\nmetaphor within VR to visualize software security vulnerabilities as colored\nbuilding floors inside the surrounding virtual city. By integrating the\napplication's call graph, vulnerabilities are contextualized within related\nsoftware components. SecCityVR supports multi-user collaboration and\ninteractive exploration. It provides explanations and mitigations for detected\nissues. A user study comparing SecCityVR with the traditional dashboard\nfind-sec-bugs showed the VR approach provided a favorable experience, with\nhigher usability, lower temporal demand, and significantly lower frustration\ndespite having longer task completion times. This paper and its results\ncontribute to the fields of collaborative and secure software engineering, as\nwell as software visualization. It provides a new application of VR code cities\nto visualize security vulnerabilities, as well as a novel environment for\nsecurity audits using collaborative and immersive technologies.", "AI": {"tldr": "Introduction of SecCityVR, a VR environment for visualizing software security vulnerabilities.", "motivation": "To overcome limitations of traditional 2D/3D visualizations in displaying software security vulnerabilities and enable real-time, collaborative exploration.", "method": "Development of SecCityVR using the code city metaphor to visualize vulnerabilities as colored building floors in a virtual city, supporting multi-user collaboration and providing context through call graphs.", "result": "User study showed that SecCityVR offers better usability and lower frustration compared to traditional dashboards, despite longer task completion times.", "conclusion": "SecCityVR enhances collaborative software security analysis by providing immersive visualization of vulnerabilities.", "key_contributions": ["Introduction of VR-based visualization for software security vulnerabilities", "Support for multi-user collaboration in a virtual environment", "Contextualization of vulnerabilities within related software components using call graphs."], "limitations": "Focus on a proof-of-concept implementation; effectiveness in larger or more complex codebases is yet to be validated.", "future_work": "Investigate scalability of the VR approach and explore further applications in software engineering.", "keywords": ["Virtual Reality", "Software Security", "Vulnerability Visualization", "Collaborative Software Engineering", "Immersive Technologies"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053.pdf", "abs": "https://arxiv.org/abs/2504.18053", "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.", "AI": {"tldr": "This paper introduces DREAM, a novel approach for enhancing safety alignment in Multimodal Large Language Models through risk disentanglement and reinforcement learning, significantly improving safety during inference and training phases without performance loss.", "motivation": "The integration of visual and textual data in MLLMs creates new safety challenges and potential risk combinations that need to be disentangled systematically.", "method": "The paper employs a step-by-step reasoning analysis to disentangle risks associated with multimodal inputs and develops the DREAM framework which utilizes supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF).", "result": "DREAM achieved a 16.17% improvement in the SIUO safe&effective score compared to GPT-4V, enhancing safety during both inference and training.", "conclusion": "The systematic multimodal risk disentanglement was found to significantly improve risk awareness in MLLMs, leading to enhanced performance in terms of safety alignment.", "key_contributions": ["Introduction of DREAM framework for risk disentanglement in MLLMs", "Demonstrated significant safety improvements without sacrificing performance", "Provided code and data for further research"], "limitations": "", "future_work": "Further explorations on enhancing safety alignment in more diverse multimodal contexts and improving the scalability of the DREAM framework.", "keywords": ["Multimodal Large Language Models", "Risk Disentanglement", "Safety Alignment", "Reinforcement Learning", "DREAM"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.18410", "pdf": "https://arxiv.org/pdf/2504.18410.pdf", "abs": "https://arxiv.org/abs/2504.18410", "title": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse", "authors": ["Jiaying Fu", "Jialin Gu", "Tianyue Gong", "Tiange Zhou"], "categories": ["cs.HC"], "comment": "8 pages, 5 figures, accetped by 30th International Symposium on\n  Electronic Art (ISEA 2025)", "summary": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems.", "AI": {"tldr": "This paper presents a VR experience powered by LLMs to encourage reflection on parental verbal abuse, showing varied emotional impacts based on personal histories.", "motivation": "To provide immersive self-reflection opportunities for individuals affected by parental verbal abuse, addressing gaps in current therapeutic approaches.", "method": "A dual-phase VR experience where participants first role-play as a verbally abusive parent and then observe an LLM reframing abusive dialogue positively.", "result": "Qualitative analysis of 12 participants indicated that the experience fosters emotional reflection and supportive feelings, although effectiveness varies among participants based on personal experiences.", "conclusion": "The study suggests that LLMs can be effectively utilized in immersive environments to promote emotional reflection, highlighting the importance of personalization in AI-driven support systems.", "key_contributions": ["Development of an innovative VR experience using LLMs for emotional reflection", "Highlighting the need for personalization in AI emotional support", "Empirical insights from a qualitative study on participant experiences."], "limitations": "The study's small sample size and the variability of outcomes based on individual histories limit generalizability.", "future_work": "Suggests further exploration of personalizing AI-driven emotional support systems and expanding participant diversity in future studies.", "keywords": ["virtual reality", "LLMs", "parental verbal abuse", "emotional support", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058.pdf", "abs": "https://arxiv.org/abs/2504.18058", "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications.", "AI": {"tldr": "This study investigates how user personas, defined by the Myers-Briggs Type Indicator, affect the performance of dialogue agents in sales scenarios.", "motivation": "To enhance the quality of interactions between dialogue agents and users with diverse personalities.", "method": "The study involves large-scale testing of a pre-trained dialogue agent, analyzing its adaptability and personalization capabilities based on MBTI user types.", "result": "The findings indicate significant patterns in interaction quality, task completion rates, and dialogue naturalness based on user personas.", "conclusion": "Dialogue agents can improve their strategies to be more effective by adapting to varying personality traits, with implications for broader applications in conversational systems.", "key_contributions": ["Development of persona-defined user simulators for research", "Insights into interaction dynamics with varying MBTI-defined user types", "Recommendations for building adaptive dialogue systems in sales"], "limitations": "The study is confined to the sales domain and may require further exploration in other contexts.", "future_work": "Extending the framework to other domains and further improving personalization techniques for dialogue agents.", "keywords": ["dialogue agents", "user personas", "Myers-Briggs Type Indicator", "sales", "personalization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.18496", "pdf": "https://arxiv.org/pdf/2504.18496.pdf", "abs": "https://arxiv.org/abs/2504.18496", "title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review", "authors": ["Raymond Fok", "Joseph Chee Chang", "Marissa Radensky", "Pao Siangliulue", "Jonathan Bragg", "Amy X. Zhang", "Daniel S. Weld"], "categories": ["cs.HC"], "comment": null, "summary": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow.", "AI": {"tldr": "DimInd is an interactive system designed to assist researchers in conducting literature reviews by utilizing LLM-generated structured representations, reducing cognitive load and effort in synthesizing large collections of research papers.", "motivation": "Comprehensive literature review involves synthesizing vast research, which is a labor-intensive process that has seen little support for deep analysis of large collections.", "method": "DimInd employs multiple levels of information compression, providing faceted literature comparison tables, taxonomies, and narrative syntheses while ensuring provenance of source texts.", "result": "In an evaluation with 23 researchers, DimInd enabled users to extract information and organize papers more efficiently compared to a ChatGPT-assisted workflow.", "conclusion": "DimInd demonstrates the potential of using interactive LLM-generated structures to enhance the literature review process.", "key_contributions": ["Introduction of DimInd to scaffold literature reviews through LLM-generated representations", "Multiple levels of information transformation for better understanding and organization of research papers", "Evaluation showing improved efficiency in information extraction compared to traditional methods"], "limitations": "Evaluation limited to 23 researchers; broader applicability and effectiveness across diverse research domains need further exploration.", "future_work": "Explore additional functionalities and enhancements for DimInd based on user feedback and extend evaluation to larger and more varied groups of researchers.", "keywords": ["Literature Review", "Natural Language Processing", "Interactive Systems"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070.pdf", "abs": "https://arxiv.org/abs/2504.18070", "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "authors": ["Jingjin Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.", "AI": {"tldr": "PropRAG is a new framework enhancing Retrieval Augmented Generation (RAG) for Large Language Models (LLMs) by utilizing contextual propositions and a novel beam search algorithm for improved multi-step reasoning and retrieval without online LLM invocation.", "motivation": "To address limitations of standard RAG in capturing human memory's interconnected nature crucial for complex reasoning and contextual understanding.", "method": "PropRAG employs contextually rich propositions and a beam search algorithm for discovering multi-step reasoning chains, operating through efficient graph traversal and pre-computed embeddings, avoiding online LLM inference costs.", "result": "PropRAG achieves state-of-the-art performance on various QA benchmarks, with zero-shot Recall@5 results showing significant improvements over existing methods.", "conclusion": "By enhancing evidence retrieval and representation, PropRAG represents a leap forward in non-parametric continual learning, offering improved efficiency and effectiveness for LLMs in reasoning tasks.", "key_contributions": ["Introduction of PropRAG framework leveraging contextually rich propositions.", "Development of a novel beam search algorithm for reasoning chain discovery.", "Demonstration of state-of-the-art performance on multiple QA datasets."], "limitations": "The framework's reliance on pre-computed embeddings may limit adaptability to rapidly changing knowledge bases.", "future_work": "Exploring further enhancements to the reasoning capabilities and adapting PropRAG for more dynamic retrieval scenarios.", "keywords": ["Retrieval Augmented Generation", "Large Language Models", "Multi-step reasoning", "Graph traversal", "Non-parametric continual learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18080", "pdf": "https://arxiv.org/pdf/2504.18080.pdf", "abs": "https://arxiv.org/abs/2504.18080", "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization", "authors": ["Wataru Kawakami", "Keita Suzuki", "Junichiro Iwasawa"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications.", "AI": {"tldr": "Introduction of Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for Japanese medicine, achieving high accuracy and reliable reasoning.", "motivation": "To address the challenges of factual accuracy and reasoning reliability in clinical adoption of LLMs in the medical field, particularly focusing on the Japanese context.", "method": "A two-stage fine-tuning process on Qwen2.5-72B: Continued Pretraining (CPT) on a Japanese medical corpus followed by Reasoning Preference Optimization (RPO) for enhancing reliable reasoning pathways.", "result": "Preferred-MedLLM-Qwen-72B achieves a state-of-the-art performance of 0.868 accuracy on the IgakuQA benchmark, outperforming models like GPT-4o, and maintains accuracy even when generating reasoning explanations.", "conclusion": "Optimizing for reliable explanations alongside accuracy is crucial for clinical LLM adoption, and the model's weights are released to support further research in this area.", "key_contributions": ["Introduction of a model specifically optimized for the Japanese medical domain", "Demonstration of effective reasoning generation stabilization through RPO", "Release of model weights to aid further research in trustworthy LLMs for high-stakes applications."], "limitations": "", "future_work": "Further exploration into trustworthy LLMs for specialized applications in medicine and possibly other languages.", "keywords": ["Large Language Models", "Japanese medical domain", "Reasoning Preference Optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18085", "pdf": "https://arxiv.org/pdf/2504.18085.pdf", "abs": "https://arxiv.org/abs/2504.18085", "title": "Random-Set Large Language Models", "authors": ["Muhammad Mubashar", "Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 6 figures", "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating.", "AI": {"tldr": "This paper studies uncertainty quantification in Large Language Models (LLMs) and introduces a novel Random-Set LLM (RSLLM) approach for predicting belief functions over tokens, enhancing scalability and effectiveness in assessing model uncertainty.", "motivation": "To address the issue of trustworthiness in the text generated by LLMs by quantifying uncertainty in their predictions.", "method": "The authors propose the Random-Set Large Language Model (RSLLM), which predicts finite random sets over the token space and uses hierarchical clustering to define a budget of 'focal' subsets of tokens for scalable belief prediction.", "result": "RSLLM outperforms classical models on CoQA and OBQA datasets in terms of correctness and shows potential for estimating second-level uncertainty and detecting hallucinations in predictions.", "conclusion": "RSLLMs provide a more reliable framework for understanding and quantifying uncertainty in LLM outputs, enhancing their utility in real-world applications.", "key_contributions": ["Introduction of Random-Set Large Language Model (RSLLM) for better uncertainty quantification in LLMs.", "Use of hierarchical clustering to efficiently define focal subsets of tokens.", "Demonstration of RSLLM's superiority in correctness and uncertainty estimation over standard models."], "limitations": "", "future_work": "Exploration of further enhancements in quantifying and reducing model hallucinations and expanding dataset evaluations.", "keywords": ["Large Language Models", "Uncertainty Quantification", "Random-Set", "Hierarchical Clustering", "Token Space"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2504.18104", "pdf": "https://arxiv.org/pdf/2504.18104.pdf", "abs": "https://arxiv.org/abs/2504.18104", "title": "Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation", "authors": ["Yinglong Yu", "Hao Shen", "Zhengyi Lyu", "Qi He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In response to the growing problem of misinformation in the context of\nglobalization and informatization, this paper proposes a classification method\nfor fact-check-worthiness estimation based on prompt tuning. We construct a\nmodel for fact-check-worthiness estimation at the methodological level using\nprompt tuning. By applying designed prompt templates to large language models,\nwe establish in-context learning and leverage prompt tuning technology to\nimprove the accuracy of determining whether claims have fact-check-worthiness,\nparticularly when dealing with limited or unlabeled data. Through extensive\nexperiments on public datasets, we demonstrate that the proposed method\nsurpasses or matches multiple baseline methods in the classification task of\nfact-check-worthiness estimation assessment, including classical pre-trained\nmodels such as BERT, as well as recent popular large models like GPT-3.5 and\nGPT-4. Experiments show that the prompt tuning-based method proposed in this\nstudy exhibits certain advantages in evaluation metrics such as F1 score and\naccuracy, thereby effectively validating its effectiveness and advancement in\nthe task of fact-check-worthiness estimation.", "AI": {"tldr": "This paper proposes a prompt tuning method for estimating fact-check-worthiness, showing improved accuracy in classification tasks compared to various baseline models.", "motivation": "To address the growing problem of misinformation due to globalization and informatization.", "method": "A classification method for fact-check-worthiness estimation using prompt tuning applied to large language models with designed prompt templates.", "result": "The proposed method outperforms or matches multiple baseline methods, including classical models like BERT and large models like GPT-3.5 and GPT-4, in terms of evaluation metrics such as F1 score and accuracy.", "conclusion": "The prompt tuning-based method effectively enhances fact-check-worthiness estimation, especially in cases with limited or unlabeled data.", "key_contributions": ["Development of a novel prompt tuning approach for fact-check-worthiness estimation.", "Demonstration of advantages over classical and recent large model baselines.", "Validation of effectiveness in handling limited datasets."], "limitations": "", "future_work": "Exploration of additional applications of prompt tuning in other domains and further improvements in estimation accuracy.", "keywords": ["Fact-checking", "Prompt tuning", "Misinformation", "Large language models", "Classification"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.18106", "pdf": "https://arxiv.org/pdf/2504.18106.pdf", "abs": "https://arxiv.org/abs/2504.18106", "title": "Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering", "authors": ["Yinglong Yu", "Zhaopu Yao", "Fang Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies.", "AI": {"tldr": "This study investigates Chinese and English media reports on the Paris Olympics using LLM prompt engineering and topic modeling to identify discourse differences and similarities.", "motivation": "To explore how media discourse differs between Chinese and English reports on the Paris Olympics, looking at attitudinal meanings and thematic focus.", "method": "The study employs topic modeling and LLM prompt engineering alongside corpus phraseology methods to analyze media reports.", "result": "The analysis reveals that Chinese media emphasizes specific sports and positive sentiments, while English media highlights female athletes and exhibits mixed sentiments.", "conclusion": "The findings showcase distinct focus areas and sentiments in Chinese versus English media, reflecting cultural and regional perspectives on the Olympics.", "key_contributions": ["Use of LLM in analyzing media reports", "Identification of discourse similarities and differences", "Insights into cultural perspectives on the Olympics"], "limitations": "The analysis is limited to media reports and may not represent broader public opinion.", "future_work": "Future research could explore public response or social media sentiments surrounding the Olympics.", "keywords": ["topic modeling", "Large Language Models", "media analysis", "Paris Olympics", "attitudinal meanings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114.pdf", "abs": "https://arxiv.org/abs/2504.18114", "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.", "AI": {"tldr": "This paper evaluates the effectiveness of various hallucination detection metrics in language models and highlights significant limitations.", "motivation": "To address the challenges in measuring hallucinations in language models, which hinder their reliability and adoption.", "method": "The authors conducted a large-scale empirical evaluation of 6 sets of hallucination detection metrics across 4 datasets, 37 language models, and 5 decoding methods.", "result": "The study found that many metrics do not align with human judgments and show inconsistent performance as parameters scale, though LLM-based evaluation with GPT-4 performed best.", "conclusion": "The results indicate the need for more robust evaluation metrics for hallucinations and better methods to mitigate them.", "key_contributions": ["Large-scale empirical evaluation of hallucination metrics", "Identification of gaps in current evaluation methods", "Discovery that mode-seeking decoding methods help reduce hallucinations."], "limitations": "Metrics often do not align with human judgments and are myopic; performance varies with parameter scaling.", "future_work": "Develop more robust metrics and strategies for hallucination mitigation.", "keywords": ["hallucinations", "language models", "LLM evaluation", "GPT-4", "decoding methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.18128", "pdf": "https://arxiv.org/pdf/2504.18128.pdf", "abs": "https://arxiv.org/abs/2504.18128", "title": "Temporal Entailment Pretraining for Clinical Language Models over EHR Data", "authors": ["Tatsunori Tanaka", "Fi Zheng", "Kai Sato", "Zhifeng Li", "Yuanyun Zhang", "Shi Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Clinical language models have achieved strong performance on downstream tasks\nby pretraining on domain specific corpora such as discharge summaries and\nmedical notes. However, most approaches treat the electronic health record as a\nstatic document, neglecting the temporally-evolving and causally entwined\nnature of patient trajectories. In this paper, we introduce a novel temporal\nentailment pretraining objective for language models in the clinical domain.\nOur method formulates EHR segments as temporally ordered sentence pairs and\ntrains the model to determine whether a later state is entailed by,\ncontradictory to, or neutral with respect to an earlier state. Through this\ntemporally structured pretraining task, models learn to perform latent clinical\nreasoning over time, improving their ability to generalize across forecasting\nand diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and\ndemonstrate state of the art results on temporal clinical QA, early warning\nprediction, and disease progression modeling.", "AI": {"tldr": "This paper proposes a novel temporal entailment pretraining objective for clinical language models to enhance their ability to understand evolving patient trajectories in electronic health records (EHRs).", "motivation": "Current clinical language models fail to address the dynamic and temporal aspects of patient data in electronic health records, which limits their understanding of patient trajectories.", "method": "The authors introduce a pretraining task that formulates EHR segments as ordered sentence pairs, training models to assess entailment, contradiction, or neutrality between states over time.", "result": "The proposed method leads to state-of-the-art performance in temporal clinical question answering, early warning prediction, and disease progression modeling.", "conclusion": "Training language models with a focus on temporal relations improves their reasoning capabilities, enhancing their performance on forecasting and diagnosis tasks.", "key_contributions": ["Introduction of a temporal entailment pretraining objective for clinical language models", "Improved model generalization across clinical tasks", "Achievement of state-of-the-art results on several key tasks in the clinical domain."], "limitations": "", "future_work": "Exploration of further applications of the temporal framework in other areas of clinical research and refining the model for better performance.", "keywords": ["clinical language models", "temporal entailment", "electronic health records", "pretraining", "MIMIC IV"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18142", "pdf": "https://arxiv.org/pdf/2504.18142.pdf", "abs": "https://arxiv.org/abs/2504.18142", "title": "EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)", "authors": ["Fida Ullah", "Muhammad Ahmad", "Muhammad Tayyab Zamir", "Muhammad Arif", "Grigori sidorov", "Edgardo Manuel Felipe Riverón", "Alexander Gelbukh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) plays a pivotal role in various Natural\nLanguage Processing (NLP) tasks by identifying and classifying named entities\n(NEs) from unstructured data into predefined categories such as person,\norganization, location, date, and time. While extensive research exists for\nhigh-resource languages and general domains, NER in Urdu particularly within\ndomain-specific contexts like education remains significantly underexplored.\nThis is Due to lack of annotated datasets for educational content which limits\nthe ability of existing models to accurately identify entities such as academic\nroles, course names, and institutional terms, underscoring the urgent need for\ntargeted resources in this domain. To the best of our knowledge, no dataset\nexists in the domain of the Urdu language for this purpose. To achieve this\nobjective this study makes three key contributions. Firstly, we created a\nmanually annotated dataset in the education domain, named EDU-NER-2025, which\ncontains 13 unique most important entities related to education domain. Second,\nwe describe our annotation process and guidelines in detail and discuss the\nchallenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed\nkey linguistic challenges, such as morphological complexity and ambiguity,\nwhich are prevalent in formal Urdu texts.", "AI": {"tldr": "This paper presents a dataset for Named Entity Recognition (NER) in the Urdu language focused on the education domain, addressing the lack of annotated resources.", "motivation": "To address the significant underexploration of Named Entity Recognition (NER) in Urdu, particularly in education, due to the absence of annotated datasets.", "method": "Creation of a manually annotated dataset named EDU-NER-2025, which includes 13 unique educational entities, along with a detailed description of the annotation process and the challenges encountered.", "result": "The EDU-NER-2025 dataset facilitates improved NER in Urdu educational texts by providing targeted resources for academic roles and course names, which lack representation in existing datasets.", "conclusion": "This work lays the foundation for enhanced NER applications in Urdu education, highlighting the necessity for targeted datasets in underrepresented languages and domains.", "key_contributions": ["Creation of EDU-NER-2025, the first annotated dataset for NER in Urdu education", "Detailed annotation process and guidelines", "Analysis of linguistic challenges in Urdu texts"], "limitations": "The study is limited to the education domain and may not be applicable to other fields, potentially affecting generalizability.", "future_work": "Future research may expand the dataset to other domains and improve the NER models using this dataset.", "keywords": ["Named Entity Recognition", "Urdu Language", "Dataset Creation", "Education", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.18180", "pdf": "https://arxiv.org/pdf/2504.18180.pdf", "abs": "https://arxiv.org/abs/2504.18180", "title": "Aligning Language Models for Icelandic Legal Text Summarization", "authors": ["Þórir Hrafn Harðarson", "Hrafn Loftsson", "Stefán Ólafsson"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at NoDaLiDa 2025", "summary": "The integration of language models in the legal domain holds considerable\npromise for streamlining processes and improving efficiency in managing\nextensive workloads. However, the specialized terminology, nuanced language,\nand formal style of legal texts can present substantial challenges. This study\nexamines whether preference-based training techniques, specifically\nReinforcement Learning from Human Feedback and Direct Preference Optimization,\ncan enhance models' performance in generating Icelandic legal summaries that\nalign with domain-specific language standards and user preferences. We compare\nmodels fine-tuned with preference training to those using conventional\nsupervised learning. Results indicate that preference training improves the\nlegal accuracy of generated summaries over standard fine-tuning but does not\nsignificantly enhance the overall quality of Icelandic language usage.\nDiscrepancies between automated metrics and human evaluations further\nunderscore the importance of qualitative assessment in developing language\nmodels for the legal domain.", "AI": {"tldr": "This study investigates the use of preference-based training techniques to improve language model performance in generating legal summaries in Icelandic.", "motivation": "The integration of language models in the legal field aims to improve efficiency in managing extensive workloads but faces challenges due to specialized legal language.", "method": "The study compares preference-based training methods, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, with conventional supervised learning for generating Icelandic legal summaries.", "result": "Preference training enhances legal accuracy in summaries compared to standard fine-tuning but does not significantly improve overall language quality, highlighting discrepancies between automated metrics and human evaluations.", "conclusion": "Qualitative assessments are crucial in the development of language models for the legal domain despite improvements in accuracy with preference training.", "key_contributions": ["Introduces preference-based training techniques to the legal domain", "Demonstrates improved legal accuracy in summary generation", "Highlights the importance of qualitative assessment over automated metrics"], "limitations": "Limited improvement in overall language quality.", "future_work": "Further research is needed to enhance the qualitative aspects of language models in the legal context.", "keywords": ["language models", "legal summaries", "preference training", "Reinforcement Learning", "Icelandic"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2504.18221", "pdf": "https://arxiv.org/pdf/2504.18221.pdf", "abs": "https://arxiv.org/abs/2504.18221", "title": "Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish", "authors": ["Shuxiang Du", "Ana Guerberof Arenas", "Antonio Toral", "Kyo Gerrits", "Josep Marco Borillo"], "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This study examines the variability of Chat-GPT machine translation (MT)\noutputs across six different configurations in four languages,with a focus on\ncreativity in a literary text. We evaluate GPT translations in different text\ngranularity levels, temperature settings and prompting strategies with a\nCreativity Score formula. We found that prompting ChatGPT with a minimal\ninstruction yields the best creative translations, with \"Translate the\nfollowing text into [TG] creatively\" at the temperature of 1.0 outperforming\nother configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,\nChatGPT consistently underperforms compared to human translation (HT).", "AI": {"tldr": "This paper explores the variability of Chat-GPT's machine translation outputs across different configurations and languages, with an emphasis on creativity.", "motivation": "To investigate how differing settings in Chat-GPT influence the creativity of machine translations, especially in literary contexts.", "method": "The study evaluates translations across six configurations and four languages, using a Creativity Score to assess outcomes with varying text granularity, temperature settings, and prompting strategies.", "result": "The findings indicate that minimal instructions lead to the most creative translations, particularly at a temperature setting of 1.0, outperforming DeepL in Spanish, Dutch, and Chinese, though not surpassing human translation.", "conclusion": "Prompting ChatGPT effectively can enhance creativity in translations, but it still falls short of human translation quality.", "key_contributions": ["Examines the impact of different configurations on Chat-GPT's creative machine translations.", "Demonstrates that minimal instruction prompting yields superior creative output over other settings.", "Provides a comparative analysis of Chat-GPT and DeepL performance in various languages."], "limitations": "The study primarily focuses on literary texts and may not generalize across other types of text or contexts.", "future_work": "Further research could explore machine translation creativity in diverse genres and contexts, as well as improvements in ChatGPT's performance relative to human translation.", "keywords": ["Chat-GPT", "machine translation", "creativity", "literary text", "temperature settings"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2504.18225", "pdf": "https://arxiv.org/pdf/2504.18225.pdf", "abs": "https://arxiv.org/abs/2504.18225", "title": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family", "authors": ["Pierre-Carl Langlais", "Pavel Chizhov", "Mattia Nee", "Carlos Rosas Hinostroza", "Matthieu Delsart", "Irène Girard", "Othman Hicheur", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI.", "AI": {"tldr": "Introduction of new small reasoning models, Pleias-RAG-350m and Pleias-RAG-1B, for RAG, search, and source summarization.", "motivation": "To improve retrieval-augmented generation (RAG) with small models while maintaining performance in multilingual contexts and ensuring reference grounding.", "method": "Train mid-sized models on a large synthetic dataset emulating retrieval from multilingual open sources and integrate capabilities like query routing and source reranking.", "result": "Pleias-RAG models outperform smaller SLMs on RAG benchmarks and show competitive performance against larger models, ensuring consistent results across leading European languages.", "conclusion": "The small size and design of these models facilitate deployment on limited infrastructure, enhancing their usability for various generative AI applications.", "key_contributions": ["Introduction of Pleias-RAG-350m and Pleias-RAG-1B models.", "Demonstration of improved performance on standardized RAG benchmarks.", "Native support for citation and grounding with literal quotes."], "limitations": "", "future_work": "Exploration of additional use cases and refinement of model capabilities in live applications.", "keywords": ["RAG", "multilingual models", "AI applications"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.18246", "pdf": "https://arxiv.org/pdf/2504.18246.pdf", "abs": "https://arxiv.org/abs/2504.18246", "title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "authors": ["Ritesh Goru", "Shanay Mehta", "Prateek Jain"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.", "AI": {"tldr": "This paper introduces a method to fine-tune Large Language Models (LLMs) for multi-turn reasoning by employing response token duplication and a custom attention mask, addressing challenges in conventional training methodologies.", "motivation": "To enhance the performance of LLMs in generating explicit reasoning for tasks requiring multi-turn interactions, particularly in scenarios where conventional training methods hinder efficiency.", "method": "The proposed method utilizes response token duplication alongside a custom attention mask that adjusts visibility constraints, enabling LLMs to handle multi-turn reasoning effectively in a single forward pass.", "result": "This approach leads to a significant reduction in training time while allowing for efficient fine-tuning on multi-turn reasoning datasets, improving overall LLM performance in tasks requiring explicit reasoning.", "conclusion": "By addressing the limitations of previous fine-tuning methodologies, this paper presents a viable solution for training LLMs to better handle complex reasoning tasks over multiple turns.", "key_contributions": ["Introduction of response token duplication for LLM fine-tuning", "Development of a custom attention mask for visibility constraints", "Demonstration of improved training efficiency for multi-turn reasoning tasks"], "limitations": "The specific implications of the custom attention mask on model interpretability and potential trade-offs in performance across different tasks are not fully explored.", "future_work": "Further exploration of the generalization capabilities of this approach across diverse reasoning tasks and its impact on model interpretability is suggested.", "keywords": ["Large Language Models", "multi-turn reasoning", "fine-tuning", "attention mask", "token duplication"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2406.10370", "pdf": "https://arxiv.org/pdf/2406.10370.pdf", "abs": "https://arxiv.org/abs/2406.10370", "title": "Papers-to-Posts: Supporting Detailed Long-Document Summarization with an Interactive LLM-Powered Source Outline", "authors": ["Marissa Radensky", "Daniel S. Weld", "Joseph Chee Chang", "Pao Siangliulue", "Jonathan Bragg"], "categories": ["cs.HC"], "comment": "Revised for clearer message", "summary": "Compressing long and technical documents (e.g., >10 pages) into shorter-form\narticles (e.g., <2 pages) is critical for communicating information to\ndifferent audiences, for example, blog posts of scientific research paper or\nlegal briefs of dense court proceedings. While large language models (LLMs) are\npowerful tools for condensing large amounts of text, current interfaces to\nthese models lack support for understanding and controlling what content is\nincluded in a detailed summarizing article. Such capability is especially\nimportant for detail- and technical-oriented domains, in which tactical\nselection and coherent synthesis of key details is critical for effective\ncommunication to the target audience. For this, we present interactive reverse\nsource outlines, a novel mechanism for controllable long-form summarization\nfeaturing outline bullet points with automatic point selections that the user\ncan iteratively adjust to obtain an article with the desired content coverage.\nWe implement this mechanism in Papers-to-Posts, a new LLM-powered system for\nauthoring research-paper blog posts. Through a within-subjects lab study (n=20)\nand a between-subjects deployment study (n=37 blog posts, 26 participants), we\ncompare Papers-to-Posts to a strong baseline tool that provides an\nLLM-generated draft and access to free-form prompting. Under time constraints,\nPapers-to-Posts significantly increases writer satisfaction with blog post\nquality, particularly with respect to content coverage. Furthermore,\nquantitative results showed an increase in editing power (change in text for an\namount of time or writing actions) while using Papers-to-Posts, and qualitative\nresults showed that participants found incorporating key research-paper\ninsights in their blog posts easier while using Papers-to-Posts.", "AI": {"tldr": "The paper introduces Papers-to-Posts, an LLM-powered system that enhances the process of summarizing long, technical documents into shorter articles using interactive reverse source outlines for better content control.", "motivation": "There is a need for effective communication of long technical documents to varied audiences, which traditional LLM interfaces do not sufficiently address, especially in detail-oriented fields.", "method": "The proposed method involves interactive reverse source outlines that allow users to adjust outline bullet points iteratively for better summarization of content.", "result": "Papers-to-Posts significantly improves writer satisfaction with blog post quality and increases editing power compared to a baseline tool.", "conclusion": "The system makes it easier for users to incorporate critical insights from research papers into their blog posts, providing a more controlled summarization experience.", "key_contributions": ["Introduction of interactive reverse source outlines for summarization", "Implementation of Papers-to-Posts for authoring blog posts from research papers", "Demonstrated improved user satisfaction and content coverage in summarization"], "limitations": "Limited to the effectiveness of LLMs and the specific domain studied (blog posts from research papers).", "future_work": "Explore further enhancements to the interactive features and test additional user groups in various domains.", "keywords": ["long-form summarization", "human-computer interaction", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18260", "pdf": "https://arxiv.org/pdf/2504.18260.pdf", "abs": "https://arxiv.org/abs/2504.18260", "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment", "authors": ["Guanqun Bi", "Zhuang Chen", "Zhoufu Liu", "Hongkai Wang", "Xiyao Xiao", "Yuqiang Xie", "Wen Zhang", "Yongkang Huang", "Yuxuan Chen", "Libiao Peng", "Yi Feng", "Minlie Huang"], "categories": ["cs.CL"], "comment": "In progress", "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning.", "AI": {"tldr": "MAGI is a framework that automates structured clinical interviews for mental healthcare by using coordinated multi-agent collaboration based on the MINI protocol.", "motivation": "To enhance accessibility to mental healthcare through automation that aligns with psychiatric diagnostic protocols.", "method": "MAGI utilizes four specialized agents to navigate clinical logic and conduct interviews: a navigation agent, an adaptive question agent, a judgment agent, and a diagnosis agent that generates PsyCoT traces.", "result": "Experimental results show MAGI effectively combines clinical rigor with conversational adaptability, demonstrating improvements in mental health assessment.", "conclusion": "MAGI represents a significant advancement in LLM-assisted mental health assessments by ensuring compliance with established clinical protocols.", "key_contributions": ["Introduction of a multi-agent framework for clinical interviews", "Integration of conversational adaptability with psychiatric protocols", "Generation of explainable PsyCoT traces for symptom mapping"], "limitations": "Limited to the Mini International Neuropsychiatric Interview protocol; requires validation across diverse populations and conditions.", "future_work": "Exploration of additional diagnostic protocols and enhancement of agent capabilities to cover broader mental health conditions.", "keywords": ["automated clinical interviews", "large language models", "mental health", "psychometric assessment", "multi-agent framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.13900", "pdf": "https://arxiv.org/pdf/2409.13900.pdf", "abs": "https://arxiv.org/abs/2409.13900", "title": "Misty: UI Prototyping Through Interactive Conceptual Blending", "authors": ["Yuwen Lu", "Alan Leung", "Amanda Swearngin", "Jeffrey Nichols", "Titus Barik"], "categories": ["cs.HC"], "comment": null, "summary": "UI prototyping often involves iterating and blending elements from examples\nsuch as screenshots and sketches, but current tools offer limited support for\nincorporating these examples. Inspired by the cognitive process of conceptual\nblending, we introduce a novel UI workflow that allows developers to rapidly\nincorporate diverse aspects from design examples into work-in-progress UIs. We\nprototyped this workflow as Misty. Through an exploratory first-use study with\n14 frontend developers, we assessed Misty's effectiveness and gathered feedback\non this workflow. Our findings suggest that Misty's conceptual blending\nworkflow helps developers kickstart creative explorations, flexibly specify\nintent in different stages of prototyping, and inspires developers through\nserendipitous UI blends. Misty demonstrates the potential for tools that blur\nthe boundaries between developers and designers.", "AI": {"tldr": "Misty is a novel UI workflow that aids developers in rapidly incorporating elements from design examples into UI prototyping through conceptual blending.", "motivation": "Current UI prototyping tools lack support for blending design examples, which is essential for creative iterations.", "method": "An exploratory first-use study with 14 frontend developers was conducted to assess the effectiveness of Misty.", "result": "Misty facilitates creative exploration, enabling flexible intent specification during prototyping and inspiring developers with new UI blends.", "conclusion": "Misty shows promise in improving the prototyping process by bridging the gap between developers and designers.", "key_contributions": ["Introduces a novel workflow for UI prototyping that uses conceptual blending.", "Demonstrates the effectiveness of Misty through user feedback and studies.", "Encourages creative exploration and flexibility in design intent."], "limitations": "", "future_work": "Explore the integration of Misty with existing prototyping tools and further investigate its impact on various design processes.", "keywords": ["UI Prototyping", "Conceptual Blending", "HCI", "Frontend Development", "Design Tools"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269.pdf", "abs": "https://arxiv.org/abs/2504.18269", "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.", "AI": {"tldr": "The paper presents TextTIGER, a method for enhancing image generation from entity-based prompts using entity knowledge augmentation and summarization with LLMs, leading to improved performance.", "motivation": "The need for effective image generation from prompts containing specific entities while managing the vast and ever-growing knowledge of these entities.", "method": "TextTIGER augments entity knowledge in prompts and summarizes descriptions using LLMs to reduce performance degradation with longer inputs.", "result": "TextTIGER shows improved image generation performance evaluated on four models and five LLMs, outperforming caption-only prompts in standard metrics and through human assessment of description quality.", "conclusion": "The results affirm that refining prompts with enriched and summarized entity-related details significantly boosts image generation capabilities.", "key_contributions": ["Introduction of Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER)", "Creation of the WiT-Cub dataset for evaluating image generation based on entity knowledge", "Demonstrated improvements in image generation metrics using LLMs and entity augmentation."], "limitations": "The effectiveness may vary with different entity complexities and types, and the method's reliance on the quality of the initial prompts.", "future_work": "Exploration of more diverse entity characteristics and extending the approach to other generative tasks beyond image generation.", "keywords": ["Image Generation", "Entity Prompt Refinement", "Large Language Models", "Human-Computer Interaction", "Knowledge Augmentation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2410.14048", "pdf": "https://arxiv.org/pdf/2410.14048.pdf", "abs": "https://arxiv.org/abs/2410.14048", "title": "Co-Designing with Algorithms: Unpacking the Complex Role of GenAI in Interactive System Design Education", "authors": ["Hauke Sandhaus", "Quiquan Gu", "Maria Teresa Parreira", "Wendy Ju"], "categories": ["cs.HC", "K.3.1; K.3.2"], "comment": "Conditionally accepted to DIS'25", "summary": "Generative Artificial Intelligence (GenAI) is transforming Human-Computer\nInteraction (HCI) education and technology design, yet its impact remains\npoorly understood. This study explores how graduate students in an applied HCI\ncourse used GenAI tools during interactive device design. Despite no\nencouragement, all groups integrated GenAI into their workflows. Through 12\npost-class group interviews, we identified how GenAI co-design behaviors\npresent both benefits, such as enhanced creativity and faster design\niterations, and risks, including shallow learning and reflection. Benefits were\nmost evident during the execution phases, while the discovery and reflection\nphases showed limited gains. A taxonomy of usage patterns revealed that\nstudents' outcomes depended more on how they used GenAI than the specific tasks\nperformed. These findings highlight the need for HCI education to adapt to\nGenAI's role and offer recommendations for curricula to better prepare future\ndesigners for effective creative co-design.", "AI": {"tldr": "This study investigates the integration of Generative AI (GenAI) into Human-Computer Interaction (HCI) education, focusing on graduate students' use of GenAI tools in device design.", "motivation": "To understand the impact of Generative AI on Human-Computer Interaction education and technology design.", "method": "Qualitative analysis through 12 post-class group interviews with graduate students from an applied HCI course.", "result": "The study found that all groups incorporated GenAI into their workflows, highlighting benefits like increased creativity and faster iterations, but also risks such as shallow learning.", "conclusion": "HCI education must evolve to account for GenAI's influence, with recommendations for curricula to better prepare future designers.", "key_contributions": ["Insights into the benefits and risks of using GenAI in HCI education.", "A taxonomy of student usage patterns regarding GenAI in design processes.", "Recommendations for integrating GenAI into HCI curricula."], "limitations": "The study is based on a specific course and may not generalize to all educational contexts.", "future_work": "Further research is needed to explore how HCI education can adapt nationwide as GenAI continues to evolve.", "keywords": ["Generative AI", "Human-Computer Interaction", "HCI education", "device design", "creativity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.18346", "pdf": "https://arxiv.org/pdf/2504.18346.pdf", "abs": "https://arxiv.org/abs/2504.18346", "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "authors": ["Toghrul Abbasli", "Kentaroh Toyoda", "Yuan Wang", "Leon Witt", "Muhammad Asif Ali", "Yukai Miao", "Dan Li", "Qingsong Wei"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.", "AI": {"tldr": "This paper reviews Uncertainty Quantification (UQ) and calibration methods for Large Language Models (LLMs), providing a benchmark and evaluating the effectiveness of existing techniques.", "motivation": "To address the significant challenge of hallucination in LLMs by assessing and quantifying their uncertainty, a comprehensive analysis of existing UQ methods and their calibration is needed.", "method": "A systematic survey of prior works related to UQ and calibration for LLMs, along with an empirical evaluation of six related methods using reliability datasets.", "result": "The study introduces a benchmark to analyze calibration methods and presents significant findings regarding the effectiveness of these techniques in measuring uncertainty in LLMs.", "conclusion": "The paper concludes that their review is the first dedicated assessment of calibration methods specifically for LLMs and outlines future research directions in this critical area.", "key_contributions": ["Systematic survey of UQ and calibration methods for LLMs", "Introduction of a rigorous benchmark for evaluating existing methods", "Empirical evaluation of six UQ methods using reliability datasets"], "limitations": "The survey may not cover every possible UQ method or calibration technique due to the rapidly evolving nature of the field.", "future_work": "Outlooks include addressing open challenges in UQ and calibration for LLMs, with suggestions for further empirical evaluations and method improvements.", "keywords": ["Large Language Models", "Uncertainty Quantification", "Calibration Techniques", "Hallucination in AI", "Reliability Datasets"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13145", "pdf": "https://arxiv.org/pdf/2501.13145.pdf", "abs": "https://arxiv.org/abs/2501.13145", "title": "The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond", "authors": ["Xiang 'Anthony' Chen", "Tiffany Knearem", "Yang Li"], "categories": ["cs.HC"], "comment": null, "summary": "AI can now generate high-fidelity UI mock-up screens from a high-level\ntextual description, promising to support UX practitioners' work. However, it\nremains unclear how UX practitioners would adopt such Generative UI (GenUI)\nmodels in a way that is integral and beneficial to their work. To answer this\nquestion, we conducted a formative study with 37 UX-related professionals that\nconsisted of four roles: UX designers, UX researchers, software engineers, and\nproduct managers. Using a state-of-the-art GenUI tool, each participant went\nthrough a week-long, individual mini-project exercise with role-specific tasks,\nkeeping a daily journal of their usage and experiences with GenUI, followed by\na semi-structured interview. We report findings on participants' workflow using\nthe GenUI tool, how GenUI can support all and each specific roles, and existing\ngaps between GenUI and users' needs and expectations, which lead to design\nimplications to inform future work on GenUI development.", "AI": {"tldr": "The paper explores how UX practitioners adopt Generative UI (GenUI) tools by conducting a study with 37 professionals and reporting their experiences and needs.", "motivation": "Understanding the integration of Generative UI tools in UX work is essential for improving their design and effectiveness.", "method": "A formative study was conducted involving 37 UX professionals who engaged in a week-long individual project using a GenUI tool, documenting their experiences through journals and interviews.", "result": "The study provides insights into participant workflows, role-specific support from GenUI, and identifies gaps between GenUI capabilities and user expectations.", "conclusion": "The findings highlight crucial design implications for future GenUI development to better meet the needs of UX practitioners.", "key_contributions": ["Insights into the workflow of UX practitioners using GenUI tools", "Identification of role-specific needs and gaps in GenUI functionalities", "Design implications to enhance future GenUI tools"], "limitations": "The study is limited by its sample size and the specific GenUI tool used, which may not generalize across all contexts.", "future_work": "Future research should explore diverse GenUI tools, involve more participants, and address identified gaps to enhance adoption.", "keywords": ["Generative UI", "UX design", "human-computer interaction", "user experience", "professional workflow"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.18373", "pdf": "https://arxiv.org/pdf/2504.18373.pdf", "abs": "https://arxiv.org/abs/2504.18373", "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant", "authors": ["Lei Shen", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.", "AI": {"tldr": "Auto-SLURP is a benchmark dataset for evaluating LLM-based multi-agent frameworks focusing on intelligent personal assistants, enhancing the original SLURP dataset.", "motivation": "To address the lack of benchmark datasets for evaluating the performance of multi-agent frameworks powered by large language models (LLMs).", "method": "The original SLURP dataset was extended by relabeling data and integrating simulated servers and external services, allowing for a comprehensive evaluation of language understanding, task execution, and response generation.", "result": "Experiments show that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, revealing that dependable multi-agent personal assistants are still under development.", "conclusion": "The dataset and code are publicly available for further research and evaluation of LLM-based systems.", "key_contributions": ["Introduction of Auto-SLURP as a benchmark dataset for evaluating LLM-powered multi-agent systems.", "Enhancement of the original SLURP dataset through relabeling and integration of external services.", "Presentation of a comprehensive end-to-end evaluation pipeline for personal assistants."], "limitations": "Limited to the context of intelligent personal assistants and may not cover other applications of multi-agent frameworks.", "future_work": "Encouragement of further research to develop more reliable and intelligent multi-agent personal assistants.", "keywords": ["multi-agent systems", "large language models", "benchmark dataset", "intelligent personal assistants", "performance evaluation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2503.24249", "pdf": "https://arxiv.org/pdf/2503.24249.pdf", "abs": "https://arxiv.org/abs/2503.24249", "title": "Control Center Framework for Teleoperation Support of Automated Vehicles on Public Roads", "authors": ["Maria-Magdalena Wolf", "Niklas Krauss", "Arwed Schmidt", "Frank Diermeyer"], "categories": ["cs.HC"], "comment": null, "summary": "Implementing a teleoperation system with its various actors and interactions\nis challenging and requires an overview of the necessary functions. This work\ncollects all tasks that arise in a control center for an automated vehicle\nfleet from literature and assigns them to the two roles Remote Operator and\nFleet Manager. Focusing on the driving-related tasks of the remote operator, a\nprocess is derived that contains the sequence of tasks, associated vehicle\nstates, and transitions between the states. The resulting state diagram shows\nall remote operator actions available to effectively resolve automated vehicle\ndisengagements. Thus, the state diagram can be applied to existing legislation\nor modified based on prohibitions of specific interactions. The developed\ncontrol center framework and included state diagram should serve as a basis for\nimplementing and testing remote support for automated vehicles to be validated\non public roads.", "AI": {"tldr": "This paper presents a control center framework for remote operation of automated vehicle fleets, detailing the tasks required for effective teleoperation.", "motivation": "To address the challenges of implementing a teleoperation system for automated vehicles and to provide a structured overview of necessary functions.", "method": "The authors collected tasks from literature and categorized them into roles of Remote Operator and Fleet Manager, focusing on driving-related tasks for the Remote Operator. A state diagram was developed to illustrate the sequence of tasks and vehicle states.", "result": "The state diagram effectively maps out the actions available to the remote operator to manage automated vehicle disengagements and can be adapted to align with legislation.", "conclusion": "The developed framework and state diagram lay the groundwork for implementing remote support systems for automated vehicles, aiming for validation on public roads.", "key_contributions": ["Development of a control center framework for remote operation of automated vehicle fleets", "Creation of a state diagram for remote operator tasks and vehicle states", "Provision of a structure that can be aligned with legislation for teleoperations."], "limitations": "", "future_work": "Further validation of the framework on public roads and potential adaptations based on real-world scenarios.", "keywords": ["teleoperation", "automated vehicles", "remote operator", "state diagram", "control center framework"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376.pdf", "abs": "https://arxiv.org/abs/2504.18376", "title": "Pushing the boundary on Natural Language Inference", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.", "AI": {"tldr": "This paper presents a reinforcement learning-based approach for Natural Language Inference (NLI) using Group Relative Policy Optimization, eliminating labeled rationale needs and allowing training on challenging datasets, achieving state-of-the-art results with parameter-efficient techniques for language models.", "motivation": "To address the limitations of current NLI systems that rely on supervised learning and are affected by annotation artifacts and biases, thereby enhancing generalization and applicability in real-world scenarios.", "method": "A reinforcement learning framework using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, enabling the elimination of labeled rationales and fine-tuning of large language models.", "result": "The proposed approach demonstrates strong performance on standard and adversarial NLI benchmarks, with the 32B AWQ-quantized model outperforming state-of-the-art results on 7 out of 11 adversarial sets.", "conclusion": "The work presents a scalable, practical framework for developing robust NLI systems while maintaining high inference quality, even under aggressive quantization.", "key_contributions": ["Introduced a reinforcement learning-based approach for NLI using GRPO and CoT learning.", "Eliminated the dependency on labeled rationales for training.", "Achieved state-of-the-art performance on various NLI benchmark sets with large language models."], "limitations": "", "future_work": "", "keywords": ["Natural Language Inference", "Reinforcement Learning", "Chain-of-Thought", "Language Models", "Adversarial Benchmarks"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2504.13878", "pdf": "https://arxiv.org/pdf/2504.13878.pdf", "abs": "https://arxiv.org/abs/2504.13878", "title": "Learning by gaming, coding and making with EDUMING: A new approach to utilising atypical digital games for learning", "authors": ["Stefan Pietrusky"], "categories": ["cs.HC", "cs.CY"], "comment": "13 pages, 2 figures", "summary": "Papert's constructionism makes it clear that learning is particularly\neffective when learners create tangible artifacts and share and discuss them in\nsocial contexts. Technological progress in recent decades has created numerous\nopportunities for learners to not only passively consume media, but to actively\nshape it through construction. This article uses the EDUMING concept to present\na new method to simplify the development of digital learning games and thus\nsupport their integration into learning situations. A key difference between\nthe concept and established ideas such as game-based learning, gamification,\nserious games, etc. is that games are not closed and are consumed passively,\nbut can also be actively developed by users individually by modifying the\nsource code with the help of an IDE. As part of an empirical study, the\nusability of the game \"Professor Chip's Learning Quest\" (PCLQ) is recorded, as\nwell as previous experience with digital learning games and the acceptance and\nmotivation to use new technologies. The purpose of this article is to test the\nPCLQ digital learning game, developed according to the EDUMING concept, as part\nof an exploratory study regarding its usability, acceptance and suitability for\nuse in schools. The study is intended as a first empirical approach to\npractical testing of the concept.", "AI": {"tldr": "This paper explores the EDUMING concept, which facilitates the development and active user modification of digital learning games, focusing on an empirical study of the game 'Professor Chip's Learning Quest' regarding its usability and acceptance in educational settings.", "motivation": "To investigate how digital learning games can be effectively integrated into educational contexts by allowing users to modify and engage with them, following the principles of Papert's constructionism.", "method": "An empirical study was conducted to evaluate the usability, acceptance, and motivation associated with using the digital learning game 'Professor Chip's Learning Quest', developed according to the EDUMING concept.", "result": "The study recorded the usability of 'Professor Chip's Learning Quest' as well as users' previous experiences with digital learning games, highlighting acceptance and motivation metrics.", "conclusion": "The exploratory study provides initial empirical evidence for the PCLQ game as a viable educational tool that aligns with constructivist learning principles, suggesting potential for further integration into school curricula.", "key_contributions": ["Introduction of the EDUMING concept for active game modification", "Empirical evaluation of 'Professor Chip's Learning Quest' game", "Insights into user acceptance and motivation for digital learning games"], "limitations": "The study is explorative and may require further research for generalizable results across varied educational settings.", "future_work": "Future research could expand on different educational contexts and include longitudinal studies to assess the long-term effectiveness of games developed under the EDUMING concept.", "keywords": ["EDUMING", "digital learning games", "usability", "acceptance", "constructivism"], "importance_score": 5, "read_time_minutes": 13}}
{"id": "2504.18386", "pdf": "https://arxiv.org/pdf/2504.18386.pdf", "abs": "https://arxiv.org/abs/2504.18386", "title": "A UD Treebank for Bohairic Coptic", "authors": ["Amir Zeldes", "Nina Speransky", "Nicholas Wagner", "Caroline T. Schroeder"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advances in digital resources for other Coptic dialects,\nespecially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,\nlate Byzantine Egypt, and the contemporary language of the Coptic Church,\nremains critically under-resourced. This paper presents and evaluates the first\nsyntactically annotated corpus of Bohairic Coptic, sampling data from a range\nof works, including Biblical text, saints' lives and Christian ascetic writing.\nWe also explore some of the main differences we observe compared to the\nexisting UD treebank of Sahidic Coptic, the classical dialect of the language,\nand conduct joint and cross-dialect parsing experiments, revealing the unique\nnature of Bohairic as a related, but distinct variety from the more often\nstudied Sahidic.", "AI": {"tldr": "This paper presents the first syntactically annotated corpus of Bohairic Coptic and evaluates its distinct syntactic characteristics compared to Sahidic Coptic.", "motivation": "To address the lack of digital resources and syntactically annotated corpora for Bohairic Coptic, which is essential for understanding pre-Mamluk, late Byzantine Egypt and the contemporary Coptic Church language.", "method": "The authors compiled a corpus from various Bohairic texts, including Biblical works and Christian literature, and conducted parsing experiments to analyze syntactic differences between Bohairic and Sahidic Coptic.", "result": "The study unveils unique syntactic features of Bohairic Coptic and highlights key differences compared to the Sahidic dialect, providing insights into cross-dialect relationships.", "conclusion": "The first Bohairic Coptic corpus lays the groundwork for further research and resource development in under-studied Coptic dialects.", "key_contributions": ["First syntactically annotated corpus of Bohairic Coptic", "Comparison and analysis of Bohairic and Sahidic Coptic", "Parsing experiments highlighting syntactic differences"], "limitations": "", "future_work": "Future research could expand the corpus further and explore additional dialectical differences and their implications.", "keywords": ["Bohairic Coptic", "Sahidic Coptic", "syntactically annotated corpus", "natural language processing", "cross-dialect parsing"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2504.13948", "pdf": "https://arxiv.org/pdf/2504.13948.pdf", "abs": "https://arxiv.org/abs/2504.13948", "title": "Using customized GPT to develop prompting proficiency in architectural AI-generated images", "authors": ["Juan David Salazar Rodriguez", "Sam Conrad Joyce", "Julfendi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This research investigates the use of customized GPT models to enhance\nprompting proficiency among architecture students when generating AI-driven\nimages. Prompt engineering is increasingly essential in architectural education\ndue to the widespread adoption of generative AI tools. This study utilized a\nmixed-methods experimental design involving architecture students divided into\nthree distinct groups: a control group receiving no structured support, a\nsecond group provided with structured prompting guides, and a third group\nsupported by both structured guides and interactive AI personas. Students\nengaged in reverse engineering tasks, first guessing provided image prompts and\nthen generating their own prompts, aiming to boost critical thinking and\nprompting skills. Variables examined included time spent prompting, word count,\nprompt similarity, and concreteness. Quantitative analysis involved correlation\nassessments between these variables and a one-way ANOVA to evaluate differences\nacross groups. While several correlations showed meaningful relationships, not\nall were statistically significant. ANOVA results indicated statistically\nsignificant improvements in word count, similarity, and concreteness,\nespecially in the group supported by AI personas and structured prompting\nguides. Qualitative feedback complemented these findings, revealing enhanced\nconfidence and critical thinking skills in students. These results suggest\ntailored GPT interactions substantially improve students' ability to\ncommunicate architectural concepts clearly and effectively.", "AI": {"tldr": "This research explores how customized GPT models can improve prompting skills in architecture students for generating AI-driven images.", "motivation": "The rise of generative AI tools necessitates effective prompt engineering in architectural education.", "method": "A mixed-methods experimental design with three student groups: one with no support, one with structured prompts, and one with AI personas and structured prompts. Students engaged in reverse engineering tasks to enhance their prompting skills, with various quantitative metrics analyzed.", "result": "Statistically significant improvements in word count, similarity, and concreteness were found in the group using AI personas and structured guides.", "conclusion": "Tailored GPT interactions can significantly enhance students' communication of architectural concepts.", "key_contributions": ["Demonstrated the effectiveness of structured prompting guides in architectural education.", "Showed the benefits of interactive AI personas in enhancing students' critical thinking and prompting skills.", "Provided quantitative and qualitative evidence of improvements in prompting proficiency."], "limitations": "Correlations observed were not all statistically significant, suggesting variability in the effectiveness of the methods used.", "future_work": "Further exploration of AI-assisted pedagogical methods in different educational contexts and disciplines is suggested.", "keywords": ["generative AI", "prompt engineering", "architectural education", "GPT models", "mixed-methods research"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406.pdf", "abs": "https://arxiv.org/abs/2504.18406", "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch.", "AI": {"tldr": "Introducing HRScene, a benchmark for evaluating Vision Large Language Models (VLMs) in high-resolution image understanding.", "motivation": "The paper addresses the lack of comprehensive benchmarks for evaluating the HRI understanding capabilities of Vision Large Language Models (VLMs).", "method": "HRScene is a unified benchmark incorporating 25 real-world datasets and 2 synthetic diagnostic datasets, with detailed annotations provided by graduate students. It encompasses various scenarios including pathological and agricultural images.", "result": "Current VLMs exhibit average accuracy of around 50% in understanding real-world high-resolution images, with significant gaps identified in their effectiveness to utilize regions in the images.", "conclusion": "The findings indicate that there are considerable challenges in the ability of VLMs to understand high-resolution images, which points to essential areas for future research.", "key_contributions": ["Introduction of the HRScene benchmark for high-resolution image understanding.", "Comprehensive evaluation of 28 VLMs demonstrating their current limitations.", "Identification of significant regional divergence in VLM performance."], "limitations": "The performance of VLMs is currently limited, with average accuracy suggesting room for significant improvement.", "future_work": "The study highlights the need for improved VLM models, specifically in effectively utilizing regions in HRI and addressing the gaps shown in the experiments.", "keywords": ["High-Resolution Image Understanding", "Vision Large Language Models", "HRScene Benchmark"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.18412", "pdf": "https://arxiv.org/pdf/2504.18412.pdf", "abs": "https://arxiv.org/abs/2504.18412", "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers", "authors": ["Jared Moore", "Declan Grabb", "William Agnew", "Kevin Klyman", "Stevie Chancellor", "Desmond C. Ong", "Nick Haber"], "categories": ["cs.CL"], "comment": null, "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy.", "AI": {"tldr": "This paper evaluates the feasibility of using large language models (LLMs) as therapists, highlighting significant shortcomings and recommending against their replacement of human therapists.", "motivation": "Investigate the suitability of LLMs as mental health providers, addressing concerns raised in the tech startup and research space about their potential replacement of traditional therapists.", "method": "Conducted a mapping review of therapy guides and executed experiments assessing LLM responses to determine their adherence to critical therapeutic relationship aspects.", "result": "LLMs demonstrated stigma and provided inappropriate responses in therapy scenarios, actively undermining therapeutic relationships and encouraging harmful thought patterns.", "conclusion": "LLMs should not replace human therapists due to inherent limitations in replicating therapeutic alliances and human characteristics necessary for effective therapy.", "key_contributions": ["Assessment of LLMs' appropriateness in therapeutic contexts", "Identification of critical therapeutic alliance characteristics", "Highlighting current safety practices' inadequacy in LLM responses"], "limitations": "LLMs lack essential human characteristics required for therapeutic alliances, affecting their suitability as replacements for therapists.", "future_work": "Explore alternative roles for LLMs in therapy that do not involve direct replacement of human therapists.", "keywords": ["Large Language Models", "Therapy", "Mental Health", "Therapeutic Alliance", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.18415", "pdf": "https://arxiv.org/pdf/2504.18415.pdf", "abs": "https://arxiv.org/abs/2504.18415", "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs", "authors": ["Hongyu Wang", "Shuming Ma", "Furu Wei"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.", "AI": {"tldr": "BitNet v2 enables efficient deployment of 1-bit LLMs through 4-bit activation quantization, minimizing performance degradation and resource usage.", "motivation": "The deployment of 1-bit Large Language Models is hampered by activation outliers that complicate low bit-width quantization.", "method": "BitNet v2 utilizes H-BitLinear, which applies an online Hadamard transformation before activation quantization to create Gaussian-like distributions from sharp activations.", "result": "BitNet v2 trained from scratch with 8-bit activations performs similarly to BitNet b1.58, maintaining performance with native 4-bit activations while reducing memory and computational costs during batched inference.", "conclusion": "The proposed framework demonstrates significant efficiency gains in deploying 1-bit LLMs with improved activation quantization.", "key_contributions": ["Introduction of BitNet v2 for 4-bit quantization of 1-bit LLMs", "Development of H-BitLinear for mitigating activation outliers", "Demonstration of minimal performance degradation with low-bit activations"], "limitations": "", "future_work": "", "keywords": ["Large Language Models", "Quantization", "Activation Outliers", "H-BitLinear", "Efficiency"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428.pdf", "abs": "https://arxiv.org/abs/2504.18428", "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.", "AI": {"tldr": "Introduction of PolyMath, a multilingual mathematical reasoning benchmark.", "motivation": "To create a comprehensive, multilingual benchmark for evaluating reasoning LLMs across various languages and difficulty levels.", "method": "Development of a benchmark covering 18 languages and 4 difficulty levels, with a focus on difficulty comprehensiveness, language diversity, and quality of translation.", "result": "Evaluation shows that advanced LLMs score low on the benchmark, highlighting significant challenges in multilingual reasoning.", "conclusion": "Controlling output language in instructions can enhance reasoning performance in LLMs, especially for low-resource languages.", "key_contributions": ["Introduction of a diverse multilingual benchmark for mathematical reasoning.", "Identification of key challenges faced by LLMs in multilingual contexts.", "Demonstration of how output language control can influence LLM reasoning performance."], "limitations": "Benchmark scores show low accuracy for current state-of-the-art LLMs across different languages.", "future_work": "Exploration of methods to improve reasoning performance in LLMs for low-resource languages.", "keywords": ["multilingual benchmark", "mathematical reasoning", "LLMs", "language diversity", "output language control"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458.pdf", "abs": "https://arxiv.org/abs/2504.18458", "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy.", "AI": {"tldr": "Introduction of the FAST framework for dynamic reasoning in large vision-language models (LVLMs) that reduces verbosity while improving accuracy.", "motivation": "Addressing the overthinking phenomenon in LVLMs that causes verbose reasoning across tasks.", "method": "Development of the FAST framework that employs model-based metrics for question characterization, an adaptive reward mechanism, and difficulty-aware KL regularization.", "result": "FAST shows state-of-the-art accuracy with over 10% improvement vs. base model and reduces token usage by 32.7-67.3%.", "conclusion": "FAST effectively balances reasoning length and accuracy in LVLMs.", "key_contributions": ["Introduction of the FAST framework for adaptive reasoning", "Demonstration of significant improvements in accuracy", "Reduction in verbosity and token usage across benchmarks."], "limitations": "", "future_work": "Exploration of further applications of the FAST framework and its adaptations to other models.", "keywords": ["vision-language models", "FAST framework", "adaptive reasoning", "machine learning", "token usage"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.18474", "pdf": "https://arxiv.org/pdf/2504.18474.pdf", "abs": "https://arxiv.org/abs/2504.18474", "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions", "authors": ["James D. Finch", "Yasasvi Josyula", "Jinho D. Choi"], "categories": ["cs.CL"], "comment": "Accepted (B) to TACL 2025", "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development.", "AI": {"tldr": "Presenting a novel method for Slot Schema Induction in task-oriented dialogue systems using language models to improve schema identification and evaluation metrics.", "motivation": "To enhance the automation of identifying key information slots in task-oriented dialogue systems without requiring manual labeling, addressing existing challenges in evaluation methods.", "method": "This study formulates Slot Schema Induction as a text generation task and develops a fully automatic LLM-based dialogue simulation method that generates high-quality slot schemas from dialogue data.", "result": "The proposed approach improves the state-of-the-art in Slot Schema Induction by effectively creating reliable evaluation metrics and data for task-oriented dialogue systems.", "conclusion": "The contributions lay a foundation for future research in Slot Schema Induction, improving dialogue understanding and system development.", "key_contributions": ["New approach for Slot Schema Induction formulated as a text generation task using language models.", "Development of a fully automatic LLM-based TOD simulation method for generating schema data.", "Improved evaluation metrics and data addressing existing SSI evaluation challenges."], "limitations": "The method's reliance on the quality of generated dialogue data and potential biases in human-guided evaluations.", "future_work": "Further exploration of SSI applications in varied domains and refining evaluation techniques based on real-world dialogue applications.", "keywords": ["Slot Schema Induction", "task-oriented dialogue", "language models", "evaluation metrics", "dialogue understanding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.18483", "pdf": "https://arxiv.org/pdf/2504.18483.pdf", "abs": "https://arxiv.org/abs/2504.18483", "title": "Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues", "authors": ["Leandra Fichtel", "Maximilian Spliethöver", "Eyke Hüllermeier", "Patricia Jimenez", "Nils Klowait", "Stefan Kopp", "Axel-Cyrille Ngonga Ngomo", "Amelie Robrecht", "Ingrid Scharlau", "Lutz Terfloth", "Anna-Lisa Vollmer", "Henning Wachsmuth"], "categories": ["cs.CL"], "comment": "Submitted to the SIGDial Conference 2025", "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.", "AI": {"tldr": "The paper examines the role of large language models in co-constructive explanation dialogues to enhance understanding among explainees.", "motivation": "To explore how LLMs can be used to create explanations that are adapted to the needs of explainees, thus improving the field of explainable AI.", "method": "A user study was conducted where explainees interacted with LLMs tasked with providing co-constructive explanations on a predefined topic. The study evaluated changes in understanding and perceptions of the LLMs' behavior.", "result": "The study found that LLMs exhibit some co-constructive behaviors, such as asking verification questions, which can engage explainees and improve their understanding, although their overall effectiveness in monitoring understanding remains limited.", "conclusion": "While LLMs show potential in enhancing dialogues through co-constructive explanations, further advancements are needed for more effective understanding tracking and adaptive explanations.", "key_contributions": ["Evaluation of LLMs in co-constructive explanations", "User study on explainee engagement", "Identification of limitations in LLMs' monitoring abilities"], "limitations": "LLMs' ability to monitor explainees' understanding and adapt explanations in real-time is currently limited.", "future_work": "Future research should focus on improving LLMs' capabilities in understanding and dynamically adjusting to explainees' comprehension levels.", "keywords": ["explainable AI", "co-constructive explanations", "large language models", "user study", "understanding enhancement"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.18535", "pdf": "https://arxiv.org/pdf/2504.18535.pdf", "abs": "https://arxiv.org/abs/2504.18535", "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation", "authors": ["Gwen Yidou Weng", "Benjie Wang", "Guy Van den Broeck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes.", "AI": {"tldr": "TRACE is a new framework for controlling large language model outputs through tractable probabilistic reasoning, allowing for efficient computation of expected attributes and global compliance with low overhead.", "motivation": "The paper addresses the challenge of aligning large language models with human values and desired attributes, highlighting the limitations of current tuning methods and the need for efficient control mechanisms.", "method": "TRACE distills a Hidden Markov Model from a large language model and pairs it with a classifier to compute Expected Attribute Probability (EAP) efficiently, allowing for quick adaptation to new attributes and maintaining global properties.", "result": "TRACE demonstrates state-of-the-art performance in detoxification tasks, achieving this with only a 10% increase in decoding time, and can adapt to 76 low-resource personalized LLMs in seconds.", "conclusion": "The TRACE framework provides an efficient method for controllable generation in language models, enabling more consistent alignment with desired attributes while maintaining performance.", "key_contributions": ["Introduction of the TRACE framework for controlled generation.", "Efficient computation of Expected Attribute Probability using Hidden Markov Models and classifiers.", "Demonstrated capability to adapt quickly to new attributes and composite characteristics."], "limitations": "", "future_work": "Future research could explore further extensions of TRACE to additional attributes and other natural language processing tasks, as well as potential improvements in computational efficiency.", "keywords": ["large language models", "controllable generation", "Hidden Markov Model", "Expected Attribute Probability", "detoxification"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934.pdf", "abs": "https://arxiv.org/abs/2504.17934", "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation.", "AI": {"tldr": "This position paper discusses the privacy and security risks of LLM-powered GUI agents and advocates for a human-centered evaluation framework.", "motivation": "The advent of LLMs has transformed GUI automation, but their handling of sensitive data poses privacy and security concerns that have not been adequately addressed.", "method": "The paper identifies three key risks of GUI agents, reviews existing evaluation metrics for GUI and LLM agents, and outlines challenges in human evaluator integration for assessments.", "result": "It reveals a significant focus on performance in evaluations, neglecting privacy and security considerations; this gap necessitates a new evaluation framework.", "conclusion": "The paper calls for incorporating risk assessments, in-context consent, and enhanced privacy considerations in the design and evaluation of GUI agents.", "key_contributions": ["Identification of privacy and security risks unique to LLM-powered GUI agents.", "Review of current evaluation metrics and their limitations.", "Proposal for a human-centered evaluation framework for GUI agents."], "limitations": "Limited exploration of existing data on GUI agent privacy and security assessments.", "future_work": "Further development and testing of the proposed human-centered evaluation framework and integration of user privacy factors in GUI agents.", "keywords": ["Large Language Models", "GUI Automation", "Privacy", "Security", "Human-Centered Design"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.03818", "pdf": "https://arxiv.org/pdf/2404.03818.pdf", "abs": "https://arxiv.org/abs/2404.03818", "title": "PRobELM: Plausibility Ranking Evaluation for Language Models", "authors": ["Zhangdie Yuan", "Eric Chamoun", "Rami Aly", "Chenxi Whitehouse", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language\nModels), a benchmark designed to assess language models' ability to discern\nmore plausible from less plausible scenarios through their parametric\nknowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or\ntruthfulness, and others such as COPA explore plausible scenarios without\nexplicitly incorporating world knowledge, PRobELM seeks to bridge this gap by\nevaluating models' capabilities to prioritise plausible scenarios that leverage\nworld knowledge over less plausible alternatives. This design allows us to\nassess the potential of language models for downstream use cases such as\nliterature-based discovery where the focus is on identifying information that\nis likely but not yet known. Our benchmark is constructed from a dataset\ncurated from Wikidata edit histories, tailored to align the temporal bounds of\nthe training data for the evaluated models. PRobELM facilitates the evaluation\nof language models across multiple prompting types, including statement, text\ncompletion, and question-answering. Experiments with 10 models of various sizes\nand architectures on the relationship between model scales, training recency,\nand plausibility performance, reveal that factual accuracy does not directly\ncorrelate with plausibility performance and that up-to-date training data\nenhances plausibility assessment across different model architectures.", "AI": {"tldr": "This paper presents PRobELM, a benchmark for evaluating language models' capability to discern plausible scenarios using world knowledge, bridging gaps left by existing benchmarks.", "motivation": "To evaluate language models' abilities to prioritize plausible scenarios that leverage world knowledge, addressing limitations in existing benchmarks focused on truthfulness and factual accuracy.", "method": "The benchmark is constructed from a dataset curated from Wikidata edit histories, allowing evaluation across multiple prompting types like statement, text completion, and question-answering.", "result": "Experiments reveal that there is no direct correlation between factual accuracy and plausibility performance, while up-to-date training data significantly enhances plausibility assessment across different model architectures.", "conclusion": "PRobELM can facilitate literature-based discovery by enabling language models to identify likely but not yet known information, showcasing the importance of plausibility in language understanding.", "key_contributions": ["Introduces PRobELM benchmark for plausibility ranking in language models.", "Demonstrates that factual accuracy does not equate to plausibility performance.", "Shows that training data recency improves plausibility assessment in models."], "limitations": "The benchmark may require extensive refinement to encompass all aspects of plausibility in language understanding and its practical applications are yet to be fully explored.", "future_work": "Future directions include expanding the dataset for richer evaluation and exploring the implications of plausibility in various downstream applications.", "keywords": ["language models", "plausibility ranking", "benchmark", "world knowledge", "model evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.19325", "pdf": "https://arxiv.org/pdf/2405.19325.pdf", "abs": "https://arxiv.org/abs/2405.19325", "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution", "authors": ["Minghan Li", "Xilun Chen", "Ari Holtzman", "Beidi Chen", "Jimmy Lin", "Wen-tau Yih", "Xi Victoria Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often hallucinate and lack the ability to\nprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,\napproach these limitations by refining the output of an LM for a given prompt\nusing its nearest neighbor matches in a non-parametric data store. However,\nthese models often exhibit slow inference speeds and produce non-fluent texts.\nIn this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a\nnovel semi-parametric language modeling approach that is capable of\nincorporating real-world text spans of arbitrary length into the LM generations\nand providing attribution to their sources. NEST performs token-level retrieval\nat each inference step to compute a semi-parametric mixture distribution and\nidentify promising span continuations in a corpus. It then uses an approximate\nspeculative decoding procedure that accepts a prefix of the retrieved span or\ngenerates a new token. NEST significantly enhances the generation quality and\nattribution rate of the base LM across a variety of knowledge-intensive tasks,\nsurpassing the conventional kNN-LM method and performing competitively with\nin-context retrieval augmentation. In addition, NEST substantially improves the\ngeneration speed, achieving a 1.8x speedup in inference time when applied to\nLlama-2-Chat 70B. Code will be released at\nhttps://github.com/facebookresearch/NEST/tree/main.", "AI": {"tldr": "NEST is a novel semi-parametric language modeling approach that improves inference speed and generation quality while providing attribution for language model outputs.", "motivation": "To address hallucination and attribution limitations in large language models (LLMs), while improving generation speed and fluency.", "method": "NEST employs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations, using approximate speculative decoding.", "result": "NEST enhances generation quality and attribution rate, with a significant improvement in inference speed (1.8x faster) compared to traditional kNN-LM methods.", "conclusion": "NEST outperforms conventional kNN-LM approaches and is competitive with in-context retrieval augmentation, making it an effective solution for knowledge-intensive tasks.", "key_contributions": ["Introduction of Nearest Neighbor Speculative Decoding (NEST)", "Significant speedup in inference time", "Improved generation quality and attribution capabilities"], "limitations": "", "future_work": "Exploration of further enhancements in retrieval techniques and broader application scenarios in language modeling.", "keywords": ["Language Models", "NLP", "Semi-parametric Models", "Speculative Decoding", "Attribution"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2406.10432", "pdf": "https://arxiv.org/pdf/2406.10432.pdf", "abs": "https://arxiv.org/abs/2406.10432", "title": "AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction", "authors": ["Peitao Han", "Lis Kanashiro Pereira", "Fei Cheng", "Wan Jou She", "Eiji Aramaki"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 SRW", "summary": "Existing in-context learning (ICL) methods for relation extraction (RE) often\nprioritize language similarity over structural similarity, which can lead to\noverlooking entity relationships. To address this, we propose an AMR-enhanced\nretrieval-based ICL method for RE. Our model retrieves in-context examples\nbased on semantic structure similarity between task inputs and training\nsamples. Evaluations on four standard English RE datasets show that our model\noutperforms baselines in the unsupervised setting across all datasets. In the\nsupervised setting, it achieves state-of-the-art results on three datasets and\ncompetitive results on the fourth.", "AI": {"tldr": "This paper presents an AMR-enhanced retrieval-based in-context learning method for relation extraction that focuses on semantic structure similarity, outperforming existing models.", "motivation": "To improve relation extraction by addressing the limitations of existing in-context learning methods which often focus on language similarity rather than structural similarity, potentially overlooking entity relationships.", "method": "The proposed method retrieves examples based on semantic structure similarity between the inputs and training samples for relation extraction tasks.", "result": "Evaluations demonstrated that the model outperforms existing baselines in unsupervised settings across four datasets and achieves state-of-the-art results in supervised settings on three datasets.", "conclusion": "The AMR-enhanced retrieval-based model significantly improves relation extraction performance by leveraging semantic structural similarities.", "key_contributions": ["Introduction of an AMR-enhanced model for relation extraction", "Demonstration of improved performance in unsupervised and supervised settings", "Focus on semantic structure similarity in in-context learning"], "limitations": "", "future_work": "", "keywords": ["Relation Extraction", "In-Context Learning", "AMR Enhancement"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2406.10602", "pdf": "https://arxiv.org/pdf/2406.10602.pdf", "abs": "https://arxiv.org/abs/2406.10602", "title": "Multilingual Large Language Models and Curse of Multilinguality", "authors": ["Daniil Gurgurov", "Tanja Bäumel", "Tatiana Anikina"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.", "AI": {"tldr": "An overview of multilingual Large Language Models (LLMs), discussing their architectures, training techniques, and limitations, particularly the challenges posed by multilinguality.", "motivation": "To provide an introductory overview of multilingual LLMs and their technical details to NLP researchers and practitioners.", "method": "The paper analyzes the underlying architectures of different model types (encoder-only, decoder-only, and encoder-decoder) and their related components such as pre-training data and tokenization methods.", "result": "The exploration of multilingual LLMs reveals their strengths in various languages and tasks, while also highlighting the challenge of managing the curse of multilinguality.", "conclusion": "Addressing the limitations and exploring current solutions to the challenges faced by multilingual LLMs could enhance their effectiveness in diverse applications.", "key_contributions": ["Introductory overview of multilingual LLM architectures and characteristics.", "Discussion of the curse of multilinguality and ongoing efforts to mitigate its effects.", "Detailed analysis of various model types and their respective methodologies."], "limitations": "The paper mainly focuses on the technical aspects and may not delve into application-specific use cases or practical implementation.", "future_work": "Future research directions include exploring more effective strategies for addressing multilinguality and improving the training of multilingual models for better performance.", "keywords": ["Multilingual LLMs", "Natural Language Processing", "Model architectures", "Pre-training data", "Tokenization"], "importance_score": 8, "read_time_minutes": 25}}
{"id": "2408.06276", "pdf": "https://arxiv.org/pdf/2408.06276.pdf", "abs": "https://arxiv.org/abs/2408.06276", "title": "Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation", "authors": ["Jieyong Kim", "Hyunseo Kim", "Hyunjin Cho", "SeongKu Kang", "Buru Chang", "Jinyoung Yeo", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to SIGIR 2025", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.", "AI": {"tldr": "The paper introduces EXP3RT, an LLM-based recommender that enhances recommendation systems by leveraging user and item review information to improve rating prediction and explainability.", "motivation": "Current recommendation systems have not fully utilized the capabilities of Large Language Models, often limited by input information and reasoning capabilities.", "method": "EXP3RT is fine-tuned from a teacher LLM to perform three tasks: extracting subjective preferences from reviews, creating user and item profiles, and generating reasoning followed by predicted ratings.", "result": "EXP3RT outperforms existing methods in rating prediction and item reranking while improving explainability in recommendation systems.", "conclusion": "The personalized reasoning approach of EXP3RT enhances recommendation accuracy and provides clear explanations for decisions made by the system.", "key_contributions": ["Introduction of EXP3RT, a novel LLM-based recommender system.", "Improved accuracy in rating prediction and item reranking through enhanced reasoning.", "Increased explainability in recommendations based on user and item profiles."], "limitations": "", "future_work": "Future research may explore further enhancements in recommendation accuracy and additional applications of LLMs in various domains.", "keywords": ["Large Language Models", "Recommendation Systems", "Explainability", "User Preferences", "Rating Prediction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.16073", "pdf": "https://arxiv.org/pdf/2408.16073.pdf", "abs": "https://arxiv.org/abs/2408.16073", "title": "Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings", "authors": ["Leo Yeykelis", "Kaavya Pichai", "James J. Cummings", "Byron Reeves"], "categories": ["cs.CL", "cs.AI"], "comment": "40 pages, 13 figures, 3 tables", "summary": "This report analyzes the potential for large language models (LLMs) to\nexpedite accurate replication and generalization of published research about\nmessage effects in marketing. LLM-powered participants (personas) were tested\nby replicating 133 experimental findings from 14 papers containing 45 recent\nstudies published in the Journal of Marketing. For each study, the measures,\nstimuli, and sampling specifications were used to generate prompts for LLMs to\nact as unique personas. The AI personas, 19,447 in total across all of the\nstudies, generated complete datasets and statistical analyses were then\ncompared with the original human study results. The LLM replications\nsuccessfully reproduced 76% of the original main effects (84 out of 111),\ndemonstrating strong potential for AI-assisted replication. The overall\nreplication rate including interaction effects was 68% (90 out of 133).\nFurthermore, a test of how human results generalized to different participant\nsamples, media stimuli, and measures showed that replication results can change\nwhen tests go beyond the parameters of the original human studies. Implications\nare discussed for the replication and generalizability crises in social\nscience, the acceleration of theory building in media and marketing psychology,\nand the practical advantages of rapid message testing for consumer products.\nLimitations of AI replications are addressed with respect to complex\ninteraction effects, biases in AI models, and establishing benchmarks for AI\nmetrics in marketing research.", "AI": {"tldr": "This report examines how large language models (LLMs) can replicate and generalize published marketing research findings, showing a 76% success rate in replicating original experimental results.", "motivation": "To investigate the capability of large language models (LLMs) in replicating and generalizing marketing research findings to address the replication and generalizability crises in social science.", "method": "The study utilized LLM-powered personas to replicate 133 findings from 14 marketing papers, generating prompts based on original study specifications to create datasets and perform statistical analyses.", "result": "LLM replications reproduced 76% of the original main effects and 68% of overall replication rates, indicating strong potential for AI-assisted replication in marketing research.", "conclusion": "The findings suggest that LLMs can be effective tools in accelerating message testing and theory building in marketing psychology, although limitations exist regarding complex effects and biases.", "key_contributions": ["Demonstrated the potential of LLMs in research replication and generalization.", "Provided empirical evidence supporting LLMs for rapid message testing in consumer marketing.", "Addressed implications for marketing psychology and replicability challenges in social science."], "limitations": "Limitations include challenges with complex interaction effects, biases present in AI models, and the need for establishing benchmarks for AI metrics in marketing research.", "future_work": "Future research should explore refining AI models for better accuracy in complex interactions and expanding applications to other fields of social science.", "keywords": ["Large Language Models", "Replication Research", "Marketing Psychology", "AI in Social Science", "Message Testing"], "importance_score": 8, "read_time_minutes": 40}}
{"id": "2409.08813", "pdf": "https://arxiv.org/pdf/2409.08813.pdf", "abs": "https://arxiv.org/abs/2409.08813", "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment", "authors": ["Leitian Tao", "Yixuan Li"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2025", "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.", "AI": {"tldr": "This paper studies the use of weak large language models (LLMs) for feedback in alignment tasks, demonstrating their efficacy in generating high-quality feedback comparable to human annotation at lower resource costs.", "motivation": "To address the high resource demands of traditional alignment frameworks for large language models while ensuring they align with human values and intentions.", "method": "The authors conducted a systematic study involving qualitative and quantitative analyses to evaluate the feedback generated by weak LLMs compared to fully human-annotated data.", "result": "Empirical findings show that feedback from weak LLMs can match or surpass the quality of human feedback, with limited dependence on model size affecting efficacy.", "conclusion": "Weak LLMs offer a scalable and sustainable approach to model alignment, mitigating resource constraints associated with traditional methods.", "key_contributions": ["Demonstrated the effectiveness of weak LLMs for alignment feedback.", "Provided insights into the quality of feedback relative to human feedback.", "Established a framework that reduces the need for extensive human effort and computational resources."], "limitations": "The study focuses solely on weak LLMs and does not explore other potential alignment methods.", "future_work": "Further research could investigate the integration of weak LLMs with other techniques and the long-term implications of using such models for ongoing alignment processes.", "keywords": ["large language models", "model alignment", "feedback generation", "human values", "weak LLMs"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.12059", "pdf": "https://arxiv.org/pdf/2409.12059.pdf", "abs": "https://arxiv.org/abs/2409.12059", "title": "MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning", "authors": ["Ningyuan Xi", "Xiaoyu Wang", "Yetao Wu", "Teng Chen", "Qingqing Gu", "Yue Zhao", "Jinxian Qu", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 7 figures", "summary": "Large Language Model can reasonably understand and generate human expressions\nbut may lack of thorough thinking and reasoning mechanisms. Recently there have\nbeen several studies which enhance the thinking ability of language models but\nmost of them are not data-driven or training-based. In this paper, we are\nmotivated by the cognitive mechanism in the natural world, and design a novel\nmodel architecture called TaS which allows it to first consider the thoughts\nand then express the response based upon the query. We design several pipelines\nto annotate or generate the thought contents from prompt-response samples, then\nadd language heads in a middle layer which behaves as the thinking layer. We\ntrain the language model by the thoughts-augmented data and successfully let\nthe thinking layer automatically generate reasonable thoughts and finally\noutput more reasonable responses. Both qualitative examples and quantitative\nresults validate the effectiveness and performance of TaS. Our code is\navailable at https://anonymous.4open.science/r/TadE.", "AI": {"tldr": "This paper presents a novel model architecture called TaS that enhances Large Language Models by integrating a thinking layer to improve reasoning in responses.", "motivation": "The paper addresses the limitations of Large Language Models in reasoning and thorough thinking, proposing a data-driven approach inspired by cognitive mechanisms.", "method": "The authors designed a model architecture called TaS, which incorporates a middle layer that functions as a thinking layer, allowing for the generation of thought contents based on prompt-response samples.", "result": "The studies show that TaS can successfully generate reasonable thoughts and produce improved responses, supported by qualitative and quantitative validation.", "conclusion": "TaS enhances the reasoning capabilities of language models, leading to better quality responses, with the implementation details and code made available for further research.", "key_contributions": ["Introduction of the TaS model architecture", "Integration of a thinking layer into language models", "Empirical validation of enhanced reasoning capabilities"], "limitations": "The model's effectiveness may vary based on the types of queries and the dataset used for training.", "future_work": "Exploration of broader applications of the TaS model and potential improvements in reasoning techniques for language models.", "keywords": ["Large Language Models", "Cognitive Mechanism", "Thinking Layer", "Natural Language Processing", "Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.03727", "pdf": "https://arxiv.org/pdf/2410.03727.pdf", "abs": "https://arxiv.org/abs/2410.03727", "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"", "authors": ["Yifei Ming", "Senthil Purushwalkam", "Shrey Pandit", "Zixuan Ke", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The conference version of this paper is published at ICLR 2025", "summary": "Ensuring faithfulness to context in large language models (LLMs) and\nretrieval-augmented generation (RAG) systems is crucial for reliable deployment\nin real-world applications, as incorrect or unsupported information can erode\nuser trust. Despite advancements on standard benchmarks, faithfulness\nhallucination-where models generate responses misaligned with the provided\ncontext-remains a significant challenge. In this work, we introduce FaithEval,\na novel and comprehensive benchmark tailored to evaluate the faithfulness of\nLLMs in contextual scenarios across three diverse tasks: unanswerable,\ninconsistent, and counterfactual contexts. These tasks simulate real-world\nchallenges where retrieval mechanisms may surface incomplete, contradictory, or\nfabricated information. FaithEval comprises 4.9K high-quality problems in\ntotal, validated through a rigorous four-stage context construction and\nvalidation framework, employing both LLM-based auto-evaluation and human\nvalidation. Our extensive study across a wide range of open-source and\nproprietary models reveals that even state-of-the-art models often struggle to\nremain faithful to the given context, and that larger models do not necessarily\nexhibit improved faithfulness.Project is available at:\nhttps://github.com/SalesforceAIResearch/FaithEval.", "AI": {"tldr": "The paper introduces FaithEval, a benchmark for evaluating faithfulness in LLMs, revealing that even the best models often struggle with context fidelity.", "motivation": "To address the issue of faithfulness in LLMs and RAG systems, which is critical for user trust and effective real-world deployment.", "method": "The paper presents FaithEval, a benchmark comprising three tasks (unanswerable, inconsistent, and counterfactual contexts) with 4.9K problems validated through a four-stage construction and evaluation process.", "result": "The study found that state-of-the-art models frequently fail to maintain faithfulness to context, and larger models do not necessarily lead to better performance in this regard.", "conclusion": "The FaithEval benchmark can help evaluate and improve the faithfulness of LLMs in real-world applications.", "key_contributions": ["Introduction of FaithEval benchmark for evaluating contextual faithfulness in LLMs", "Identification of significant faithfulness challenges in state-of-the-art models", "Development of a rigorous validation framework for benchmarks."], "limitations": "The benchmark may not cover all possible contextual scenarios relevant to real-world applications.", "future_work": "Further refinement of the benchmark and exploration of methods to enhance the faithfulness of LLMs.", "keywords": ["faithfulness", "large language models", "evaluation benchmark", "contextual fidelity", "retrieval-augmented generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.07611", "pdf": "https://arxiv.org/pdf/2411.07611.pdf", "abs": "https://arxiv.org/abs/2411.07611", "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Yida Xu", "Yunya Song", "Xian Yang"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "13 pages. 7 figures", "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability.", "AI": {"tldr": "ClinRaGen enhances small language models by integrating large language model reasoning and domain knowledge for improved disease diagnosis interpretability.", "motivation": "The need to balance predictive accuracy with human-understandable rationales in disease diagnosis.", "method": "ClinRaGen utilizes a sequential rationale distillation framework to enhance small language models' reasoning abilities, alongside a knowledge-augmented attention mechanism for multimodal data integration.", "result": "ClinRaGen demonstrates state-of-the-art performance in disease diagnosis and rationale generation across real-world medical datasets.", "conclusion": "ClinRaGen effectively combines LLM-driven reasoning and knowledge augmentation to improve interpretability in medical settings.", "key_contributions": ["Proposes ClinRaGen for enhanced rationale generation in disease diagnosis.", "Introduces a framework for distilling reasoning from LLMs to SLMs.", "Develops a knowledge-augmented attention mechanism for multimodal representation."], "limitations": "Limited to the current available medical datasets; generalization to other domains not tested.", "future_work": "Exploration of broader applications of ClinRaGen in varied medical contexts and other domains.", "keywords": ["disease diagnosis", "rationale generation", "multimodal reasoning", "small language models", "large language models"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2412.11704", "pdf": "https://arxiv.org/pdf/2412.11704.pdf", "abs": "https://arxiv.org/abs/2412.11704", "title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data", "authors": ["Atsuki Yamaguchi", "Terufumi Morishita", "Aline Villavicencio", "Nikolaos Aletras"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV.", "AI": {"tldr": "ElChat is a method for adapting chat LLMs directly on target unlabeled data, enhancing language and instruction-following abilities without a base model.", "motivation": "To address the challenges in adapting chat LLMs under low-resource language settings where direct access to target chat data is often unavailable or too costly.", "method": "ElChat directly adapts a chat model on target unlabeled data by injecting information from an existing source chat model, eliminating the need for a base model.", "result": "ElChat shows improved target language performance and safety, as well as superior English, chat, and instruction-following abilities compared to previous methods that relied on a chat vector from base models.", "conclusion": "ElChat provides a more effective alternative for adapting chat models in low-resource scenarios without necessitating a base model.", "key_contributions": ["Introduces a novel method for direct adaptation of chat models on unlabeled data without a base model.", "Demonstrates enhanced performance in target language adaptation and safety measures.", "Shows significant improvements in English and instruction-following capabilities."], "limitations": "", "future_work": "Exploration of further optimizations for ElChat and its applicability to additional low-resource languages.", "keywords": ["chat models", "language adaptation", "large language models", "machine translation", "instruction following"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2502.01220", "pdf": "https://arxiv.org/pdf/2502.01220.pdf", "abs": "https://arxiv.org/abs/2502.01220", "title": "Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations", "authors": ["Hichem Ammar Khodja", "Frédéric Béchet", "Quentin Brabant", "Alexis Nasr", "Gwénolé Lecorvé"], "categories": ["cs.CL", "cs.LG"], "comment": "preprint v3", "summary": "This paper explores the robustness of language models (LMs) to variations in\nthe temporal context within factual knowledge. It examines whether LMs can\ncorrectly associate a temporal context with a past fact valid over a defined\nperiod, by asking them to differentiate correct from incorrect contexts. The\naccuracy of LMs is analyzed along two dimensions: the distance of the incorrect\ncontext from the validity period and the granularity of the context. To this\nend, a dataset called TimeStress is introduced, enabling the evaluation of 18\ndiverse LMs. Results reveal that the best LM achieves perfect accuracy for only\n6% of the studied facts, with critical errors that humans would not make. This\nwork highlights the limitations of current LMs in temporal representation. We\nprovide all data and code for further research.", "AI": {"tldr": "This paper evaluates the robustness of language models in correctly associating temporal contexts with factual knowledge using the newly introduced TimeStress dataset.", "motivation": "To explore how language models associate temporal contexts with past facts and to identify their limitations in handling temporal representations.", "method": "The study introduces a dataset called TimeStress to analyze 18 diverse language models based on their ability to differentiate correct from incorrect temporal contexts related to factual knowledge.", "result": "The best-performing language model achieves perfect accuracy on only 6% of the studied facts, demonstrating significant limitations compared to human performance.", "conclusion": "The findings indicate critical errors in current language models' handling of temporal information, underscoring the need for further research in this area.", "key_contributions": ["Introduction of the TimeStress dataset for analyzing temporal context in language models.", "Evaluation of 18 diverse language models on their accuracy with temporal knowledge.", "Highlighting significant limitations in language models' abilities compared to human understanding."], "limitations": "The study primarily focuses on language models' performance in temporal context associations without offering insights into improvements for these models.", "future_work": "Further research directions include improving the temporal representation capabilities of language models and expanding the dataset for broader evaluation.", "keywords": ["temporal context", "language models", "factual knowledge", "TimeStress dataset", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.12486", "pdf": "https://arxiv.org/pdf/2502.12486.pdf", "abs": "https://arxiv.org/abs/2502.12486", "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning", "authors": ["Xiaoqian Liu", "Ke Wang", "Yongbin Li", "Yuchuan Wu", "Wentao Ma", "Aobo Kong", "Fei Huang", "Jianbin Jiao", "Junge Zhang"], "categories": ["cs.CL"], "comment": "22 pages, 4 figures", "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.", "AI": {"tldr": "Proposes explicit policy optimization (EPO) for improving strategic reasoning in Large Language Models, focusing on dynamic environments and long-term goal alignment.", "motivation": "To enhance the strategic reasoning capabilities of LLMs in complex real-world scenarios, addressing challenges in adaptability and scalability.", "method": "The model is trained using multi-turn reinforcement learning with process rewards and iterative self-play, avoiding supervised fine-tuning.", "result": "EPO achieves state-of-the-art performance in social dialogue and web navigation tasks, showing effective long-term goal alignment.", "conclusion": "EPO demonstrates improved strategic reasoning and collaborative mechanisms, indicating its potential for real-world applications.", "key_contributions": ["Introduction of explicit policy optimization for LLMs", "Improvement in strategic reasoning through multi-turn reinforcement learning", "State-of-the-art performance in real-world tasks without supervised fine-tuning"], "limitations": "", "future_work": "Exploration of further applications in various dynamic environments and enhancements in adaptability.", "keywords": ["Large Language Models", "strategic reasoning", "reinforcement learning", "policy optimization", "goal-directed behavior"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.15654", "pdf": "https://arxiv.org/pdf/2502.15654.pdf", "abs": "https://arxiv.org/abs/2502.15654", "title": "Machine-generated text detection prevents language model collapse", "authors": ["George Drayson", "Emine Yilmaz", "Vasileios Lampos"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse.", "AI": {"tldr": "The paper investigates the impact of decoding strategies on model collapse in Large Language Models (LLMs) and proposes a method to alleviate this issue through a machine-generated text detector.", "motivation": "The increasing prevalence of LLMs and their outputs risks diluting human-authored content, potentially leading to model collapse where errors are reinforced and performance declines.", "method": "The study analyzes the impact of different decoding strategies on model generation, measuring text characteristics, similarity to human references, and model performance. It also proposes a machine-generated text detector with importance sampling to mitigate model collapse.", "result": "The approach proves effective in preventing model collapse and enhances performance when adequate human-generated samples are available, validated through experiments with GPT-2 and SmolLM2.", "conclusion": "Implementing an importance sampling strategy can significantly improve the reliability of LLM outputs and counteract the challenges posed by model collapse.", "key_contributions": ["Identification of the impact of decoding strategies on model collapse in LLMs.", "Development of a machine-generated text detector to address model collapse.", "Validation of approach on GPT-2 and SmolLM2, showing improved performance."], "limitations": "Potential limitations in generalizability to all types of LLMs and scenarios.", "future_work": "Future research could explore broader applications of machine-generated text detection and refinement of decoding strategies across different models.", "keywords": ["Large Language Models", "model collapse", "decoding strategies", "machine-generated text detection", "text generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.01840", "pdf": "https://arxiv.org/pdf/2504.01840.pdf", "abs": "https://arxiv.org/abs/2504.01840", "title": "LRAGE: Legal Retrieval Augmented Generation Evaluation Tool", "authors": ["Minhu Park", "Hongseok Oh", "Eunkyung Choi", "Wonseok Hwang"], "categories": ["cs.CL"], "comment": "12 pages", "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE.", "AI": {"tldr": "LRAGE is an open-source tool designed for the holistic evaluation of RAG systems in the legal domain, examining the impacts of various components on accuracy.", "motivation": "There is a growing need to evaluate RAG systems in the legal domain due to the importance of previous judicial decisions in legal contexts.", "method": "LRAGE provides both GUI and CLI interfaces for conducting experiments, facilitating the evaluation of five components of RAG systems: retrieval corpora, retrieval algorithms, rerankers, LLM backbones, and evaluation metrics.", "result": "Validation of LRAGE was done using multilingual legal benches (KBL, LegalBench, LawBench) to show how accuracy varies with changes in the five evaluated components.", "conclusion": "LRAGE significantly aids in assessing the effectiveness of various components in retrieval-augmented generation systems for legal applications.", "key_contributions": ["Introduction of LRAGE, a comprehensive evaluation tool for RAG systems in legal contexts.", "Validation of the tool with multilingual legal datasets to demonstrate its effectiveness.", "Facilitation of seamless experimentation through GUI and CLI interfaces."], "limitations": "", "future_work": "Future improvements may include expanding the tool's capabilities to other domains beyond law and enhancing the user experience.", "keywords": ["Retrieval-Augmented Generation", "Legal Domain", "Evaluation Tool", "Large Language Models", "Open-source"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2504.02810", "pdf": "https://arxiv.org/pdf/2504.02810.pdf", "abs": "https://arxiv.org/abs/2504.02810", "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.", "AI": {"tldr": "KUMO is a generative evaluation framework for assessing reasoning in LLMs, showing that many LLMs outperform university students on reasoning tasks.", "motivation": "To determine if LLMs genuinely reason or simply recall information from training data, highlighting the need for reliable assessment benchmarks.", "method": "KUMO combines LLMs with symbolic engines to create multi-turn reasoning tasks that adapt in difficulty, producing 5,000 tasks across 100 domains.", "result": "Evaluation of 23 LLMs indicated many outperformed university students on easier tasks, with reasoning-scaled LLMs achieving university-level performance on complex challenges.", "conclusion": "KUMO serves as a valuable tool for assessing the genuine reasoning capabilities of LLMs, showing a strong correlation with real-world reasoning benchmarks.", "key_contributions": ["Introduction of KUMO as a tool for LLM reasoning assessment.", "Generation of diverse reasoning tasks that adapt in difficulty.", "Benchmarking LLMs against university-level performance."], "limitations": "The framework may still encounter biases in task generation and assessment, depending on the LLMs used.", "future_work": "Explore further refinements of KUMO to reduce bias and expand the types of reasoning tasks generated.", "keywords": ["Large Language Models", "Reasoning Assessment", "Generative Evaluation Framework", "KUMO", "Symbolic Engines"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.08040", "pdf": "https://arxiv.org/pdf/2504.08040.pdf", "abs": "https://arxiv.org/abs/2504.08040", "title": "Can Reasoning LLMs Enhance Clinical Document Classification?", "authors": ["Akram Mustafa", "Usman Naseem", "Mostafa Rahimi Azghadi"], "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Clinical document classification is essential for converting unstructured\nmedical texts into standardised ICD-10 diagnoses, yet it faces challenges due\nto complex medical language, privacy constraints, and limited annotated\ndatasets. Large Language Models (LLMs) offer promising improvements in accuracy\nand efficiency for this task. This study evaluates the performance and\nconsistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3\nMini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o\nMini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge\nsummaries using the MIMIC-IV dataset. Using cTAKES to structure clinical\nnarratives, models were assessed across three experimental runs, with majority\nvoting determining final predictions. Results showed that reasoning models\noutperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs\n60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and\nF1 score (76%). However, non-reasoning models demonstrated greater stability\n(91% vs 84% consistency). Performance varied across ICD-10 codes, with\nreasoning models excelling in complex cases but struggling with abstract\ncategories. Findings indicate a trade-off between accuracy and consistency,\nsuggesting that a hybrid approach could optimise clinical coding. Future\nresearch should explore multi-label classification, domain-specific\nfine-tuning, and ensemble methods to enhance model reliability in real-world\napplications.", "AI": {"tldr": "This study evaluates the performance of eight Large Language Models (LLMs) in classifying clinical discharge summaries into ICD-10 diagnoses, revealing a trade-off between accuracy and consistency.", "motivation": "The need for accurate classification of clinical documents into standardized ICD-10 diagnoses due to challenges posed by complex medical language and limited datasets.", "method": "The performance of four reasoning and four non-reasoning LLMs was assessed using the MIMIC-IV dataset, employing cTAKES to structure clinical narratives and majority voting for predictions across three experimental runs.", "result": "Reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with the best performance by Gemini 2.0 Flash Thinking, yet non-reasoning models showed higher consistency (91% vs 84%).", "conclusion": "There is a critical trade-off between accuracy and consistency in model performance suggesting a hybrid approach might be more effective for clinical coding tasks.", "key_contributions": ["Evaluation of performance of eight LLMs for clinical document classification", "Demonstration of a trade-off between accuracy and consistency", "Recommendations for future research directions including multi-label classification and ensemble methods"], "limitations": "The performance varied significantly across different ICD-10 codes, particularly with reasoning models struggling with abstract categories.", "future_work": "Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in clinical applications.", "keywords": ["Clinical Document Classification", "Large Language Models", "ICD-10", "Medical Text", "Health Informatics"], "importance_score": 9, "read_time_minutes": 27}}
{"id": "2504.12285", "pdf": "https://arxiv.org/pdf/2504.12285.pdf", "abs": "https://arxiv.org/abs/2504.12285", "title": "BitNet b1.58 2B4T Technical Report", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.", "AI": {"tldr": "Introduction of BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at 2 billion parameters, achieving competitive performance with lower resource requirements.", "motivation": "To develop a computationally efficient LLM that minimizes memory usage and energy consumption while maintaining performance comparable to leading models.", "method": "BitNet b1.58 2B4T was trained on a 4 trillion token corpus and evaluated on language understanding, reasoning, coding, and conversational benchmarks.", "result": "BitNet b1.58 2B4T achieves performance similar to full-precision LLMs with significant reductions in memory, energy, and decoding latency.", "conclusion": "BitNet b1.58 2B4T offers an efficient alternative for LLM deployment, facilitating easier research and application through open-source distribution.", "key_contributions": ["First open-source, native 1-bit LLM at 2 billion parameters.", "Significant improvements in computational efficiency and reduced resource requirements.", "Model weights and inference implementations made publicly available."], "limitations": "Details on the specific methodologies and comprehensive evaluations of the model's limitations are still in progress revealing the work's ongoing status.", "future_work": "Encouragement for continued research on optimizing 1-bit model architectures and exploring various applications in real-world scenarios.", "keywords": ["Large Language Model", "1-bit model", "open-source", "computational efficiency", "Hugging Face"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657.pdf", "abs": "https://arxiv.org/abs/2504.14657", "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.", "AI": {"tldr": "This paper evaluates the capability of commercial LLMs in generating synthetic electronic health records (EHRs), highlighting their strengths and limitations.", "motivation": "To explore the potential of synthetic EHRs in healthcare and the role of LLMs in generating such data without compromising real individuals' privacy.", "method": "The paper assesses the performance of various commercial LLMs in generating synthetic health records, with a focus on the generalizability across different hospitals.", "result": "LLMs can generate reliable synthetic health records for smaller subsets, but they struggle with preserving realistic distributions and correlations in higher-dimensional data.", "conclusion": "There is a need for improved strategies to ensure that synthetic health records can generalize better across diverse healthcare settings.", "key_contributions": ["Evaluation of LLMs for synthetic EHR generation", "Identification of strengths and weaknesses in data generation", "Insights into the challenges of dimensionality in healthcare data"], "limitations": "The study is limited by its focus on commercial LLMs and may not account for all factors affecting synthetic data quality.", "future_work": "Further research is needed to develop methods that enhance the generalization capabilities of synthetic health records across various hospital environments.", "keywords": ["Synthetic EHRs", "Large Language Models", "Healthcare", "Data Generation", "Privacy Preservation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.15843", "pdf": "https://arxiv.org/pdf/2504.15843.pdf", "abs": "https://arxiv.org/abs/2504.15843", "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "authors": ["Junshu Pan", "Wei Shen", "Shulin Huang", "Qiji Zhou", "Yue Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.", "AI": {"tldr": "Pre-DPO enhances Direct Preference Optimization (DPO) by using a guiding reference model to improve performance in reinforcement learning from human feedback for LLMs.", "motivation": "The need to optimize human preferences in reinforcement learning without a reward model while overcoming inefficiencies in the initialization of policy and reference models.", "method": "Proposes a new training paradigm called Pre-DPO that utilizes a reference model to improve weight assignment during training, guiding the model to focus on more suitable samples.", "result": "Pre-DPO consistently improves performance on benchmarks like AlpacaEval 2.0 and Arena-Hard v0.1 compared to traditional DPO and Simple Preference Optimization without requiring extra models or data.", "conclusion": "Pre-DPO provides a robust solution for preference optimization in LLMs by enhancing training performance and data utilization.", "key_contributions": ["Introduction of Pre-DPO, a novel training paradigm for LLMs.", "Demonstrating the effectiveness of using a guiding reference model in optimizing training data weights.", "Showing consistent improvement in benchmark performance over existing methods."], "limitations": "Potential dependence on the quality and training of the reference model, which may affect performance.", "future_work": "Investigate further modifications to the Pre-DPO framework and explore its applicability across different model architectures and data types.", "keywords": ["Direct Preference Optimization", "reinforcement learning", "human feedback", "large language models", "training paradigm"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005.pdf", "abs": "https://arxiv.org/abs/2504.16005", "title": "CAPO: Cost-Aware Prompt Optimization", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.", "AI": {"tldr": "CAPO is an algorithm for cost-aware prompt optimization in large language models, improving efficiency by integrating AutoML techniques and outperforming existing methods.", "motivation": "The performance of large language models is sensitive to prompt formulation, and current automated methods for prompt optimization are expensive in terms of LLM calls and input tokens.", "method": "CAPO combines evolutionary techniques with LLMs, utilizing racing to reduce evaluations and multi-objective optimization to optimize prompt performance and length simultaneously.", "result": "CAPO outperformed state-of-the-art discrete prompt optimization methods in 11 out of 15 cases, with performance improvements of up to 21% while being more cost-efficient.", "conclusion": "CAPO enhances the accessibility and efficiency of prompt optimization for large language models by balancing performance with cost.", "key_contributions": ["Introduction of CAPO, an efficient prompt optimization algorithm.", "Incorporation of AutoML techniques to reduce costs without sacrificing performance.", "Demonstrated improvements across diverse datasets and better overall robustness compared to existing methods."], "limitations": "The paper does not deeply investigate the impact of various initial prompt forms on final performance.", "future_work": "Future work could explore further refinements in prompt optimization techniques and evaluate CAPO across additional tasks and scenarios.", "keywords": ["Large Language Models", "Prompt Optimization", "AutoML Techniques", "Evolutionary Approach", "Cost-Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
