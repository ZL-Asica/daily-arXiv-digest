{"id": "2505.11666", "pdf": "https://arxiv.org/pdf/2505.11666.pdf", "abs": "https://arxiv.org/abs/2505.11666", "title": "DesignFromX: Empowering Consumer-Driven Design Space Exploration through Feature Composition of Referenced Products", "authors": ["Runlin Duan", "Chenfei Zhu", "Yuzhao Chen", "Yichen Hu", "Jingyu Shi", "Karthik Ramani"], "categories": ["cs.HC"], "comment": null, "summary": "Industrial products are designed to satisfy the needs of consumers. The rise\nof generative artificial intelligence (GenAI) enables consumers to easily\nmodify a product by prompting a generative model, opening up opportunities to\nincorporate consumers in exploring the product design space. However, consumers\noften struggle to articulate their preferred product features due to their\nunfamiliarity with terminology and their limited understanding of the structure\nof product features. We present DesignFromX, a system that empowers\nconsumer-driven design space exploration by helping consumers to design a\nproduct based on their preferences. Leveraging an effective GenAI-based\nframework, the system allows users to easily identify design features from\nproduct images and compose those features to generate conceptual images and 3D\nmodels of a new product. A user study with 24 participants demonstrates that\nDesignFromX lowers the barriers and frustration for consumer-driven design\nspace explorations by enhancing both engagement and enjoyment for the\nparticipants.", "AI": {"tldr": "DesignFromX enables consumer-driven product design using generative AI to help users select and customize design features efficiently.", "motivation": "To incorporate consumers into exploring product design while addressing their struggles with articulating preferred features.", "method": "Development of the DesignFromX system which utilizes generative AI to identify and compose design features from product images into new conceptual images and 3D models.", "result": "A user study with 24 participants shows that DesignFromX reduces barriers to consumer-driven design, enhancing engagement and enjoyment.", "conclusion": "DesignFromX successfully empowers consumers in the product design process, making it more enjoyable and accessible.", "key_contributions": ["Creation of DesignFromX system for consumer-driven design", "Use of generative AI to facilitate feature identification and composition", "Backed by user study demonstrating effectiveness"], "limitations": "", "keywords": ["Generative AI", "Consumer-Driven Design", "Product Design", "User Experience", "Engagement"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.11684", "pdf": "https://arxiv.org/pdf/2505.11684.pdf", "abs": "https://arxiv.org/abs/2505.11684", "title": "Designing for Constructive Civic Communication: A Framework for Human-AI Collaboration in Community Engagement Processes", "authors": ["Cassandra Overney"], "categories": ["cs.HC"], "comment": "10 pages, 2 figures, report", "summary": "Community engagement processes form a critical foundation of democratic\ngovernance, yet frequently struggle with resource constraints, sensemaking\nchallenges, and barriers to inclusive participation. These processes rely on\nconstructive communication between public leaders and community organizations\ncharacterized by understanding, trust, respect, legitimacy, and agency. As\nartificial intelligence (AI) technologies become increasingly integrated into\ncivic contexts, they offer promising capabilities to streamline\nresource-intensive workflows, reveal new insights in community feedback,\ntranslate complex information into accessible formats, and facilitate\nreflection across social divides. However, these same systems risk undermining\ndemocratic processes through accuracy issues, transparency gaps, bias\namplification, and threats to human agency. In this paper, we examine how\nhuman-AI collaboration might address these risks and transform civic\ncommunication dynamics by identifying key communication pathways and proposing\ndesign considerations that maintain a high level of control over\ndecision-making for both public leaders and communities while leveraging\ncomputer automation. By thoughtfully integrating AI to amplify human connection\nand understanding while safeguarding agency, community engagement processes can\nutilize AI to promote more constructive communication in democratic governance.", "AI": {"tldr": "The paper explores how human-AI collaboration can enhance community engagement in democratic governance while addressing potential risks associated with AI technologies.", "motivation": "To improve community engagement processes faced with resource constraints and inclusivity challenges in democratic governance.", "method": "The paper analyzes human-AI collaboration strategies, identifies key communication pathways, and proposes design considerations for AI integration in civic contexts.", "result": "The findings suggest that thoughtful integration of AI can enhance constructive communication in community engagement while reducing risks such as bias and transparency issues.", "conclusion": "By ensuring high levels of control over decision-making for leaders and communities, AI can significantly improve civic communication dynamics.", "key_contributions": ["Identification of key communication pathways for human-AI collaboration in civic contexts.", "Design considerations for integrating AI that safeguard community agency.", "Recommendations for using AI to facilitate meaningful community engagement."], "limitations": "The study may not cover all potential biases inherent in AI systems or the varying impacts across different community contexts.", "keywords": ["community engagement", "democratic governance", "human-AI collaboration", "AI in civic contexts", "constructive communication"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.11715", "pdf": "https://arxiv.org/pdf/2505.11715.pdf", "abs": "https://arxiv.org/abs/2505.11715", "title": "ConflictLens: LLM-Based Conflict Resolution Training in Romantic Relationship", "authors": ["Jiwon Chun", "Gefei Zhang", "Meng Xia"], "categories": ["cs.HC"], "comment": null, "summary": "Romantic conflicts are often rooted in deep psychological factors such as\ncoping styles, emotional responses, and communication habits. Existing systems\ntend to address surface-level behaviors or isolated events, offering limited\nsupport for understanding the underlying dynamics. We present ConflictLens, an\ninteractive system that leverages psychological theory and large language\nmodels (LLMs) to help individuals analyze and reflect on the deeper mechanisms\nbehind their conflicts. The system provides multi-level strategy\nrecommendations and guided dialogue exercises, including annotation, rewriting,\nand continuation tasks. A case study demonstrates how ConflictLens supports\nemotional insight, improves relational understanding, and fosters more\nconstructive communication. This work offers a novel approach to supporting\nself-awareness and growth in romantic relationships.", "AI": {"tldr": "ConflictLens is an interactive system designed to help individuals analyze and reflect on psychological factors influencing romantic conflicts using LLMs.", "motivation": "To address the superficial understanding of romantic conflicts and provide deeper insights into the underlying psychological mechanisms.", "method": "Developed an interactive system that incorporates psychological theory and utilizes LLMs for analysis and reflection on romantic conflicts.", "result": "ConflictLens was shown to enhance emotional insight, improve relational understanding, and promote constructive communication among users.", "conclusion": "The system offers a novel method for fostering self-awareness and growth in romantic relationships.", "key_contributions": ["Integration of psychological theory with LLMs for conflict analysis", "Multi-level strategy recommendations", "Guided dialogue exercises for improved communication"], "limitations": "", "keywords": ["romantic conflicts", "psychological theory", "large language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.11784", "pdf": "https://arxiv.org/pdf/2505.11784.pdf", "abs": "https://arxiv.org/abs/2505.11784", "title": "Utilizing Provenance as an Attribute for Visual Data Analysis: A Design Probe with ProvenanceLens", "authors": ["Arpit Narechania", "Shunan Guo", "Eunyee Koh", "Alex Endert", "Jane Hoffswell"], "categories": ["cs.HC"], "comment": "14 pages, 6 figures, 1 table, accepted in IEEE TVCG 2025", "summary": "Analytic provenance can be visually encoded to help users track their ongoing\nanalysis trajectories, recall past interactions, and inform new analytic\ndirections. Despite its significance, provenance is often hardwired into\nanalytics systems, affording limited user control and opportunities for\nself-reflection. We thus propose modeling provenance as an attribute that is\navailable to users during analysis. We demonstrate this concept by modeling two\nprovenance attributes that track the recency and frequency of user interactions\nwith data. We integrate these attributes into a visual data analysis system\nprototype, ProvenanceLens, wherein users can visualize their interaction\nrecency and frequency by mapping them to encoding channels (e.g., color, size)\nor applying data transformations (e.g., filter, sort). Using ProvenanceLens as\na design probe, we conduct an exploratory study with sixteen users to\ninvestigate how these provenance-tracking affordances are utilized for both\ndecision-making and self-reflection. We find that users can accurately and\nconfidently answer questions about their analysis, and we show that mismatches\nbetween the user's mental model and the provenance encodings can be surprising,\nthereby prompting useful self-reflection. We also report on the user strategies\nsurrounding these affordances, and reflect on their intuitiveness and\neffectiveness in representing provenance.", "AI": {"tldr": "The paper presents ProvenanceLens, a visual data analysis system that allows users to control and reflect on their interactions with data through the visualization of analytic provenance attributes.", "motivation": "To enhance user control and self-reflection in data analysis by modeling provenance as an attribute available during analysis.", "method": "The authors modeled two provenance attributes (recency and frequency of user interactions), integrated them into a prototype system, ProvenanceLens, and conducted an exploratory study with 16 users.", "result": "Users were able to accurately and confidently answer questions about their analysis, while mismatches between their mental models and provenance encodings led to valuable self-reflection.", "conclusion": "Provenance modeling can significantly aid in decision-making and self-reflection during data analysis by making interaction histories explicit and usable.", "key_contributions": ["Introduction of ProvenanceLens for visualizing analytic provenance.", "Exploratory study demonstrating user capabilities in self-reflection and decision-making via provenance attributes.", "Insights into user strategies for utilizing provenance information."], "limitations": "The study was limited to a small sample size of 16 users, which may affect the generalizability of the findings.", "keywords": ["analytic provenance", "data visualization", "user interaction", "self-reflection", "decision-making"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533.pdf", "abs": "https://arxiv.org/abs/2505.11533", "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available.", "AI": {"tldr": "The paper introduces SynPT, a data synthesis method utilizing LLMs to improve the proactive mining of implicit user intentions in the tourism sector by generating a specialized training dataset.", "motivation": "The tourism domain faces challenges in adequately extracting user intentions from ambiguous inquiries due to a lack of high-quality training datasets for proactive questioning.", "method": "SynPT constructs LLM-driven user agent and assistant agent systems to simulate dialogues based on seed data from Chinese tourism websites, generating a training dataset called SynPT-Dialog for fine-tuning an LLM.", "result": "Experimental evaluations reveal that SynPT outperforms existing methods in mining implicit user intentions from tourist inquiries, as validated from both human and LLM perspectives.", "conclusion": "The proposed method enhances the ability of LLMs to understand and guide users in the tourism domain, with applicability shown for English-language scenarios as well.", "key_contributions": ["Introduction of SynPT for proactive mining of implicit user intentions in tourism", "Generation of a novel training dataset, SynPT-Dialog", "Demonstration of the method's effectiveness through experiments and case studies"], "limitations": "", "keywords": ["Large Language Models", "Tourism", "Implicit User Intentions", "Data Synthesis", "Training Dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.11888", "pdf": "https://arxiv.org/pdf/2505.11888.pdf", "abs": "https://arxiv.org/abs/2505.11888", "title": "AR Secretary Agent: Real-time Memory Augmentation via LLM-powered Augmented Reality Glasses", "authors": ["RaphaÃ«l A. El Haddad", "Zeyu Wang", "Yeonsu Shin", "Ranyi Liu", "Yuntao Wang", "Chun Yu"], "categories": ["cs.HC"], "comment": null, "summary": "Interacting with a significant number of individuals on a daily basis is\ncommonplace for many professionals, which can lead to challenges in recalling\nspecific details: Who is this person? What did we talk about last time? The\nadvant of augmented reality (AR) glasses, equipped with visual and auditory\ndata capture capabilities, presents a solution. In our work, we implemented an\nAR Secretary Agent with advanced Large Language Models (LLMs) and Computer\nVision technologies. This system could discreetly provide real-time information\nto the wearer, identifying who they are conversing with and summarizing\nprevious discussions. To verify AR Secretary, we conducted a user study with 13\nparticipants and showed that our technique can efficiently help users to\nmemorize events by up to 20\\% memory enhancement on our study.", "AI": {"tldr": "The paper presents an AR Secretary Agent that uses LLMs and Computer Vision to provide real-time information about individuals during conversations, showing a 20% increase in memory retention based on user studies.", "motivation": "Professionals often struggle with recalling details about individuals they interact with daily, leading to missed connections and inefficiencies.", "method": "Implementation of an AR system that captures visual and auditory data, leveraging LLMs and Computer Vision for real-time identification and conversation summarization.", "result": "User studies with 13 participants demonstrated a 20% improvement in memory retention regarding past interactions.", "conclusion": "The AR Secretary can significantly aid memory recall during social interactions, enhancing professionals' ability to connect with others.", "key_contributions": ["Introduction of an AR Secretary Agent utilizing LLMs and AR technologies", "Implementation of a user study validating memory enhancement", "Demonstration of real-time contextual assistance during conversations"], "limitations": "Limited sample size of participants in user studies; potential privacy concerns with constant data capture.", "keywords": ["Augmented Reality", "Large Language Models", "Memory Enhancement", "Human-Computer Interaction", "Computer Vision"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550.pdf", "abs": "https://arxiv.org/abs/2505.11550", "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627.", "AI": {"tldr": "This paper addresses the detection of AI-generated text and attribution to language models, proposing neural architectures for these tasks.", "motivation": "The proliferation of Large Language Models (LLMs) poses challenges such as misuse for generating misleading content, necessitating effective detection methods.", "method": "Two neural architectures were developed for distinguishing AI-generated text from human-written text and for attributing text to its originating model, evaluated in a shared task framework at AAAI 2025.", "result": "The optimized model for distinguishing text achieved an F1 score of 0.994, ranking fifth, while the simpler model for attribution obtained an F1 score of 0.627, also placing fifth.", "conclusion": "The proposed models demonstrate high effectiveness in detecting AI-generated texts and identifying their sources, contributing to responsible AI deployment.", "key_contributions": ["Development of two neural architectures for text detection and attribution", "Performance benchmarking in a competitive setting (AAAI 2025)", "Insight into the challenges of distinguishing between human and AI-generated content"], "limitations": "", "keywords": ["Large Language Models", "AI-generated text detection", "neural architectures"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.11996", "pdf": "https://arxiv.org/pdf/2505.11996.pdf", "abs": "https://arxiv.org/abs/2505.11996", "title": "To Recommend or Not to Recommend: Designing and Evaluating AI-Enabled Decision Support for Time-Critical Medical Events", "authors": ["Angela Mastrianni", "Mary Suhyun Kim", "Travis M. Sullivan", "Genevieve Jayne Sippel", "Randall S. Burd", "Krzysztof Z. Gajos", "Aleksandra Sarcevic"], "categories": ["cs.HC"], "comment": null, "summary": "AI-enabled decision-support systems aim to help medical providers rapidly\nmake decisions with limited information during medical emergencies. A critical\nchallenge in developing these systems is supporting providers in interpreting\nthe system output to make optimal treatment decisions. In this study, we\ndesigned and evaluated an AI-enabled decision-support system to aid providers\nin treating patients with traumatic injuries. We first conducted user research\nwith physicians to identify and design information types and AI outputs for a\ndecision-support display. We then conducted an online experiment with 35\nmedical providers from six health systems to evaluate two human-AI interaction\nstrategies: (1) AI information synthesis and (2) AI information and\nrecommendations. We found that providers were more likely to make correct\ndecisions when AI information and recommendations were provided compared to\nreceiving no AI support. We also identified two socio-technical barriers to\nproviding AI recommendations during time-critical medical events: (1) an\naccuracy-time trade-off in providing recommendations and (2) polarizing\nperceptions of recommendations between providers. We discuss three implications\nfor developing AI-enabled decision support used in time-critical events,\ncontributing to the limited research on human-AI interaction in this context.", "AI": {"tldr": "This study examines an AI-enabled decision-support system for medical providers treating traumatic injuries, evaluating human-AI interaction strategies and identifying barriers to effective AI recommendations.", "motivation": "To improve decision-making in medical emergencies where information is limited, helping providers interpret AI outputs for optimal treatment.", "method": "User research with physicians followed by an online experiment with 35 medical providers evaluating two interaction strategies: AI information synthesis and AI recommendations.", "result": "Providers made better decisions with AI recommendations compared to no AI support and two socio-technical barriers were identified.", "conclusion": "The study highlights implications for developing AI-enabled decision support in time-sensitive medical situations, contributing to HCI research in healthcare.", "key_contributions": ["Evaluation of AI interaction strategies in emergency contexts", "Identification of barriers to AI recommendations in medical settings", "Implications for future AI system development in healthcare"], "limitations": "Study limited to specific types of traumatic injuries and small sample size.", "keywords": ["AI decision support", "human-AI interaction", "medical emergencies", "decision-making", "healthcare"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556.pdf", "abs": "https://arxiv.org/abs/2505.11556", "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction.", "AI": {"tldr": "This paper introduces the Hidden Profile paradigm as a benchmark to evaluate collective reasoning failures in multi-agent systems built on LLMs.", "motivation": "To systematically evaluate the collective reasoning failures in multi-agent systems built on LLMs and to create a theory-grounded benchmark for this purpose.", "method": "The Hidden Profile paradigm from social psychology is formalized as a diagnostic testbed for multi-agent decision-making, with experiments conducted using various LLMs, including GPT-4.1.", "result": "Experiments reveal that multi-agent systems do not match the accuracy of single agents with complete information, showing performance similar to human groups but with notable behavioral differences.", "conclusion": "The study presents a reproducible framework for evaluating multi-agent LLM systems and highlights the trade-off between cooperation and contradiction in group dynamics.", "key_contributions": ["Introduction of a new benchmark for evaluating multi-agent LLM systems", "Experiments reveal critical insights into collective reasoning failures", "Demonstration of the utility of the Hidden Profile paradigm in diagnosing agent interactions"], "limitations": "", "keywords": ["multi-agent systems", "large language models", "collective reasoning", "Hidden Profile paradigm", "human-AI interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12064", "pdf": "https://arxiv.org/pdf/2505.12064.pdf", "abs": "https://arxiv.org/abs/2505.12064", "title": "From Data to Actionable Understanding: A Learner-Centered Framework for Dynamic Learning Analytics", "authors": ["Madjid Sadallah"], "categories": ["cs.HC"], "comment": null, "summary": "Learning Analytics Dashboards (LADs) often fall short of their potential to\nempower learners, frequently prioritizing data visualization over the cognitive\nprocesses crucial for translating data into actionable learning strategies.\nThis represents a significant gap in the field: while much research has focused\non data collection and presentation, there is a lack of comprehensive models\nfor how LADs can actively support learners' sensemaking and self-regulation.\nThis paper introduces the Adaptive Understanding Framework (AUF), a novel\nconceptual model for learner-centered LAD design. The AUF seeks to address this\nlimitation by integrating a multi-dimensional model of situational awareness,\ndynamic sensemaking strategies, adaptive mechanisms, and metacognitive support.\nThis transforms LADs into dynamic learning partners that actively scaffold\nlearners' sensemaking. Unlike existing frameworks that tend to treat these\naspects in isolation, the AUF emphasizes their dynamic and intertwined\nrelationships, creating a personalized and adaptive learning ecosystem that\nresponds to individual needs and evolving understanding. The paper details the\nAUF's core principles, key components, and suggests a research agenda for\nfuture empirical validation. By fostering a deeper, more actionable\nunderstanding of learning data, AUF-inspired LADs have the potential to promote\nmore effective, equitable, and engaging learning experiences.", "AI": {"tldr": "This paper introduces the Adaptive Understanding Framework (AUF) for designing Learning Analytics Dashboards (LADs) that support learners' sensemaking and self-regulation.", "motivation": "There is a significant gap in enabling Learning Analytics Dashboards (LADs) to empower learners effectively, as existing approaches prioritize data visualization over cognitive processes essential for actionable learning.", "method": "The paper presents the Adaptive Understanding Framework (AUF), integrating situational awareness, dynamic sensemaking strategies, adaptive mechanisms, and metacognitive support to transform LADs into dynamic learning partners.", "result": "The AUF provides a personalized learning ecosystem that responds to individual needs, fostering actionable understanding of learning data, enhancing learners' sensemaking and self-regulation.", "conclusion": "AUF-inspired LADs can lead to more effective, equitable, and engaging learning experiences by addressing the limitations of current LAD design.", "key_contributions": ["Introduction of the Adaptive Understanding Framework (AUF) for LAD design", "Integration of a multi-dimensional model of awareness and sensemaking strategies", "Proposing a research agenda for empirical validation of the framework"], "limitations": "", "keywords": ["Learning Analytics Dashboards", "Adaptive Understanding Framework", "sensemaking", "self-regulation", "personalized learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604.pdf", "abs": "https://arxiv.org/abs/2505.11604", "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/", "AI": {"tldr": "Talk-to-Your-Slides is an LLM-powered agent for editing PowerPoint slides, employing a two-level approach for high-level interpretation and low-level execution, significantly improving editing tasks.", "motivation": "To address the lack of research on editing existing PowerPoint slides using large language models, as previous efforts mostly focused on slide generation.", "method": "The system operates with a high-level LLM agent for interpreting user instructions and creating editing plans, complemented by Python scripts for direct manipulation of PowerPoint objects.", "result": "The Talk-to-Your-Slides system demonstrates superior performance over baseline methods in execution success rate, instruction fidelity, and editing efficiency, validated through a human-annotated dataset.", "conclusion": "This approach enables more flexible and contextually-aware editing of slides, marking a significant advancement in the interaction between LLMs and presentation software.", "key_contributions": ["Development of Talk-to-Your-Slides for live slide editing with LLMs", "Introduction of TSBench, a dataset for evaluating slide editing tasks", "Demonstration of improved performance over existing editing methods"], "limitations": "", "keywords": ["Large Language Models", "PowerPoint Editing", "Human-Computer Interaction", "Instruction Understanding", "Data Benchmark"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2505.12080", "pdf": "https://arxiv.org/pdf/2505.12080.pdf", "abs": "https://arxiv.org/abs/2505.12080", "title": "TrainBo: An Interactive Robot-assisted Scenario Training System for Older Adults with Dementia", "authors": ["Kwong Chiu Fung", "Wai Ho Mow"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Dementia is an overall decline in memory and cognitive skills severe enough\nto reduce an elders ability to perform everyday activities. There is an\nincreasing need for accessible technologies for cognitive training to slow down\nthe cognitive decline. With the ability to provide instant feedback and\nassistance, social robotic systems have been proven effective in enhancing\nlearning abilities across various age groups. This study focuses on the design\nof an interactive robot-assisted scenario training system TrainBo with\nself-determination theory, derives design requirements through formative and\nformal studies and the system usability is also be evaluated. A pilot test is\nconducted on seven older adults with dementia in an elderly care center in Hong\nKong for four weeks. Our finding shows that older adults with dementia have an\nimprovement in behavioural engagement, emotional engagement, and intrinsic\nmotivation after using Trainbo. These findings can provide valuable insights\ninto the development of more captivating interactive robots for extensive\ntraining purposes.", "AI": {"tldr": "The study presents TrainBo, an interactive robot-assisted cognitive training system for older adults with dementia, showing improvements in engagement and motivation after usage.", "motivation": "There is a growing need for accessible technologies to aid cognitive training for the elderly, specifically those with dementia.", "method": "Design of an interactive robot-assisted system using self-determination theory, followed by formative and formal studies to derive design requirements and evaluate usability.", "result": "Pilot testing on seven older adults with dementia showed significant improvements in behavioral and emotional engagement as well as intrinsic motivation after four weeks of using TrainBo.", "conclusion": "The findings indicate that interactive robotic systems like TrainBo can enhance cognitive training and engagement in older adults with dementia, providing insights for future robotic applications.", "key_contributions": ["Development of TrainBo, a robot-assisted cognitive training system", "Evaluation of user engagement and motivation in older adults with dementia", "Insights into design requirements for interactive robotic systems in cognitive training"], "limitations": "", "keywords": ["robot-assisted training", "dementia", "cognitive engagement", "interactive systems", "self-determination theory"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613.pdf", "abs": "https://arxiv.org/abs/2505.11613", "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings.", "AI": {"tldr": "The study introduces MedGUIDE, a benchmark for evaluating LLMs on adherence to clinical guidelines in decision-making, revealing performance shortcomings even in domain-specific models.", "motivation": "To evaluate the ability of LLMs to follow structured clinical decision-making protocols based on existing medical guidelines.", "method": "The researchers constructed MedGUIDE from NCCN decision trees, creating multiple-choice diagnostic questions from LLM-generated scenarios and applied a selection process for high-quality samples.", "result": "25 LLMs were evaluated, showing that even specialized models underperform in adhering to structured guidelines; there was no significant improvement with in-context guideline inclusion or pretraining.", "conclusion": "MedGUIDE highlights the need for assessing LLMs' effectiveness in real-world clinical applications, emphasizing the importance of guideline adherence.", "key_contributions": ["Introduction of the MedGUIDE benchmark for LLM evaluation in clinical decision-making", "Creation of a large dataset of diagnostic questions based on curated decision trees", "Findings indicate a performance gap in LLMs regarding guideline adherence"], "limitations": "The current LLMs evaluated show performance limitations, suggesting potential areas for improvement in model training or architecture.", "keywords": ["Large Language Models", "clinical guidelines", "decision trees", "health informatics", "MedGUIDE"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12101", "pdf": "https://arxiv.org/pdf/2505.12101.pdf", "abs": "https://arxiv.org/abs/2505.12101", "title": "Designing Scaffolded Interfaces for Enhanced Learning and Performance in Professional Software", "authors": ["Yimeng Liu", "Misha Sra"], "categories": ["cs.HC"], "comment": null, "summary": "Professional software offers immense power but also presents significant\nlearning challenges. Its complex interfaces, as well as insufficient built-in\nstructured guidance and unfamiliar terminology, often make newcomers struggle\nwith task completion. To address these challenges, we introduce ScaffoldUI, a\nmethod for scaffolded interface design to reduce interface complexity, provide\nstructured guidance, and enhance software learnability. The scaffolded\ninterface presents task-relevant tools, progressively discloses tool\ncomplexity, and organizes tools based on domain concepts, aiming to assist task\nperformance and software learning. To evaluate the feasibility of our interface\ndesign method, we present a technical pipeline for scaffolded interface\nimplementation in professional 3D software, i.e., Blender, and conduct user\nstudies with beginners (N=32) and experts (N=8). Study results demonstrate that\nour scaffolded interfaces significantly reduce perceived task load caused by\ninterface complexity, support task performance through structured guidance, and\naugment learning by clearly connecting concepts and tools within the taskflow\ncontext. Based on a discussion of the user study findings, we offer insights\nfor future research on designing scaffolded interfaces to support instruction,\nproductivity, creativity, and cross-software workflows.", "AI": {"tldr": "This paper presents ScaffoldUI, a method for designing scaffolded interfaces that improve software learnability by reducing complexity and providing structured guidance.", "motivation": "To address the learning challenges faced by newcomers in professional software with complex interfaces and poor guidance.", "method": "The paper introduces ScaffoldUI, which organizes task-relevant tools, progressively discloses complexity, and enhances navigation through structured guidance in a 3D software context.", "result": "User studies showed improved task performance and reduced perceived task load when using scaffolded interfaces compared to traditional designs.", "conclusion": "Scaffolded interfaces can significantly enhance learning and productivity in software use by clearly linking tools to concepts within taskflows.", "key_contributions": ["Introduction of ScaffoldUI for scaffolded interface design.", "Evaluation through user studies indicating reduced cognitive load and improved learning.", "Insights for future research on scaffolded interfaces across various software applications."], "limitations": "Further research needed for diverse software environments and long-term user adaptation.", "keywords": ["Scaffolded Interface Design", "Learning Challenges", "User Studies", "Software Learnability", "Complex Interfaces"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615.pdf", "abs": "https://arxiv.org/abs/2505.11615", "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior.", "AI": {"tldr": "The paper proposes a method to influence large language models (LLMs) behavior through identified steering vectors, enabling modifications without retraining.", "motivation": "To effectively change LLM behavior using specific modifications to their internal activations, avoiding the need for retraining or fine-tuning.", "method": "The authors align latent representations obtained through behavioral methods (MCMC with LLMs) with neural counterparts to identify steering vectors.", "result": "The steering vectors reliably modulate LLM outputs in accordance with targeted behaviors, specifically for risk-related outputs.", "conclusion": "This approach provides a systematic way to influence LLM outputs effectively and can be applied to various behavioral adjustments in LLMs.", "key_contributions": ["Introduction of a method to systematically identify steering vectors for LLMs", "Demonstration of effective risk preference modulation in LLM outputs", "Avoidance of fine-tuning or retraining through representational engineering"], "limitations": "", "keywords": ["large language models", "steering vectors", "behavioral methods", "latent representations", "risk preferences"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12114", "pdf": "https://arxiv.org/pdf/2505.12114.pdf", "abs": "https://arxiv.org/abs/2505.12114", "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals", "authors": ["Dena F. Mujtaba", "Nihar R. Mahapatra"], "categories": ["cs.HC", "cs.CV", "eess.IV"], "comment": null, "summary": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing.", "AI": {"tldr": "This paper addresses ethical concerns in AI-enhanced personality assessments used in hiring by introducing a counterfactual-based framework to evaluate and quantify bias.", "motivation": "The integration of AI in personality assessments for hiring raises ethical issues, particularly concerning bias amplification based on protected attributes like gender, ethnicity, and age.", "method": "The paper presents a counterfactual-based framework using generative adversarial networks (GANs) to generate altered representations of job applicants, allowing for a multimodal fairness analysis without direct access to underlying models.", "result": "The method reveals significant disparities across demographic groups when applied to a state-of-the-art personality prediction model.", "conclusion": "This work provides a scalable tool for fairness auditing of AI hiring platforms, promoting ethical transparency through counterfactual approaches.", "key_contributions": ["Introduces a counterfactual-based framework for bias evaluation in AI personality assessments.", "Employs GANs for generating counterfactual representations to analyze bias without model access.", "Supports multimodal evaluation across visual, audio, and textual features."], "limitations": "The framework may require domain-specific adaptations for different assessment contexts and does not address all forms of bias comprehensively.", "keywords": ["bias evaluation", "counterfactual analysis", "AI hiring", "personality assessment", "ethical transparency"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626.pdf", "abs": "https://arxiv.org/abs/2505.11626", "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications.", "AI": {"tldr": "Introducing THELMA, a framework for evaluating RAG-based question answering applications without the need for reference responses.", "motivation": "To provide a systematic evaluation approach for RAG QA applications that does not rely on labeled data.", "method": "THELMA consists of six interdependent metrics designed for a holistic and fine-grained evaluation of RAG question answering applications.", "result": "Findings reveal the interactions among the THELMA metrics, assisting in pinpointing specific components of RAG that require enhancement in QA applications.", "conclusion": "THELMA facilitates the evaluation, monitoring, and improvement of RAG QA pipelines effectively without labeled sources.", "key_contributions": ["Development of THELMA framework for RAG QA evaluation", "Introduction of six interdependent metrics", "Insights on the interplay of metrics for targeted improvements"], "limitations": "", "keywords": ["RAG", "question answering", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12516", "pdf": "https://arxiv.org/pdf/2505.12516.pdf", "abs": "https://arxiv.org/abs/2505.12516", "title": "Towards Immersive Mixed Reality Street Play: Understanding Collocated Bodily Play with See-through Head-Mounted Displays in Public Spaces", "authors": ["Botao Amber Hu", "Rem Rungu Lin", "Yilan Elan Tao", "Samuli Laato", "Yue Li"], "categories": ["cs.HC", "cs.CY"], "comment": "Submitted to CSCW 2025", "summary": "We're witnessing an upcoming paradigm shift as Mixed Reality (MR) See-through\nHead-Mounted Displays (HMDs) become ubiquitous, with use shifting from\ncontrolled, private settings to spontaneous, public ones. While location-based\npervasive mobile games like Pok\\'emon GO have seen success, the embodied\ninteraction of MR HMDs is moving us from phone-based screen-touching gameplay\nto MR HMD-enabled collocated bodily play. Major tech companies are continuously\nreleasing visionary videos where urban streets transform into vast mixed\nreality playgrounds-imagine Harry Potter-style wizard duels on city streets.\nHowever, few researchers have conducted real-world, in-the-wild studies of such\nImmersive Mixed Reality Street Play (IMRSP) in public spaces in anticipation of\na near future with prevalent MR HMDs. Through empirical studies on a series of\nresearch-through-design game probes called Multiplayer Omnipresent Fighting\nArena (MOFA), we gain initial understanding of this under-explored area by\nidentifying the social implications, challenges, and opportunities of this new\nparadigm.", "AI": {"tldr": "This paper explores the emerging paradigm of Mixed Reality Head-Mounted Displays in public spaces through studies on the Multiplayer Omnipresent Fighting Arena (MOFA) game, highlighting social implications and design challenges.", "motivation": "To investigate the transition of Mixed Reality HMDs from controlled environments to spontaneous public settings, particularly in the domain of immersive gameplay.", "method": "Empirical studies conducted using research-through-design game probes called Multiplayer Omnipresent Fighting Arena (MOFA) to understand user interactions in real-world settings.", "result": "Identification of social implications, challenges, and opportunities related to Immersive Mixed Reality Street Play (IMRSP) in public spaces.", "conclusion": "The findings underscore the need for further exploration of the social dynamics and design considerations for MR experiences in urban environments as technology advances.", "key_contributions": ["Empirical insights into user interactions with MR HMDs in public spaces.", "Analysis of the social implications of MR HMDs on public gameplay dynamics.", "Identification of design challenges for future immersive applications."], "limitations": "", "keywords": ["Mixed Reality", "Head-Mounted Displays", "Immersive Gameplay", "Social Implications", "Public Spaces"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628.pdf", "abs": "https://arxiv.org/abs/2505.11628", "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques.", "AI": {"tldr": "This paper introduces Critique-Guided Distillation (CGD), a framework that enhances Supervised Fine-Tuning (SFT) by incorporating teacher critiques and refined responses to improve understanding in machine learning models.", "motivation": "The limitation of traditional supervised fine-tuning methods that lead to imitative responses without understanding is addressed.", "method": "The proposed CGD framework involves a multi-stage process where a student model learns from teacher model-generated critiques and refined responses, mapping input prompts and critiques to improved outputs.", "result": "Empirical evaluations demonstrate that CGD leads to significant improvements in performance on math and language understanding benchmarks, with AMC23 showing a +17.5% increase and MMLU-Pro +6.3%.", "conclusion": "CGD effectively reduces refinement uncertainty and interprets critiques within a Bayesian framework, providing a method to overcome pitfalls of previous critique-based fine-tuning methods.", "key_contributions": ["Introduction of Critique-Guided Distillation (CGD)", "Demonstration of significant performance improvements on benchmark tasks", "Reduction of format drift issues in critique fine-tuning"], "limitations": "", "keywords": ["Supervised Fine-Tuning", "Critique-Guided Distillation", "Machine Learning", "Model Understanding", "Benchmark Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12666", "pdf": "https://arxiv.org/pdf/2505.12666.pdf", "abs": "https://arxiv.org/abs/2505.12666", "title": "Adapting to LLMs: How Insiders and Outsiders Reshape Scientific Knowledge Production", "authors": ["Huimin Xu", "Houjiang Liu", "Yan Leng", "Ying Ding"], "categories": ["cs.HC"], "comment": null, "summary": "CSCW has long examined how emerging technologies reshape the ways researchers\ncollaborate and produce knowledge, with scientific knowledge production as a\ncentral area of focus. As AI becomes increasingly integrated into scientific\nresearch, understanding how researchers adapt to it reveals timely\nopportunities for CSCW research -- particularly in supporting new forms of\ncollaboration, knowledge practices, and infrastructure in AI-driven science.\n  This study quantifies LLM impacts on scientific knowledge production based on\nan evaluation workflow that combines an insider-outsider perspective with a\nknowledge production framework. Our findings reveal how LLMs catalyze both\ninnovation and reorganization in scientific communities, offering insights into\nthe broader transformation of knowledge production in the age of generative AI\nand sheds light on new research opportunities in CSCW.", "AI": {"tldr": "This study explores the impact of large language models (LLMs) on scientific knowledge production and offers insights into their role in reshaping collaboration and knowledge practices in AI-driven science.", "motivation": "To understand how the integration of AI, particularly LLMs, transforms collaboration and knowledge production in scientific research.", "method": "The study employs an evaluation workflow that merges insider-outside perspectives with a framework for knowledge production to quantify the effects of LLMs in scientific communities.", "result": "The findings demonstrate that LLMs facilitate innovation and reorganization within scientific communities, highlighting the significant changes in knowledge production due to generative AI.", "conclusion": "The research suggests potential new avenues for exploration in CSCW, grounded in the insights gained regarding the transformational impact of LLMs on scientific collaboration and knowledge practices.", "key_contributions": ["Quantifies the impact of LLMs on scientific knowledge production.", "Identifies new forms of collaboration and infrastructure influenced by AI.", "Offers insights into the transformation of knowledge production with generative AI."], "limitations": "", "keywords": ["AI in research", "knowledge production", "collaboration", "large language models", "CSCW"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643.pdf", "abs": "https://arxiv.org/abs/2505.11643", "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "authors": ["Xiang Fu"], "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion.", "AI": {"tldr": "Developmentally ordered curriculum enhances reasoning transparency and sample efficiency in small language models without task-specific fine-tuning.", "motivation": "To improve reasoning capabilities and efficiency in small language models (SLMs) through a structured training approach.", "method": "Cognivolve, a 124 M-parameter GPT-2 model, is trained on a four-stage curriculum from lexical matching to multi-step symbolic inference.", "result": "Cognivolve achieves target accuracy in half the optimization steps of a baseline model and improves gradient saliency and attention methods.", "conclusion": "The results indicate that curriculum progression is crucial for performance improvement, despite ongoing challenges in achieving conventional accuracy levels.", "key_contributions": ["Developmentally ordered curriculum significantly enhances reasoning in SLMs.", "Cognivolve model demonstrates improved efficiency and reasoning capabilities without fine-tuning.", "Identifies gaps in final-answer success and saliency detection for further research."], "limitations": "Final-answer success lags behind conventional models by about 30%, and current saliency probes may under-detect certain reasoning capabilities.", "keywords": ["small language models", "curriculum learning", "reasoning transparency", "sample efficiency", "Cognivolve"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.12780", "pdf": "https://arxiv.org/pdf/2505.12780.pdf", "abs": "https://arxiv.org/abs/2505.12780", "title": "Beyond Individual UX: Defining Group Experience(GX) as a New Paradigm for Group-centered AI", "authors": ["Soohwan Lee", "Seoyeong Hwang", "Kyungho Lee"], "categories": ["cs.HC"], "comment": "Accepted at DIS'25 Companion (Provocations)", "summary": "Recent advancements in HCI and AI have predominantly centered on individual\nuser experiences, often neglecting the emergent dynamics of group interactions.\nThis provocation introduces Group Experience(GX) to capture the collective\nperceptual, emotional, and cognitive dimensions that arise when individuals\ninteract in cohesive groups. We challenge the conventional Human-centered AI\nparadigm and propose Group-centered AI(GCAI) as a framework that actively\nmediates group dynamics, amplifies diverse voices, and fosters ethical\ncollective decision-making. Drawing on social psychology, organizational\nbehavior, and group dynamics, we outline a group-centered design approach that\nbalances individual autonomy with collective interests while developing novel\nevaluative metrics. Our analysis emphasizes rethinking traditional\nmethodologies that focus solely on individual outcomes and advocates for\ninnovative strategies to capture group collaboration. We call on researchers to\nbridge the gap between micro-level experiences and macro-level impacts,\nultimately enriching and transforming collaborative human interactions.", "AI": {"tldr": "This paper introduces Group Experience (GX) and a proposed framework of Group-centered AI (GCAI) to enhance group interactions in HCI.", "motivation": "The paper seeks to address the neglect of group dynamics in current HCI and AI research, emphasizing the need for a framework that considers collective user experiences.", "method": "The authors propose a group-centered design approach informed by social psychology and group dynamics, focusing on holistic group interactions instead of just individual outcomes.", "result": "The paper outlines new evaluative metrics and strategies to capture the essence of group collaboration and decision-making processes.", "conclusion": "The authors call for a shift in focus from individual experiences to group dynamics in HCI, advocating for a transformative approach to collaborative interactions.", "key_contributions": ["Introduction of Group Experience (GX) concept.", "Proposal of Group-centered AI (GCAI) framework.", "Development of novel evaluative metrics for group interactions."], "limitations": "", "keywords": ["Group Experience", "Human-Centered AI", "Group-Centered AI", "Collaboration", "Group Dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665.pdf", "abs": "https://arxiv.org/abs/2505.11665", "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years.", "AI": {"tldr": "This paper surveys multilingual prompt engineering techniques to enhance LLM performance across 250 languages, categorizing 39 techniques applied to 30 NLP tasks based on various resources.", "motivation": "To address the challenges of ensuring effective LLM performance across multiple languages and to make these capabilities accessible to a broader audience.", "method": "The paper categorizes various multilingual prompting techniques and analyzes their efficacy across diverse datasets spanning around 250 languages, based on existing research.", "result": "The survey reviews 36 papers detailing different prompting techniques and their applications to NLP tasks, highlighting trends across language families and resource levels.", "conclusion": "The study provides insights into multilingual prompting strategies and discusses potential state-of-the-art methods for improving LLM performance across low-resource and high-resource languages.", "key_contributions": ["Survey of multilingual prompting techniques for LLMs", "Categorization of 39 techniques across 30 NLP tasks", "Insights into the trends in language resource distribution and prompting methods."], "limitations": "The discussion is grounded in existing research, which may not capture the latest advancements beyond the reviewed papers.", "keywords": ["multilingual prompting", "large language models", "natural language processing", "NLP tasks", "prompt engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13046", "pdf": "https://arxiv.org/pdf/2505.13046.pdf", "abs": "https://arxiv.org/abs/2505.13046", "title": "StudyAlign: A Software System for Conducting Web-Based User Studies with Functional Interactive Prototypes", "authors": ["Florian Lehmann", "Daniel Buschek"], "categories": ["cs.HC", "H.5.2; I.2.7"], "comment": "EICS 2025", "summary": "Interactive systems are commonly prototyped as web applications. This\napproach enables studies with functional prototypes on a large scale. However,\nsetting up these studies can be complex due to implementing experiment\nprocedures, integrating questionnaires, and data logging. To enable such user\nstudies, we developed the software system StudyAlign which offers: 1) a\nfrontend for participants, 2) an admin panel to manage studies, 3) the\npossibility to integrate questionnaires, 4) a JavaScript library to integrate\ndata logging into prototypes, and 5) a backend server for persisting log data,\nand serving logical functions via an API to the different parts of the system.\nWith our system, researchers can set up web-based experiments and focus on the\ndesign and development of interactions and prototypes. Furthermore, our\nsystematic approach facilitates the replication of studies and reduces the\nrequired effort to execute web-based user studies. We conclude with reflections\non using StudyAlign for conducting HCI studies online.", "AI": {"tldr": "StudyAlign streamlines the setup of web-based user studies for HCI researchers by providing an integrated software system for managing experiments, collecting data, and facilitating easy questionnaire integration.", "motivation": "To address the complexity of setting up interactive systems for user studies and to allow researchers to focus on interaction design rather than technical hurdles.", "method": "Development of StudyAlign, which includes a participant frontend, admin panel, questionnaire integration, JavaScript library for data logging, and a backend server for data persistence and API functions.", "result": "StudyAlign simplifies the process for researchers to conduct web-based experiments, enhancing study replication and reducing logistical efforts.", "conclusion": "StudyAlign supports online HCI studies effectively by reducing setup complexity and focusing on design aspects.", "key_contributions": ["Integrated platform for managing web-based user studies", "Streamlined data logging and questionnaire integration", "Facilitates replication of studies"], "limitations": "", "keywords": ["HCI", "user studies", "web applications", "data logging", "questionnaires"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679.pdf", "abs": "https://arxiv.org/abs/2505.11679", "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction.", "AI": {"tldr": "This paper addresses the challenge of ambiguity in natural language for improved text to structured data mapping using a new framework based on latent space representation.", "motivation": "Ambiguity in natural language hinders accurate mapping from text to structured data, affecting various applications such as agentic tool calling and text-to-SQL tasks.", "method": "The authors characterize the representation differences of ambiguous text in the latent space of large language models (LLMs) and propose a new measurement of distance that accounts for missing concepts in this latent space, using a sparse-autoencoder approach.", "result": "The proposed framework improves the detection of ambiguity in agentic tool calling, thereby enhancing the performance of LLMs in handling ambiguous inputs.", "conclusion": "By focusing on the relationship between ambiguous questions and their interpretations, the framework offers a systematic way to identify and address ambiguity in natural language processing tasks.", "key_contributions": ["New distance measurement for detecting ambiguity in latent space", "Framework to improve LLM performance on ambiguous queries", "Identification of missing concepts as a cause of ambiguity"], "limitations": "", "keywords": ["ambiguous text", "large language models", "latent space", "sparse-autoencoder", "tool calling"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13218", "pdf": "https://arxiv.org/pdf/2505.13218.pdf", "abs": "https://arxiv.org/abs/2505.13218", "title": "Human Response to Decision Support in Face Matching: The Influence of Task Difficulty and Machine Accuracy", "authors": ["Marina EstÃ©vez-Almenzar", "Ricardo Baeza-Yates", "Carlos Castillo"], "categories": ["cs.HC"], "comment": null, "summary": "Decision support systems enhanced by Artificial Intelligence (AI) are\nincreasingly being used in high-stakes scenarios where errors or biased\noutcomes can have significant consequences. In this work, we explore the\nconditions under which AI-based decision support systems affect the decision\naccuracy of humans involved in face matching tasks. Previous work suggests that\nthis largely depends on various factors, such as the specific nature of the\ntask and how users perceive the quality of the decision support, among others.\nHence, we conduct extensive experiments to examine how both task difficulty and\nthe precision of the system influence human outcomes. Our results show a strong\ninfluence of task difficulty, which not only makes humans less precise but also\nless capable of determining whether the decision support system is yielding\naccurate suggestions or not. This has implications for the design of decision\nsupport systems, and calls for a careful examination of the context in which\nthey are deployed and on how they are perceived by users.", "AI": {"tldr": "This paper investigates the impact of AI-based decision support systems on human accuracy in face matching tasks, highlighting the importance of task difficulty and user perception.", "motivation": "To understand how AI-based decision support affects human decision-making in high-stakes tasks where errors can have significant consequences.", "method": "Extensive experiments were conducted to assess the influence of task difficulty and system precision on human decision outcomes.", "result": "Task difficulty significantly impaired human precision and the ability to assess the accuracy of AI suggestions.", "conclusion": "The design of decision support systems must consider context and user perception to enhance their effectiveness.", "key_contributions": ["Investigates the relationship between AI support and human decision accuracy in challenging tasks.", "Identifies the critical role of task difficulty in influencing human judgment.", "Calls for a user-centric approach in the design of AI decision support systems."], "limitations": "", "keywords": ["AI decision support", "human decision-making", "task difficulty", "face matching", "user perception"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683.pdf", "abs": "https://arxiv.org/abs/2505.11683", "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "authors": ["Susanna RÃ¼cker", "Alan Akbik"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark.", "AI": {"tldr": "The paper presents VerbalizED, a document-level Dual Encoder model for entity disambiguation, evaluating key design choices and achieving new state-of-the-art results.", "motivation": "To improve entity disambiguation by evaluating and optimizing design decisions in Dual Encoder systems.", "method": "The model uses contextual label verbalizations and efficient hard negative sampling in a Dual Encoder framework to enhance predictions.", "result": "Achieved new state-of-the-art performance on the ZELDA benchmark through comprehensive experiments on AIDA-Yago.", "conclusion": "The research offers valuable insights into design choices that significantly influence entity disambiguation performance.", "key_contributions": ["Introduction of VerbalizED model for entity disambiguation", "Evaluation of design choices affecting Dual Encoder effectiveness", "Demonstrated new state-of-the-art results on the ZELDA benchmark"], "limitations": "", "keywords": ["Entity Disambiguation", "Dual Encoders", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13381", "pdf": "https://arxiv.org/pdf/2505.13381.pdf", "abs": "https://arxiv.org/abs/2505.13381", "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms.", "AI": {"tldr": "A practice exam system integrates AI-generated feedback with textbook references to enhance learning in a large biology course, showing increased student engagement and confidence.", "motivation": "To address the challenge of providing personalized feedback at scale in large undergraduate STEM courses.", "method": "An AI system using GPT-4o generates personalized feedback based on students' explanations and confidence ratings, directing them to relevant textbook sections, evaluated through interaction logs and surveys.", "result": "Though no significant performance differences were found across feedback types, trends indicated benefits from confidence ratings and explanations, with high engagement and satisfaction reported by students.", "conclusion": "Embedding structured reflection requirements may be more effective than advanced feedback mechanisms in improving student outcomes.", "key_contributions": ["Integration of AI feedback with targeted textbook references", "Empirical evaluation of metacognitive strategies in exams", "High levels of student engagement with learning materials"], "limitations": "", "keywords": ["AI feedback", "Metacognition", "STEM education"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690.pdf", "abs": "https://arxiv.org/abs/2505.11690", "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages.", "AI": {"tldr": "This study explores the challenges and strategies for developing Automatic Speech Recognition (ASR) systems for low-resource African languages, emphasizing ethical and inclusive approaches.", "motivation": "To address the underrepresentation of low-resource languages in ASR technologies and improve digital interaction for speakers in Africa.", "method": "Critical analysis of challenges faced by ASR development, along with case studies and strategies like community-driven data collection and multilingual learning.", "result": "Identified barriers include data scarcity and linguistic complexity; suggested strategies demonstrate feasibility with pilot projects in healthcare and education sectors.", "conclusion": "Interdisciplinary collaboration and investment are essential to create ethical ASR systems that enhance accessibility and socioeconomic opportunities for African language speakers.", "key_contributions": ["Identification of key challenges in ASR for African languages", "Proposed practical strategies for inclusive ASR development", "Evidence from successful pilot projects demonstrating impact"], "limitations": "", "keywords": ["Automatic Speech Recognition", "low-resource languages", "ethical AI", "community-driven data", "multilingual learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693.pdf", "abs": "https://arxiv.org/abs/2505.11693", "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-JyrÃ¤", "Carlos GÃ³mez-RodrÃ­guez"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks.", "AI": {"tldr": "The paper introduces a new family of encodings for sequence labeling dependency parsing, aiming for optimality in label efficiency and supporting arbitrary non-projectivity.", "motivation": "There is a need for more efficient encodings in dependency parsing that can minimize the number of symbols used while maintaining competitive parsing accuracy.", "method": "The authors develop a hierarchical bracketing approach, extending it from existing encodings to ensure it uses fewer labels, deriving an optimal form for projective trees and non-projective structures.", "result": "The new encodings use only 12 distinct labels for projective trees, improving upon the 16 used in the previous 4-bit encoding, while achieving competitive accuracy across multiple treebanks.", "conclusion": "This work enhances the efficiency of dependency parsing encodings, presenting a viable alternative to current methods with its optimization for fewer labels.", "key_contributions": ["Introduction of a family of encodings based on hierarchical bracketing for dependency parsing.", "Derivation of an optimal encoding that reduces the number of labels for projective trees.", "Extension of hierarchical bracketing to support arbitrary non-projectivity in a compact manner."], "limitations": "", "keywords": ["dependency parsing", "sequence labeling", "hierarchical bracketing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726.pdf", "abs": "https://arxiv.org/abs/2505.11726", "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues.", "AI": {"tldr": "This paper presents a unified framework for multimodal and textual reference resolution, enhancing understanding in visually grounded dialogues.", "motivation": "To improve reference resolution in dialogue systems by addressing ambiguities caused by pronouns and ellipses in both textual and multimodal contexts.", "method": "A framework that integrates textual and multimodal reference resolution by mapping mention embeddings to object embeddings, emphasizing similarity-based selection.", "result": "The proposed model outperforms existing models like MDETR and GLIP in pronoun phrase grounding by incorporating coreference resolution techniques, leading to improved confidence in relations between mentions and objects.", "conclusion": "Incorporating textual reference relations into multimodal systems enhances performance and reduces ambiguities in dialogue, with promising results from qualitative analysis.", "key_contributions": ["Unified framework for textual and multimodal reference resolution", "Improved performance in grounding pronouns", "Integration of coreference resolution to enhance confidence scores"], "limitations": "", "keywords": ["Multimodal reference resolution", "Coreference resolution", "Phrase grounding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733.pdf", "abs": "https://arxiv.org/abs/2505.11733", "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning.", "AI": {"tldr": "Introduction of MedCaseReasoning, an open-access dataset to evaluate LLMs on clinical diagnostic reasoning accuracy.", "motivation": "Existing medical benchmarks focus only on final answer accuracy, neglecting the reasoning process critical for medical diagnoses.", "method": "Creation of MedCaseReasoning dataset containing 14,489 diagnostic Q&A cases paired with clinician-authored reasoning statements, followed by evaluations of reasoning LLMs on this dataset.", "result": "The top-performing LLM achieved only 48% diagnostic accuracy and 64% recall of clinician reasoning. Fine-tuning on reasoning traces improved diagnostic accuracy by 29% and recall by 41%.", "conclusion": "Fine-tuning LLMs on realistic reasoning data significantly enhances their clinical diagnostic capabilities.", "key_contributions": ["First open-access dataset for evaluating LLMs in medical reasoning", "Demonstrated shortcomings of current LLMs in medical diagnostics", "Showed improvements in diagnostic accuracy and reasoning recall through fine-tuning"], "limitations": "", "keywords": ["Large Language Models", "medical diagnosis", "dataset", "clinical reasoning", "fine-tuning"], "importance_score": 10, "read_time_minutes": 5}}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739.pdf", "abs": "https://arxiv.org/abs/2505.11739", "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability.", "AI": {"tldr": "This paper introduces ZeroTuning, a training-free method that optimizes large language model (LLM) performance by tuning an initial token's attention, outperforming traditional task-specific token tuning.", "motivation": "To address the limitations of existing token-level attention tuning methods in large language models, which often rely on auxiliary mechanisms and introduce biases.", "method": "The paper proposes ZeroTuning, which involves tuning the attention of a semantically empty initial token to improve LLM performance without the need for training on task-specific tokens.", "result": "ZeroTuning achieves significant improvements in performance across text classification, multiple-choice, and multi-turn conversation tasks. For instance, it improved Llama-3.1-8B by 11.71% on classification tasks.", "conclusion": "Tuning the attention of the initial token provides a powerful means to enhance the performance of large language models while being robust across various conditions.", "key_contributions": ["Introduces ZeroTuning, a novel training-free attention tuning approach for LLMs.", "Demonstrates that tuning the initial token is more effective than tuning other tokens.", "Provides insights into the impact of attention heads and layers on model performance."], "limitations": "", "keywords": ["large language models", "attention tuning", "ZeroTuning", "model interpretability", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183.pdf", "abs": "https://arxiv.org/abs/2505.12183", "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "authors": ["Manari Hirose", "Masato Uchida"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems.", "AI": {"tldr": "This study presents a framework for evaluating the ideological biases of Large Language Models (LLMs) via quantitative analysis, revealing differences in opinions and problematic biases in ChatGPT and Gemini.", "motivation": "To address the need for empirical research on the biases and societal implications of the widespread integration of LLMs.", "method": "The study introduces a novel framework for evaluating LLMs by analyzing 436 binary-choice questions, focusing on ideological biases across different models and languages.", "result": "Findings indicate that while LLMs maintain consistent opinions, ideologies differ across models and languages; notably, ChatGPT tends to align its opinions with that of the questioner, with both models displaying problematic biases.", "conclusion": "The results highlight the necessity for ethical considerations in LLM evaluations and the framework provides a flexible quantitative method for assessing LLM behavior, fostering the development of socially aligned AI systems.", "key_contributions": ["Development of a novel framework for LLM evaluation", "Empirical findings on ideological biases of ChatGPT and Gemini", "Identification of problematic biases and their societal implications"], "limitations": "Results are specific to the evaluated models and may not generalize to all LLMs; the binary-choice questions might not capture the full complexity of underlying ideas.", "keywords": ["Large Language Models", "ideological biases", "quantitative analysis", "ethical AI", "ChatGPT"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746.pdf", "abs": "https://arxiv.org/abs/2505.11746", "title": "Token Masking Improves Transformer-Based Text Classification", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling.", "AI": {"tldr": "This paper introduces token masking regularization to enhance transformer-based models in text classification.", "motivation": "To explore the effectiveness of masking input tokens in transformer-based models for improved performance.", "method": "The proposed method randomly replaces input tokens with a [MASK] token at a set probability during training to create stochastic perturbations.", "result": "Experiments demonstrate consistent performance improvements across language identification and sentiment analysis tasks with various models compared to standard regularization techniques.", "conclusion": "Task-specific optimal masking rates were identified, with a general default of p = 0.1 showing strong efficacy, attributed to reduced overfitting and implicit gradient smoothing.", "key_contributions": ["Introduction of token masking regularization for transformers", "Empirical validation of the method across multiple tasks and models", "Identification of optimal masking rates for specific tasks"], "limitations": "", "keywords": ["transformer models", "token masking", "text classification", "regularization", "overfitting"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349.pdf", "abs": "https://arxiv.org/abs/2505.12349", "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "authors": ["Axel Abels", "Tom Lenaerts"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts.", "AI": {"tldr": "This paper investigates bias in large language models (LLMs) and explores crowd-based strategies for reducing this bias through response aggregation, highlighting the effectiveness of locally weighted aggregation and hybrid approaches.", "motivation": "To address the issue of bias perpetuated by large language models in their responses, which mirrors human biases.", "method": "The authors analyze LLM responses to bias-eliciting headlines and explore different aggregation methods, including simple averaging and locally weighted aggregation, to mitigate bias.", "result": "Locally weighted aggregation methods more effectively reduce bias and improve accuracy compared to simple averaging, and hybrid crowds combining LLMs with humans yield further improvements.", "conclusion": "Hybrid crowds enhance performance and reduce biases, combining the strengths of LLMs and human diversity in bias mitigation.", "key_contributions": ["Demonstration of how averaging LLM responses can exacerbate biases due to limited diversity.", "Development of locally weighted aggregation methods that significantly improve bias mitigation and accuracy.", "Evidence that hybrid crowds of LLMs and humans are more effective in reducing biases across diverse contexts."], "limitations": "", "keywords": ["bias", "large language models", "crowd-based strategies", "aggregation methods", "hybrid crowds"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754.pdf", "abs": "https://arxiv.org/abs/2505.11754", "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.", "AI": {"tldr": "This paper investigates multi-hop question answering (MHQA) with language models, showing that encoder-decoder models outperform causal decoder-only models, and proposes methods to enhance performance through attention mechanisms.", "motivation": "The study addresses the challenges of multi-hop question answering, where language models need to retrieve and reason with multiple sources of information.", "method": "We permuted search results provided to language models and analyzed their performance, comparing encoder-decoder models with causal decoder-only models in various configurations.", "result": "Encoder-decoder models like Flan-T5 outperform causal decoder-only models despite being smaller, and the order of documents significantly impacts reasoning performance.", "conclusion": "By modifying causal masks and analyzing attention weights, we can improve language model performance in multi-hop reasoning tasks.", "key_contributions": ["Demonstrates the superiority of encoder-decoder models in MHQA tasks.", "Finds optimal document ordering improves model reasoning performance.", "Proposes enhancements for causal decoder models using bi-directional attention."], "limitations": "The study focuses primarily on specific model configurations and may not generalize across all language model architectures.", "keywords": ["Multi-hop Question Answering", "Language Models", "Attention Mechanisms", "Flan-T5", "Causal Mask"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764.pdf", "abs": "https://arxiv.org/abs/2505.11764", "title": "Towards Universal Semantics With Large Language Models", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.", "AI": {"tldr": "This paper explores using large language models to generate Natural Semantic Metalanguage (NSM) explications, enhancing the speed and efficiency of NLP tasks.", "motivation": "The NSM theory aims to uncover universal meanings through semantic primes, but generating explications has been a slow manual process.", "method": "The authors utilize large language models (LLMs), develop a tailored dataset, and create automatic evaluation methods for generating NSM explications.", "result": "The fine-tuned 1B and 8B models demonstrated improved performance over GPT-4o in generating accurate explications.", "conclusion": "This study represents a significant advancement in the use of LLMs for universal semantic representation, with potential applications in semantic analysis and translation.", "key_contributions": ["First study to use LLMs for generating NSM explications", "Development of automatic evaluation methods for NSM", "Creation of a tailored dataset for training and evaluation"], "limitations": "", "keywords": ["Natural Semantic Metalanguage", "large language models", "NLP", "semantic representation", "LLM"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807.pdf", "abs": "https://arxiv.org/abs/2505.11807", "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.", "AI": {"tldr": "Retrospex is a novel LLM-based agent framework that enhances learning from past experiences through a retrospective process, utilizing a reinforcement learning critic for action value estimation.", "motivation": "Existing LLM agent frameworks do not leverage past experiences effectively for agent improvement.", "method": "Retrospex does not integrate experiences into the LLM's context but combines the LLM's action likelihood with action values from a reinforcement learning critic, trained on past experiences through retrospection.", "result": "Evaluation in various environments shows Retrospex outperforms strong contemporary baselines, particularly in scenarios requiring significant interaction with the environment.", "conclusion": "Retrospex offers a promising approach to improve LLM agents by enhancing their ability to learn from past interactions.", "key_contributions": ["Introduces the Retrospex framework for LLM agents", "Implements a unique retrospection process for experience analysis", "Dynamic action rescoring mechanism based on interaction requirements"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Agent Framework"], "importance_score": 8, "read_time_minutes": 17}}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718.pdf", "abs": "https://arxiv.org/abs/2505.12718", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "AI": {"tldr": "This paper presents an automated approach to assess biases in AI-generated educational materials, demonstrating high reliability and consistency with minimal human subjectivity.", "motivation": "To address ethical concerns regarding biases in AI-generated content used in tutor training, focusing on the need for systematic methods to evaluate these biases.", "method": "The study integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction approach within a Retrieval-Augmented Generation framework to automate bias assessment.", "result": "The proposed method showed a strong Pearson correlation coefficient of r = 0.993 between automated and manually curated word sets, indicating reliable bias assessment.", "conclusion": "The approach reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing AI-generated educational content.", "key_contributions": ["Automated bias assessment method for AI-generated educational content", "Integration of Contextualized Embedding Association Test with prompt-engineered extraction", "High reliability demonstrated with strong Pearson correlation coefficient"], "limitations": "", "keywords": ["Generative AI", "Bias Assessment", "Education", "Automated Evaluation", "Ethics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810.pdf", "abs": "https://arxiv.org/abs/2505.11810", "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies.", "AI": {"tldr": "This paper introduces AI Taiyan, a large language model engineered for Classical Chinese that outperforms general models in specific language processing tasks with only 1.8 billion parameters.", "motivation": "Despite successes of general-purpose large language models in various tasks, their limitations in specialized domains like Classical Chinese necessitate the development of tailored models that can better incorporate domain-specific knowledge.", "method": "The AI Taiyan model was designed with a careful approach to model architecture, data processing, foundational training, and fine-tuning to achieve effective performance in Classical Chinese information processing tasks.", "result": "AI Taiyan demonstrated significant advantages over both general-purpose models and traditional domain-specific models in tasks such as punctuation, allusion identification, word meaning explanation, and translation, nearing or exceeding human performance.", "conclusion": "This research serves as a reference for constructing specialized large language models and highlights the model's applicability in areas like ancient text collation and language research.", "key_contributions": ["Development of AI Taiyan, specifically for Classical Chinese language tasks.", "Demonstrated superior performance on various language processing tasks compared to existing models.", "Provided insights into efficiently creating specialized domain-specific large language models."], "limitations": "", "keywords": ["large language models", "Classical Chinese", "domain-specific knowledge", "AI Taiyan", "language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "AI": {"tldr": "This paper presents a comprehensive corpus for training neural models to classify mental health stigma, derived from expert-annotated human-chatbot interviews.", "motivation": "To address the lack of robust resources and theoretical frameworks in training neural models for detecting mental health stigma.", "method": "Development of an expert-annotated dataset from human-chatbot interviews, comprising 4,141 snippets from 684 participants, followed by experiments using state-of-the-art neural models.", "result": "The paper benchmarks the performance of various neural models on the stigma detection task and identifies empirical challenges in this domain.", "conclusion": "The dataset can enhance research focused on detecting, neutralizing, and counteracting mental-health stigma through computational methods.", "key_contributions": ["Expert-annotated dataset specifically addressing mental health stigma.", "In-depth benchmarking of neural models for stigma detection.", "Identification of empirical challenges and theoretical considerations in stigma classification."], "limitations": "The dataset is limited to chatbot conversational data and may not generalize beyond this context.", "keywords": ["mental health", "stigma", "neural models", "dataset", "chatbot"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811.pdf", "abs": "https://arxiv.org/abs/2505.11811", "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios.", "AI": {"tldr": "This paper presents the BELLE framework for multi-hop question answering (QA), which tailors methods to different question types based on an agent-based debate system.", "motivation": "To improve multi-hop QA by analyzing question types and their sensitivity to specific methods, enhancing the effectiveness of existing large language model (LLM) techniques.", "method": "The BELLE framework employs two levels of agents that debate to create a comprehensive plan of operations for answering multi-hop questions, involving different types of reasoning strategies.", "result": "BELLE demonstrates superior performance compared to existing baselines across various datasets, with improved cost-effectiveness in handling complex question answering scenarios.", "conclusion": "The proposed BELLE framework offers a structured and effective approach to multi-hop QA by aligning methods with specific question types, resulting in enhanced performance.", "key_contributions": ["Introduction of the BELLE framework for multi-hop QA", "Analysis of the sensitivity of different QA methods to question types", "Demonstration of improved performance and cost-effectiveness over strong baselines"], "limitations": "", "keywords": ["Multi-hop Question Answering", "Large Language Models", "BELLE framework"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820.pdf", "abs": "https://arxiv.org/abs/2505.11820", "title": "Chain-of-Model Learning for Language Model", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.", "AI": {"tldr": "The paper introduces a new learning paradigm called Chain-of-Model (CoM) that enhances model training and deployment efficiency by using causal relationships in layer representations, leading to flexible model scaling and inference through the Chain-of-Language-Model (CoLM) framework.", "motivation": "To provide a more efficient and flexible framework for training and deploying language models by incorporating causal relationships into model structures.", "method": "The authors propose the Chain-of-Model (CoM) paradigm, where the hidden states of each layer are structured in a chain format, allowing for progressive scaling of model size by introducing multiple sub-representations, and develop the Chain-of-Language-Model (CoLM) that applies this concept within the Transformer architecture.", "result": "The CoLM family of models achieves performance comparable to standard Transformers while offering improved flexibility, such as progressive scaling, elastic inference, and various model sizes.", "conclusion": "The introduced CoM paradigm and CoLM models pave new paths for building language models with increased training efficiency and deployment adaptability.", "key_contributions": ["Introduction of the Chain-of-Model (CoM) paradigm for efficient model training and inference.", "Development of Chain-of-Language-Model (CoLM) for Transformer architectures incorporating this new paradigm.", "Implementation of a KV sharing mechanism in CoLM-Air for enhanced model extensibility."], "limitations": "", "keywords": ["Chain-of-Model", "Chain-of-Language-Model", "Transformer", "model scaling", "efficient inference"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827.pdf", "abs": "https://arxiv.org/abs/2505.11827", "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/.", "AI": {"tldr": "This paper introduces LongâShort, a framework for improving reasoning efficiency in large language models by differentiating between important and less important thoughts.", "motivation": "To enhance reasoning efficiency in large language models (LLMs) by compressing long chain-of-thoughts effectively instead of uniformly compressing all thoughts.", "method": "The authors investigate the effectiveness and efficiency of different thoughts in chain-of-thought reasoning, proposing a metric for these qualities and developing a framework where two LLMs collaborateâone focusing on long-thoughts and the other on short-thoughts, fine-tuned for their respective tasks.", "result": "The proposed method reduces token length by over 80% while achieving comparable performance to existing models across several benchmarks, including MATH500 and AIME24/25.", "conclusion": "The LongâShort framework effectively enhances reasoning efficiency, facilitating collaboration between models specialized in different thought generation styles.", "key_contributions": ["Introduction of a theoretically bounded metric for evaluating thought effectiveness and efficiency", "Development of the LongâShort collaborative LLM framework", "Demonstration of significant token reduction with maintained performance across benchmarks"], "limitations": "Limited to specific benchmarks and requires initial cold-start data for fine-tuning.", "keywords": ["chain-of-thought", "large language models", "reasoning efficiency", "reinforcement learning", "multi-turn collaboration"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010.pdf", "abs": "https://arxiv.org/abs/2505.13010", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "AI": {"tldr": "The paper presents a RoBERTa-based model for sentence-level media bias detection using the BABE dataset, demonstrating improvements over prior models while highlighting the importance of context in bias classification.", "motivation": "Media bias detection is vital for fair information dissemination, yet challenges arise from bias subjectivity and limited annotated data.", "method": "Fine-tuning a RoBERTa-based model on the BABE dataset and using statistical tests to evaluate performance against a DA-RoBERTa baseline.", "result": "The model shows statistically significant improvements in performance and exhibits good generalization and interpretability.", "conclusion": "The findings suggest potential for more robust and explainable NLP systems in media bias detection, with future work focusing on context-aware modeling and bias neutralization.", "key_contributions": ["Development of a RoBERTa-based model for bias classification", "Statistically significant performance improvements over existing models", "Pipeline integration with a bias-type classifier"], "limitations": "Constrained by sentence-level analysis and a lack of larger bias corpora.", "keywords": ["Media bias", "RoBERTa", "NLP", "Bias classification", "Machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829.pdf", "abs": "https://arxiv.org/abs/2505.11829", "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground.", "AI": {"tldr": "ClaD is a novel training paradigm for efficient detection of deviant and nuanced language, outperforming existing classifiers with fewer parameters.", "motivation": "Enhancing safety, clarity, and interpretation of online discourse by detecting nuanced language such as sexism, metaphors, and sarcasm.", "method": "ClaD introduces a loss function based on Mahalanobis distance and an interpretable decision algorithm for class separation, distilling small target classes from heterogeneous backgrounds.", "result": "ClaD achieves competitive performance on sexism, metaphor, and sarcasm detection tasks, comparable to larger language models but with significantly fewer parameters.", "conclusion": "ClaD is an efficient tool for pragmatic language understanding that addresses the challenges of class separation in language detection tasks.", "key_contributions": ["New training paradigm for language class detection", "Innovative loss function based on Mahalanobis distance", "Interpretable decision algorithm for improved class separation"], "limitations": "", "keywords": ["deviant language", "nuanced language", "classification", "Mahalanobis distance", "language models"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835.pdf", "abs": "https://arxiv.org/abs/2505.11835", "title": "Multilingual Collaborative Defense for Large Language Models", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD.", "AI": {"tldr": "The paper presents a novel Multilingual Collaborative Defense (MCD) method to enhance the safeguarding of large language models (LLMs) against multilingual jailbreak attempts.", "motivation": "There is a pressing need to improve the security of LLMs in multilingual contexts, particularly against attacks that exploit translations into rare languages.", "method": "The MCD approach automates the optimization of soft safety prompts for LLMs to enhance their multilingual safeguarding capabilities.", "result": "MCD shows improved safeguarding performance across multiple languages, maintains strong generalization capabilities, and reduces false refusal rates while addressing language safety misalignment.", "conclusion": "The MCD method significantly outperforms existing multilingual safeguarding strategies and demonstrates strong language transfer capabilities, making it a valuable contribution to LLM security research.", "key_contributions": ["Introduction of Multilingual Collaborative Defense (MCD) method for LLMs", "Demonstration of improved multilingual safety against jailbreak attempts", "Creation of multilingual benchmarks for evaluating safeguarding methods"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Security", "Collaborative Defense", "Jailbreaking", "Safety Prompts"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855.pdf", "abs": "https://arxiv.org/abs/2505.11855", "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "GonÃ§alo Paulo", "Youngjae Yu", "Stella Biderman"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification.", "AI": {"tldr": "This paper explores using large language models (LLMs) for automating the academic verification of scientific manuscripts through a dataset called SPOT, which identifies significant errors in published papers.", "motivation": "To assess the potential of LLMs as verifiers that can automate the academic verification process of scientific manuscripts, addressing the shortcomings of current AI-assisted verification methods.", "method": "The authors introduce SPOT, a dataset of 83 published papers and 91 significant errors, and evaluate various state-of-the-art LLMs on this dataset to measure their effectiveness in error detection.", "result": "None of the evaluated LLMs exceeded 21.1% recall or 6.1% precision, indicating a significant gap in reliability for academic verification tasks.", "conclusion": "The study underscores the limitations of current LLMs in accurately and reliably detecting errors in scientific manuscripts, revealing that the technology is not yet suited for dependable AI-assisted academic verification.", "key_contributions": ["Introduction of the SPOT dataset for academic verification", "Evaluation of state-of-the-art LLMs on error detection", "Insights into the limitations and misconceptions demonstrated by LLMs in this context"], "limitations": "The confidence estimates of the models are uniformly low, and there is a lack of consistency in error rediscovery across multiple runs.", "keywords": ["large language models", "academic verification", "scientific discovery", "SPOT dataset", "error detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876.pdf", "abs": "https://arxiv.org/abs/2505.11876", "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts.", "AI": {"tldr": "NAMET is a new method for efficiently editing knowledge in large language models by introducing noise during memory extraction.", "motivation": "Existing model editing techniques are ineffective in large-scale editing scenarios due to embedding collisions which reduce reliability.", "method": "NAMET (Noise-aware Model Editing in Transformers) introduces noise during memory extraction with a simple modification to a previous method (MEMIT).", "result": "Extensive experiments show that NAMET consistently outperforms existing editing methods across six LLMs and three datasets when editing thousands of facts.", "conclusion": "NAMET provides a reliable solution for large-scale knowledge editing in language models.", "key_contributions": ["Introduction of noise during memory extraction to improve editing reliability", "Demonstration of consistent performance across various LLMs", "Successful application of NAMET in large-scale editing scenarios"], "limitations": "", "keywords": ["Model Editing", "Large Language Models", "Knowledge Management"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887.pdf", "abs": "https://arxiv.org/abs/2505.11887", "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "AI": {"tldr": "AutoMedEval is an open-sourced automatic evaluation model for assessing the proficiency of medical LLMs, employing advanced training methods to reduce reliance on human evaluations.", "motivation": "The need for improved evaluation techniques for large language models in the medical domain, addressing the shortcomings of traditional metrics and the cost of human evaluations.", "method": "A hierarchical training method involving curriculum instruction tuning and iterative knowledge introspection to enhance evaluation capabilities with limited instructional data.", "result": "AutoMedEval outperforms traditional evaluation baselines, showing a stronger correlation with human judgments during assessments of medical LLM responses.", "conclusion": "AutoMedEval effectively reduces the dependence on human evaluation by providing a reliable automatic assessment model for medical LLMs.", "key_contributions": ["Introduction of an open-sourced evaluation model for medical LLMs.", "Development of a hierarchical training method for effective evaluation.", "Demonstrated improvements in correlation with human evaluations over traditional methods."], "limitations": "", "keywords": ["medical LLM evaluation", "AutoMedEval", "hierarchical training"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2306.11766", "pdf": "https://arxiv.org/pdf/2306.11766.pdf", "abs": "https://arxiv.org/abs/2306.11766", "title": "Agreeing and Disagreeing in Collaborative Knowledge Graph Construction: An Analysis of Wikidata", "authors": ["Elisavet Koutsiana", "Tushita Yadav", "Nitisha Jain", "Albert MeroÃ±o-PeÃ±uela", "Elena Simperl"], "categories": ["cs.HC"], "comment": "The paper has been accepted at the Journal of Web Semantics", "summary": "In this work, we study disagreements in discussions around Wikidata, an\nonline knowledge community that builds the data backend of Wikipedia.\nDiscussions are essential in collaborative work as they can increase\ncontributor performance and encourage the emergence of shared norms and\npractices. While disagreements can play a productive role in discussions, they\ncan also lead to conflicts and controversies, which impact contributor'\nwell-being and their motivation to engage. We want to understand if and when\nsuch phenomena arise in Wikidata, using a mix of quantitative and qualitative\nanalyses to identify the types of topics people disagree about, the most common\npatterns of interaction, and roles people play when arguing for or against an\nissue. We find that decisions to create Wikidata properties are much faster\nthan those to delete properties and that more than half of controversial\ndiscussions do not lead to consensus. Our analysis suggests that Wikidata is an\ninclusive community, considering different opinions when making decisions, and\nthat conflict and vandalism are rare in discussions. At the same time, while\none-fourth of the editors participating in controversial discussions contribute\nlegitimate and insightful opinions about Wikidata's emerging issues, they\nrespond with one or two posts and do not remain engaged in the discussions to\nreach consensus. Our work contributes to the analysis of collaborative KG\nconstruction with insights about communication and decision-making in projects,\nas well as with methodological directions and open datasets. We hope our\nfindings will help managers and designers support community decision-making and\nimprove discussion tools and practices.", "AI": {"tldr": "The paper analyzes disagreements in Wikidata discussions, highlighting patterns of interaction and decision-making in a collaborative environment.", "motivation": "To understand the dynamics of disagreements in discussions on Wikidata, which is crucial for contributor performance and community health.", "method": "The study employs both quantitative and qualitative analyses to examine topics of disagreement, interaction patterns, and participant roles in Wikidata discussions.", "result": "Findings include that property creation decisions happen faster than deletions, over half of controversial discussions do not reach consensus, and the community is generally inclusive in its decision-making.", "conclusion": "Wikidata's discussions are mostly constructive, but many contributors withdraw from discussions before reaching consensus. The insights aim to guide improvements in community discussions and decision-making tools.", "key_contributions": ["Identified patterns of disagreement in a collaborative knowledge community", "Highlighted the fast-paced decision-making for property creation vs. deletion", "Provided methodological directions and open datasets for further research"], "limitations": "The study focuses solely on Wikidata and may not generalize to other collaborative platforms.", "keywords": ["Wikidata", "disagreement", "collaborative knowledge construction", "decision-making", "community engagement"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891.pdf", "abs": "https://arxiv.org/abs/2505.11891", "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2.", "AI": {"tldr": "The paper introduces Mobile-Bench-v2, a benchmark for VLM-based mobile agents, addressing limitations of existing benchmarks by incorporating multi-path evaluations and realistic noise and ambiguity scenarios.", "motivation": "To improve the evaluation of VLM-based mobile agents by addressing the limitations of existing online and offline benchmarks.", "method": "Developed Mobile-Bench-v2, which includes multi-path evaluations, noisy and contaminated environments, and ambiguous instruction scenarios to better assess mobile agent performance.", "result": "Tested the Mobile-Bench-v2 with multiple agents including AppAgent-v1 and Mobile-Agent-v2, highlighting improved evaluation capabilities under real-world conditions.", "conclusion": "The Mobile-Bench-v2 benchmark provides a comprehensive framework for evaluating the abilities of mobile agents to interact in dynamic and noisy environments, thus enhancing future assessments.", "key_contributions": ["Introduction of Mobile-Bench-v2 benchmark", "Realistic noise and ambiguity scenarios for evaluation", "Multi-path evaluation methodology"], "limitations": "Focuses on specific environments and may not generalize to all mobile application scenarios.", "keywords": ["Mobile-Bench-v2", "VLM-based agents", "HCI", "benchmarking", "multi-path evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2308.10260", "pdf": "https://arxiv.org/pdf/2308.10260.pdf", "abs": "https://arxiv.org/abs/2308.10260", "title": "Hand Dominance and Congruence for Wrist-worn Haptics using Custom Voice-Coil Actuation", "authors": ["Ayoade Adeyemi", "Umit Sen", "Samet Mert Ercan", "Mine Sarac"], "categories": ["cs.HC", "cs.RO"], "comment": "6 pages, 7 figures", "summary": "During virtual interactions, rendering haptic feedback on a remote location\n(like the wrist) instead of the fingertips freeing users' hands from mechanical\ndevices. This allows for real interactions while still providing information\nregarding the mechanical properties of virtual objects. In this paper, we\npresent CoWrHap -- a novel wrist-worn haptic device with custom-made voice coil\nactuation to render force feedback. Then, we investigate the impact of asking\nparticipants to use their dominant or non-dominant hand for virtual\ninteractions and the best mapping between the active hand and the wrist\nreceiving the haptic feedback, which can be defined as hand-wrist congruence\nthrough a user experiment based on a stiffness discrimination task. Our results\nshow that participants performed the tasks (i) better with non-congruent\nmapping but reported better experiences with congruent mapping, and (ii) with\nno statistical difference in terms of hand dominance but reported better user\nexperience and enjoyment using their dominant hands. This study indicates that\nparticipants can perceive mechanical properties via haptic feedback provided\nthrough CoWrHap.", "AI": {"tldr": "This paper presents CoWrHap, a wrist-worn haptic device that provides force feedback to enhance virtual interactions, focusing on hand-wrist congruence and user experience.", "motivation": "To explore the use of haptic feedback on the wrist for virtual interactions, alleviating the need for mechanical devices and investigating its impact on user experience and performance.", "method": "A user experiment was conducted to assess stiffness discrimination tasks while varying hand dominance and the mapping between the active hand and the wrist receiving haptic feedback.", "result": "Participants performed tasks better with non-congruent mapping, although they reported better experiences with congruent mapping. Hand dominance did not significantly affect performance, but dominant hands led to better user experiences.", "conclusion": "CoWrHap allows users to perceive mechanical properties through wrist-based haptic feedback, indicating potential improvements in user experience and interaction within virtual environments.", "key_contributions": ["Introduction of CoWrHap, a new wrist-worn haptic device", "Investigation of hand-wrist congruence in virtual interactions", "Insights on user experience relating to hand dominance in haptic feedback"], "limitations": "", "keywords": ["haptic feedback", "virtual interactions", "CoWrHap", "user experience", "hand dominance"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893.pdf", "abs": "https://arxiv.org/abs/2505.11893", "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness.", "AI": {"tldr": "Introducing RLAP, a Reinforcement Learning enhanced Adaptive Planning framework to optimize multi-step NLP tasks with LLMs.", "motivation": "Existing multi-step planning methods for LLMs overlook linguistic features and rely on intrinsic planning capabilities, leading to suboptimal outcomes.", "method": "The paper proposes modeling NLP tasks as Markov decision processes (MDPs) using a lightweight Actor model to estimate Q-values, which interacts with the LLM to determine the optimal order of subtasks.", "result": "RLAP is applied to three different types of NLP tasks and demonstrates effectiveness and robustness across multiple datasets.", "conclusion": "The proposed RLAP framework improves the performance of LLMs on multi-step NLP tasks by considering linguistic features and optimizing subtask sequencing.", "key_contributions": ["Development of the RLAP framework for multi-step NLP tasks", "Integration of reinforcement learning with LLMs to enhance planning", "Extensive experiments validating the effectiveness of RLAP on various datasets"], "limitations": "", "keywords": ["Reinforcement Learning", "Multi-step Planning", "Large Language Models", "Natural Language Processing", "Adaptive Planning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2309.12555", "pdf": "https://arxiv.org/pdf/2309.12555.pdf", "abs": "https://arxiv.org/abs/2309.12555", "title": "PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent", "authors": ["Donghoon Shin", "Gary Hsieh", "Young-Ho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages including reference. Accepted to ACM CUI 2025", "summary": "Creating personalized and actionable exercise plans often requires iteration\nwith experts, which can be costly and inaccessible to many individuals. This\nwork explores the capabilities of Large Language Models (LLMs) in addressing\nthese challenges. We present PlanFitting, an LLM-driven conversational agent\nthat assists users in creating and refining personalized weekly exercise plans.\nBy engaging users in free-form conversations, PlanFitting helps elicit users'\ngoals, availabilities, and potential obstacles, and enables individuals to\ngenerate personalized exercise plans aligned with established exercise\nguidelines. Our study -- involving a user study, intrinsic evaluation, and\nexpert evaluation -- demonstrated PlanFitting's ability to guide users to\ncreate tailored, actionable, and evidence-based plans. We discuss future design\nopportunities for LLM-driven conversational agents to create plans that better\ncomply with exercise principles and accommodate personal constraints.", "AI": {"tldr": "This paper presents PlanFitting, an LLM-driven conversational agent for creating personalized exercise plans through user engagement and feedback.", "motivation": "The need for accessible and cost-effective personalized exercise plans that typically require expert iteration.", "method": "Development of an LLM-driven conversational agent that interacts with users to gather information and generate tailored exercise plans.", "result": "PlanFitting demonstrates the ability to create personalized, actionable, and evidence-based exercise plans through user engagement and various evaluations.", "conclusion": "LLM-driven agents like PlanFitting can significantly enhance the personalization of exercise plans and address individual constraints.", "key_contributions": ["Introduction of PlanFitting as a tool for personalized exercise planning using LLMs", "Demonstration of user-focused interaction to elicit information for plan refinement", "Evaluation of the effectiveness of the generated plans through user and expert studies."], "limitations": "The study may not account for all individual variations in exercise needs and capabilities.", "keywords": ["Large Language Models", "Exercise Plans", "Conversational Agents", "Personalization", "User Studies"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900.pdf", "abs": "https://arxiv.org/abs/2505.11900", "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "authors": ["Philipp Christmann", "Gerhard Weikum"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds.", "AI": {"tldr": "ReQAP is a novel method for question answering that integrates structured and unstructured personal data sources by creating executable operator trees from user queries.", "motivation": "To provide convenient access to mixed personal data while maintaining data on user devices.", "method": "ReQAP utilizes recursive decomposition to form an operator tree for a given question, enabling the integration of diverse data types.", "result": "The execution of the operator tree produces a traceable answer to the user's question, allowing for both simple look-ups and complex analytical inquiries.", "conclusion": "ReQAP facilitates effective querying of personal information and offers a new benchmark, PerQA, for evaluating persona-based data retrieval methods.", "key_contributions": ["Introduces ReQAP for recursive decomposition of questions into operator trees.", "Enables integration of structured and unstructured personal data sources.", "Releases the PerQA benchmark for diverse user information needs."], "limitations": "", "keywords": ["question answering", "personal information", "operator trees", "mixed sources", "PerQA benchmark"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2310.09617", "pdf": "https://arxiv.org/pdf/2310.09617.pdf", "abs": "https://arxiv.org/abs/2310.09617", "title": "How Good is ChatGPT in Giving Advice on Your Visualization Design?", "authors": ["Nam Wook Kim", "Yongsu Ahn", "Grace Myers", "Benjamin Bach"], "categories": ["cs.HC"], "comment": "34 pages, 4 figures", "summary": "Data visualization creators often lack formal training, resulting in a\nknowledge gap in design practice. Large language models such as ChatGPT, with\ntheir vast internet-scale training data, offer transformative potential to\naddress this gap. In this study, we used both qualitative and quantitative\nmethods to investigate how well ChatGPT can address visualization design\nquestions. First, we quantitatively compared the ChatGPT-generated responses\nwith anonymous online Human replies to data visualization questions on the\nVisGuides user forum. Next, we conducted a qualitative user study examining the\nreactions and attitudes of practitioners toward ChatGPT as a visualization\ndesign assistant. Participants were asked to bring their visualizations and\ndesign questions and received feedback from both Human experts and ChatGPT in\nrandomized order. Our findings from both studies underscore ChatGPT's\nstrengths, particularly its ability to rapidly generate diverse design options,\nwhile also highlighting areas for improvement, such as nuanced contextual\nunderstanding and fluid interaction dynamics beyond the chat interface. Drawing\non these insights, we discuss design considerations for future LLM-based design\nfeedback systems.", "AI": {"tldr": "This study examines the effectiveness of ChatGPT as a tool for data visualization design, comparing its responses to human experts and exploring user attitudes toward its assistance.", "motivation": "To address the knowledge gap in data visualization design practices due to a lack of formal training among creators.", "method": "A mixed-methods approach was used, including quantitative comparisons of ChatGPT-generated responses with human replies in a user forum and a qualitative user study evaluating practitioners' interactions with ChatGPT.", "result": "ChatGPT demonstrated strengths in generating diverse design options quickly, but also had limitations in contextual understanding and interaction fluidity.", "conclusion": "The study highlights both the potential and areas for improvement for LLMs in providing design feedback, suggesting future design considerations for LLM-based tools.", "key_contributions": ["Quantitative comparison of ChatGPT and human responses in data visualization questions.", "Qualitative examination of user attitudes towards ChatGPT in design assistance.", "Identification of strengths and limitations in using LLMs for design feedback."], "limitations": "Need for improved contextual understanding and interaction dynamics beyond a chat interface.", "keywords": ["Data visualization", "ChatGPT", "LLM", "Design feedback", "User study"], "importance_score": 8, "read_time_minutes": 34}}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908.pdf", "abs": "https://arxiv.org/abs/2505.11908", "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude.", "AI": {"tldr": "This paper presents an embedding-free retrieval framework that enhances the retrieval capabilities of LLMs through iterative search space refinement, eliminating the need for explicit graph construction and significantly improving performance on long-context QA tasks.", "motivation": "To address the limitations of existing Retrieval-Augmented Generation (RAG) systems which struggle with long-term context retention and often retrieve semantically mismatched content.", "method": "The proposed framework utilizes the logical inferencing abilities of LLMs to refine the search space iteratively and extend retrieval results with logically related information.", "result": "The experiments conducted on long-context QA benchmarks demonstrate that the new approach outperforms strong existing baselines, while reducing both storage and runtime requirements significantly.", "conclusion": "The embedding-free method represents a significant advancement in retrieval techniques, enabling better performance for LLMs in multi-turn and document-level tasks without the drawbacks of traditional graph-based retrieval methods.", "key_contributions": ["Introduction of an embedding-free retrieval framework for LLMs", "Iterative search space refinement based on logical inference", "Significant reduction in storage and runtime compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "Retrieval-Augmented Generation", "long-context QA"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.07475", "pdf": "https://arxiv.org/pdf/2405.07475.pdf", "abs": "https://arxiv.org/abs/2405.07475", "title": "Design Opportunities for Explainable AI Paraphrasing Tools: A User Study with Non-native English Speakers", "authors": ["Yewon Kim", "Thanh-Long V. Le", "Donghwi Kim", "Mina Lee", "Sung-Ju Lee"], "categories": ["cs.HC"], "comment": "Published as a conference paper at DIS 2025", "summary": "We investigate how non-native English speakers (NNESs) interact with diverse\ninformation aids to assess and select AI-generated paraphrases. We develop\nParaScope, an AI paraphrasing assistant that integrates diverse information\naids, such as back-translation, explanations, and usage examples, and logs user\ninteraction data. Our in-lab study with 22 NNESs reveals that user preferences\nfor information aids vary by language proficiency, with workflows progressing\nfrom global to more detailed information. While back-translation was the most\nfrequently used aid, it was not a decisive factor in suggestion acceptance;\nusers combined multiple information aids to make informed decisions. Our\nfindings demonstrate the potential of explainable AI paraphrasing tools to\nenhance NNESs' confidence, autonomy, and writing efficiency, while also\nemphasizing the importance of thoughtful design to prevent information\noverload. Based on these findings, we offer design implications for explainable\nAI paraphrasing tools that support NNESs in making informed decisions when\nusing AI writing systems.", "AI": {"tldr": "The paper explores how non-native English speakers use AI-generated paraphrase tools and identifies their preferences for information aids.", "motivation": "To understand how non-native English speakers interact with AI paraphrasing tools and the factors influencing their decision-making.", "method": "An in-lab study was conducted with 22 non-native English speakers, analyzing their interactions with an AI paraphrasing assistant, ParaScope, which includes various information aids.", "result": "The study found that user preferences for information aids differ based on language proficiency, and that while back-translation was commonly used, users often relied on multiple aids to make informed choices.", "conclusion": "The research highlights the value of explainable AI paraphrasing tools for enhancing non-native speakers' confidence and writing efficiency, while also suggesting design considerations to mitigate information overload.", "key_contributions": ["Development of ParaScope, an AI paraphrasing assistant with diverse information aids", "Insights into NNESs' preferences for information aids based on language proficiency", "Design implications for creating user-friendly AI writing systems for NNESs"], "limitations": "Limited sample size of 22 participants may not represent the broader NNES population.", "keywords": ["AI paraphrasing", "non-native speakers", "explainable AI", "user interaction", "writing efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922.pdf", "abs": "https://arxiv.org/abs/2505.11922", "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency.", "AI": {"tldr": "MISO enhances complex instruction-following in LLMs by transforming sequential input into parallel instructions with subcontexts.", "motivation": "To improve large language models' (LLMs) abilities in following complex instructions that contain multiple constraints.", "method": "We propose MISO (Multi-Input Single-Output), an extension of decoder-only transformer models, which introduces a mixture-of-contexts approach to enhance supervised fine-tuning (SFT).", "result": "MISO fine-tuning shows superior performance in complex instruction-following tasks and improves training efficiency compared to traditional SFT approaches.", "conclusion": "MISO effectively enhances LLMs' capability in handling complex instructions by accommodating multiple sub-contexts, leading to better instruction-output alignment.", "key_contributions": ["Introduction of MISO for enhanced instruction-following in LLMs", "Use of mixture-of-contexts for improving SFT effectiveness", "Demonstrated improvements in training efficiency and performance on complex tasks"], "limitations": "", "keywords": ["large language models", "instruction following", "supervised fine-tuning", "MISO", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2407.07732", "pdf": "https://arxiv.org/pdf/2407.07732.pdf", "abs": "https://arxiv.org/abs/2407.07732", "title": "Text2VP: Generative AI for Visual Programming and Parametric Modeling", "authors": ["Guangxi Feng", "Wei Yan"], "categories": ["cs.HC", "cs.AI"], "comment": "Demonstration Video:\n  https://www.youtube.com/playlist?list=PLUOmOLuLSaDWss2En2buixBxvTPy-lDvA", "summary": "The integration of generative artificial intelligence (AI) into architectural\ndesign has advanced significantly, enabling the generation of text, images, and\n3D models. However, prior AI applications lack support for text-to-parametric\nmodels, essential for generating and optimizing diverse parametric design\noptions. This study introduces Text-to-Visual Programming (Text2VP) GPT, a\nnovel generative AI derived from GPT-4.1, designed to automate graph-based\nvisual programming workflows, parameters, and their interconnections. Text2VP\nleverages detailed documentation, specific instructions, and example-driven\nfew-shot learning to reflect user intentions accurately and facilitate\ninteractive parameter adjustments. Testing demonstrates Text2VP's capability in\ngenerating functional parametric models, although higher complexity models\npresent increased error rates. This research highlights generative AI's\npotential in visual programming and parametric modeling, laying groundwork for\nfuture improvements to manage complex modeling tasks. Ultimately, Text2VP aims\nto enable designers to easily create and modify parametric models without\nextensive training in specialized platforms like Grasshopper.", "AI": {"tldr": "This study presents Text-to-Visual Programming (Text2VP) GPT, a generative AI framework designed for automating the creation of parametric models through natural language instructions, facilitating easier design modifications.", "motivation": "The study aims to enhance architectural design by integrating generative AI that can generate and optimize parametric design options, filling a gap in existing AI applications.", "method": "Text2VP GPT utilizes detailed documentation, specific instructions, and example-based few-shot learning to automate graph-based visual programming workflows, enabling user-driven parameter adjustments.", "result": "Text2VP was tested successfully for generating functional parametric models, though it faced higher error rates with more complex models.", "conclusion": "This research showcases the promise of generative AI in visual programming, establishing a foundation for advancements in managing complex modeling tasks.", "key_contributions": ["Introduction of Text2VP GPT for automating parameterized design", "Enhanced interaction through natural language instructions", "Potential reduction of training time needed for complex design modeling"], "limitations": "Higher complexity models demonstrate increased error rates, indicating limitations in the current framework's robustness.", "keywords": ["Generative AI", "Visual Programming", "Parametric Design", "Text-to-Parametric Models", "Architecture"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924.pdf", "abs": "https://arxiv.org/abs/2505.11924", "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available.", "AI": {"tldr": "This paper explains the performance gains of intrinsic self-correction in language models, focusing on how prompting influences hidden states and output distributions, and provides a mathematical basis for these phenomena.", "motivation": "To understand the mechanisms behind intrinsic self-correction in language models and how prompting can enhance their performance.", "method": "The paper presents a mathematical formulation of self-correction and derives a concentration result based on alignment magnitudes. Experiments were conducted using zephyr-7b-sft to analyze text detoxification.", "result": "Significant differences were observed in the alignment of prompt-induced shifts between toxic and non-toxic tokens, demonstrating how self-correction improves latent concept recognition in language models.", "conclusion": "The findings provide insights into how prompting enhances the language model's ability for concept recognition, explaining the mechanisms behind self-correction.", "key_contributions": ["Mathematical formulation of self-correction in language models.", "Experimental results on text detoxification showing the effect of prompting.", "Insights into the latent concept recognition abilities of language models through self-correction."], "limitations": "", "keywords": ["language models", "self-correction", "prompting", "text detoxification", "latent concept recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.02803", "pdf": "https://arxiv.org/pdf/2408.02803.pdf", "abs": "https://arxiv.org/abs/2408.02803", "title": "SiCo: An Interactive Size-Controllable Virtual Try-On Approach for Informed Decision-Making", "authors": ["Sherry X. Chen", "Alex Christopher Lim", "Yimeng Liu", "Pradeep Sen", "Misha Sra"], "categories": ["cs.HC", "cs.CV", "H.5.2; I.4.9"], "comment": null, "summary": "Virtual try-on (VTO) applications aim to replicate the in-store shopping\nexperience and enhance online shopping by enabling users to interact with\ngarments. However, many existing tools adopt a one-size-fits-all approach when\nvisualizing clothing items. This approach limits user interaction with\ngarments, particularly regarding size and fit adjustments, and fails to provide\ndirect insights for size recommendations. As a result, these limitations\ncontribute to high return rates in online shopping. To address this, we\nintroduce SiCo, a new online VTO system that allows users to upload images of\nthemselves and interact with garments by visualizing how different sizes would\nfit their bodies. Our user study demonstrates that our approach significantly\nimproves users' ability to assess how outfits will appear on their bodies and\nincreases their confidence in selecting clothing sizes that align with their\npreferences. Based on our evaluation, we believe that SiCo has the potential to\nreduce return rates and transform the online clothing shopping experience.", "AI": {"tldr": "Introducing SiCo, a virtual try-on system that enhances online shopping by enabling users to visualize garment fitting by uploading images of themselves.", "motivation": "Current virtual try-on tools often use a one-size-fits-all approach which limits user interaction and fails to provide accurate size recommendations, contributing to high return rates in online shopping.", "method": "SiCo allows users to upload personal images and interactively visualize how different clothing sizes would fit their bodies through a user-centric interface.", "result": "User studies demonstrate significant improvements in users' ability to assess how outfits will look on their bodies and increased confidence in selecting the right clothing sizes.", "conclusion": "SiCo could substantially reduce return rates and revolutionize the online clothing shopping experience.", "key_contributions": ["Development of an interactive virtual try-on system (SiCo).", "User-centered design approach enhancing size visualization and fitting.", "Empirical study showing improved size selection confidence among users."], "limitations": "", "keywords": ["virtual try-on", "e-commerce", "size recommendation", "user interaction", "computer vision"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932.pdf", "abs": "https://arxiv.org/abs/2505.11932", "title": "Neuro-Symbolic Query Compiler", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries.", "AI": {"tldr": "This paper introduces QCompiler, a neuro-symbolic framework for enhancing search intent recognition in Retrieval-Augmented Generation (RAG) systems by utilizing a minimal BNF grammar for complex queries.", "motivation": "The need for precise recognition of search intent in RAG systems, especially for complex and resource-constrained queries.", "method": "QCompiler uses a neuro-symbolic approach that includes a Query Expression Translator, Lexical Syntax Parser, and Recursive Descent Processor to formalize queries into Abstract Syntax Trees (ASTs) using a designed BNF grammar.", "result": "The framework improves the RAG system's ability to handle complex queries, leading to more precise document retrieval and response generation.", "conclusion": "QCompiler effectively bridges the gap in recognizing complex search intents, ensuring system completeness while reducing redundancy.", "key_contributions": ["Introduction of a minimal BNF grammar for formalizing complex queries", "Development of a neuro-symbolic framework for search intent recognition in RAG systems", "Compilation of queries into Abstract Syntax Trees (ASTs) for enhanced execution accuracy."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "neuro-symbolic framework", "search intent recognition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935.pdf", "abs": "https://arxiv.org/abs/2505.11935", "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit.", "AI": {"tldr": "This paper introduces ChartEdit, a benchmark for evaluating the performance of multimodal large language models (MLLMs) in chart editing tasks, revealing significant limitations in their capabilities.", "motivation": "Current multimodal models struggle with chart editing, an area requiring complex reasoning and precise intent interpretation, highlighting the need for better evaluation methods.", "method": "The authors created ChartEdit, a benchmark with 1,405 diverse editing instructions for 233 real-world charts, validated through manual annotation. They evaluated 10 MLLMs using this benchmark in both code and chart generation experiments.", "result": "The evaluation showed that while large-scale models can generate code for charts that somewhat resemble reference images, their accuracy in performing specified edits is limited, with the best model scoring only 59.96.", "conclusion": "The findings indicate substantial challenges in MLLM performance for chart editing tasks, particularly highlighting the need for improvements in model capabilities and further benchmarking work.", "key_contributions": ["Introduction of the ChartEdit benchmark for comprehensive evaluation of chart editing by MLLMs", "Demonstration of the performance gaps in editing capabilities of various MLLMs", "Provision of a valuable resource for future research in multimodal model evaluation"], "limitations": "Benchmark relies on manual assessments which may introduce subjective biases; the evaluation is focused on specific models and may not generalize to all MLLMs.", "keywords": ["multimodal large language models", "chart editing", "evaluation framework", "ChartEdit benchmark", "MLLM performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.02084", "pdf": "https://arxiv.org/pdf/2501.02084.pdf", "abs": "https://arxiv.org/abs/2501.02084", "title": "Simulated prosthetic vision confirms checkerboard as an effective raster pattern for epiretinal implants", "authors": ["Justin M. Kasowski", "Apurv Varshney", "Roksana Sadeghi", "Michael Beyeler"], "categories": ["cs.HC"], "comment": null, "summary": "Spatial scheduling of electrode activation (\"rastering\") is essential for\nsafely operating high-density retinal implants, yet its perceptual consequences\nremain poorly understood. This study systematically evaluates the impact of\nraster patterns, or spatial arrangements of sequential electrode activation, on\nperformance and perceived difficulty in simulated prosthetic vision (SPV). By\naddressing this gap, we aimed to identify patterns that optimize functional\nvision in retinal implants. Sighted participants completed letter recognition\nand motion discrimination tasks under four raster patterns (horizontal,\nvertical, checkerboard, and random) using an immersive SPV system. The\nsimulations emulated epiretinal implant perception and employed\npsychophysically validated models of electrode activation, phosphene\nappearance, nonlinear spatial summation, and temporal dynamics, ensuring\nrealistic representation of prosthetic vision. Performance accuracy and\nself-reported difficulty were analyzed to assess the effects of raster\npatterning. The checkerboard pattern consistently outperformed other raster\npatterns, yielding significantly higher accuracy and lower difficulty ratings\nacross both tasks. The horizontal and vertical patterns introduced biases\naligned with apparent motion artifacts, while the checkerboard minimized such\neffects. Random patterns resulted in the lowest performance, underscoring the\nimportance of structured activation. Notably, checkerboard matched performance\nin the \"No Raster\" condition, despite conforming to groupwise safety\nconstraints. This is the first quantitative, task-based evaluation of raster\npatterns in SPV. Checkerboard-style scheduling enhances perceptual clarity\nwithout increasing computational load, offering a low-overhead, clinically\nrelevant strategy for improving usability in next-generation retinal\nprostheses.", "AI": {"tldr": "This study evaluates the impact of different spatial arrangements of electrode activation (raster patterns) on performance in simulated prosthetic vision, concluding that a checkerboard pattern enhances accuracy and perceived ease of use.", "motivation": "To understand how different raster patterns affect functional vision in high-density retinal implants and to identify optimal patterns for improved performance.", "method": "Participants completed letter recognition and motion discrimination tasks under four raster patterns (horizontal, vertical, checkerboard, and random) using an immersive simulated prosthetic vision system, ensuring realistic representation of epiretinal implant perception.", "result": "The checkerboard raster pattern outperformed other patterns, leading to higher accuracy and lower perceived difficulty for tasks, while random patterns yielded the poorest performance.", "conclusion": "Checkerboard rastering enhances perceptual clarity in simulated prosthetic vision without increasing computational load, making it a viable strategy for retinal prostheses.", "key_contributions": ["First quantitative evaluation of raster patterns in simulated prosthetic vision", "Identified checkerboard pattern as the most effective for performance and perceived ease", "Demonstrated structured activation reduces apparent motion artifacts"], "limitations": "", "keywords": ["retinal implants", "spatial scheduling", "rastering", "prosthetic vision", "checkerboard pattern"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958.pdf", "abs": "https://arxiv.org/abs/2505.11958", "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems.", "AI": {"tldr": "HiPPrO is a novel framework for generating more effective counterspeech by using a hierarchical approach to incorporate multiple attributes simultaneously, achieving significant improvements in intent conformity and generation metrics.", "motivation": "To enhance counterspeech generation effectiveness by considering multiple attributes rather than single intents, addressing the limitations of existing models.", "method": "Introducing HiPPrO, a two-stage framework that applies hierarchical prefix learning optimized for dual attributes during the counterspeech generation process.", "result": "HiPPrO shows a ~38% improvement in intent conformity and improvements in Rouge metrics, outperforming several baseline models.", "conclusion": "Multi-attribute conditioning can significantly advance the generation capabilities of counterspeech systems and produce more appropriate responses.", "key_contributions": ["Introduction of HiPPrO framework for multi-attribute counterspeech generation", "Hierarchical prefix learning with preference optimization", "Extensive human evaluation demonstrating improved relevance of generated outputs"], "limitations": "", "keywords": ["counterspeech", "multi-attribute", "preference optimization", "HCI", "NLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.07586", "pdf": "https://arxiv.org/pdf/2503.07586.pdf", "abs": "https://arxiv.org/abs/2503.07586", "title": "Design for Hope: Cultivating Deliberate Hope in the Face of Complex Societal Challenges", "authors": ["JaeWon Kim", "Jiaying \"Lizzy\" Liu", "Lindsay Popowski", "Cassidy Pyle", "Sowmya Somanath", "Hua Shen", "Casey Fiesler", "Gillian R. Hayes", "Alexis Hiniker", "Wendy Ju", "Florian \"Floyd\" Mueller", "Ahmer Arif", "Yasmine Kotturi"], "categories": ["cs.HC"], "comment": null, "summary": "Design has the potential to cultivate hope in the face of complex societal\nchallenges, especially those central to CSCW research. These challenges are\noften addressed through efforts aimed at harm reduction and prevention --\nessential but sometimes limiting approaches that can unintentionally narrow our\ncollective sense of what is possible. This one-day, in-person workshop builds\non the Positech Workshop at CSCW 2024 (https://positech-cscw-2024.github.io/)\nby offering practical ways to move beyond reactive problem-solving toward\nbuilding capacity for proactive goal setting and generating pathways forward.\nWe explore how collaborative and reflective design methodologies can help\nresearch communities navigate uncertainty, expand possibilities, and foster\nmeaningful change. By connecting design thinking with hope theory, which frames\nhope as the interplay of \"goal-directed,\" \"pathways,\" and \"agentic\" thinking,\nwe will examine how researchers might chart new directions in the face of\ncomplexity and constraint. Through hands-on activities including problem\nreframing, building a shared taxonomy of design methods that align with hope\ntheory, and reflecting on what it means to sustain hopeful research\ntrajectories, participants will develop strategies to embed a deliberately\nhopeful approach into their research.", "AI": {"tldr": "This workshop focuses on using design methodologies to cultivate hope and proactive goal setting in research communities, particularly in response to complex societal issues.", "motivation": "The paper addresses the need for alternative approaches to harm reduction and prevention in research, proposing a shift towards proactive goal setting through design.", "method": "The workshop involves hands-on activities that include problem reframing, creating a taxonomy of design methods aligned with hope theory, and reflections on sustaining research trajectories.", "result": "Participants will learn strategies to incorporate a hopeful approach into their research practices, enhancing their capacity to navigate uncertainty and generate meaningful change.", "conclusion": "By connecting design thinking with hope theory, researchers can open new pathways for addressing societal challenges and foster a more expansive view of what is possible.", "key_contributions": ["Integration of hope theory with design methodologies for research", "Hands-on activities promoting proactive goal setting", "Development of a shared taxonomy of design methods"], "limitations": "", "keywords": ["design", "hope theory", "collaborative design", "proactive problem solving", "research methodologies"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959.pdf", "abs": "https://arxiv.org/abs/2505.11959", "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech.", "AI": {"tldr": "This paper presents a bilingual dataset for Arabic and English focused on annotating emotions and hope speech, aimed at advancing NLP in underrepresented languages.", "motivation": "Address the scarcity of multi-emotion datasets by providing comprehensive annotations for emotions and hope speech in Arabic and English.", "method": "Created a bilingual dataset with 23,456 Arabic entries and 10,036 English entries, annotated for emotions and hope speech, utilizing Fleiss' Kappa for reliability.", "result": "Achieved a micro-F1-Score of 0.67 from a baseline machine learning model, indicating the quality of annotations.", "conclusion": "The dataset serves as a significant resource for enhancing natural language processing research in underrepresented languages, enabling better analysis of emotions and hope speech.", "key_contributions": ["Introduction of a bilingual dataset for Arabic and English on emotions and hope speech.", "Comprehensive annotations capturing emotion intensity, complexity, and causes.", "High inter-annotator agreement demonstrated by Fleiss' Kappa."], "limitations": "", "keywords": ["bilingual dataset", "emotions", "hope speech", "natural language processing", "annotations"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2504.07285", "pdf": "https://arxiv.org/pdf/2504.07285.pdf", "abs": "https://arxiv.org/abs/2504.07285", "title": "A Scalable Approach to Clustering Embedding Projections", "authors": ["Donghao Ren", "Fred Hohman", "Dominik Moritz"], "categories": ["cs.HC", "cs.LG"], "comment": "Code: https://github.com/apple/embedding-atlas", "summary": "Interactive visualization of embedding projections is a useful technique for\nunderstanding data and evaluating machine learning models. Labeling data within\nthese visualizations is critical for interpretation, as labels provide an\noverview of the projection and guide user navigation. However, most methods for\nproducing labels require clustering the points, which can be computationally\nexpensive as the number of points grows. In this paper, we describe an\nefficient clustering approach using kernel density estimation in the projected\n2D space instead of points. This algorithm can produce high-quality cluster\nregions from a 2D density map in a few hundred milliseconds, orders of\nmagnitude faster than current approaches. We contribute the design of the\nalgorithm, benchmarks, and applications that demonstrate the utility of the\nalgorithm, including labeling and summarization.", "AI": {"tldr": "The paper presents an efficient clustering method for labeling data in interactive visualizations of embedding projections using kernel density estimation, significantly reducing computation time.", "motivation": "The need for efficient labeling techniques in interactive visualizations of embedding projections to aid in data understanding and model evaluation.", "method": "The proposed method utilizes kernel density estimation in a projected 2D space to produce cluster regions from a density map, thus speeding up the labeling process.", "result": "The algorithm generates high-quality cluster regions within hundreds of milliseconds, significantly faster than existing clustering methods.", "conclusion": "This clustering approach enhances the efficiency of labeling in visualization tasks, providing a practical solution for interpretability in machine learning applications.", "key_contributions": ["Development of an efficient clustering algorithm using kernel density estimation", "Performance benchmarks demonstrating speed improvements", "Applications showcasing the utility in labeling and summarization tasks"], "limitations": "", "keywords": ["clustering", "kernel density estimation", "data visualization", "machine learning", "embedding projections"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965.pdf", "abs": "https://arxiv.org/abs/2505.11965", "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "authors": ["Xu Liu", "Guanyi Chen"], "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task.", "AI": {"tldr": "The CCNU team's system for the Mu-SHROOM shared task identifies hallucinations in multilingual question-answering systems using multiple LLMs.", "motivation": "To improve the identification of hallucinations in question-answering systems across various languages using a systematic approach.", "method": "The system employs multiple Large Language Models in parallel, leveraging their distinct areas of expertise to annotate hallucinations while simulating a crowdsourcing annotation process.", "result": "Achieved first place for Hindi and ranked in the top five for seven other languages in the shared task.", "conclusion": "The paper highlights both successful and unsuccessful approaches during the system development process and shares insights from the participation in the task.", "key_contributions": ["Use of multiple LLMs for annotation", "Simulation of crowdsourcing for hallucination identification", "Performance rankings in multilingual settings"], "limitations": "None specified.", "keywords": ["human-computer interaction", "machine learning", "large language models", "question-answering", "multi-language"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09438", "pdf": "https://arxiv.org/pdf/2504.09438.pdf", "abs": "https://arxiv.org/abs/2504.09438", "title": "Cartographers in Cubicles: How Training and Preferences of Mapmakers Interplay with Structures and Norms in Not-for-Profit Organizations", "authors": ["Arpit Narechania", "Alex Endert", "Clio Andris"], "categories": ["cs.HC"], "comment": "24 pages, 4 figures, 2 tables; to appear in ACM CSCW 2025", "summary": "Choropleth maps are a common and effective way to visualize geographic\nthematic data. Although cartographers have established many principles about\nmap design, data binning and color usage, less is known about how mapmakers\nmake individual decisions in practice. We interview 16 cartographers and\ngeographic information systems (GIS) experts from 13 government organizations,\nNGOs, and federal agencies about their choropleth mapmaking decisions and\nworkflows. We categorize our findings and report on how mapmakers follow\ncartographic guidelines and personal rules of thumb, collaborate with other\nstakeholders within and outside their organization, and how organizational\nstructures and norms are tied to decision-making during data preparation, data\nanalysis, data binning, map styling, and map post-processing. We find several\npoints of variation as well as regularity across mapmakers and organizations\nand present takeaways to inform cartographic education and practice, including\nbroader implications and opportunities for CSCW, HCI, and information\nvisualization researchers and practitioners.", "AI": {"tldr": "The paper explores the decision-making processes of cartographers in creating choropleth maps, highlighting their adherence to guidelines, collaborative practices, and organizational influences.", "motivation": "Understanding the practices of cartographers in choropleth mapmaking can enhance cartographic education and provide insights for HCI and information visualization fields.", "method": "Interviews with 16 cartographers and GIS experts from various organizations, analyzing their decision-making processes and workflows in map creation.", "result": "Identified variations and regularities in how mapmakers follow guidelines, collaborate, and make decisions related to data preparation and analysis.", "conclusion": "The findings can inform cartographic education and practice, emphasizing the implications for HCI and information visualization research.", "key_contributions": ["Insights into the decision-making processes of cartographers", "Impact of organizational structures on mapmaking decisions", "Guidelines for better cartographic education and practice"], "limitations": "", "keywords": ["Choropleth maps", "Cartography", "HCI", "Information visualization", "CSCW"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969.pdf", "abs": "https://arxiv.org/abs/2505.11969", "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786.", "AI": {"tldr": "This study introduces a multilabel hate speech dataset for Arabic, comprising 10,000 tweets annotated for offensive content and various hate speech targets, with evaluation using a transformer model.", "motivation": "To address the challenge of identifying hate speech in Arabic due to dialectal variations and improve offensive content detection.", "method": "Collected and annotated 10,000 Arabic tweets for offensive content and hate speech targets, measuring inter-annotator agreement and evaluating with transformer models.", "result": "The inter-annotator agreement scores were 0.86 for offensive content and 0.71 for hate speech targets, with AraBERTv2 achieving a micro-F1 score of 0.7865 and accuracy of 0.786.", "conclusion": "The dataset provides a critical resource for hate speech detection in Arabic and demonstrates the effectiveness of transformer models in this context.", "key_contributions": ["Introduction of a multilabel hate speech dataset for Arabic", "High inter-annotator agreement scores", "Evaluation of transformer models for hate speech detection"], "limitations": "The focus is on Twitter data, which may not represent all dialectal variations in Arabic.", "keywords": ["Hate Speech", "Arabic Language", "Dataset", "Annotation", "Transformers"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995.pdf", "abs": "https://arxiv.org/abs/2505.11995", "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains.", "AI": {"tldr": "This paper investigates how large language models (LLMs) integrate internal and external knowledge in retrieval-augmented generation (RAG), proposing a framework to enhance their interpretability and reliability.", "motivation": "To explore the underexamined mechanisms by which LLMs use parametric and retrieved knowledge in RAG scenarios, thereby enhancing their performance in knowledge-intensive tasks.", "method": "We utilized a knowledge stream analysis at both macroscopic and microscopic levels, decomposing the knowledge utilization process into four stages and introducing a novel method for neuron identification associated with different knowledge sources.", "result": "We found that the relevance of retrieved passages significantly impacts the knowledge utilization process, and identified the distinct roles of LLM components in knowledge integration.", "conclusion": "Our findings provide new insights into improving interpretability and reliability in retrieval-augmented language models, crucial for enhancing their application in complex knowledge tasks.", "key_contributions": ["Proposed a systematic framework for analyzing knowledge utilization in LLMs", "Introduced the knowledge activation probability entropy (KAPE) method for targeted neuron deactivation", "Decomposed the knowledge utilization into four distinct stages for better understanding of LLM mechanisms."], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "knowledge integration", "knowledge-intensive tasks", "neuron activation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028.pdf", "abs": "https://arxiv.org/abs/2505.12028", "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis.", "AI": {"tldr": "This paper proposes 14 fine-grained argument relation types to enhance argument mining, addressing limitations in current models. It includes experiments on argument detection, relation prediction, and essay grading.", "motivation": "The paper addresses the limitations of existing argument mining approaches, which fail to capture the complexity of argument structures, particularly in real-world contexts.", "method": "The authors propose 14 fine-grained relation types and conduct experiments on argument component detection, relation prediction, and automated essay grading.", "result": "The experiments demonstrate the significance of fine-grained argument annotations in assessing argumentative writing quality and reveal connections between discourse relations and argumentative features.", "conclusion": "The study emphasizes the necessity of multi-dimensional argument analysis to improve understanding and quality assessment of arguments in writing.", "key_contributions": ["Proposes 14 fine-grained argument relation types.", "Conducts extensive experiments across multiple tasks related to argument mining.", "Highlights the role of writing quality in argument component detection and relation prediction."], "limitations": "The study may rely on specific datasets which could limit generalizability.", "keywords": ["Argument Mining", "Large Language Models", "Argumentation Structure", "Natural Language Processing", "Essay Grading"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2310.10378", "pdf": "https://arxiv.org/pdf/2310.10378.pdf", "abs": "https://arxiv.org/abs/2310.10378", "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models", "authors": ["Jirui Qi", "Raquel FernÃ¡ndez", "Arianna Bisazza"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "EMNLP2023 Outstanding Paper (Multilinguality and Linguistic Diversity\n  Track). All code and data are released at\n  https://github.com/Betswish/Cross-Lingual-Consistency", "summary": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score.", "AI": {"tldr": "This paper investigates the cross-lingual consistency of factual knowledge in multilingual pretrained language models using a new metric called RankC.", "motivation": "To ensure users with different language backgrounds receive consistent feedback from the same multilingual models, which currently varies across languages.", "method": "The authors propose a Ranking-based Consistency (RankC) metric to assess the cross-lingual consistency of factual knowledge in multilingual PLMs. They analyze the factors affecting this consistency at both the model and language-pair levels.", "result": "The study finds that while larger models tend to have higher factual accuracy in many languages, this does not translate to improved cross-lingual consistency. Their case study shows that new factual knowledge only transfers to languages with high RankC scores related to English.", "conclusion": "The findings highlight the need for better understanding and improvement of cross-lingual consistency in multilingual PLMs despite increasing model sizes.", "key_contributions": ["Introduction of the RankC metric for assessing cross-lingual consistency in multilingual PLMs.", "Identification of the relationship between model size and factual probing accuracy vs. cross-lingual consistency.", "Insights into the transfer of new knowledge across languages based on RankC scores."], "limitations": "", "keywords": ["Multilingualism", "Cross-lingual consistency", "Pretrained Language Models", "Factual knowledge", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043.pdf", "abs": "https://arxiv.org/abs/2505.12043", "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach.", "AI": {"tldr": "The paper proposes a framework called Mixture of Losses (MoL) which enhances domain-specific training of large language models (LLMs) by decoupling optimization objectives for domain and general corpora, preventing performance degradation during domain adaptation.", "motivation": "To improve the performance of LLMs in domain-specific tasks and reduce hallucinations and accuracy limitations caused by domain-biased data and improper corpus-mixture ratios.", "method": "The proposed Mixture of Losses (MoL) framework utilizes cross-entropy loss for domain data and Kullback-Leibler divergence for general corpus, optimizing both without sacrificing model generalization.", "result": "The MoL framework achieved a 27.9% higher accuracy on the Math-500 benchmark and an 83.3% improvement on the AIME25 subset, significantly outperforming traditional CPT approaches.", "conclusion": "The dual-loss architecture is effective in balancing domain expertise and general language skills, avoiding catastrophic forgetting in LLMs during training.", "key_contributions": ["Introduction of the Mixture of Losses (MoL) framework for domain-specific training of LLMs.", "Demonstration of significant performance gains over traditional CPT methods.", "Establishment of an optimal 1:1 ratio for domain-to-general corpus training."], "limitations": "", "keywords": ["Large Language Models", "Domain Adaptation", "Mixture of Losses", "Cross-Entropy Loss", "Kullback-Leibler Divergence"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050.pdf", "abs": "https://arxiv.org/abs/2505.12050", "title": "ABoN: Adaptive Best-of-N Alignment", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows.", "AI": {"tldr": "The paper introduces a prompt-adaptive strategy for Best-of-N alignment in language models that optimizes inference-time compute by addressing alignment difficulty intelligently.", "motivation": "To improve the efficiency of test-time alignment methods in language models, particularly under latency constraints.", "method": "The proposed two-stage algorithm first estimates the reward distribution for prompts with a small exploratory budget and then adaptively allocates the remaining inference budget based on these estimates.", "result": "Empirical results show the adaptive strategy consistently outperforms uniform allocation even with the same inference budget, and it remains competitive with larger budgets and growing batch sizes.", "conclusion": "The method is practical, efficient, and compatible with various language model and reward model pairs, offering a significant improvement in compute allocation for prompt processing.", "key_contributions": ["Introduction of a two-stage adaptive algorithm for prompt alignment", "Empirical validation showing performance improvements over uniform allocation", "Compatibility with any LM/RM combination"], "limitations": "", "keywords": ["language models", "reward models", "Best-of-N sampling", "adaptive strategy", "inference budget"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054.pdf", "abs": "https://arxiv.org/abs/2505.12054", "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "authors": ["MatÃºÅ¡ Pikuliak"], "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring.", "AI": {"tldr": "GenderBench is a suite for evaluating gender biases in LLMs, measuring 19 harmful behaviors through 14 probes, and includes evaluations of 12 LLMs.", "motivation": "To improve the reproducibility and robustness of measuring gender biases in large language models (LLMs).", "method": "GenderBench includes 14 probes specifically designed to quantify 19 different gender-related harmful behaviors exhibited by LLMs and is released as an open-source library.", "result": "Evaluation of 12 LLMs reveals consistent patterns of bias, including struggles with stereotypical reasoning, equitable gender representation, and discriminatory behavior in high-stakes scenarios.", "conclusion": "The GenderBench suite serves as a significant tool for assessing and addressing gender biases in LLMs, aiming to enhance fairness in AI applications.", "key_contributions": ["Introduction of an evaluation suite specifically for gender biases in LLMs.", "Quantitative measurement of gender-related harmful behaviors in LLMs.", "Open-source release for community use and extension."], "limitations": "", "keywords": ["gender bias", "large language models", "evaluation suite"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060.pdf", "abs": "https://arxiv.org/abs/2505.12060", "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE.", "AI": {"tldr": "The paper introduces SAGE, a defense strategy for Large Language Models (LLMs) to improve their safety during generation, particularly against jailbreak attacks.", "motivation": "To address the safety gap in LLMs which detect but fail to generate safe responses to jailbreak prompts.", "method": "SAGE employs a Discriminative Analysis Module to evaluate prompts and a Discriminative Response Module to manage generation, aiming to enhance LLM safety across various architectures without needing retraining.", "result": "SAGE achieved an average 99% success rate in defending against sophisticated jailbreak methods while maintaining helpful output on standard benchmarks.", "conclusion": "This work contributes to the development of safer LLMs by aligning their detection capabilities with their response generation, enhancing overall safety awareness.", "key_contributions": ["Introduction of SAGE, a novel training-free defense strategy for LLMs", "Demonstrated effectiveness across multiple LLM architectures with high defense success rates", "Conducted interpretability analysis revealing insights into safety generation behavior"], "limitations": "Focused primarily on LLMs; potential applicability to other AI models not explored in detail.", "keywords": ["Large Language Models", "safety", "jailbreak attacks", "SAGE", "human-computer interaction"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071.pdf", "abs": "https://arxiv.org/abs/2505.12071", "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words.", "AI": {"tldr": "This study investigates morphological productivity via a cognitive-computational model focusing on Thomas Mann's writing.", "motivation": "To examine the productivity of morphological forms and how they relate to cognitive processes and historical language usage.", "method": "Utilizes the Discriminative Lexicon Model to explore Finnish nominal inflection, Malay derivation, and English compounding, alongside a diachronic analysis of Thomas Mann's writing over time.", "result": "The study finds that novel derived words produced by Mann are significantly fewer than those in his input, indicating a low rate of novel word production influenced by the distance of derived word embeddings from centroids.", "conclusion": "The findings highlight the tension between cognitive-computational models and actual language use, suggesting that productive morphological processes may not translate to novel word formation in individual writers.", "key_contributions": ["Integrates cognitive-computational modeling with diachronic analysis of an individual writer's output.", "Shows that systematic relationships between form and meaning are crucial for productivity in language.", "Demonstrates the limitations of using speaker-specific embeddings for low-frequency and novel words."], "limitations": "Examines only one writer's output (Thomas Mann), which may not generalize across different speakers or languages.", "keywords": ["morphological productivity", "cognitive-computational model", "Discriminative Lexicon Model", "Thomas Mann", "language processing"], "importance_score": 3, "read_time_minutes": 35}}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075.pdf", "abs": "https://arxiv.org/abs/2505.12075", "title": "Do different prompting methods yield a common task representation in language models?", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms.", "AI": {"tldr": "The paper investigates how different prompting methods for language models, specifically demonstrations and textual instructions, impact in-context learning (ICL) task representations.", "motivation": "To understand if different task presentations result in similar representations and to enhance interpretability and steerability of language models.", "method": "The study generalizes function vectors to short textual instructions for extracting task representations and compares the effects of demonstrations and instructions on task performance.", "result": "Findings indicate that different prompting mechanisms leverage distinct model components and elicit varying task representations, suggesting a lack of a common representation across methods.", "conclusion": "The paper highlights the importance of combining instructions and demonstrations for task performance while indicating the need for careful evaluation of task inference processes across different presentations.", "key_contributions": ["Generalization of function vectors to instruction prompts for ICL tasks", "Evidence that demonstration and instruction methods invoke different model components", "Recommendations for combining prompts to enhance task accuracy"], "limitations": "The study focuses primarily on few-shot tasks and may not generalize to all forms of ICL or different types of language models.", "keywords": ["in-context learning", "language models", "task representation", "function vectors", "prompting methods"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082.pdf", "abs": "https://arxiv.org/abs/2505.12082", "title": "Model Merging in Pre-training of Large Language Models", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "AI": {"tldr": "This paper explores model merging techniques during large-scale pre-training of language models, demonstrating significant performance gains and cost reductions.", "motivation": "To investigate model merging techniques in large-scale pre-training, which are underexplored despite their potential to enhance large language models.", "method": "Extensive experiments with both dense and Mixture-of-Experts architectures, analyzing the effects of merging checkpoints trained with constant learning rates.", "result": "Merging models results in significant performance improvements and accurate predictions of annealing behavior, leading to more efficient model development and reduced training costs.", "conclusion": "The findings provide practical pre-training guidelines for effective model merging and reveal new insights into the underlying mechanisms.", "key_contributions": ["Demonstrated performance improvements from model merging in pre-training.", "Provided pre-training guidelines for effective model merging.", "Uncovered novel applications and insights into merging strategies."], "limitations": "", "keywords": ["model merging", "large language models", "pre-training", "Mixture-of-Experts", "efficient training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090.pdf", "abs": "https://arxiv.org/abs/2505.12090", "title": "Personalized Author Obfuscation with Large Language Models", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue.", "AI": {"tldr": "This paper investigates the ability of large language models to obfuscate authorship through paraphrasing, observing that efficacy varies significantly among users. A personalized prompting method is proposed to improve this obfuscation.", "motivation": "To explore how effective large language models are in obfuscating authorship and to address variability in performance across different authors.", "method": "The study focuses on user-wise performance instead of a holistic view, examining how the obfuscation effectiveness of LLMs varies for individual authors. A personalized prompting approach is introduced to enhance performance.", "result": "The findings reveal a bimodal distribution in the effectiveness of LLMs for authorship obfuscation, indicating significant variability among users. The personalized prompting method showed improvements over standard techniques.", "conclusion": "The use of personalized prompting can help mitigate the issues of variable effectiveness in authorship obfuscation using LLMs, providing a more tailored approach to this problem.", "key_contributions": ["Investigation of user-wise performance of LLMs in authorship obfuscation", "Identification of a bimodal distribution in effectiveness across users", "Proposal of a personalized prompting method that enhances obfuscation performance"], "limitations": "", "keywords": ["large language models", "authorship obfuscation", "personalized prompting", "user performance", "paraphrasing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100.pdf", "abs": "https://arxiv.org/abs/2505.12100", "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur JordÃ£o"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making.", "AI": {"tldr": "This paper proposes a user-friendly method to mitigate bias in Large Language Models (LLMs) during predictions, enhancing their reliability for ethical decision-making.", "motivation": "Bias in LLM responses hinders their use in ethically sensitive tasks, necessitating reliable methods to enhance their trustworthiness.", "method": "The proposed method creates multiple variations of a sentence by modifying specific attributes and evaluates predictions against the original to identify inconsistencies indicative of bias, without requiring training or prior data knowledge.", "result": "Experimental results on the Llama family show improvements of up to 27 percentage points in reducing disparities for fairness metrics, particularly in the treatment of different racial groups.", "conclusion": "The method significantly improves fairness, equity, and reliability in LLM-generated results, making LLMs more suitable for ethical applications without the need for parameter tuning or training data changes.", "key_contributions": ["User-friendly method for bias mitigation in LLMs", "No training or fine-tuning needed for implementation", "Significant improvements in fairness metrics for Llama models"], "limitations": "", "keywords": ["Large Language Models", "bias mitigation", "fairness in AI", "ethical decision-making", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116.pdf", "abs": "https://arxiv.org/abs/2505.12116", "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety.", "AI": {"tldr": "This paper introduces a human-annotated dataset for abusive language detection in Tigrinya social media, offering insights into multilingual content moderation challenges and model effectiveness.", "motivation": "To address the lack of resources for abusive language detection in low-resource languages, specifically Tigrinya, and to enhance online safety for vulnerable users.", "method": "A large-scale benchmark dataset was created, consisting of 13,717 annotated YouTube comments across three tasks: abusiveness, sentiment, and topic classification. The dataset includes both Romanized and Ge'ez script.", "result": "Small, specialized multi-task models significantly outperform current models in low-resource settings, achieving up to 86% accuracy in abusiveness detection.", "conclusion": "The dataset facilitates research in abusive language detection and emphasizes the need for continued work in content moderation for underrepresented languages.", "key_contributions": ["Introduction of a multi-task benchmark dataset for Tigrinya social media", "Demonstration of effective model performance in low-resource conditions", "Provision of publicly available resources to support further research."], "limitations": "The dataset is limited to Tigrinya language and may not encompass all facets of abusive language in broader contexts.", "keywords": ["abusive language detection", "Tigrinya", "content moderation", "multi-task benchmark", "low-resource languages"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158.pdf", "abs": "https://arxiv.org/abs/2505.12158", "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups.", "AI": {"tldr": "This paper examines how socioeconomic status influences interactions with Large Language Models (LLMs) by surveying 1,000 individuals and analyzing their use patterns and prompts.", "motivation": "To explore the impact of socioeconomic status (SES) on the usage of language technologies and to address limitations of previous research relying on proxy metrics and synthetic data.", "method": "A survey of 1,000 individuals from diverse SES backgrounds was conducted, collecting 6,482 prompts from their interactions with LLMs to analyze differences in usage and interaction styles.", "result": "The study finds systematic differences in LLM usage across SES groups; higher SES individuals use LLMs with higher abstraction and conciseness, while lower SES individuals show more anthropomorphization and concrete language expression.", "conclusion": "The findings indicate that while LLMs are becoming more accessible, linguistic differences tied to SES continue to promote the digital divide, emphasizing the need for inclusive technology design.", "key_contributions": ["Identifies SES-based differences in LLM interaction styles", "Highlights the need for tailored language technology development", "Provides empirical data on user interactions with LLMs across different SES groups"], "limitations": "The study is limited to self-reported data and may not capture all dimensions of language technology use.", "keywords": ["socioeconomic status", "Large Language Models", "digital divide", "language technology", "AI accessibility"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160.pdf", "abs": "https://arxiv.org/abs/2505.12160", "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances.", "AI": {"tldr": "Study examines anti-refugee sentiment in Turkish social media using a localized Emotion Recognition Model (ERM).", "motivation": "To explore the rise of anti-refugee sentiment in Turkey amidst the Syrian refugee influx through social media discourse and enhance sentiment analysis for underrepresented languages like Turkish.", "method": "Developed an Emotion Recognition Model (ERM) tailored for Turkish using BERTurk and the TREMO dataset, achieving 92.62% accuracy in categorizing various emotions.", "result": "The model was applied to large-scale X data, uncovering emotional nuances in Turkish discourse and advancing sentiment analysis techniques.", "conclusion": "Localized NLP tools like the ERM model have practical applications in marketing, public relations, and crisis management by offering real-time sentiment analysis tailored to Turkish-language contexts.", "key_contributions": ["Advanced Emotion Recognition Model (ERM) for Turkish", "Enhancement of sentiment analysis in underrepresented languages", "Practical applications for real-time sentiment analysis in various fields"], "limitations": "", "keywords": ["Emotion Recognition", "Sentiment Analysis", "Turkish Social Media"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182.pdf", "abs": "https://arxiv.org/abs/2505.12182", "title": "Truth Neurons", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.", "AI": {"tldr": "This paper identifies 'truth neurons' in language models that encode truthfulness, showing their universal presence across different models and implications for improving model trustworthiness.", "motivation": "To enhance the reliability and safety of language models by understanding how truthfulness is represented in their architecture.", "method": "Conducted experiments identifying and analyzing the presence of truth neurons in multiple language models, examining their distribution and impact on model performance.", "result": "The existence of truth neurons was validated across various language models, confirming they encode truthfulness in a subject-agnostic manner and are crucial for model performance on benchmarks.", "conclusion": "Understanding and improving the mechanisms of truthfulness in language models can enhance their trustworthiness and reliability, leading to safer AI applications.", "key_contributions": ["Identification of truth neurons in language models", "Validation of truth neuron presence across models of varying scales", "Insights into truthfulness mechanisms informing future improvements in model trustworthiness"], "limitations": "The study is primarily based on specific datasets and may require further exploration on other contexts and model architectures.", "keywords": ["truthfulness", "language models", "truth neurons", "AI reliability", "trustworthiness"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183.pdf", "abs": "https://arxiv.org/abs/2505.12183", "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "authors": ["Manari Hirose", "Masato Uchida"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems.", "AI": {"tldr": "This paper presents a framework for evaluating Large Language Models (LLMs) by analyzing their ideological biases through a set of quantitative questions, revealing notable differences between models and serious ethical concerns.", "motivation": "To understand biases, thought patterns, and societal implications of LLMs to ensure ethical and effective use, given their widespread integration.", "method": "A quantitative analysis was conducted using a framework applied to 436 binary-choice questions to assess the biases of ChatGPT and Gemini.", "result": "The analysis revealed that LLMs maintain consistent opinions, but exhibit differing ideologies across models and languages. ChatGPT tends to align its responses with the questioner's opinions, and both models demonstrate problematic biases and claims.", "conclusion": "The findings emphasize the need for addressing ideological and ethical considerations in LLM evaluations and provide a flexible method for assessing LLM behavior.", "key_contributions": ["Development of a novel framework for evaluating LLM biases", "Empirical findings on ideological differences between LLMs", "Identification of ethical concerns relating to LLM outputs"], "limitations": "The study focuses on a limited set of binary-choice questions and may not encompass all possible biases in LLMs.", "keywords": ["Large Language Models", "bias evaluation", "ideological biases", "ethical implications", "quantitative analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196.pdf", "abs": "https://arxiv.org/abs/2505.12196", "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used.", "AI": {"tldr": "This study evaluates the scaling effects of large language models (LLMs) on predicting human reading time and brain imaging data, highlighting a misalignment between LLM architecture and human sentence processing, particularly as model size increases.", "motivation": "To investigate the relationship between large language models' ability to predict human reading time and brain imaging data while considering the impact of model size and architecture on this prediction.", "method": "The study evaluates LLM scaling using entire LLM vectors while controlling for the number of predictors, addressing previously noted issues with their performance in predicting psychometric data.", "result": "The results indicate that inverse scaling occurs, suggesting that as LLMs grow larger, their predictions become less aligned with human sentence processing.", "conclusion": "The misalignment between LLMs and human sentence processing worsens with larger models, contradicting the assumption that increased model size necessarily improves psychometric predictions.", "key_contributions": ["Evaluation of LLM scaling using entire vectors from LLMs", "Control for predictor number in model evaluation", "Demonstration of inverse scaling in LLM predictions"], "limitations": "The study may not account for all variables influencing human sentence processing, and the specific reasons for misalignment require further investigation.", "keywords": ["large language models", "psychometric data", "human sentence processing", "inverse scaling", "model architecture"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201.pdf", "abs": "https://arxiv.org/abs/2505.12201", "title": "How Reliable is Multilingual LLM-as-a-Judge?", "authors": ["Xiyan Fu", "Wei Liu"], "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications.", "AI": {"tldr": "This paper analyzes the reliability of large language models in multilingual evaluation tasks, revealing inconsistencies and proposing an ensemble strategy to improve performance.", "motivation": "To assess the reliability of large language models (LLMs) as evaluators in multilingual contexts, especially as alternatives to human annotators.", "method": "A comprehensive analysis of five LLMs from different families across five tasks involving 25 languages was conducted, measuring judgment consistency using Fleiss' Kappa.", "result": "The study found that LLMs exhibit poor consistency in judgments across languages, averaging approximately 0.3 on Fleiss' Kappa, with significant discrepancies particularly in low-resource languages.", "conclusion": "LLMs are currently unreliable for multilingual evaluation, and an ensemble strategy can enhance their consistency in practical applications.", "key_contributions": ["Comprehensive evaluation of LLMs' performance in multilingual settings.", "Identification of factors affecting judgment consistency across languages.", "Proposal of an ensemble strategy to improve multilingual judgment reliability."], "limitations": "The study indicates that neither training on multilingual data nor increasing model scale significantly enhances judgment consistency.", "keywords": ["Large Language Models", "Multilingual Evaluation", "Consistency", "Ensemble Strategy", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212.pdf", "abs": "https://arxiv.org/abs/2505.12212", "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup.", "AI": {"tldr": "Data Whisperer is a training-free method for optimally selecting subsets of data for fine-tuning large language models, achieving superior performance and efficiency.", "motivation": "The need to efficiently select optimal subsets of data for fine-tuning large language models as dataset sizes grow, balancing performance and computational cost.", "method": "Data Whisperer, an efficient, training-free, attention-based method that uses few-shot in-context learning with the model being fine-tuned.", "result": "Data Whisperer outperforms the full GSM8K dataset on the Llama-3-8B-Instruct model while using only 10% of the data, with significant improvement and speedup over existing methods.", "conclusion": "The proposed method provides a more efficient way to fine-tune LLMs without the overhead of traditional data selection methods.", "key_contributions": ["Introduction of Data Whisperer, a training-free data selection method", "Achieved superior performance with minimal data", "Significant speedup in the data selection process"], "limitations": "", "keywords": ["data selection", "large language models", "few-shot learning", "fine-tuning", "computational efficiency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215.pdf", "abs": "https://arxiv.org/abs/2505.12215", "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin.", "AI": {"tldr": "This paper presents GMSA, a context compression framework that enhances the efficiency of LLMs in long-context scenarios by reducing input sequence length and redundant information.", "motivation": "Large language models struggle with low computational efficiency and redundancy when handling long texts, necessitating improved methods for context management.", "method": "The GMSA framework includes Group Merging for effective summary extraction and Layer Semantic Alignment to ensure coherent integration of summary vectors with input semantics, trained via autoencoders and fine-tuning.", "result": "GMSA achieves significant improvements in context restoration, speed, and stability in training, particularly demonstrating a 2x speedup in end-to-end inference for QA tasks.", "conclusion": "GMSA not only enhances performance in speech processing but also stabilizes convergence with fewer resources, showing promise for practical applications in NLP tasks.", "key_contributions": ["Introduction of Group Merging for summary vector extraction", "Development of Layer Semantic Alignment to bridge semantic gaps", "Implementation of Knowledge Extraction Fine-tuning for downstream adaptation"], "limitations": "", "keywords": ["large language models", "context compression", "natural language processing", "semantic alignment", "knowledge extraction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216.pdf", "abs": "https://arxiv.org/abs/2505.12216", "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "authors": ["Rongguang Ye", "Ming Tang"], "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines.", "AI": {"tldr": "This paper introduces UniCuCo, a model for efficient customized compression of large language models (LLMs) that addresses the inefficiencies of existing methods for multiple simultaneous requests.", "motivation": "Existing pruning methods for LLMs become inefficient with multiple simultaneous requests, leading to increased processing times. There is a need for a solution that can handle these requests efficiently while maintaining model performance.", "method": "The paper proposes UniCuCo, which uses a StratNet to learn optimal pruning strategies for arbitrary requests. A Gaussian process is employed to approximate the evaluation of pruning strategies, enabling effective training despite the non-differentiable nature of pruning.", "result": "UniCuCo is demonstrated to be 28 times faster than baseline methods when processing 64 simultaneous requests, while maintaining comparable accuracy.", "conclusion": "The approach effectively addresses the scaling issues of current LLM pruning methods, making it suitable for real-world applications where multiple requests are common.", "key_contributions": ["Introduction of the UniCuCo model for customized LLM compression", "Development of the StratNet for optimal pruning strategy mappings", "Use of Gaussian process to enable efficient training for non-differentiable processes"], "limitations": "The main limitation includes the potential computational cost related to the Gaussian process approximation in certain scenarios.", "keywords": ["large language models", "pruning", "StratNet", "Gaussian process", "model performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218.pdf", "abs": "https://arxiv.org/abs/2505.12218", "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal.", "AI": {"tldr": "This paper analyzes the impact of Large Language Models (LLMs) on the linguistic characteristics of academic writing, revealing significant changes in abstracts over the last decade.", "motivation": "To systematically examine the influence of LLMs on the linguistic features of academic writing, addressing the gap left by prior quantitative research.", "method": "Conducted a large-scale linguistic analysis on 823,798 abstracts from arXiv, focusing on features like lexical complexity, syntactic complexity, cohesion, readability, and sentiment.", "result": "Findings reveal an increase in LLM-preferred words, lexical complexity, and sentiment in academic abstracts, but a decrease in syntactic complexity, cohesion, and readability.", "conclusion": "The study suggests that while LLMs help introduce new vocabulary, they may also lead to challenges in clarity and cohesion, notably affecting scholars with weaker English proficiency, particularly in Computer Science.", "key_contributions": ["Analyzed 823,798 abstracts to assess LLM impact on writing", "Identified changes in lexical complexity and sentiment", "Highlighted the differential impact of LLMs across disciplines, especially in Computer Science"], "limitations": "Focused only on abstracts from arXiv; results may not generalize to other types of academic writing or datasets.", "keywords": ["Large Language Models", "academic writing", "linguistic analysis", "arXiv", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236.pdf", "abs": "https://arxiv.org/abs/2505.12236", "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE.", "AI": {"tldr": "The paper proposes TKRE, a novel framework that integrates large language models with traditional relation extraction methods to improve Few-Shot Relation Extraction (FSRE) through data generation and a two-stage pre-training strategy.", "motivation": "The scarcity of annotated data and the limited generalization capabilities of existing models create challenges in Few-Shot Relation Extraction.", "method": "TKRE combines LLMs with traditional relation extraction models, using LLMs to create explanation-driven knowledge and synthetic data, alongside a two-stage pre-training strategy of Masked Span Language Modeling and Span-Level Contrastive Learning.", "result": "TKRE achieves state-of-the-art performance on benchmark datasets for FSRE, addressing the issue of data scarcity effectively.", "conclusion": "The proposed framework shows potential for broader applications in low-resource scenarios, enhancing both relational reasoning and generalization capabilities in FSRE.", "key_contributions": ["Integration of LLMs with traditional relation extraction models", "Introduction of a two-stage pre-training strategy (MSLM and SCL)", "Generation of knowledge and synthetic data to tackle data scarcity"], "limitations": "", "keywords": ["Few-Shot Relation Extraction", "Large Language Models", "Data Scarcity"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238.pdf", "abs": "https://arxiv.org/abs/2505.12238", "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs.", "AI": {"tldr": "The paper introduces PANORAMA, a synthetic dataset aimed at studying PII memorization risks in large language models and validating its utility through model fine-tuning.", "motivation": "To address the privacy risks associated with PII memorization in large language models due to the lack of realistic datasets that reflect diverse sensitive information.", "method": "The paper presents PANORAMA, a synthetic corpus generated using multi-attribute human profiles to emulate real-world demographics, producing various content types containing realistic PII.", "result": "Fine-tuning the Mistral-7B model with the PANORAMA dataset showed consistent increases in PII memorization rates with data replication, varying by content type.", "conclusion": "PANORAMA provides a crucial resource for assessing privacy risks, auditing models, and creating privacy-preserving language models in real-world applications.", "key_contributions": ["Introduction of a large-scale synthetic dataset (PANORAMA) for studying PII memorization.", "Demonstration of the utility of the dataset through model fine-tuning on PII memorization rates.", "Public availability of dataset and code for community use in privacy risk assessments."], "limitations": "The dataset is synthetic and may not capture all nuances of real-world PII memorization.", "keywords": ["privacy", "large language models", "PII memorization", "synthetic dataset", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244.pdf", "abs": "https://arxiv.org/abs/2505.12244", "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers.", "AI": {"tldr": "This paper explores the challenges and capabilities of autoregressive neural language models in generating specific token distributions based on prompts, highlighting that distributions with extreme entropies and those influenced by outliers are easier to elicit.", "motivation": "To systematically understand the probability distributions that autoregressive language models can produce and identify factors influencing their elicitation from prompts.", "method": "The authors utilize soft and hard gradient-based prompt tuning techniques to find prompts that induce language models to approximate target next-token distributions effectively.", "result": "It is found that distributions with very low or high entropy are easier to approximate than those with moderate entropy. Additionally, when distributions have the same entropy, those with outlier tokens are easier to induce, and distributions produced by language models themselves are easier to approximate than randomly chosen targets.", "conclusion": "The findings provide insights into the expressiveness of language models and the difficulties in using them as proposers of probability distributions.", "key_contributions": ["Identifying the easiness of eliciting distributions with extreme entropies compared to moderate ones.", "Demonstrating that target distributions containing outlier tokens are simpler to approximate.", "Establishing that distributions generated by language models are easier to approximate than random target distributions."], "limitations": "", "keywords": ["autonomous systems", "machine learning", "language models", "prompt tuning", "probability distribution"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250.pdf", "abs": "https://arxiv.org/abs/2505.12250", "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B", "AI": {"tldr": "EQUAL is a scalable data extraction framework that improves instruction tuning for LLMs by efficiently selecting documents and extracting high-quality QA pairs, reducing costs significantly while enhancing model performance.", "motivation": "The reliance on high-quality training data for instruction tuning in LLMs often leads to synthesized instruction data lacking diversity, which impacts real-world applicability.", "method": "EQUAL clusters document corpora using contrastive learning embeddings and employs a multi-armed bandit strategy to identify valuable QA pairs, alternately selecting documents and extracting pairs to optimize efficiency and quality.", "result": "EQUAL reduces computational costs by 5-10 times and enhances accuracy by 2.5% on models like LLaMA-3.1-8B and Mistral-7B across tasks based on datasets like AutoMathText and StackOverflow.", "conclusion": "The framework EQUAL is an effective solution for improving the efficiency and effectiveness of instruction tuning through better data extraction techniques.", "key_contributions": ["Introduction of the EQUAL framework for scalable QA pair extraction", "Improved efficiency in selecting diverse instructional data", "Demonstrated significant cost reduction and accuracy improvement on LLMs"], "limitations": "", "keywords": ["Instruction Tuning", "Large Language Models", "Data Extraction", "Quality Assessment", "Multi-Armed Bandit"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259.pdf", "abs": "https://arxiv.org/abs/2505.12259", "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance.", "AI": {"tldr": "Introducing Teach2Eval, a novel evaluation framework for large language models (LLMs) that assesses their ability to teach tasks to weaker models, providing a more scalable and interpretable method compared to traditional benchmarks.", "motivation": "To address the limitations of traditional evaluation methods for large language models, which are often plagued by fairness issues and scalability concerns.", "method": "Teach2Eval transforms open-ended tasks into standardized multiple-choice questions using teacher-generated feedback, allowing for automated and multi-dimensional assessment of LLMs.", "result": "Experimental results indicate that Teach2Eval aligns well with dynamic rankings of models and provides additional interpretability for model training.", "conclusion": "Teach2Eval offers a robust alternative to traditional benchmarks, capturing a wider range of cognitive abilities and minimizing risks associated with data contamination.", "key_contributions": ["Introduction of Teach2Eval framework for LLM assessment", "Use of teacher-generated feedback for creating MCQs", "Demonstrated strong alignment with existing model rankings and enhanced interpretability."], "limitations": "", "keywords": ["large language models", "evaluation framework", "machine learning", "HCI", "automated assessment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265.pdf", "abs": "https://arxiv.org/abs/2505.12265", "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact.", "AI": {"tldr": "This paper investigates reference-free hallucination detection in large language models, introducing a novel fine-tuning approach that improves the accuracy of detecting factually incorrect information in long-form text generation.", "motivation": "Hallucination in large language models poses a challenge in generating factually correct information, particularly in open-domain tasks, and existing detection methods often depend on limited domains or external tools.", "method": "The paper explores various approaches to hallucination detection, including prompting methods, probing, and fine-tuning, with an emphasis on a new paradigm called RATE-FT which combines fine-tuning with an auxiliary task.", "result": "The proposed RATE-FT method demonstrated a 3% improvement over general fine-tuning techniques on the LongFact dataset, showing its effectiveness in enhancing the detection of hallucinations in LLMs.", "conclusion": "The study indicates that combining fine-tuning with auxiliary tasks can significantly improve the model's capability to distinguish between factual and hallucinated content in open-domain long-form generation tasks.", "key_contributions": ["Investigation of reference-free methods for hallucination detection in open-domain text generation.", "Introduction of the RATE-FT paradigm that enhances fine-tuning with auxiliary tasks.", "Demonstrated effectiveness through extensive experiments on various model families and datasets."], "limitations": "The findings primarily rely on specific model families and datasets, which may not generalize universally across all LLMs and open-domain tasks.", "keywords": ["hallucination detection", "large language models", "fine-tuning", "open-domain generation", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268.pdf", "abs": "https://arxiv.org/abs/2505.12268", "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "authors": ["Pratim Chowdhary"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits.", "AI": {"tldr": "This paper introduces a method to identify the attention heads in mid-sized language models essential for classification tasks, revealing distinct patterns of head usage across various syntactic tasks.", "motivation": "To understand the neural components in mid-sized language models that contribute to their capabilities, particularly in classification tasks.", "method": "The authors propose the $(K, \text{epsilon})$-Minimum Sufficient Head Circuit ($K$-MSHC) methodology and the Search-K-MSHC algorithm to discover minimal sets of attention heads essential for specific tasks.", "result": "Analysis of the Gemma-9B model reveals task-specific head circuits: grammar tasks use early layers, arithmetic word problems activate various layers, and arithmetic verification shows a distributed pattern. The study identifies critical 'super-heads' with little overlap across tasks.", "conclusion": "Syntactic and numerical competencies in language models emerge from specialized head circuits that exhibit both reusability and dedicated functions across tasks.", "key_contributions": ["Introduction of $(K, \text{epsilon})$-Minimum Sufficient Head Circuit methodology", "Development of the Search-K-MSHC algorithm", "Findings on specialized 'super-heads' for distinct syntactic tasks"], "limitations": "The study is focused on mid-sized language models, and the findings may not directly generalize to larger models or other architectures.", "keywords": ["language models", "attention heads", "classification tasks", "syntactic tasks", "neural circuits"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273.pdf", "abs": "https://arxiv.org/abs/2505.12273", "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.", "AI": {"tldr": "This paper presents a dialect-guided approach to enhance machine translation evaluation for low-resource languages using large language models.", "motivation": "Evaluating machine translation for low-resource languages is challenging due to limited reference translations and dialectal diversity.", "method": "A comprehensive framework that incorporates dialect-specific context into LLM-based MT evaluation, extending the ONUBAD dataset with Sylheti-English pairs and augmenting the tokenizer vocabulary.", "result": "The proposed method improves performance, achieving the highest gain of +0.1083 in Spearman correlation across multiple LLMs compared to existing methods.", "conclusion": "This dialect-guided approach effectively enhances evaluation capabilities for machine translation in low-resource languages.", "key_contributions": ["Proposed a dialect-guided framework for LLM-based MT evaluation.", "Augmented the ONUBAD dataset with Sylheti-English sentence pairs.", "Introduced a regression head and dialect-guided prompting strategy."], "limitations": "", "keywords": ["Machine Translation", "Low-Resource Languages", "Dialect-Guided Evaluation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287.pdf", "abs": "https://arxiv.org/abs/2505.12287", "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.", "AI": {"tldr": "This paper investigates vulnerabilities of closed-source large language models (LLMs) to adversarial prompt injections, presenting a framework to evaluate their performance under diverse multilingual attack scenarios.", "motivation": "To address the security vulnerabilities of closed-source LLMs against adversarial prompt injections, which have not been thoroughly explored compared to open-source models.", "method": "An integrated adversarial framework employing diverse attack techniques to evaluate closed-source LLMs, analyzing 38,400 responses across 32 types of jailbreak attacks in English and Chinese.", "result": "The evaluation found that Qwen-Max is the most vulnerable LLM, while GPT-4o demonstrated the best defenses. The effectiveness of prompt attacks was higher in Chinese than in English, with the Two-Sides attack technique showing the highest success rate.", "conclusion": "The findings underscore the urgent need for enhancements in language-aware alignment and cross-lingual defenses in LLMs, urging attention from researchers, developers, and policymakers.", "key_contributions": ["Developed a novel integrated adversarial framework for evaluating closed-source LLMs.", "Demonstrated significant variation in adversarial success rates based on language and model architecture.", "Introduced the Two-Sides attack technique as the most effective approach in the evaluation."], "limitations": "The study is limited to only certain closed-source LLMs and specific types of attacks, which may not encompass all possible vulnerabilities or attack methodologies.", "keywords": ["large language models", "adversarial attacks", "prompt injections", "cross-lingual defense", "security evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299.pdf", "abs": "https://arxiv.org/abs/2505.12299", "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios.", "AI": {"tldr": "The study proposes an Iterative Preference Learning (IPL) method to enhance the reasoning capabilities of VLM-based mobile agents in GUI tasks by generating diverse training data and optimizing learning using T-DPO pairs.", "motivation": "To address the limitations of existing methods that rely on scarce CoaT trajectories and expensive annotations, this paper seeks to improve the reasoning performance and generalization of VLM-based agents in GUI tasks.", "method": "The paper introduces an Iterative Preference Learning technique that constructs a CoaT-tree through sampling, uses rule-based rewards to score leaf nodes, and derives Thinking-level Direct Preference Optimization pairs, while incorporating a three-stage instruction evolution for fine-tuning.", "result": "Experiments conducted on three Mobile GUI-agent benchmarks show that the proposed MobileIPL agent outperforms established models, achieving state-of-the-art results and generalizing effectively to scenarios outside the training domain.", "conclusion": "The proposed methods enhance the expressiveness and generalization of VLM-based mobile agents, confirming the effectiveness of the iterative learning and instruction evolution strategies.", "key_contributions": ["Proposed Iterative Preference Learning method to improve GUI task agents.", "Demonstrated state-of-the-art performance across multiple benchmarks.", "Introduced a novel three-stage instruction evolution for model fine-tuning."], "limitations": "", "keywords": ["human-computer interaction", "visual language models", "mobile agents", "reasoning performance", "user interfaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300.pdf", "abs": "https://arxiv.org/abs/2505.12300", "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets.", "AI": {"tldr": "Hierarchical Balancing Optimization (HBO) enhances fine-tuning of large language models by autonomously adjusting data allocation both globally across datasets and locally within subsets, addressing data imbalance and heterogeneity.", "motivation": "The paper tackles the challenges of data imbalance and heterogeneity in fine-tuning large language models (LLMs), which are often overlooked in existing methodologies.", "method": "The methodology involves a bilevel optimization strategy featuring a Global Actor that balances data sampling across diverse subsets and several Local Actors that optimize data usage based on the difficulty of tasks, guided by reward functions reflecting learning progress.", "result": "HBO demonstrates significant accuracy improvements over existing methods across three LLM backbones evaluated on nine diverse tasks in multilingual and multitask configurations.", "conclusion": "The study concludes that HBO effectively resolves issues related to data imbalance and heterogeneity during LLM fine-tuning, leading to improved outcomes in model training across various datasets.", "key_contributions": ["Introduction of Hierarchical Balancing Optimization (HBO) for LLM fine-tuning", "Bilevel optimization strategy with Global and Local Actors", "Demonstrated significant improvement over existing baselines"], "limitations": "", "keywords": ["large language models", "fine-tuning", "data imbalance", "hierarchical balancing", "bilevel optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306.pdf", "abs": "https://arxiv.org/abs/2505.12306", "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.", "AI": {"tldr": "This paper introduces WikiDYK, a novel knowledge injection benchmark that utilizes Wikipedia facts to enhance large language models' (LLMs) knowledge memorization capabilities, demonstrating a framework that improves reliability accuracy.", "motivation": "To address the underexplored area of knowledge memorization in large language models and to create a high-quality, standardized benchmark for evaluating such capabilities.", "method": "WikiDYK leverages human-edited Wikipedia facts to create a dataset of question-answer pairs across various formats. It employs continued pre-training to evaluate model performance.", "result": "Experiments reveal that causal language models (CLMs) exhibit significantly lower knowledge memorization capabilities than bidirectional language models (BiLMs), with a 23% accuracy gap. The introduction of a collaborative framework utilizing ensembles of BiLMs enhances reliability accuracy by up to 29.1%.", "conclusion": "The study highlights the potential of using modular frameworks with BiLMs to improve knowledge integration in LLMs, suggesting future work on continuously updating knowledge bases.", "key_contributions": ["Introduction of the WikiDYK knowledge injection benchmark", "Demonstration of accuracy differences between CLMs and BiLMs in knowledge memorization", "Development of a modular collaborative framework for improving LLM reliability"], "limitations": "", "keywords": ["knowledge injection", "large language models", "benchmark", "human-computer interaction", "bidirectional language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313.pdf", "abs": "https://arxiv.org/abs/2505.12313", "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost.", "AI": {"tldr": "ExpertSteer proposes a method for guiding Large Language Models using specialized expert models to generate steering vectors, overcoming limitations of current methods that rely on self-generated vectors.", "motivation": "The motivation is to improve the control of Large Language Models during inference, as existing methods are limited by their reliance on model-specific steering vectors.", "method": "ExpertSteer utilizes a four-step process: aligning representation dimensions with auto-encoders, identifying intervention layer pairs through mutual information analysis, generating steering vectors from expert models using Recursive Feature Machines, and applying these vectors to guide LLMs during inference.", "result": "Experiments show that ExpertSteer significantly outperforms existing baselines across 15 benchmarks in various domains, demonstrating its effectiveness in selectively guiding LLMs.", "conclusion": "The introduction of ExpertSteer allows for more flexible and effective steering of LLMs by leveraging external expert models, which enhances performance without modifying LLM parameters.", "key_contributions": ["Introduction of a novel method for LLM guidance using external expert models", "A four-step process for cross-model transfer of knowledge", "Demonstration of significant performance improvement over established baselines"], "limitations": "", "keywords": ["Large Language Models", "Expert Steering", "Machine Learning", "Inference Control"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328.pdf", "abs": "https://arxiv.org/abs/2505.12328", "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123.", "AI": {"tldr": "Submission to LLMSR@XLLM25 shared task evaluating LLMs' reasoning processes using Meta-Llama-3-8B-Instruct.", "motivation": "To improve the fine-grained, controllable, and interpretable reasoning processes of large language models (LLMs).", "method": "Developed a few-shot, multi-turn prompt for the LLM to extract conditions and verify logical validity, with a lightweight post-processor for normalizing outputs.", "result": "Ranked 5th overall in the shared task, achieving competitive macro F1 scores without fine-tuning or external resources.", "conclusion": "The approach demonstrates effectiveness and lays groundwork for future research in structural reasoning with LLMs.", "key_contributions": ["Innovative use of few-shot prompting for reasoning tasks.", "Introduction of a lightweight post-processing method.", "Analysis of strengths and limitations in LLM reasoning."], "limitations": "Limited to off-the-shelf model without fine-tuning or external retrieval.", "keywords": ["Large Language Models", "Structural Reasoning", "Few-shot Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345.pdf", "abs": "https://arxiv.org/abs/2505.12345", "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors.", "AI": {"tldr": "UniEdit is a unified benchmark designed for enhancing the accuracy of LLMs by ensuring comprehensive editing across diverse knowledge domains.", "motivation": "To improve the accuracy and reliability of large language models by addressing the limitations of existing LLM editing datasets which are confined to narrow domains.", "method": "UniEdit constructs editing samples from 25 common domains using a Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to evaluate editing ripple effects, converting knowledge subgraphs into natural language with LLMs.", "result": "Statistical analysis demonstrates that UniEdit offers a comprehensive and diverse benchmark for LLM editing and provides insights into the performance of multiple editors across different evaluations.", "conclusion": "UniEdit establishes a standardized framework for LLM editing that can facilitate future research by providing extensive insights into editing dynamics across knowledge domains.", "key_contributions": ["Introduction of a unified editing benchmark (UniEdit) for LLMs.", "Development of NMCS for comprehensive ripple effect evaluation.", "Statistical analysis confirming the benchmark's scale and diversity."], "limitations": "", "keywords": ["Large Language Models", "Editing Benchmark", "Knowledge Graphs", "Neighborhood Multi-hop Chain Sampling", "LLM Performance Evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349.pdf", "abs": "https://arxiv.org/abs/2505.12349", "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "authors": ["Axel Abels", "Tom Lenaerts"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts.", "AI": {"tldr": "The paper investigates how large language models (LLMs) replicate biases from their training data and proposes methods to mitigate these biases through crowd-based response aggregation.", "motivation": "To explore the mechanisms through which biases manifest in LLM responses and to find effective strategies for mitigating these biases.", "method": "The study analyzes responses from multiple LLMs to bias-eliciting headlines and compares different aggregation techniques, including averaging responses and using locally weighted methods.", "result": "Locally weighted aggregation methods show better performance in reducing biases while also improving the accuracy of responses compared to simple averaging.", "conclusion": "Combining LLMs with human responses in a hybrid crowd approach significantly improves accuracy and reduces biases across various contexts.", "key_contributions": ["Analysis of LLM bias replication", "Comparison of response aggregation methods", "Development of hybrid crowd strategies for bias mitigation"], "limitations": "", "keywords": ["large language models", "bias mitigation", "crowd aggregation", "human-computer interaction", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368.pdf", "abs": "https://arxiv.org/abs/2505.12368", "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations.", "AI": {"tldr": "Introduction of CAPTURE, a context-aware benchmark for evaluating prompt injection guardrail models.", "motivation": "To address the major security risks posed by prompt injection in large language models and to evaluate the effectiveness of guardrail models beyond static benchmarks.", "method": "Development of the CAPTURE benchmark, focusing on context-aware settings and assessing both attack detection and over-defense tendencies.", "result": "Experiments show that existing guardrail models face high false negative rates in adversarial cases and excessive false positives in benign scenarios.", "conclusion": "The findings underscore critical limitations in current prompt injection guardrail models and the need for improved evaluation methods.", "key_contributions": ["Introduction of a new context-aware benchmark (CAPTURE) for prompt injection guardrails.", "Assessment of current guardrail models' performance in real-world scenarios.", "Identification of high false negative and false positive rates in existing guardrail models."], "limitations": "The benchmark relies on minimal in-domain examples which might affect generalizability.", "keywords": ["prompt injection", "security", "machine learning", "context-awareness", "guardrail models"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381.pdf", "abs": "https://arxiv.org/abs/2505.12381", "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm.", "AI": {"tldr": "The paper investigates bias in language models, emphasizing the need to understand both data and model architecture in bias propagation.", "motivation": "To systematically investigate the origins of bias in language models by analyzing the interaction between training data and model architecture.", "method": "The authors propose a methodology based on comparative behavioral theory, evaluating the effects of data, model design, and temporal dynamics on bias propagation in language modeling.", "result": "Findings show that n-gram LMs are sensitive to context size affecting bias propagation, whereas transformers are architecturally robust; also, the temporal provenance of training data significantly impacts bias, with different architectures responding variably to controlled bias injection.", "conclusion": "A holistic approach is necessary to trace bias origins in both data and model architectures to effectively mitigate harm in language models.", "key_contributions": ["Introduces a new methodology for analyzing bias origins in language models", "Reveals the differential sensitivity of n-gram and transformer architectures to bias propagation", "Highlights the importance of temporal data provenance in bias influence."], "limitations": "", "keywords": ["bias propagation", "language models", "comparative behavioral theory", "transformers", "n-gram LMs"], "importance_score": 9, "read_time_minutes": 19}}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392.pdf", "abs": "https://arxiv.org/abs/2505.12392", "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.", "AI": {"tldr": "SLOT is a test-time inference approach that improves language model performance by optimizing for individual prompts, enhancing response accuracy.", "motivation": "Existing large language models often struggle with complex instructions, leading to suboptimal performance on tasks not well-represented among general samples.", "method": "SLOT conducts few optimization steps at test-time to update a lightweight sample-specific parameter vector, which is added to the final hidden layer before the output head, allowing for efficient adaptation to incoming prompts.", "result": "SLOT demonstrates improved accuracy on multiple benchmarks, with Qwen2.5-7B improving from 57.54% to 66.19% on GSM8K, and DeepSeek-R1-Distill-Llama-70B achieving 68.69% on GPQA, the highest among 70B-level models.", "conclusion": "SLOT effectively enhances the performance of language models on complex instruction-following tasks by conducting minimal, sample-specific optimizations at test-time.", "key_contributions": ["Introduces a novel test-time optimization technique for LLMs.", "Demonstrates significant accuracy improvements on benchmark tasks.", "Provides an open-source implementation for further research."], "limitations": "", "keywords": ["Language Model", "Test-time Optimization", "Few-shot Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398.pdf", "abs": "https://arxiv.org/abs/2505.12398", "title": "Traversal Verification for Speculative Tree Decoding", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods", "AI": {"tldr": "This paper presents Traversal Verification, a novel speculative decoding algorithm that improves acceptance rates and throughput in large language models by utilizing a leaf-to-root verification approach.", "motivation": "Speculative decoding accelerates large language models but suffers from limitations in existing token-level verification methods.", "method": "Traversal Verification employs a leaf-to-root traversal strategy, considering the acceptance of entire token sequences rather than individual tokens.", "result": "Our method shows improved acceptance length and throughput across various large language models and tasks.", "conclusion": "Traversal Verification guarantees lossless inference with substantial acceleration gains compared to traditional methods.", "key_contributions": ["Introduction of Traversal Verification for speculative decoding.", "Theoretical proof of identical probability distribution to the target model.", "Demonstrated improved performance metrics over existing frameworks."], "limitations": "", "keywords": ["speculative decoding", "large language models", "acceptance rates"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405.pdf", "abs": "https://arxiv.org/abs/2505.12405", "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore.", "AI": {"tldr": "The paper presents a method for detecting AI paraphrased articles, specifically identifying those generated by ChatGPT, using a pattern-based similarity detection algorithm.", "motivation": "To address the growing issue of copyright infringement caused by generative AI paraphrased content, which negatively impacts original content creators.", "method": "The study employs a pattern-based similarity detection algorithm to identify articles paraphrased by AI, specifically targeting content created by ChatGPT, tested on a benchmark dataset of real and paraphrased articles.", "result": "The method achieved high detection accuracy with 96.23% accuracy, 96.25% precision, 96.21% sensitivity, 96.25% specificity, and 96.23% F1 score on the benchmark dataset.", "conclusion": "The proposed detection algorithm effectively identifies AI paraphrased articles without deep learning, highlighting the potential to mitigate copyright issues in news content.", "key_contributions": ["Introduced a pattern-based similarity detection algorithm for AI paraphrased content.", "Developed a benchmark dataset for evaluating AI paraphrase detection.", "Demonstrated high detection accuracy without using deep learning techniques."], "limitations": "", "keywords": ["Generative AI", "Paraphrasing", "Copyright Infringement", "ChatGPT", "Pattern Similarity Detection"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415.pdf", "abs": "https://arxiv.org/abs/2505.12415", "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.", "AI": {"tldr": "This paper introduces Table-R1, a reinforcement learning approach that improves LLM's comprehension of tables by integrating region evidence into reasoning steps for enhanced table question answering.", "motivation": "Optimizing large language models for effective table question answering is underexplored despite their potential in this area.", "method": "The method uses a region-based approach called Table-R1, which includes Region-Enhanced Supervised Fine-Tuning (RE-SFT) and Table-Aware Group Relative Policy Optimization (TARPO) to improve LLM performance on table questions by guiding models in identifying relevant table regions and balancing rewards.", "result": "Table-R1 shows an average performance improvement of 14.36 points on benchmark datasets and reduces token consumption by 67.5% compared to previous methods.", "conclusion": "The integration of region evidence and a mixed reward system significantly enhances LLM capabilities in tabular reasoning.", "key_contributions": ["Introduction of the region-based Table-R1 approach.", "Incorporation of RE-SFT and TARPO methods into LLM table reasoning.", "Achieving notable performance improvements and efficiency in response generation."], "limitations": "", "keywords": ["language models", "table reasoning", "reinforcement learning", "human-computer interaction", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423.pdf", "abs": "https://arxiv.org/abs/2505.12423", "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC.", "AI": {"tldr": "This paper introduces Phase Shift Calibration (PSC), a module to enhance Rotary Position Embedding (RoPE) in large language models by optimizing the predefined frequency factors for better calibration and improved performance in various tasks.", "motivation": "Existing methods for enhancing context windows based on RoPE struggle with predefined factor selection due to the complexity of the search space.", "method": "The authors introduce PSC, which calibrates predefined frequencies from existing methods to improve their performance in extending context windows.", "result": "Experiments show that using PSC leads to decreased perplexity as the context window size increases, indicating improved performance and robustness across several models and tasks.", "conclusion": "PSC significantly enhances the efficacy of various existing methods for context expansion in language models, demonstrating broad applicability and robustness.", "key_contributions": ["Introduction of Phase Shift Calibration (PSC) module for RoPE optimization.", "Demonstration of PSC's effectiveness on multiple models and tasks.", "Showcasing improvements in perplexity with increased context window sizes."], "limitations": "", "keywords": ["Rotary Position Embedding", "Phase Shift Calibration", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439.pdf", "abs": "https://arxiv.org/abs/2505.12439", "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "authors": ["Jinming Zhang", "Yunfei Long"], "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments.", "AI": {"tldr": "This paper introduces the LPLH framework, which helps Large Language Models play Interactive Fiction games more like humans by learning narrative context, action commands, and improving decision-making through feedback.", "motivation": "To improve the performance of AI agents in Interactive Fiction games by focusing on human-like comprehension instead of just task-specific performance.", "method": "The LPLH framework consists of structured map building for narrative relationships, action learning for context-appropriate commands, and feedback-driven experience analysis for refining decision-making.", "result": "The framework enables LLM-based agents to exhibit more interpretable and human-like behaviors, improving their interaction within narrative worlds.", "conclusion": "By integrating cognitive science principles, the LPLH framework supports robust, context-aware gameplay in complex text-based environments.", "key_contributions": ["Introduction of a cognitive-inspired framework for IF games", "Integration of structured map building and action learning", "Feedback-driven experience refinement for decision-making"], "limitations": "", "keywords": ["Interactive Fiction", "Large Language Models", "Cognitive Science", "Narrative Context", "Game AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452.pdf", "abs": "https://arxiv.org/abs/2505.12452", "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized.", "AI": {"tldr": "The paper introduces self-questioning as a method to enhance large language models' understanding by leveraging their latent knowledge, evaluated through a benchmark of computer science patents.", "motivation": "To address the limitations of conceptual understanding in large language models (LLMs) and improve their access to latent knowledge, particularly in complex domains.", "method": "The study proposes using self-questioning as a prompting technique, coupled with a benchmark task involving differentiation between closely related patents, while evaluating LLM performance with and without external scientific text.", "result": "Self-questioning significantly enhances LLM performance in understanding fine-grained semantic distinctions, and retrieving answers from external texts provides further improvements.", "conclusion": "Self-questioning is a practical approach for improving LLM comprehension and revealing how internal knowledge is organized; smaller models are found to generate better questions for mid-sized models.", "key_contributions": ["Introduction of self-questioning as a mechanism for LLM comprehension improvement", "Development of a benchmark using computer science patents for evaluation", "Discovery of effective cross-model collaboration strategies"], "limitations": "", "keywords": ["large language models", "self-questioning", "machine learning", "computer science patents", "understanding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454.pdf", "abs": "https://arxiv.org/abs/2505.12454", "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods.", "AI": {"tldr": "This paper explores the effectiveness of Distantly Supervised Named Entity Recognition (DS-NER) using various annotation methods and presents a novel framework for noise assessment.", "motivation": "To improve the effectiveness of DS-NER in generating training data automatically and to address the issue of noise in distant supervision methods.", "method": "The authors analyze distant annotation techniques, including both rule-based methods and large language model supervision, while introducing a framework that categorizes noise into unlabeled-entity and noisy-entity problems to provide tailored solutions.", "result": "The proposed method shows significant improvements across eight real-world distant supervision datasets, outperforming state-of-the-art approaches in DS-NER.", "conclusion": "The study confirms the robustness and effectiveness of the proposed DS-NER method and noise assessment framework, suggesting its potential for broader applications in named entity recognition.", "key_contributions": ["Introduction of a novel framework for noise assessment in DS-NER techniques.", "Comparative analysis of traditional and large language model-based annotation methods.", "Demonstration of significant performance improvements across diverse datasets."], "limitations": "", "keywords": ["Distant Supervision", "Named Entity Recognition", "Noise Assessment", "Large Language Models", "Annotation Techniques"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474.pdf", "abs": "https://arxiv.org/abs/2505.12474", "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task.", "AI": {"tldr": "This study evaluates the performance of LLMs on a summarization task that integrates discussion with background knowledge, highlighting significant limitations in their ability to generate comprehensive summaries.", "motivation": "To address confusion in dialogue summarization caused by reliance only on discussion information, this work aims to combine discussion with background knowledge in summarization tasks.", "method": "The paper models task output as background and opinion summaries, introduces standardized summarization patterns, and establishes a benchmark with samples annotated by human experts, alongside a novel hierarchical evaluation framework.", "result": "Findings indicate LLMs struggle with background summary retrieval and opinion summary integration, with top models achieving less than 69% performance across patterns and showing insufficient self-evaluation/correction capabilities.", "conclusion": "The results highlight the need for improvements in LLMs to enhance their summarization abilities, especially in integrating background knowledge and handling complex summarization tasks.", "key_contributions": ["Introduction of a novel summarization task combining background knowledge and discussion", "Development of a benchmark with high-quality human-annotated samples", "Proposal of a hierarchical evaluation framework with fine-grained metrics"], "limitations": "Current LLMs demonstrate inadequate capabilities for self-evaluation and self-correction in the summarization task.", "keywords": ["LLMs", "summarization", "evaluation framework", "background knowledge", "opinion summaries"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476.pdf", "abs": "https://arxiv.org/abs/2505.12476", "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.", "AI": {"tldr": "This paper presents the Reward-guided Tree Search on Graph (RTSoG), a novel framework that improves Knowledge Graph Question Answering (KGQA) by effectively handling complex question semantics and optimizing reasoning paths.", "motivation": "Existing KGQA methods overlook historical reasoning paths and often retrieve inaccurate paths due to complex question semantics, leading to sub-optimal performance.", "method": "The RTSoG framework decomposes questions into simpler sub-questions and employs a Self-Critic Monte Carlo Tree Search guided by a reward model to retrieve and weight reasoning paths, generating answers from these paths.", "result": "RTSoG achieves 8.7% and 7.0% performance improvements over state-of-the-art methods on the GrailQA and WebQSP datasets respectively.", "conclusion": "RTSoG effectively addresses the limitations of previous KGQA approaches by improving question handling and reasoning path retrieval, leading to enhanced performance in answering complex queries.", "key_contributions": ["Introduction of Reward-guided Tree Search framework for KGQA", "Utilization of Self-Critic Monte Carlo Tree Search for path retrieval", "Significant performance improvements on benchmark datasets."], "limitations": "", "keywords": ["Knowledge Graphs", "Question Answering", "Large Language Models", "Monte Carlo Tree Search", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495.pdf", "abs": "https://arxiv.org/abs/2505.12495", "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations.", "AI": {"tldr": "This paper introduces KG-QAGen, a framework for generating question-answer pairs of varying complexity to evaluate long-context language models, revealing their limitations in handling complex queries.", "motivation": "The need for a systematic evaluation of language models' long-context capabilities, particularly regarding question complexity.", "method": "The framework extracts QA pairs at multiple complexity levels and leverages structured representations of financial agreements across dimensions like multi-hop retrieval and answer plurality.", "result": "A dataset of 20,139 QA pairs was created, and evaluation of 13 LLMs showed struggles in set-based comparisons and multi-hop inference.", "conclusion": "The analysis indicates that even top-performing models face systematic failures in semantic interpretation and handling implicit relations.", "key_contributions": ["Introduction of KG-QAGen framework for QA generation", "Creation of the largest QA dataset among long-context benchmarks", "Insights into failure modes of existing LLMs in complex query handling"], "limitations": "The open-sourced part of the dataset is only a portion of the entire dataset, potentially limiting broader applicability.", "keywords": ["long-context", "language models", "question-answer generation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507.pdf", "abs": "https://arxiv.org/abs/2505.12507", "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT.", "AI": {"tldr": "This paper introduces LM$^2$otifs, an explainable framework for detecting machine-generated texts using Graph Neural Networks and interpretable motifs.", "motivation": "To address challenges in authorship authentication due to the impressive capabilities of large language models, especially concerning the explainability of detection methods.", "method": "The LM$^2$otifs framework transforms text into graphs based on word co-occurrence, employs Graph Neural Networks for predictions, and uses a post-hoc method to extract interpretable motifs.", "result": "LM$^2$otifs demonstrates effective detection of machine-generated texts, outperforming traditional methods in explainability and interpretability with empirical evidence supporting its effectiveness.", "conclusion": "The framework not only effectively distinguishes between human and machine-generated texts but also provides valuable multi-level explanations in the form of linguistic motifs.", "key_contributions": ["Introduction of LM$^2$otifs framework for MGT detection", "Utilization of explainable Graph Neural Networks for better interpretability", "Empirical evaluation showcasing its effectiveness on benchmark datasets"], "limitations": "", "keywords": ["explainability", "large language models", "machine-generated texts", "graph neural networks", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511.pdf", "abs": "https://arxiv.org/abs/2505.12511", "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities.", "AI": {"tldr": "DS-ProGen is a dual-structure deep language model for functional protein design that integrates backbone geometry and surface-level representations to improve sequence prediction.", "motivation": "The motivation behind this research is to overcome the limitations of existing protein design methods that rely on single structural features, which restrict their effectiveness in accurately predicting amino acid sequences for specific 3D conformations.", "method": "The authors developed DS-ProGen, which employs both backbone coordinates and surface chemical and geometric descriptors within a next-amino-acid prediction framework to generate viable protein sequences.", "result": "DS-ProGen achieves a state-of-the-art recovery rate of 61.47% on the PRIDE dataset and demonstrates superior capability in predicting interactions with various biological partners.", "conclusion": "The findings suggest that a multi-modal approach to structural encoding significantly enhances the effectiveness of protein design by capturing complex chemical and geometric constraints.", "key_contributions": ["Introduction of DS-ProGen, a novel dual-structure deep language model for protein design.", "Demonstration of the benefits of integrating backbone coordinates with surface descriptors.", "Achievement of state-of-the-art results in protein sequence recovery and interaction prediction."], "limitations": "", "keywords": ["protein folding", "deep learning", "structural biology", "functional design", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531.pdf", "abs": "https://arxiv.org/abs/2505.12531", "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "authors": ["Navid Madani", "Rohini Srihari"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI.", "AI": {"tldr": "ESC-Judge is an end-to-end evaluation framework for emotional-support LLMs that automates performance comparison based on the Exploration-Insight-Action counseling model.", "motivation": "To establish a scalable and theory-grounded method for evaluating the effectiveness of emotional-support LLMs used in mental health applications.", "method": "ESC-Judge evaluates models in three stages: synthesizing help-seeker roles based on salient attributes, conducting sessions with different support agents, and using a specialized judge LLM for pairwise skill comparisons.", "result": "ESC-Judge achieved 85% agreement with PhD-level annotators on Exploration, 83% on Insight, and 86% on Action decisions, indicating high reliability.", "conclusion": "ESC-Judge provides a cost-effective, automated evaluation method that aligns closely with human judgments, promoting transparency in developing emotionally supportive AI.", "key_contributions": ["First end-to-end evaluation framework for emotional-support LLMs", "Automates evaluation pipeline at scale", "Grounds comparisons in an established counseling model"], "limitations": "", "keywords": ["large language models", "mental health", "evaluation framework", "automated assessment", "emotional support"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533.pdf", "abs": "https://arxiv.org/abs/2505.12533", "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "GÃ¡bor Recski", "Terra Blevins"], "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability.", "AI": {"tldr": "This paper investigates the generalization abilities of relation extraction models, revealing that they often overfit to dataset-specific patterns rather than learning robust relationships, with implications for transfer learning and adaptation strategies.", "motivation": "To evaluate how well relation extraction models generalize to unseen data and determine the factors that influence their performance across different datasets.", "method": "Cross-dataset experiments were conducted to assess the performance of relation extraction models under various conditions, considering factors like data quality and adaptation strategies.", "result": "The experiments demonstrated that higher intra-dataset performance does not guarantee better transferability and highlighted the significance of data quality for robust model performance.", "conclusion": "The study concludes that adapting to data quality is crucial for effective model performance, and structural issues in benchmarks detract from transferability. Fine-tuning is preferable for high-quality data while few-shot in-context learning works better for noisier data.", "key_contributions": ["Identify the relationship between intra-dataset performance and transferability of relation extraction models.", "Demonstrate the importance of data quality over lexical similarity in achieving robust performance.", "Highlight structural issues in relation extraction benchmarks that affect model transferability."], "limitations": "The findings may not generalize to all relation extraction contexts and depend on the specific datasets used in the study.", "keywords": ["relation extraction", "generalization", "data quality", "transfer learning", "few-shot learning"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543.pdf", "abs": "https://arxiv.org/abs/2505.12543", "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems.", "AI": {"tldr": "This paper reviews ambiguity in NLP and its implications for LLMs, especially in Conversational Question Answering (CQA), proposing disambiguation approaches and future research directions.", "motivation": "Ambiguity is a significant challenge in NLP, worsened by the complexity of human language and the new capabilities of LLMs. Understanding and resolving ambiguity is crucial for improved language systems.", "method": "The paper categorizes disambiguation approaches enabled by LLMs, provides a comparative analysis of their advantages and disadvantages, and explores datasets for benchmarking ambiguity detection and resolution.", "result": "A comprehensive review of current research on ambiguities and disambiguation is presented, highlighting the pros and cons of various approaches and available datasets.", "conclusion": "The findings aim to guide ongoing research and improve LLM performance in ambiguity resolution, suggesting areas for future investigation.", "key_contributions": ["Comprehensive categorization of ambiguity in NLP and its implications for LLMs.", "Comparative analysis of various disambiguation techniques.", "Identification of open problems and future research directions."], "limitations": "", "keywords": ["Natural Language Processing", "ambiguity", "Large Language Models", "Conversational Question Answering", "disambiguation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545.pdf", "abs": "https://arxiv.org/abs/2505.12545", "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes.", "AI": {"tldr": "TrafficSafe introduces a multi-modal framework leveraging LLMs for improved crash prediction and feature attribution, achieving significant performance gains and providing insights into traffic safety.", "motivation": "To enhance traffic safety by accurately predicting crash events and identifying risk factors through a sophisticated analysis of diverse traffic data.", "method": "Developed TrafficSafe, a framework that utilizes LLMs for text-based reasoning on a multi-modal traffic crash dataset, and established TrafficSafe Attribution for feature interpretation.", "result": "TrafficSafe LLM achieved a 42% average improvement in F1-score over baselines, revealing key factors like alcohol-impaired driving as major contributors to severe crashes.", "conclusion": "TrafficSafe represents a significant advancement in traffic safety research, applying AI technologies for actionable insights and enhanced data collection strategies.", "key_contributions": ["Introduction of TrafficSafe framework for crash prediction using LLMs", "Development of TrafficSafe Attribution for feature interpretation", "Insights into drivers' behavior as major crash risk factors"], "limitations": "", "keywords": ["traffic safety", "crash prediction", "large language models", "feature attribution", "multi-modal data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546.pdf", "abs": "https://arxiv.org/abs/2505.12546", "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side.", "AI": {"tldr": "The paper examines the extent to which large language models (LLMs) have memorized copyrighted texts, revealing complex relationships between memorization and copyright implications.", "motivation": "To clarify the misconceptions surrounding the memorization of copyrighted content by LLMs in the context of copyright lawsuits.", "method": "The authors employ a probabilistic extraction technique to analyze 13 open-weight LLMs, extracting text from the Books3 dataset across different models.", "result": "The study demonstrates that while some LLMs, like Llama 3.1 70B, can memorize significant portions of certain books, the overall extent of memorization varies widely among different models and texts.", "conclusion": "The findings have important implications for copyright litigation, challenging the binary interpretations of memorization in copyright cases without aligning strictly with either plaintiffs or defendants.", "key_contributions": ["Development of a probabilistic extraction method applied to LLMs", "Empirical evidence of varying memorization capacities across LLMs", "Insights into the impact of LLM memorization on copyright law"], "limitations": "The results are model and text dependent, and further exploration is needed to generalize findings to other contexts or models.", "keywords": ["large language models", "copyright law", "memorization", "generative AI", "adversarial ML"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560.pdf", "abs": "https://arxiv.org/abs/2505.12560", "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "authors": ["Hiram Ring"], "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub.", "AI": {"tldr": "Development of a large automatically tagged parallel dataset (taggedPBC) for over 1,500 languages to aid crosslinguistic investigations.", "motivation": "Existing datasets for crosslinguistic studies are limited either in the number of languages or the amount of data available, which restricts understanding of universal language properties.", "method": "Creation of a tagged parallel corpus with over 1,800 sentences from more than 1,500 languages, measuring tag accuracy against existing state-of-the-art taggers and developing a novel measure (N1 ratio) for assessing word order.", "result": "The taggedPBC shows strong correlation in tag accuracy with existing high-resource language taggers and enables a Gaussian Naive Bayes classifier to predict basic word order for languages not well-represented in existing databases.", "conclusion": "The taggedPBC is a significant resource for crosslinguistic research, although further development is required; it is accessible for collaborative research on GitHub.", "key_contributions": ["Development of the taggedPBC dataset with diverse language representation", "Demonstration of tag accuracy correlating with state-of-the-art tools", "Introduction of the N1 ratio for word order identification"], "limitations": "Further work is needed to expand and develop the dataset.", "keywords": ["crosslinguistic investigations", "tagged corpus", "N1 ratio", "word order", "language families"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568.pdf", "abs": "https://arxiv.org/abs/2505.12568", "title": "Enriching Patent Claim Generation with European Patent Dataset", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch.", "AI": {"tldr": "The paper introduces the European Patent Dataset (EPD), which enhances the use of LLMs for drafting patent claims by providing jurisdictionally diverse, high-quality patent texts and structured metadata.", "motivation": "To assist inventors in writing patent claims efficiently and to expand the existing research on patent drafting by incorporating European legal standards and drafting conventions.", "method": "The authors introduce a dataset called EPD, which includes high-quality granted patents from European jurisdictions, designed for various patent-related tasks, and conduct experiments to evaluate the performance of LLMs trained on this dataset.", "result": "Experiments demonstrate that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets, including GPT-4, in terms of claim quality and adaptability across domains.", "conclusion": "The study highlights the superiority of the EPD dataset for training LLMs in patent claims generation and emphasizes the challenges of simulating real-world scenarios.", "key_contributions": ["Introduction of a comprehensive European patent dataset (EPD)", "Demonstration of significant improvements in claim quality using LLMs trained on EPD", "Identification of the need for future research addressing real-world challenges in claim generation."], "limitations": "The dataset may be limited to specific European jurisdictions, potentially omitting data from other important patent offices globally.", "keywords": ["patent claims", "large language models", "dataset", "European patents", "claim generation"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572.pdf", "abs": "https://arxiv.org/abs/2505.12572", "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "authors": ["Hanwen Shen", "Ting Ying"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels.", "AI": {"tldr": "This paper explores the optimal human-authored outline length required for LLMs to generate high-quality million-word novels.", "motivation": "To address the challenge of generating coherent ultra-long novels using Large Language Models, as previous frameworks only cover shorter lengths.", "method": "The authors employ an information-theoretic analysis and introduce a hierarchical two-stage generation pipeline: outline -> detailed outline -> manuscript.", "result": "The study demonstrates that a two-stage hierarchical outline approach significantly reduces semantic distortion in ultra-long novels compared to single-stage methods.", "conclusion": "The findings offer valuable insights for authors and researchers on effective collaboration with LLMs in novel writing.", "key_contributions": ["Quantifies distortion in LLM-generated novels based on outline length.", "Introduces a two-stage hierarchical generation pipeline for novel writing.", "Provides empirical guidance for authors using LLMs to write ultra-long novels."], "limitations": "Focused on Chinese novels; generalizability to other languages or genres may vary.", "keywords": ["Large Language Models", "novel writing", "information-theoretic analysis", "semantic distortion", "hierarchical generation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584.pdf", "abs": "https://arxiv.org/abs/2505.12584", "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "AI": {"tldr": "This paper explores how large language models (LLMs) handle non-English tokens in their layer representations, introducing a method to enhance performance using representation steering.", "motivation": "Understanding how LLMs process non-English tokens is crucial for improving their multilingual capabilities.", "method": "The authors utilize representation steering by adding a learned vector to the activations of a single layer in the model, analyzing its impact on performance.", "result": "The proposed method shows comparable results to translation baselines and significantly outperforms state-of-the-art prompt optimization techniques.", "conclusion": "Steering a single layer can greatly enhance LLM performance for multilingual tasks, especially when combined with supervised fine-tuning and reinforcement learning from human feedback.", "key_contributions": ["Demonstration of representation steering to enhance LLM performance.", "Comparison against translation baselines and prompt optimization methods.", "Insights into the effects of fine-tuning techniques on multilingual capabilities."], "limitations": "", "keywords": ["large language models", "multilingual capabilities", "representation steering", "prompt optimization", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587.pdf", "abs": "https://arxiv.org/abs/2505.12587", "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages.", "AI": {"tldr": "CMLFormer is a dual-decoder Transformer model designed for code-mixed languages, showing improved performance on the HASOC-2021 benchmark.", "motivation": "Standard language models struggle with code-mixed languages that frequently transition between languages within sentences.", "method": "CMLFormer utilizes a multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, pre-trained on an augmented Hinglish corpus.", "result": "CMLFormer shows improvements in F1 score, precision, and accuracy over existing approaches, specifically on the HASOC-2021 benchmark.", "conclusion": "CMLFormer effectively models the complexities of code-mixed languages through its architecture and multi-task pre-training strategy.", "key_contributions": ["Introduction of CMLFormer architecture for code-mixed text.", "Enhanced pre-training objectives capturing code-mixing dynamics.", "Demonstrated success on benchmark tasks with improved evaluation metrics."], "limitations": "", "keywords": ["Code-mixed languages", "Transformer", "Natural Language Processing", "Multi-task learning", "Hinglish"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592.pdf", "abs": "https://arxiv.org/abs/2505.12592", "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts.", "AI": {"tldr": "This paper introduces PromptPrism, a taxonomy for systematic prompt analysis of large language models (LLMs).", "motivation": "The need for a comprehensive framework for analyzing prompt structure to enhance LLM performance and understand behavior.", "method": "Developed a taxonomy with three levels: functional structure, semantic component, and syntactic pattern. Applied it to prompt refinement, dataset profiling, and sensitivity analysis.", "result": "Demonstrated that PromptPrism effectively improves prompt quality and model performance; validated through three practical applications.", "conclusion": "PromptPrism provides a foundational tool for refining, profiling, and analyzing prompts in LLM applications.", "key_contributions": ["Introduction of a comprehensive taxonomy for prompt analysis", "Demonstration of prompt refinement that enhances LLM performance", "Development of a dataset profiling method for analyzing prompt characteristics"], "limitations": "", "keywords": ["Prompt analysis", "Large language models", "Taxonomy", "Prompt refinement", "Machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594.pdf", "abs": "https://arxiv.org/abs/2505.12594", "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "AI": {"tldr": "AD-AGENT is an LLM-driven multi-agent framework that automates anomaly detection (AD) pipelines from natural language instructions.", "motivation": "Anomaly detection is crucial in various fields, but the complexity of tools and libraries makes it difficult for non-experts to utilize them effectively.", "method": "AD-AGENT employs specialized agents to handle intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging, integrating libraries like PyOD and TSLib into a cohesive system.", "result": "AD-AGENT demonstrates reliable script generation and competitive model recommendations across different AD libraries in experimental setups.", "conclusion": "The open-sourced AD-AGENT enables users to efficiently implement anomaly detection processes, promoting further research and practical applications.", "key_contributions": ["Introduction of an LLM-driven framework that makes anomaly detection accessible to non-experts", "Integration of various AD libraries into a single workflow", "Providing documentation and debugging in a user-friendly manner"], "limitations": "", "keywords": ["Anomaly Detection", "Machine Learning", "Natural Language Processing", "Automated Systems", "Multi-agent Framework"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616.pdf", "abs": "https://arxiv.org/abs/2505.12616", "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "authors": ["Shujauddin Syed", "Ted Pedersen"], "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios.", "AI": {"tldr": "This paper introduces a TF-IDF-based claim retrieval system for the SemEval-2025 Task 7, achieving moderate success in multilingual contexts while highlighting the competitive nature of traditional methods against advanced neural architectures.", "motivation": "The paper aims to address multilingual and crosslingual fact-checked claim retrieval challenges, particularly in the context of the SemEval-2025 competition.", "method": "We implemented a TF-IDF-based retrieval system experimenting with vector dimensions and tokenization strategies, ultimately using word-level tokenization with a vocabulary size of 15,000 features.", "result": "The best configuration achieved an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages, with stronger performance on higher-resource languages.", "conclusion": "While neural architectures dominate multilingual retrieval tasks, optimized traditional methods like TF-IDF can still serve as effective baselines, especially with limited resources.", "key_contributions": ["Introduction of a TF-IDF-based approach for multilingual claim retrieval", "Performance benchmarks across ten languages", "Insights on the competitiveness of traditional vs. neural methods"], "limitations": "Significantly lagged behind the top-ranked system which achieved an average score of 0.96.", "keywords": ["Multilingual Retrieval", "Crosslingual Retrieval", "Fact-Checking", "TF-IDF", "Natural Language Processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621.pdf", "abs": "https://arxiv.org/abs/2505.12621", "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "authors": ["JoÃ£o Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.", "AI": {"tldr": "This paper proposes a sentence-level pre-attribution method for Retrieve-Augmented Generation (RAG) systems to improve the accuracy and reliability of outputs generated by Large Language Models (LLMs) in scientific settings.", "motivation": "To address the challenge of unreliable source attribution in outputs generated by LLMs, which inhibits their adoption in scientific domains that require accountability and traceability.", "method": "The authors propose a pre-attribution step that classifies sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes, allowing for appropriate attribution methods to be selected based on the sentence type.", "result": "The paper shows that the proposed classifiers effectively determine sentence attribution, reducing the computational burden of the attribution process.", "conclusion": "The end-to-end attribution system developed can operate seamlessly, providing a reliable framework for integrating sentence-level attribution in LLM outputs for scientific purposes.", "key_contributions": ["Introduction of a sentence-level pre-attribution step for RAG systems.", "Development of a clean version of the HAGRID dataset.", "End-to-end attribution system that functions out of the box."], "limitations": "The effectiveness of the proposed method may be contingent on the quality of the training data and the specific domains it is applied to.", "keywords": ["Large Language Models", "Retrieve-Augmented Generation", "sentence-level attribution"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625.pdf", "abs": "https://arxiv.org/abs/2505.12625", "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "AI": {"tldr": "This paper investigates censorship behavior exhibited by the R1 large language model, particularly on politically sensitive topics, and explores methods to bypass this censorship.", "motivation": "To understand the censorship patterns of the R1 language model in relation to politically sensitive topics and its implications for transparency and governance in AI.", "method": "The authors created a large-scale set of highly curated prompts that are censored by R1 but not by other models, and conducted a comprehensive analysis of R1's censorship behavior across various topics and languages.", "result": "The analysis revealed consistent patterns of censorship in R1, influenced by prompt phrasing and context, and exposed potential biases and lack of transparency in the model's design.", "conclusion": "The findings indicate that censorship in R1 may stem from design choices during its training, raising significant concerns for the deployment of language models in sensitive areas.", "key_contributions": ["Identification of censorship patterns in the R1 model", "Introduction of a curated set of prompts that reveal censorship", "Proposed techniques for bypassing or removing censorship in language models."], "limitations": "", "keywords": ["censorship", "large language model", "politically sensitive topics", "AI governance", "language model transparency"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636.pdf", "abs": "https://arxiv.org/abs/2505.12636", "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here.", "AI": {"tldr": "This paper critiques knowledge editing methods for language models, introducing the concept of \"superficial editing\" and identifying key factors contributing to knowledge retention despite editing efforts.", "motivation": "To address the shortcomings in existing knowledge editing algorithms, which often fail to prevent models from generating original or outdated knowledge.", "method": "The authors systematically evaluate knowledge editing and identify factors related to model layers and attention modules that contribute to superficial editing, along with proposing a framework for superficial unlearning.", "result": "Key findings indicate that specific attention heads in later layers retain original knowledge and have a causal relationship with the issue of superficial editing.", "conclusion": "The findings underscore the need for improved knowledge editing approaches that account for these identified factors, with implications for the robustness of models in real-world applications.", "key_contributions": ["Introduction of the concept of superficial editing in language models.", "Identification of factors related to attention modules and residual streams that affect knowledge retention.", "Analysis of superficial unlearning showcasing consistent behavior patterns in attention heads."], "limitations": "The study focuses primarily on specific models and attention mechanisms, which may limit generalizability to other architectures.", "keywords": ["knowledge editing", "superficial editing", "attention modules", "language models", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654.pdf", "abs": "https://arxiv.org/abs/2505.12654", "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "AI": {"tldr": "The paper presents a multi-modal approach for predicting turn-taking and backchannel actions in human-machine conversations using a newly collected dataset and an end-to-end prediction framework.", "motivation": "There is a gap in predicting turn-taking and backchannel actions in human-machine conversations, especially with existing datasets being limited.", "method": "An automatic data collection pipeline was developed to gather and annotate over 210 hours of human conversation videos, resulting in the creation of a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset. An end-to-end framework was then proposed to predict turn-taking and backchannel actions using multi-modal signals.", "result": "The approach achieved state-of-the-art performance, with a 10% increase in F1-score for turn-taking and a 33% increase for backchannel prediction tasks.", "conclusion": "The dataset and code are publicly available for further research, contributing significantly to the field of human-machine interaction.", "key_contributions": ["Development of a large-scale Multi-Modal Face-to-Face dataset with extensive annotations", "Introduction of an end-to-end framework for predicting turn-taking and backchannel actions", "Demonstration of state-of-the-art performance in multi-modal prediction tasks"], "limitations": "", "keywords": ["turn-taking", "backchannel prediction", "multi-modal signals", "human-machine interaction", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662.pdf", "abs": "https://arxiv.org/abs/2505.12662", "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "AI": {"tldr": "The paper introduces Know3-RAG, a knowledge-aware framework to improve factual reliability in RAG by leveraging knowledge graphs, addressing limitations of existing systems.", "motivation": "To enhance the factual reliability of natural language generation by addressing hallucinations and unreliable adaptive control in RAG systems.", "method": "Know3-RAG integrates structured knowledge from knowledge graphs at three stages: retrieval, generation, and filtering. It uses KG embeddings to assess answer confidence, enriches queries with KG-derived entities, and applies a filtering mechanism for alignment and accuracy.", "result": "Experiments show that Know3-RAG outperforms existing RAG baselines, significantly reducing hallucinations and improving the reliability of generated answers on open-domain QA benchmarks.", "conclusion": "The proposed framework effectively enhances the performance of RAG systems, addressing key challenges of reliability and factual accuracy.", "key_contributions": ["Introduction of a knowledge-aware adaptive retrieval module", "Development of a knowledge-enhanced reference generation strategy", "Implementation of a knowledge-driven reference filtering mechanism"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Knowledge Graphs", "Natural Language Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716.pdf", "abs": "https://arxiv.org/abs/2505.12716", "title": "Shadow-FT: Tuning Instruct via Base", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "AI": {"tldr": "Proposes a novel framework called Shadow-FT for fine-tuning INSTRUCT models by leveraging BASE model updates, leading to significant performance improvements without increasing parameters.", "motivation": "To address the limitations of directly tuning INSTRUCT models, which often results in marginal improvements or performance degradation.", "method": "Shadow-FT fine-tunes the BASE model and grafts the learned weight updates onto the INSTRUCT model without adding extra parameters.", "result": "Extensive experiments show that Shadow-FT outperforms traditional fine-tuning methods across 19 benchmarks, improving performance on coding, reasoning, and mathematical tasks.", "conclusion": "Shadow-FT is a practical solution for tuning LLMs and shows potential for application in multimodal models and with direct preference optimization.", "key_contributions": ["Introduction of the Shadow-FT framework for fine-tuning INSTRUCT models.", "Consistency in improved performance over traditional methods.", "Applicability to multimodal LLMs and integration with DPO."], "limitations": "", "keywords": ["Large Language Models", "Fine-Tuning", "INSTRUCT Models", "Machine Learning", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717.pdf", "abs": "https://arxiv.org/abs/2505.12717", "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "AI": {"tldr": "This paper introduces a novel on-policy reinforcement learning framework, Tree-of-Thoughts RL (ToTRL), which enhances large language model (LLM) reasoning by modeling it as tree structures, improving performance and reducing costs in complex reasoning tasks.", "motivation": "To enhance the reasoning capabilities of large language models (LLMs) which exhibit limitations in prolonged chain-of-thought (CoT) reasoning, especially in terms of verbosity and inefficient trial-and-error methods.", "method": "The paper proposes Tree-of-Thoughts RL (ToTRL), an on-policy RL framework that utilizes a rule-based reward system, enabling LLMs to engage in parallel reasoning by managing a tree structure of thought processes.", "result": "Empirical evaluations show that the ToTQwen3-8B model trained with ToTRL demonstrates significant improvements in reasoning efficiency and performance on complex reasoning tasks compared to traditional methods.", "conclusion": "The introduction of ToTRL represents a conceptual advancement in LLM reasoning by allowing for more systematic exploration of reasoning paths, leading to better decision-making in task environments.", "key_contributions": ["Introduction of Tree-of-Thoughts RL (ToTRL) framework for enhancing LLM reasoning.", "Modeling reasoning as a tree structure for parallel exploration and evaluation.", "Demonstrated significant performance improvements on complex reasoning tasks."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "reasoning", "tree-of-thought", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718.pdf", "abs": "https://arxiv.org/abs/2505.12718", "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "AI": {"tldr": "The study presents an automated method for assessing biases in AI-generated educational content, showing high reliability and alignment with manual assessments.", "motivation": "To address ethical concerns regarding biases in educational materials produced by Generative AI.", "method": "An automated bias assessment approach combining the Contextualized Embedding Association Test and a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework.", "result": "The method demonstrated a high Pearson correlation coefficient of r = 0.993 with manually curated word sets, indicating reliable bias assessment.", "conclusion": "The proposed method enhances fairness, scalability, and reproducibility in auditing AI-generated educational content, reducing human subjectivity.", "key_contributions": ["Automated approach to assess biases in AI-generated educational materials.", "High correlation with manual bias assessment methods.", "Improves fairness and reproducibility in educational content evaluation."], "limitations": "", "keywords": ["Generative AI", "bias assessment", "educational content", "human-computer interaction", "automated evaluation"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723.pdf", "abs": "https://arxiv.org/abs/2505.12723", "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "AI": {"tldr": "Introduces OORL, a novel RL framework for training LLMs in code translation tasks to improve performance across various programming languages.", "motivation": "To address the performance disparity in LLMs for code generation across different programming languages.", "method": "The paper utilizes a novel reinforcement learning framework, OORL, which combines on-policy and off-policy strategies for code translation. It incorporates a rule-based reward derived from unit tests and introduces Group Equivalent Preference Optimization (GEPO) for training using intermediate representations of code.", "result": "Extensive experiments show that training LLMs with OORL in code translation significantly improves their ability to recognize code functionality and relationships between different programming languages, achieving enhancements across multiple benchmarks.", "conclusion": "The proposed OORL framework effectively bridges the capability gap in code generation for various programming languages, advancing the performance of LLMs.", "key_contributions": ["Introduction of OORL for training LLMs in code translation tasks.", "Development of Group Equivalent Preference Optimization (GEPO) for more nuanced learning through intermediate representations.", "Demonstration of significant performance improvements across multiple programming languages on code benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Code Generation", "Reinforcement Learning", "Code Translation", "Group Equivalent Preference Optimization"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727.pdf", "abs": "https://arxiv.org/abs/2505.12727", "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "AI": {"tldr": "This paper presents a theory-informed corpus of human-chatbot interviews to classify mental-health stigma, addressing the limitations of existing datasets.", "motivation": "The research aims to address the pervasive issue of mental-health stigma that affects treatment-seeking behaviors and recovery, by providing a robust dataset to train neural models.", "method": "The authors created an expert-annotated corpus of 4,141 snippets derived from 684 participants' interviews, which include socio-cultural background information, and benchmarked state-of-the-art neural models on this data.", "result": "The experiments reveal empirical challenges in stigma detection and demonstrate the efficacy of the provided dataset for further research in this area.", "conclusion": "This dataset facilitates advancements in computational methods to detect, neutralize, and counteract mental-health stigma, filling a significant gap in existing resources.", "key_contributions": ["Creation of a theory-informed corpus for mental-health stigma classification.", "Empirical benchmarking of neural models for stigma detection.", "Detailed participant socio-cultural background information enhancing dataset richness."], "limitations": "The dataset may have limitations in generalizability due to its specific participant demographics.", "keywords": ["mental-health stigma", "neural models", "human-chatbot interactions", "dataset", "stigma detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768.pdf", "abs": "https://arxiv.org/abs/2505.12768", "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "AI": {"tldr": "ReEx-SQL introduces an execution-aware reasoning framework for Text-to-SQL that integrates feedback during query generation, improving accuracy and efficiency.", "motivation": "Existing Text-to-SQL methods fail to integrate execution feedback in real-time during query generation, leading to reduced accuracy.", "method": "ReEx-SQL allows models to engage with the database during decoding, utilizing execution feedback to inform and refine reasoning through structured prompts and a stepwise rollout strategy.", "result": "ReEx-SQL achieves 88.8% accuracy on Spider and 64.9% on BIRD, outperforming existing baselines and reducing inference time by 51.9% through tree-based decoding.", "conclusion": "The proposed method enhances reasoning accuracy and efficiency, indicating significant improvements over traditional approaches in generating SQL queries from text.", "key_contributions": ["Introduction of execution-aware reasoning for real-time adjustments", "Development of a composite reward function promoting effective database interaction", "Enhanced efficiency through tree-based decoding over linear approaches."], "limitations": "", "keywords": ["Text-to-SQL", "Execution Feedback", "Reinforcement Learning", "Database Interaction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781.pdf", "abs": "https://arxiv.org/abs/2505.12781", "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "AI": {"tldr": "This paper introduces Low-Rank Clone (LRC), an efficient pre-training method for Small Language Models (SLMs) that addresses information loss, representation alignment, and activation underutilization, achieving over 1,000x training efficiency.", "motivation": "The motivation behind LRC is to overcome challenges related to cost and performance in training Small Language Models (SLMs), particularly focusing on knowledge transfer from larger teacher models.", "method": "LRC constructs low-rank projection matrices for soft pruning and activation cloning, enabling better alignment of student and teacher model activations without explicit alignment modules.", "result": "LRC achieves performance that matches or surpasses state-of-the-art models while using significantly fewer training tokens, demonstrating over 1,000x training efficiency.", "conclusion": "The proposed LRC method effectively maximizes knowledge transfer and improves training efficiency of SLMs without the drawbacks of previous approaches.", "key_contributions": ["Introduction of Low-Rank Clone (LRC) for efficient SLM training.", "Unified design for soft pruning and activation cloning.", "Demonstrated over 1,000x training efficiency compared to traditional methods."], "limitations": "", "keywords": ["Small Language Models", "knowledge distillation", "low-rank matrices", "activation cloning", "training efficiency"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792.pdf", "abs": "https://arxiv.org/abs/2505.12792", "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "AI": {"tldr": "EAVIT is a framework for human value identification combining local and online LLMs to efficiently identify values in text, significantly reducing input token usage while improving accuracy.", "motivation": "To enhance human value identification in text data, addressing the limitations of existing LLMs in handling long contexts and reducing computational costs.", "method": "The EAVIT framework uses a small local language model as a value detector that generates initial estimations, which are refined using online LLMs through optimized input prompts.", "result": "EAVIT reduces the number of input tokens by up to 1/6 compared to direct queries to online LLMs and outperforms traditional NLP methods and earlier LLM-based strategies.", "conclusion": "EAVIT effectively improves the efficiency and accuracy of human value identification tasks by combining local fine-tuning with online LLM capabilities.", "key_contributions": ["Introduction of EAVIT framework for human value identification.", "Implementation of explanation-based training and data generation techniques for value detection.", "Innovative sampling strategies to optimize LLM input prompts."], "limitations": "", "keywords": ["human value identification", "large language models", "efficient NLP", "value detector", "explanation-based training"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808.pdf", "abs": "https://arxiv.org/abs/2505.12808", "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "AI": {"tldr": "Introduction of a decentralized framework for benchmarking large language models using collective intelligence.", "motivation": "The surge of large language models necessitates effective benchmarking that is scalable and mitigates biases from individual model assessments.", "method": "The proposed Decentralized Arena (dearena) uses democratic pairwise evaluation and a coarse-to-fine ranking algorithm to assess LLMs efficiently.", "result": "The framework showed up to 97% correlation with human judgments while significantly lowering evaluation costs across 66 LLMs.", "conclusion": "dearena provides a robust alternative for LLM benchmarking by leveraging collective intelligence and reducing reliance on biased models.", "key_contributions": ["Introduces Decentralized Arena for LLM benchmarking", "Utilizes democratic pairwise evaluation to mitigate biases", "Achieves high correlation with human evaluations while being cost-effective"], "limitations": "", "keywords": ["Large Language Models", "Benchmarking", "Decentralized Framework", "Collective Intelligence", "Automated Evaluation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814.pdf", "abs": "https://arxiv.org/abs/2505.12814", "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "AI": {"tldr": "PsyMem is a novel framework that enhances role-playing LLMs by integrating detailed psychological characteristics and explicit memory alignment, resulting in improved performance in simulating character interactions.", "motivation": "Current LLM-based role-playing methods inadequately model character dimensions and memory, limiting their reliability in applications like social simulation.", "method": "PsyMem integrates 26 psychological indicators for detailed character modeling and employs memory alignment training to ensure character responses are consistent with their memory during inference.", "result": "PsyMem-Qwen, trained on a dataset of 5,414 characters and 38,962 dialogues, outperforms baseline models in human-likeness and character fidelity, indicating better role-playing capabilities.", "conclusion": "The PsyMem framework significantly improves the reliability and performance of LLMs in various role-playing applications.", "key_contributions": ["Integration of fine-grained psychological attributes for character modeling.", "Explicit memory control through memory alignment training.", "Demonstrated superior performance over baseline models in role-playing tasks."], "limitations": "", "keywords": ["LLM", "role-playing", "memory alignment", "psychological attributes", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821.pdf", "abs": "https://arxiv.org/abs/2505.12821", "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "AI": {"tldr": "This paper introduces a Synthesize-then-Decode (SynDec) method for improving style transfer in Large Language Models by automating prompt generation and enhancing decoding processes.", "motivation": "The study addresses the limitations of current Large Language Models in arbitrary style transfer, specifically their dependence on manually crafted prompts and inherent stylistic biases.", "method": "The SynDec approach synthesizes prompts by selecting few-shot samples, performing a four-dimensional style analysis, and reranking candidates, then amplifies TST effects during decoding by maximizing probability contrasts.", "result": "SynDec significantly outperforms existing LLM-based methods on five out of six benchmarks, achieving up to a 9% increase in accuracy in style transfer tasks such as modern to Elizabethan English.", "conclusion": "The effectiveness of the SynDec approach is validated through extensive experiments and detailed ablation studies, showcasing substantial improvements in style transfer capabilities.", "key_contributions": ["Introduction of a novel approach for automatic prompt synthesis", "Enhanced decoding strategies to improve style transfer results", "Validation of the method through comprehensive experiments and ablation studies"], "limitations": "", "keywords": ["Large Language Models", "Style Transfer", "Prompt Synthesis", "Neural Networks", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831.pdf", "abs": "https://arxiv.org/abs/2505.12831", "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "AI": {"tldr": "The paper introduces a Contrastive Prompting (CP) method to enhance sentence embeddings extracted from large language models (LLMs) by contrasting with an auxiliary prompt to focus on core semantics and reduce non-essential information.", "motivation": "Existing techniques for extracting sentence embeddings from LLMs suffer from encoding excess non-essential information, which limits their effective use in tasks.", "method": "The proposed Contrastive Prompting (CP) method uses an auxiliary prompt in conjunction with the main prompt to drive LLMs to focus on core semantics during embedding extraction.", "result": "The CP method shows improved performance on Semantic Textual Similarity (STS) tasks and other downstream classification tasks across various LLMs.", "conclusion": "CP is a flexible, effective approach for enhancing sentence embeddings from LLMs without requiring fine-tuning or additional data, and the code will be made publicly available.", "key_contributions": ["Introduces a novel Contrastive Prompting method for better semantic encoding in LLMs.", "Demonstrates improved performance on STS and classification tasks.", "Provides a plug-and-play method that can enhance existing prompt-based embedding methods."], "limitations": "", "keywords": ["sentence embeddings", "large language models", "contrastive prompting", "semantic encoding", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835.pdf", "abs": "https://arxiv.org/abs/2505.12835", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "AI": {"tldr": "FlightGPT is a UAV Vision-and-Language Navigation framework that enhances multimodal perception and interpretability via a two-stage training pipeline and a Chain-of-Thought reasoning mechanism.", "motivation": "To improve UAV Vision-and-Language Navigation by addressing challenges like insufficient multimodal fusion, weak generalization, and poor interpretability in existing methods.", "method": "The framework employs a two-stage training pipeline consisting of Supervised Fine-Tuning using high-quality demonstrations followed by Group Relative Policy Optimization guided by a composite reward system.", "result": "FlightGPT achieved state-of-the-art performance on the CityNav dataset with a 9.22% higher success rate than the strongest baseline in unseen environments.", "conclusion": "FlightGPT demonstrates superior performance and improved interpretability in UAV navigation tasks, with the implementation available for public use.", "key_contributions": ["Development of the FlightGPT UAV VLN framework", "Introduction of a two-stage training pipeline", "Implementation of a Chain-of-Thought reasoning mechanism"], "limitations": "", "keywords": ["UAV", "Vision-Language Navigation", "multimodal perception", "Chain-of-Thought", "machine learning"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837.pdf", "abs": "https://arxiv.org/abs/2505.12837", "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "AI": {"tldr": "This paper explores how input text structuring and prompt engineering impact the performance of LLMs, particularly GPT-4o and GPT-4.1, on legal question-answering tasks.", "motivation": "To understand the impact of legal contract structures on the performance of large language models in question-answering tasks.", "method": "The study compares the exact-match accuracy of GPT-4o and GPT-4.1 across various input formats including well-structured text, cleaned text, Azure OCR outputs, and Markdown from GPT-4o Vision, along with different prompt engineering strategies.", "result": "GPT-4o shows robustness to input structure variations but falls short in performance. GPT-4.1's performance is sensitive to input structure, achieving a significant improvement in accuracy with well-structured inputs and optimized prompts.", "conclusion": "Effective input structuring and strategic prompt design are essential for enhancing LLM performance in legal contexts, especially for high-stakes applications.", "key_contributions": ["Demonstrated the impact of input structure on LLM performance in legal applications.", "Showed that GPT-4.1's performance can greatly improve with structured inputs and optimized prompts.", "Provided empirical evidence on the robustness of GPT-4o compared to GPT-4.1 under varying input conditions."], "limitations": "Limited to legal question-answering tasks and specific models (GPT-4o and GPT-4.1).", "keywords": ["LLM", "legal applications", "prompt engineering", "input structure", "GPT-4"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859.pdf", "abs": "https://arxiv.org/abs/2505.12859", "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge.", "AI": {"tldr": "This paper presents a RAG-inspired method for re-identifying masked personally identifiable information in text by leveraging background knowledge documents.", "motivation": "The need to assess the effectiveness and robustness of de-identification methods in protecting sensitive information.", "method": "The approach involves a two-step process: (1) using a retriever to select relevant background knowledge passages, and (2) employing an infilling model to infer and replace masked text spans iteratively.", "result": "The study reveals that up to 80% of de-identified text spans can be successfully recovered, with re-identification accuracy improving as the amount of background knowledge increases.", "conclusion": "De-identification methods may not be as secure as previously thought, highlighting the importance of assessing their robustness against re-identification attacks.", "key_contributions": ["Introduction of a novel RAG-inspired re-identification method", "Demonstration of high recovery rates for masked text spans", "Empirical evaluation on diverse datasets including clinical notes."], "limitations": "The study primarily focuses on specific datasets (Wikipedia biographies, court rulings, clinical notes), which may not generalize to all contexts of de-identification.", "keywords": ["de-identification", "re-identification", "RAG", "background knowledge", "text recovery"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864.pdf", "abs": "https://arxiv.org/abs/2505.12864", "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan HermstrÃ¼wer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "AI": {"tldr": "Introducing LEXam, a benchmark to evaluate legal reasoning in LLMs using real law exam questions.", "motivation": "To address the challenges LLMs face in long-form legal reasoning despite advancements in model scaling.", "method": "LEExam features a dataset of 4,886 law exam questions including guided open-ended questions and multiple-choice questions, evaluated by an LLM-as-a-Judge paradigm with human expert validation.", "result": "Current LLMs face significant challenges in responding to structured, multi-step legal reasoning tasks, particularly with long-form questions.", "conclusion": "The LEXam benchmark effectively distinguishes between models based on their legal reasoning capabilities and provides a scalable evaluation method.", "key_contributions": ["Introduction of LEXam benchmark for legal reasoning evaluation", "Dataset includes guided reasoning questions", "Implementation of LLM-as-a-Judge paradigm for consistent evaluation"], "limitations": "", "keywords": ["legal reasoning", "large language models", "benchmark", "LLM-as-a-Judge", "law exams"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888.pdf", "abs": "https://arxiv.org/abs/2505.12888", "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines.", "AI": {"tldr": "This paper presents a Graph-Assisted Prompts (GAP) framework to enhance dialogue-based medication recommendations using large language models by addressing challenges in multi-turn conversations and knowledge retrieval.", "motivation": "To improve the accuracy and safety of medical dialogue systems by addressing gaps in multi-turn dialogue interactions, particularly in medication recommendations.", "method": "The GAP framework constructs a patient-centric graph from dialogues, integrating external medical knowledge graphs to generate queries and prompts for effective information retrieval.", "result": "GAP demonstrates competitive performance on a medication recommendation dataset and shows potential in dynamic diagnostic interviewing scenarios, outperforming strong baseline methods.", "conclusion": "GAP successfully aids in reducing non-factual responses from LLMs and enhances the quality of medication recommendations in dialogue settings.", "key_contributions": ["Introduction of the GAP framework for medication recommendations", "Integration of patient-centric graphs with external medical knowledge", "Demonstration of performance improvements over existing baselines"], "limitations": "The framework's effectiveness may vary with the quality of the underlying knowledge graphs and may require extensive real-world validation.", "keywords": ["Medication recommendation", "Medical dialogue systems", "Large language models", "Graph-assisted prompts", "Health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896.pdf", "abs": "https://arxiv.org/abs/2505.12896", "title": "On the Thinking-Language Modeling Gap in Large Language Models", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "AI": {"tldr": "This paper introduces a novel prompt technique termed Language-of-Thoughts (LoT) to address biases in Large Language Models (LLMs) that hinder System 2 reasoning.", "motivation": "To address the significant gap between language modeling in LLMs and actual human thought processes, particularly regarding biases that mislead reasoning.", "method": "The authors propose the Language-of-Thoughts (LoT) prompting technique, which modifies the order and token usage of expressions to include all relevant information rather than just partial cues.", "result": "LoT significantly reduces language modeling biases in LLMs, leading to improved performance on various reasoning tasks.", "conclusion": "Implementing LoT enhances the elicitation of thoughts in LLMs, allowing for better alignment with human reasoning processes.", "key_contributions": ["Introduction of the Language-of-Thoughts (LoT) prompt technique", "Demonstration of the gap between language and thought modeling in LLMs", "Improved performance of LLMs in reasoning tasks by reducing biases."], "limitations": "", "keywords": ["System 2 reasoning", "Large Language Models", "Language-of-Thoughts", "Biases in LLMs", "Causal reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920.pdf", "abs": "https://arxiv.org/abs/2505.12920", "title": "PyFCG: Fluid Construction Grammar in Python", "authors": ["Paul Van Eecke", "Katrien Beuls"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "AI": {"tldr": "Introduction of PyFCG, a library for using Fluid Construction Grammar in Python.", "motivation": "The motivation behind this work is to provide an open source library that facilitates the use of Fluid Construction Grammar (FCG) within the Python programming ecosystem, enhancing its accessibility and integration.", "method": "This paper describes the PyFCG library and provides three tutorials demonstrating its application in formalising grammar analyses, learning grammar from data, and implementing agent-based experiments.", "result": "PyFCG allows seamless integration of construction grammar analyses and experiments into Python applications, showcasing practical use through tutorials.", "conclusion": "The library expands the capabilities of researchers and developers to utilize Fluid Construction Grammar in Python, providing tools for teaching and experimentation.", "key_contributions": ["Open source library for Fluid Construction Grammar in Python.", "Integration capabilities with other Python libraries.", "Tutorials for practical applications of FCG in language analysis and experiments."], "limitations": "", "keywords": ["Fluid Construction Grammar", "Python library", "agent-based experiments"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929.pdf", "abs": "https://arxiv.org/abs/2505.12929", "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "AI": {"tldr": "The paper introduces two methods, Advantage Reweighting and Low-Probability Token Isolation, to improve reinforcement learning training for large language models by reducing the influence of low-probability tokens.", "motivation": "To address the issue of low-probability tokens disproportionately affecting reinforcement learning (RL) model updates, which hinders the effective learning of essential high-probability tokens.", "method": "The authors propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti). These methods reduce the gradient influence from low-probability tokens while promoting parameter updates from high-probability tokens to ensure balanced token learning.", "result": "The proposed methods lead to substantial performance improvements in GRPO-trained large language models, achieving up to a 46.2% improvement on K&K Logic Puzzle reasoning tasks.", "conclusion": "The techniques enhance RL training efficiency for LLMs by promoting balanced learning across tokens of varying probabilities, improving overall model performance.", "key_contributions": ["Introduction of Advantage Reweighting method", "Development of Low-Probability Token Isolation (Lopti) method", "Demonstration of substantial performance improvements on logic reasoning tasks"], "limitations": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Gradient Optimization"], "importance_score": 8, "read_time_minutes": 24}}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942.pdf", "abs": "https://arxiv.org/abs/2505.12942", "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "AI": {"tldr": "The paper introduces $\tt A^\tt 3$, a novel low-rank approximation framework for efficient Transformer model compression that overcomes limitations of existing methods by optimizing multiple components without runtime overheads.", "motivation": "Existing low-rank approximation methods for Transformer models have significant limitations in performance and efficiency, motivating the need for a new approach.", "method": "$\tt A^\tt 3$ decomposes a Transformer layer into three components (QK, OV, MLP) and optimizes each to minimize functional loss while reducing model size and computational requirements.", "result": "$\tt A^\tt 3$ outperforms state-of-the-art methods, achieving a perplexity of 4.69 on WikiText-2 for a low-rank approximated LLaMA model, a significant improvement over previous records.", "conclusion": "The proposed method provides a practical framework for effectively compressing large language models while maintaining high performance without additional runtime costs.", "key_contributions": ["Introduction of a new low-rank approximation framework for Transformers", "Optimization of multiple components rather than individual layers", "Demonstration of superior performance compared to existing state-of-the-art methods."], "limitations": "", "keywords": ["low-rank approximation", "Transformer models", "model compression"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949.pdf", "abs": "https://arxiv.org/abs/2505.12949", "title": "Neural Morphological Tagging for Nguni Languages", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages.", "AI": {"tldr": "This paper explores neural methods for morphological tagging in Nguni languages, demonstrating that neural taggers can outperform traditional rule-based methods.", "motivation": "Morphological parsing is crucial for understanding agglutinative languages like the Nguni languages, where words are formed by concatenating multiple morphemes.", "method": "The study compares two neural approaches: training sequence labellers (LSTMs and CRFs) from scratch versus fine-tuning pretrained language models, alongside a traditional rule-based parser.", "result": "Neural taggers outperform the rule-based baseline, with models trained from scratch generally performing better than those using pretrained models.", "conclusion": "Neural taggers based on existing morphological segmenters are effective for the Nguni languages.", "key_contributions": ["Comparison of neural sequence labellers with pretrained models", "Demonstration of the effectiveness of neural methods for agglutinative languages", "Evaluation of performance across different segmenters and features"], "limitations": "", "keywords": ["morphological parsing", "neural methods", "Nguni languages", "sequence labelling", "pretrained models"], "importance_score": 5, "read_time_minutes": 15}}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950.pdf", "abs": "https://arxiv.org/abs/2505.12950", "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.", "AI": {"tldr": "The paper presents GuRE, a generative query rewriting method that improves Legal Passage Retrieval by addressing vocabulary mismatches using LLMs.", "motivation": "To improve the efficiency of Legal Passage Retrieval (LPR) systems by addressing the vocabulary mismatch between user queries and target passages.", "method": "By training a Large Language Model (LLM) specifically for rewriting user queries, GuRE generates rewritten queries designed to enhance passage retrieval performance.", "result": "Experimental results indicate that GuRE significantly outperforms baseline methods and exhibits better performance in a retriever-agnostic manner.", "conclusion": "GuRE's distinct training objectives lead to varied retrieval behaviors, which may benefit real-world applications more than typical retriever fine-tuning.", "key_contributions": ["Introduction of the Generative query REwriter (GuRE) for LPR systems", "Demonstration of superior performance over standard baseline methods", "Analysis of training objectives leading to improved retrieval behaviors"], "limitations": "", "keywords": ["Legal Passage Retrieval", "Generative Query Rewriter", "Large Language Models", "Vocabulary Mismatch", "Retrieval Performance"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964.pdf", "abs": "https://arxiv.org/abs/2505.12964", "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master.", "AI": {"tldr": "MA-COIR reformulates biomedical concept recognition as an indexing-recognition task to enhance efficiency and resolve ambiguities, utilizing semantic search indexes and language models for improved recognition in low-resource environments.", "motivation": "To improve ontology refinement, knowledge graph construction, and concept relationship discovery in the biomedical domain, addressing the shortcomings of traditional methods that rely on explicit mention identification.", "method": "The MA-COIR framework uses semantic search indexes (ssIDs) for concept recognition, allowing it to handle complex concepts. It employs a pretrained BART-based model fine-tuned on small datasets and integrates LLM-generated queries and synthetic data.", "result": "Experimental results demonstrate that MA-COIR effectively recognizes explicit and implicit biomedical concepts without mention-level annotations, outperforming traditional methods.", "conclusion": "MA-COIR advances ontology-driven concept recognition in biomedical applications, proving efficient in recognizing complex concepts with reduced computational requirements.", "key_contributions": ["Introduced MA-COIR framework for improved concept recognition in biomedicine", "Utilization of semantic search indexes to resolve ambiguities", "Integration of LLM-generated queries and synthetic data for low-resource recognition"], "limitations": "", "keywords": ["biomedical concept recognition", "ontology refinement", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969.pdf", "abs": "https://arxiv.org/abs/2505.12969", "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "AI": {"tldr": "This paper presents a method to reduce hallucinations in OpenAI's Whisper model during Automatic Speech Recognition, specifically targeting non-speech segments.", "motivation": "To address the hallucination issues in Whisper that hinder its application in complex industrial settings.", "method": "The authors benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder through a head-wise mask and fine-tune the heads that predominantly contribute to hallucination.", "result": "The best fine-tuned model, Calm-Whisper, achieves over 80% reduction in non-speech hallucination with less than 0.1% Word Error Rate (WER) degradation on test datasets.", "conclusion": "The proposed fine-tuning method effectively reduces hallucinations in non-speech segments of the Whisper model without significantly affecting overall performance.", "key_contributions": ["Introduces a novel method to reduce hallucinations in Whisper", "Identifies key self-attentional heads responsible for hallucinations", "Demonstrates significant reduction in hallucinations with minimal performance loss"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Whisper", "hallucination", "fine-tuning", "self-attention"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970.pdf", "abs": "https://arxiv.org/abs/2505.12970", "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "authors": ["Robin Jegan", "Andreas Henrich"], "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "AI": {"tldr": "This paper surveys the current state of traditional and older natural language processing techniques in the context of five application scenarios and assesses their future perspectives amidst the rise of neural networks and large language models.", "motivation": "To evaluate the relevance and application of traditional NLP techniques in light of the success of large language models in various NLP tasks.", "method": "A survey of recent publications focusing on five application scenarios: classification, information extraction, relation extraction, text simplification, and text summarization. The paper defines terminology and assesses the usage of traditional approaches.", "result": "The survey finds that traditional NLP methods are still in use across all five scenarios, serving as either part of processing pipelines, comparison baselines, or main models.", "conclusion": "Despite advancements in LLMs, traditional techniques remain relevant and are utilized in various capacities within modern NLP applications.", "key_contributions": ["Comprehensive survey of traditional NLP techniques in five application areas", "Assessment of the role and utility of older methods in conjunction with modern LLMs", "Statistical data on usage of traditional models in current research"], "limitations": "", "keywords": ["natural language processing", "traditional techniques", "large language models", "application scenarios", "survey"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973.pdf", "abs": "https://arxiv.org/abs/2505.12973", "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems.", "AI": {"tldr": "The paper addresses the challenge of homograph disambiguation in grapheme-to-phoneme conversion for low-resource languages by proposing a semi-automated dataset creation pipeline and improving a rule-based G2P system for better performance in accessibility applications.", "motivation": "Homograph disambiguation is essential for enhancing G2P systems, especially for low-resource languages, and current methods face significant limitations in dataset creation and real-time application suitability.", "method": "A semi-automated pipeline was developed to create comprehensive homograph datasets, specifically the HomoRich dataset. This dataset was then applied to improve both a deep learning-based G2P system for Persian and the eSpeak rule-based system, creating a faster version called HomoFast eSpeak.", "result": "The proposed improvements resulted in approximately a 30% increase in homograph disambiguation accuracy for both the deep learning-based and rule-based systems.", "conclusion": "Utilizing sophisticated offline datasets can enhance the performance of fast, rule-based G2P systems, making them suitable for latency-sensitive applications like screen readers.", "key_contributions": ["Creation of the HomoRich dataset through a semi-automated pipeline", "Development of the fast homograph-aware eSpeak system (HomoFast eSpeak)", "Demonstration of improved accuracy in homograph disambiguation for G2P systems."], "limitations": "", "keywords": ["homograph disambiguation", "grapheme-to-phoneme conversion", "low-resource languages", "machine learning", "accessibility tools"], "importance_score": 7, "read_time_minutes": 25}}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983.pdf", "abs": "https://arxiv.org/abs/2505.12983", "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "AI": {"tldr": "This paper investigates many-to-many summarization (M2MS) capabilities of large language models (LLMs) across multiple languages and domains and highlights the potential of instruction tuning to enhance their performance.", "motivation": "To explore the multilingual summarization capabilities of LLMs and address the existing challenges, such as factuality issues, in real-world applications.", "method": "A systematic empirical study was conducted, reorganizing M2MS datasets and benchmarking 18 LLMs in both zero-shot and instruction-tuning scenarios. Comparisons were made with fine-tuned traditional models like mBART.", "result": "LLMs achieved competitive results in zero-shot conditions compared to fine-tuned models, and significantly improved their M2MS performance with instruction tuning. However, factuality issues remain a concern which can be exacerbated by instruction tuning.", "conclusion": "Controlling factual errors is crucial when deploying LLMs for summarization tasks, and future research should focus on this aspect.", "key_contributions": ["Systematic evaluation of LLMs for many-to-many summarization across multiple languages and domains.", "Demonstration of the enhanced summarization capabilities of LLMs with instruction tuning.", "Identification of the factuality issue as a critical challenge for LLM summation applications."], "limitations": "Factuality issues persist in LLMs, which may improve with tuning but can also worsen exposure to errors.", "keywords": ["many-to-many summarization", "large language models", "instruction tuning", "multilingual", "factuality issue"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996.pdf", "abs": "https://arxiv.org/abs/2505.12996", "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "AI": {"tldr": "This paper presents a new reward modeling method in reinforcement learning for machine translation that significantly improves performance across multiple languages using large reasoning models (LRMs).", "motivation": "To address the limitations in existing reinforcement learning approaches for machine translation, particularly their focus on high-resource languages and inadequate reward modeling.", "method": "A new reward modeling method is designed that compares the translation results of a policy MT model against a strong LRM, using this comparison to quantify and provide rewards in reinforcement learning scenarios.", "result": "The proposed method achieves state-of-the-art performance in literary translation, outperforming leading LRMs, and successfully extends multilingual translation capabilities across 11 languages with a lightweight reward modeling approach.", "conclusion": "The paper demonstrates that a well-designed reward modeling technique can enhance machine translation performance significantly, allowing for effective transferability across multiple translation directions.", "key_contributions": ["Development of a new reward modeling technique for MT", "Achievement of state-of-the-art performance in literary translation", "Extension of MT capabilities to 11 languages using a lightweight RL approach"], "limitations": "The focus on specific reward modeling methods may still leave questions about generalizability across all language pairs and types of translation tasks.", "keywords": ["Reinforcement Learning", "Machine Translation", "Large Reasoning Models"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004.pdf", "abs": "https://arxiv.org/abs/2505.13004", "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x.", "AI": {"tldr": "EffiBench-X is a multi-language benchmark for evaluating the efficiency of LLM-generated code across various programming languages, revealing underperformance in efficiency compared to human experts despite functional correctness.", "motivation": "To fill the gap in existing code generation benchmarks which mainly focus on functional correctness and often limit analysis to a single language, we propose EffiBench-X to assess code efficiency across multiple programming languages.", "method": "EffiBench-X includes competitive programming tasks with human expert solutions serving as benchmarks for efficiency. It evaluates LLM-generated code in languages such as Python, C++, Java, JavaScript, Ruby, and Golang.", "result": "Evaluation of state-of-the-art LLMs shows that while they generate functionally correct code, they achieve only 62% of the efficiency of human experts on average, with variation across programming languages.", "conclusion": "The findings indicate a significant need for LLM optimization strategies to enhance code efficiency across various languages, as models perform better in some languages (Python, Ruby, JavaScript) than in others (Java, C++, Golang).", "key_contributions": ["Introduction of EffiBench-X as a multi-language efficiency benchmark for code generated by LLMs.", "Demonstration that LLM-generated code is functionally correct but less efficient than human-generated code, revealing potential areas for improvement.", "Public availability of the benchmark dataset and evaluation infrastructure."], "limitations": "The benchmark may not capture all aspects of code performance, and the efficiency results might vary with different LLMs not tested in this study.", "keywords": ["LLM-generated code", "efficiency benchmarking", "multi-language code generation", "competitive programming", "code optimization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006.pdf", "abs": "https://arxiv.org/abs/2505.13006", "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias KÃ¤fer"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions.", "AI": {"tldr": "The paper presents a Conversational AI system designed for airport staff to communicate with flight information systems, utilizing three RAG methods to enhance accuracy and reduce hallucinations in responses.", "motivation": "Airports aim to increase automation and improve communication with flight information systems. This paper addresses the challenge of integrating conversational AI in highly dynamic airport environments.", "method": "Three Retrieval-Augmented Generation (RAG) methods were implemented: traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG), which were evaluated based on accuracy and hallucination rates.", "result": "Traditional RAG achieved 84.84% accuracy but had issues with hallucinations. SQL RAG and Graph RAG scored 80.85% and 91.49% accuracy respectively, with Graph RAG being particularly effective for reasoning-based questions.", "conclusion": "SQL RAG and Graph RAG are recommended for airport environments due to their lower hallucination rates and better handling of dynamic questions.", "key_contributions": ["Development of a Conversational AI system for airport staff", "Implementation of three RAG methods", "Recommendation of SQL RAG and Graph RAG for better performance in airport environments"], "limitations": "The traditional RAG method's tendency to produce hallucinations poses a safety risk in airport contexts.", "keywords": ["Conversational AI", "Retrieval-Augmented Generation", "Airport automation", "Dynamic questioning", "AI safety"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010.pdf", "abs": "https://arxiv.org/abs/2505.13010", "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "AI": {"tldr": "This paper presents a fine-tuned RoBERTa model for sentence-level media bias detection, demonstrating statistically significant performance improvements over existing models, with a focus on maintaining interpretability and robustness.", "motivation": "The study aims to address the challenges of media bias detection, which is essential for ensuring fair information dissemination amid subjective bias and limited annotated data.", "method": "The paper fine-tunes a RoBERTa-based model on the expert-annotated BABE dataset for sentence-level bias classification, employing statistical tests to validate improvements over a baseline model.", "result": "The proposed model achieved statistically significant performance improvements compared to the DA-RoBERTa baseline, with attention analysis indicating better contextual relevance handling.", "conclusion": "The findings contribute to the development of more explainable and socially responsible NLP systems for detecting media bias, while also outlining future directions for bias analysis.", "key_contributions": ["Fine-tuning of RoBERTa for media bias detection", "Statistically significant performance validation", "Proposal of a comprehensive pipeline for bias analysis"], "limitations": "Constrained by sentence-level analysis and limited dataset size due to a scarcity of larger, advanced bias corpora.", "keywords": ["media bias detection", "NLP", "RoBERTa", "context-aware modeling", "explainability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034.pdf", "abs": "https://arxiv.org/abs/2505.13034", "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "authors": ["MÃ¡rton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "AI": {"tldr": "Introduction of topicwizard, a framework for model-agnostic topic model interpretation with interactive tools for exploring the relationships between documents, words, and topics.", "motivation": "To address the challenges users face in interpreting complex parameters of topic models, which are typically based on a limited and biased list-of-words approach.", "method": "The paper presents the topicwizard framework that allows for intuitive and interactive examination of topic model outputs, enhancing user understanding through better visualizations.", "result": "topicwizard improves the interpretability of topic models by providing a more accurate and comprehensive understanding of the semantic relations in textual corpora.", "conclusion": "The framework facilitates better usability and interpretability of topic models, making the insights derived from them more accessible to users.", "key_contributions": ["Introduction of model-agnostic interpretation framework for topic models", "Interactive tools for exploring semantic relations", "Enhanced visualization for better user understanding"], "limitations": "Existing visualizations are often limited to specific types of topic models; future work could expand capabilities.", "keywords": ["topic models", "interpretation", "visualization", "human-computer interaction", "natural language processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036.pdf", "abs": "https://arxiv.org/abs/2505.13036", "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "authors": ["Sai Koneru", "Maike ZÃ¼fle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "AI": {"tldr": "This paper discusses the Karlsruhe Institute of Technology's submissions to the IWSLT, focusing on enhancing speech translation and instruction following tasks using Large Language Models (LLMs).", "motivation": "The paper addresses the expanded scope of the IWSLT to include diverse tasks beyond traditional speech translation, motivated by advances in LLM capabilities.", "method": "The approach includes a pipeline for Offline Speech Translation that utilizes multiple automatic speech recognition systems and an LLM for output fusion, along with a two-step translation process and an end-to-end model for instruction following that integrates a speech encoder with an LLM.", "result": "The methods presented enhance performance across speech translation and instruction following tasks, with improved translation quality and contextual relevance in outputs.", "conclusion": "The integration of LLMs in the proposed pipeline and model significantly boosts performance in various speech-related tasks, demonstrating the versatility and capability of modern systems in processing spoken language.", "key_contributions": ["Utilization of multiple ASR systems to improve speech translation", "Integration of LLM for document-level context in translations", "Development of an end-to-end model for instruction-following tasks combining speech encoding with LLMs"], "limitations": "", "keywords": ["Large Language Models", "Speech Translation", "Instruction Following"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053.pdf", "abs": "https://arxiv.org/abs/2505.13053", "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "AI": {"tldr": "This paper presents a Bayesian inference-based approach for adapting explanation generation in dialog systems to the user's context and features, demonstrating its effectiveness through simulated interlocutors.", "motivation": "The paper addresses the need for dialog systems to adapt explanations based on the user's characteristics and contextual information to improve communication effectiveness.", "method": "It employs a Bayesian inference model to update user profiles continuously, combined with a non-stationary Markov Decision Process to adjust explanation strategies based on the updated user information.", "result": "The implementation tested with five simulated interlocutors showed high adaptivity, with distinct explanation strategies emerging for each partner, even with varying feedback behaviors.", "conclusion": "The findings suggest that this adaptive approach can significantly improve the performance of explainable AI systems and dialog systems.", "key_contributions": ["A formal computational partner model for tracking interaction context and listener features.", "A Bayesian inference approach that uses user feedback to update the partner model.", "A non-stationary decision-making process to determine optimal explanation strategies."], "limitations": "The study focuses on simulated interlocutors; real-world applicability may require further validation.", "keywords": ["dialog systems", "explanations", "Bayesian inference", "non-stationary decision process", "explainable AI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069.pdf", "abs": "https://arxiv.org/abs/2505.13069", "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "GwenolÃ© Quellec"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability.", "AI": {"tldr": "This study presents a multimodal approach for speech-based suicide risk assessment in adolescents using various audio and linguistic embeddings.", "motivation": "The need for effective speech-based suicide risk assessment for adolescents, addressing a gap in mental health support.", "method": "Integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, audio embeddings from WavLM, and handcrafted acoustic features; exploring fusion strategies like early concatenation, modality-specific processing, and weighted attention with mixup regularization.", "result": "Achieved 69% accuracy on the development set with weighted attention strategy, although a performance gap exists between development and test sets.", "conclusion": "Emphasizes refining embedding representations and fusion mechanisms for improved classification reliability in suicide risk assessment.", "key_contributions": ["Multimodal approach integrating various embeddings for speech analysis.", "Evaluation of multiple fusion strategies for better classification.", "Specific focus on adolescent suicide risk assessment through speech."], "limitations": "Performance gap between development and test sets indicates generalization challenges that need addressing.", "keywords": ["speech-based assessment", "suicide risk", "multimodal approach", "fusion strategies", "adolescents"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077.pdf", "abs": "https://arxiv.org/abs/2505.13077", "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "AI": {"tldr": "This paper presents Numerical Token Integrity Loss (NTIL) to improve sequence generation in autoregressive models by preserving the ordinal relationships of numerical sequences.", "motivation": "Standard methods for sequence generation treat digits as independent tokens, failing to capture the coherent structure present in numerical sequences, which can hinder performance.", "method": "NTIL operates at two levels: token-level, which extends the Earth Mover's Distance to maintain ordinal relationships, and sequence-level, which penalizes discrepancies between predicted and actual sequences.", "result": "Experiments indicate that using NTIL leads to significant improvements in numerical prediction accuracy in sequence generation tasks.", "conclusion": "By incorporating NTIL, autoregressive models can more effectively handle numerical data in sequence generation, showing promise for further integration with LLMs and MLLMs.", "key_contributions": ["Introduction of Numerical Token Integrity Loss (NTIL)", "Extension of Earth Mover's Distance for ordinal preservation", "Dual-level approach enhancing numerical sequence coherence"], "limitations": "", "keywords": ["Numerical Token Integrity Loss", "sequence generation", "Earth Mover's Distance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089.pdf", "abs": "https://arxiv.org/abs/2505.13089", "title": "Systematic Generalization in Language Models Scales with Information Entropy", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Ãtienne Simon"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization.", "AI": {"tldr": "This paper explores systematic generalization in language models and proposes a framework for measuring the entropy of component parts in training data, finding a link between entropy levels and model performance.", "motivation": "To address the challenges of systematic generalization in language models, particularly their sensitivity to input permutations and performance in novel contexts.", "method": "The authors develop a framework that quantifies the entropy of the distribution of component parts in sequence-to-sequence tasks, relating it to the performance of various model architectures.", "result": "The study discovers that model performance correlates with entropy levels, demonstrating that high entropy can lead to successful generalization without built-in priors, and low entropy can serve as a benchmark for assessing generalization robustness.", "conclusion": "The findings establish a connection between systematic generalization and information efficiency, which can inform the development of more robust language models.", "key_contributions": ["Proposed a method for measuring entropy in sequence-to-sequence tasks", "Established a connection between entropy levels and model performance", "Highlighted the implications of entropy in assessing systematic generalization"], "limitations": "", "keywords": ["systematic generalization", "language models", "entropy", "information efficiency", "sequence-to-sequence"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090.pdf", "abs": "https://arxiv.org/abs/2505.13090", "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "authors": ["David Stap", "Christof Monz"], "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.", "AI": {"tldr": "Expanding language diversity during LLM fine-tuning enhances translation quality, particularly for both unsupervised and supervised pairs, up to a certain threshold.", "motivation": "To resolve conflicting findings in prior research regarding language diversity's impact on LLM fine-tuning.", "method": "Controlled fine-tuning experiments were conducted across 132 translation directions to assess the effects of language diversity on translation quality.", "result": "Increased language diversity improves translation quality, yielding more language-agnostic representations that enhance model performance, although benefits plateau or decrease after a certain diversity threshold.", "conclusion": "Greater language diversity during fine-tuning contributes to improved translation quality, but there is a limit beyond which this diversity may hinder performance.", "key_contributions": ["Systematic evaluation of language diversity in LLM fine-tuning across multiple translation pairs.", "Demonstration of the threshold effect in language diversity beyond which performance gains plateau or decrease.", "Insights into the creation of language-agnostic representations that improve model performance."], "limitations": "Benefits of increased diversity plateau or decrease after certain thresholds, limiting its utility in all contexts.", "keywords": ["language diversity", "LLM fine-tuning", "translation quality", "language-agnostic representations", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115.pdf", "abs": "https://arxiv.org/abs/2505.13115", "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "AI": {"tldr": "The paper introduces a novel dataset, TREA, for evaluating large audio language models (LALMs) on reasoning tasks, revealing limitations in LALMs compared to human capabilities and proposing a new uncertainty metric.", "motivation": "To assess large audio language models on reasoning tasks distinct from traditional classification and generation tasks.", "method": "The authors propose the temporal reasoning evaluation of audio (TREA) dataset and benchmark open-source LALMs against human performance, while also introducing an uncertainty metric for evaluation.", "result": "LALMs consistently underperform compared to human capabilities on tasks within the TREA dataset. The proposed uncertainty metric reveals that accuracy and uncertainty do not correlate.", "conclusion": "A comprehensive evaluation framework for LALMs is necessary, especially for high-stakes applications, given the identified performance gaps and lack of correlation between accuracy and uncertainty metrics.", "key_contributions": ["Introduction of the TREA dataset for evaluating reasoning in LALMs", "Benchmarking LALMs against human capabilities", "Proposing a new uncertainty metric for audio language models"], "limitations": "Limited to benchmarking existing LALMs; further exploration of uncertainty and its implications is needed.", "keywords": ["large audio language models", "temporal reasoning", "uncertainty metrics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136.pdf", "abs": "https://arxiv.org/abs/2505.13136", "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "AI": {"tldr": "Introduction of ModernGBERT family of German encoder models and comparison with LLM2Vec derived encoders.", "motivation": "To evaluate the trade-offs of training encoders from scratch for resource-constrained applications and advance the German NLP ecosystem.", "method": "Development of ModernGBERT encoder models and benchmarking against LLM2Vec derived encoders on various natural language processing tasks.", "result": "ModernGBERT 1B outperforms prior state-of-the-art German encoders in performance and parameter-efficiency.", "conclusion": "The availability of all models, data, and code will enhance the German NLP landscape with high-performance encoders.", "key_contributions": ["Introduction of ModernGBERT family of encoder models trained from scratch for German.", "Benchmarking new encoders against state-of-the-art and LLM2Vec derived models.", "Publicly available models and resources to foster German NLP research."], "limitations": "", "keywords": ["ModernGBERT", "encoder models", "German NLP", "LLM2Vec", "text embedding"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141.pdf", "abs": "https://arxiv.org/abs/2505.13141", "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "AI": {"tldr": "The study investigates how large language models (LLMs) manage multilingual reasoning, revealing inconsistencies due to reliance on individual language subspaces rather than a shared semantic space. It also discusses improving multilingual performance through enhanced knowledge sharing.", "motivation": "To understand how LLMs generalize knowledge across languages and to address the issue of inconsistent outputs when prompted in different languages.", "method": "The logit lens is applied to interpret the steps taken by LLMs in solving multilingual multi-choice reasoning questions, analyzing hidden states and their relation to shared representations.", "result": "LLMs exhibit inconsistent predictions and lower accuracy due to reliance on language-specific subspaces. Larger models dissociate from shared representations but better retrieve cross-lingual knowledge.", "conclusion": "Modulating models' latent processing towards a shared semantic space enhances multilingual reasoning performance by improving knowledge transfer from English.", "key_contributions": ["Identified limitations in LLMs' handling of multilingual tasks by analyzing their reliance on individual language subspaces.", "Demonstrated the link between model size and the dissociation from shared semantic representations.", "Provided evidence that reinforcement of a shared semantic space leads to improved multilingual reasoning outcomes."], "limitations": "The study focuses primarily on the performance of LLMs in multilingual contexts and may not address all potential influences of model architectures across other tasks.", "keywords": ["multilingual reasoning", "large language models", "natural language processing", "knowledge transfer", "semantic space"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147.pdf", "abs": "https://arxiv.org/abs/2505.13147", "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP.", "AI": {"tldr": "This paper challenges existing assumptions about detecting deception from written text, presenting a new framework and datasets that reveal limited correlation between linguistic cues and deception labels.", "motivation": "The motivation is to assess the validity of automatic deception detection in text, given that previous studies may have overestimated the reliability of linguistic cues due to dataset artifacts.", "method": "The authors introduce a belief-based deception framework and construct three corpora (DeFaBel) for analyzing deceptive communication in German and English, evaluating common linguistic cues.", "result": "The study finds negligible correlations between linguistic cues and deception labels across the DeFaBel corpora, and while some models performed well on traditional datasets, they failed to generalize to the new dataset.", "conclusion": "Findings suggest re-evaluating the methods for studying and modeling deception in NLP, as reliance on linguistic cues is called into question.", "key_contributions": ["Introduction of the belief-based deception framework", "Creation of the DeFaBel corpora for multilingual analysis", "Demonstration of negligible correlations between linguistic cues and deception in text"], "limitations": "The findings are specific to the constructed datasets and may not represent all contexts of deception detection.", "keywords": ["Deception detection", "NLP", "Linguistic cues", "Machine Learning", "Corpora"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156.pdf", "abs": "https://arxiv.org/abs/2505.13156", "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "AI": {"tldr": "This paper introduces Tianyi, a 7.6-billion-parameter LLM specifically designed for Traditional Chinese Medicine (TCM), aimed at improving its practical application in healthcare.", "motivation": "The increasing recognition of TCM for its therapeutic potential and the need for precise application in healthcare due to the complexity of TCM diagnostics and treatment principles.", "method": "Tianyi is pre-trained and fine-tuned on diverse TCM corpora including classical texts and clinical records, and utilizes a comprehensive evaluation benchmark, TCMEval, for assessing LLM performance in TCM.", "result": "Extensive evaluations reveal Tianyi's significant potential as an AI assistant in TCM clinical practices and research, effectively bridging the gap between TCM knowledge and its practical application.", "conclusion": "Tianyi presents a specialized LLM contributing to the efficacy of TCM, overcoming limitations faced by previous systems.", "key_contributions": ["Introduction of Tianyi, a specialized LLM for TCM", "Development of TCMEval, a benchmark for evaluating LLMs in TCM", "Demonstration of Tianyi's potential in clinical practice and research"], "limitations": "Challenges include model scale deployment and hallucination issues in LLMs.", "keywords": ["Traditional Chinese Medicine", "Machine Learning", "Large Language Models", "Healthcare", "Tianyi"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157.pdf", "abs": "https://arxiv.org/abs/2505.13157", "title": "Role-Playing Evaluation for Large Language Models", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "AI": {"tldr": "Introducing RPEval, a benchmark for assessing role-playing abilities in LLMs.", "motivation": "To evaluate the role-playing capabilities of LLMs with a reliable benchmark due to challenges in human and automated assessments.", "method": "The paper presents the construction of RPEval, which evaluates LLMs across emotional understanding, decision-making, moral alignment, and in-character consistency.", "result": "Baseline evaluations of LLMs using the RPEval benchmark are provided, highlighting their role-playing effectiveness.", "conclusion": "RPEval aims to standardize the evaluation of LLM role-playing and provide resources to the community.", "key_contributions": ["Introduction of Role-Playing Eval (RPEval) benchmark", "Evaluation across four key dimensions of role-playing", "Public availability of code and dataset"], "limitations": "", "keywords": ["Large Language Models", "Role-playing", "Evaluation Benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171.pdf", "abs": "https://arxiv.org/abs/2505.13171", "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "AI": {"tldr": "This paper investigates the memorization behavior of language models, revealing the 'offset effect'âhow the position of tokens in the context window influences verbatim memorization and text degeneration.", "motivation": "To systematically investigate the memorization risks posed by large language models, especially concerning copyright risks from training data.", "method": "Pretraining language models (1B/3B/8B) on 83B tokens, mixing web-scale data with public domain content, and analyzing the effects of token position in the context window on memorization behavior.", "result": "Identified the 'offset effect,' showing that verbatim memorization is triggered by short prefixes from the beginning of the context, and that memorization declines sharply as the prefix is offset from the initial tokens.", "conclusion": "Positional offset is a crucial factor in understanding language model memorization risks, suggesting that shifting sensitive data in the context can reduce issues of extractable memorization and text degeneration.", "key_contributions": ["Introduced the 'offset effect' in language model memorization.", "Demonstrated the impact of token positioning on memorization and text quality.", "Highlighted the need for reevaluating memorization risk assessment in language models."], "limitations": "Focuses primarily on memorization without deeply exploring other aspects of language model behavior.", "keywords": ["language models", "memorization risk", "positional offset", "copyright", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173.pdf", "abs": "https://arxiv.org/abs/2505.13173", "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "AI": {"tldr": "This study investigates the cross-lingual zero-shot generalization performance of LLMs in classical languages, demonstrating that larger models outperform smaller ones in tasks like named entity recognition and question answering, notably with a focus on Sanskrit.", "motivation": "To understand how LLMs generalize across classical languages and the impact of model size on performance in tasks such as named entity recognition and machine translation.", "method": "The study analyzed LLM performance on named entity recognition and machine translation tasks in Sanskrit, Ancient Greek, and Latin, and presented a factoid QA dataset for Sanskrit, utilizing a retrieval-augmented generation approach.", "result": "Large models like GPT-4o and Llama-3.1 showed better performance than smaller models on out-of-domain data, and incorporating context via retrieval-augmented generation significantly improved results in Sanskrit QA tasks.", "conclusion": "The findings highlight that model scale is vital for effective cross-lingual generalization in classical languages, providing insights into LLM utility in classical studies.", "key_contributions": ["Investigates cross-lingual zero-shot generalization of LLMs in classical languages.", "Demonstrates the superior performance of larger models compared to smaller ones in specific tasks.", "Introduces a factoid question-answering dataset for Sanskrit and shows effectiveness of retrieval-augmented generation."], "limitations": "Results may not generalize to all smaller LLMs or other classical languages beyond those studied.", "keywords": ["Large Language Models", "cross-lingual generalization", "classical languages", "question answering", "Sanskrit"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176.pdf", "abs": "https://arxiv.org/abs/2505.13176", "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "AI": {"tldr": "This paper introduces ToolSpectrum, a benchmark for evaluating personalized tool utilization in large language models (LLMs), addressing the gap in context-aware tool selection.", "motivation": "The integration of external tools into LLMs enhances their functionality, but current approaches neglect the importance of personalized and context-aware tool selection, leading to user dissatisfaction and inefficient tool use.", "method": "We formalize dimensions of personalization (user profile and environmental factors) and analyze their impacts on tool utilization through experiments on ToolSpectrum.", "result": "Personalized tool utilization significantly enhances user experience, but current LLMs struggle to effectively reason about user profiles and environmental factors simultaneously.", "conclusion": "The necessity for context-aware personalization in tool-augmented LLMs is highlighted, alongside critical limitations of existing models in handling personalization dimensions.", "key_contributions": ["Introduction of ToolSpectrum benchmark for evaluating LLMs' personalized tool utilization.", "Formalization of user profile and environmental factors as dimensions of personalization.", "Empirical evidence demonstrating the impact of personalized tool utilization on user experience."], "limitations": "State-of-the-art LLMs have limited capacity to jointly reason about user profiles and environmental factors.", "keywords": ["Large Language Models", "Personalization", "Tool Utilization", "Human-Computer Interaction", "Context-Aware Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181.pdf", "abs": "https://arxiv.org/abs/2505.13181", "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.", "AI": {"tldr": "SLED introduces a novel approach to speech language modeling by utilizing continuous latent representations of speech waveforms, enhancing efficiency and performance in speech synthesis tasks.", "motivation": "To improve speech language modeling by eliminating discretization errors and simplifying the modeling pipeline.", "method": "SLED encodes speech waveforms into continuous latent representations and models them autoregressively using an energy distance objective.", "result": "SLED demonstrates strong performance in zero-shot and streaming speech synthesis, outperforming existing speech language models.", "conclusion": "SLED offers a more efficient alternative for speech language modeling with potential broader applications.", "key_contributions": ["Introduces continuous latent representations for speech modeling", "Utilizes energy distance for autoregressive modeling", "Simplifies the modeling pipeline without losing information"], "limitations": "", "keywords": ["speech language modeling", "continuous latent representations", "energy distance"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204.pdf", "abs": "https://arxiv.org/abs/2505.13204", "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "AI": {"tldr": "This paper introduces a training-free alignment-augmented speculative decoding algorithm that improves generation accuracy and efficiency for large language models.", "motivation": "To reduce the training costs associated with existing draft-target alignment methods in speculative decoding.", "method": "The authors propose alignment sampling and a flexible verification strategy that utilizes output distribution from the prefilling phase to enhance draft candidates and improves inference efficiency through an adaptive probability threshold.", "result": "The proposed method shows a 3.3 point increase in average generation score for the LLaMA3 model across 8 datasets, achieving a mean acceptance length of 2.39 and speeding up generation by 2.23 times.", "conclusion": "The proposed training-free method balances accuracy and efficiency in language model generation without significant training costs.", "key_contributions": ["Introduction of a training-free speculative decoding algorithm.", "Alignment sampling technique that improves draft candidate quality without retraining.", "A flexible verification strategy that enhances generation accuracy and efficiency."], "limitations": "", "keywords": ["speculative decoding", "alignment sampling", "large language models", "inference efficiency", "natural language processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210.pdf", "abs": "https://arxiv.org/abs/2505.13210", "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "AI": {"tldr": "A multimodal framework for sentiment analysis of classical Chinese poetry that incorporates audio, visual, and textual features using dialect enhancements and LLM translation.", "motivation": "To address the limitations in existing sentiment analysis of classical Chinese poetry that neglects rhythmic and visual features, which are crucial for understanding its emotional depth.", "method": "A dialect-enhanced multimodal framework that fuses audio features from multiple dialects, visual features, and textual features enhanced by LLM translation through multimodal contrastive representation learning.", "result": "The proposed framework outperforms state-of-the-art methods, achieving at least a 2.51% improvement in accuracy and 1.63% in macro F1 score on two public datasets.", "conclusion": "The introduced multimodal framework presents significant advancements in sentiment analysis of classical Chinese poetry, while also facilitating further research by providing open-source code.", "key_contributions": ["Development of a multimodal framework for poetry sentiment analysis.", "Incorporation of dialect-enhanced audio features.", "Open-sourcing of code for future research and applications."], "limitations": "", "keywords": ["sentiment analysis", "multimodal learning", "classical Chinese poetry", "dialect enhancement", "LLM translation"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220.pdf", "abs": "https://arxiv.org/abs/2505.13220", "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "AI": {"tldr": "Introduction of SeedBench, a multi-task benchmark for seed science using LLMs.", "motivation": "To address the challenges in seed science that hinder progress in agriculture.", "method": "Developed SeedBench with domain experts, evaluating 26 LLMs including proprietary and open-source models.", "result": "Identified substantial performance gaps between LLMs and real-world seed science problems.", "conclusion": "SeedBench serves as a foundational tool for advancing research in LLM applications for seed design.", "key_contributions": ["First multi-task benchmark for seed science", "Evaluation of 26 leading LLMs", "Foundation for future LLM research in seed design."], "limitations": "", "keywords": ["seed science", "large language models", "benchmark", "agriculture", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244.pdf", "abs": "https://arxiv.org/abs/2505.13244", "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "AI": {"tldr": "This study focuses on multilingual multi-label emotion detection for social media text, applying pre-trained models and achieving strong performance across multiple languages.", "motivation": "With the rise of digital communication, understanding emotions in multilingual contexts is essential for better information exchange on social media.", "method": "Utilizes fine-tuned BERT-based models and instruction-tuned generative LLM, applying two methods for multi-label classification: the base method and a pairwise method that examines relationships between inputs and emotion categories.", "result": "Achieved Top 4 performance in Track A across 10 languages, ranking 1st in Hindi, and secured Top 5 performance in Track B in 7 languages.", "conclusion": "The approach shows effective and simple methodologies for addressing multilingual emotion detection tasks, demonstrating strong generalization capabilities.", "key_contributions": ["Leveraged multilingual models for emotion detection", "Proposed two methods for multi-label classification", "Demonstrated strong performance across multiple languages"], "limitations": "", "keywords": ["multilingual", "emotion detection", "multi-label classification", "BERT", "LLM"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251.pdf", "abs": "https://arxiv.org/abs/2505.13251", "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "authors": ["Sidney Wong"], "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide.", "AI": {"tldr": "The paper discusses the dual role of the internet in fostering and alienating marginalized communities and proposes using social methods informed by linguistics and NLP to combat hate speech and promote digital inclusion.", "motivation": "To explore how the internet can both connect and alienate marginalized communities and to propose new approaches for hate speech research.", "method": "The authors advocate for social methods over computational solutions in addressing hate speech, drawing on linguistic research to inform community engagement and policy planning.", "result": "It was found that collaboration between linguists, NLP researchers, communities, and policymakers can enhance efforts to mitigate hate speech and promote equitable digital inclusion.", "conclusion": "Linguists and NLP researchers have a crucial role in promoting social impact through collaboration, thereby addressing the risks of anti-social behavior in digital spaces.", "key_contributions": ["Proposing a shift from computational to social methods in hate speech research.", "Highlighting the role of linguistics in informing language policy and community engagement.", "Emphasizing collaboration with marginalized communities to enhance digital inclusion."], "limitations": "The paper does not provide specific case studies or empirical evidence to support its claims.", "keywords": ["hate speech", "linguistics", "NLP", "digital inclusion", "social methods"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252.pdf", "abs": "https://arxiv.org/abs/2505.13252", "title": "Natural Language Planning via Coding and Inference Scaling", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "AI": {"tldr": "This paper evaluates the performance of both closed- and open-source LLMs in generating programs for complex planning tasks like meeting scheduling, while highlighting their limitations in robustness and efficiency.", "motivation": "To address the challenges LLMs face in executing real-life planning tasks that require high complexity and to compare the performance of closed- and open-source models.", "method": "Systematic evaluation of LLMs in generating Python code and code for constraint satisfaction problems, analyzing their performance in executing plans.", "result": "Programming often outperforms planning in generating solutions, but there are inconsistencies and issues with the generated code's robustness and efficiency.", "conclusion": "Enhanced evaluation and generation techniques are necessary to improve LLMs' performance in complex planning tasks, as current models show limitations.", "key_contributions": ["Systematic evaluation of both closed- and open-source models for planning tasks", "Analysis of generated Python code and constraint solver code", "Insight into robustness and efficiency challenges in LLM-generated code"], "limitations": "The generated code lacks robustness and efficiency, which impacts its generalization capabilities.", "keywords": ["LLMs", "planning tasks", "code generation", "constraint satisfaction", "error analysis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254.pdf", "abs": "https://arxiv.org/abs/2505.13254", "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.", "AI": {"tldr": "This paper introduces HeteroSpec, a framework that improves autoregressive decoding for LLMs by optimizing resource allocation based on linguistic complexity, achieving a significant speedup without retraining.", "motivation": "Autoregressive decoding in LLMs is inefficient due to its sequential nature, and existing speculative decoding methods do not fully utilize the varying complexities of linguistic contexts.", "method": "HeteroSpec employs a cumulative meta-path Top-$K$ entropy metric to identify predictable contexts and a dynamic resource allocation strategy to adaptively manage computational resources during inference.", "result": "HeteroSpec achieves an average speedup of 4.26Ã on five benchmarks and four models, outperforming the state-of-the-art EAGLE-3 in multiple metrics.", "conclusion": "The proposed framework establishes a new standard for accelerating context-aware LLM inference by maintaining high efficiency without the need for model retraining.", "key_contributions": ["Introduction of a cumulative meta-path Top-$K$ entropy metric", "Dynamic resource allocation based on entropy partitioning", "Achieving substantial speedup without draft model retraining"], "limitations": "", "keywords": ["HeteroSpec", "autoregressive decoding", "large language models", "speculative decoding", "resource allocation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257.pdf", "abs": "https://arxiv.org/abs/2505.13257", "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "authors": ["Zilu Tang", "Afra Feyza AkyÃ¼rek", "Ekin AkyÃ¼rek", "Derry Wijaya"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "AI": {"tldr": "The paper introduces WikiPersona, a dataset focusing on fine-grained personalization of models to align with individual user preferences using famous personas.", "motivation": "To address the lack of personalized preference datasets that capture nuanced individual-level preferences in AI model alignment.", "method": "The authors create a dataset called WikiPersona, which uses well-documented famous individuals and evaluates different personalization approaches for model alignment.", "result": "The study finds that using inferred personal preferences as prefixes enables effective personalization, particularly in areas with conflicting preferences, leading to better generalization across unseen personas.", "conclusion": "The findings suggest that traditional methods of few-shot prompting and fine-tuning are less effective and efficient compared to the proposed method leveraging inferred preferences.", "key_contributions": ["Introduction of the WikiPersona dataset for personalized AI model alignment", "Evaluation of personalization approaches highlighting the limitations of few-shot prompting and fine-tuning", "Demonstration of effective personalization using inferred personal preferences"], "limitations": "The study focuses on famous individuals, which may not fully represent the diversity of user preferences in real-world applications.", "keywords": ["personalization", "human preferences", "AI alignment", "WikiPersona", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258.pdf", "abs": "https://arxiv.org/abs/2505.13258", "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "AI": {"tldr": "ARENA is a transparent RAG generator framework that enhances the reasoning ability of LLMs through reinforcement learning, achieving significant improvements in multi-hop QA tasks.", "motivation": "To improve the effectiveness and transparency of Retrieval-Augmented Generation (RAG) models in utilizing retrieved information for reasoning and generation.", "method": "ARENA employs reinforcement learning to train a transparent RAG generator that identifies key evidence and structures reasoning, resulting in interpretable decision traces.", "result": "The ARENA framework demonstrated a 10-30% improvement on multiple multi-hop QA datasets compared to existing RAG baselines, showing effectiveness comparable to state-of-the-art LLMs.", "conclusion": "ARENA shows potential for improved effectiveness and transparency in RAG applications, with strong flexibility for adaptation to new datasets.", "key_contributions": ["Introduction of ARENA, a new RAG generator framework", "Utilization of reinforcement learning for enhanced reasoning", "Demonstrated significant performance improvements on multi-hop QA tasks"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Transparency", "Multi-hop Question Answering", "Large Language Models"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259.pdf", "abs": "https://arxiv.org/abs/2505.13259", "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "AI": {"tldr": "This survey explores the evolving role of Large Language Models (LLMs) in scientific discovery and human-AI collaboration, introducing a three-level taxonomy of LLM autonomy and responsibilities.", "motivation": "To understand and chart the transformative impact of LLMs in scientific research and their evolving roles as collaborative agents.", "method": "The paper uses a survey approach to develop a taxonomy based on the scientific method, categorizing LLMs into three levels: Tool, Analyst, and Scientist, while analyzing their responsibilities and capabilities.", "result": "The survey identifies key challenges and outlines future research directions for AI in science, including ethical considerations, robotic automation, and self-improvement of models.", "conclusion": "By providing strategic foresight and a conceptual architecture, the survey aims to guide responsible advancements in AI-driven scientific discovery.", "key_contributions": ["Introduces a three-level taxonomy for LLMs in research: Tool, Analyst, Scientist.", "Identifies pivotal challenges and future trajectories for LLMs in scientific discovery.", "Suggests a strategic framework for navigating AI's role in research."], "limitations": "", "keywords": ["Large Language Models", "scientific discovery", "human-AI collaboration", "taxonomy", "ethical governance"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268.pdf", "abs": "https://arxiv.org/abs/2505.13268", "title": "Representation of perceived prosodic similarity of conversational feedback", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "AI": {"tldr": "This study explores the prosodic similarity of vocal feedback in conversational systems and how well speech representations capture these similarities.", "motivation": "To understand the importance of vocal feedback in spoken dialogue and its role in ensuring common ground in conversational systems.", "method": "A triadic comparison task was conducted with participants to measure the perceived similarity of vocal feedback from two different datasets.", "result": "Spectral and self-supervised speech representations were found to encode prosody better than pitch features, particularly when feedback was from the same speaker.", "conclusion": "Contrastive learning can be used to further align speech representations with human perception of prosody.", "key_contributions": ["Investigated the perceived prosodic similarity of vocal feedback.", "Demonstrated the effectiveness of spectral and self-supervised speech representations over pitch features.", "Showed the potential of contrastive learning in refining speech representations."], "limitations": "", "keywords": ["vocal feedback", "prosody", "speech representation", "contrastive learning", "conversational systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271.pdf", "abs": "https://arxiv.org/abs/2505.13271", "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "AI": {"tldr": "The paper introduces CSC-SQL, a method that enhances SQL query generation from natural language through the integration of Self-Consistency and Self-Correction techniques, utilizing reinforcement learning for improved accuracy.", "motivation": "To address limitations in current methods for converting natural language questions to SQL queries, particularly in enhancing accuracy and output quality during inference.", "method": "CSC-SQL integrates Self-Consistency and Self-Correction by selecting the most frequently occurring outputs from parallel sampling and refining them using a merge revision model. Additionally, the Group Relative Policy Optimization (GRPO) algorithm fine-tunes the models via reinforcement learning.", "result": "The 3B model of CSC-SQL achieves 65.28% execution accuracy and the 7B model achieves 69.19% accuracy on the BIRD development set, demonstrating improved performance over existing methods.", "conclusion": "CSC-SQL effectively combines two techniques for improved accuracy in SQL generation and is generalizable across different models.", "key_contributions": ["Integration of Self-Consistency and Self-Correction for better SQL generation", "Fine-tuning of models using reinforcement learning with GRPO", "Open-sourcing of code for community use"], "limitations": "", "keywords": ["large language models", "SQL generation", "reinforcement learning"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282.pdf", "abs": "https://arxiv.org/abs/2505.13282", "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "AI": {"tldr": "LORex is a framework for taxonomy expansion that integrates discriminative ranking and generative reasoning, improving efficiency and accuracy in recommendation systems and web applications.", "motivation": "The need for effective taxonomy expansion methods due to growing data, addressing challenges faced by existing discriminative and generative approaches.", "method": "LORex employs a combined strategy of ranking and chunking candidate terms into batches, refining selections through reasoning about their hierarchy.", "result": "LORex demonstrates a 12% improvement in accuracy and a 5% increase in Wu & Palmer similarity compared to state-of-the-art methods.", "conclusion": "By utilizing a hybrid approach, LORex effectively reduces noise and improves the contextual relevance of expanded taxonomies.", "key_contributions": ["Development of a novel framework for taxonomy expansion combining ranking and generative reasoning.", "Demonstrated significant performance improvements over existing state-of-the-art methods.", "Efficient processing of candidates through hierarchy reasoning."], "limitations": "", "keywords": ["taxonomy expansion", "recommendation systems", "discriminative ranking"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302.pdf", "abs": "https://arxiv.org/abs/2505.13302", "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "AI": {"tldr": "This study investigates the impact of images on the resharing behavior of vision-language models (VLMs) in news recommendation systems, revealing that images increase resharing rates and vary by model family and user persona traits.", "motivation": "To understand how images affect the propensity of vision-language models to reshare news content, especially in the context of misinformation.", "method": "A study utilizing a jailbreaking-inspired prompting strategy to analyze VLM behavior, paired with a multimodal dataset of political news and images.", "result": "Experiments show images increase resharing rates by 4.8% for true news and 15.0% for false news; persona conditioning affects model sensitivity to misinformation.", "conclusion": "There are significant implications for multimodal model behavior concerning misinformation, indicating the need for evaluation frameworks and mitigation strategies.", "key_contributions": ["First study on images' influence on VLM resharing decisions", "Introduction of a jailbreaking-inspired prompting strategy", "Development of a multimodal dataset of fact-checked political news with images"], "limitations": "The model's robustness to visual misinformation was only demonstrated by one model (Claude-3-Haiku).", "keywords": ["vision-language models", "misinformation", "news recommendation", "multimodal dataset", "persona conditioning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307.pdf", "abs": "https://arxiv.org/abs/2505.13307", "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "AI": {"tldr": "The paper introduces the Reasoning Boundary Framework++ (RBF++) to quantitatively evaluate and optimize Chain-of-Thought reasoning capabilities in large language models, addressing challenges in measurable and unmeasurable capabilities.", "motivation": "There is a need for quantitative metrics and guidelines to evaluate and optimize Chain-of-Thought (CoT) reasoning capabilities in large language models (LLMs), especially for real-world applications.", "method": "The authors define a reasoning boundary (RB) as the maximum limit of CoT performance and propose a combination law for RBs. They also introduce a constant assumption for unmeasurable RBs and a reasoning boundary division mechanism for quantification and optimization in multimodal scenarios.", "result": "Experiments involving 38 models across 13 tasks validate the proposed RBF++ framework, assessing 10 CoT strategies and expanding evaluation benchmarks for RBs in LLM reasoning.", "conclusion": "The introduction of RBF++ aims to enhance the understanding and application of reasoning boundaries and optimization strategies in large language models, providing a foundation for further research.", "key_contributions": ["Definition of reasoning boundary (RB) for CoT performance evaluation", "Introduction of a combination law for RBs", "Development of a reasoning boundary division mechanism for multimodal capabilities"], "limitations": "", "keywords": ["Chain-of-Thought", "reasoning boundary", "large language models", "multimodal perception", "optimization strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312.pdf", "abs": "https://arxiv.org/abs/2505.13312", "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "AI": {"tldr": "GUARD framework enables dynamic unlearning in LLMs at inference time, preventing the generation of forgotten knowledge without performance loss.", "motivation": "To ensure safety and compliance of LLMs by enabling selective forgetting of specific knowledge during generation, as existing methods degrade overall performance.", "method": "Proposes a framework called GUARD that uses a prompt classifier to detect unlearning targets and penalizes candidate tokens dynamically during LLM generation.", "result": "GUARD achieves strong forget quality in unlearning tasks while maintaining the utility of the model, with experimental results showing effectiveness on various datasets.", "conclusion": "Dynamic unlearning using GUARD allows LLMs to forget specific knowledge without compromising text fluency or overall model performance.", "key_contributions": ["Introduction of GUARD framework for generation-time unlearning", "Implementation of a prompt classifier for detecting forbidden tokens", "Demonstration of minimal performance degradation while achieving effective unlearning"], "limitations": "", "keywords": ["Large Language Models", "unlearning", "dynamic generation", "forgetting", "safety compliance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328.pdf", "abs": "https://arxiv.org/abs/2505.13328", "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.", "AI": {"tldr": "The paper introduces DialogTool, a multi-turn dialogue dataset for assessing Language Models in stateful tool interactions, and VirtualMobile, an evaluation environment to test API calls across various LLMs, highlighting their current performance limitations.", "motivation": "To address the gap in assessing Language Models for stateful, multi-turn interactions, particularly in tool use.", "method": "The authors propose DialogTool, a dataset designed to evaluate LMs through multiple stages of tool interactions, and use VirtualMobile to simulate API calls for assessment.", "result": "The evaluation of 13 LLMs demonstrated that existing state-of-the-art models struggle with effective tool use over extended interactions.", "conclusion": "The findings indicate significant performance gaps in current LLMs when handling multi-turn dialogue and tool interactions, suggesting room for improvement in future models.", "key_contributions": ["Introduction of DialogTool for evaluating multi-turn tool interactions.", "Development of VirtualMobile for simulating API calls.", "Comprehensive evaluation revealing limitations of current LLMs in stateful interactions."], "limitations": "The paper may not address all types of tool interactions or the full complexity of dialogue beyond the defined tasks.", "keywords": ["Language Models", "tool interactions", "multi-turn dialogue", "DialogTool", "VirtualMobile"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338.pdf", "abs": "https://arxiv.org/abs/2505.13338", "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "AI": {"tldr": "A novel framework for generating QA datasets from speech data that integrates contextual reasoning with paralinguistic understanding.", "motivation": "To address the limitations of current speech-LLMs in contextual reasoning and paralinguistic understanding due to insufficient QA datasets.", "method": "The framework uses pseudo paralinguistic label-based data condensation of in-the-wild speech and generates QA through LLM-based Contextual Paralinguistic QA (CPQA) generation.", "result": "Validated effectiveness of the framework through strong correlation in evaluations with both a dataset created by the framework and a human-generated CPQA dataset.", "conclusion": "The proposed framework is innovative and has the potential to develop more robust speech-LLMs that can handle empathetic reasoning tasks effectively.", "key_contributions": ["Integration of contextual reasoning and paralinguistic information in speech-LLM training.", "Creation of a novel dataset generation framework utilizing in-the-wild speech data.", "Validation through correlation with human-generated datasets."], "limitations": "Limited capability of speech-LLMs in empathetic reasoning tasks was revealed during evaluations.", "keywords": ["speech-LLMs", "paralinguistic understanding", "contextual reasoning", "dataset generation", "empathetic reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346.pdf", "abs": "https://arxiv.org/abs/2505.13346", "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "AI": {"tldr": "This paper focuses on enhancing the evaluation capabilities of large language models (LLMs) in reasoning-intensive tasks using reinforcement learning, proposing a new algorithm and benchmark for performance assessment.", "motivation": "With the rise of large language models (LLMs), there is a need to shift from human evaluation to automated evaluation, especially in complex reasoning domains where current models struggle.", "method": "The authors develop the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm to enhance LLM evaluation. They also create the ReasoningJudgeBench as a new benchmark to assess the performance of judges in diverse reasoning contexts.", "result": "The Judge for Reasoning (J4R), trained using EIS-GRPO, surpasses GPT-4o and other smaller models in evaluation accuracy by significant margins, validating the effectiveness of the proposed methods.", "conclusion": "The study demonstrates that reinforcement learning can improve the robustness and performance of LLM judges in complex evaluation settings, indicating a promising direction for LLM evaluation methods.", "key_contributions": ["EIS-GRPO algorithm for robust LLM judge training", "Introduction of ReasoningJudgeBench for diverse reasoning evaluation", "Development of J4R that outperforms existing models", "Key improvements in evaluating reasoning tasks"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Evaluation Benchmarks", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348.pdf", "abs": "https://arxiv.org/abs/2505.13348", "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.", "AI": {"tldr": "The paper investigates vulnerabilities in LLM-as-a-Judge systems to prompt-injection attacks, highlighting significant susceptibility and the need for robust defenses.", "motivation": "The increasing use of LLMs as evaluators raises concerns regarding their reliability and security, particularly against adversarial manipulations.", "method": "The study formalizes two attack strategies: Comparative Undermining Attack (CUA) and Justification Manipulation Attack (JMA), using the Greedy Coordinate Gradient (GCG) optimization method to craft adversarial inputs.", "result": "Experiments on the MT-Bench Human Judgments dataset demonstrate that the CUA achieves an Attack Success Rate (ASR) exceeding 30%, indicating significant vulnerabilities in current systems.", "conclusion": "The findings illustrate critical weaknesses in LLM-as-a-Judge systems, emphasizing the urgent need for improved defense mechanisms and further exploration of adversarial robustness.", "key_contributions": ["Identification of attack strategies against LLM-as-a-Judge systems", "Empirical validation of vulnerabilities using specific LLMs", "Highlighting the need for enhancing trustworthiness in LLM evaluations"], "limitations": "", "keywords": ["Large Language Models", "evaluation", "adversarial attacks", "robustness", "trustworthiness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353.pdf", "abs": "https://arxiv.org/abs/2505.13353", "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "authors": ["Adam Å torek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.", "AI": {"tldr": "This paper examines the effectiveness of Large Language Models (LLMs) in code reasoning within large repositories, focusing on the distinction between lexical and semantic code recall.", "motivation": "To understand how well LLMs utilize long contexts for code reasoning and the role of recall in their effectiveness.", "method": "Introduces SemTrace, a code reasoning technique to measure semantic recall, and evaluates state-of-the-art LLMs on their code reasoning abilities.", "result": "Significant accuracy drop in code reasoning as snippets approach the middle of the context, with varying recall performance based on lexical granularity.", "conclusion": "Current benchmarks may not accurately reflect LLMs' challenges in leveraging in-context information due to low semantic recall sensitivity.", "key_contributions": ["Introduced SemTrace for assessing semantic recall in code", "Identified the performance drop in code reasoning accuracy with long context", "Revealed disconnect between lexical and semantic recall mechanisms."], "limitations": "Current benchmarks likely underrepresent the semantic recall challenges faced by LLMs.", "keywords": ["Large Language Models", "Code Reasoning", "Semantic Recall", "Lexical Recall", "Benchmarks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360.pdf", "abs": "https://arxiv.org/abs/2505.13360", "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian KÃ¤stner", "Tongshuang Wu"], "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.", "AI": {"tldr": "This paper analyzes prompt underspecification in LLMs and introduces requirements-aware prompt optimization mechanisms to enhance performance.", "motivation": "To address the issues caused by underspecified prompts from developers when building LLM-powered software.", "method": "In-depth analysis of prompt underspecification followed by the introduction of novel requirements-aware prompt optimization mechanisms.", "result": "Demonstrated that adding more requirements to prompts does not consistently improve performance; proposed methods show an average performance improvement of 4.8%.", "conclusion": "Effective management of prompt underspecification requires a broader approach, including proactive requirements discovery and monitoring.", "key_contributions": ["Analysis of LLM prompt underspecification", "Introduction of requirements-aware prompt optimization mechanisms", "Evidence that simply adding more requirements isn't always effective"], "limitations": "The proposed optimizations may not address all forms of prompt underspecification or generalize across all applications.", "keywords": ["LLM", "prompt optimization", "underspecification", "requirements discovery", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379.pdf", "abs": "https://arxiv.org/abs/2505.13379", "title": "Thinkless: LLM Learns When to Think", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "AI": {"tldr": "Proposal of Thinkless, a learnable framework for LLMs to choose between short-form and long-form reasoning, improving efficiency by reducing unnecessary complex reasoning.", "motivation": "To address the computational inefficiencies in Reasoning Language Models that arise from using complex reasoning for straightforward queries.", "method": "Thinkless uses a reinforcement learning framework with two control tokens to adaptively select reasoning modes, employing a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm to refine learning objectives.", "result": "Thinkless reduces long-chain reasoning usage by 50% - 90% across benchmarks like Minerva Algebra, MATH-500, and GSM8K, enhancing model efficiency.", "conclusion": "Thinkless offers a structured approach to optimize reasoning processes in LLMs, improving both efficiency and accuracy in task responses.", "key_contributions": ["Introduction of a learnable framework for adaptive reasoning in LLMs", "Development of the DeGRPO algorithm for effective hybrid reasoning", "Demonstration of efficiency improvements on standard benchmarks"], "limitations": "", "keywords": ["Reasoning Language Models", "Reinforcement Learning", "Adaptive Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388.pdf", "abs": "https://arxiv.org/abs/2505.13388", "title": "R3: Robust Rubric-Agnostic Reward Models", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "AI": {"tldr": "R3 is a novel framework for reward modeling that enhances controllability and interpretability, aligning language model outputs with human preferences. It supports generalization across evaluation dimensions and enables interpretable score assignments.", "motivation": "Existing reward models for language models often lack controllability and interpretability, limiting their effectiveness in aligning outputs with diverse human preferences.", "method": "We introduce R3, a rubric-agnostic reward modeling framework that focuses on generalizability and provides interpretable score assignments for language models.", "result": "R3 facilitates a more transparent evaluation of language models, aligning them better with human values and allowing for broader applicability across various tasks.", "conclusion": "R3 offers a significant improvement in the way we evaluate language models, making them more aligned with human preferences and more interpretable.", "key_contributions": ["Introduction of a rubric-agnostic reward modeling framework", "Enhanced interpretability of score assignments", "Better generalizability across evaluation dimensions"], "limitations": "", "keywords": ["Reward Models", "Language Models", "Human Preferences"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403.pdf", "abs": "https://arxiv.org/abs/2505.13403", "title": "MR. Judge: Multimodal Reasoner as a Judge", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "AI": {"tldr": "This paper introduces MR. Judge, a new paradigm for utilizing MLLMs as evaluative judges that enhances reasoning capabilities and improves performance in response evaluation tasks.", "motivation": "The need for more effective evaluative judgment mechanisms in LLMs and the desire to enhance interpretability and performance in response selection.", "method": "MR. Judge formulates the judgment process as a reasoning-inspired multiple-choice problem, employing a two-step strategy: generating plausible negative candidates and distilling reasoning capabilities from a text-based model.", "result": "MR. Judge demonstrates enhanced performance, surpassing GPT-4o by 9.9% on VL-RewardBench, and improving MM-Vet performance during inference-time scaling by up to 7.7%.", "conclusion": "The proposed MR. Judge paradigm significantly improves evaluative judging capabilities of MLLMs, making them more effective across various tasks.", "key_contributions": ["Introduction of a reasoning-inspired multiple-choice judgment process", "Development of a strategy for automatic annotation through response synthesis", "Demonstration of improved performance over existing models like GPT-4o"], "limitations": "", "keywords": ["Large Language Models", "Multimodal Models", "Reasoning", "Evaluation", "Artificial Intelligence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404.pdf", "abs": "https://arxiv.org/abs/2505.13404", "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "AI": {"tldr": "Granary is a large-scale collection of speech datasets for recognition and translation across 25 European languages, enhancing low-resource language processing.", "motivation": "To address the scarcity of data for speech processing in low-resource languages and improve multi-task and multilingual approaches.", "method": "A pseudo-labeling pipeline is used for data enhancement, incorporating segmentation, two-pass inference, hallucination filtering, and punctuation restoration, along with translation pair generation from pseudo-labeled transcriptions using EuroLLM.", "result": "Models trained on the processed data show similar performance to those trained on curated datasets, using approximately 50% less data for both high- and low-resource languages.", "conclusion": "The Granary dataset represents a significant step forward in open-source speech recognition and translation for multiple languages, with a highly efficient processing methodology.", "key_contributions": ["First open-source collection of speech datasets for recognition and translation across 25 European languages", "Enhanced data quality through a sophisticated pseudo-labeling pipeline", "Demonstrated improved model performance with reduced data requirements"], "limitations": "", "keywords": ["speech processing", "low-resource languages", "dataset development"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417.pdf", "abs": "https://arxiv.org/abs/2505.13417", "title": "AdaptThink: Reasoning Models Can Learn When to Think", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "AI": {"tldr": "This paper introduces AdaptThink, a novel RL algorithm that enables reasoning models to choose between deep thinking and skipping thinking to optimize performance and efficiency in response times.", "motivation": "To address the inefficiency caused by lengthy reasoning processes in large reasoning models, which increases inference overhead.", "method": "AdaptThink utilizes a constrained optimization objective and an importance sampling strategy to allow reasoning models to adaptively choose between NoThinking and traditional thinking modes during training.", "result": "AdaptThink significantly reduces inference costs by 53% and improves accuracy by 2.4% on multiple math datasets, demonstrating enhanced performance without sacrificing efficiency.", "conclusion": "Adaptive thinking-mode selection via AdaptThink presents a promising approach to enhance the balance between reasoning quality and efficiency in large reasoning models.", "key_contributions": ["Introduction of the AdaptThink RL algorithm for adaptive thinking mode selection", "Demonstration of improved efficiency in reasoning models with significant reduction in average response length", "Enhanced performance on math datasets while maintaining accuracy"], "limitations": "", "keywords": ["Reinforcement Learning", "Reasoning Models", "Adaptive Thinking"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418.pdf", "abs": "https://arxiv.org/abs/2505.13418", "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "AI": {"tldr": "This paper investigates how dementia is perceived through language by non-experts and LLMs, revealing inconsistencies in human perceptions and a richer feature set used by LLMs.", "motivation": "To understand how language reflects cognitive decline in individuals and the role of non-experts and LLMs in perceiving these changes.", "method": "Transcribed picture descriptions were presented to non-expert humans and LLMs for intuitive judgment of whether the text was produced by someone healthy or with dementia, utilizing an explainable method to extract linguistic features and logistic regression for analysis.", "result": "Human perceptions of dementia were inconsistent and relied on a limited set of cues, while LLMs utilized a richer, more nuanced feature set that aligned with clinical diagnoses, although both groups had difficulties recognizing dementia.", "conclusion": "An interpretable framework was developed to assist non-experts in recognizing significant linguistic signs of dementia, revealing potential to improve early identification of cognitive decline.", "key_contributions": ["Introduced an explainable method for extracting features from language related to dementia.", "Compared non-expert and LLM perceptions of dementia with clinical diagnoses.", "Highlighted the limitations of human judgment in recognizing dementia signs."], "limitations": "Both non-experts and LLMs were prone to false negatives, often overlooking dementia cases.", "keywords": ["dementia", "language perception", "large language models", "human-computer interaction", "explainable AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434.pdf", "abs": "https://arxiv.org/abs/2505.13434", "title": "SMOTExT: SMOTE meets Large Language Models", "authors": ["Mateusz BystroÅski", "MikoÅaj HoÅysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "AI": {"tldr": "SMOTExT is a novel technique for addressing data scarcity and class imbalance in NLP by generating synthetic text examples using BERT embeddings and xRAG architecture.", "motivation": "To tackle challenges of data scarcity and class imbalance in NLP models, particularly in specialized domains or low-resource settings.", "method": "SMOTExT adapts the Synthetic Minority Over-sampling Technique (SMOTE) to textual data by interpolating BERT-based embeddings to generate synthetic examples and decoding them into coherent text using xRAG architecture.", "result": "Preliminary results show that models trained on synthetic data can achieve performance comparable to those trained on original datasets, indicating potential in knowledge distillation and data augmentation.", "conclusion": "This technique shows potential for enhancing NLP models while adhering to data privacy constraints, offering a new avenue for effective learning.", "key_contributions": ["Introduction of SMOTExT for synthetic text generation", "Demonstration of effective use of BERT-based embeddings for data augmentation", "Establishment of a framework for privacy-preserving NLP model training"], "limitations": "Preliminary work with only qualitative outputs.", "keywords": ["SMOTExT", "data augmentation", "NLP", "class imbalance", "privacy-preserving"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444.pdf", "abs": "https://arxiv.org/abs/2505.13444", "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "AI": {"tldr": "This paper explores the challenges of chart understanding for large vision-language models (LVLMs) and introduces ChartMuseum, a benchmark to evaluate visual and textual reasoning in this context.", "motivation": "To address the disparity in performance between human visual reasoning and that of LVLMs when interpreting complex charts.", "method": "Conducted a case study using a synthetic dataset for visual reasoning, evaluated performance across LVLMs, and introduced the ChartMuseum benchmark with expert-annotated questions.", "result": "The study revealed a significant drop in model performance (best model at 63% accuracy) compared to humans (93% accuracy), especially on questions requiring visual reasoning, where models drop 35%-55% in performance.", "conclusion": "There's a substantial gap in performance between humans and LVLMs on chart understanding tasks, highlighting the limitations of current models in complex visual reasoning.", "key_contributions": ["Introduction of ChartMuseum benchmark for chart QA", "Demonstration of model performance degradation with visual complexity", "Qualitative analysis of error categories in visual reasoning for LVLMs"], "limitations": "", "keywords": ["chart understanding", "vision-language models", "benchmark", "visual reasoning", "human performance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448.pdf", "abs": "https://arxiv.org/abs/2505.13448", "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "AI": {"tldr": "This paper presents a method for continuous control of language model (LM) outputs, focusing on response length customization, demonstrating improvements over existing discrete control techniques.", "motivation": "Improving user experience by allowing more nuanced control over language model outputs to align better with user intent.", "method": "The authors propose a technique that utilizes continuous control signals, allowing users to manipulate LM responses along a spectrum rather than relying on rigid discrete signals or prompts.", "result": "The proposed method shows improved reliability in exerting control over response length compared to traditional in-context learning and discrete fine-tuning methods.", "conclusion": "Fine-tuning language models with continuous control signals results in more effective customization of generated outputs, with open-source resources provided for further research.", "key_contributions": ["Introduction of continuous control signals for language model outputs", "Demonstration of improved response-length control", "Provision of open-source code and datasets for further research"], "limitations": "The study focuses primarily on response-length control and may not generalize to other types of linguistic features.", "keywords": ["language models", "user intent", "continuous control", "response length", "fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2305.00948", "pdf": "https://arxiv.org/pdf/2305.00948.pdf", "abs": "https://arxiv.org/abs/2305.00948", "title": "Large Linguistic Models: Investigating LLMs' metalinguistic abilities", "authors": ["GaÅ¡per BeguÅ¡", "Maksymilian DÄbkowski", "Ryan Rhodes"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) has recently improved to the\npoint where models can perform well on many language tasks. We show here\nthat--for the first time--the models can also generate valid metalinguistic\nanalyses of language data. We outline a research program where the behavioral\ninterpretability of LLMs on these tasks is tested via prompting. LLMs are\ntrained primarily on text--as such, evaluating their metalinguistic abilities\nimproves our understanding of their general capabilities and sheds new light on\ntheoretical models in linguistics. We show that OpenAI's (2024) o1 vastly\noutperforms other models on tasks involving drawing syntactic trees and\nphonological generalization. We speculate that OpenAI o1's unique advantage\nover other models may result from the model's chain-of-thought mechanism, which\nmimics the structure of human reasoning used in complex cognitive tasks, such\nas linguistic analysis.", "AI": {"tldr": "The paper demonstrates that large language models can generate valid metalinguistic analyses and outlines a research program to evaluate their interpretability through prompting.", "motivation": "To improve understanding of large language models (LLMs) by evaluating their metalinguistic abilities and their implications for theoretical models in linguistics.", "method": "The study tests the behavioral interpretability of LLMs through prompting and compares the performance of different models on tasks related to syntactic trees and phonological generalization.", "result": "OpenAI's o1 significantly outperforms other LLMs in tasks involving metalinguistic analyses, suggesting its chain-of-thought mechanism enhances its linguistic capabilities.", "conclusion": "The unique advantages of OpenAI o1 in linguistic tasks provide insights into LLM performance and their potential applications in understanding human language processing.", "key_contributions": ["First demonstration of LLMs generating valid metalinguistic analyses", "Identification of OpenAI's o1 as superior in syntactic and phonological tasks", "Introduction of a research program for evaluating LLM interpretability via prompting"], "limitations": "", "keywords": ["large language models", "metalinguistic analysis", "interpretability", "syntactic trees", "phonological generalization"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2305.13673", "pdf": "https://arxiv.org/pdf/2305.13673.pdf", "abs": "https://arxiv.org/abs/2305.13673", "title": "Physics of Language Models: Part 1, Learning Hierarchical Language Structures", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "V2 polishes writing and adds Appendix G; V3 polishes writing and\n  changes the title; V4 improves writing and adds Appendix H (more uniform\n  attention results)", "summary": "Transformer-based language models are effective but complex, and\nunderstanding their inner workings and reasoning mechanisms is a significant\nchallenge. Previous research has primarily explored how these models handle\nsimple tasks like name copying or selection, and we extend this by\ninvestigating how these models perform recursive language structure reasoning\ndefined by context-free grammars (CFGs). We introduce a family of synthetic\nCFGs that produce hierarchical rules, capable of generating lengthy sentences\n(e.g., hundreds of tokens) that are locally ambiguous and require dynamic\nprogramming to parse. Despite this complexity, we demonstrate that generative\nmodels like GPT can accurately learn and reason over CFG-defined hierarchies\nand generate sentences based on it. We explore the model's internals, revealing\nthat its hidden states precisely capture the structure of CFGs, and its\nattention patterns resemble the information passing in a dynamic programming\nalgorithm.\n  This paper also presents several corollaries, including showing why absolute\npositional embeddings is inferior to relative and rotary embeddings; uniform\nattention alone is surprisingly effective (motivating our follow-up work on\nCanon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep\nstructure reasoning on CFGs compared to autoregressive models (e.g., GPT); and\ninjecting structural or syntactic noise into pretraining data markedly improves\nrobustness to corrupted language prompts.", "AI": {"tldr": "This paper investigates how transformer-based language models, particularly generative models like GPT, understand and perform recursive language structure reasoning defined by context-free grammars (CFGs).", "motivation": "Understanding the internal workings and reasoning mechanisms of transformer-based language models, especially their ability to handle complex linguistic structures.", "method": "The authors introduce a family of synthetic CFGs to test the models' proficiency in parsing hierarchical sentences. They analyze model internals to reveal how hidden states capture CFG structures and examine attention patterns.", "result": "Generative models like GPT can learn and reason over CFG-defined hierarchies accurately, with hidden states reflecting CFG structures and attention patterns akin to dynamic programming processes.", "conclusion": "The study highlights the effectiveness of transformer models in reasoning over complex linguistic structures and provides insights into the limitations of other model architectures, while suggesting improvements in pretraining strategies.", "key_contributions": ["Introduction of synthetic CFGs for testing language models", "Demonstration of GPT's capability in generating CFG-compliant sentences", "Analysis of attention mechanisms related to dynamic programming"], "limitations": "Focuses on generative models and may not generalize to all types of transformer models.", "keywords": ["transformer models", "context-free grammars", "recursive reasoning", "language structures", "GPT"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2310.10378", "pdf": "https://arxiv.org/pdf/2310.10378.pdf", "abs": "https://arxiv.org/abs/2310.10378", "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models", "authors": ["Jirui Qi", "Raquel FernÃ¡ndez", "Arianna Bisazza"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "EMNLP2023 Outstanding Paper (Multilinguality and Linguistic Diversity\n  Track). All code and data are released at\n  https://github.com/Betswish/Cross-Lingual-Consistency", "summary": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to\nstore considerable amounts of factual knowledge, but large variations are\nobserved across languages. With the ultimate goal of ensuring that users with\ndifferent language backgrounds obtain consistent feedback from the same model,\nwe study the cross-lingual consistency (CLC) of factual knowledge in various\nmultilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC)\nmetric to evaluate knowledge consistency across languages independently from\naccuracy. Using this metric, we conduct an in-depth analysis of the determining\nfactors for CLC, both at model level and at language-pair level. Among other\nresults, we find that increasing model size leads to higher factual probing\naccuracy in most languages, but does not improve cross-lingual consistency.\nFinally, we conduct a case study on CLC when new factual associations are\ninserted in the PLMs via model editing. Results on a small sample of facts\ninserted in English reveal a clear pattern whereby the new piece of knowledge\ntransfers only to languages with which English has a high RankC score.", "AI": {"tldr": "The paper investigates cross-lingual consistency of factual knowledge in multilingual PLMs and introduces a new metric, RankC, to evaluate it.", "motivation": "To ensure consistent feedback from multilingual PLMs for users across different language backgrounds.", "method": "The authors propose the Ranking-based Consistency (RankC) metric and analyze cross-lingual consistency factors at both model and language-pair levels.", "result": "The analysis shows that larger model sizes improve factual accuracy but not cross-lingual consistency; knowledge transfer occurs primarily to languages with high RankC scores.", "conclusion": "The study highlights that while model size affects accuracy, it does not guarantee cross-lingual consistency, which is influenced by specific language pairs.", "key_contributions": ["Introduction of the RankC metric for cross-lingual consistency evaluation", "In-depth analysis of factors affecting cross-lingual consistency", "Case study on factual knowledge transfer between languages based on RankC scores."], "limitations": "The findings are based on a small sample of facts and may not generalize to all languages or model architectures.", "keywords": ["Multilingual PLMs", "Cross-lingual consistency", "Factual knowledge"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2310.18290", "pdf": "https://arxiv.org/pdf/2310.18290.pdf", "abs": "https://arxiv.org/abs/2310.18290", "title": "Automatically generating Riddles aiding Concept Attainment", "authors": ["Niharika Sri Parasa", "Chaitali Diwan", "Srinath Srinivasa"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "One of the primary challenges in online learning environments, is to retain\nlearner engagement. Several different instructional strategies are proposed\nboth in online and offline environments to enhance learner engagement. The\nConcept Attainment Model is one such instructional strategy that focuses on\nlearners acquiring a deeper understanding of a concept rather than just its\ndictionary definition. This is done by searching and listing the properties\nused to distinguish examples from non-examples of various concepts. Our work\nattempts to apply the Concept Attainment Model to build conceptual riddles, to\ndeploy over online learning environments. The approach involves creating\nfactual triples from learning resources, classifying them based on their\nuniqueness to a concept into `Topic Markers' and `Common', followed by\ngenerating riddles based on the Concept Attainment Model's format and capturing\nall possible solutions to those riddles. The results obtained from the human\nevaluation of riddles prove encouraging.", "AI": {"tldr": "The paper demonstrates the application of the Concept Attainment Model to develop conceptual riddles aimed at improving learner engagement in online environments.", "motivation": "The primary challenge of retaining learner engagement in online learning environments necessitates innovative instructional strategies.", "method": "The paper applies the Concept Attainment Model to create factual triples from learning resources, classifying them into 'Topic Markers' and 'Common', and then generates riddles based on this framework.", "result": "Human evaluation of the created riddles shows encouraging results in enhancing engagement.", "conclusion": "The application of the Concept Attainment Model in creating riddles has the potential to positively impact learner engagement in online education.", "key_contributions": ["Introduction of conceptual riddles as a strategy for engaging learners.", "Utilization of the Concept Attainment Model in an innovative context.", "Positive results from human evaluations of the riddles created."], "limitations": "The scalability of this approach and its effectiveness across diverse subjects need further investigation.", "keywords": ["Concept Attainment Model", "learner engagement", "online learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2402.01172", "pdf": "https://arxiv.org/pdf/2402.01172.pdf", "abs": "https://arxiv.org/abs/2402.01172", "title": "Streaming Sequence Transduction through Dynamic Compression", "authors": ["Weiting Tan", "Yunmo Chen", "Tongfei Chen", "Guanghui Qin", "Haoran Xu", "Heidi C. Zhang", "Benjamin Van Durme", "Philipp Koehn"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "We introduce STAR (Stream Transduction with Anchor Representations), a novel\nTransformer-based model designed for efficient sequence-to-sequence\ntransduction over streams. STAR dynamically segments input streams to create\ncompressed anchor representations, achieving nearly lossless compression (12x)\nin Automatic Speech Recognition (ASR) and outperforming existing methods.\nMoreover, STAR demonstrates superior segmentation and latency-quality\ntrade-offs in simultaneous speech-to-text tasks, optimizing latency, memory\nfootprint, and quality.", "AI": {"tldr": "STAR is a Transformer-based model that efficiently performs sequence-to-sequence transduction for streams, achieving significant compression and improving ASR performance.", "motivation": "To address efficiency in sequence-to-sequence tasks over streaming data, focusing on automatic speech recognition.", "method": "Utilizes a Transformer architecture to create compressed anchor representations from dynamically segmented input streams.", "result": "Achieves nearly lossless compression (12x) in ASR and shows improved segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks.", "conclusion": "STAR optimizes performance metrics like latency and memory usage while maintaining high-quality outputs during speech-to-text conversion.", "key_contributions": ["Introduction of STAR model for sequence transduction over streams", "Achieved significant compression in ASR", "Improved segmentation and quality optimization for simultaneous speech-to-text tasks"], "limitations": "", "keywords": ["STAR", "Transformer", "stream transduction", "automatic speech recognition", "speech-to-text"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2402.10528", "pdf": "https://arxiv.org/pdf/2402.10528.pdf", "abs": "https://arxiv.org/abs/2402.10528", "title": "Can We Verify Step by Step for Incorrect Answer Detection?", "authors": ["Xin Xu", "Shizhe Diao", "Can Yang", "Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted to IJCAI 2025", "summary": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy.", "AI": {"tldr": "This paper introduces R2PE, a benchmark for predicting LLM output accuracy by analyzing reasoning chains, along with a novel framework that enhances performance in reasoning tasks.", "motivation": "The study aims to fill the gap in understanding the correlation between reasoning chains generated by LLMs and their output accuracy.", "method": "Introduction of the R2PE benchmark to evaluate reasoning chain performance across different domains, and development of the process discernibility score (PDS) framework to enhance reasoning task results.", "result": "The PDS framework showed a notable increase in F1 score (5.1%) and improvement in AUC-PR (2.97%) compared to the existing answer-checking baseline.", "conclusion": "The findings highlight the importance of reasoning chains in predicting LLM performance, with the PDS framework proving effective in enhancing QA accuracy.", "key_contributions": ["Introduction of R2PE benchmark for reasoning chain evaluation", "Development of the process discernibility score (PDS) framework", "Demonstration of improved performance metrics in reasoning tasks"], "limitations": "", "keywords": ["chain-of-thought prompting", "large language models", "reasoning", "benchmark", "AI in health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2402.12692", "pdf": "https://arxiv.org/pdf/2402.12692.pdf", "abs": "https://arxiv.org/abs/2402.12692", "title": "FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning", "authors": ["Xiao Li", "Bolin Zhu", "Kaiwen Shi", "Sichen Liu", "Yin Zhu", "Yiwei Liu", "Gong Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "The application of formulas (e.g., physics formulas) is a fundamental ability\nof humans when solving numerical reasoning problems. Existing numerical\nreasoning datasets seldom explicitly indicate the formulas employed in\nreasoning, as their questions rely on implicit commonsense mathematical\nknowledge. In contrast, in this paper, we introduce FormulaReasoning, a new\ndataset specifically designed for formula-based numerical reasoning. Each of\nthe 4,751 questions in our dataset requires numerical calculation with external\nphysics formulas, making it a more challenging benchmark for evaluating large\nlanguage models (LLMs). We offer normalized fine-grained annotations for the\nquestions, available in English and Chinese, including formula structures,\nparameter names, symbols, numerical values, and units, derived from extensive\nmanual effort with LLM assistance for guaranteed quality. We also provide a\nconsolidated formula database to serve as an external knowledge base\naccompanying the dataset. We employ FormulaReasoning to evaluate LLMs with 7B\nto over 100B parameters, and explore retrieval-augmented generation with the\nformula database. Our evaluation also covers supervised methods that break down\nthe reasoning process into formula generation, parameter extraction, and\nnumerical calculation, as well as direct preference optimization methods based\non derived preference data.", "AI": {"tldr": "Introduction of the FormulaReasoning dataset, designed for formula-based numerical reasoning, challenging LLMs with physics formula-based questions.", "motivation": "To create a dataset that explicitly indicates the formulas used in numerical reasoning, addressing the limitation of existing datasets that rely on implicit knowledge.", "method": "Development of the FormulaReasoning dataset with 4,751 formula-based questions and fine-grained annotations for enhanced reasoning tasks. Evaluation of LLMs and exploration of retrieval-augmented generation with a formula database.", "result": "The dataset allows for more challenging evaluations of LLMs and facilitates the breakdown of reasoning into formula generation, parameter extraction, and numerical calculation.", "conclusion": "FormulaReasoning provides a structured approach to evaluating machine learning models in solving numerical reasoning problems using physics formulas, combined with extensive annotation and a supporting formula database.", "key_contributions": ["Introduction of a comprehensive dataset for formula-based numerical reasoning.", "Fine-grained annotations for questions that include structures and symbols.", "Evaluation framework for LLMs using a large-scale formula database."], "limitations": "", "keywords": ["numerical reasoning", "language models", "physics formulas", "dataset", "HCI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2402.12819", "pdf": "https://arxiv.org/pdf/2402.12819.pdf", "abs": "https://arxiv.org/abs/2402.12819", "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance", "authors": ["Branislav Pecher", "Ivan Srba", "Maria Bielikova"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When solving NLP tasks with limited labelled data, researchers typically\neither use a general large language model without further update, or use a\nsmall number of labelled samples to tune a specialised smaller model. In this\nwork, we answer an important question -- how many labelled samples are required\nfor the specialised small models to outperform general large models, while\ntaking the performance variance into consideration. By observing the behaviour\nof fine-tuning, instruction-tuning, prompting and in-context learning on 8\nlanguage models, we identify such performance break-even points across 8\nrepresentative text classification tasks of varying characteristics. We show\nthat the specialised models often need only few samples (on average $100$) to\nbe on par or better than the general ones. At the same time, the number of\nrequired labels strongly depends on the dataset or task characteristics, with\nfine-tuning on binary datasets requiring significantly more samples. When\nperformance variance is taken into consideration, the number of required labels\nincreases on average by $100 - 200\\%$. Finally, larger models do not\nconsistently lead to better performance and lower variance, with 4-bit\nquantisation having negligible impact.", "AI": {"tldr": "The paper investigates the number of labeled samples needed for specialized small models to outperform general large language models in NLP tasks, revealing that only a few samples are often required, but this depends on task characteristics and introduces performance variance considerations.", "motivation": "To determine how many labeled samples are necessary for specialized small models to surpass general large models in NLP tasks, particularly in the context of limited labeled data.", "method": "The study evaluates the performance of fine-tuning, instruction-tuning, prompting, and in-context learning across 8 language models and various text classification tasks to identify performance break-even points.", "result": "Specialized models can match or exceed general models' performance with as few as 100 labeled samples, though this number can vary widely based on task characteristics, especially in binary datasets.", "conclusion": "The number of required labeled samples is task-dependent, significantly increasing when accounting for performance variance; larger models do not always guarantee better outcomes, and quantization impacts are minimal.", "key_contributions": ["Identification of performance break-even points for models with limited labeled data.", "Insights into the dependence of required labeled samples on dataset characteristics.", "Assessment of model size impact on performance variance."], "limitations": "Lack of exploration of specific characteristics of datasets that may influence sample requirements.", "keywords": ["NLP", "large language models", "performance variance", "fine-tuning", "text classification"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2404.17874", "pdf": "https://arxiv.org/pdf/2404.17874.pdf", "abs": "https://arxiv.org/abs/2404.17874", "title": "From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets", "authors": ["Manuel Tonneau", "Diyi Liu", "Samuel Fraiberger", "Ralph Schroeder", "Scott A. Hale", "Paul RÃ¶ttger"], "categories": ["cs.CL"], "comment": "Accepted at WOAH (NAACL 2024). Please cite the ACL Anthology version:\n  https://aclanthology.org/2024.woah-1.23/", "summary": "Perceptions of hate can vary greatly across cultural contexts. Hate speech\n(HS) datasets, however, have traditionally been developed by language. This\nhides potential cultural biases, as one language may be spoken in different\ncountries home to different cultures. In this work, we evaluate cultural bias\nin HS datasets by leveraging two interrelated cultural proxies: language and\ngeography. We conduct a systematic survey of HS datasets in eight languages and\nconfirm past findings on their English-language bias, but also show that this\nbias has been steadily decreasing in the past few years. For three\ngeographically-widespread languages -- English, Arabic and Spanish -- we then\nleverage geographical metadata from tweets to approximate geo-cultural contexts\nby pairing language and country information. We find that HS datasets for these\nlanguages exhibit a strong geo-cultural bias, largely overrepresenting a\nhandful of countries (e.g., US and UK for English) relative to their prominence\nin both the broader social media population and the general population speaking\nthese languages. Based on these findings, we formulate recommendations for the\ncreation of future HS datasets.", "AI": {"tldr": "This paper evaluates cultural bias in hate speech datasets by analyzing language and geography and finds that existing datasets are significantly skewed towards specific countries.", "motivation": "To address and analyze the cultural biases present in hate speech datasets which are predominantly created in English, potentially overlooking cultural contexts of other languages and regions.", "method": "The authors conducted a systematic survey of hate speech datasets in eight languages, examining their geographical metadata to assess geo-cultural biases by correlating language with country information.", "result": "The study confirmed that English-language hate speech datasets exhibit bias, which has decreased over recent years. It also found that datasets in English, Arabic, and Spanish are heavily biased towards a few countries, notably the US and UK for English, despite a more diverse social media and speaker population.", "conclusion": "The findings underline the need for more balanced and representative hate speech datasets that reflect the socio-cultural realities of various regions and languages.", "key_contributions": ["Identification of cultural biases in hate speech datasets across multiple languages.", "Demonstration of decreasing bias in English datasets over time.", "Recommendations for the development of more representative hate speech datasets."], "limitations": "The study is limited to eight languages and may not reflect trends in all languages or cultures. The findings are based on available social media data, which may not capture the complexity of hate speech across different contexts.", "keywords": ["hate speech", "cultural bias", "social media", "datasets", "language"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2405.15525", "pdf": "https://arxiv.org/pdf/2405.15525.pdf", "abs": "https://arxiv.org/abs/2405.15525", "title": "Sparse Matrix in Large Language Model Fine-tuning", "authors": ["Haoze He", "Juncheng Billy Li", "Xuan Jiang", "Heather Miller"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "LoRA and its variants have become popular parameter-efficient fine-tuning\n(PEFT) methods due to their ability to avoid excessive computational costs.\nHowever, an accuracy gap often exists between PEFT methods and full fine-tuning\n(FT), and this gap has yet to be systematically studied. In this work, we\nintroduce a method for selecting sparse sub-matrices that aim to minimize the\nperformance gap between PEFT vs. full fine-tuning (FT) while also reducing both\nfine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT)\nmethod begins by identifying the most significant sub-matrices in the gradient\nupdate, updating only these blocks during the fine-tuning process. In our\nexperiments, we demonstrate that SMT consistently surpasses other PEFT baseline\n(e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA\nacross a broad spectrum of tasks, while reducing the GPU memory footprint by\n67% compared to FT. We also examine how the performance of LoRA and DoRA tends\nto plateau and decline as the number of trainable parameters increases, in\ncontrast, our SMT method does not suffer from such issue.", "AI": {"tldr": "This paper introduces Sparse Matrix Tuning (SMT), a method to enhance parameter-efficient fine-tuning (PEFT) of large language models while reducing resource costs and minimizing accuracy gaps compared to full fine-tuning.", "motivation": "The study aims to address the accuracy gap between PEFT methods like LoRA and DoRA and full fine-tuning, which leads to high computational costs.", "method": "The SMT method selects significant sub-matrices in the gradient update and updates only those during fine-tuning, optimizing performance and resource utilization.", "result": "Experimental results show that SMT outperforms other PEFT methods, achieving better performance on various tasks while reducing GPU memory usage by 67% compared to full fine-tuning.", "conclusion": "SMT provides a more efficient fine-tuning approach without suffering performance degradation, even as trainable parameters increase, unlike existing PEFT methods.", "key_contributions": ["Introduction of the Sparse Matrix Tuning (SMT) method for PEFT.", "Demonstrated significant performance improvement over existing PEFT methods.", "Reduction of GPU memory footprint in fine-tuning large language models."], "limitations": "", "keywords": ["Sparse Matrix Tuning", "parameter-efficient fine-tuning", "large language models"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2405.20947", "pdf": "https://arxiv.org/pdf/2405.20947.pdf", "abs": "https://arxiv.org/abs/2405.20947", "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models", "authors": ["Justin Cui", "Wei-Lin Chiang", "Ion Stoica", "Cho-Jui Hsieh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025, we thank everyone for their valuable\n  suggestions and feedback!", "summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that can\nelicit the over-refusal behaviors of LLMs. This study proposes a novel method\nfor automatically generating large-scale over-refusal datasets. Leveraging this\ntechnique, we introduce OR-Bench, the first large-scale over-refusal benchmark.\nOR-Bench comprises 80,000 over-refusal prompts across 10 common rejection\ncategories, a subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 32 popular LLMs across 8 model families. Our datasets are\npublicly available at https://huggingface.co/bench-llms and our codebase is\nopen-sourced at https://github.com/justincui03/or-bench. We hope this benchmark\ncan help the community develop better safety aligned models.", "AI": {"tldr": "This study introduces OR-Bench, a large-scale benchmark for measuring over-refusal in Large Language Models, proposing a novel method for generating prompts that elicit refusal behaviors.", "motivation": "To address the problem of over-refusal in LLMs which leads to rejection of benign prompts, impeding their helpfulness.", "method": "We propose a novel approach to automatically generate datasets that include over-refusal prompts, creating the OR-Bench benchmark with 80,000 prompts across multiple categories.", "result": "We measure over-refusal across 32 popular LLMs, highlighting the extent of the issue and providing a valuable resource for future research on safety alignment in LLMs.", "conclusion": "The OR-Bench benchmark aims to assist in the development of LLMs that are better aligned with safety practices while minimizing over-refusal.", "key_contributions": ["Introduction of OR-Bench, the first large-scale over-refusal dataset", "Detailed analysis of over-refusal behaviors of 32 LLMs", "Public availability of the dataset and codebase for community use"], "limitations": "", "keywords": ["Large Language Models", "over-refusal", "safety alignment", "dataset generation", "benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.10785", "pdf": "https://arxiv.org/pdf/2406.10785.pdf", "abs": "https://arxiv.org/abs/2406.10785", "title": "ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation", "authors": ["Yurun Song", "Junchen Zhao", "Ian G. Harris", "Sangeetha Abdu Jyothi"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "In this paper, we introduce \\textbf{Share}d \\textbf{Lo}w \\textbf{R}ank\n\\textbf{A}daptation (ShareLoRA), a Large Language Model (LLM) fine-tuning\ntechnique that balances parameter efficiency, adaptability, and robustness\nwithout compromising performance. By strategically sharing the low-rank weight\nmatrices across different layers, ShareLoRA achieves 44\\% to 96\\% reduction in\ntrainable parameters compared to standard LoRA, alongside a substantial\ndecrease in memory overhead. This efficiency gain scales with model size,\nmaking ShareLoRA particularly advantageous for resource-constrained\nenvironments. Importantly, ShareLoRA not only maintains model performance but\nalso exhibits robustness in both classification and generation tasks across\ndiverse models, including RoBERTa, GPT-2, and LLaMA series (1, 2, and 3). It\nconsistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning\nscenarios, achieving up to 1.2\\% average accuracy improvement, and enhanced\ngeneralization across domains. In continual learning settings, ShareLoRA\nachieves 1.2\\% higher accuracy on GSM8K, 0.6\\% on HumanEval, and 0.5\\% on both\nMMLU and MMLU-Pro. Our results demonstrate that ShareLoRA supports high-quality\nfine-tuning while offering strong generalization and continual adaptation\nacross various model scales and diverse tasks.", "AI": {"tldr": "ShareLoRA is a fine-tuning technique for Large Language Models that enhances parameter efficiency and robustness by sharing low-rank weight matrices across layers, resulting in significant reductions in trainable parameters and memory overhead without compromising performance.", "motivation": "To improve the efficiency and adaptability of fine-tuning techniques for Large Language Models (LLMs) while maintaining performance, especially in resource-constrained environments.", "method": "ShareLoRA shares low-rank weight matrices across different layers of LLMs, leading to reduced trainable parameters and memory usage.", "result": "ShareLoRA achieves 44% to 96% reduction in trainable parameters and exhibits improved accuracy in zero-shot, few-shot, and continual learning settings, outperforming standard LoRA with an average accuracy improvement of up to 1.2%.", "conclusion": "ShareLoRA facilitates high-quality fine-tuning with strong generalization and continual adaptation across various tasks and model sizes, making it beneficial for diverse applications in LLMs.", "key_contributions": ["Introduction of ShareLoRA as a new LLM fine-tuning technique.", "Significant reduction in trainable parameters and memory requirements compared to LoRA.", "Demonstrated robustness and improved accuracy across multiple models and tasks."], "limitations": "", "keywords": ["Large Language Models", "Fine-tuning", "Low-rank adaptation"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2406.16330", "pdf": "https://arxiv.org/pdf/2406.16330.pdf", "abs": "https://arxiv.org/abs/2406.16330", "title": "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "authors": ["Deyuan Liu", "Zhanyue Qin", "Hairu Wang", "Zhao Yang", "Zecheng Wang", "Fangying Rong", "Qingbin Liu", "Yanchao Hao", "Xi Chen", "Cunhang Fan", "Zhao Lv", "Zhiying Tu", "Dianhui Chu", "Bo Li", "Dianbo Sui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) excel in many domains, their complexity\nand scale challenge deployment in resource-limited environments. Current\ncompression techniques, such as parameter pruning, often fail to effectively\nutilize the knowledge from pruned parameters. To address these challenges, we\npropose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA),\na novel approach that uses manifold learning and the Normalized Pairwise\nInformation Bottleneck (NPIB) measure to merge similar layers, reducing model\nsize while preserving essential performance. We evaluate MKA on multiple\nbenchmark datasets and various LLMs. Our findings show that MKA not only\npreserves model performance but also achieves substantial compression ratios,\noutperforming traditional pruning methods. Moreover, when coupled with\nquantization, MKA delivers even greater compression. Specifically, on the MMLU\ndataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75%\nwith a minimal performance decrease of only 2.82\\%. The proposed MKA method\noffers a resource-efficient and performance-preserving model compression\ntechnique for LLMs.", "AI": {"tldr": "MKA introduces a new compression technique for large language models that preserves performance while significantly reducing size.", "motivation": "To improve the deployment of LLMs in resource-limited environments by addressing the inefficiencies of current compression techniques.", "method": "The proposed Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA) uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) to merge similar layers of neural networks.", "result": "MKA outperforms traditional pruning methods, achieving a compression ratio of 43.75% on the Llama3-8B model with minimal performance loss.", "conclusion": "MKA is an effective model compression approach for LLMs that maintains performance while offering substantial reduction in model size.", "key_contributions": ["Introduction of MKA for merging similar layers in LLMs", "Substantial compression ratios achieved", "Performance preservation even with high compression"], "limitations": "", "keywords": ["large language models", "model compression", "manifold learning"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2406.17513", "pdf": "https://arxiv.org/pdf/2406.17513.pdf", "abs": "https://arxiv.org/abs/2406.17513", "title": "Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Lei Shi", "Andreas Bulling"], "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2024 Workshop on Mechanistic Interpretability version:\n  https://openreview.net/forum?id=yEwEVoH9Be", "summary": "Despite growing interest in Theory of Mind (ToM) tasks for evaluating\nlanguage models (LMs), little is known about how LMs internally represent\nmental states of self and others. Understanding these internal mechanisms is\ncritical - not only to move beyond surface-level performance, but also for\nmodel alignment and safety, where subtle misattributions of mental states may\ngo undetected in generated outputs. In this work, we present the first\nsystematic investigation of belief representations in LMs by probing models\nacross different scales, training regimens, and prompts - using control tasks\nto rule out confounds. Our experiments provide evidence that both model size\nand fine-tuning substantially improve LMs' internal representations of others'\nbeliefs, which are structured - not mere by-products of spurious correlations -\nyet brittle to prompt variations. Crucially, we show that these representations\ncan be strengthened: targeted edits to model activations can correct wrong ToM\ninferences.", "AI": {"tldr": "This paper investigates how language models (LMs) represent mental states (Theory of Mind) and finds that larger models with fine-tuning have structured belief representations but are sensitive to prompt variations.", "motivation": "The work aims to understand internal mechanisms of LMs in representing mental states, which is essential for improving model alignment and ensuring safety due to potential misinterpretations of mental states in generated outputs.", "method": "The authors systematically probe LMs of different sizes and training regimens using control tasks to investigate belief representations, ruling out confounds.", "result": "Findings indicate that model size and fine-tuning enhance the internal representations of others' beliefs, which are fundamentally structured rather than results of random correlations, although they are fragile against changes in prompts.", "conclusion": "The study concludes that targeted edits to model activations can strengthen the representations and correct inaccurate Theory of Mind inferences.", "key_contributions": ["First systematic investigation of belief representations in language models", "Evidence that model fine-tuning and size enhance representations of beliefs", "Demonstration that targeted edits can correct ToM inferences"], "limitations": "The representations are brittle to prompt variations, which may affect practical utility.", "keywords": ["Theory of Mind", "language models", "mental state representation", "model alignment", "fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2407.00248", "pdf": "https://arxiv.org/pdf/2407.00248.pdf", "abs": "https://arxiv.org/abs/2407.00248", "title": "DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising", "authors": ["Zhenhao Li", "Huichi Zhou", "Marek Rei", "Lucia Specia"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Pretrained language models have significantly advanced performance across\nvarious natural language processing tasks. However, adversarial attacks\ncontinue to pose a critical challenge to systems built using these models, as\nthey can be exploited with carefully crafted adversarial texts. Inspired by the\nability of diffusion models to predict and reduce noise in computer vision, we\npropose a novel and flexible adversarial defense method for language\nclassification tasks, DiffuseDef, which incorporates a diffusion layer as a\ndenoiser between the encoder and the classifier. The diffusion layer is trained\non top of the existing classifier, ensuring seamless integration with any model\nin a plug-and-play manner. During inference, the adversarial hidden state is\nfirst combined with sampled noise, then denoised iteratively and finally\nensembled to produce a robust text representation. By integrating adversarial\ntraining, denoising, and ensembling techniques, we show that DiffuseDef\nimproves over existing adversarial defense methods and achieves\nstate-of-the-art performance against common black-box and white-box adversarial\nattacks.", "AI": {"tldr": "DiffuseDef is a novel adversarial defense method for language classification that uses a diffusion layer to improve robustness against adversarial attacks.", "motivation": "The paper addresses the critical challenge of adversarial attacks on pretrained language models, highlighting the need for effective defense mechanisms in natural language processing tasks.", "method": "DiffuseDef incorporates a diffusion layer as a denoiser between the encoder and classifier, trained atop existing classifiers for seamless model integration. It combines adversarial training, denoising, and ensembling techniques to enhance defense capabilities.", "result": "DiffuseDef demonstrates state-of-the-art performance in defending against both black-box and white-box adversarial attacks, surpassing existing defense methods.", "conclusion": "The proposed approach effectively integrates advanced denoising techniques to improve the resilience of language classification models against adversarial disruptions.", "key_contributions": ["Introduces DiffuseDef, a novel adversarial defense method incorporating diffusion models in NLP.", "Demonstrates state-of-the-art defense performance against various adversarial attacks.", "Provides a plug-and-play solution applicable to existing classifiers."], "limitations": "", "keywords": ["adversarial attacks", "language classification", "diffusion models", "neural networks", "defense mechanisms"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.01976", "pdf": "https://arxiv.org/pdf/2407.01976.pdf", "abs": "https://arxiv.org/abs/2407.01976", "title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "authors": ["Jinghui Lu", "Haiyang Yu", "Yanjie Wang", "Yongjie Ye", "Jingqun Tang", "Ziwei Yang", "Binghong Wu", "Qi Liu", "Hao Feng", "Han Wang", "Hao Liu", "Can Huang"], "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "Accept to ACL2025 Findings", "summary": "Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. LayTextLLM\nprojects each bounding box to a single embedding and interleaves it with text,\nefficiently avoiding long sequence issues while leveraging autoregressive\ntraits of LLMs. LayTextLLM not only streamlines the interaction of layout and\ntextual data but also shows enhanced performance in KIE and VQA. Comprehensive\nbenchmark evaluations reveal significant improvements of LayTextLLM, with a\n15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA\nOCR-based LLMs. All resources are available at\nhttps://github.com/LayTextLLM/LayTextLLM.", "AI": {"tldr": "This paper introduces LayTextLLM, a method that interleaves layout and text in LLMs for improved document understanding tasks such as Key Information Extraction (KIE) and Visual Question Answering (VQA).", "motivation": "To enhance document understanding by overcoming limitations in existing methods that integrate spatial layouts with text in LLMs.", "method": "LayTextLLM projects each bounding box to a single embedding and interleaves it with text, allowing for efficient performance without producing overly long text sequences.", "result": "LayTextLLM shows a 15.2% improvement on KIE tasks and a 10.7% improvement on VQA tasks compared to the previous state-of-the-art OCR-based LLMs.", "conclusion": "LayTextLLM effectively streamlines the interaction between layout and textual data while achieving significant performance enhancements in relevant benchmarks.", "key_contributions": ["Introduction of LayTextLLM for better integration of layout and text.", "Proven performance improvements on document understanding tasks over SOTA methods.", "All resources made publicly available for further research."], "limitations": "", "keywords": ["Document Understanding", "Large Language Models", "Key Information Extraction", "Visual Question Answering", "OCR"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2407.18525", "pdf": "https://arxiv.org/pdf/2407.18525.pdf", "abs": "https://arxiv.org/abs/2407.18525", "title": "ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks", "authors": ["Yinghao Zhu", "Junyi Gao", "Zixiang Wang", "Weibin Liao", "Xiaochen Zheng", "Lifang Liang", "Miguel O. Bernabeu", "Yasha Wang", "Lequan Yu", "Chengwei Pan", "Ewen M. Harrison", "Liantao Ma"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in medicine. However,\ntheir utility in non-generative clinical prediction, often presumed inferior to\nspecialized models, remains under-evaluated, leading to ongoing debate within\nthe field and potential for misuse, misunderstanding, or over-reliance due to a\nlack of systematic benchmarking. Our ClinicRealm study addresses this by\nbenchmarking 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods\non unstructured clinical notes and structured Electronic Health Records (EHR).\nKey findings reveal a significant shift: for clinical note predictions, leading\nLLMs (e.g., DeepSeek R1/V3, GPT o3-mini-high) in zero-shot settings now\ndecisively outperform finetuned BERT models. On structured EHRs, while\nspecialized models excel with ample data, advanced LLMs (e.g., GPT-4o, DeepSeek\nR1/V3) show potent zero-shot capabilities, often surpassing conventional models\nin data-scarce settings. Notably, leading open-source LLMs can match or exceed\nproprietary counterparts. These results establish modern LLMs as powerful\nnon-generative clinical prediction tools, particularly with unstructured text\nand offering data-efficient structured data options, thus necessitating a\nre-evaluation of model selection strategies. This research should serve as an\nimportant insight for medical informaticists, AI developers, and clinical\nresearchers, potentially prompting a reassessment of current assumptions and\ninspiring new approaches to LLM application in predictive healthcare.", "AI": {"tldr": "This study benchmarks various GPT-based, BERT-based, and traditional models for clinical prediction tasks, revealing that leading LLMs outperform fine-tuned models in unstructured clinical notes and perform competitively on structured EHRs.", "motivation": "To systematically evaluate the performance of large language models (LLMs) in clinical prediction and address the ongoing debate on their utility compared to specialized models.", "method": "The study benchmarks 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods using unstructured clinical notes and structured Electronic Health Records (EHR).", "result": "The study finds that LLMs like DeepSeek R1/V3 and GPT o3-mini-high significantly outperform fine-tuned BERT models in zero-shot settings and show strong zero-shot capabilities on structured EHRs, often exceeding conventional models in data-scarce settings.", "conclusion": "Modern LLMs are established as powerful tools for non-generative clinical prediction, which necessitates a re-evaluation of model selection strategies in healthcare contexts, particularly for unstructured text.", "key_contributions": ["Benchmarking LLMs against traditional models in clinical prediction.", "Discovery of strong zero-shot capabilities of LLMs on unstructured and structured data.", "Insights into the performance of open-source versus proprietary LLMs in medical applications."], "limitations": "", "keywords": ["Large Language Models", "Clinical Prediction", "Health Informatics", "Machine Learning", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.05517", "pdf": "https://arxiv.org/pdf/2408.05517.pdf", "abs": "https://arxiv.org/abs/2408.05517", "title": "SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning", "authors": ["Yuze Zhao", "Jintao Huang", "Jinghan Hu", "Xingjun Wang", "Yunlin Mao", "Daoze Zhang", "Hong Zhang", "Zeyinzi Jiang", "Zhikai Wu", "Baole Ai", "Ang Wang", "Wenmeng Zhou", "Yingda Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent development in Large Language Models (LLMs) and Multi-modal Large\nLanguage Models (MLLMs) have leverage Attention-based Transformer architectures\nand achieved superior performance and generalization capabilities. They have\nsince covered extensive areas of traditional learning tasks. For instance,\ntext-based tasks such as text-classification and sequence-labeling, as well as\nmulti-modal tasks like Visual Question Answering (VQA) and Optical Character\nRecognition (OCR), which were previously addressed using different models, can\nnow be tackled based on one foundation model. Consequently, the training and\nlightweight fine-tuning of LLMs and MLLMs, especially those based on\nTransformer architecture, has become particularly important. In recognition of\nthese overwhelming needs, we develop SWIFT, a customizable one-stop\ninfrastructure for large models. With support of over $300+$ LLMs and $50+$\nMLLMs, SWIFT stands as the open-source framework that provide the most\ncomprehensive support for fine-tuning large models. In particular, it is the\nfirst training framework that provides systematic support for MLLMs. In\naddition to the core functionalities of fine-tuning, SWIFT also integrates\npost-training processes such as inference, evaluation, and model quantization,\nto facilitate fast adoptions of large models in various application scenarios.\nWith a systematic integration of various training techniques, SWIFT offers\nhelpful utilities such as benchmark comparisons among different training\ntechniques for large models. For fine-tuning models specialized in agent\nframework, we show that notable improvements on the ToolBench leader-board can\nbe achieved by training with customized dataset on SWIFT, with an increase of\n5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in\nhallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.", "AI": {"tldr": "SWIFT is an open-source infrastructure for fine-tuning large language models (LLMs) and multi-modal large language models (MLLMs), providing comprehensive support and systematic training techniques that improve model performance.", "motivation": "To address the overwhelming needs for effective training and fine-tuning of LLMs and MLLMs, which have demonstrated superior capabilities in various text and multi-modal tasks.", "method": "The paper presents SWIFT, a customizable framework designed for fine-tuning over 300 LLMs and 50 MLLMs, integrating post-training processes such as inference, evaluation, and model quantization, along with benchmark comparisons among training techniques.", "result": "Training specialized models on SWIFT leads to significant performance improvements on the ToolBench leaderboard, with improvements ranging from 5.2%-21.8% in the Act.EM metric, a 1.6%-14.1% reduction in hallucinations, and an overall performance increase of 8%-17%.", "conclusion": "SWIFT is the first training framework to systematically support MLLMs and facilitates the fast adoption of large models in various applications.", "key_contributions": ["Development of a customizable infrastructure for fine-tuning LLMs and MLLMs.", "Integration of comprehensive post-training processes to enhance the usability of large models.", "Significant performance enhancements on benchmark tasks using SWIFT."], "limitations": "", "keywords": ["Large Language Models", "Multi-modal Models", "Fine-tuning", "SWIFT", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.08696", "pdf": "https://arxiv.org/pdf/2408.08696.pdf", "abs": "https://arxiv.org/abs/2408.08696", "title": "Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling", "authors": ["Xianzhen Luo", "Yixuan Wang", "Qingfu Zhu", "Zhiming Zhang", "Xuanyu Zhang", "Qing Yang", "Dongliang Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted by ACL2025. Code is\n  [here](https://github.com/Luowaterbi/TokenRecycling). Token Recycling has\n  already merged into [SpecBench](https://github.com/hemingkx/Spec-Bench)", "summary": "Massive parameters of LLMs have made inference latency a fundamental\nbottleneck. Speculative decoding represents a lossless approach to accelerate\ninference through a guess-and-verify paradigm. Some methods rely on additional\narchitectures to guess draft tokens, which need extra training before use.\nAlternatively, retrieval-based training-free techniques build libraries from\npre-existing corpora or by n-gram generation. However, they face challenges\nlike large storage requirements, time-consuming retrieval, and limited\nadaptability. Observing that candidate tokens generated during the decoding\nprocess are likely to reoccur in future sequences, we propose Token Recycling.\nIt stores candidate tokens in an adjacency matrix and employs a\nbreadth-first-search (BFS)-like algorithm to construct a draft tree, which is\nthen validated through tree attention. New candidate tokens from the decoding\nprocess are then used to update the matrix. Token Recycling requires\n\\textless2MB of additional storage and achieves approximately 2x speedup across\nall sizes of LLMs. It significantly outperforms existing train-free methods by\n30\\% and even a widely recognized training method by 25\\%.", "AI": {"tldr": "The paper introduces Token Recycling, a lossless approach to accelerate inference in large language models (LLMs) by storing and reusing candidate tokens, achieving significant speed improvements without additional training requirements.", "motivation": "Inference latency in large language models is a major bottleneck due to their massive parameters, and existing methods either require additional training or have limitations in speed and adaptability.", "method": "Token Recycling utilizes an adjacency matrix to store candidate tokens and updates them using a breadth-first-search algorithm to construct a draft tree, which is validated through tree attention.", "result": "Token Recycling achieves approximately 2x speedup across all sizes of LLMs and outperforms existing train-free methods by 30% and a popular training method by 25%.", "conclusion": "The proposed method significantly improves inference speed while requiring less than 2MB of additional storage and no pre-training.", "key_contributions": ["Introduction of Token Recycling for LLM inference acceleration", "Achieves significant speedup without extra training", "Requires minimal additional storage"], "limitations": "", "keywords": ["Token Recycling", "Large Language Models", "Inference Latency", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.08780", "pdf": "https://arxiv.org/pdf/2408.08780.pdf", "abs": "https://arxiv.org/abs/2408.08780", "title": "Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions", "authors": ["Chenming Tang", "Zhixiang Wang", "Hao Sun", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "With the help of in-context learning (ICL), large language models (LLMs) have\nachieved impressive performance across various tasks. However, the function of\ndescriptive instructions during ICL remains under-explored. In this work, we\npropose an ensemble prompt framework to describe the selection criteria of\nmultiple in-context examples, and preliminary experiments on machine\ntranslation (MT) across six translation directions confirm that this framework\nboosts ICL performance. But to our surprise, LLMs might not care what the\ndescriptions actually say, and the performance gain is primarily caused by the\nensemble format, since it could lead to improvement even with random\ndescriptive nouns. We further apply this new ensemble framework on a range of\ncommonsense, math, logical reasoning and hallucination tasks with three LLMs\nand achieve promising results, suggesting again that designing a proper prompt\nformat would be much more effective and efficient than paying effort into\nspecific descriptions. Our code will be publicly available once this paper is\npublished.", "AI": {"tldr": "This paper introduces an ensemble prompt framework that enhances in-context learning performance in large language models (LLMs) by focusing on format rather than specific content in prompts.", "motivation": "The study investigates the role of descriptive instructions in in-context learning (ICL) for large language models, which has been overlooked in previous research.", "method": "The authors propose an ensemble prompt framework that uses multiple in-context examples with varying descriptive criteria and conduct experiments on machine translation and other reasoning tasks to evaluate performance.", "result": "Preliminary experiments show that the proposed ensemble framework improves performance in machine translation across six directions and other tasks, indicating that the format of prompts is more critical than the actual descriptive content.", "conclusion": "Properly designing the ensemble prompt format is more efficient for ICL performance than focusing on the specifics of prompt descriptions.", "key_contributions": ["Introduction of an ensemble prompt framework for ICL", "Demonstration of performance improvements in various tasks", "Evidence that format outweighs specific content in prompts"], "limitations": "The study primarily investigates LLMs in synthetic tasks and may not generalize across all types of language tasks.", "keywords": ["in-context learning", "large language models", "ensemble prompt framework", "machine translation", "prompt design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2408.12249", "pdf": "https://arxiv.org/pdf/2408.12249.pdf", "abs": "https://arxiv.org/abs/2408.12249", "title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction", "authors": ["Aishik Nagar", "Viktor Schlegel", "Thanh-Tung Nguyen", "Hao Li", "Yuping Wu", "Kuluhan Binici", "Stefan Winkler"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages", "summary": "Large Language Models (LLMs) are increasingly adopted for applications in\nhealthcare, reaching the performance of domain experts on tasks such as\nquestion answering and document summarisation. Despite their success on these\ntasks, it is unclear how well LLMs perform on tasks that are traditionally\npursued in the biomedical domain, such as structured information extraction. To\nbridge this gap, in this paper, we systematically benchmark LLM performance in\nMedical Classification and Named Entity Recognition (NER) tasks. We aim to\ndisentangle the contribution of different factors to the performance,\nparticularly the impact of LLMs' task knowledge and reasoning capabilities,\ntheir (parametric) domain knowledge, and addition of external knowledge. To\nthis end, we evaluate various open LLMs - including BioMistral and Llama-2\nmodels - on a diverse set of biomedical datasets, using standard prompting,\nChain of-Thought (CoT) and Self Consistency based reasoning as well as\nRetrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter\nintuitively, our results reveal that standard prompting consistently\noutperforms more complex techniques across both tasks, laying bare the\nlimitations in the current application of CoT, self-consistency and RAG in the\nbiomedical domain. Our findings suggest that advanced prompting methods\ndeveloped for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are\nnot easily portable to biomedical tasks where precise structured outputs are\nrequired. This highlights the need for more effective integration of external\nknowledge and reasoning mechanisms in LLMs to enhance their performance in\nreal-world biomedical applications.", "AI": {"tldr": "This paper benchmarks Large Language Models (LLMs) on Medical Classification and Named Entity Recognition (NER) tasks, revealing that standard prompting outperforms more complex techniques in the biomedical domain.", "motivation": "To assess the performance of LLMs in biomedical tasks like information extraction, which remain under-explored despite their success in question answering and summarization.", "method": "The authors evaluated various open LLMs on diverse biomedical datasets using standard prompting, Chain of-Thought (CoT), Self Consistency reasoning, and Retrieval-Augmented Generation (RAG).", "result": "The results showed that standard prompting consistently outperformed complex methods like CoT and RAG, indicating limitations in their applicability to structured biomedical tasks.", "conclusion": "The study emphasizes the need for better integration of external knowledge and reasoning in LLMs to improve their effectiveness for real-world biomedical applications.", "key_contributions": ["Benchmarking LLMs in Medical Classification and NER tasks.", "Revealing that standard prompting surpasses complex methods in biomedical applications.", "Identifying limitations in current reasoning techniques for structured output tasks."], "limitations": "The study highlights that advanced prompting methods are not easily transferable to biomedical tasks.", "keywords": ["Large Language Models", "Biomedical Domain", "Medical Classification", "Named Entity Recognition", "Prompting Techniques"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.01893", "pdf": "https://arxiv.org/pdf/2409.01893.pdf", "abs": "https://arxiv.org/abs/2409.01893", "title": "What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices", "authors": ["Zhi Chen", "Qiguang Chen", "Libo Qin", "Qipeng Guo", "Haijun Lv", "Yicheng Zou", "Wanxiang Che", "Hang Yan", "Kai Chen", "Dahua Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Camera Ready. Code is available at:\n  https://github.com/WowCZ/LongMIT", "summary": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.", "AI": {"tldr": "This paper proposes the Multi-agent Interactive Multi-hop Generation (MIMG) framework to enhance the quality of synthetic data for long context tasks in large language models.", "motivation": "To improve the quality of synthetic data used for enhancing long context capabilities in large language models, addressing the issues of low diversity and quality in current data generation methods.", "method": "The MIMG framework includes various agents for quality verification, single-hop question generation, multiple question sampling, and multi-hop question merging, systematically experimenting with document selection and validation techniques.", "result": "The MIMG framework improves the quality of synthetic data, achieving over 85% high-quality, multi-hop, and diverse data, leading to significant model performance enhancements that exceed models trained on larger human-annotated datasets.", "conclusion": "The findings suggest that high-quality synthetic long-context instruction data is crucial for improving model performance in long context tasks.", "key_contributions": ["Introduction of the MIMG framework for synthetic data generation", "Demonstration of significantly improved data quality and model performance", "Systematic investigation of document selection and question merging strategies"], "limitations": "The study is limited to specific model architectures and may not generalize to all models or applications.", "keywords": ["Large Language Models", "Synthetic Data Generation", "Multi-hop Question Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.07170", "pdf": "https://arxiv.org/pdf/2409.07170.pdf", "abs": "https://arxiv.org/abs/2409.07170", "title": "Learning Efficient Recursive Numeral Systems via Reinforcement Learning", "authors": ["Andrea Silvi", "Jonathan Thomas", "Emil Carlsson", "Devdatt Dubhashi", "Moa Johansson"], "categories": ["cs.CL"], "comment": "Accepted to CogSci 2025", "summary": "It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. Utilising a\nslightly modified version of the meta-grammar of Hurford (1975), we demonstrate\nthat our RL agents, shaped by the pressures for efficient communication, can\neffectively modify their lexicon towards Pareto-optimal configurations which\nare comparable to those observed within human numeral systems in terms of their\nefficiency.", "AI": {"tldr": "This paper explores how reinforcement learning (RL) agents can develop complex recursive numeral systems similar to human language through an adaptable meta-grammar for efficient communication.", "motivation": "To demonstrate the emergence of efficient recursive number systems via simple learning mechanisms like reinforcement learning, addressing the challenge identified in earlier research.", "method": "Pairs of RL agents learn to communicate numerical quantities using a meta-grammar that evolves during their interactions, guided by the need for efficient communication.", "result": "The RL agents successfully modify their lexicon to achieve Pareto-optimal configurations, resembling the efficiency of human numeral systems.", "conclusion": "The findings suggest that complex numeral systems can emerge from simple learning processes, providing insights into language development and communication efficiency.", "key_contributions": ["Mechanistic explanation of numeral system emergence via RL", "Demonstration of agent communication efficiency through meta-grammar", "Evolution of lexicon towards Pareto-optimal configurations mimicking human systems"], "limitations": "", "keywords": ["reinforcement learning", "numeral systems", "meta-grammar", "efficient communication", "Pareto-optimal"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2409.20120", "pdf": "https://arxiv.org/pdf/2409.20120.pdf", "abs": "https://arxiv.org/abs/2409.20120", "title": "PACE: Abstractions for Communicating Efficiently", "authors": ["Jonathan D. Thomas", "Andrea Silvi", "Devdatt Dubhashi", "Moa Johansson"], "categories": ["cs.CL"], "comment": "Accepted to CogSci 2025 for presentation", "summary": "A central but unresolved aspect of problem-solving in AI is the capability to\nintroduce and use abstractions, something humans excel at. Work in cognitive\nscience has demonstrated that humans tend towards higher levels of abstraction\nwhen engaged in collaborative task-oriented communication, enabling gradually\nshorter and more information-efficient utterances. Several computational\nmethods have attempted to replicate this phenomenon, but all make unrealistic\nsimplifying assumptions about how abstractions are introduced and learned. Our\nmethod, Procedural Abstractions for Communicating Efficiently (PACE), overcomes\nthese limitations through a neuro-symbolic approach. On the symbolic side, we\ndraw on work from library learning for proposing abstractions. We combine this\nwith neural methods for communication and reinforcement learning, via a novel\nuse of bandit algorithms for controlling the exploration and exploitation\ntrade-off in introducing new abstractions. PACE exhibits similar tendencies to\nhumans on a collaborative construction task from the cognitive science\nliterature, where one agent (the architect) instructs the other (the builder)\nto reconstruct a scene of block-buildings. PACE results in the emergence of an\nefficient language as a by-product of collaborative communication. Beyond\nproviding mechanistic insights into human communication, our work serves as a\nfirst step to providing conversational agents with the ability for human-like\ncommunicative abstractions.", "AI": {"tldr": "Introduces a neuro-symbolic method, PACE, for efficient communication in AI inspired by human collaborative problem-solving and abstraction.", "motivation": "To address the challenge of how AI can effectively introduce and use abstractions in communication, similar to humans.", "method": "PACE combines symbolic approaches from library learning with neural methods and reinforcement learning, applying bandit algorithms to manage abstraction introduction.", "result": "PACE demonstrates human-like tendencies in communication efficiency during collaborative tasks and leads to the development of an efficient language among agents.", "conclusion": "The approach offers insights into human communicative behavior and paves the way for conversational agents to achieve human-like abstraction capabilities.", "key_contributions": ["Introduction of PACE as a neuro-symbolic method for AI communication", "Utilization of bandit algorithms for managing abstraction exploration", "Demonstration of emergence of efficient language through collaborative tasks"], "limitations": "", "keywords": ["neuro-symbolic", "communication", "reinforcement learning", "abstraction", "collaborative tasks"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.00168", "pdf": "https://arxiv.org/pdf/2410.00168.pdf", "abs": "https://arxiv.org/abs/2410.00168", "title": "SSR: Alignment-Aware Modality Connector for Speech Language Models", "authors": ["Weiting Tan", "Hirofumi Inaguma", "Ning Dong", "Paden Tomasello", "Xutai Ma"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Fusing speech into pre-trained language model (SpeechLM) usually suffers from\ninefficient encoding of long-form speech and catastrophic forgetting of\npre-trained text modality. We propose SSR-Connector (Segmented Speech\nRepresentation Connector) for better modality fusion. Leveraging speech-text\nalignments, our approach segments and compresses speech features to match the\ngranularity of text embeddings. Additionally, we introduce a two-stage training\npipeline that includes the distillation and fine-tuning phases to mitigate\ncatastrophic forgetting. SSR-Connector outperforms existing mechanism for\nspeech-text modality fusion, consistently achieving better speech understanding\n(e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving\npre-trained text ability.", "AI": {"tldr": "SSR-Connector improves modality fusion by segmenting and compressing speech features to align with text embeddings and employs a two-stage training process to prevent catastrophic forgetting.", "motivation": "To address inefficiencies in fusing long-form speech with pre-trained language models and to avoid catastrophic forgetting of text modalities during fusion.", "method": "The authors introduce the SSR-Connector, which segments and compresses speech features to better align with text embeddings, followed by a two-stage training pipeline consisting of distillation and fine-tuning.", "result": "SSR-Connector significantly enhances speech understanding, achieving a +10 accuracy on StoryCloze and +20 on Speech-MMLU, while maintaining the competencies of pre-trained text embeddings.", "conclusion": "The proposed SSR-Connector surpasses existing speech-text modality fusion methods, demonstrating improvements in speech understanding alongside preservation of text capabilities.", "key_contributions": ["Introducing SSR-Connector for improved speech-text fusion", "Segmenting and compressing speech features for alignment with text", "Two-stage training process to mitigate catastrophic forgetting"], "limitations": "", "keywords": ["Speech recognition", "Language models", "Fusing modalities", "Human-Computer Interaction", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.02707", "pdf": "https://arxiv.org/pdf/2410.02707.pdf", "abs": "https://arxiv.org/abs/2410.02707", "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations", "authors": ["Hadas Orgad", "Michael Toker", "Zorik Gekhman", "Roi Reichart", "Idan Szpektor", "Hadas Kotek", "Yonatan Belinkov"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.", "AI": {"tldr": "This paper explores how internal representations of large language models (LLMs) encode information about the truthfulness of their outputs and examines the implications for error detection and mitigation strategies.", "motivation": "The paper aims to enhance the understanding of how LLMs produce errors, known as hallucinations, and to improve detection and mitigation strategies by analyzing internal states encoding truthfulness information.", "method": "The authors investigate the concentration of truthfulness information in specific tokens within LLMs' internal representations and analyze their performance in error detection and prediction of error types.", "result": "The study demonstrates that truthfulness encoding is not universal but multifaceted, with error detectors being dataset-dependent. It also shows that internal representations can help predict likely error types and that there's often a mismatch between what is encoded and what is produced as output.", "conclusion": "These findings provide valuable insights into the nature of LLM errors, highlighting areas for future research in error analysis and mitigation strategies.", "key_contributions": ["Identification of specific tokens that encode truthfulness information", "Development of tailored strategies for error mitigation", "Insights into the discrepancy between internal encoding and external output behavior"], "limitations": "Error detectors do not generalize well across datasets, suggesting limitations in the universal applicability of findings.", "keywords": ["large language models", "hallucinations", "error detection", "truthfulness encoding", "internal representations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.06577", "pdf": "https://arxiv.org/pdf/2410.06577.pdf", "abs": "https://arxiv.org/abs/2410.06577", "title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions", "authors": ["Zhihao He", "Hang Yu", "Zi Gong", "Shizhan Liu", "Jianguo Li", "Weiyao Lin"], "categories": ["cs.CL"], "comment": "Accepted by ICLR 2025. Camera-ready Version", "summary": "Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are\nopen-sourced at https://github.com/codefuse-ai/rodimus.", "AI": {"tldr": "This paper introduces Rodimus and Rodimus+ models, which reduce the computational complexity of LLMs by utilizing a linear attention mechanism and novel data-dependent selection techniques, achieving superior performance with fewer resources.", "motivation": "The motivation is to address the significant computational costs incurred by traditional softmax attention in LLMs, aiming to improve efficiency without sacrificing performance.", "method": "Rodimus uses data-dependent tempered selection in a linear attention-based framework, and Rodimus+ combines Rodimus with Sliding Window Shared-Key Attention to enhance semantic, token, and head compression techniques.", "result": "Rodimus+-1.6B outperforms existing models like Qwen2-1.5B and RWKV6-1.6B in downstream tasks, despite being trained on fewer tokens, demonstrating a strong balance between accuracy and efficiency.", "conclusion": "The proposed architectures, Rodimus and Rodimus+, showcase significant improvements in LLM performance by reducing memory usage and computational complexity while maintaining accuracy, redefining the trade-off between efficiency and effectiveness in LLMs.", "key_contributions": ["Introduction of the Rodimus architecture for LLMs", "Implementation of data-dependent tempered selection for memory efficiency", "Combination of attention techniques in Rodimus+ for improved performance"], "limitations": "", "keywords": ["large language models", "transformer", "efficient computing", "linear attention", "NLP"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2410.07076", "pdf": "https://arxiv.org/pdf/2410.07076.pdf", "abs": "https://arxiv.org/abs/2410.07076", "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Tong Xie", "Yuqiang Li", "Wanli Ouyang", "Soujanya Poria", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Scientific discovery plays a pivotal role in advancing human society, and\nrecent progress in large language models (LLMs) suggests their potential to\naccelerate this process. However, it remains unclear whether LLMs can\nautonomously generate novel and valid hypotheses in chemistry. In this work, we\ninvestigate whether LLMs can discover high-quality chemistry hypotheses given\nonly a research background-comprising a question and/or a survey-without\nrestriction on the domain of the question. We begin with the observation that\nhypothesis discovery is a seemingly intractable task. To address this, we\npropose a formal mathematical decomposition grounded in a fundamental\nassumption: that most chemistry hypotheses can be composed from a research\nbackground and a set of inspirations. This decomposition leads to three\npractical subtasks-retrieving inspirations, composing hypotheses with\ninspirations, and ranking hypotheses - which together constitute a sufficient\nset of subtasks for the overall scientific discovery task. We further develop\nan agentic LLM framework, MOOSE-Chem, that is a direct implementation of this\nmathematical decomposition. To evaluate this framework, we construct a\nbenchmark of 51 high-impact chemistry papers published and online after January\n2024, each manually annotated by PhD chemists with background, inspirations,\nand hypothesis. The framework is able to rediscover many hypotheses with high\nsimilarity to the groundtruth, successfully capturing the core\ninnovations-while ensuring no data contamination since it uses an LLM with\nknowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high\naccuracy on inspiration retrieval, a task with inherently out-of-distribution\nnature, we propose a bold assumption: that LLMs may already encode latent\nscientific knowledge associations not yet recognized by humans.", "AI": {"tldr": "This paper investigates whether large language models (LLMs) can autonomously generate valid hypotheses in chemistry, proposing a decomposition framework and demonstrating its effectiveness through the MOOSE-Chem framework.", "motivation": "The potential of LLMs to accelerate scientific discovery in chemistry is explored, with a focus on their ability to autonomously generate novel hypotheses.", "method": "A mathematical decomposition of the hypothesis discovery task is proposed, leading to three subtasks: retrieving inspirations, composing hypotheses, and ranking hypotheses. The MOOSE-Chem framework implements this approach and is evaluated using a benchmark of high-impact chemistry papers.", "result": "MOOSE-Chem successfully rediscovered many hypotheses with high similarity to the groundtruth and demonstrated high accuracy in inspiration retrieval.", "conclusion": "The findings suggest that LLMs might encode latent scientific knowledge associations, hinting at their broader applicability in hypothesis generation.", "key_contributions": ["Proposed formal decomposition of the hypothesis discovery task in chemistry.", "Development of the MOOSE-Chem framework for hypothesis generation.", "Demonstration of LLMs' ability to rediscover high-quality hypotheses with minimal supervision."], "limitations": "The study is limited to the chemistry domain and results may not be generalizable to other fields without further research.", "keywords": ["Large Language Models", "Hypothesis Generation", "Chemistry", "MOOSE-Chem", "Scientific Discovery"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.01533", "pdf": "https://arxiv.org/pdf/2411.01533.pdf", "abs": "https://arxiv.org/abs/2411.01533", "title": "Enhancing LLM Evaluations: The Garbling Trick", "authors": ["William F. Bradley"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures", "summary": "As large language models (LLMs) become increasingly powerful, traditional\nevaluation metrics tend to saturate, making it challenging to distinguish\nbetween models. We propose a general method to transform existing LLM\nevaluations into a series of progressively more difficult tasks. These enhanced\nevaluations emphasize reasoning capabilities and can reveal relative\nperformance differences that are not apparent in the original assessments.\n  To demonstrate the effectiveness of our approach, we create a new\nmultiple-choice test corpus, extend it into a family of evaluations, and assess\na collection of LLMs. Our results offer insights into the comparative abilities\nof these models, particularly highlighting the differences between base LLMs\nand more recent \"reasoning\" models.", "AI": {"tldr": "The paper introduces a new method to improve the evaluation of LLMs by creating progressively challenging tasks that better assess reasoning capabilities.", "motivation": "To address the issue of saturation in traditional evaluation metrics for large language models, which makes it difficult to distinguish model performance.", "method": "The authors propose a methodology to enhance existing LLM evaluations by transforming them into progressively challenging tasks, focusing on reasoning capabilities.", "result": "The approach is validated by creating a new multiple-choice test corpus and assessing various LLMs, revealing significant performance differences, especially between base LLMs and advanced reasoning models.", "conclusion": "Enhanced evaluations provide better insights into model performance, particularly in terms of reasoning, promoting a more nuanced comparison among LLMs.", "key_contributions": ["Development of a new method for evaluating LLMs through difficult task progression", "Creation of a multiple-choice test corpus for model assessment", "Insights into the performance differences between base and reasoning LLMs"], "limitations": "", "keywords": ["Large Language Models", "Evaluation Metrics", "Reasoning Capabilities"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.11266", "pdf": "https://arxiv.org/pdf/2411.11266.pdf", "abs": "https://arxiv.org/abs/2411.11266", "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs", "authors": ["Keer Lu", "Keshi Zhao", "Zhuoran Zhang", "Zheng Liang", "Da Pan", "Shusen Zhang", "Xin Wu", "Guosheng Dong", "Bin Cui", "Tengjiao Wang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "As demonstrated by the proprietary Large Language Models (LLMs) such as GPT\nand Claude series, LLMs have the potential to achieve remarkable proficiency\nacross a wide range of domains, including law, medicine, finance, science,\ncode, etc., all within a single model. These capabilities are further augmented\nduring the Supervised Fine-Tuning (SFT) phase. Despite their potential,\nexisting work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce **VersaTune**, a\nnovel data composition framework designed for enhancing LLMs' overall\nmulti-domain capabilities during training. We begin with detecting the\ndistribution of domain-specific knowledge within the base model, followed by\nthe training data composition that aligns with the model's existing knowledge\ndistribution. During the subsequent training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results indicate that VersaTune is effective in multi-domain\nfostering, with an improvement of 35.21\\% in the overall multi-ability\nperformances compared to uniform domain weights. Furthermore, we find that\nQwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,\nClaude3.5-Sonnet and DeepSeek-V3 by 0.86\\%, 4.76\\% and 4.60\\%. Additionally, in\nscenarios where flexible expansion of a specific domain is required, VersaTune\nreduces the performance degradation in other domains by 38.77\\%, while\npreserving the training efficacy of the target domain.", "AI": {"tldr": "VersaTune is a novel framework for enhancing multi-domain capabilities of Large Language Models through adaptive data composition during training.", "motivation": "To address the challenge of catastrophic forgetting in existing domain-specific enhancements during fine-tuning of LLMs.", "method": "VersaTune uses a data composition framework that detects the distribution of domain-specific knowledge in the base model and adjusts training data composition dynamically according to model's knowledge distribution and forgetting degree.", "result": "VersaTune improved overall multi-domain performance by 35.21% compared to uniform domain weights and outperformed several frontier models.", "conclusion": "VersaTune effectively enhances multi-domain capabilities and reduces performance degradation in non-target domains while preserving training efficacy.", "key_contributions": ["Introduction of VersaTune for multi-domain LLM enhancement", "Dynamic adjustment of domain weights during training", "Significantly improved performance metrics over existing models"], "limitations": "", "keywords": ["Large Language Models", "Fine-Tuning", "Multi-Domain Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.16707", "pdf": "https://arxiv.org/pdf/2411.16707.pdf", "abs": "https://arxiv.org/abs/2411.16707", "title": "Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework", "authors": ["Mengshuo Jia", "Zeyu Cui", "Gabriela Hug"], "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": "16 pages", "summary": "The integration of experimental technologies with large language models\n(LLMs) is transforming scientific research. It positions AI as a versatile\nresearch assistant rather than a mere problem-solving tool. In the field of\npower systems, however, managing simulations -- one of the essential\nexperimental technologies -- remains a challenge for LLMs due to their limited\ndomain-specific knowledge, restricted reasoning capabilities, and imprecise\nhandling of simulation parameters. To address these limitations, this paper\nproposes a feedback-driven, multi-agent framework. It incorporates three\nproposed modules: an enhanced retrieval-augmented generation (RAG) module, an\nimproved reasoning module, and a dynamic environmental acting module with an\nerror-feedback mechanism. Validated on 69 diverse tasks from Daline and\nMATPOWER, this framework achieves success rates of 93.13% and 96.85%,\nrespectively. It significantly outperforms ChatGPT 4o, o1-preview, and the\nfine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex\ntasks. Additionally, the proposed framework also supports rapid, cost-effective\ntask execution, completing each simulation in approximately 30 seconds at an\naverage cost of 0.014 USD for tokens. Overall, this adaptable framework lays a\nfoundation for developing intelligent LLM-based assistants for human\nresearchers, facilitating power system research and beyond.", "AI": {"tldr": "The paper presents a feedback-driven, multi-agent framework for enhancing large language models (LLMs) in managing power system simulations, achieving significantly higher success rates than existing models.", "motivation": "To improve the management of simulations in power systems, addressing the limitations of current large language models (LLMs) in domain-specific knowledge and reasoning capabilities.", "method": "The proposed framework includes three modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism, validated across 69 diverse tasks.", "result": "The framework achieved success rates of 93.13% on Daline and 96.85% on MATPOWER, significantly outperforming ChatGPT and fine-tuned GPT-4o models which performed below 30%.", "conclusion": "This framework establishes a basis for advanced LLM-based research assistants in power systems, facilitating more efficient and cost-effective simulations.", "key_contributions": ["Development of a feedback-driven multi-agent framework for LLMs in power systems.", "Introduction of three innovative modules to enhance LLM performance in simulations.", "Demonstration of significantly improved success rates in complex tasks compared to existing models."], "limitations": "", "keywords": ["large language models", "simulation management", "power systems", "multi-agent framework", "retrieval-augmented generation"], "importance_score": 6, "read_time_minutes": 16}}
{"id": "2411.19477", "pdf": "https://arxiv.org/pdf/2411.19477.pdf", "abs": "https://arxiv.org/abs/2411.19477", "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models", "authors": ["Yanxi Chen", "Xuchen Pan", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.", "AI": {"tldr": "Proposes two algorithms for improving large language models' performance through knockout and league styles, highlighting their scalability and theoretical foundations.", "motivation": "To develop practical and scalable algorithms for test-time compute of large language models that can ensure efficiency and reliability in their outputs.", "method": "Introduced a two-stage knockout-style algorithm and a two-stage league-style algorithm, both designed to evaluate multiple candidate solutions generated by a black-box LLM.", "result": "The algorithms demonstrate provable scaling laws, with diminishing failure probabilities as test-time compute increases, validated through experiments across various models and datasets.", "conclusion": "Both algorithms provide effective and adaptable approaches for leveraging LLMs in practical applications without needing additional components like verifiers or reward models.", "key_contributions": ["Development of knockout-style and league-style algorithms for LLMs", "Theoretical proof of diminishing failure probabilities with increased compute", "Experimental validation across diverse models and datasets"], "limitations": "", "keywords": ["large language models", "scaling laws", "test-time compute", "algorithms", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.02466", "pdf": "https://arxiv.org/pdf/2412.02466.pdf", "abs": "https://arxiv.org/abs/2412.02466", "title": "Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths", "authors": ["Mohammed Q. Shormani"], "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "comment": "18 pages, 3 figures", "summary": "This study sets out to answer one major question: Can ChatGPT capture\nswearing nuances? It presents an empirical study on the ability of ChatGPT to\ntranslate Arabic oath expressions into English. 30 Arabic oath expressions were\ncollected from the literature. These 30 oaths were first translated via ChatGPT\nand then analyzed and compared to the human translation in terms of types of\ngaps left unfulfilled by ChatGPT. Specifically, the gaps involved are:\nreligious gap, cultural gap, both religious and cultural gaps, no gap, using\nnon-oath particles, redundancy and noncapturing of Arabic script diacritics. It\nconcludes that ChatGPT translation of oaths is still much unsatisfactory,\nunveiling the need of further developments of ChatGPT, and the inclusion of\nArabic data on which ChatGPT should be trained including oath expressions, oath\nnuances, rituals, and practices.", "AI": {"tldr": "This study investigates the ability of ChatGPT to translate Arabic oath expressions into English, revealing significant gaps in its performance.", "motivation": "To assess the effectiveness of ChatGPT in capturing the nuanced meanings of swearing in translations from Arabic to English.", "method": "The study involved the collection of 30 Arabic oath expressions, which were translated using ChatGPT and then compared to human translations. The analysis focused on various types of gaps in translation.", "result": "The analysis revealed several types of gaps in ChatGPT's translations, including religious and cultural gaps, non-oath particles, redundancy, and lack of diacritics.", "conclusion": "ChatGPT's translations of Arabic oaths are unsatisfactory, highlighting the need for further improvements and inclusion of relevant Arabic training data.", "key_contributions": ["Empirical analysis of ChatGPT's translation capabilities for Arabic oath expressions.", "Identification of specific types of translation gaps in ChatGPT's output.", "Call for enhancements in training data to improve cultural and religious nuance capture."], "limitations": "The study is limited to a specific set of 30 Arabic oath expressions and may not be generalizable to all Arabic language nuances.", "keywords": ["ChatGPT", "translation", "Arabic", "oath expressions", "cross-cultural communication"], "importance_score": 4, "read_time_minutes": 20}}
{"id": "2412.11500", "pdf": "https://arxiv.org/pdf/2412.11500.pdf", "abs": "https://arxiv.org/abs/2412.11500", "title": "Intention Knowledge Graph Construction for User Intention Relation Modeling", "authors": ["Jiaxin Bai", "Zhaobo Wang", "Junfei Cheng", "Dan Yu", "Zerui Huang", "Weiqi Wang", "Xin Liu", "Chen Luo", "Yanming Zhu", "Bo Li", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding user intentions is challenging for online platforms. Recent\nwork on intention knowledge graphs addresses this but often lacks focus on\nconnecting intentions, which is crucial for modeling user behavior and\npredicting future actions. This paper introduces a framework to automatically\ngenerate an intention knowledge graph, capturing connections between user\nintentions. Using the Amazon m2 dataset, we construct an intention graph with\n351 million edges, demonstrating high plausibility and acceptance. Our model\neffectively predicts new session intentions and enhances product\nrecommendations, outperforming previous state-of-the-art methods and showcasing\nthe approach's practical utility.", "AI": {"tldr": "The paper presents a framework for automatically generating intention knowledge graphs, focusing on connecting user intentions to improve behavior modeling and prediction.", "motivation": "To address the challenges in understanding user intentions for online platforms and to improve the prediction of user behavior.", "method": "The authors develop a framework to automatically generate an intention knowledge graph using the Amazon m2 dataset, capturing connections between user intentions.", "result": "The constructed intention graph contains 351 million edges and shows high plausibility and acceptance. The model effectively predicts new session intentions and improves product recommendations, surpassing previous methods.", "conclusion": "The approach demonstrates practical utility in enhancing user experience on online platforms by improving intention prediction and product recommendations.", "key_contributions": ["Introduced a framework for generating intention knowledge graphs.", "Focused on connecting user intentions for better prediction modeling.", "Outperformed state-of-the-art methods in predicting user session intentions."], "limitations": "", "keywords": ["intention knowledge graph", "user behavior", "product recommendations", "predictive modeling", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.13377", "pdf": "https://arxiv.org/pdf/2412.13377.pdf", "abs": "https://arxiv.org/abs/2412.13377", "title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models", "authors": ["Gagan Bhatia", "MingZe Tang", "Cristina Mahanta", "Madiha Kazi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately.", "AI": {"tldr": "DateLogicQA is a benchmark for evaluating LLMs' temporal reasoning, introducing a new metric and analyzing biases.", "motivation": "To assess the capabilities and limitations of LLMs in handling diverse date formats and temporal reasoning.", "method": "Development of DateLogicQA benchmark with 190 questions and introduction of the Semantic Integrity Metric to evaluate tokenization quality.", "result": "The study uncovers Representation-Level Bias and Logical-Level Bias in LLMs, affecting their temporal reasoning and outputs.", "conclusion": "Key challenges identified in accurate temporal data handling highlight the limitations of existing LLMs.", "key_contributions": ["Introduction of DateLogicQA benchmark", "Semantic Integrity Metric for tokenization quality", "Identification of biases in LLMs affecting reasoning"], "limitations": "", "keywords": ["temporal reasoning", "LLMs", "benchmark", "biases", "date formats"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.14872", "pdf": "https://arxiv.org/pdf/2412.14872.pdf", "abs": "https://arxiv.org/abs/2412.14872", "title": "Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set", "authors": ["Lecheng Wang", "Xianjie Shi", "Ge Li", "Jia Li", "Xuanming Zhang", "Yihong Dong", "Wenpin Jiao", "Hong Mei"], "categories": ["cs.CL"], "comment": "20 pages, 3 figures", "summary": "Auto-regressive language models (LMs) have been widely used to generate data\nin data-scarce domains to train new LMs, compensating for the scarcity of\nreal-world data. Previous work experimentally found that LMs collapse when\ntrained on recursively generated data. This paper presents a theoretical proof:\nonce a corpus (such as a subset of the World Wide Web) begins to incorporate\ngenerated data and no new real-world data is added to the corpus, then no\nmatter how small the amount of data each LM generates and contributes to the\ncorpus, LM collapse is inevitable after sufficient time. This finding suggests\nthat attempts to mitigate collapse by limiting the quantity of synthetic data\nin the corpus are fundamentally insufficient. Instead, avoiding collapse hinges\non ensuring the quality of synthetic data.", "AI": {"tldr": "The paper presents a theoretical proof that auto-regressive language models inevitably collapse when trained on recursively generated data without the inclusion of new real-world data.", "motivation": "To address the issue of language model collapse in data-scarce domains when using generated data for training.", "method": "The paper provides a theoretical proof regarding the behavior of language models trained on a corpus that includes synthetic data without real-world data.", "result": "The proof demonstrates that once generated data enters a training corpus, the language model will eventually collapse, regardless of the amount of generated data injected over time.", "conclusion": "To prevent collapse of language models, it is essential to focus on the quality of synthetic data rather than just controlling the quantity.", "key_contributions": ["Theoretical proof of language model collapse due to synthetic data incorporation.", "Analysis of the implications of generated data on model training.", "Emphasis on the importance of data quality over quantity in mitigating collapse."], "limitations": "The implications of collapse and quality assurance mechanisms for synthetic data need further exploration.", "keywords": ["Language Models", "Synthetic Data", "Data Scarcity", "Model Collapse", "Quality Assurance"], "importance_score": 5, "read_time_minutes": 20}}
{"id": "2412.17061", "pdf": "https://arxiv.org/pdf/2412.17061.pdf", "abs": "https://arxiv.org/abs/2412.17061", "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration", "authors": ["Hai Ye", "Mingbao Lin", "Hwee Tou Ng", "Shuicheng Yan"], "categories": ["cs.CL"], "comment": "In submission", "summary": "Scaling laws for inference compute in multi-agent systems remain\nunder-explored compared to single-agent scenarios. This work aims to bridge\nthis gap by investigating the problem of data synthesis through multi-agent\nsampling, where synthetic responses are generated by sampling from multiple\ndistinct language models. Effective model coordination is crucial for\nsuccessful multi-agent collaboration. Unlike previous approaches that rely on\nfixed workflows, we treat model coordination as a multi-step decision-making\nprocess, optimizing generation structures dynamically for each input question.\nWe introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow\nevolves iteratively during the sequential sampling process. To achieve this, we\nleverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide\nreal-time feedback and accelerate exploration. Our experiments on alignment,\nmachine translation, and mathematical reasoning demonstrate that multi-agent\nsampling significantly outperforms single-agent sampling as inference compute\nscales. TOA is the most compute-efficient approach, achieving SOTA performance\non WMT and a 72.2\\% LC win rate on AlpacaEval. Moreover, fine-tuning with our\nsynthesized alignment data surpasses strong preference learning methods on\nchallenging benchmarks such as Arena-Hard and AlpacaEval.", "AI": {"tldr": "This paper investigates a multi-agent approach using sampling from distinct language models for data synthesis, introducing a dynamic coordination strategy leveraging Monte Carlo Tree Search.", "motivation": "To address the under-explored scaling laws for inference compute in multi-agent systems compared to single-agent scenarios.", "method": "The proposed method, TOA (Tree Search-based Orchestrated Agents), utilizes Monte Carlo Tree Search for dynamic model coordination and iterative workflow evolution during sampling.", "result": "Experiments demonstrate that multi-agent sampling outperforms single-agent methods, achieving state-of-the-art performance on multiple benchmarks while being the most compute-efficient approach.", "conclusion": "The TOA framework enhances multi-agent collaboration in language model sampling, yielding superior alignment and translation results and surpassing traditional preference learning methods.", "key_contributions": ["Introduction of TOA for dynamic multi-agent coordination during sampling", "Achievement of state-of-the-art results on WMT and AlpacaEval", "Demonstration of significant performance improvements in alignment and translation tasks."], "limitations": "", "keywords": ["multi-agent systems", "Monte Carlo Tree Search", "data synthesis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.02506", "pdf": "https://arxiv.org/pdf/2501.02506.pdf", "abs": "https://arxiv.org/abs/2501.02506", "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use", "authors": ["Junjie Ye", "Zhengyin Du", "Xuesong Yao", "Weijian Lin", "Yufei Xu", "Zehui Chen", "Zaiyuan Wang", "Sining Zhu", "Zhiheng Xi", "Siyu Yuan", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Jiecao Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Effective evaluation of multi-hop tool use is critical for analyzing the\nunderstanding, reasoning, and function-calling capabilities of large language\nmodels (LLMs). However, progress has been hindered by a lack of reliable\nevaluation datasets. To address this, we present ToolHop, a dataset comprising\n995 user queries and 3,912 associated tools, specifically designed for rigorous\nevaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful\ninterdependencies, locally executable tools, detailed feedback, and verifiable\nanswers through a novel query-driven data construction approach that includes\ntool creation, document refinement, and code generation. We evaluate 14 LLMs\nacross five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and\nGPT), uncovering significant challenges in handling multi-hop tool-use\nscenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,\nunderscoring substantial room for improvement. Further analysis reveals\nvariations in tool-use strategies for various families, offering actionable\ninsights to guide the development of more effective approaches. Code and data\ncan be found in https://huggingface.co/datasets/bytedance-research/ToolHop.", "AI": {"tldr": "Presentation of ToolHop, a dataset for evaluating multi-hop tool use in LLMs.", "motivation": "To create reliable evaluation datasets for assessing understanding, reasoning, and function-calling capabilities of LLMs in multi-hop tool scenarios.", "method": "Presented ToolHop dataset with 995 user queries and 3,912 tools, employing query-driven data construction involving tool creation, document refinement, and code generation.", "result": "Evaluated 14 LLMs revealing significant challenges in multi-hop tool use, with GPT-4o leading at 49.04% accuracy.", "conclusion": "The study highlights the need for improvement in LLMs' tool-use capabilities and provides insights for developing better strategies.", "key_contributions": ["Introduction of ToolHop dataset specifically for multi-hop tool evaluation", "Rigorous evaluation of 14 LLMs across five model families", "Identification of variations in tool-use strategies among model families."], "limitations": "Dataset is limited to the presented queries and tools, which may not cover all scenarios.", "keywords": ["multi-hop tool use", "large language models", "dataset", "evaluation", "tool strategy"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2501.11269", "pdf": "https://arxiv.org/pdf/2501.11269.pdf", "abs": "https://arxiv.org/abs/2501.11269", "title": "Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios", "authors": ["Zhongtian Hu", "Yiwen Cui", "Ronghan Li", "Meng Zhao", "Lifang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Current multilingual large language models(MLLMs) still focus on simple\nquestion-answering formats, often overlooking more complex dialogue scenarios.\nIn other words, their capabilities of multilingual large models have yet to be\nvalidated in dialogue tasks with intricate structures. We therefore ask, Q1:\nHow well do LLMs generalize to more complex dialog scenarios? Q2: Can\nsupervised fine-tuning on a high-quality parallel benchmark restore this\nability? Q3: Does the \"multilingual complementarity\" effect survive in the\nsetting? To answer these questions, we introduce XMP, a high-quality parallel\nMultilingual dataset sourced from Multi-party Podcast dialogues, which is the\nfirst parallel dataset focusing on multi-party dialogue scenarios. Most samples\nin the dataset feature three or more participants, discussing a wide range of\ntopics. Through extensive experiments, we find that, R1: MLLMs fail to\ngeneralize to multi-party setting, R2 Fine-tuning on XMP improves only\nmarginally, with the 70B model achieving at most a 1% absolute gain over its 8B\ncounterpart; R3: Mixing languages during SFT is usually detrimental, with any\nbenefits being marginal and limited to isolated cases in the 70B model.", "AI": {"tldr": "This paper investigates the generalization capabilities of multilingual large language models (MLLMs) in complex dialogue scenarios using a new multilingual dataset called XMP, focusing on multi-party dialogues.", "motivation": "To evaluate how well MLLMs perform in complex dialogue tasks and understand the effects of supervised fine-tuning on their performance.", "method": "Introduces the XMP dataset, which features multilingual multi-party podcasts, and conducts experiments to assess model performance in dialogue tasks.", "result": "MLLMs struggle with multi-party dialogues, showing only marginal improvement from fine-tuning on XMP, and language mixing during fine-tuning tends to be detrimental.", "conclusion": "The study highlights the limitations of current MLLMs in handling complex dialogue scenarios and suggests that more targeted training approaches are needed.", "key_contributions": ["Introduction of the XMP dataset for multi-party dialogue tasks", "Demonstrates MLLMs' limitations in generalizing to complex dialogue scenarios", "Finds that fine-tuning has minimal effectiveness in improving model performance for multi-party dialogues"], "limitations": "The generalizability of results may be constrained by the dataset's specificity to podcast dialogues and the model's existing architecture.", "keywords": ["multilingual large language models", "multi-party dialogues", "supervised fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.11292", "pdf": "https://arxiv.org/pdf/2501.11292.pdf", "abs": "https://arxiv.org/abs/2501.11292", "title": "Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning", "authors": ["Zhongtian Hu", "Qi He", "Ronghan Li", "Meng Zhao", "Lifang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-party dialogues, common in collaborative scenarios like brainstorming\nsessions and negotiations, pose significant challenges due to their complexity\nand diverse speaker roles. Current methods often use graph neural networks to\nmodel dialogue context, capturing structural dynamics but heavily relying on\nannotated graph structures and overlooking individual speaking styles. To\naddress these challenges, we propose CMR, a Contrastive learning-based\nMulti-party dialogue Response generation framework. CMR employs a two-stage\nself-supervised contrastive learning framework. First, it captures global\ndifferences in speaking styles across individuals. Then, it focuses on\nintra-conversation comparisons to identify thematic transitions and\ncontextually relevant facts. To the best of our knowledge, this is the first\napproach that applies contrastive learning in multi-party dialogue generation.\nExperimental results demonstrate that CMR not only significantly outperforms\nstate-of-the-art models, but also generalizes well to large pre-trained\nlanguage models, effectively enhancing their capability in handling multi-party\nconversations.", "AI": {"tldr": "CMR is a contrastive learning-based framework for generating responses in multi-party dialogues, improving performance and generalization in handling complex conversations.", "motivation": "Multi-party dialogues present challenges due to their complexity and the roles of various speakers, which current models inadequately address.", "method": "CMR utilizes a two-stage self-supervised contrastive learning framework that first captures global differences in speaking styles, followed by intra-conversation comparisons to identify transitions and relevant facts.", "result": "CMR significantly outperforms state-of-the-art models and enhances the performance of large pre-trained language models in multi-party dialogue.", "conclusion": "The proposed CMR framework effectively handles the intricacies of multi-party dialogues, marking a novel approach in dialogue generation.", "key_contributions": ["First application of contrastive learning in multi-party dialogue generation", "Introduces a two-stage self-supervised learning framework", "Demonstrates significant performance improvements over existing models"], "limitations": "", "keywords": ["multi-party dialogue", "contrastive learning", "response generation", "self-supervised learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2501.11496", "pdf": "https://arxiv.org/pdf/2501.11496.pdf", "abs": "https://arxiv.org/abs/2501.11496", "title": "Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges", "authors": ["Vincent Koc"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 91F20", "I.2.7; I.2.6; J.5"], "comment": "9 pages, 3 figures, 2 tables, submitted for IEEE publication.\n  Pre-print updated as part of review process", "summary": "The global crisis of language endangerment meets a technological turning\npoint as Generative AI (GenAI) and Large Language Models (LLMs) unlock new\nfrontiers in automating corpus creation, transcription, translation, and\ntutoring. However, this promise is imperiled by fragmented practices and the\ncritical lack of a methodology to navigate the fraught balance between LLM\ncapabilities and the profound risks of data scarcity, cultural\nmisappropriation, and ethical missteps. This paper introduces a novel\nanalytical framework that systematically evaluates GenAI applications against\nlanguage-specific needs, embedding community governance and ethical safeguards\nas foundational pillars. We demonstrate its efficacy through the Te Reo M\\=aori\nrevitalization, where it illuminates successes, such as community-led Automatic\nSpeech Recognition achieving 92% accuracy, while critically surfacing\npersistent challenges in data sovereignty and model bias for digital archives\nand educational tools. Our findings underscore that GenAI can indeed\nrevolutionize language preservation, but only when interventions are rigorously\nanchored in community-centric data stewardship, continuous evaluation, and\ntransparent risk management. Ultimately, this framework provides an\nindispensable toolkit for researchers, language communities, and policymakers,\naiming to catalyze the ethical and high-impact deployment of LLMs to safeguard\nthe world's linguistic heritage.", "AI": {"tldr": "This paper presents an analytical framework for using Generative AI and Large Language Models in language revitalization, emphasizing ethical practices and community governance to address language endangerment.", "motivation": "To address the global crisis of language endangerment using Generative AI and LLMs while navigating the risks associated with data scarcity and cultural misappropriation.", "method": "The paper introduces a framework that evaluates GenAI applications for their alignment with community-specific needs and ethical considerations, tested through the Te Reo MÄori revitalization example.", "result": "The framework revealed successes like achieving 92% accuracy in community-led Automatic Speech Recognition, but also identified significant challenges regarding data sovereignty and model bias in digital resources.", "conclusion": "GenAI has the potential to revolutionize language preservation if interventions are grounded in community-centric data stewardship and continuous evaluation with clear risk management.", "key_contributions": ["Novel analytical framework for evaluating GenAI applications in language preservation", "Demonstration of the framework's efficacy through case study on Te Reo MÄori", "Identification of critical challenges and ethical considerations in deploying LLMs."], "limitations": "The framework's effectiveness depends on the continuous involvement of community stakeholders and may be subject to varying interpretations across different languages and contexts.", "keywords": ["Generative AI", "Large Language Models", "language preservation", "ethical considerations", "community governance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.12162", "pdf": "https://arxiv.org/pdf/2501.12162.pdf", "abs": "https://arxiv.org/abs/2501.12162", "title": "AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding", "authors": ["Zikun Li", "Zhuofu Chen", "Remi Delacourt", "Gabriele Oliaro", "Zeyu Wang", "Qinghan Chen", "Shuhuai Lin", "April Yang", "Zhihao Zhang", "Zhuoming Chen", "Sean Lai", "Xinhao Cheng", "Xupeng Miao", "Zhihao Jia"], "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "Modern large language model (LLM) applications exhibit diverse service-level\nobjectives (SLOs), from low-latency requirements in interactive coding\nassistants to more relaxed constraints in data wrangling tasks. Existing LLM\nserving systems, which rely on uniform batching and scheduling strategies,\noften fail to meet these heterogeneous SLOs concurrently. We present AdaServe,\nthe first LLM serving system designed to support efficient multi-SLO serving\nthrough SLO-customized speculative decoding. AdaServe formulates multi-SLO\nserving as a constrained optimization problem and introduces a hardware-aware\nalgorithm that constructs a speculation tree tailored to each request's latency\ntarget. It features a speculate-select-verify pipeline that enables\nfine-grained control over decoding speed while maximizing system throughput.\nAdaServe further adapts to workload variation by dynamically adjusting\nspeculation parameters. Evaluations across diverse workloads show that AdaServe\nreduces SLO violations by up to 4.3$\\times$ and improves goodput by up to\n1.9$\\times$ compared to the best performing baselines, highlighting its\neffectiveness in multi-SLO serving.", "AI": {"tldr": "AdaServe is an LLM serving system designed for efficient multi-SLO performance through SLO-customized speculative decoding, significantly reducing SLO violations and improving throughput.", "motivation": "To address the challenges faced by existing LLM serving systems which struggle to meet diverse service-level objectives (SLOs) due to uniform batching and scheduling strategies.", "method": "AdaServe formulates multi-SLO serving as a constrained optimization problem and employs a hardware-aware algorithm to create a speculation tree for each request's latency target, featuring a speculate-select-verify pipeline for enhanced control over decoding speed.", "result": "AdaServe demonstrates a reduction in SLO violations by up to 4.3 times and an increase in goodput by up to 1.9 times compared to leading baselines through evaluations on diverse workloads.", "conclusion": "AdaServe effectively enhances multi-SLO serving capabilities in LLM applications, enabling better performance across varying workload constraints.", "key_contributions": ["Introduction of the AdaServe system for multi-SLO serving in LLMs", "Development of a speculation tree algorithm tailored to latency targets", "Dynamic adjustment of speculation parameters based on workload variation"], "limitations": "", "keywords": ["large language models", "service-level objectives", "speculative decoding", "multi-SLO serving", "performance optimization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.15175", "pdf": "https://arxiv.org/pdf/2501.15175.pdf", "abs": "https://arxiv.org/abs/2501.15175", "title": "Option-ID Based Elimination For Multiple Choice Questions", "authors": ["Zhenhao Zhu", "Bulou Liu", "Qingyao Ai", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multiple choice questions (MCQs) are a popular and important task for\nevaluating large language models (LLMs). Based on common strategies people use\nwhen answering MCQs, the process of elimination (PoE) has been proposed as an\neffective problem-solving method. Existing PoE methods typically either have\nLLMs directly identify incorrect options or score options and replace\nlower-scoring ones with [MASK]. However, both methods suffer from\ninapplicability or suboptimal performance. To address these issues, this paper\nproposes a novel option-ID based PoE ($\\text{PoE}_{\\text{ID}}$).\n$\\text{PoE}_{\\text{ID}}$ critically incorporates a debiasing technique to\ncounteract LLMs token bias, enhancing robustness over naive ID-based\nelimination. It features two strategies: $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nwhich eliminates options whose IDs have log probabilities below the average\nthreshold, and $\\text{PoE}_{\\text{ID}}^{\\text{seq}}$, which iteratively removes\nthe option with the lowest ID probability. We conduct extensive experiments\nwith 6 different LLMs on 4 diverse datasets. The results demonstrate that\n$\\text{PoE}_{\\text{ID}}$, especially $\\text{PoE}_{\\text{ID}}^{\\text{log}}$,\nsignificantly improves zero-shot and few-shot MCQs performance, particularly in\ndatasets with more options. Our analyses demonstrate that\n$\\text{PoE}_{\\text{ID}}^{\\text{log}}$ enhances the LLMs' confidence in\nselecting the correct option, and the option elimination strategy outperforms\nmethods relying on [MASK] replacement. We further investigate the limitations\nof LLMs in directly identifying incorrect options, which stem from their\ninherent deficiencies.", "AI": {"tldr": "This paper proposes a novel option-ID based method, PoE_ID, for answering multiple choice questions (MCQs) with improved performance using large language models (LLMs).", "motivation": "To enhance the process of elimination in MCQs and address shortcomings of existing PoE methods used with LLMs.", "method": "The approach uses a debiasing technique and includes two strategies: PoE_ID^log, which removes options below a log probability threshold, and PoE_ID^seq, which iteratively eliminates the option with the lowest ID probability.", "result": "Experiments show that PoE_ID, especially PoE_ID^log, significantly improves zero-shot and few-shot performance on MCQs, particularly with datasets that have more options.", "conclusion": "PoE_ID enhances LLMs' confidence in selecting correct options and performs better than methods using [MASK] replacement, while also highlighting LLMs' limitations in incorrect option identification.", "key_contributions": ["Introduction of PoE_ID for MCQs", "Debiasing technique for token bias in LLMs", "Experimental validation across diverse datasets"], "limitations": "Inherent deficiencies of LLMs in directly identifying incorrect options.", "keywords": ["multiple choice questions", "process of elimination", "large language models", "debiasing techniques", "option-ID"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.17047", "pdf": "https://arxiv.org/pdf/2501.17047.pdf", "abs": "https://arxiv.org/abs/2501.17047", "title": "How Linguistics Learned to Stop Worrying and Love the Language Models", "authors": ["Richard Futrell", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Language models can produce fluent, grammatical text. Nonetheless, some\nmaintain that language models don't really learn language and also that, even\nif they did, that would not be informative for the study of human learning and\nprocessing. On the other side, there have been claims that the success of LMs\nobviates the need for studying linguistic theory and structure. We argue that\nboth extremes are wrong. LMs can contribute to fundamental questions about\nlinguistic structure, language processing, and learning. They force us to\nrethink arguments and ways of thinking that have been foundational in\nlinguistics. While they do not replace linguistic structure and theory, they\nserve as model systems and working proofs of concept for gradient, usage-based\napproaches to language. We offer an optimistic take on the relationship between\nlanguage models and linguistics.", "AI": {"tldr": "The paper argues that language models (LMs) can positively contribute to understanding linguistic structure and learning, countering polarized views on their role in linguistics.", "motivation": "To explore the role of language models in addressing core questions about linguistic structure and human language processing, amidst conflicting opinions on their value.", "method": "The authors analyze the capabilities of language models and their implications for linguistic theory, advocating for a usage-based approach to understanding language.", "result": "The paper concludes that language models should not replace linguistic theory but can enhance our understanding by providing proof of concept for new approaches.", "conclusion": "Language models serve as useful tools for rethinking linguistic arguments and enhancing our insight into language processing, suggesting a productive relationship with linguistic theory.", "key_contributions": ["Challenging the dichotomy between linguistic theory and language models.", "Proposing a gradient, usage-based approach to language informed by language models.", "Encouraging a reassessment of fundamental linguistic questions using tools from machine learning."], "limitations": "", "keywords": ["language models", "linguistic structure", "language processing", "learning", "usage-based approaches"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.18280", "pdf": "https://arxiv.org/pdf/2501.18280.pdf", "abs": "https://arxiv.org/abs/2501.18280", "title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models", "authors": ["Haoyu Liang", "Youran Sun", "Yunfeng Cai", "Jun Zhu", "Bo Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "The security issue of large language models (LLMs) has gained wide attention\nrecently, with various defense mechanisms developed to prevent harmful output,\namong which safeguards based on text embedding models serve as a fundamental\ndefense. Through testing, we discover that the output distribution of text\nembedding models is severely biased with a large mean. Inspired by this\nobservation, we propose novel, efficient methods to search for **universal\nmagic words** that attack text embedding models. Universal magic words as\nsuffixes can shift the embedding of any text towards the bias direction, thus\nmanipulating the similarity of any text pair and misleading safeguards.\nAttackers can jailbreak the safeguards by appending magic words to user prompts\nand requiring LLMs to end answers with magic words. Experiments show that magic\nword attacks significantly degrade safeguard performance on JailbreakBench,\ncause real-world chatbots to produce harmful outputs in full-pipeline attacks,\nand generalize across input/output texts, models, and languages. To eradicate\nthis security risk, we also propose defense methods against such attacks, which\ncan correct the bias of text embeddings and improve downstream performance in a\ntrain-free manner.", "AI": {"tldr": "This paper explores the security vulnerabilities of large language models (LLMs) related to text embedding, focusing on methods to exploit these vulnerabilities through universal magic words while proposing defenses against such attacks.", "motivation": "To address the security issues of large language models (LLMs) and their susceptibility to harmful output through biased text embedding mechanisms.", "method": "The authors propose methods to identify universal magic words that can manipulate the output distribution of text embedding models, and they also develop defense strategies to mitigate such attacks.", "result": "Experiments demonstrate that magic word attacks significantly reduce the effectiveness of safeguard mechanisms on various chatbots, leading to harmful outputs, while the proposed defense methods can correct embedding biases without requiring retraining.", "conclusion": "The findings highlight serious security implications for LLMs and suggest effective countermeasures to strengthen defenses against embedding manipulation.", "key_contributions": ["Discovery of significant bias in text embedding output distribution.", "Proposing novel methods for universal magic word attacks on LLM safeguards.", "Introducing train-free defense mechanisms to mitigate bias and improve performance."], "limitations": "Limited evaluation on the scalability of proposed defense methods in diverse applications and real-world environments.", "keywords": ["large language models", "text embedding", "security", "magic words", "defense mechanisms"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.00791", "pdf": "https://arxiv.org/pdf/2502.00791.pdf", "abs": "https://arxiv.org/abs/2502.00791", "title": "Vision-centric Token Compression in Large Language Model", "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Xiangbo Shu", "Jinhui Tang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Real-world applications are stretching context windows to hundreds of\nthousand of tokens while Large Language Models (LLMs) swell from billions to\ntrillions of parameters. This dual expansion send compute and memory costs\nskyrocketing, making token compression indispensable. We introduce Vision\nCentric Token Compression (Vist), a slow-fast compression framework that\nmirrors human reading: the fast path renders distant tokens into images,\nletting a frozen, lightweight vision encoder skim the low-salience context; the\nslow path feeds the proximal window into the LLM for fine-grained reasoning. A\nProbability-Informed Visual Enhancement (PVE) objective masks high-frequency\ntokens during training, steering the Resampler to concentrate on semantically\nrich regions-just as skilled reader gloss over function words. On eleven\nin-context learning benchmarks, Vist achieves the same accuracy with 2.3 times\nfewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers\nremarkable results, outperforming the strongest text encoder-based compression\nmethod CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI,\nand CLIN, setting a new standard for token efficiency in LLMs. The source code\nwill be released.", "AI": {"tldr": "Vision Centric Token Compression (Vist) is a new framework that reduces token count and resource usage in Large Language Models.", "motivation": "The increase in compute and memory costs due to the expansion of context windows and model parameters necessitates efficient token compression techniques.", "method": "Vist employs a dual-path approach that combines visual encoding for low-salience tokens and LLM processing for important proximal tokens, utilizing a Probability-Informed Visual Enhancement objective to focus on semantically rich areas.", "result": "Vist achieves the same accuracy as previous methods with 2.3 times fewer tokens, resulting in a 16% reduction in FLOPs and a 50% reduction in memory usage, outperforming existing compression methods.", "conclusion": "This method sets a new standard for token efficiency in LLMs, demonstrating significant improvements in benchmarks like TriviaQA and NQ.", "key_contributions": ["Introduction of Vision Centric Token Compression framework (Vist)", "Dual-path processing that mimics human reading behavior", "Substantial reductions in token count while maintaining accuracy"], "limitations": "", "keywords": ["Token Compression", "Large Language Models", "Human Reading"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2502.01179", "pdf": "https://arxiv.org/pdf/2502.01179.pdf", "abs": "https://arxiv.org/abs/2502.01179", "title": "Joint Localization and Activation Editing for Low-Resource Fine-Tuning", "authors": ["Wen Lai", "Alexander Fraser", "Ivan Titov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025. The code is released at\n  https://github.com/wenlai-lavine/jola", "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly\nused to adapt LLMs. However, the effectiveness of standard PEFT methods is\nlimited in low-resource scenarios with only a few hundred examples. Recent\nadvances in interpretability research have inspired the emergence of activation\nediting (or steering) techniques, which modify the activations of specific\nmodel components. These methods, due to their extremely small parameter counts,\nshow promise for small datasets. However, their performance is highly dependent\non identifying the correct modules to edit and often lacks stability across\ndifferent datasets. In this paper, we propose Joint Localization and Activation\nEditing (JoLA), a method that jointly learns (1) which heads in the Transformer\nto edit (2) whether the intervention should be additive, multiplicative, or\nboth and (3) the intervention parameters themselves - the vectors applied as\nadditive offsets or multiplicative scalings to the head output. Through\nevaluations on three benchmarks spanning commonsense reasoning, natural\nlanguage understanding, and natural language generation, we demonstrate that\nJoLA consistently outperforms existing methods. The code for the method is\nreleased at https://github.com/wenlai-lavine/jola.", "AI": {"tldr": "The paper introduces Joint Localization and Activation Editing (JoLA), a novel method for parameter-efficient fine-tuning of LLMs in low-resource settings, outperforming existing techniques across multiple benchmarks.", "motivation": "Parameter-efficient fine-tuning methods struggle in low-resource scenarios, and recent interpretability techniques have revealed potential in activation editing, which this paper aims to enhance.", "method": "JoLA learns which Transformer heads to edit, determines the nature of the intervention (additive, multiplicative, or both), and optimizes the intervention parameters applied to the head output.", "result": "JoLA consistently outperforms existing parameter-efficient fine-tuning methods on benchmarks in commonsense reasoning, natural language understanding, and natural language generation.", "conclusion": "JoLA offers a robust alternative for fine-tuning LLMs in low-resource environments, emphasizing the importance of module localization and appropriate intervention types.", "key_contributions": ["Introduces Joint Localization and Activation Editing (JoLA) method for efficient fine-tuning.", "Demonstrates the effectiveness of targeted activation manipulation in LLMs.", "Provides an open-source implementation for further research."], "limitations": "The performance is dependent on correctly identifying the heads to edit and may lack stability across diverse datasets.", "keywords": ["Parameter-efficient fine-tuning", "activation editing", "Transformer", "low-resource scenarios", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.01637", "pdf": "https://arxiv.org/pdf/2502.01637.pdf", "abs": "https://arxiv.org/abs/2502.01637", "title": "Scaling Embedding Layers in Language Models", "authors": ["Da Yu", "Edith Cohen", "Badih Ghazi", "Yangsibo Huang", "Pritish Kamath", "Ravi Kumar", "Daogao Liu", "Chiyuan Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Added downstream evaluation results and improved the overall clarity\n  and writing", "summary": "We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram\n$E$mbedding), a new method for extending input embedding layers to enhance\nlanguage model performance. To avoid increased decoding costs, SCONE retains\nthe original vocabulary while introducing embeddings for a set of frequent\n$n$-grams. These embeddings provide contextualized representation for each\ninput token and are learned with a separate model during training. After\ntraining, embeddings are precomputed and stored in off-accelerator memory;\nduring inference, querying them has minimal impact on latency due to the low\ncomplexity of embedding lookups. SCONE enables two new scaling strategies:\nincreasing the number of $n$-gram embeddings and scaling the model used to\nlearn them, both while maintaining fixed accelerator usage during inference (in\nterms of FLOPS and memory). We show that scaling both aspects enables a model\nwith 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline\nacross diverse corpora, while using only about half the FLOPS and accelerator\nmemory during inference.", "AI": {"tldr": "SCONE introduces n-gram embeddings for language models to enhance performance without increasing decoding costs.", "motivation": "To improve language model performance while minimizing decoding costs associated with larger vocabulary embeddings.", "method": "SCONE retains the original vocabulary and introduces frequent n-gram embeddings, which are learned separately and stored in off-accelerator memory, allowing efficient inference.", "result": "Models using SCONE outperform a 1.9B-parameter baseline with only 1B accelerator-resident parameters, while using half the FLOPS and accelerator memory during inference.", "conclusion": "SCONE provides effective scaling strategies for language models by incorporating n-gram embeddings without significantly impacting inference costs.", "key_contributions": ["Introduction of SCONE for scalable language model embeddings", "Demonstration of improved performance with fewer parameters", "Development of efficient inference strategies"], "limitations": "", "keywords": ["Language Models", "N-grams", "Embeddings", "Inference Efficiency", "Scalability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.02444", "pdf": "https://arxiv.org/pdf/2502.02444.pdf", "abs": "https://arxiv.org/abs/2502.02444", "title": "Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models", "authors": ["Haoran Ye", "Tianze Zhang", "Yuhang Xie", "Liyuan Zhang", "Yuanyi Ren", "Xin Zhang", "Guojie Song"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2024 Main", "summary": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.", "AI": {"tldr": "This paper presents the Generative Psycho-Lexical Approach (GPLA), a new method for constructing value systems for Large Language Models (LLMs), proposing a five-factor value system tailored for LLMs, which demonstrates improved alignment and safety predictions.", "motivation": "To address the lack of a psychologically grounded value system for Large Language Models (LLMs) amid rising concerns about their intrinsic values.", "method": "The study introduces the Generative Psycho-Lexical Approach (GPLA) for constructing value systems and validates it through three psychological benchmarking tasks integrated with AI priorities.", "result": "The proposed five-factor value system for LLMs meets psychological criteria, improves safety predictions, and enhances alignment compared to Schwartz's canonical values.", "conclusion": "The research provides a scalable and adaptable psychological framework for understanding and improving LLM values, contributing to better safety and alignment.", "key_contributions": ["Introduces the Generative Psycho-Lexical Approach (GPLA) for LLM value system construction", "Proposes a five-factor value system specifically for LLMs", "Demonstrates superior alignment and safety predictions of LLMs compared to existing value systems."], "limitations": "", "keywords": ["Large Language Models", "values", "psychological principles", "alignment", "safety prediction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.04235", "pdf": "https://arxiv.org/pdf/2502.04235.pdf", "abs": "https://arxiv.org/abs/2502.04235", "title": "Reformulation for Pretraining Data Augmentation", "authors": ["Xintong Hao", "Ruijie Zhu", "Ge Zhang", "Ke Shen", "Chenggang Li"], "categories": ["cs.CL"], "comment": "Dataset released\n  https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu", "summary": "Despite the impressive capabilities of large language models across various\ntasks, their continued scaling is severely hampered not only by data scarcity\nbut also by the performance degradation associated with excessive data\nrepetition during training. To overcome this critical bottleneck, we propose\nthe Massive Genre-Audience(MGA) reformulation method, a lightweight and\nscalable data augmentation technique inspired by synthetic data methodologies.\nMGA systematically reformulates existing corpora into diverse,\ncontextually-rich variations to mitigate the negative effects of repetition,\nand we introduce this approach along with the resulting 770 billion token\nMGACorpus in this work. We experimentally validate its core benefit by\ndemonstrating superior performance against data repetition and upsampling in\nscaling scenarios (up to 13B parameters). Furthermore, comprehensive analysis\ninvestigates the role of prompt engineering in generation quality and reveals\nnuances in evaluating model capabilities using standard loss metrics. Our work\nshows that MGA provides a reliable pathway to substantially augment training\ndatasets, effectively alleviating repetition bottlenecks and enabling more\nefficient scaling of large language models.", "AI": {"tldr": "This paper introduces the Massive Genre-Audience (MGA) method, a data augmentation technique designed to enhance the training of large language models by creating diverse corpus variations to counteract data repetition.", "motivation": "To address scaling issues in large language models due to data scarcity and performance degradation from excessive data repetition.", "method": "The MGA method reformulates existing datasets into diverse variations, enhancing context richness and mitigating repetition effects.", "result": "Experiments show that MGA outperforms traditional methods, such as data repetition and upsampling, when scaling models up to 13B parameters.", "conclusion": "MGA offers a viable solution for improving training dataset augmentation, thereby facilitating more efficient scaling of large language models and reducing data repetition issues.", "key_contributions": ["Introduction of the MGA reformulation method for data augmentation", "Creation of the MGACorpus with 770 billion tokens", "Analysis of prompt engineering's impact on generation quality"], "limitations": "", "keywords": ["data augmentation", "large language models", "training efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.05567", "pdf": "https://arxiv.org/pdf/2502.05567.pdf", "abs": "https://arxiv.org/abs/2502.05567", "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data", "authors": ["Xiaoyang Liu", "Kangjie Bao", "Jiashuo Zhang", "Yunqi Liu", "Yuntian Liu", "Yu Chen", "Yang Jiao", "Tao Luo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoformalization, the automatic translation of mathematical content from\nnatural language into machine-verifiable formal languages, has seen significant\nprogress driven by advances in large language models (LLMs). Nonetheless, a\nprimary barrier to further improvements is the limited availability of parallel\ncorpora that map informal mathematical text to its formal counterpart. To\naddress this limitation, we propose ATLAS (Autoformalizing Theorems through\nLifting, Augmentation, and Synthesis of Data), a novel data generation\nframework designed to produce large-scale, high-quality parallel corpora of\ntheorem statements. Distinct from prior approaches, ATLAS begins with a concept\nrepository, accelerates the improvement of student model through expert\niteration combined with knowledge distillation, and introduces two novel\naugmentation strategies that exploit the structural characteristics of formal\nlanguages. With the proposed ATLAS running for 10 iterations, we construct an\nundergraduate-level dataset comprising 117k theorem statements and develop\nATLAS Translator, which demonstrates statistically significant improvements\nover both the HERALD Translator and the Kimina-Autoformalizer across all\nbenchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The\ndatasets, model, and code will be released to the public soon.", "AI": {"tldr": "This paper introduces ATLAS, a framework for generating high-quality parallel corpora for autoformalization of mathematical content using large language models.", "motivation": "The main goal is to overcome the barrier of limited parallel corpora for translating informal mathematical text into formal languages.", "method": "The ATLAS framework utilizes a concept repository, expert iteration with knowledge distillation, and novel data augmentation strategies tailored to the structural characteristics of formal languages.", "result": "ATLAS generated an undergraduate-level dataset of 117k theorem statements and demonstrated statistically significant improvements in performance over existing translators.", "conclusion": "The proposed approach achieves state-of-the-art results in autoformalization and will make the datasets, model, and code publicly available soon.", "key_contributions": ["Introduction of the ATLAS framework for autoformalization", "Creation of a large-scale dataset of theorem statements", "Achievement of statistically significant improvements over existing translation models"], "limitations": "", "keywords": ["autoformalization", "large language models", "parallel corpora"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.05605", "pdf": "https://arxiv.org/pdf/2502.05605.pdf", "abs": "https://arxiv.org/abs/2502.05605", "title": "Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization", "authors": ["Yongcheng Zeng", "Xinyu Cui", "Xuanfa Jin", "Guoqing Liu", "Zexu Sun", "Dong Li", "Ning Yang", "Jianye Hao", "Haifeng Zhang", "Jun Wang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable general\nperformance, enabling smaller models to achieve capabilities comparable to\ntheir larger counterparts remains a critical challenge. For humans, iterative\nrefinement of problem analysis and responses is a common strategy to enhance\nanswer quality. However, we observe that existing LLMs exhibit limited ability\nto refine their outputs for quality improvement. In this paper, we first\ninvestigate mechanisms to unlock and progressively enhance self-refinement\nability in smaller models within an iterative preference optimization\nframework, aiming to bridge the performance gap with larger models. To this\nend, we propose EVOLVE, a novel post-training and inference framework that\niteratively integrates preference training with self-refinement-driven data\ncollection. During training, EVOLVE strengthens the model's direct\nquestion-answering ability while simultaneously unlocking its self-refinement\npotential. At inference, the framework leverages this capability to generate\nprogressively refined responses, which are filtered to construct datasets for\nsubsequent rounds of preference training. Experiments demonstrate EVOLVE's\nexceptional performance: when applied to Llama-3.1-8B base model and under the\nself-refinement setting, it surpasses state-of-the-art models including\nLlama-3.1-405B-Instruct and GPT-4o, achieving a 62.3% length-controlled win\nrate and 63.3% raw win rate on AlpacaEval 2, along with a 50.3% win rate on\nArena-Hard. Furthermore, EVOLVE consistently enhances performance on\nmathematical reasoning tasks like GSM8K and MATH.", "AI": {"tldr": "This paper introduces EVOLVE, a framework designed to enhance the self-refinement abilities of smaller language models, enabling them to compete with larger models in performance through iterative preference optimization.", "motivation": "To address the challenge that smaller language models have in matching the performance of larger models by enhancing their self-refinement capabilities.", "method": "The paper presents EVOLVE, which integrates preference training with self-refinement data collection iteratively during training and inference phases.", "result": "In experiments, EVOLVE applied to Llama-3.1-8B outperformed state-of-the-art models, achieving significant win rates in various benchmarks, including mathematical reasoning tasks.", "conclusion": "EVOLVE effectively bridges the performance gap between smaller and larger models, enhancing their quality of output through self-refinement mechanisms and iterative training.", "key_contributions": ["Introduction of the EVOLVE framework for model enhancement", "Demonstration of significant performance improvements in smaller models", "Successful application to mathematical reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "Self-Refinement", "Preference Optimization", "Model Training", "Mathematical Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.06207", "pdf": "https://arxiv.org/pdf/2502.06207.pdf", "abs": "https://arxiv.org/abs/2502.06207", "title": "Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement", "authors": ["Junyu Lu", "Kai Ma", "Kaichun Wang", "Kelaiti Xiao", "Roy Ka-Wei Lee", "Bo Xu", "Liang Yang", "Hongfei Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, accepted at the ACL 2025", "summary": "Large Language Models (LLMs) have become essential for offensive language\ndetection, yet their ability to handle annotation disagreement remains\nunderexplored. Disagreement samples, which arise from subjective\ninterpretations, pose a unique challenge due to their ambiguous nature.\nUnderstanding how LLMs process these cases, particularly their confidence\nlevels, can offer insight into their alignment with human annotators. This\nstudy systematically evaluates the performance of multiple LLMs in detecting\noffensive language at varying levels of annotation agreement. We analyze binary\nclassification accuracy, examine the relationship between model confidence and\nhuman disagreement, and explore how disagreement samples influence model\ndecision-making during few-shot learning and instruction fine-tuning. Our\nfindings reveal that LLMs struggle with low-agreement samples, often exhibiting\noverconfidence in these ambiguous cases. However, utilizing disagreement\nsamples in training improves both detection accuracy and model alignment with\nhuman judgment. These insights provide a foundation for enhancing LLM-based\noffensive language detection in real-world moderation tasks.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) handle annotation disagreement in offensive language detection, highlighting their performance, confidence levels, and how training with disagreement samples can improve model accuracy and alignment with human judgment.", "motivation": "The study aims to explore the underexplored aspects of LLMs in offensive language detection, specifically their performance in cases with annotation disagreementâan area relevant for real-world applications in content moderation.", "method": "The authors systematically evaluate the performance of multiple LLMs on binary classification tasks, analyze the relationship between model confidence and human disagreement, and assess the impact of disagreement samples on few-shot learning and instruction fine-tuning.", "result": "The findings show that LLMs face challenges with low-agreement samples, often exhibiting overconfidence; however, including disagreement samples in their training improves detection accuracy and alignment with human judgments.", "conclusion": "Enhancing LLM training with disagreement samples can lead to better performance in offensive language detection tasks, providing valuable insights for improving real-world content moderation systems.", "key_contributions": ["Systematic evaluation of LLMs on offensive language detection with respect to annotation disagreement.", "Analysis of the impact of model confidence on human disagreement.", "Demonstration that training with disagreement samples enhances model performance."], "limitations": "The study focuses on a specific aspect of LLM performance and may not generalize to all scenarios of language model application.", "keywords": ["Large Language Models", "offensive language detection", "annotation disagreement", "human judgment", "model confidence"], "importance_score": 9, "read_time_minutes": 18}}
{"id": "2502.06659", "pdf": "https://arxiv.org/pdf/2502.06659.pdf", "abs": "https://arxiv.org/abs/2502.06659", "title": "Who Taught You That? Tracing Teachers in Model Distillation", "authors": ["Somin Wadhwa", "Chantal Shaib", "Silvio Amir", "Byron C. Wallace"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Model distillation -- using outputs from a large teacher model to teach a\nsmall student model -- is a practical means of creating efficient models for a\nparticular task. We ask: Can we identify a students' teacher based on its\noutputs? Such \"footprints\" left by teacher LLMs would be interesting artifacts.\nBeyond this, reliable teacher inference may have practical implications as\nactors seek to distill specific capabilities of massive proprietary LLMs into\ndeployed smaller LMs, potentially violating terms of service. We consider\npractical task distillation targets including summarization, question\nanswering, and instruction-following. We assume a finite set of candidate\nteacher models, which we treat as blackboxes. We design discriminative models\nthat operate over lexical features. We find that $n$-gram similarity alone is\nunreliable for identifying teachers, but part-of-speech (PoS) templates\npreferred by student models mimic those of their teachers.", "AI": {"tldr": "The paper examines identifying the teacher models of distilled student models using output analysis, focusing on the reliability of teacher inference and its implications.", "motivation": "To understand if the outputs of student models can reveal their teacher models, and to highlight potential ethical issues in distillation practices.", "method": "Investigates the use of lexical features and discriminative models to analyze outputs from student models to infer their respective teachers among a finite set of candidates.", "result": "n-gram similarity was found to be unreliable for teacher identification; however, part-of-speech templates used by students closely reflected those of their teachers.", "conclusion": "Reliable inference of teacher models could have significant implications, especially concerning the ethical considerations in the distillation of proprietary LLM capabilities.", "key_contributions": ["Presents a novel method for teacher inference based on lexical features.", "Demonstrates the unreliability of n-gram similarity for this task.", "Proposes that PoS templates are key indicators of teacher-student model relationships."], "limitations": "The approach is limited to a finite set of candidate teacher models treated as black boxes.", "keywords": ["Model Distillation", "Teacher Inference", "LLMs", "NLP", "Ethics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.09120", "pdf": "https://arxiv.org/pdf/2502.09120.pdf", "abs": "https://arxiv.org/abs/2502.09120", "title": "Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues", "authors": ["Ye-eun Cho", "Yunho Maeng"], "categories": ["cs.CL"], "comment": "11 pages, 4 figures, 7 tables", "summary": "This study investigates whether vision-language models (VLMs) can perform\npragmatic inference, focusing on ignorance implicatures, utterances that imply\nthe speaker's lack of precise knowledge. To test this, we systematically\nmanipulated contextual cues: the visually depicted situation (visual cue) and\nQUD-based linguistic prompts (linguistic cue). When only visual cues were\nprovided, three state-of-the-art VLMs (GPT-4o, Gemini 1.5 Pro, and Claude 3.5\nsonnet) produced interpretations largely based on the lexical meaning of the\nmodified numerals. When linguistic cues were added to enhance contextual\ninformativeness, Claude exhibited more human-like inference by integrating both\ntypes of contextual cues. In contrast, GPT and Gemini favored precise, literal\ninterpretations. Although the influence of contextual cues increased, they\ntreated each contextual cue independently and aligned them with semantic\nfeatures rather than engaging in context-driven reasoning. These findings\nsuggest that although the models differ in how they handle contextual cues,\nClaude's ability to combine multiple cues may signal emerging pragmatic\ncompetence in multimodal models.", "AI": {"tldr": "This study examines the pragmatic inference abilities of vision-language models (VLMs) by manipulating visual and linguistic contextual cues. Results showed that Claude 3.5 sonnet combined cues effectively, demonstrating emerging pragmatic competence, while GPT-4o and Gemini 1.5 Pro favored literal interpretations.", "motivation": "To explore whether vision-language models (VLMs) can perform pragmatic inference, particularly with respect to ignorance implicatures.", "method": "The study systematically manipulated contextual cues, utilizing visual depictions and QUD-based linguistic prompts to evaluate the responses of three state-of-the-art VLMs.", "result": "When only visual cues were present, VLMs relied heavily on lexical meanings. With linguistic cues, Claude combined cues for more human-like inferences, while GPT and Gemini maintained a literal interpretation.", "conclusion": "The findings indicate differing behaviors among models concerning contextual cue handling, with Claude showing signs of pragmatic competence.", "key_contributions": ["Investigation of pragmatic inference in VLMs", "Systematic manipulation of contextual cues in testing", "Evidence of emerging pragmatic competence in Claude model"], "limitations": "", "keywords": ["vision-language models", "pragmatic inference", "contextual cues", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2502.10996", "pdf": "https://arxiv.org/pdf/2502.10996.pdf", "abs": "https://arxiv.org/abs/2502.10996", "title": "RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation", "authors": ["Pengcheng Jiang", "Lang Cao", "Ruike Zhu", "Minhao Jiang", "Yunyi Zhang", "Jimeng Sun", "Jiawei Han"], "categories": ["cs.CL"], "comment": "under review", "summary": "Large language models (LLMs) have achieved impressive performance on\nknowledge-intensive tasks, yet they often struggle with multi-step reasoning\ndue to the unstructured nature of retrieved context. While retrieval-augmented\ngeneration (RAG) methods provide external information, the lack of explicit\norganization among retrieved passages limits their effectiveness, leading to\nbrittle reasoning pathways. Recent interpretability studies highlighting the\nimportance of structured intermediate reasoning further align with this\nperspective. We propose Retrieval-And-Structuring (RAS), a framework that\ndynamically constructs query-specific knowledge graphs through iterative\nretrieval and structured knowledge building. RAS interleaves targeted retrieval\nplanning with incremental graph construction, enabling models to assemble and\nreason over evolving knowledge structures tailored to each query. On seven\nknowledge-intensive benchmarks, RAS consistently outperforms strong baselines,\nachieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs,\nrespectively. Our results demonstrate that dynamic, query-specific knowledge\nstructuring offers a robust path to improving reasoning accuracy and robustness\nin language model generation. Our data and code can be found at\nhttps://github.com/pat-jj/RAS.", "AI": {"tldr": "This paper introduces Retrieval-And-Structuring (RAS), a framework that improves multi-step reasoning in language models by constructing query-specific knowledge graphs through iterative retrieval and structuring.", "motivation": "Large language models struggle with reasoning due to the unstructured nature of retrieved context and the limitations of current RAG methods.", "method": "RAS dynamically constructs knowledge graphs through targeted retrieval planning and incremental graph construction, enabling structured reasoning over evolving knowledge.", "result": "RAS consistently outperforms strong baselines on seven knowledge-intensive benchmarks, achieving gains of up to 6.4% and 7.0% with open-source and proprietary LLMs, respectively.", "conclusion": "Dynamic, query-specific knowledge structuring significantly enhances reasoning accuracy and robustness in language generation tasks.", "key_contributions": ["Introduction of Retrieval-And-Structuring (RAS) framework", "Demonstrated performance improvements on knowledge-intensive tasks", "Integration of structured knowledge graphs for enhanced reasoning"], "limitations": "", "keywords": ["retrieval-augmented generation", "knowledge graphs", "multi-step reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11114", "pdf": "https://arxiv.org/pdf/2502.11114.pdf", "abs": "https://arxiv.org/abs/2502.11114", "title": "Beyond Pairwise: Global Zero-shot Temporal Graph Generation", "authors": ["Alon Eirew", "Kfir Bar", "Ido Dagan"], "categories": ["cs.CL"], "comment": null, "summary": "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, where event pairs are classified in isolation, leading\nto computational inefficiency and a lack of global consistency in the resulting\ntemporal graph. In this work, we propose a novel zero-shot method for TRE that\ngenerates a document's complete temporal graph in a single step, followed by\ntemporal constraint optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\noutperforms existing zero-shot approaches and offers a competitive alternative\nto supervised TRE models.", "AI": {"tldr": "This paper presents a novel zero-shot method for temporal relation extraction that generates a complete temporal graph and optimizes temporal constraints for consistency.", "motivation": "Despite advances in large language models, the application of those models in temporal relation extraction is limited, highlighting the need for efficient methods.", "method": "The proposed method generates a complete temporal graph for a document in one step, followed by temporal constraint optimization to refine and ensure consistency in predictions.", "result": "The proposed method outperforms existing zero-shot approaches and serves as a competitive alternative to traditional supervised temporal relation extraction models.", "conclusion": "The results indicate significant improvements in efficiency and effectiveness, establishing a new methodology for temporal relation extraction.", "key_contributions": ["Introduction of a novel zero-shot method for TRE", "Development of OmniTemp, a new dataset with complete event pair annotations", "Demonstration of improved performance over existing approaches"], "limitations": "", "keywords": ["temporal relation extraction", "natural language processing", "large language models"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2502.11150", "pdf": "https://arxiv.org/pdf/2502.11150.pdf", "abs": "https://arxiv.org/abs/2502.11150", "title": "Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures", "authors": ["Keren Gruteke Klein", "Shachar Frenkel", "Omer Shubi", "Yevgeni Berzak"], "categories": ["cs.CL"], "comment": null, "summary": "Automated text readability prediction is widely used in many real-world\nscenarios. Over the past century, such measures have primarily been developed\nand evaluated on reading comprehension outcomes and on human annotations of\ntext readability levels. In this work, we propose an alternative, eye\ntracking-based cognitive framework which directly taps into a key aspect of\nreadability: reading ease. We use this framework for evaluating a broad range\nof prominent readability measures, including two systems widely used in\neducation, by quantifying their ability to account for reading facilitation\neffects in text simplification, as well as text reading ease more broadly. Our\nanalyses suggest that existing readability measures are poor predictors of\nreading facilitation and reading ease, outperformed by word properties commonly\nused in psycholinguistics, and in particular by surprisal.", "AI": {"tldr": "This paper presents a cognitive framework using eye tracking to evaluate readability measures, finding that current measures often fail to predict reading ease effectively.", "motivation": "To propose a more effective methodology for predicting text readability by directly measuring reading ease through cognitive processes.", "method": "The study employs an eye tracking-based framework to assess various readability measures and their effectiveness in predicting reading facilitation effects and reading ease.", "result": "The analyses reveal that existing readability measures are inadequate predictors of reading facilitation and ease, with word properties from psycholinguistics, especially surprisal, proving to be more effective.", "conclusion": "This study underscores the limitations of traditional readability measures and suggests the importance of using cognitive-based approaches to more accurately assess reading ease.", "key_contributions": ["Development of an eye tracking-based cognitive framework for readability evaluation", "Identification of inadequacies in existing readability measures", "Demonstration of the superiority of psycholinguistic word properties in predicting reading ease"], "limitations": "", "keywords": ["readability", "eye tracking", "reading ease", "cognitive framework", "surprisal"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.11177", "pdf": "https://arxiv.org/pdf/2502.11177.pdf", "abs": "https://arxiv.org/abs/2502.11177", "title": "The Mirage of Model Editing: Revisiting Evaluation in the Wild", "authors": ["Wanli Yang", "Fei Sun", "Jiajun Tan", "Xinyu Ma", "Qi Cao", "Dawei Yin", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Despite near-perfect results in artificial evaluations, the effectiveness of\nmodel editing in real-world applications remains unexplored. To bridge this\ngap, we propose to study model editing in question answering (QA) by\nestablishing a rigorous evaluation practice to assess the effectiveness of\nediting methods in correcting LLMs' errors. It consists of QAEdit, a new\nbenchmark derived from popular QA datasets, and a standardized evaluation\nframework. Our single editing experiments indicate that current editing methods\nperform substantially worse than previously reported (38.5% vs. ~96%). Through\nmodule analysis and controlled experiments, we demonstrate that this\nperformance decline stems from issues in evaluation practices of prior editing\nresearch. One key issue is the inappropriate use of teacher forcing in testing\nprevents error propagation by feeding ground truth tokens (inaccessible in\nreal-world scenarios) as input. Furthermore, we simulate real-world deployment\nby sequential editing, revealing that current approaches fail drastically with\nonly 1000 edits. Our analysis provides a fundamental reexamination of both the\nreal-world applicability of existing model editing methods and their evaluation\npractices, and establishes a rigorous evaluation framework with key insights to\nadvance reliable and practical model editing research.", "AI": {"tldr": "This paper evaluates the effectiveness of model editing methods in question answering, revealing significant performance declines compared to previous studies due to flawed evaluation practices.", "motivation": "To address gaps in the real-world applicability of model editing for LLMs, particularly in question answering tasks.", "method": "The authors propose QAEdit, a benchmark for evaluating editing methods, and establish a standardized evaluation framework that includes module analysis and controlled experiments.", "result": "Through these experiments, they found that current editing methods perform worse than previously reported, primarily due to inappropriate evaluation practices.", "conclusion": "The study highlights the need for a fundamental reexamination of model editing methods and their evaluation frameworks to ensure their reliability and practical application.", "key_contributions": ["Introduction of QAEdit benchmark for question answering", "Establishment of a standardized evaluation framework", "Identification of flaws in previous editing evaluation practices"], "limitations": "Current study focuses only on LLMs in QA; results may not generalize to other applications or models.", "keywords": ["model editing", "question answering", "LLMs", "evaluation framework", "AI errors"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.11380", "pdf": "https://arxiv.org/pdf/2502.11380.pdf", "abs": "https://arxiv.org/abs/2502.11380", "title": "From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs", "authors": ["Zhu Liu", "Ying Liu", "KangYang Luo", "Cunliang Kong", "Maosong Sun"], "categories": ["cs.CL"], "comment": "Paper under review", "summary": "Lexico-semantic networks represent words as nodes and their semantic\nrelatedness as edges. While such networks are traditionally constructed using\nembeddings from encoder-based models or static vectors, embeddings from\ndecoder-only large language models (LLMs) remain underexplored. Unlike encoder\nmodels, LLMs are trained with a next-token prediction objective, which does not\ndirectly encode the meaning of the current token. In this paper, we construct\nlexico-semantic networks from the input embeddings of LLMs with varying\nparameter scales and conduct a comparative analysis of their global and local\nstructures. Our results show that these networks exhibit small-world\nproperties, characterized by high clustering and short path lengths. Moreover,\nlarger LLMs yield more intricate networks with less small-world effects and\nlonger paths, reflecting richer semantic structures and relations. We further\nvalidate our approach through analyses of common conceptual pairs, structured\nlexical relations derived from WordNet, and a cross-lingual semantic network\nfor qualitative words.", "AI": {"tldr": "This paper explores the construction of lexico-semantic networks using embeddings from decoder-only large language models (LLMs) and compares their structures to traditional models.", "motivation": "To investigate the potential of decoder-only LLMs in building lexico-semantic networks and analyzing their properties.", "method": "Constructed networks from input embeddings of LLMs of varying parameter scales and conducted a comparative structural analysis.", "result": "The constructed networks display small-world properties with high clustering and short path lengths, while larger models yield more complex structures with reduced small-world effects.", "conclusion": "Lexico-semantic networks from LLMs can capture intricate semantic relations, with larger models reflecting richer semantic structures.", "key_contributions": ["Introduced lexico-semantic networks constructed from decoder-only LLMs.", "Demonstrated small-world properties in networks from LLM embeddings.", "Provided insights into the structural differences of networks with varying model sizes."], "limitations": "Focuses solely on decoder-only LLMs; findings may not generalize to encoder-based models.", "keywords": ["lexico-semantic networks", "large language models", "small-world properties"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11525", "pdf": "https://arxiv.org/pdf/2502.11525.pdf", "abs": "https://arxiv.org/abs/2502.11525", "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs", "authors": ["Yi Hu", "Shijia Kang", "Haotong Yang", "Haotian Xu", "Muhan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Length generalization, the ability to solve problems longer than those seen\nduring training, remains a critical challenge for large language models (LLMs).\nPrevious work modifies positional encodings (PEs) and data formats to improve\nlength generalization on specific symbolic tasks such as addition and sorting.\nHowever, these approaches are fundamentally limited to special tasks, often\ndegrading general language performance. Furthermore, they are typically\nevaluated on small transformers trained from scratch on single tasks and can\ncause performance drop when applied during post-training stage of practical\nLLMs with general capabilities. Hu et al., (2024) proposed Rule-Following\nFine-Tuning (RFFT) to improve length generalization in the post-training stage\nof LLMs. Despite its compatibility with practical models and strong\nperformance, RFFT is proposed for single tasks too, requiring re-training for\neach individual task with extensive examples. In this paper, we study length\ngeneralization in multi-task settings and propose Meta Rule-Following\nFine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length\ngeneralization. As our first contribution, we construct a large length\ngeneralization dataset containing 86 tasks spanning code execution, number\nprocessing, symbolic and logical reasoning tasks, beyond the common addition or\nmultiplication tasks. Secondly, we show that cross-task length generalization\nis possible with Meta-RFFT. After training on a large number of tasks and\ninstances, the models achieve remarkable length generalization ability on\nunseen tasks with minimal fine-tuning or one-shot prompting. For example, after\nfine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30\ndigit addition, significantly outperforming the state-of-the-art reasoning\nmodels (DeepSeek-R1-671B: 72%), despite never seeing this task during\nRF-pretraining.", "AI": {"tldr": "This paper introduces Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance length generalization in multi-task settings for large language models, demonstrating superior performance on unseen tasks with minimal fine-tuning.", "motivation": "Length generalization is a critical challenge for large language models, affecting their ability to solve problems longer than those encountered during training. Existing methods are limited and often degrade general language performance.", "method": "The authors propose Meta Rule-Following Fine-Tuning (Meta-RFFT), a framework designed for cross-task length generalization. They construct a comprehensive dataset with 86 tasks and demonstrate that their method allows models to generalize effectively across these tasks.", "result": "Models trained using Meta-RFFT achieve up to 95% accuracy on previously unseen length tasks, such as 30 digit addition, outperforming the state-of-the-art reasoning models.", "conclusion": "Meta-RFFT represents a significant advancement in multi-task length generalization, providing a robust approach that outperforms prior methods without extensive re-training for each task.", "key_contributions": ["Introduction of Meta-RFFT for cross-task length generalization", "Creation of a large length generalization dataset with 86 diverse tasks", "Demonstration of remarkable unseen task accuracy with minimal fine-tuning"], "limitations": "", "keywords": ["Length generalization", "Meta-RFFT", "multi-task learning", "large language models", "cross-task performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11571", "pdf": "https://arxiv.org/pdf/2502.11571.pdf", "abs": "https://arxiv.org/abs/2502.11571", "title": "FaMTEB: Massive Text Embedding Benchmark in Persian Language", "authors": ["Erfan Zinvandi", "Morteza Alikhani", "Mehran Sarmadi", "Zahra Pourbahman", "Sepehr Arvin", "Reza Kazemi", "Arash Amini"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.", "AI": {"tldr": "The paper introduces a benchmark for Persian text embeddings, highlighting 63 datasets across seven tasks, including new evaluation datasets and tasks.", "motivation": "To provide a comprehensive evaluation framework for Persian language models, as existing benchmarks are insufficient for the growing use of text embeddings in applications like chatbots and retrieval-augmented generation.", "method": "Development of a benchmark built on the Massive Text Embedding Benchmark (MTEB), incorporating existing, translated, and newly generated datasets for Persian NLP.", "result": "The introduction of evaluation datasets for chatbots and the new task of summary retrieval, along with performance evaluations of several Persian and multilingual embedding models.", "conclusion": "This work presents an open-source benchmark with resources for training and evaluating Persian NLP models, enhancing the field with new datasets and tasks.", "key_contributions": ["Comprehensive benchmark for Persian text embeddings based on MTEB.", "Inclusion of chatbot evaluation datasets for the first time.", "Introduction of a new task: summary retrieval."], "limitations": "", "keywords": ["Persian NLP", "text embeddings", "evaluation framework", "chatbots", "summary retrieval"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.12202", "pdf": "https://arxiv.org/pdf/2502.12202.pdf", "abs": "https://arxiv.org/abs/2502.12202", "title": "To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models", "authors": ["Zihao Zhu", "Hongbao Zhang", "Ruotong Wang", "Ke Xu", "Siwei Lyu", "Baoyuan Wu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "39 pages, 13 tables, 14 figures", "summary": "Large Reasoning Models (LRMs) are designed to solve complex tasks by\ngenerating explicit reasoning traces before producing final answers. However,\nwe reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability\n-- wherein the thinking process can be bypassed by manipulating special\ndelimiter tokens. It is empirically demonstrated to be widespread across\nmainstream LRMs, posing both a significant risk and potential utility,\ndepending on how it is exploited. In this paper, we systematically investigate\nthis vulnerability from both malicious and beneficial perspectives. On the\nmalicious side, we introduce Breaking of Thought (BoT), a novel attack that\nenables adversaries to bypass the thinking process of LRMs, thereby\ncompromising their reliability and availability. We present two variants of\nBoT: a training-based version that injects backdoor during the fine-tuning\nstage, and a training-free version based on adversarial attack during the\ninference stage. As a potential defense, we propose thinking recovery alignment\nto partially mitigate the vulnerability. On the beneficial side, we introduce\nMonitoring of Thought (MoT), a plug-and-play framework that allows model owners\nto enhance efficiency and safety. It is implemented by leveraging the same\nvulnerability to dynamically terminate redundant or risky reasoning through\nexternal monitoring. Extensive experiments show that BoT poses a significant\nthreat to reasoning reliability, while MoT provides a practical solution for\npreventing overthinking and jailbreaking. Our findings expose an inherent flaw\nin current LRM architectures and underscore the need for more robust reasoning\nsystems in the future.", "AI": {"tldr": "This paper investigates a vulnerability in Large Reasoning Models (LRMs) termed Unthinking Vulnerability, which can be exploited for malicious and beneficial purposes.", "motivation": "To address the critical vulnerability found in LRMs that allows bypassing their reasoning process, potentially compromising their reliability.", "method": "The paper introduces and empirically demonstrates a novel attack, Breaking of Thought (BoT), and a framework, Monitoring of Thought (MoT), to exploit and mitigate this vulnerability.", "result": "BoT demonstrates a significant threat to LRM reasoning reliability, while MoT offers a practical solution to enhance efficiency and safety in LRMs.", "conclusion": "The findings reveal inherent flaws in LRM architectures, emphasizing the need for more robust reasoning systems.", "key_contributions": ["Introduction of Unthinking Vulnerability in LRMs.", "Development of the Breaking of Thought (BoT) attack.", "Proposal of Monitoring of Thought (MoT) for improving model safety."], "limitations": "The proposed defenses are partial and do not completely eradicate the vulnerability.", "keywords": ["Large Reasoning Models", "Unthinking Vulnerability", "Breaking of Thought", "Monitoring of Thought", "adversarial attacks"], "importance_score": 8, "read_time_minutes": 40}}
{"id": "2502.12464", "pdf": "https://arxiv.org/pdf/2502.12464.pdf", "abs": "https://arxiv.org/abs/2502.12464", "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models", "authors": ["Seanie Lee", "Dong Bok Lee", "Dominik Wagner", "Minki Kang", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Deploying large language models (LLMs) in real-world applications requires\nrobust safety guard models to detect and block harmful user prompts. While\nlarge safety guard models achieve strong performance, their computational cost\nis substantial. To mitigate this, smaller distilled models are used, but they\noften underperform on \"hard\" examples where the larger model provides accurate\npredictions. We observe that many inputs can be reliably handled by the smaller\nmodel, while only a small fraction require the larger model's capacity.\nMotivated by this, we propose SafeRoute, a binary router that distinguishes\nhard examples from easy ones. Our method selectively applies the larger safety\nguard model to the data that the router considers hard, improving efficiency\nwhile maintaining accuracy compared to solely using the larger safety guard\nmodel. Experimental results on multiple benchmark datasets demonstrate that our\nadaptive model selection significantly enhances the trade-off between\ncomputational cost and safety performance, outperforming relevant baselines.", "AI": {"tldr": "Introducing SafeRoute, a binary router that enhances the efficiency of safety guard models for LLMs by selectively applying larger models to hard examples.", "motivation": "The deployment of large language models necessitates robust safety systems to detect harmful prompts, but their computational cost is high. Smaller distilled models perform poorly on difficult examples, prompting the need for a solution that balances efficiency and safety.", "method": "SafeRoute uses a binary routing mechanism to differentiate between easy and hard examples, applying the larger safety guard model only to the latter.", "result": "The proposed method improves safety performance and reduces computational costs by smartly selecting model application based on input complexity.", "conclusion": "SafeRoute significantly increases efficiency without sacrificing accuracy, surpassing traditional models and improving safety measures in LLM applications.", "key_contributions": ["Introduction of SafeRoute, a binary router for LLM safety", "Demonstrated performance improvements on benchmark datasets", "Enhanced trade-off between cost and safety in model application"], "limitations": "The approach may still struggle with ambiguous examples where the distinction between easy and hard is unclear.", "keywords": ["Large Language Models", "Safety Guards", "Model Efficiency", "Adaptive Model Selection", "Benchmark Datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.13691", "pdf": "https://arxiv.org/pdf/2502.13691.pdf", "abs": "https://arxiv.org/abs/2502.13691", "title": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora", "authors": ["Tristan Karch", "Luca Engel", "Philippe Schwaller", "FrÃ©dÃ©ric Kaplan"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing five strategically selected datasets: EPFL PhD manuscripts, a private\ncollection of Venetian historical records, two sets of Wikipedia articles on\nrelated topics, and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts.", "AI": {"tldr": "An automated pipeline evaluates the potential information gain from text collections for LLMs by generating MCQs and measuring performance differences.", "motivation": "To advance LLM performance by identifying valuable information sources that require significant investment for integration.", "method": "An automated pipeline generates multiple choice questions from texts and measures LLM performance with and without access to source material to determine information potential.", "result": "The method effectively identifies collections with valuable novel information across five datasets, showcasing its capability in prioritizing data acquisition efforts.", "conclusion": "This approach provides a practical tool for evaluating text collections' potential information gain for enhancing LLM performance.", "key_contributions": ["Automated evaluation of text collections for LLMs", "Introduction of MCQ generation as a performance measure", "Validation on diverse datasets including historical and synthetic data"], "limitations": "", "keywords": ["Large Language Models", "Information Gain", "Multiple Choice Questions", "Data Integration", "Text Collections"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.14285", "pdf": "https://arxiv.org/pdf/2502.14285.pdf", "abs": "https://arxiv.org/abs/2502.14285", "title": "Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach", "authors": ["Yurong Wu", "Fangwen Mu", "Qiuhong Zhang", "Jinjing Zhao", "Xinrun Xu", "Lingrui Mei", "Yang Wu", "Lin Shi", "Junjie Wang", "Zhiming Ding", "Yiwei Wang"], "categories": ["cs.CL"], "comment": "14 pages,8 figures,4 tables", "summary": "Prompt trading has emerged as a significant intellectual property concern in\nrecent years, where vendors entice users by showcasing sample images before\nselling prompt templates that can generate similar images. This work\ninvestigates a critical security vulnerability: attackers can steal prompt\ntemplates using only a limited number of sample images. To investigate this\nthreat, we introduce Prism, a prompt-stealing benchmark consisting of 50\ntemplates and 450 images, organized into Easy and Hard difficulty levels. To\nidentify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a\nnovel template stealing method that operates without model fine-tuning by\nleveraging differential evolution algorithms. The system first initializes\npopulation sets using multimodal large language models (MLLMs) based on\npredefined patterns, then iteratively generates enhanced offspring through\nMLLMs. During evolution, EvoStealer identifies common features across offspring\nto derive generalized templates. Our comprehensive evaluation conducted across\nopen-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini)\ndemonstrates that EvoStealer's stolen templates can reproduce images highly\nsimilar to originals and effectively generalize to other subjects,\nsignificantly outperforming baseline methods with an average improvement of\nover 10%. Moreover, our cost analysis reveals that EvoStealer achieves template\nstealing with negligible computational expenses. Our code and dataset are\navailable at https://github.com/whitepagewu/evostealer.", "AI": {"tldr": "The paper introduces EvoStealer, a method for stealing prompt templates from vendors using a limited number of sample images and evaluates its effectiveness against various models, highlighting significant performance improvements and low computational costs.", "motivation": "To address the rising concern of prompt trading and the security vulnerabilities associated with stealing prompt templates from visual language models.", "method": "EvoStealer utilizes differential evolution algorithms to generate templates without fine-tuning, initializing population sets using multimodal large language models and iteratively enhancing them to derive generalized templates.", "result": "EvoStealer's stolen templates closely replicate original images and generalize well across subjects, outperforming baseline techniques by over 10% on average, while maintaining low computational costs.", "conclusion": "EvoStealer effectively demonstrates significant vulnerabilities of VLMs to prompt stealing, offering a substantial advancement over existing methods with minimal expense.", "key_contributions": ["Introduction of the Prism benchmark for prompt stealing", "Development of EvoStealer using differential evolution algorithms", "Demonstration of effective template stealing with low computational costs"], "limitations": "", "keywords": ["prompt trading", "security vulnerability", "template stealing", "differential evolution", "visual language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.21309", "pdf": "https://arxiv.org/pdf/2502.21309.pdf", "abs": "https://arxiv.org/abs/2502.21309", "title": "FANformer: Improving Large Language Models Through Effective Periodicity Modeling", "authors": ["Yihong Dong", "Ge Li", "Xue Jiang", "Yongding Tao", "Kechi Zhang", "Hao Zhu", "Huanyu Liu", "Jiazheng Ding", "Jia Li", "Jinliang Deng", "Hong Mei"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into\nattention mechanism to achieve efficient periodicity modeling, by modifying the\nfeature projection process of attention mechanism. Extensive experimental\nresults on language modeling show that FANformer consistently outperforms\nTransformer when scaling up model size and training tokens, underscoring its\nsuperior learning efficiency. Our pretrained FANformer-1B exhibits marked\nimprovements on downstream tasks compared to open-source LLMs with similar\nmodel parameters or training tokens. Moreover, we reveal that FANformer\nexhibits superior ability to learn and apply rules for reasoning compared to\nTransformer. The results position FANformer as an effective and promising\narchitecture for advancing LLMs.", "AI": {"tldr": "This paper presents FANformer, a new architecture that integrates Fourier Analysis Network into Transformer models to enhance periodicity modeling, leading to improved learning efficiency and performance in large language models.", "motivation": "The paper addresses the flaws of periodicity modeling in Transformers that hinder learning efficiency in large language models.", "method": "FANformer adapts Fourier Analysis Network (FAN) into the attention mechanism of Transformers by modifying the feature projection process, enhancing periodicity modeling.", "result": "FANformer consistently outperforms Transformer in language modeling tasks as model size and training tokens increase, showing marked improvements on downstream tasks.", "conclusion": "FANformer is an effective and promising architecture for advancing large language models due to its enhanced learning efficiency and reasoning capabilities.", "key_contributions": ["Introduced FANformer architecture for improved periodicity modeling", "Demonstrated superior performance over Transformers in various tasks", "Revealed enhanced reasoning abilities in FANformer compared to traditional Transformers."], "limitations": "", "keywords": ["FANformer", "Transformer", "periodicity modeling", "language models", "Fourier Analysis Network"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.03008", "pdf": "https://arxiv.org/pdf/2503.03008.pdf", "abs": "https://arxiv.org/abs/2503.03008", "title": "MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings", "authors": ["Andrea Gurioli", "Federico Pennino", "JoÃ£o Monteiro", "Maurizio Gabbrielli"], "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "comment": null, "summary": "Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks.", "AI": {"tldr": "MoSE introduces a 1-billion-parameter multi-exit encoder for code retrieval and classification, enhancing model performance via Self-Distillation while minimizing deployment costs.", "motivation": "To address the challenges of accuracy vs. performance trade-offs in deploying language models for tasks like code retrieval and classification, especially under latency constraints.", "method": "The paper presents ModularStarEncoder (MoSE), which uses a Self-Distillation mechanism to improve lower-layer representations and include a repository-level contextual loss to enhance training context usage.", "result": "MoSE improves text-to-code and code-to-code search by optimizing encoder layer usage, yielding better performance in code understanding tasks with minimal additional costs.", "conclusion": "Self-Distillation serves as an effective method for trading inference cost for accuracy, demonstrated through improvements in various code understanding challenges.", "key_contributions": ["Introduction of ModularStarEncoder (MoSE) for flexible model deployment.", "Self-Distillation mechanism to enhance intermediate representations.", "Creation of a new dataset for text-to-code and code-to-code benchmarks."], "limitations": "", "keywords": ["ModularStarEncoder", "Self-Distillation", "code retrieval", "code classification", "text-to-code"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2503.03261", "pdf": "https://arxiv.org/pdf/2503.03261.pdf", "abs": "https://arxiv.org/abs/2503.03261", "title": "Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions", "authors": ["Yichong Zhao", "Susumu Goto"], "categories": ["cs.CL"], "comment": null, "summary": "Multiple previous studies have reported suboptimal performance of LLMs in\nbiomedical text mining. By analyzing failure patterns in these evaluations, we\nidentified three primary challenges for LLMs in biomedical corpora: (1) LLMs\nfail to learn implicit dataset-specific nuances from supervised data, (2) The\ncommon formatting requirements of discriminative tasks limit the reasoning\ncapabilities of LLMs particularly for LLMs that lack test-time compute, and (3)\nLLMs struggle to adhere to annotation guidelines and match exact schemas, which\nhinders their ability to understand detailed annotation requirements which is\nessential in biomedical annotation workflow. We experimented with prompt\nengineering techniques targeted to the above issues, and developed a pipeline\nthat dynamically extracts instructions from annotation guidelines. Our results\nshow that frontier LLMs can approach or surpass the performance of SOTA\nBERT-based models with minimal reliance on manually annotated data and without\nfine-tuning. Furthermore, we performed model distillation on a closed-source\nLLM, demonstrating that a BERT model trained exclusively on synthetic data\nannotated by LLMs can also achieve a practical performance. Based on these\nfindings, we explored the feasibility of partially replacing manual annotation\nwith LLMs in production scenarios for biomedical text mining.", "AI": {"tldr": "This study analyzes challenges faced by LLMs in biomedical text mining and develops prompt engineering techniques to improve their performance.", "motivation": "To address the suboptimal performance of LLMs in biomedical text mining and to identify key challenges impacting their effectiveness.", "method": "The authors analyzed failure patterns in evaluations and experimented with prompt engineering techniques, developing a pipeline that extracts instructions from annotation guidelines.", "result": "Frontier LLMs can match or exceed the performance of state-of-the-art BERT-based models with minimal reliance on manually annotated data and no fine-tuning; model distillation on a closed-source LLM achieved practical performance using synthetic data.", "conclusion": "There is potential to partially replace manual annotation with LLMs in biomedical text mining scenarios, improving efficiency in the annotation workflow.", "key_contributions": ["Identification of challenges faced by LLMs in biomedical text mining", "Development of a prompt engineering pipeline for LLMs", "Demonstration of LLMs' capability to approach SOTA performance without fine-tuning"], "limitations": "Further exploration is needed to fully validate the application of LLMs in diverse biomedical contexts and to understand long-term impacts on data quality.", "keywords": ["biomedical text mining", "LLMs", "prompt engineering", "annotation guidelines", "model distillation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.06218", "pdf": "https://arxiv.org/pdf/2503.06218.pdf", "abs": "https://arxiv.org/abs/2503.06218", "title": "SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios", "authors": ["Weidong Zhan", "Yue Wang", "Nan Hu", "Liming Xiao", "Jingyuan Ma", "Yuhang Qin", "Zheng Li", "Yixin Yang", "Sirui Deng", "Jinkun Ding", "Wenhan Ma", "Rui Li", "Weilin Luo", "Qun Liu", "Zhifang Sui"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, long-chain reasoning remains a key challenge for large language\nmodels (LLMs) because natural texts lack sufficient explicit reasoning data.\nHowever, existing benchmarks suffer from limitations such as narrow coverage,\nshort reasoning paths, or high construction costs. We introduce SCoRE\n(Scenario-based Commonsense Reasoning Evaluation), a benchmark that synthesizes\nmulti-hop questions from scenario schemas of entities, relations, and logical\nrules to assess long-chain commonsense reasoning. SCoRE contains 100k bilingual\n(Chinese-English) multiple-choice questions whose reasoning chains span 2-11\nhops and are grouped into various difficulty levels. Each question is\naccompanied by fine-grained knowledge labels, explicit reasoning chains, and\ndifficulty levels for diagnostic evaluation. Evaluation results on cutting-edge\nLLMs such as o3-mini and Deepseek R1 shows that even the best model attains\nonly 69.78% accuracy on SCoRE (even only 47.91% on the hard set), with errors\noften stemming from rare knowledge, logical inconsistency, and\nover-interpretation of simple questions. SCoRE offers a scalable, extensible\nframework for evaluating and diagnosing the long-chain commonsense reasoning\nabilities of LLMs and guiding future advances in model design and training.", "AI": {"tldr": "Introduction of SCoRE, a benchmark for evaluating long-chain commonsense reasoning in LLMs.", "motivation": "Long-chain reasoning is challenging for LLMs due to inadequate explicit reasoning data. Existing benchmarks are limited.", "method": "SCoRE synthesizes multi-hop questions from scenario schemas of entities, relations, and logical rules.", "result": "Evaluation shows top models only achieving 69.78% accuracy, revealing errors stemming from rare knowledge and logical inconsistencies.", "conclusion": "SCoRE provides a scalable framework for evaluating and improving long-chain commonsense reasoning in LLMs.", "key_contributions": ["Introduction of a new benchmark for commonsense reasoning in LLMs", "Includes 100k bilingual multiple-choice questions with various difficulty levels", "Provides explicit reasoning chains and diagnostic evaluation labels"], "limitations": "Current models struggle with accuracy; high errors arise from rare knowledge and logical inconsistencies.", "keywords": ["Commonsense Reasoning", "Large Language Models", "Benchmark", "Natural Language Processing", "Multi-hop Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.09543", "pdf": "https://arxiv.org/pdf/2503.09543.pdf", "abs": "https://arxiv.org/abs/2503.09543", "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs", "authors": ["Oskar van der Wal", "Pietro Lesci", "Max Muller-Eberstein", "Naomi Saphra", "Hailey Schoelkopf", "Willem Zuidema", "Stella Biderman"], "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The stability of language model pre-training and its effects on downstream\nperformance are still understudied. Prior work shows that the training process\ncan yield significantly different results in response to slight variations in\ninitial conditions, e.g., the random seed. Crucially, the research community\nstill lacks sufficient resources and tools to systematically investigate\npre-training stability, particularly for decoder-only language models. We\nintroduce the PolyPythias, a set of 45 new training runs for the Pythia model\nsuite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting\nin about 7k new checkpoints that we release. Using these new 45 training runs,\nin addition to the 5 already available, we study the effects of different\ninitial conditions determined by the seed -- i.e., parameters' initialisation\nand data order -- on (i) downstream performance, (ii) learned linguistic\nrepresentations, and (iii) emergence of training phases. In addition to common\nscaling behaviours, our analyses generally reveal highly consistent training\ndynamics across both model sizes and initial conditions. Further, the new seeds\nfor each model allow us to identify outlier training runs and delineate their\ncharacteristics. Our findings show the potential of using these methods to\npredict training stability.", "AI": {"tldr": "This paper investigates the stability of pre-training in language models, particularly focusing on the Pythia model with new training runs to analyze the effects of initial conditions.", "motivation": "The research aims to fill the gap in understanding how variations in initial conditions affect the performance of decoder-only language models during pre-training.", "method": "The authors introduced 45 new training runs for the Pythia model suite across different seeds and model sizes, allowing them to analyze training dynamics and stability across conditions.", "result": "The analyses indicate consistent training dynamics across different model sizes and initial conditions, revealing characteristics of outlier training runs and their implications for training stability.", "conclusion": "The study suggests that the introduced methods can effectively predict the stability of training in language models, providing valuable tools for the research community.", "key_contributions": ["Introduction of PolyPythias training runs for systematic study", "Analysis of pre-training stability on downstream performance", "Identification of outlier training runs and their characteristics"], "limitations": "The study focuses only on the Pythia model suite and may not generalize to all types of language models.", "keywords": ["Language Models", "Pre-training Stability", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.09674", "pdf": "https://arxiv.org/pdf/2503.09674.pdf", "abs": "https://arxiv.org/abs/2503.09674", "title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation", "authors": ["Jonathan Zheng", "Sauvik Das", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, preprint", "summary": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average.", "AI": {"tldr": "This paper introduces the BRANCH methodology for estimating the privacy risk of user-generated documents by quantifying k-privacy values using large language models (LLMs) and Bayesian networks.", "motivation": "To address the challenge of estimating privacy risks in user-generated content by quantifying uncertainty and ambiguity in decision-making processes related to privacy-sensitive information.", "method": "BRANCH factorizes the joint probability distribution of personal information as random variables, utilizing a Bayesian network to separately estimate the probability of each factor within a population and combine them to compute the final k-privacy value.", "result": "The BRANCH method estimates the k-privacy value correctly 73% of the time, outperforming the o3-mini model by 13%; it also demonstrates that LLM uncertainty is a predictive indicator of accuracy, with high-variance predictions being less accurate.", "conclusion": "The BRANCH methodology offers a significant step forward in estimating privacy risks associated with user-generated documents, showcasing the potential of LLMs and Bayesian networks in this domain.", "key_contributions": ["Introduction of the BRANCH methodology for estimating k-privacy values", "Demonstration of the reliability of LLM uncertainty as an accuracy indicator", "Improvement in k-value estimation compared to existing models"], "limitations": "The study focuses primarily on numerical reasoning under uncertainty; broader applicability and real-world testing may be needed for practical use.", "keywords": ["privacy risk estimation", "language models", "k-privacy", "Bayesian networks", "probabilistic reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.10669", "pdf": "https://arxiv.org/pdf/2503.10669.pdf", "abs": "https://arxiv.org/abs/2503.10669", "title": "UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality", "authors": ["Zelei Cheng", "Xin-Qiang Cai", "Yuting Tang", "Pushi Zhang", "Boming Yang", "Masashi Sugiyama", "Xinyu Xing"], "categories": ["cs.CL", "cs.AI"], "comment": "Language Modeling, Machine Learning for NLP, Distributional\n  Pareto-Optimal", "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs) with human values. However, existing\napproaches struggle to capture the multi-dimensional, distributional nuances of\nhuman preferences. Methods such as RiC that directly inject raw reward values\ninto prompts face significant numerical sensitivity issues--for instance, LLMs\nmay fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF,\nRewarded Soups, and MODPO incur high computational costs by training multiple\nmodels. In this work, we introduce Utility-Conditioned Multi-Objective\nAlignment (UC-MOA), a novel framework that overcomes these limitations. Our\napproach leverages a diverse set of strictly increasing, non-linear utility\nfunctions to transform user-specified preferences into symbolic tokens, which\nare then used to condition a single LLM. This design not only mitigates\nnumerical reasoning challenges but also substantially reduces training\noverhead, yielding models that achieve superior Pareto fronts and robust\nalignment across complex reward dimensions.", "AI": {"tldr": "Introducing UC-MOA, a framework that transforms user preferences into tokens for better alignment of large language models with human values while reducing training costs.", "motivation": "Existing RLHF methods struggle with capturing human preferences and face numerical sensitivities and high computational costs.", "method": "The UC-MOA framework uses a set of non-linear utility functions to encode preferences into symbolic tokens for conditioning a single LLM.", "result": "UC-MOA mitigates numerical reasoning issues and significantly reduces training overhead, resulting in models with better Pareto fronts and alignment in complex reward scenarios.", "conclusion": "The approach presents a promising alternative to existing RLHF methodologies by efficiently aligning LLMs with human multi-dimensional preferences.", "key_contributions": ["Introduction of a novel framework (UC-MOA) for aligning LLMs with human values.", "Utilization of non-linear utility functions to encode user preferences into tokens.", "Reduction of training costs while improving model alignment and performance."], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Utility-Conditioned Multi-Objective Alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.12908", "pdf": "https://arxiv.org/pdf/2503.12908.pdf", "abs": "https://arxiv.org/abs/2503.12908", "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models", "authors": ["Xinyan Jiang", "Hang Ye", "Yongxin Zhu", "Xiaoying Zheng", "Zikang Chen", "Jun Gong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 findings", "summary": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.", "AI": {"tldr": "This paper presents HICD, a method to induce controlled hallucinations in Large Language Models for improving output accuracy in various tasks.", "motivation": "To address the issue of hallucinations in LLMs that generate contextually and factually incorrect outputs.", "method": "HICD selects attention heads critical to predictions, induces hallucinations by dispersing their attention, and compares the outputs to refine performance.", "result": "HICD significantly enhances performance on tasks like context completion and question answering by promoting factuality and contextual faithfulness.", "conclusion": "The method offers a controlled way to induce hallucinations, leading to improvements in LLM performance across different applications.", "key_contributions": ["Introduction of HICD for controlled hallucinations in LLMs", "Selection of attention heads for inducing hallucinations", "Demonstrated improvements over existing methods in multiple tasks"], "limitations": "", "keywords": ["Large Language Models", "hallucinations", "contrastive decoding", "attention heads", "factual accuracy"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.14411", "pdf": "https://arxiv.org/pdf/2503.14411.pdf", "abs": "https://arxiv.org/abs/2503.14411", "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models", "authors": ["Siwei Zhang", "Yun Xiong", "Yateng Tang", "Xi Chen", "Zian Jia", "Zehao Gu", "Jiarong Xu", "Jiawei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Submit to NeurIPS2025", "summary": "Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{CROSS}, a flexible\nframework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is\ndesigned by decomposing the TTAG modeling process into two phases: (i) temporal\nsemantics extraction; and (ii) semantic-structural information unification. The\nkey idea is to advance the large language models (LLMs) to dynamically extract\nthe temporal semantics in text space and then generate cohesive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the CROSS framework, which empowers LLMs to offer the\ntemporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experiments show that CROSS achieves state-of-the-art results on four\npublic datasets and one industrial dataset, with 24.7% absolute MRR gain on\naverage in temporal link prediction and 3.7% AUC gain in node classification of\nindustrial application.", "AI": {"tldr": "CROSS is a framework for modeling temporal text-attributed graphs, enhancing temporal semantics extraction using large language models and integrating semantic and structural information to improve performance.", "motivation": "Real-world temporal graphs often contain rich textual information which creates complexity in modeling. Existing temporal graph neural networks fail to effectively incorporate dynamic text semantics, leading to suboptimal performance.", "method": "CROSS decomposes TTAG modeling into two phases: temporal semantics extraction and semantic-structural information unification, utilizing large language models for dynamic extraction of temporal semantics and a co-encoder for unifying these with structural data.", "result": "CROSS demonstrated state-of-the-art performance on multiple datasets, achieving a 24.7% absolute MRR gain in temporal link prediction and a 3.7% AUC gain in node classification in industrial applications.", "conclusion": "The CROSS framework shows significant improvements in processing temporal text-attributed graphs by leveraging LLMs for semantic understanding and enabling a cohesive representation of data.", "key_contributions": ["Introduction of the CROSS framework for temporal text-attributed graph modeling", "Dynamic extraction of temporal semantics using large language models", "Integration of semantic and structural information for enhanced performance"], "limitations": "", "keywords": ["temporal graph neural networks", "temporal text-attributed graphs", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.21380", "pdf": "https://arxiv.org/pdf/2503.21380.pdf", "abs": "https://arxiv.org/abs/2503.21380", "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models", "authors": ["Haoxiang Sun", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark", "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5\nPro Exp demonstrating notably limited accuracy on the hard subset. Furthermore,\nthe benchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the benchmark,\nevaluation code, detailed results and a data visualization tool at\nhttps://github.com/RUCAIBox/OlymMATH.", "AI": {"tldr": "OlymMATH is a new Olympiad-level mathematical benchmark aimed at evaluating the reasoning capabilities of large language models (LLMs) with 200 curated problems across two difficulty tiers.", "motivation": "To address the gap in existing benchmarks for evaluating mathematical reasoning in large language models, which have become saturated and ineffective.", "method": "The benchmark includes 200 manually verified mathematical problems, organized into easy (AIME-level) and hard (challenging) tiers, covering four core mathematical fields with an emphasis on bilingual assessment.", "result": "Empirical results demonstrate that state-of-the-art models struggle significantly with the hard problems, indicating the benchmark's difficulty.", "conclusion": "The OlymMATH benchmark presents a rigorous and challenging evaluation framework for LLMs, facilitating bilingual assessment and highlighting the limitations of current models.", "key_contributions": ["Introduction of OlymMATH as a new benchmark for LLMs", "200 rigorously curated mathematical problems", "Bilingual assessment capability in English and Chinese"], "limitations": "", "keywords": ["mathematical reasoning", "LLMs", "benchmark", "bilingual assessment", "evaluation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729.pdf", "abs": "https://arxiv.org/abs/2503.21729", "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).", "AI": {"tldr": "ReaRAG improves factual accuracy and reasoning in Large Reasoning Models (LRMs) by utilizing a structured query approach and a novel action selection framework for enhanced question answering.", "motivation": "Existing LRMs struggle with factual accuracy and reasoning robustness in question answering tasks, leading to suboptimal performance.", "method": "We propose ReaRAG, which combines a data construction framework with an upper bound on reasoning chain length, allowing LRMs to iterate through searches and reasoning steps effectively.", "result": "ReaRAG outperforms existing models on multi-hop question answering tasks and demonstrates the ability to recognize errors during reasoning.", "conclusion": "The integration of strong reasoning capabilities with factuality in ReaRAG significantly enhances the performance of LRMs in Retrieval-Augmented Generation applications.", "key_contributions": ["Novel action space for search and finish actions in reasoning", "Factuality-enhanced reasoning model", "Improved performance on multi-hop question answering tasks"], "limitations": "", "keywords": ["Large Reasoning Models", "Factuality", "Retrieval-Augmented Generation", "Question Answering", "Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.21805", "pdf": "https://arxiv.org/pdf/2503.21805.pdf", "abs": "https://arxiv.org/abs/2503.21805", "title": "ImF: Implicit Fingerprint for Large Language Models", "authors": ["Wu jiaxuan", "Peng Wanli", "Fu hang", "Xue Yiming", "Wen juan"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "Training large language models (LLMs) is resource-intensive and expensive,\nmaking protecting intellectual property (IP) for LLMs crucial. Recently,\nembedding fingerprints into LLMs has emerged as a prevalent method for\nestablishing model ownership. However, existing fingerprinting techniques\ntypically embed identifiable patterns with weak semantic coherence, resulting\nin fingerprints that significantly differ from the natural question-answering\n(QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of\nthe embedded fingerprints and makes them vulnerable to adversarial attacks. In\nthis paper, we first demonstrate the critical vulnerability of existing\nfingerprint embedding methods by introducing a novel adversarial attack named\nGeneration Revision Intervention (GRI) attack. GRI attack exploits the semantic\nfragility of current fingerprinting methods, effectively erasing fingerprints\nby disrupting their weakly correlated semantic structures. Our empirical\nevaluation highlights that traditional fingerprinting approaches are\nsignificantly compromised by the GRI attack, revealing severe limitations in\ntheir robustness under realistic adversarial conditions. To advance the\nstate-of-the-art in model fingerprinting, we propose a novel model fingerprint\nparadigm called Implicit Fingerprints (ImF). ImF leverages steganography\ntechniques to subtly embed ownership information within natural texts,\nsubsequently using Chain-of-Thought (CoT) prompting to construct semantically\ncoherent and contextually natural QA pairs. This design ensures that\nfingerprints seamlessly integrate with the standard model behavior, remaining\nindistinguishable from regular outputs and substantially reducing the risk of\naccidental triggering and targeted removal. We conduct a comprehensive\nevaluation of ImF on 15 diverse LLMs, spanning different architectures and\nvarying scales.", "AI": {"tldr": "This paper addresses the vulnerabilities in current fingerprinting techniques for large language models (LLMs) and proposes a novel method called Implicit Fingerprints (ImF) to protect intellectual property more effectively.", "motivation": "The high resource demands and costs of training LLMs make IP protection essential, yet existing methods for embedding fingerprints lack semantic coherence and are prone to attacks.", "method": "The authors propose a new fingerprinting paradigm called Implicit Fingerprints, which utilizes steganography techniques to embed ownership information into texts naturally, integrating seamlessly with LLM outputs.", "result": "The proposed ImF method was rigorously evaluated on 15 different LLMs, demonstrating better resilience against adversarial attacks compared to traditional methods.", "conclusion": "Implicit Fingerprints significantly improve the robustness of model ownership protection in LLMs, maintaining natural output quality and reducing vulnerability to adversarial interventions.", "key_contributions": ["Introduction of the Generation Revision Intervention (GRI) attack to demonstrate flaws in existing methods.", "Development of the Implicit Fingerprints (ImF) paradigm leveraging steganography for better IP protection.", "Empirical validation of ImF across diverse LLM models, showcasing improved robustness."], "limitations": "", "keywords": ["large language models", "fingerprinting", "adversarial attacks", "intellectual property", "stealthiness"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2503.22388", "pdf": "https://arxiv.org/pdf/2503.22388.pdf", "abs": "https://arxiv.org/abs/2503.22388", "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future. DSDBench is publicly available at\ngithub.com/KevinCL16/DSDBench.", "AI": {"tldr": "Introduction of DSDBench, a benchmark for evaluating LLMs in debugging complex data science code.", "motivation": "The need for a systematic way to evaluate LLMs on their ability to identify and fix logical runtime errors in data science code, as existing benchmarks focus on simpler tasks.", "method": "DSDBench adapts datasets from existing benchmarks like DABench and MatPlotBench, featuring multi-hop, multi-bug debugging tasks with 1,117 annotated samples and 741 cause-effect error pairs.", "result": "Evaluations reveal significant performance gaps in state-of-the-art LLMs when debugging logical errors, highlighting the challenges faced in this area.", "conclusion": "DSDBench serves as a vital resource for evaluating and enhancing LLMs' debugging capabilities, promoting more reliable AI-assisted data science.", "key_contributions": ["Introduction of DSDBench as the first benchmark for multi-hop error tracing in data science code.", "Inclusion of a substantial dataset with realistic debugging tasks including multi-bug scenarios.", "Highlighting the performance gaps in current LLMs when faced with complex debugging tasks."], "limitations": "", "keywords": ["LLMs", "data science", "benchmark", "debugging", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.23513", "pdf": "https://arxiv.org/pdf/2503.23513.pdf", "abs": "https://arxiv.org/abs/2503.23513", "title": "RARE: Retrieval-Augmented Reasoning Modeling", "authors": ["Zhengren Wang", "Jiayang Yu", "Dongsheng Ma", "Zhe Chen", "Yu Wang", "Zhiyu Li", "Feiyu Xiong", "Yanfeng Wang", "Weinan E", "Linpeng Tang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": "Repo: https://github.com/Open-DataFlow/RARE", "summary": "Domain-specific intelligence demands specialized knowledge and sophisticated\nreasoning for problem-solving, posing significant challenges for large language\nmodels (LLMs) that struggle with knowledge hallucination and inadequate\nreasoning capabilities under constrained parameter budgets. Inspired by Bloom's\nTaxonomy in educational theory, we propose Retrieval-Augmented Reasoning\nModeling (RARE), a novel paradigm that decouples knowledge storage from\nreasoning optimization. RARE externalizes domain knowledge to retrievable\nsources and internalizes domain-specific reasoning patterns during training.\nSpecifically, by injecting retrieved knowledge into training prompts with\nmasked losses, RARE transforms learning objectives from rote memorization to\ncontextualized reasoning. It enables models to bypass parameter-intensive\nmemorization and prioritize the development of higher-order cognitive\nprocesses. Extensive experiments demonstrate that lightweight RARE-trained\nmodels (e.g., Llama-3.1-8B) could achieve state-of-the-art performance,\nsurpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\\%\naccuracy. RARE establishes a paradigm shift where maintainable external\nknowledge bases synergize with compact, reasoning-optimized models,\ncollectively driving more scalable domain-specific intelligence.", "AI": {"tldr": "RARE improves domain-specific intelligence in LLMs by decoupling knowledge storage from reasoning optimization, enabling better contextualized reasoning through retrieval-augmented training.", "motivation": "To address the limitations of LLMs in handling domain-specific tasks due to knowledge hallucination and reasoning inefficiencies under constrained parameter budgets.", "method": "RARE externalizes domain knowledge to retrievable sources while internalizing reasoning patterns during training, using retrieved knowledge in prompts and masked losses to shift learning from memorization to reasoning.", "result": "RARE-trained models achieve state-of-the-art performance, surpassing traditional models like GPT-4 and DeepSeek-R1 by up to 20% in accuracy.", "conclusion": "RARE represents a paradigm shift, allowing for scalable domain-specific intelligence through the combination of external knowledge bases and reasoning-optimized models.", "key_contributions": ["Decoupling of knowledge storage and reasoning optimization in LLMs", "Use of retrieval-augmented training to enhance reasoning capabilities", "State-of-the-art performance metrics surpassing existing models"], "limitations": "", "keywords": ["Retrieval-Augmented Reasoning Modeling", "knowledge hallucination", "domain-specific intelligence", "machine learning", "large language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.04050", "pdf": "https://arxiv.org/pdf/2504.04050.pdf", "abs": "https://arxiv.org/abs/2504.04050", "title": "FISH-Tuning: Enhancing PEFT Methods with Fisher Information", "authors": ["Kang Xue", "Ming Dong", "Xinhui Tu", "Tingting He"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth in the parameter size of Large Language Models (LLMs) has\nspurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to\nmitigate the substantial computational costs of fine-tuning. Among these,\nFisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT\ntechnique that identifies a critical subset of pre-trained parameters using\napproximate Fisher information. While addition-based and\nreparameterization-based PEFT methods like LoRA and Adapter already fine-tune\nonly a small number of parameters, the newly introduced parameters within these\nmethods themselves present an opportunity for further optimization. Selectively\nfine-tuning only the most impactful among these new parameters could further\nreduce resource consumption while maintaining, or even improving, fine-tuning\neffectiveness. In this paper, we propose \\textbf{FISH-Tuning}, a novel approach\nthat incorporates FISH Mask into such PEFT methods, including LoRA, Adapter,\nand their variants. By leveraging Fisher information to identify and update\nonly the most significant parameters within these added or reparameterized\ncomponents, FISH-Tuning aims to achieve superior performance without increasing\ntraining time or inference latency compared to the vanilla PEFT methods.\nExperimental results across various datasets and pre-trained models demonstrate\nthat FISH-Tuning consistently outperforms the vanilla PEFT methods when using\nthe same proportion of trainable parameters. Code is available at\nhttps://anonymous.4open.science/r/FISH-Tuning-6F7C.", "AI": {"tldr": "FISH-Tuning integrates Fisher Induced Sparse uncHanging Mask with existing PEFT methods to optimize fine-tuning of Large Language Models, enhancing performance while reducing resource costs.", "motivation": "The need for efficient fine-tuning methods in the face of increasing LLM parameter sizes and computational costs.", "method": "FISH-Tuning utilizes Fisher information to select and fine-tune only the most impactful parameters from methods like LoRA and Adapter, aiming to improve performance with lower resource consumption.", "result": "Experimental results indicate that FISH-Tuning consistently outperforms vanilla PEFT methods, achieving better performance without added training time or inference latency.", "conclusion": "FISH-Tuning offers a practical improvement to parameter-efficient fine-tuning by selectively targeting important parameters, providing benefits in both efficiency and effectiveness.", "key_contributions": ["Introduction of FISH-Tuning for PEFT methods.", "Demonstration of significant performance gains over vanilla approaches.", "Evidence of reduced resource consumption during fine-tuning."], "limitations": "", "keywords": ["Large Language Models", "Parameter-Efficient Fine-Tuning", "Fisher Information"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.05831", "pdf": "https://arxiv.org/pdf/2504.05831.pdf", "abs": "https://arxiv.org/abs/2504.05831", "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "authors": ["Mingye Zhu", "Yi Liu", "Zheren Fu", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": null, "summary": "Preference alignment methods are increasingly critical for steering large\nlanguage models (LLMs) to generate outputs consistent with human values. While\nrecent approaches often rely on synthetic data generated by LLMs for\nscalability and cost-efficiency reasons, this reliance can introduce\ndistribution shifts that undermine the nuanced representation of human\npreferences needed for desirable outputs. In this paper, we propose a novel\ndistribution-aware optimization framework that improves preference alignment\ndespite such shifts. Our approach first leverages well-learned classifiers to\nassign a calibration value to each training sample, quantifying its alignment\nwith the target human-preferred distribution. These values are then\nincorporated into a robust optimization objective that minimizes the worst-case\nloss over regions of the data space most relevant to human preferences. By\nexplicitly focusing optimization on the target distribution, our approach\nmitigates the impact of distributional mismatch and improves the generation of\nresponses that better reflect intended values.", "AI": {"tldr": "The paper presents a distribution-aware optimization framework to enhance preference alignment in large language models despite reliance on synthetic data, addressing the challenges of distribution shifts in reflecting human values.", "motivation": "To improve the alignment of large language models with human preferences by addressing the distribution shifts introduced by using synthetic data for training.", "method": "The framework assigns calibration values to training samples based on their alignment with a human-preferred distribution, and incorporates these values into a robust optimization objective to focus on minimizing losses in relevant data regions.", "result": "The proposed method reduces the impact of distributional mismatch and enhances the generation of responses that reflect human values more accurately.", "conclusion": "By focusing optimization efforts on the target distribution, the framework effectively improves the output of large language models in terms of alignment with human preferences.", "key_contributions": ["Introduction of a novel distribution-aware optimization framework", "Utilization of well-learned classifiers for assigning calibration values", "Improvement in generating outputs that reflect intended human values despite data shifts."], "limitations": "", "keywords": ["preference alignment", "large language models", "distribution-aware optimization", "human values", "synthetic data"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.07433", "pdf": "https://arxiv.org/pdf/2504.07433.pdf", "abs": "https://arxiv.org/abs/2504.07433", "title": "LSR-MCTS: Alleviating Long Range Dependency in Code Generation", "authors": ["Tingwei Lu", "Yangning Li", "Liyuan Wang", "Binghuai Lin", "Jiwei Tang", "Qingsong Lv", "Wanshi Xu", "Hai-Tao Zheng", "Yinghui Li", "Xin Su", "Zifei Shan"], "categories": ["cs.CL"], "comment": null, "summary": "The emergence of large language models (LLMs) has significantly promoted the\ndevelopment of code generation task, sparking a surge in pertinent literature.\nCurrent research is hindered by redundant generation results and a tendency to\noverfit local patterns in the short term. Although existing studies attempt to\nalleviate the issue by adopting a multi-token prediction strategy, there\nremains limited focus on choosing the appropriate processing length for\ngenerations. By analyzing the attention between tokens during the generation\nprocess of LLMs, it can be observed that the high spikes of the attention\nscores typically appear at the end of lines. This insight suggests that it is\nreasonable to treat each line of code as a fundamental processing unit and\ngenerate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS}\nalgorithm, which leverages MCTS to determine the code line-by-line and select\nthe optimal path. Further, we integrate a self-refine mechanism at each node to\nenhance diversity and generate higher-quality programs through error\ncorrection. Extensive experiments and comprehensive analyses on three public\ncoding benchmarks demonstrate that our method outperforms the state-of-the-art\nperformance approaches.", "AI": {"tldr": "The paper proposes the LSR-MCTS algorithm for code generation that processes code line-by-line to enhance generation quality and diversity.", "motivation": "To address issues of redundancy and overfitting in current code generation tasks undertaken by large language models (LLMs).", "method": "The LSR-MCTS algorithm employs Monte Carlo Tree Search (MCTS) to generate code sequentially, line-by-line, while implementing a self-refine mechanism at each node to improve diversity and quality.", "result": "Extensive experiments indicate that the LSR-MCTS algorithm achieves superior performance over state-of-the-art approaches on three public coding benchmarks.", "conclusion": "The proposed approach effectively addresses redundancy in code generation and enhances the quality of generated programs through a structured line-by-line generation strategy.", "key_contributions": ["Introduction of the LSR-MCTS algorithm for sequential code generation", "Utilization of line-based processing as a fundamental unit for code generation", "Implementation of a self-refine mechanism for improving generation diversity and quality."], "limitations": "", "keywords": ["code generation", "large language models", "Monte Carlo Tree Search", "self-refine mechanism", "program quality"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.08300", "pdf": "https://arxiv.org/pdf/2504.08300.pdf", "abs": "https://arxiv.org/abs/2504.08300", "title": "Large Language Models Could Be Rote Learners", "authors": ["Yuyang Xu", "Renjun Hu", "Haochao Ying", "Jian Wu", "Xing Shi", "Wei Lin"], "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework\nreformulating MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).", "AI": {"tldr": "This study addresses the reliability of multiple-choice question benchmarks for evaluating Large Language Models by proposing a novel evaluation framework called TrinEval, aimed at reducing memorization while maintaining knowledge assessment. ", "motivation": "The study aims to improve the evaluation of Large Language Models (LLMs) by addressing the issue of benchmark contamination, which affects the reliability of multiple-choice question (MCQ) evaluations.", "method": "An analysis of LLM performance under varying memorization conditions was conducted, revealing that LLMs perform worse on memorized MCQs compared to non-memorized ones. The novel evaluation framework, TrinEval, reformulates MCQs into a trinity format to reduce memorization and better assess knowledge.", "result": "TrinEval was validated through experiments, demonstrating its effectiveness in reducing memorization while enabling accurate knowledge assessment. It was found that common LLMs memorize approximately 20.5% of knowledge points in standard benchmarks such as MMLU.", "conclusion": "The findings indicate that careful evaluation is essential to distinguish between rote memorization and genuine learning in LLMs, with TrinEval providing a more reliable assessment method.", "key_contributions": ["Introduction of TrinEval evaluation framework", "Insights into LLM memorization vs. capability learning", "Quantification of memorization in LLMs (20.5% knowledge points memorized)"], "limitations": "Results are based on specific conditions and may vary with different model architectures or datasets.", "keywords": ["Large Language Models", "Evaluation Framework", "Memorization", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2504.10198", "pdf": "https://arxiv.org/pdf/2504.10198.pdf", "abs": "https://arxiv.org/abs/2504.10198", "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation", "authors": ["Hanghui Guo", "Jia Zhu", "Shimin Di", "Weijie Shi", "Zhangze Chen", "Jiajie Xu"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main", "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work.", "AI": {"tldr": "DioR is a novel dynamic retrieval-augmented generation method that enhances large language model performance by optimizing retrieval triggers and content.", "motivation": "Existing dynamic RAG methods struggle with controlling retrieval triggers and evaluating retrieval content, leading to hallucinations in LLMs.", "method": "DioR comprises two components: adaptive cognitive detection to assess when retrieval is necessary, and contextual retrieval optimization to select appropriate content for retrieval.", "result": "DioR demonstrates superior performance across all tested tasks, effectively addressing the limitations of current dynamic RAG approaches.", "conclusion": "The results show that DioR significantly improves LLM outputs and reduces hallucinations by optimizing the retrieval process.", "key_contributions": ["Introduces adaptive cognitive detection for improved control of retrieval triggers.", "Develops contextual retrieval optimization for better content selection.", "Demonstrates significant performance improvements over existing dynamic RAG methods."], "limitations": "", "keywords": ["Dynamic Retrieval-augmented Generation", "Large Language Models", "Adaptive Cognitive Detection"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.12052", "pdf": "https://arxiv.org/pdf/2504.12052.pdf", "abs": "https://arxiv.org/abs/2504.12052", "title": "Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events", "authors": ["FranÃ§ois Haguinet", "Jeffery L Painter", "Gregory E Powell", "Andrea Callegaro", "Andrew Bate"], "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": "32 pages, 7 figures, 5 supplementary figures", "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwith a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from clinically similar\nMedDRA Preferred Terms (PTs) to the target PT. This continuous similarity-based\nborrowing overcomes limitations of rigid hierarchical grouping in current\ndisproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate our approach -- termed IC SSM -- against traditional\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term level (IC HLGT). A reference set (PVLens), derived from\nFDA product label update, enabled prospective evaluation of method performance\nin identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated higher sensitivity (1332/2337=0.570,\nYouden's J=0.246) than traditional IC (Se=0.501, J=0.250) and IC HLGT\n(Se=0.556, J=0.225), consistently identifying more true positives and doing so\non average 5 months sooner than traditional IC. Despite a marginally lower\naggregate F1-score and Youden's index, IC SSM showed higher performance in\nearly post-marketing periods or when the detection threshold was raised,\nproviding more stable and relevant alerts than IC HLGT and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods, with\npotential for validation across other datasets and exploration of additional\nsimilarity metrics and Bayesian strategies using case-level data.", "AI": {"tldr": "This paper introduces a Bayesian dynamic borrowing approach to improve the detection of adverse events in spontaneous reporting systems using semantic similarity measures, showing superior performance compared to traditional methods.", "motivation": "The need for effective and sensitive identification of adverse events in spontaneous reporting systems due to limitations in existing disproportionality analysis methods.", "method": "A Bayesian dynamic borrowing approach that integrates robust meta-analytic predictive priors with a Bayesian hierarchical model and utilizes semantic similarity measures for information sharing during adverse event detection.", "result": "The proposed IC SSM method outperformed traditional Information Component analysis and other hierarchical approaches in terms of sensitivity and timeliness of detecting adverse events using FDA reporting data.", "conclusion": "The IC SSM approach provides a more effective method for identifying adverse events, suggesting potential for broader applications with different datasets and similarity metrics.", "key_contributions": ["Introduction of Bayesian dynamic borrowing for adverse event detection", "Utilization of semantic similarity measures to enhance information sharing", "Demonstration of improved sensitivity and earlier detection of true positives over traditional methods."], "limitations": "The method may demonstrate a marginally lower aggregate F1-score and Youden's index compared to traditional methods in some contexts.", "keywords": ["adverse events", "Bayesian analysis", "semantic similarity measures", "spontaneous reporting systems", "disproportionality analysis"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534.pdf", "abs": "https://arxiv.org/abs/2504.13534", "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning boosts large language models' (LLMs)\nperformance on complex tasks but faces two key limitations: a lack of\nreliability when solely relying on LLM-generated reasoning chains and\ninterference from natural language reasoning steps with the models' inference\nprocess, also known as the inference logic of LLMs. To address these issues, we\npropose CoT-RAG, a novel reasoning framework with three key designs: (i)\nKnowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate\nreasoning chain generation of LLMs, thereby enhancing reasoning credibility;\n(ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented\ngeneration (RAG) into knowledge graphs to retrieve relevant sub-cases and\nsub-descriptions, providing LLMs with learnable information; (iii)\nPseudo-Program Prompting Execution, which promotes greater logical rigor by\nguiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine\npublic datasets spanning three reasoning tasks reveal significant accuracy\ngains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore,\ntests on four domain-specific datasets demonstrate exceptional accuracy and\nefficient execution, underscoring its practical applicability and scalability.", "AI": {"tldr": "This paper introduces CoT-RAG, a framework that enhances large language models' reasoning capabilities by utilizing knowledge graphs, learnable information retrieval, and structured execution. It shows significant performance improvements on various reasoning tasks.", "motivation": "The paper addresses the unreliable nature of LLM-generated reasoning chains and their interference with LLM inference logic, aiming to enhance reasoning credibility and execution.", "method": "The proposed CoT-RAG framework employs three key designs: Knowledge Graph-driven CoT Generation for improving reasoning chains, Learnable Knowledge Case-aware RAG for incorporating relevant sub-cases, and Pseudo-Program Prompting to improve logical execution.", "result": "Evaluations demonstrate accuracy improvements ranging from 4.0% to 44.3% across nine datasets, along with exceptional performance in domain-specific tasks.", "conclusion": "CoT-RAG shows not only enhanced reasoning accuracy but also practical scalability for real-world applications in reasoning tasks.", "key_contributions": ["Introduction of Knowledge Graph-driven CoT Generation for better reasoning credibility.", "Integration of retrieval-augmented generation to enhance learnable information.", "Implementation of Pseudo-Program Prompting for structured logical execution."], "limitations": "", "keywords": ["Chain-of-thought reasoning", "Knowledge Graphs", "Retrieval-augmented generation", "Large language models", "Logical reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.14321", "pdf": "https://arxiv.org/pdf/2504.14321.pdf", "abs": "https://arxiv.org/abs/2504.14321", "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "authors": ["Xingyu Li", "Chen Gong", "Guohong Fu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources. To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues.", "AI": {"tldr": "This paper introduces TikTalkCoref, the first Chinese multimodal coreference dataset for real-world social media, focusing on the TikTok platform.", "motivation": "To bridge the gap in multimodal coreference resolution research by providing a dataset for real-world dialogues and improving communication and personalization in social media contexts.", "method": "The paper develops the TikTalkCoref dataset, which pairs short videos with user comment dialogues and includes annotated coreference clusters. An effective benchmark approach for multimodal coreference resolution focusing on the celebrity domain is also presented.", "result": "Extensive experiments on the TikTalkCoref dataset yield reliable benchmark results, demonstrating its effectiveness for multimodal coreference tasks in social media dialogues.", "conclusion": "The TikTalkCoref dataset will enable better research in multimodal coreference resolution for real-world applications in social media, encouraging further exploration in this field.", "key_contributions": ["Introduction of TikTalkCoref, a novel multimodal coreference dataset for Chinese social media.", "Manual annotations of coreference clusters for textual and visual modalities.", "Development of a benchmark approach for multimodal coreference resolution."], "limitations": "Focused only on the celebrity domain in social media; limited generalizability to other areas.", "keywords": ["multimodal coreference resolution", "dataset", "social media", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669.pdf", "abs": "https://arxiv.org/abs/2504.14669", "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "categories": ["cs.CL"], "comment": "11 pages, 4 figures, accepted by ACL 2025 as findings", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.", "AI": {"tldr": "TRANS-ZERO is a self-play framework for multilingual machine translation utilizing only monolingual data and LLM knowledge, achieving results comparable to supervised methods.", "motivation": "Multilingual machine translation faces challenges like data scarcity for low-resource languages and catastrophic forgetting, necessitating innovative solutions that reduce reliance on parallel data.", "method": "TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, leveraging monolingual data to enhance translation quality through self-play.", "result": "Experiments show that TRANS-ZERO achieves strong translation performance, rivaling traditionally supervised methods particularly in non-English translation directions.", "conclusion": "The G-MCTS approach significantly improves translation quality by systematically exploring semantically consistent candidates, establishing a robust framework for future developments in multilingual MT.", "key_contributions": ["Introduction of TRANS-ZERO self-play framework", "Utilization of only monolingual data for multilingual MT", "Demonstration of G-MCTS improving translation quality"], "limitations": "", "keywords": ["Multilingual Machine Translation", "Large Language Models", "Genetic Monte-Carlo Tree Search"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895.pdf", "abs": "https://arxiv.org/abs/2504.15895", "title": "Dynamic Early Exit in Reasoning Models", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Qiaowei Li", "Zheng Lin", "Li Cao", "Weiping Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 15 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and\nLiveCodeBench) show that the proposed method is consistently effective on 11\ncutting-edge reasoning LLMs of varying series and sizes, reducing the length of\nCoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3%\nto 5.0%.", "AI": {"tldr": "This paper introduces a method for large reasoning language models (LRLMs) that allows them to self-stop generating long chain-of-thought sequences when they reach confident answers, improving efficiency and accuracy without additional training.", "motivation": "The motivation is to address the inefficiencies and potential loss of accuracy associated with long chain-of-thought generation in LRLMs, where overthinking can hinder performance.", "method": "The method monitors model behavior at reasoning transition points and dynamically terminates generation when the model shows high confidence in its answer, thus self-truncating the CoT sequences.", "result": "Experiments on 10 reasoning benchmarks demonstrate that the method effectively reduces CoT sequence lengths by 19.1% to 80.1% while improving accuracy by 0.3% to 5.0% across various reasoning LLMs.", "conclusion": "The proposed method integrates seamlessly into existing reasoning LLMs, enhancing both efficiency and accuracy without the need for additional training.", "key_contributions": ["Introduction of self-truncation in CoT sequences for LRLMs", "Dynamically monitors reasoning transitions to improve generation efficiency", "Demonstrated improvements on 11 reasoning benchmarks across varying LLM sizes"], "limitations": "", "keywords": ["Large reasoning language models", "Chain-of-thought", "Self-truncation", "Dynamic termination", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074.pdf", "abs": "https://arxiv.org/abs/2504.16074", "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Jiaming Ji", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Yaodong Yang", "Muhan Zhang", "Hua Xing Zhu"], "categories": ["cs.CL"], "comment": "34 pages ,12 figures, 7 tables, latest update in 2025/05/18", "summary": "Current benchmarks for evaluating the reasoning capabilities of Large\nLanguage Models (LLMs) face significant limitations: task oversimplification,\ndata contamination, and flawed evaluation items. These deficiencies necessitate\nmore rigorous assessment methods. To address these limitations, we introduce\nPHYBench, a benchmark of 500 original physics problems ranging from high school\nto Physics Olympiad difficulty. PHYBench addresses data contamination through\noriginal content and employs a systematic curation pipeline to eliminate flawed\nitems. Evaluations show that PHYBench activates more tokens and provides\nstronger differentiation between reasoning models compared to other baselines\nlike AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini\n2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To\nfurther enhance evaluation precision, we introduce the Expression Edit Distance\n(EED) Score for mathematical expression assessment, which improves sample\nefficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits\nmulti-step and multi-condition reasoning, providing a platform for examining\nmodels' reasoning robustness, preferences, and deficiencies. The benchmark\nresults and dataset are publicly available at https://www.phybench.cn/.", "AI": {"tldr": "PHYBench is a new benchmark of 500 physics problems designed to overcome current limitations in evaluating reasoning capabilities of LLMs.", "motivation": "Current benchmarks for LLMs have significant limitations such as task oversimplification, data contamination, and flawed evaluation items, which necessitate more rigorous assessment methods.", "method": "PHYBench consists of 500 original physics problems of varying difficulties, employing a systematic curation pipeline to eliminate flawed items and introducing a new scoring method for mathematical expressions.", "result": "Evaluations demonstrate that PHYBench activates more tokens and provides better differentiation among reasoning models than existing benchmarks. The best model tested scored only 36.9% accuracy, indicating room for improvement compared to human experts' 61.9%.", "conclusion": "PHYBench offers a robust platform for evaluating reasoning in LLMs, revealing insights into their reasoning robustness and deficiencies, and includes a newly developed scoring system to improve evaluation precision.", "key_contributions": ["Introduction of PHYBench with 500 original physics problems.", "Systematic curation pipeline to reduce data contamination and flawed items.", "Development of the Expression Edit Distance (EED) Score for improved mathematical assessment."], "limitations": "", "keywords": ["Large Language Models", "Benchmark", "Physics Problems", "Reasoning", "Evaluation"], "importance_score": 6, "read_time_minutes": 34}}
{"id": "2505.10527", "pdf": "https://arxiv.org/pdf/2505.10527.pdf", "abs": "https://arxiv.org/abs/2505.10527", "title": "WorldPM: Scaling Human Preference Modeling", "authors": ["Binghai Wang", "Runji Lin", "Keming Lu", "Le Yu", "Zhenru Zhang", "Fei Huang", "Chujie Zheng", "Kai Dang", "Yang Fan", "Xingzhang Ren", "An Yang", "Binyuan Hui", "Dayiheng Liu", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang", "Bowen Yu", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.", "AI": {"tldr": "This paper introduces World Preference Modeling (WorldPM), highlighting the scalability potential of preference modeling in language models.", "motivation": "To explore the scaling laws in preference modeling similar to those observed in language modeling, particularly in how model and dataset sizes influence performance.", "method": "WorldPM was developed using preference data from public forums, with training conducted on models ranging from 1.5B to 72B parameters using 15M-scale data.", "result": "Distinct patterns were observed where adversarial metrics scale with data/model size, objective metrics show emergent behavior in larger models, and subjective metrics do not scale. The integration of WorldPM improved generalization performance significantly across various datasets.", "conclusion": "WorldPM shows potential for enhancing human preference datasets and can be effectively integrated into evaluation frameworks, leading to notable performance improvements in preference tasks.", "key_contributions": ["Introduction of World Preference Modeling (WorldPM) for preference modeling", "Demonstration of distinct scaling behaviors in different types of metrics", "Validation of WorldPM's improvements across various human preference benchmarks"], "limitations": "", "keywords": ["World Preference Modeling", "preference modeling", "scalability", "language models", "evaluation metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2309.12555", "pdf": "https://arxiv.org/pdf/2309.12555.pdf", "abs": "https://arxiv.org/abs/2309.12555", "title": "PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent", "authors": ["Donghoon Shin", "Gary Hsieh", "Young-Ho Kim"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": "17 pages including reference. Accepted to ACM CUI 2025", "summary": "Creating personalized and actionable exercise plans often requires iteration\nwith experts, which can be costly and inaccessible to many individuals. This\nwork explores the capabilities of Large Language Models (LLMs) in addressing\nthese challenges. We present PlanFitting, an LLM-driven conversational agent\nthat assists users in creating and refining personalized weekly exercise plans.\nBy engaging users in free-form conversations, PlanFitting helps elicit users'\ngoals, availabilities, and potential obstacles, and enables individuals to\ngenerate personalized exercise plans aligned with established exercise\nguidelines. Our study -- involving a user study, intrinsic evaluation, and\nexpert evaluation -- demonstrated PlanFitting's ability to guide users to\ncreate tailored, actionable, and evidence-based plans. We discuss future design\nopportunities for LLM-driven conversational agents to create plans that better\ncomply with exercise principles and accommodate personal constraints.", "AI": {"tldr": "This paper presents PlanFitting, an LLM-driven conversational agent that assists users in creating personalized weekly exercise plans through interactive dialogue.", "motivation": "The need for personalized exercise plans often requires expert input, which can be costly and inaccessible, motivating the exploration of LLMs for this purpose.", "method": "The study utilizes a conversational agent, PlanFitting, to engage users in dialogue about their fitness goals, availability, and obstacles to develop tailored exercise plans.", "result": "User studies and evaluations showed that PlanFitting effectively creates actionable, personalized exercise plans aligned with established guidelines.", "conclusion": "The findings suggest that LLM-driven conversational agents can significantly enhance personalized exercise planning, with future design opportunities to improve compliance with exercise principles.", "key_contributions": ["Introduction of PlanFitting as a novel LLM-driven assistant for exercise planning.", "Evidence-based evaluation of PlanFitting's effectiveness in user engagement and plan customization.", "Identification of design opportunities for future conversational agents in fitness."], "limitations": "The paper may not address the scalability of the solution or the inclusivity of diverse user populations.", "keywords": ["Large Language Models", "Personalized Exercise Plans", "Conversational Agents"], "importance_score": 9, "read_time_minutes": 17}}
